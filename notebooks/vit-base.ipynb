{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit(model_name):\n",
    "    if model_name == 'tiny': model = timm.create_model('vit_tiny_patch16_224', pretrained=False, num_classes=4)\n",
    "    if model_name == 'small': model = timm.create_model('vit_small_patch16_224', pretrained=False, num_classes=4)\n",
    "    if model_name == 'base': model = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=4)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "dataset = datasets.ImageFolder(root='/home/ubuntu/Parkinson-Project/non-keyframes/energy_images', transform=transform)\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_size = int(total_size * 0.8) \n",
    "validation_size = int(total_size * 0.1) \n",
    "test_size = total_size - train_size - validation_size\n",
    "generator = torch.Generator().manual_seed(0) \n",
    "train_dataset, validation_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size], generator=generator)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'vit-base-130-epochs-early-stopping'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (patch_drop): Identity()\n",
      "  (norm_pre): Identity()\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (fc_norm): Identity()\n",
      "  (head_drop): Dropout(p=0.0, inplace=False)\n",
      "  (head): Linear(in_features=768, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_type = 'base'\n",
    "model = create_vit(model_type)\n",
    "\n",
    "# Model summary to check architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Batch 1, Loss: 1.436512, Accuracy: 39.06%\n",
      "Batch 2, Loss: 3.353968, Accuracy: 33.59%\n",
      "Batch 3, Loss: 2.269833, Accuracy: 35.42%\n",
      "Batch 4, Loss: 2.297403, Accuracy: 32.03%\n",
      "Batch 5, Loss: 1.784640, Accuracy: 32.19%\n",
      "Batch 6, Loss: 1.389590, Accuracy: 30.21%\n",
      "Batch 7, Loss: 1.519007, Accuracy: 31.25%\n",
      "Batch 8, Loss: 1.966599, Accuracy: 31.84%\n",
      "Batch 9, Loss: 1.614476, Accuracy: 31.42%\n",
      "Batch 10, Loss: 1.753208, Accuracy: 30.94%\n",
      "Batch 11, Loss: 1.532360, Accuracy: 31.53%\n",
      "Batch 12, Loss: 1.703444, Accuracy: 31.77%\n",
      "Batch 13, Loss: 1.273622, Accuracy: 31.97%\n",
      "Batch 14, Loss: 1.462834, Accuracy: 31.58%\n",
      "Batch 15, Loss: 1.549080, Accuracy: 31.25%\n",
      "Batch 16, Loss: 1.585295, Accuracy: 30.76%\n",
      "Batch 17, Loss: 1.382443, Accuracy: 30.24%\n",
      "Batch 18, Loss: 1.296087, Accuracy: 30.21%\n",
      "Batch 19, Loss: 1.355434, Accuracy: 30.10%\n",
      "Batch 20, Loss: 1.280186, Accuracy: 30.70%\n",
      "Batch 21, Loss: 1.361238, Accuracy: 30.43%\n",
      "Batch 22, Loss: 1.424227, Accuracy: 30.61%\n",
      "Batch 23, Loss: 1.314524, Accuracy: 31.18%\n",
      "Batch 24, Loss: 1.417202, Accuracy: 31.05%\n",
      "Batch 25, Loss: 1.376943, Accuracy: 31.00%\n",
      "Batch 26, Loss: 1.224887, Accuracy: 31.31%\n",
      "Batch 27, Loss: 1.205937, Accuracy: 32.23%\n",
      "Batch 28, Loss: 1.292538, Accuracy: 32.20%\n",
      "Batch 29, Loss: 1.259958, Accuracy: 32.06%\n",
      "Batch 30, Loss: 1.235128, Accuracy: 32.14%\n",
      "Batch 31, Loss: 1.321348, Accuracy: 32.06%\n",
      "Batch 32, Loss: 1.332324, Accuracy: 31.98%\n",
      "Batch 33, Loss: 1.359266, Accuracy: 31.63%\n",
      "Batch 34, Loss: 1.268334, Accuracy: 31.25%\n",
      "Batch 35, Loss: 1.233685, Accuracy: 31.29%\n",
      "Batch 36, Loss: 1.365928, Accuracy: 31.38%\n",
      "Batch 37, Loss: 1.183453, Accuracy: 31.55%\n",
      "Batch 38, Loss: 1.282839, Accuracy: 31.70%\n",
      "Batch 39, Loss: 1.235983, Accuracy: 31.73%\n",
      "Batch 40, Loss: 1.303109, Accuracy: 31.95%\n",
      "Batch 41, Loss: 1.303420, Accuracy: 31.97%\n",
      "Batch 42, Loss: 1.236535, Accuracy: 32.03%\n",
      "Batch 43, Loss: 1.293522, Accuracy: 32.05%\n",
      "Batch 44, Loss: 1.316099, Accuracy: 32.14%\n",
      "Batch 45, Loss: 1.367641, Accuracy: 31.98%\n",
      "Batch 46, Loss: 1.215863, Accuracy: 32.24%\n",
      "Batch 47, Loss: 1.240282, Accuracy: 32.41%\n",
      "Batch 48, Loss: 1.243038, Accuracy: 32.55%\n",
      "Batch 49, Loss: 1.248697, Accuracy: 32.65%\n",
      "Batch 50, Loss: 1.266594, Accuracy: 32.78%\n",
      "Batch 51, Loss: 1.233563, Accuracy: 32.81%\n",
      "Batch 52, Loss: 1.278610, Accuracy: 32.81%\n",
      "Batch 53, Loss: 1.323564, Accuracy: 32.69%\n",
      "Batch 54, Loss: 1.184201, Accuracy: 32.99%\n",
      "Batch 55, Loss: 1.196775, Accuracy: 33.18%\n",
      "Batch 56, Loss: 1.192605, Accuracy: 33.40%\n",
      "Batch 57, Loss: 1.314040, Accuracy: 33.44%\n",
      "Batch 58, Loss: 1.302982, Accuracy: 33.62%\n",
      "Batch 59, Loss: 1.192752, Accuracy: 33.71%\n",
      "Batch 60, Loss: 1.274226, Accuracy: 33.62%\n",
      "Batch 61, Loss: 1.241699, Accuracy: 33.68%\n",
      "Batch 62, Loss: 1.266332, Accuracy: 33.72%\n",
      "Batch 63, Loss: 1.281524, Accuracy: 33.73%\n",
      "Batch 64, Loss: 1.386913, Accuracy: 33.67%\n",
      "Batch 65, Loss: 1.354273, Accuracy: 33.58%\n",
      "Batch 66, Loss: 1.226400, Accuracy: 33.55%\n",
      "Batch 67, Loss: 1.248069, Accuracy: 33.61%\n",
      "Batch 68, Loss: 1.314783, Accuracy: 33.52%\n",
      "Batch 69, Loss: 1.282002, Accuracy: 33.56%\n",
      "Batch 70, Loss: 1.279091, Accuracy: 33.66%\n",
      "Batch 71, Loss: 1.221634, Accuracy: 33.80%\n",
      "Batch 72, Loss: 1.259280, Accuracy: 33.83%\n",
      "Batch 73, Loss: 1.213195, Accuracy: 33.78%\n",
      "Batch 74, Loss: 1.213988, Accuracy: 33.85%\n",
      "Batch 75, Loss: 1.215192, Accuracy: 33.96%\n",
      "Batch 76, Loss: 1.446715, Accuracy: 33.88%\n",
      "Batch 77, Loss: 1.308436, Accuracy: 33.87%\n",
      "Batch 78, Loss: 1.236796, Accuracy: 33.87%\n",
      "Batch 79, Loss: 1.232799, Accuracy: 33.86%\n",
      "Batch 80, Loss: 1.188511, Accuracy: 33.98%\n",
      "Batch 81, Loss: 1.287460, Accuracy: 33.97%\n",
      "Batch 82, Loss: 1.323746, Accuracy: 33.92%\n",
      "Batch 83, Loss: 1.243636, Accuracy: 33.90%\n",
      "Batch 84, Loss: 1.277558, Accuracy: 33.97%\n",
      "Batch 85, Loss: 1.194413, Accuracy: 34.06%\n",
      "Batch 86, Loss: 1.238316, Accuracy: 34.23%\n",
      "Batch 87, Loss: 1.254518, Accuracy: 34.25%\n",
      "Batch 88, Loss: 1.369030, Accuracy: 34.30%\n",
      "Batch 89, Loss: 1.252116, Accuracy: 34.34%\n",
      "Batch 90, Loss: 1.288064, Accuracy: 34.48%\n",
      "Batch 91, Loss: 1.223712, Accuracy: 34.50%\n",
      "Batch 92, Loss: 1.276258, Accuracy: 34.53%\n",
      "Batch 93, Loss: 1.275622, Accuracy: 34.46%\n",
      "Batch 94, Loss: 1.186130, Accuracy: 34.46%\n",
      "Batch 95, Loss: 1.346781, Accuracy: 34.52%\n",
      "Batch 96, Loss: 1.180071, Accuracy: 34.64%\n",
      "Batch 97, Loss: 1.251063, Accuracy: 34.71%\n",
      "Batch 98, Loss: 1.416530, Accuracy: 34.61%\n",
      "Batch 99, Loss: 1.332171, Accuracy: 34.55%\n",
      "Batch 100, Loss: 1.312477, Accuracy: 34.56%\n",
      "Batch 101, Loss: 1.253513, Accuracy: 34.59%\n",
      "Batch 102, Loss: 1.366930, Accuracy: 34.48%\n",
      "Batch 103, Loss: 1.286276, Accuracy: 34.38%\n",
      "Batch 104, Loss: 1.350533, Accuracy: 34.24%\n",
      "Batch 105, Loss: 1.344792, Accuracy: 34.21%\n",
      "Batch 106, Loss: 1.172944, Accuracy: 34.39%\n",
      "Batch 107, Loss: 1.264111, Accuracy: 34.52%\n",
      "Batch 108, Loss: 1.228465, Accuracy: 34.77%\n",
      "Batch 109, Loss: 1.388895, Accuracy: 34.82%\n",
      "Batch 110, Loss: 1.286124, Accuracy: 34.93%\n",
      "Batch 111, Loss: 1.176291, Accuracy: 35.04%\n",
      "Batch 112, Loss: 1.324561, Accuracy: 35.11%\n",
      "Batch 113, Loss: 1.216368, Accuracy: 35.25%\n",
      "Batch 114, Loss: 1.133227, Accuracy: 35.36%\n",
      "Batch 115, Loss: 1.230223, Accuracy: 35.48%\n",
      "Batch 116, Loss: 1.366007, Accuracy: 35.43%\n",
      "Batch 117, Loss: 1.199955, Accuracy: 35.48%\n",
      "Batch 118, Loss: 1.168697, Accuracy: 35.63%\n",
      "Batch 119, Loss: 1.280017, Accuracy: 35.77%\n",
      "Batch 120, Loss: 1.214076, Accuracy: 35.86%\n",
      "Batch 121, Loss: 1.148334, Accuracy: 36.04%\n",
      "Batch 122, Loss: 1.160670, Accuracy: 36.12%\n",
      "Batch 123, Loss: 1.238359, Accuracy: 36.15%\n",
      "Batch 124, Loss: 1.142691, Accuracy: 36.29%\n",
      "Batch 125, Loss: 1.128894, Accuracy: 36.36%\n",
      "Batch 126, Loss: 1.149205, Accuracy: 36.38%\n",
      "Batch 127, Loss: 1.073872, Accuracy: 36.50%\n",
      "Batch 128, Loss: 1.071326, Accuracy: 36.65%\n",
      "Batch 129, Loss: 1.068404, Accuracy: 36.85%\n",
      "Batch 130, Loss: 1.148327, Accuracy: 36.91%\n",
      "Batch 131, Loss: 1.114192, Accuracy: 36.96%\n",
      "Batch 132, Loss: 1.054724, Accuracy: 37.10%\n",
      "Batch 133, Loss: 1.204503, Accuracy: 37.21%\n",
      "Batch 134, Loss: 1.194602, Accuracy: 37.31%\n",
      "Batch 135, Loss: 0.973761, Accuracy: 37.50%\n",
      "Batch 136, Loss: 1.111101, Accuracy: 37.58%\n",
      "Batch 137, Loss: 1.218214, Accuracy: 37.61%\n",
      "Batch 138, Loss: 1.101367, Accuracy: 37.67%\n",
      "Batch 139, Loss: 1.125014, Accuracy: 37.77%\n",
      "Batch 140, Loss: 1.015773, Accuracy: 37.88%\n",
      "Batch 141, Loss: 1.136666, Accuracy: 38.00%\n",
      "Batch 142, Loss: 1.166475, Accuracy: 38.08%\n",
      "Batch 143, Loss: 1.178308, Accuracy: 38.11%\n",
      "Batch 144, Loss: 1.029006, Accuracy: 38.22%\n",
      "Batch 145, Loss: 0.928932, Accuracy: 38.37%\n",
      "Batch 146, Loss: 1.131684, Accuracy: 38.42%\n",
      "Batch 147, Loss: 1.160020, Accuracy: 38.47%\n",
      "Batch 148, Loss: 0.970540, Accuracy: 38.62%\n",
      "Batch 149, Loss: 1.121035, Accuracy: 38.74%\n",
      "Batch 150, Loss: 1.007002, Accuracy: 38.85%\n",
      "Batch 151, Loss: 1.179363, Accuracy: 38.92%\n",
      "Batch 152, Loss: 1.127334, Accuracy: 38.95%\n",
      "Batch 153, Loss: 1.051803, Accuracy: 39.06%\n",
      "Batch 154, Loss: 1.095846, Accuracy: 39.17%\n",
      "Batch 155, Loss: 1.014486, Accuracy: 39.30%\n",
      "Batch 156, Loss: 1.012608, Accuracy: 39.41%\n",
      "Batch 157, Loss: 1.121518, Accuracy: 39.52%\n",
      "Batch 158, Loss: 0.999348, Accuracy: 39.63%\n",
      "Batch 159, Loss: 1.158380, Accuracy: 39.68%\n",
      "Batch 160, Loss: 1.153645, Accuracy: 39.71%\n",
      "Batch 161, Loss: 1.166535, Accuracy: 39.74%\n",
      "Batch 162, Loss: 1.120134, Accuracy: 39.83%\n",
      "Batch 163, Loss: 0.982113, Accuracy: 39.94%\n",
      "Batch 164, Loss: 1.071371, Accuracy: 40.01%\n",
      "Batch 165, Loss: 1.208881, Accuracy: 40.04%\n",
      "Batch 166, Loss: 1.071288, Accuracy: 40.12%\n",
      "Batch 167, Loss: 1.139311, Accuracy: 40.24%\n",
      "Batch 168, Loss: 1.046551, Accuracy: 40.35%\n",
      "Batch 169, Loss: 0.958077, Accuracy: 40.45%\n",
      "Batch 170, Loss: 0.977373, Accuracy: 40.55%\n",
      "Batch 171, Loss: 1.015745, Accuracy: 40.66%\n",
      "Batch 172, Loss: 0.973786, Accuracy: 40.77%\n",
      "Batch 173, Loss: 1.031464, Accuracy: 40.88%\n",
      "Batch 174, Loss: 1.087090, Accuracy: 40.97%\n",
      "Batch 175, Loss: 1.174331, Accuracy: 41.03%\n",
      "Batch 176, Loss: 0.971822, Accuracy: 41.16%\n",
      "Batch 177, Loss: 1.066909, Accuracy: 41.23%\n",
      "Batch 178, Loss: 1.061641, Accuracy: 41.28%\n",
      "Batch 179, Loss: 1.040156, Accuracy: 41.34%\n",
      "Batch 180, Loss: 1.026157, Accuracy: 41.41%\n",
      "Batch 181, Loss: 0.918783, Accuracy: 41.56%\n",
      "Batch 182, Loss: 1.133778, Accuracy: 41.60%\n",
      "Batch 183, Loss: 0.973719, Accuracy: 41.72%\n",
      "Batch 184, Loss: 1.042476, Accuracy: 41.81%\n",
      "Batch 185, Loss: 0.964042, Accuracy: 41.89%\n",
      "Batch 186, Loss: 0.940338, Accuracy: 42.03%\n",
      "Batch 187, Loss: 0.999164, Accuracy: 42.13%\n",
      "Batch 188, Loss: 0.807019, Accuracy: 42.28%\n",
      "Batch 189, Loss: 0.934281, Accuracy: 42.34%\n",
      "Batch 190, Loss: 1.151415, Accuracy: 42.40%\n",
      "Batch 191, Loss: 0.860926, Accuracy: 42.55%\n",
      "Batch 192, Loss: 0.860978, Accuracy: 42.67%\n",
      "Batch 193, Loss: 1.057183, Accuracy: 42.72%\n",
      "Batch 194, Loss: 0.831740, Accuracy: 42.84%\n",
      "Batch 195, Loss: 0.864618, Accuracy: 42.96%\n",
      "Batch 196, Loss: 1.043008, Accuracy: 43.05%\n",
      "Batch 197, Loss: 0.935573, Accuracy: 43.16%\n",
      "Batch 198, Loss: 0.956914, Accuracy: 43.24%\n",
      "Batch 199, Loss: 1.132009, Accuracy: 43.30%\n",
      "Batch 200, Loss: 0.867086, Accuracy: 43.46%\n",
      "Batch 201, Loss: 0.964075, Accuracy: 43.55%\n",
      "Batch 202, Loss: 0.984899, Accuracy: 43.64%\n",
      "Batch 203, Loss: 0.893385, Accuracy: 43.71%\n",
      "Batch 204, Loss: 0.919919, Accuracy: 43.80%\n",
      "Batch 205, Loss: 1.033520, Accuracy: 43.86%\n",
      "Batch 206, Loss: 0.867884, Accuracy: 43.94%\n",
      "Batch 207, Loss: 0.946316, Accuracy: 44.04%\n",
      "Batch 208, Loss: 0.943353, Accuracy: 44.12%\n",
      "Batch 209, Loss: 0.903857, Accuracy: 44.21%\n",
      "Batch 210, Loss: 0.865680, Accuracy: 44.30%\n",
      "Batch 211, Loss: 0.919070, Accuracy: 44.37%\n",
      "Batch 212, Loss: 1.017675, Accuracy: 44.44%\n",
      "Batch 213, Loss: 0.767114, Accuracy: 44.58%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6880/532418957.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch {batch_idx+1}, Loss: {loss.item():.6f}, Accuracy: {100 * correct / total:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mtrain_accuracy_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loss' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 130  \n",
    "patience = 20  \n",
    "best_val_loss = float('inf')  \n",
    "best_model = None  \n",
    "\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "train_accuracy_list = list()\n",
    "val_accuracy_list = list()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)  \n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(images)  \n",
    "        loss = criterion(outputs, labels)  \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Batch {batch_idx+1}, Loss: {loss.item():.6f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Training - Epoch {epoch+1}, Loss: {train_loss:.6f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()  \n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for batch_idx, (images, labels) in enumerate(val_loader):\n",
    "            images, labels = images.to(device), labels.to(device)  \n",
    "\n",
    "            outputs = model(images) \n",
    "            loss = criterion(outputs, labels) \n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            print(f\"Validation Batch {batch_idx+1}, Loss: {loss.item():.6f}, Accuracy: {100 * val_correct / val_total:.2f}%\")\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    print(f\"Validation - Epoch {epoch+1}, Loss: {val_loss:.6f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accuracy_list.append(val_accuracy)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    print(f'Patienceâ€”{patience_counter}')\n",
    "\n",
    "    # Check for early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "model = best_model\n",
    "torch.save(model.state_dict(), f'{experiment}-{model_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_accuracy_list, label='Train Accuracy')\n",
    "\n",
    "# Plotting the second graph\n",
    "plt.plot(val_accuracy_list, label='Val Accuracy')\n",
    "\n",
    "# Adding labels and a legend\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Train vs Val Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_loss_list, label='Train Loss')\n",
    "\n",
    "# Plotting the second graph\n",
    "plt.plot(val_loss_list, label='Val Loss')\n",
    "\n",
    "# Adding labels and a legend\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train vs Val Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        print(f\"Test Batch {batch_idx+1}, Loss: {loss.item():.6f}, Accuracy: {100 * test_correct / test_total:.2f}%\")\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"Test Set - Loss: {test_loss:.6f}, Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "# Calculate percentages for each element\n",
    "cm_percentage = (cm / np.sum(cm)) * 100\n",
    "\n",
    "# Plot confusion matrix with count and percentages\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=['MILD', 'MODERATE', 'NORMAL', 'SEVERE'],\n",
    "            yticklabels=['MILD', 'MODERATE', 'NORMAL', 'SEVERE'],\n",
    "            ax=ax)\n",
    "\n",
    "# Annotate each box with count and percentage\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm[0])):\n",
    "        ax.text(j + 0.5, i + 0.5, f'\\n\\n{cm_percentage[i, j]:.2f}%',\n",
    "                ha='center', va='center', color='black')\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
