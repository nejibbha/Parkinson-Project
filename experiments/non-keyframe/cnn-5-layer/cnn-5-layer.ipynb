{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting timm\n",
      "  Downloading timm-0.9.16-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (from timm) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /usr/lib/python3/dist-packages (from timm) (0.15.2)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from timm) (5.4.1)\n",
      "Collecting huggingface_hub (from timm)\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors (from timm)\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface_hub->timm) (3.6.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub->timm)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub->timm) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub->timm) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/lib/python3/dist-packages (from huggingface_hub->timm) (21.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface_hub->timm) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface_hub->timm) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface_hub->timm) (2020.6.20)\n",
      "Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m150.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: flatbuffers 1.12.1-git20200711.33e2d80-dfsg1-0.6 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of flatbuffers or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: safetensors, fsspec, huggingface_hub, timm\n",
      "Successfully installed fsspec-2024.2.0 huggingface_hub-0.21.4 safetensors-0.4.2 timm-0.9.16\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_5Layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_5Layer, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        dummy_input = torch.randn(1, 3, 224, 224)\n",
    "        conv_output_size = self._get_conv_output_size(dummy_input)\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(conv_output_size, 4, bias=True),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.l1_regularizer = nn.L1Loss()\n",
    "\n",
    "    def _get_conv_output_size(self, x):\n",
    "        with torch.no_grad():\n",
    "            conv_output = self.conv_layers(x)\n",
    "        return conv_output.view(x.size(0), -1).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_6Layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_6Layer, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        dummy_input = torch.randn(1, 3, 224, 224)\n",
    "        conv_output_size = self._get_conv_output_size(dummy_input)\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(conv_output_size, 4, bias=True),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.l1_regularizer = nn.L1Loss()\n",
    "\n",
    "    def _get_conv_output_size(self, x):\n",
    "        with torch.no_grad():\n",
    "            conv_output = self.conv_layers(x)\n",
    "        return conv_output.view(x.size(0), -1).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(model_name):\n",
    "    if model_name == '5-layer': model = CNN_5Layer()\n",
    "    if model_name == '6-layer': model = CNN_6Layer()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "dataset = datasets.ImageFolder(root='/home/ubuntu/Parkinson-Project/non-keyframes/energy_images', transform=transform)\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_size = int(total_size * 0.8) \n",
    "validation_size = int(total_size * 0.1) \n",
    "test_size = total_size - train_size - validation_size\n",
    "generator = torch.Generator().manual_seed(0) \n",
    "train_dataset, validation_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size], generator=generator)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'cnn-5-layer-130-epochs-early-stopping-with-regularization'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_5Layer(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU()\n",
      "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Dropout(p=0.5, inplace=False)\n",
      "    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Dropout(p=0.5, inplace=False)\n",
      "    (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): ReLU()\n",
      "    (17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=12544, out_features=4, bias=True)\n",
      "    (2): Softmax(dim=1)\n",
      "  )\n",
      "  (l1_regularizer): L1Loss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_type = '5-layer'\n",
    "model = create_cnn(model_type)\n",
    "\n",
    "# Model summary to check architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Epoch 1\n",
      "Batch 1, Loss: 1.387073, Accuracy: 29.69%\n",
      "Batch 2, Loss: 1.387005, Accuracy: 28.91%\n",
      "Batch 3, Loss: 1.343186, Accuracy: 30.21%\n",
      "Batch 4, Loss: 1.373602, Accuracy: 30.47%\n",
      "Batch 5, Loss: 1.384724, Accuracy: 30.00%\n",
      "Batch 6, Loss: 1.337503, Accuracy: 31.51%\n",
      "Batch 7, Loss: 1.336178, Accuracy: 30.80%\n",
      "Batch 8, Loss: 1.307613, Accuracy: 32.42%\n",
      "Batch 9, Loss: 1.361801, Accuracy: 32.29%\n",
      "Batch 10, Loss: 1.327869, Accuracy: 32.66%\n",
      "Batch 11, Loss: 1.330405, Accuracy: 32.53%\n",
      "Batch 12, Loss: 1.299042, Accuracy: 33.46%\n",
      "Batch 13, Loss: 1.394024, Accuracy: 33.05%\n",
      "Batch 14, Loss: 1.358895, Accuracy: 32.59%\n",
      "Batch 15, Loss: 1.344712, Accuracy: 32.92%\n",
      "Batch 16, Loss: 1.288465, Accuracy: 33.20%\n",
      "Batch 17, Loss: 1.278545, Accuracy: 33.82%\n",
      "Batch 18, Loss: 1.341334, Accuracy: 33.85%\n",
      "Batch 19, Loss: 1.299443, Accuracy: 34.21%\n",
      "Batch 20, Loss: 1.297258, Accuracy: 34.30%\n",
      "Batch 21, Loss: 1.319953, Accuracy: 34.45%\n",
      "Batch 22, Loss: 1.264321, Accuracy: 35.01%\n",
      "Batch 23, Loss: 1.219920, Accuracy: 35.80%\n",
      "Batch 24, Loss: 1.289260, Accuracy: 36.07%\n",
      "Batch 25, Loss: 1.352210, Accuracy: 35.88%\n",
      "Batch 26, Loss: 1.329318, Accuracy: 36.06%\n",
      "Batch 27, Loss: 1.329313, Accuracy: 36.23%\n",
      "Batch 28, Loss: 1.323146, Accuracy: 36.33%\n",
      "Batch 29, Loss: 1.313154, Accuracy: 36.58%\n",
      "Batch 30, Loss: 1.315994, Accuracy: 36.67%\n",
      "Batch 31, Loss: 1.282097, Accuracy: 36.90%\n",
      "Batch 32, Loss: 1.245332, Accuracy: 37.21%\n",
      "Batch 33, Loss: 1.266301, Accuracy: 37.55%\n",
      "Batch 34, Loss: 1.315676, Accuracy: 37.64%\n",
      "Batch 35, Loss: 1.383057, Accuracy: 37.54%\n",
      "Batch 36, Loss: 1.339120, Accuracy: 37.50%\n",
      "Batch 37, Loss: 1.283080, Accuracy: 37.67%\n",
      "Batch 38, Loss: 1.238204, Accuracy: 37.87%\n",
      "Batch 39, Loss: 1.286006, Accuracy: 37.98%\n",
      "Batch 40, Loss: 1.234982, Accuracy: 38.28%\n",
      "Batch 41, Loss: 1.207713, Accuracy: 38.61%\n",
      "Batch 42, Loss: 1.309579, Accuracy: 38.65%\n",
      "Batch 43, Loss: 1.267497, Accuracy: 38.84%\n",
      "Batch 44, Loss: 1.334532, Accuracy: 38.81%\n",
      "Batch 45, Loss: 1.244188, Accuracy: 38.99%\n",
      "Batch 46, Loss: 1.242929, Accuracy: 39.16%\n",
      "Batch 47, Loss: 1.289454, Accuracy: 39.26%\n",
      "Batch 48, Loss: 1.249308, Accuracy: 39.49%\n",
      "Batch 49, Loss: 1.209346, Accuracy: 39.76%\n",
      "Batch 50, Loss: 1.289614, Accuracy: 39.88%\n",
      "Batch 51, Loss: 1.284568, Accuracy: 39.89%\n",
      "Batch 52, Loss: 1.213484, Accuracy: 40.05%\n",
      "Batch 53, Loss: 1.251801, Accuracy: 40.12%\n",
      "Batch 54, Loss: 1.197887, Accuracy: 40.39%\n",
      "Batch 55, Loss: 1.354429, Accuracy: 40.34%\n",
      "Batch 56, Loss: 1.272209, Accuracy: 40.43%\n",
      "Batch 57, Loss: 1.250767, Accuracy: 40.52%\n",
      "Batch 58, Loss: 1.250212, Accuracy: 40.71%\n",
      "Batch 59, Loss: 1.203201, Accuracy: 40.89%\n",
      "Batch 60, Loss: 1.241582, Accuracy: 40.99%\n",
      "Batch 61, Loss: 1.286912, Accuracy: 41.01%\n",
      "Batch 62, Loss: 1.306875, Accuracy: 40.98%\n",
      "Batch 63, Loss: 1.260529, Accuracy: 41.05%\n",
      "Batch 64, Loss: 1.245072, Accuracy: 41.21%\n",
      "Batch 65, Loss: 1.313463, Accuracy: 41.20%\n",
      "Batch 66, Loss: 1.276333, Accuracy: 41.24%\n",
      "Batch 67, Loss: 1.274983, Accuracy: 41.28%\n",
      "Batch 68, Loss: 1.239490, Accuracy: 41.43%\n",
      "Batch 69, Loss: 1.172121, Accuracy: 41.69%\n",
      "Batch 70, Loss: 1.333929, Accuracy: 41.63%\n",
      "Batch 71, Loss: 1.240609, Accuracy: 41.77%\n",
      "Batch 72, Loss: 1.248039, Accuracy: 41.82%\n",
      "Batch 73, Loss: 1.232272, Accuracy: 41.93%\n",
      "Batch 74, Loss: 1.230850, Accuracy: 42.04%\n",
      "Batch 75, Loss: 1.218715, Accuracy: 42.15%\n",
      "Batch 76, Loss: 1.260404, Accuracy: 42.17%\n",
      "Batch 77, Loss: 1.225076, Accuracy: 42.25%\n",
      "Batch 78, Loss: 1.314723, Accuracy: 42.21%\n",
      "Batch 79, Loss: 1.263946, Accuracy: 42.27%\n",
      "Batch 80, Loss: 1.307506, Accuracy: 42.29%\n",
      "Batch 81, Loss: 1.278583, Accuracy: 42.34%\n",
      "Batch 82, Loss: 1.270267, Accuracy: 42.42%\n",
      "Batch 83, Loss: 1.229200, Accuracy: 42.51%\n",
      "Batch 84, Loss: 1.191383, Accuracy: 42.65%\n",
      "Batch 85, Loss: 1.270014, Accuracy: 42.67%\n",
      "Batch 86, Loss: 1.240650, Accuracy: 42.77%\n",
      "Batch 87, Loss: 1.261460, Accuracy: 42.83%\n",
      "Batch 88, Loss: 1.196664, Accuracy: 42.97%\n",
      "Batch 89, Loss: 1.180772, Accuracy: 43.14%\n",
      "Batch 90, Loss: 1.205553, Accuracy: 43.23%\n",
      "Batch 91, Loss: 1.171781, Accuracy: 43.39%\n",
      "Batch 92, Loss: 1.198959, Accuracy: 43.50%\n",
      "Batch 93, Loss: 1.273109, Accuracy: 43.55%\n",
      "Batch 94, Loss: 1.179927, Accuracy: 43.68%\n",
      "Batch 95, Loss: 1.236125, Accuracy: 43.75%\n",
      "Batch 96, Loss: 1.241878, Accuracy: 43.80%\n",
      "Batch 97, Loss: 1.164358, Accuracy: 43.94%\n",
      "Batch 98, Loss: 1.242076, Accuracy: 44.02%\n",
      "Batch 99, Loss: 1.226011, Accuracy: 44.11%\n",
      "Batch 100, Loss: 1.177685, Accuracy: 44.22%\n",
      "Batch 101, Loss: 1.203477, Accuracy: 44.29%\n",
      "Batch 102, Loss: 1.223472, Accuracy: 44.36%\n",
      "Batch 103, Loss: 1.262105, Accuracy: 44.39%\n",
      "Batch 104, Loss: 1.210993, Accuracy: 44.47%\n",
      "Batch 105, Loss: 1.218355, Accuracy: 44.48%\n",
      "Batch 106, Loss: 1.259680, Accuracy: 44.49%\n",
      "Batch 107, Loss: 1.318615, Accuracy: 44.44%\n",
      "Batch 108, Loss: 1.278736, Accuracy: 44.46%\n",
      "Batch 109, Loss: 1.231314, Accuracy: 44.52%\n",
      "Batch 110, Loss: 1.256149, Accuracy: 44.55%\n",
      "Batch 111, Loss: 1.188423, Accuracy: 44.64%\n",
      "Batch 112, Loss: 1.352423, Accuracy: 44.60%\n",
      "Batch 113, Loss: 1.182759, Accuracy: 44.73%\n",
      "Batch 114, Loss: 1.281201, Accuracy: 44.71%\n",
      "Batch 115, Loss: 1.270397, Accuracy: 44.74%\n",
      "Batch 116, Loss: 1.254023, Accuracy: 44.81%\n",
      "Batch 117, Loss: 1.165311, Accuracy: 44.94%\n",
      "Batch 118, Loss: 1.173989, Accuracy: 45.05%\n",
      "Batch 119, Loss: 1.237370, Accuracy: 45.08%\n",
      "Batch 120, Loss: 1.239793, Accuracy: 45.12%\n",
      "Batch 121, Loss: 1.214546, Accuracy: 45.18%\n",
      "Batch 122, Loss: 1.261810, Accuracy: 45.18%\n",
      "Batch 123, Loss: 1.285872, Accuracy: 45.17%\n",
      "Batch 124, Loss: 1.286691, Accuracy: 45.14%\n",
      "Batch 125, Loss: 1.234105, Accuracy: 45.17%\n",
      "Batch 126, Loss: 1.210748, Accuracy: 45.25%\n",
      "Batch 127, Loss: 1.276363, Accuracy: 45.21%\n",
      "Batch 128, Loss: 1.258383, Accuracy: 45.21%\n",
      "Batch 129, Loss: 1.251875, Accuracy: 45.22%\n",
      "Batch 130, Loss: 1.261731, Accuracy: 45.22%\n",
      "Batch 131, Loss: 1.240871, Accuracy: 45.26%\n",
      "Batch 132, Loss: 1.162452, Accuracy: 45.37%\n",
      "Batch 133, Loss: 1.212699, Accuracy: 45.43%\n",
      "Batch 134, Loss: 1.250841, Accuracy: 45.48%\n",
      "Batch 135, Loss: 1.225569, Accuracy: 45.52%\n",
      "Batch 136, Loss: 1.181067, Accuracy: 45.60%\n",
      "Batch 137, Loss: 1.189410, Accuracy: 45.65%\n",
      "Batch 138, Loss: 1.188381, Accuracy: 45.72%\n",
      "Batch 139, Loss: 1.174638, Accuracy: 45.81%\n",
      "Batch 140, Loss: 1.201813, Accuracy: 45.88%\n",
      "Batch 141, Loss: 1.204470, Accuracy: 45.93%\n",
      "Batch 142, Loss: 1.171005, Accuracy: 46.02%\n",
      "Batch 143, Loss: 1.174910, Accuracy: 46.08%\n",
      "Batch 144, Loss: 1.190946, Accuracy: 46.15%\n",
      "Batch 145, Loss: 1.263649, Accuracy: 46.14%\n",
      "Batch 146, Loss: 1.299910, Accuracy: 46.10%\n",
      "Batch 147, Loss: 1.241565, Accuracy: 46.12%\n",
      "Batch 148, Loss: 1.227543, Accuracy: 46.15%\n",
      "Batch 149, Loss: 1.281749, Accuracy: 46.13%\n",
      "Batch 150, Loss: 1.222744, Accuracy: 46.17%\n",
      "Batch 151, Loss: 1.208452, Accuracy: 46.20%\n",
      "Batch 152, Loss: 1.139332, Accuracy: 46.32%\n",
      "Batch 153, Loss: 1.215760, Accuracy: 46.34%\n",
      "Batch 154, Loss: 1.261931, Accuracy: 46.35%\n",
      "Batch 155, Loss: 1.279734, Accuracy: 46.33%\n",
      "Batch 156, Loss: 1.238680, Accuracy: 46.34%\n",
      "Batch 157, Loss: 1.182117, Accuracy: 46.40%\n",
      "Batch 158, Loss: 1.240538, Accuracy: 46.40%\n",
      "Batch 159, Loss: 1.241777, Accuracy: 46.44%\n",
      "Batch 160, Loss: 1.190536, Accuracy: 46.48%\n",
      "Batch 161, Loss: 1.210423, Accuracy: 46.52%\n",
      "Batch 162, Loss: 1.172182, Accuracy: 46.59%\n",
      "Batch 163, Loss: 1.207051, Accuracy: 46.63%\n",
      "Batch 164, Loss: 1.153350, Accuracy: 46.72%\n",
      "Batch 165, Loss: 1.233719, Accuracy: 46.73%\n",
      "Batch 166, Loss: 1.232203, Accuracy: 46.75%\n",
      "Batch 167, Loss: 1.204083, Accuracy: 46.79%\n",
      "Batch 168, Loss: 1.194157, Accuracy: 46.84%\n",
      "Batch 169, Loss: 1.122302, Accuracy: 46.94%\n",
      "Batch 170, Loss: 1.210507, Accuracy: 46.96%\n",
      "Batch 171, Loss: 1.260838, Accuracy: 46.96%\n",
      "Batch 172, Loss: 1.195509, Accuracy: 46.99%\n",
      "Batch 173, Loss: 1.219455, Accuracy: 47.03%\n",
      "Batch 174, Loss: 1.339575, Accuracy: 46.97%\n",
      "Batch 175, Loss: 1.161349, Accuracy: 47.03%\n",
      "Batch 176, Loss: 1.279188, Accuracy: 47.02%\n",
      "Batch 177, Loss: 1.182543, Accuracy: 47.07%\n",
      "Batch 178, Loss: 1.148589, Accuracy: 47.17%\n",
      "Batch 179, Loss: 1.233475, Accuracy: 47.19%\n",
      "Batch 180, Loss: 1.227077, Accuracy: 47.20%\n",
      "Batch 181, Loss: 1.219739, Accuracy: 47.21%\n",
      "Batch 182, Loss: 1.133562, Accuracy: 47.28%\n",
      "Batch 183, Loss: 1.153056, Accuracy: 47.32%\n",
      "Batch 184, Loss: 1.148323, Accuracy: 47.40%\n",
      "Batch 185, Loss: 1.170820, Accuracy: 47.46%\n",
      "Batch 186, Loss: 1.191274, Accuracy: 47.52%\n",
      "Batch 187, Loss: 1.237560, Accuracy: 47.55%\n",
      "Batch 188, Loss: 1.248549, Accuracy: 47.56%\n",
      "Batch 189, Loss: 1.107914, Accuracy: 47.64%\n",
      "Batch 190, Loss: 1.156598, Accuracy: 47.68%\n",
      "Batch 191, Loss: 1.259330, Accuracy: 47.68%\n",
      "Batch 192, Loss: 1.202818, Accuracy: 47.71%\n",
      "Batch 193, Loss: 1.178491, Accuracy: 47.76%\n",
      "Batch 194, Loss: 1.216303, Accuracy: 47.79%\n",
      "Batch 195, Loss: 1.145523, Accuracy: 47.85%\n",
      "Batch 196, Loss: 1.242953, Accuracy: 47.84%\n",
      "Batch 197, Loss: 1.201743, Accuracy: 47.87%\n",
      "Batch 198, Loss: 1.208593, Accuracy: 47.88%\n",
      "Batch 199, Loss: 1.288032, Accuracy: 47.86%\n",
      "Batch 200, Loss: 1.171454, Accuracy: 47.91%\n",
      "Batch 201, Loss: 1.202594, Accuracy: 47.94%\n",
      "Batch 202, Loss: 1.191636, Accuracy: 47.97%\n",
      "Batch 203, Loss: 1.397355, Accuracy: 47.88%\n",
      "Batch 204, Loss: 1.144696, Accuracy: 47.95%\n",
      "Batch 205, Loss: 1.235904, Accuracy: 47.96%\n",
      "Batch 206, Loss: 1.214716, Accuracy: 47.97%\n",
      "Batch 207, Loss: 1.249349, Accuracy: 47.95%\n",
      "Batch 208, Loss: 1.205613, Accuracy: 47.98%\n",
      "Batch 209, Loss: 1.205273, Accuracy: 48.02%\n",
      "Batch 210, Loss: 1.266372, Accuracy: 48.00%\n",
      "Batch 211, Loss: 1.181118, Accuracy: 48.03%\n",
      "Batch 212, Loss: 1.289395, Accuracy: 48.02%\n",
      "Batch 213, Loss: 1.296099, Accuracy: 47.99%\n",
      "Training - Epoch 1, Loss: 1.246797, Accuracy: 47.99%\n",
      "Validation Batch 1, Loss: 1.334823, Accuracy: 37.50%\n",
      "Validation Batch 2, Loss: 1.327242, Accuracy: 38.28%\n",
      "Validation Batch 3, Loss: 1.405379, Accuracy: 35.42%\n",
      "Validation Batch 4, Loss: 1.308294, Accuracy: 37.11%\n",
      "Validation Batch 5, Loss: 1.390733, Accuracy: 35.94%\n",
      "Validation Batch 6, Loss: 1.437524, Accuracy: 33.85%\n",
      "Validation Batch 7, Loss: 1.349618, Accuracy: 33.93%\n",
      "Validation Batch 8, Loss: 1.347427, Accuracy: 34.18%\n",
      "Validation Batch 9, Loss: 1.380111, Accuracy: 34.03%\n",
      "Validation Batch 10, Loss: 1.402895, Accuracy: 33.59%\n",
      "Validation Batch 11, Loss: 1.339652, Accuracy: 33.81%\n",
      "Validation Batch 12, Loss: 1.325050, Accuracy: 34.11%\n",
      "Validation Batch 13, Loss: 1.388447, Accuracy: 33.89%\n",
      "Validation Batch 14, Loss: 1.266010, Accuracy: 34.82%\n",
      "Validation Batch 15, Loss: 1.331962, Accuracy: 35.00%\n",
      "Validation Batch 16, Loss: 1.422026, Accuracy: 34.47%\n",
      "Validation Batch 17, Loss: 1.362944, Accuracy: 34.47%\n",
      "Validation Batch 18, Loss: 1.353573, Accuracy: 34.46%\n",
      "Validation Batch 19, Loss: 1.360774, Accuracy: 34.46%\n",
      "Validation Batch 20, Loss: 1.369790, Accuracy: 34.38%\n",
      "Validation Batch 21, Loss: 1.346465, Accuracy: 34.45%\n",
      "Validation Batch 22, Loss: 1.365029, Accuracy: 34.45%\n",
      "Validation Batch 23, Loss: 1.361345, Accuracy: 34.51%\n",
      "Validation Batch 24, Loss: 1.405393, Accuracy: 34.31%\n",
      "Validation Batch 25, Loss: 1.313425, Accuracy: 34.50%\n",
      "Validation Batch 26, Loss: 1.271661, Accuracy: 34.92%\n",
      "Validation Batch 27, Loss: 1.375704, Accuracy: 34.88%\n",
      "Validation - Epoch 1, Loss: 1.357159, Accuracy: 34.88%\n",
      "Patience—0\n",
      "Epoch 2\n",
      "Batch 1, Loss: 1.233213, Accuracy: 46.88%\n",
      "Batch 2, Loss: 1.202165, Accuracy: 51.56%\n",
      "Batch 3, Loss: 1.228961, Accuracy: 50.52%\n",
      "Batch 4, Loss: 1.253521, Accuracy: 49.61%\n",
      "Batch 5, Loss: 1.255362, Accuracy: 49.38%\n",
      "Batch 6, Loss: 1.241362, Accuracy: 48.96%\n",
      "Batch 7, Loss: 1.148440, Accuracy: 50.45%\n",
      "Batch 8, Loss: 1.170057, Accuracy: 51.56%\n",
      "Batch 9, Loss: 1.145618, Accuracy: 52.60%\n",
      "Batch 10, Loss: 1.088602, Accuracy: 54.22%\n",
      "Batch 11, Loss: 1.196408, Accuracy: 54.12%\n",
      "Batch 12, Loss: 1.117373, Accuracy: 54.95%\n",
      "Batch 13, Loss: 1.275683, Accuracy: 54.33%\n",
      "Batch 14, Loss: 1.235840, Accuracy: 53.91%\n",
      "Batch 15, Loss: 1.166250, Accuracy: 54.06%\n",
      "Batch 16, Loss: 1.159385, Accuracy: 54.20%\n",
      "Batch 17, Loss: 1.267502, Accuracy: 53.77%\n",
      "Batch 18, Loss: 1.212028, Accuracy: 53.65%\n",
      "Batch 19, Loss: 1.231430, Accuracy: 53.29%\n",
      "Batch 20, Loss: 1.225437, Accuracy: 53.05%\n",
      "Batch 21, Loss: 1.181319, Accuracy: 53.20%\n",
      "Batch 22, Loss: 1.147610, Accuracy: 53.55%\n",
      "Batch 23, Loss: 1.125156, Accuracy: 53.94%\n",
      "Batch 24, Loss: 1.117859, Accuracy: 54.43%\n",
      "Batch 25, Loss: 1.238078, Accuracy: 54.19%\n",
      "Batch 26, Loss: 1.228455, Accuracy: 54.03%\n",
      "Batch 27, Loss: 1.236092, Accuracy: 53.76%\n",
      "Batch 28, Loss: 1.259091, Accuracy: 53.57%\n",
      "Batch 29, Loss: 1.212298, Accuracy: 53.56%\n",
      "Batch 30, Loss: 1.178334, Accuracy: 53.70%\n",
      "Batch 31, Loss: 1.132643, Accuracy: 53.93%\n",
      "Batch 32, Loss: 1.197830, Accuracy: 53.86%\n",
      "Batch 33, Loss: 1.178582, Accuracy: 53.88%\n",
      "Batch 34, Loss: 1.221974, Accuracy: 53.72%\n",
      "Batch 35, Loss: 1.089115, Accuracy: 53.97%\n",
      "Batch 36, Loss: 1.159963, Accuracy: 54.08%\n",
      "Batch 37, Loss: 1.181594, Accuracy: 54.18%\n",
      "Batch 38, Loss: 1.156064, Accuracy: 54.40%\n",
      "Batch 39, Loss: 1.238635, Accuracy: 54.37%\n",
      "Batch 40, Loss: 1.109135, Accuracy: 54.61%\n",
      "Batch 41, Loss: 1.224603, Accuracy: 54.50%\n",
      "Batch 42, Loss: 1.145657, Accuracy: 54.65%\n",
      "Batch 43, Loss: 1.164379, Accuracy: 54.69%\n",
      "Batch 44, Loss: 1.275608, Accuracy: 54.51%\n",
      "Batch 45, Loss: 1.223527, Accuracy: 54.44%\n",
      "Batch 46, Loss: 1.135652, Accuracy: 54.55%\n",
      "Batch 47, Loss: 1.238663, Accuracy: 54.42%\n",
      "Batch 48, Loss: 1.249278, Accuracy: 54.30%\n",
      "Batch 49, Loss: 1.228479, Accuracy: 54.15%\n",
      "Batch 50, Loss: 1.195583, Accuracy: 54.16%\n",
      "Batch 51, Loss: 1.057899, Accuracy: 54.47%\n",
      "Batch 52, Loss: 1.285347, Accuracy: 54.33%\n",
      "Batch 53, Loss: 1.278725, Accuracy: 54.19%\n",
      "Batch 54, Loss: 1.165002, Accuracy: 54.22%\n",
      "Batch 55, Loss: 1.235973, Accuracy: 54.15%\n",
      "Batch 56, Loss: 1.244766, Accuracy: 54.05%\n",
      "Batch 57, Loss: 1.186714, Accuracy: 54.06%\n",
      "Batch 58, Loss: 1.211112, Accuracy: 54.04%\n",
      "Batch 59, Loss: 1.203186, Accuracy: 54.03%\n",
      "Batch 60, Loss: 1.222468, Accuracy: 53.98%\n",
      "Batch 61, Loss: 1.137555, Accuracy: 54.10%\n",
      "Batch 62, Loss: 1.231705, Accuracy: 54.08%\n",
      "Batch 63, Loss: 1.135480, Accuracy: 54.22%\n",
      "Batch 64, Loss: 1.224313, Accuracy: 54.13%\n",
      "Batch 65, Loss: 1.273525, Accuracy: 53.94%\n",
      "Batch 66, Loss: 1.243266, Accuracy: 53.81%\n",
      "Batch 67, Loss: 1.148911, Accuracy: 53.92%\n",
      "Batch 68, Loss: 1.229772, Accuracy: 53.88%\n",
      "Batch 69, Loss: 1.267950, Accuracy: 53.74%\n",
      "Batch 70, Loss: 1.236330, Accuracy: 53.68%\n",
      "Batch 71, Loss: 1.231216, Accuracy: 53.61%\n",
      "Batch 72, Loss: 1.140946, Accuracy: 53.71%\n",
      "Batch 73, Loss: 1.236771, Accuracy: 53.60%\n",
      "Batch 74, Loss: 1.229396, Accuracy: 53.53%\n",
      "Batch 75, Loss: 1.251824, Accuracy: 53.46%\n",
      "Batch 76, Loss: 1.260646, Accuracy: 53.33%\n",
      "Batch 77, Loss: 1.212615, Accuracy: 53.29%\n",
      "Batch 78, Loss: 1.184477, Accuracy: 53.31%\n",
      "Batch 79, Loss: 1.218109, Accuracy: 53.28%\n",
      "Batch 80, Loss: 1.189977, Accuracy: 53.32%\n",
      "Batch 81, Loss: 1.210705, Accuracy: 53.30%\n",
      "Batch 82, Loss: 1.105010, Accuracy: 53.37%\n",
      "Batch 83, Loss: 1.140632, Accuracy: 53.43%\n",
      "Batch 84, Loss: 1.262387, Accuracy: 53.35%\n",
      "Batch 85, Loss: 1.183183, Accuracy: 53.38%\n",
      "Batch 86, Loss: 1.234392, Accuracy: 53.36%\n",
      "Batch 87, Loss: 1.155730, Accuracy: 53.41%\n",
      "Batch 88, Loss: 1.109089, Accuracy: 53.55%\n",
      "Batch 89, Loss: 1.215166, Accuracy: 53.53%\n",
      "Batch 90, Loss: 1.153296, Accuracy: 53.56%\n",
      "Batch 91, Loss: 1.242321, Accuracy: 53.50%\n",
      "Batch 92, Loss: 1.310774, Accuracy: 53.35%\n",
      "Batch 93, Loss: 1.241066, Accuracy: 53.31%\n",
      "Batch 94, Loss: 1.235164, Accuracy: 53.26%\n",
      "Batch 95, Loss: 1.276781, Accuracy: 53.21%\n",
      "Batch 96, Loss: 1.247294, Accuracy: 53.14%\n",
      "Batch 97, Loss: 1.225180, Accuracy: 53.06%\n",
      "Batch 98, Loss: 1.202802, Accuracy: 53.06%\n",
      "Batch 99, Loss: 1.248850, Accuracy: 53.01%\n",
      "Batch 100, Loss: 1.174341, Accuracy: 53.00%\n",
      "Batch 101, Loss: 1.289046, Accuracy: 52.89%\n",
      "Batch 102, Loss: 1.198273, Accuracy: 52.88%\n",
      "Batch 103, Loss: 1.206710, Accuracy: 52.85%\n",
      "Batch 104, Loss: 1.219795, Accuracy: 52.87%\n",
      "Batch 105, Loss: 1.148350, Accuracy: 52.92%\n",
      "Batch 106, Loss: 1.181052, Accuracy: 52.95%\n",
      "Batch 107, Loss: 1.204185, Accuracy: 52.94%\n",
      "Batch 108, Loss: 1.180648, Accuracy: 52.99%\n",
      "Batch 109, Loss: 1.250731, Accuracy: 52.94%\n",
      "Batch 110, Loss: 1.244168, Accuracy: 52.90%\n",
      "Batch 111, Loss: 1.197987, Accuracy: 52.94%\n",
      "Batch 112, Loss: 1.250037, Accuracy: 52.87%\n",
      "Batch 113, Loss: 1.302598, Accuracy: 52.77%\n",
      "Batch 114, Loss: 1.216771, Accuracy: 52.73%\n",
      "Batch 115, Loss: 1.247023, Accuracy: 52.68%\n",
      "Batch 116, Loss: 1.242340, Accuracy: 52.61%\n",
      "Batch 117, Loss: 1.209434, Accuracy: 52.58%\n",
      "Batch 118, Loss: 1.217763, Accuracy: 52.58%\n",
      "Batch 119, Loss: 1.156320, Accuracy: 52.61%\n",
      "Batch 120, Loss: 1.157668, Accuracy: 52.68%\n",
      "Batch 121, Loss: 1.193064, Accuracy: 52.71%\n",
      "Batch 122, Loss: 1.284083, Accuracy: 52.64%\n",
      "Batch 123, Loss: 1.141582, Accuracy: 52.72%\n",
      "Batch 124, Loss: 1.239294, Accuracy: 52.68%\n",
      "Batch 125, Loss: 1.163349, Accuracy: 52.74%\n",
      "Batch 126, Loss: 1.202326, Accuracy: 52.75%\n",
      "Batch 127, Loss: 1.224945, Accuracy: 52.72%\n",
      "Batch 128, Loss: 1.197575, Accuracy: 52.73%\n",
      "Batch 129, Loss: 1.231964, Accuracy: 52.74%\n",
      "Batch 130, Loss: 1.180239, Accuracy: 52.74%\n",
      "Batch 131, Loss: 1.233555, Accuracy: 52.71%\n",
      "Batch 132, Loss: 1.181157, Accuracy: 52.75%\n",
      "Batch 133, Loss: 1.101799, Accuracy: 52.82%\n",
      "Batch 134, Loss: 1.143382, Accuracy: 52.86%\n",
      "Batch 135, Loss: 1.175088, Accuracy: 52.89%\n",
      "Batch 136, Loss: 1.193885, Accuracy: 52.91%\n",
      "Batch 137, Loss: 1.169467, Accuracy: 52.94%\n",
      "Batch 138, Loss: 1.280971, Accuracy: 52.86%\n",
      "Batch 139, Loss: 1.254615, Accuracy: 52.82%\n",
      "Batch 140, Loss: 1.227844, Accuracy: 52.81%\n",
      "Batch 141, Loss: 1.303290, Accuracy: 52.75%\n",
      "Batch 142, Loss: 1.085199, Accuracy: 52.82%\n",
      "Batch 143, Loss: 1.238489, Accuracy: 52.79%\n",
      "Batch 144, Loss: 1.155236, Accuracy: 52.81%\n",
      "Batch 145, Loss: 1.114483, Accuracy: 52.89%\n",
      "Batch 146, Loss: 1.219671, Accuracy: 52.87%\n",
      "Batch 147, Loss: 1.143242, Accuracy: 52.92%\n",
      "Batch 148, Loss: 1.136585, Accuracy: 52.97%\n",
      "Batch 149, Loss: 1.276058, Accuracy: 52.94%\n",
      "Batch 150, Loss: 1.297883, Accuracy: 52.89%\n",
      "Batch 151, Loss: 1.181706, Accuracy: 52.89%\n",
      "Batch 152, Loss: 1.184211, Accuracy: 52.89%\n",
      "Batch 153, Loss: 1.068950, Accuracy: 52.98%\n",
      "Batch 154, Loss: 1.122587, Accuracy: 53.03%\n",
      "Batch 155, Loss: 1.145226, Accuracy: 53.05%\n",
      "Batch 156, Loss: 1.124089, Accuracy: 53.14%\n",
      "Batch 157, Loss: 1.177325, Accuracy: 53.17%\n",
      "Batch 158, Loss: 1.147959, Accuracy: 53.19%\n",
      "Batch 159, Loss: 1.122448, Accuracy: 53.25%\n",
      "Batch 160, Loss: 1.217130, Accuracy: 53.22%\n",
      "Batch 161, Loss: 1.143743, Accuracy: 53.24%\n",
      "Batch 162, Loss: 1.202300, Accuracy: 53.23%\n",
      "Batch 163, Loss: 1.223901, Accuracy: 53.21%\n",
      "Batch 164, Loss: 1.187003, Accuracy: 53.24%\n",
      "Batch 165, Loss: 1.210473, Accuracy: 53.23%\n",
      "Batch 166, Loss: 1.290470, Accuracy: 53.15%\n",
      "Batch 167, Loss: 1.211408, Accuracy: 53.13%\n",
      "Batch 168, Loss: 1.227337, Accuracy: 53.11%\n",
      "Batch 169, Loss: 1.161853, Accuracy: 53.14%\n",
      "Batch 170, Loss: 1.140860, Accuracy: 53.17%\n",
      "Batch 171, Loss: 1.279207, Accuracy: 53.11%\n",
      "Batch 172, Loss: 1.219132, Accuracy: 53.09%\n",
      "Batch 173, Loss: 1.154848, Accuracy: 53.12%\n",
      "Batch 174, Loss: 1.256944, Accuracy: 53.09%\n",
      "Batch 175, Loss: 1.178655, Accuracy: 53.09%\n",
      "Batch 176, Loss: 1.212958, Accuracy: 53.08%\n",
      "Batch 177, Loss: 1.156744, Accuracy: 53.14%\n",
      "Batch 178, Loss: 1.194391, Accuracy: 53.14%\n",
      "Batch 179, Loss: 1.277291, Accuracy: 53.10%\n",
      "Batch 180, Loss: 1.155064, Accuracy: 53.14%\n",
      "Batch 181, Loss: 1.139542, Accuracy: 53.19%\n",
      "Batch 182, Loss: 1.145232, Accuracy: 53.21%\n",
      "Batch 183, Loss: 1.290684, Accuracy: 53.16%\n",
      "Batch 184, Loss: 1.143303, Accuracy: 53.19%\n",
      "Batch 185, Loss: 1.245101, Accuracy: 53.18%\n",
      "Batch 186, Loss: 1.185806, Accuracy: 53.18%\n",
      "Batch 187, Loss: 1.201457, Accuracy: 53.20%\n",
      "Batch 188, Loss: 1.148435, Accuracy: 53.22%\n",
      "Batch 189, Loss: 1.147141, Accuracy: 53.26%\n",
      "Batch 190, Loss: 1.090230, Accuracy: 53.32%\n",
      "Batch 191, Loss: 1.228612, Accuracy: 53.28%\n",
      "Batch 192, Loss: 1.092975, Accuracy: 53.35%\n",
      "Batch 193, Loss: 1.241393, Accuracy: 53.33%\n",
      "Batch 194, Loss: 1.185876, Accuracy: 53.33%\n",
      "Batch 195, Loss: 1.076670, Accuracy: 53.41%\n",
      "Batch 196, Loss: 1.066180, Accuracy: 53.47%\n",
      "Batch 197, Loss: 1.208446, Accuracy: 53.48%\n",
      "Batch 198, Loss: 1.230281, Accuracy: 53.46%\n",
      "Batch 199, Loss: 1.193647, Accuracy: 53.47%\n",
      "Batch 200, Loss: 1.190886, Accuracy: 53.50%\n",
      "Batch 201, Loss: 1.231193, Accuracy: 53.47%\n",
      "Batch 202, Loss: 1.207516, Accuracy: 53.47%\n",
      "Batch 203, Loss: 1.251587, Accuracy: 53.43%\n",
      "Batch 204, Loss: 1.155351, Accuracy: 53.47%\n",
      "Batch 205, Loss: 1.190442, Accuracy: 53.47%\n",
      "Batch 206, Loss: 1.230690, Accuracy: 53.44%\n",
      "Batch 207, Loss: 1.193375, Accuracy: 53.46%\n",
      "Batch 208, Loss: 1.122619, Accuracy: 53.50%\n",
      "Batch 209, Loss: 1.156453, Accuracy: 53.53%\n",
      "Batch 210, Loss: 1.220848, Accuracy: 53.53%\n",
      "Batch 211, Loss: 1.137656, Accuracy: 53.58%\n",
      "Batch 212, Loss: 1.134541, Accuracy: 53.61%\n",
      "Batch 213, Loss: 1.097311, Accuracy: 53.66%\n",
      "Training - Epoch 2, Loss: 1.196925, Accuracy: 53.66%\n",
      "Validation Batch 1, Loss: 1.343643, Accuracy: 37.50%\n",
      "Validation Batch 2, Loss: 1.337081, Accuracy: 38.28%\n",
      "Validation Batch 3, Loss: 1.423723, Accuracy: 35.42%\n",
      "Validation Batch 4, Loss: 1.316578, Accuracy: 37.11%\n",
      "Validation Batch 5, Loss: 1.409303, Accuracy: 35.94%\n",
      "Validation Batch 6, Loss: 1.464914, Accuracy: 33.85%\n",
      "Validation Batch 7, Loss: 1.360495, Accuracy: 33.93%\n",
      "Validation Batch 8, Loss: 1.359471, Accuracy: 34.18%\n",
      "Validation Batch 9, Loss: 1.392899, Accuracy: 34.03%\n",
      "Validation Batch 10, Loss: 1.422091, Accuracy: 33.59%\n",
      "Validation Batch 11, Loss: 1.355145, Accuracy: 33.81%\n",
      "Validation Batch 12, Loss: 1.332193, Accuracy: 34.11%\n",
      "Validation Batch 13, Loss: 1.402691, Accuracy: 33.89%\n",
      "Validation Batch 14, Loss: 1.266203, Accuracy: 34.82%\n",
      "Validation Batch 15, Loss: 1.344538, Accuracy: 35.00%\n",
      "Validation Batch 16, Loss: 1.446142, Accuracy: 34.47%\n",
      "Validation Batch 17, Loss: 1.376849, Accuracy: 34.47%\n",
      "Validation Batch 18, Loss: 1.366610, Accuracy: 34.46%\n",
      "Validation Batch 19, Loss: 1.372031, Accuracy: 34.46%\n",
      "Validation Batch 20, Loss: 1.381473, Accuracy: 34.38%\n",
      "Validation Batch 21, Loss: 1.362193, Accuracy: 34.45%\n",
      "Validation Batch 22, Loss: 1.375777, Accuracy: 34.45%\n",
      "Validation Batch 23, Loss: 1.372034, Accuracy: 34.51%\n",
      "Validation Batch 24, Loss: 1.425158, Accuracy: 34.31%\n",
      "Validation Batch 25, Loss: 1.321567, Accuracy: 34.50%\n",
      "Validation Batch 26, Loss: 1.273799, Accuracy: 34.92%\n",
      "Validation Batch 27, Loss: 1.391628, Accuracy: 34.88%\n",
      "Validation - Epoch 2, Loss: 1.370231, Accuracy: 34.88%\n",
      "Patience—1\n",
      "Epoch 3\n",
      "Batch 1, Loss: 1.246799, Accuracy: 50.00%\n",
      "Batch 2, Loss: 1.177836, Accuracy: 52.34%\n",
      "Batch 3, Loss: 1.108521, Accuracy: 56.25%\n",
      "Batch 4, Loss: 1.161429, Accuracy: 56.64%\n",
      "Batch 5, Loss: 1.133773, Accuracy: 56.56%\n",
      "Batch 6, Loss: 1.277848, Accuracy: 54.43%\n",
      "Batch 7, Loss: 1.170421, Accuracy: 54.69%\n",
      "Batch 8, Loss: 1.060246, Accuracy: 56.45%\n",
      "Batch 9, Loss: 1.268506, Accuracy: 55.03%\n",
      "Batch 10, Loss: 1.164824, Accuracy: 55.00%\n",
      "Batch 11, Loss: 1.212963, Accuracy: 54.69%\n",
      "Batch 12, Loss: 1.204283, Accuracy: 54.56%\n",
      "Batch 13, Loss: 1.172518, Accuracy: 54.69%\n",
      "Batch 14, Loss: 1.249744, Accuracy: 54.35%\n",
      "Batch 15, Loss: 1.059397, Accuracy: 55.52%\n",
      "Batch 16, Loss: 1.174990, Accuracy: 55.37%\n",
      "Batch 17, Loss: 1.242638, Accuracy: 54.87%\n",
      "Batch 18, Loss: 1.242638, Accuracy: 54.43%\n",
      "Batch 19, Loss: 1.200086, Accuracy: 54.44%\n",
      "Batch 20, Loss: 1.166774, Accuracy: 54.38%\n",
      "Batch 21, Loss: 1.200161, Accuracy: 54.46%\n",
      "Batch 22, Loss: 1.198877, Accuracy: 54.33%\n",
      "Batch 23, Loss: 1.184734, Accuracy: 54.28%\n",
      "Batch 24, Loss: 1.133344, Accuracy: 54.30%\n",
      "Batch 25, Loss: 1.119715, Accuracy: 54.50%\n",
      "Batch 26, Loss: 1.213834, Accuracy: 54.39%\n",
      "Batch 27, Loss: 1.219718, Accuracy: 54.28%\n",
      "Batch 28, Loss: 1.202872, Accuracy: 54.24%\n",
      "Batch 29, Loss: 1.211792, Accuracy: 54.09%\n",
      "Batch 30, Loss: 1.146516, Accuracy: 54.27%\n",
      "Batch 31, Loss: 1.200366, Accuracy: 54.28%\n",
      "Batch 32, Loss: 1.205694, Accuracy: 54.25%\n",
      "Batch 33, Loss: 1.214887, Accuracy: 54.26%\n",
      "Batch 34, Loss: 1.142808, Accuracy: 54.46%\n",
      "Batch 35, Loss: 1.242301, Accuracy: 54.29%\n",
      "Batch 36, Loss: 1.167588, Accuracy: 54.34%\n",
      "Batch 37, Loss: 1.153446, Accuracy: 54.52%\n",
      "Batch 38, Loss: 1.139018, Accuracy: 54.61%\n",
      "Batch 39, Loss: 1.226981, Accuracy: 54.53%\n",
      "Batch 40, Loss: 1.167791, Accuracy: 54.53%\n",
      "Batch 41, Loss: 1.180558, Accuracy: 54.69%\n",
      "Batch 42, Loss: 1.171995, Accuracy: 54.69%\n",
      "Batch 43, Loss: 1.233405, Accuracy: 54.51%\n",
      "Batch 44, Loss: 1.228752, Accuracy: 54.40%\n",
      "Batch 45, Loss: 1.160038, Accuracy: 54.51%\n",
      "Batch 46, Loss: 1.150867, Accuracy: 54.62%\n",
      "Batch 47, Loss: 1.160395, Accuracy: 54.59%\n",
      "Batch 48, Loss: 1.136947, Accuracy: 54.79%\n",
      "Batch 49, Loss: 1.149986, Accuracy: 54.91%\n",
      "Batch 50, Loss: 1.216925, Accuracy: 54.81%\n",
      "Batch 51, Loss: 1.213805, Accuracy: 54.78%\n",
      "Batch 52, Loss: 1.168893, Accuracy: 54.87%\n",
      "Batch 53, Loss: 1.190209, Accuracy: 54.81%\n",
      "Batch 54, Loss: 1.189043, Accuracy: 54.83%\n",
      "Batch 55, Loss: 1.206557, Accuracy: 54.74%\n",
      "Batch 56, Loss: 1.172196, Accuracy: 54.83%\n",
      "Batch 57, Loss: 1.293846, Accuracy: 54.61%\n",
      "Batch 58, Loss: 1.243111, Accuracy: 54.42%\n",
      "Batch 59, Loss: 1.214640, Accuracy: 54.40%\n",
      "Batch 60, Loss: 1.166428, Accuracy: 54.43%\n",
      "Batch 61, Loss: 1.159221, Accuracy: 54.43%\n",
      "Batch 62, Loss: 1.153499, Accuracy: 54.54%\n",
      "Batch 63, Loss: 1.149119, Accuracy: 54.56%\n",
      "Batch 64, Loss: 1.140066, Accuracy: 54.61%\n",
      "Batch 65, Loss: 1.173614, Accuracy: 54.71%\n",
      "Batch 66, Loss: 1.124094, Accuracy: 54.81%\n",
      "Batch 67, Loss: 1.214071, Accuracy: 54.76%\n",
      "Batch 68, Loss: 1.190124, Accuracy: 54.76%\n",
      "Batch 69, Loss: 1.168425, Accuracy: 54.76%\n",
      "Batch 70, Loss: 1.190868, Accuracy: 54.67%\n",
      "Batch 71, Loss: 1.186799, Accuracy: 54.67%\n",
      "Batch 72, Loss: 1.170968, Accuracy: 54.67%\n",
      "Batch 73, Loss: 1.271663, Accuracy: 54.49%\n",
      "Batch 74, Loss: 1.175542, Accuracy: 54.50%\n",
      "Batch 75, Loss: 1.161465, Accuracy: 54.48%\n",
      "Batch 76, Loss: 1.187809, Accuracy: 54.48%\n",
      "Batch 77, Loss: 1.183517, Accuracy: 54.48%\n",
      "Batch 78, Loss: 1.146464, Accuracy: 54.47%\n",
      "Batch 79, Loss: 1.227380, Accuracy: 54.41%\n",
      "Batch 80, Loss: 1.206788, Accuracy: 54.36%\n",
      "Batch 81, Loss: 1.105510, Accuracy: 54.48%\n",
      "Batch 82, Loss: 1.128091, Accuracy: 54.52%\n",
      "Batch 83, Loss: 1.151792, Accuracy: 54.57%\n",
      "Batch 84, Loss: 1.176111, Accuracy: 54.58%\n",
      "Batch 85, Loss: 1.151053, Accuracy: 54.61%\n",
      "Batch 86, Loss: 1.122922, Accuracy: 54.69%\n",
      "Batch 87, Loss: 1.151385, Accuracy: 54.76%\n",
      "Batch 88, Loss: 1.147273, Accuracy: 54.81%\n",
      "Batch 89, Loss: 1.220404, Accuracy: 54.78%\n",
      "Batch 90, Loss: 1.156235, Accuracy: 54.79%\n",
      "Batch 91, Loss: 1.220906, Accuracy: 54.77%\n",
      "Batch 92, Loss: 1.088802, Accuracy: 54.86%\n",
      "Batch 93, Loss: 1.172814, Accuracy: 54.89%\n",
      "Batch 94, Loss: 1.118379, Accuracy: 55.00%\n",
      "Batch 95, Loss: 1.130218, Accuracy: 55.07%\n",
      "Batch 96, Loss: 1.127302, Accuracy: 55.13%\n",
      "Batch 97, Loss: 1.141030, Accuracy: 55.20%\n",
      "Batch 98, Loss: 1.156498, Accuracy: 55.21%\n",
      "Batch 99, Loss: 1.243641, Accuracy: 55.15%\n",
      "Batch 100, Loss: 1.139085, Accuracy: 55.17%\n",
      "Batch 101, Loss: 1.246657, Accuracy: 55.12%\n",
      "Batch 102, Loss: 1.108441, Accuracy: 55.24%\n",
      "Batch 103, Loss: 1.138783, Accuracy: 55.28%\n",
      "Batch 104, Loss: 1.253948, Accuracy: 55.20%\n",
      "Batch 105, Loss: 1.115193, Accuracy: 55.28%\n",
      "Batch 106, Loss: 1.232019, Accuracy: 55.20%\n",
      "Batch 107, Loss: 1.182617, Accuracy: 55.17%\n",
      "Batch 108, Loss: 1.177392, Accuracy: 55.15%\n",
      "Batch 109, Loss: 1.162699, Accuracy: 55.17%\n",
      "Batch 110, Loss: 1.182434, Accuracy: 55.16%\n",
      "Batch 111, Loss: 1.164985, Accuracy: 55.18%\n",
      "Batch 112, Loss: 1.089760, Accuracy: 55.25%\n",
      "Batch 113, Loss: 1.266315, Accuracy: 55.13%\n",
      "Batch 114, Loss: 1.110515, Accuracy: 55.22%\n",
      "Batch 115, Loss: 1.098222, Accuracy: 55.33%\n",
      "Batch 116, Loss: 1.197729, Accuracy: 55.29%\n",
      "Batch 117, Loss: 1.132479, Accuracy: 55.34%\n",
      "Batch 118, Loss: 1.212592, Accuracy: 55.30%\n",
      "Batch 119, Loss: 1.134252, Accuracy: 55.34%\n",
      "Batch 120, Loss: 1.196347, Accuracy: 55.31%\n",
      "Batch 121, Loss: 1.202363, Accuracy: 55.29%\n",
      "Batch 122, Loss: 1.172059, Accuracy: 55.29%\n",
      "Batch 123, Loss: 1.164728, Accuracy: 55.31%\n",
      "Batch 124, Loss: 1.025995, Accuracy: 55.46%\n",
      "Batch 125, Loss: 1.120370, Accuracy: 55.54%\n",
      "Batch 126, Loss: 1.160577, Accuracy: 55.57%\n",
      "Batch 127, Loss: 1.137567, Accuracy: 55.59%\n",
      "Batch 128, Loss: 1.208617, Accuracy: 55.59%\n",
      "Batch 129, Loss: 1.226112, Accuracy: 55.55%\n",
      "Batch 130, Loss: 1.154005, Accuracy: 55.55%\n",
      "Batch 131, Loss: 1.201241, Accuracy: 55.51%\n",
      "Batch 132, Loss: 1.169707, Accuracy: 55.53%\n",
      "Batch 133, Loss: 1.115539, Accuracy: 55.58%\n",
      "Batch 134, Loss: 1.191596, Accuracy: 55.60%\n",
      "Batch 135, Loss: 1.144474, Accuracy: 55.62%\n",
      "Batch 136, Loss: 1.186987, Accuracy: 55.63%\n",
      "Batch 137, Loss: 1.122088, Accuracy: 55.68%\n",
      "Batch 138, Loss: 1.126253, Accuracy: 55.73%\n",
      "Batch 139, Loss: 1.305274, Accuracy: 55.63%\n",
      "Batch 140, Loss: 1.173187, Accuracy: 55.67%\n",
      "Batch 141, Loss: 1.217987, Accuracy: 55.63%\n",
      "Batch 142, Loss: 1.191334, Accuracy: 55.61%\n",
      "Batch 143, Loss: 1.171510, Accuracy: 55.62%\n",
      "Batch 144, Loss: 1.147855, Accuracy: 55.65%\n",
      "Batch 145, Loss: 1.139996, Accuracy: 55.66%\n",
      "Batch 146, Loss: 1.174169, Accuracy: 55.66%\n",
      "Batch 147, Loss: 1.089786, Accuracy: 55.72%\n",
      "Batch 148, Loss: 1.203663, Accuracy: 55.71%\n",
      "Batch 149, Loss: 1.120288, Accuracy: 55.76%\n",
      "Batch 150, Loss: 1.087630, Accuracy: 55.83%\n",
      "Batch 151, Loss: 1.173554, Accuracy: 55.83%\n",
      "Batch 152, Loss: 1.192240, Accuracy: 55.80%\n",
      "Batch 153, Loss: 1.188042, Accuracy: 55.78%\n",
      "Batch 154, Loss: 1.269725, Accuracy: 55.69%\n",
      "Batch 155, Loss: 1.143930, Accuracy: 55.71%\n",
      "Batch 156, Loss: 1.165659, Accuracy: 55.72%\n",
      "Batch 157, Loss: 1.213140, Accuracy: 55.69%\n",
      "Batch 158, Loss: 1.140997, Accuracy: 55.71%\n",
      "Batch 159, Loss: 1.193849, Accuracy: 55.71%\n",
      "Batch 160, Loss: 1.213110, Accuracy: 55.68%\n",
      "Batch 161, Loss: 1.223071, Accuracy: 55.65%\n",
      "Batch 162, Loss: 1.216862, Accuracy: 55.62%\n",
      "Batch 163, Loss: 1.068954, Accuracy: 55.71%\n",
      "Batch 164, Loss: 1.162143, Accuracy: 55.72%\n",
      "Batch 165, Loss: 1.137272, Accuracy: 55.72%\n",
      "Batch 166, Loss: 1.236144, Accuracy: 55.68%\n",
      "Batch 167, Loss: 1.162786, Accuracy: 55.69%\n",
      "Batch 168, Loss: 1.252418, Accuracy: 55.64%\n",
      "Batch 169, Loss: 1.124292, Accuracy: 55.68%\n",
      "Batch 170, Loss: 1.088782, Accuracy: 55.75%\n",
      "Batch 171, Loss: 1.249236, Accuracy: 55.69%\n",
      "Batch 172, Loss: 1.210365, Accuracy: 55.64%\n",
      "Batch 173, Loss: 1.105740, Accuracy: 55.70%\n",
      "Batch 174, Loss: 1.124987, Accuracy: 55.74%\n",
      "Batch 175, Loss: 1.193411, Accuracy: 55.73%\n",
      "Batch 176, Loss: 1.211138, Accuracy: 55.70%\n",
      "Batch 177, Loss: 1.186196, Accuracy: 55.67%\n",
      "Batch 178, Loss: 1.073824, Accuracy: 55.74%\n",
      "Batch 179, Loss: 1.185607, Accuracy: 55.74%\n",
      "Batch 180, Loss: 1.164572, Accuracy: 55.75%\n",
      "Batch 181, Loss: 1.125321, Accuracy: 55.78%\n",
      "Batch 182, Loss: 1.134792, Accuracy: 55.82%\n",
      "Batch 183, Loss: 1.201982, Accuracy: 55.80%\n",
      "Batch 184, Loss: 1.086125, Accuracy: 55.85%\n",
      "Batch 185, Loss: 1.072376, Accuracy: 55.93%\n",
      "Batch 186, Loss: 1.176651, Accuracy: 55.94%\n",
      "Batch 187, Loss: 1.244150, Accuracy: 55.89%\n",
      "Batch 188, Loss: 1.129718, Accuracy: 55.94%\n",
      "Batch 189, Loss: 1.172552, Accuracy: 55.94%\n",
      "Batch 190, Loss: 1.159641, Accuracy: 55.96%\n",
      "Batch 191, Loss: 1.186570, Accuracy: 55.96%\n",
      "Batch 192, Loss: 1.147406, Accuracy: 56.00%\n",
      "Batch 193, Loss: 1.075789, Accuracy: 56.05%\n",
      "Batch 194, Loss: 1.162676, Accuracy: 56.02%\n",
      "Batch 195, Loss: 1.152458, Accuracy: 56.06%\n",
      "Batch 196, Loss: 1.213222, Accuracy: 56.02%\n",
      "Batch 197, Loss: 1.228664, Accuracy: 55.96%\n",
      "Batch 198, Loss: 1.071158, Accuracy: 56.01%\n",
      "Batch 199, Loss: 1.034788, Accuracy: 56.09%\n",
      "Batch 200, Loss: 1.030887, Accuracy: 56.17%\n",
      "Batch 201, Loss: 1.159430, Accuracy: 56.18%\n",
      "Batch 202, Loss: 1.187637, Accuracy: 56.16%\n",
      "Batch 203, Loss: 1.232740, Accuracy: 56.10%\n",
      "Batch 204, Loss: 1.147425, Accuracy: 56.12%\n",
      "Batch 205, Loss: 1.090568, Accuracy: 56.15%\n",
      "Batch 206, Loss: 1.096240, Accuracy: 56.18%\n",
      "Batch 207, Loss: 1.259076, Accuracy: 56.13%\n",
      "Batch 208, Loss: 1.146355, Accuracy: 56.14%\n",
      "Batch 209, Loss: 1.222583, Accuracy: 56.14%\n",
      "Batch 210, Loss: 1.085145, Accuracy: 56.19%\n",
      "Batch 211, Loss: 1.181506, Accuracy: 56.17%\n",
      "Batch 212, Loss: 1.101626, Accuracy: 56.21%\n",
      "Batch 213, Loss: 1.118504, Accuracy: 56.23%\n",
      "Training - Epoch 3, Loss: 1.170105, Accuracy: 56.23%\n",
      "Validation Batch 1, Loss: 1.335135, Accuracy: 37.50%\n",
      "Validation Batch 2, Loss: 1.333019, Accuracy: 38.28%\n",
      "Validation Batch 3, Loss: 1.422057, Accuracy: 35.42%\n",
      "Validation Batch 4, Loss: 1.314634, Accuracy: 37.11%\n",
      "Validation Batch 5, Loss: 1.407881, Accuracy: 35.94%\n",
      "Validation Batch 6, Loss: 1.456210, Accuracy: 33.85%\n",
      "Validation Batch 7, Loss: 1.350733, Accuracy: 34.15%\n",
      "Validation Batch 8, Loss: 1.351468, Accuracy: 34.57%\n",
      "Validation Batch 9, Loss: 1.389121, Accuracy: 34.38%\n",
      "Validation Batch 10, Loss: 1.418738, Accuracy: 33.91%\n",
      "Validation Batch 11, Loss: 1.351227, Accuracy: 34.23%\n",
      "Validation Batch 12, Loss: 1.319792, Accuracy: 34.77%\n",
      "Validation Batch 13, Loss: 1.396767, Accuracy: 34.50%\n",
      "Validation Batch 14, Loss: 1.264165, Accuracy: 35.38%\n",
      "Validation Batch 15, Loss: 1.339842, Accuracy: 35.52%\n",
      "Validation Batch 16, Loss: 1.441829, Accuracy: 34.96%\n",
      "Validation Batch 17, Loss: 1.377685, Accuracy: 34.93%\n",
      "Validation Batch 18, Loss: 1.357579, Accuracy: 34.98%\n",
      "Validation Batch 19, Loss: 1.366717, Accuracy: 35.03%\n",
      "Validation Batch 20, Loss: 1.370201, Accuracy: 35.08%\n",
      "Validation Batch 21, Loss: 1.359842, Accuracy: 35.12%\n",
      "Validation Batch 22, Loss: 1.372795, Accuracy: 35.16%\n",
      "Validation Batch 23, Loss: 1.371639, Accuracy: 35.19%\n",
      "Validation Batch 24, Loss: 1.422043, Accuracy: 34.96%\n",
      "Validation Batch 25, Loss: 1.312408, Accuracy: 35.25%\n",
      "Validation Batch 26, Loss: 1.266988, Accuracy: 35.64%\n",
      "Validation Batch 27, Loss: 1.389935, Accuracy: 35.58%\n",
      "Validation - Epoch 3, Loss: 1.365202, Accuracy: 35.58%\n",
      "Patience—2\n",
      "Epoch 4\n",
      "Batch 1, Loss: 1.132098, Accuracy: 59.38%\n",
      "Batch 2, Loss: 1.248716, Accuracy: 51.56%\n",
      "Batch 3, Loss: 1.260083, Accuracy: 51.04%\n",
      "Batch 4, Loss: 1.158441, Accuracy: 52.73%\n",
      "Batch 5, Loss: 1.185598, Accuracy: 53.12%\n",
      "Batch 6, Loss: 1.229170, Accuracy: 52.60%\n",
      "Batch 7, Loss: 1.127179, Accuracy: 54.24%\n",
      "Batch 8, Loss: 1.149701, Accuracy: 55.27%\n",
      "Batch 9, Loss: 1.163390, Accuracy: 55.21%\n",
      "Batch 10, Loss: 1.213394, Accuracy: 55.31%\n",
      "Batch 11, Loss: 1.164497, Accuracy: 55.68%\n",
      "Batch 12, Loss: 1.156877, Accuracy: 56.12%\n",
      "Batch 13, Loss: 1.189036, Accuracy: 56.25%\n",
      "Batch 14, Loss: 1.256563, Accuracy: 55.47%\n",
      "Batch 15, Loss: 1.177121, Accuracy: 55.83%\n",
      "Batch 16, Loss: 1.159671, Accuracy: 55.96%\n",
      "Batch 17, Loss: 1.172181, Accuracy: 55.88%\n",
      "Batch 18, Loss: 1.189206, Accuracy: 55.73%\n",
      "Batch 19, Loss: 1.174291, Accuracy: 55.76%\n",
      "Batch 20, Loss: 1.122198, Accuracy: 55.94%\n",
      "Batch 21, Loss: 1.121749, Accuracy: 56.32%\n",
      "Batch 22, Loss: 1.159849, Accuracy: 56.32%\n",
      "Batch 23, Loss: 1.165572, Accuracy: 56.32%\n",
      "Batch 24, Loss: 1.167596, Accuracy: 56.38%\n",
      "Batch 25, Loss: 1.168703, Accuracy: 56.38%\n",
      "Batch 26, Loss: 1.109350, Accuracy: 56.61%\n",
      "Batch 27, Loss: 1.199310, Accuracy: 56.60%\n",
      "Batch 28, Loss: 1.035159, Accuracy: 57.09%\n",
      "Batch 29, Loss: 1.184505, Accuracy: 57.06%\n",
      "Batch 30, Loss: 1.168606, Accuracy: 57.08%\n",
      "Batch 31, Loss: 1.180526, Accuracy: 57.01%\n",
      "Batch 32, Loss: 1.124055, Accuracy: 57.18%\n",
      "Batch 33, Loss: 1.276412, Accuracy: 56.91%\n",
      "Batch 34, Loss: 1.129215, Accuracy: 57.03%\n",
      "Batch 35, Loss: 1.115957, Accuracy: 57.14%\n",
      "Batch 36, Loss: 1.175672, Accuracy: 57.20%\n",
      "Batch 37, Loss: 1.205170, Accuracy: 57.14%\n",
      "Batch 38, Loss: 1.119307, Accuracy: 57.20%\n",
      "Batch 39, Loss: 1.156979, Accuracy: 57.17%\n",
      "Batch 40, Loss: 1.184470, Accuracy: 57.19%\n",
      "Batch 41, Loss: 1.090920, Accuracy: 57.39%\n",
      "Batch 42, Loss: 1.109741, Accuracy: 57.48%\n",
      "Batch 43, Loss: 1.109761, Accuracy: 57.63%\n",
      "Batch 44, Loss: 1.094235, Accuracy: 57.81%\n",
      "Batch 45, Loss: 1.105318, Accuracy: 57.99%\n",
      "Batch 46, Loss: 1.151386, Accuracy: 57.95%\n",
      "Batch 47, Loss: 1.173273, Accuracy: 57.85%\n",
      "Batch 48, Loss: 1.086320, Accuracy: 58.04%\n",
      "Batch 49, Loss: 1.107950, Accuracy: 58.10%\n",
      "Batch 50, Loss: 1.104674, Accuracy: 58.22%\n",
      "Batch 51, Loss: 1.101497, Accuracy: 58.36%\n",
      "Batch 52, Loss: 1.209517, Accuracy: 58.17%\n",
      "Batch 53, Loss: 1.200052, Accuracy: 58.05%\n",
      "Batch 54, Loss: 1.087001, Accuracy: 58.22%\n",
      "Batch 55, Loss: 1.236372, Accuracy: 58.04%\n",
      "Batch 56, Loss: 1.116445, Accuracy: 58.12%\n",
      "Batch 57, Loss: 1.149561, Accuracy: 58.20%\n",
      "Batch 58, Loss: 1.067831, Accuracy: 58.38%\n",
      "Batch 59, Loss: 1.162473, Accuracy: 58.34%\n",
      "Batch 60, Loss: 1.258375, Accuracy: 58.12%\n",
      "Batch 61, Loss: 1.143666, Accuracy: 58.20%\n",
      "Batch 62, Loss: 1.081427, Accuracy: 58.37%\n",
      "Batch 63, Loss: 1.129474, Accuracy: 58.33%\n",
      "Batch 64, Loss: 1.184775, Accuracy: 58.25%\n",
      "Batch 65, Loss: 1.174232, Accuracy: 58.20%\n",
      "Batch 66, Loss: 1.128634, Accuracy: 58.19%\n",
      "Batch 67, Loss: 1.100830, Accuracy: 58.33%\n",
      "Batch 68, Loss: 1.180540, Accuracy: 58.30%\n",
      "Batch 69, Loss: 1.087542, Accuracy: 58.38%\n",
      "Batch 70, Loss: 1.100288, Accuracy: 58.39%\n",
      "Batch 71, Loss: 1.181506, Accuracy: 58.30%\n",
      "Batch 72, Loss: 1.047475, Accuracy: 58.49%\n",
      "Batch 73, Loss: 1.139067, Accuracy: 58.50%\n",
      "Batch 74, Loss: 1.107248, Accuracy: 58.55%\n",
      "Batch 75, Loss: 1.152936, Accuracy: 58.56%\n",
      "Batch 76, Loss: 1.203194, Accuracy: 58.47%\n",
      "Batch 77, Loss: 1.121098, Accuracy: 58.58%\n",
      "Batch 78, Loss: 1.288709, Accuracy: 58.41%\n",
      "Batch 79, Loss: 1.073423, Accuracy: 58.54%\n",
      "Batch 80, Loss: 1.189501, Accuracy: 58.48%\n",
      "Batch 81, Loss: 1.137064, Accuracy: 58.49%\n",
      "Batch 82, Loss: 1.188922, Accuracy: 58.44%\n",
      "Batch 83, Loss: 1.202566, Accuracy: 58.32%\n",
      "Batch 84, Loss: 1.073708, Accuracy: 58.43%\n",
      "Batch 85, Loss: 1.111827, Accuracy: 58.47%\n",
      "Batch 86, Loss: 1.170993, Accuracy: 58.41%\n",
      "Batch 87, Loss: 1.118857, Accuracy: 58.42%\n",
      "Batch 88, Loss: 1.121647, Accuracy: 58.49%\n",
      "Batch 89, Loss: 1.122073, Accuracy: 58.51%\n",
      "Batch 90, Loss: 1.152875, Accuracy: 58.54%\n",
      "Batch 91, Loss: 1.091586, Accuracy: 58.62%\n",
      "Batch 92, Loss: 1.180882, Accuracy: 58.59%\n",
      "Batch 93, Loss: 1.198689, Accuracy: 58.53%\n",
      "Batch 94, Loss: 1.109432, Accuracy: 58.59%\n",
      "Batch 95, Loss: 1.162245, Accuracy: 58.57%\n",
      "Batch 96, Loss: 1.237948, Accuracy: 58.48%\n",
      "Batch 97, Loss: 1.145922, Accuracy: 58.47%\n",
      "Batch 98, Loss: 1.146165, Accuracy: 58.48%\n",
      "Batch 99, Loss: 1.155452, Accuracy: 58.44%\n",
      "Batch 100, Loss: 1.141097, Accuracy: 58.45%\n",
      "Batch 101, Loss: 1.192367, Accuracy: 58.42%\n",
      "Batch 102, Loss: 1.119561, Accuracy: 58.47%\n",
      "Batch 103, Loss: 1.058324, Accuracy: 58.54%\n",
      "Batch 104, Loss: 1.174126, Accuracy: 58.52%\n",
      "Batch 105, Loss: 1.097452, Accuracy: 58.59%\n",
      "Batch 106, Loss: 1.141795, Accuracy: 58.59%\n",
      "Batch 107, Loss: 1.103975, Accuracy: 58.64%\n",
      "Batch 108, Loss: 1.247288, Accuracy: 58.56%\n",
      "Batch 109, Loss: 1.175373, Accuracy: 58.53%\n",
      "Batch 110, Loss: 1.122648, Accuracy: 58.55%\n",
      "Batch 111, Loss: 1.098259, Accuracy: 58.60%\n",
      "Batch 112, Loss: 1.226112, Accuracy: 58.52%\n",
      "Batch 113, Loss: 1.194451, Accuracy: 58.45%\n",
      "Batch 114, Loss: 1.186095, Accuracy: 58.40%\n",
      "Batch 115, Loss: 1.236594, Accuracy: 58.30%\n",
      "Batch 116, Loss: 1.162196, Accuracy: 58.32%\n",
      "Batch 117, Loss: 1.175609, Accuracy: 58.33%\n",
      "Batch 118, Loss: 1.115111, Accuracy: 58.36%\n",
      "Batch 119, Loss: 1.071223, Accuracy: 58.43%\n",
      "Batch 120, Loss: 1.106083, Accuracy: 58.45%\n",
      "Batch 121, Loss: 1.194075, Accuracy: 58.42%\n",
      "Batch 122, Loss: 1.073273, Accuracy: 58.47%\n",
      "Batch 123, Loss: 1.153413, Accuracy: 58.43%\n",
      "Batch 124, Loss: 1.118445, Accuracy: 58.47%\n",
      "Batch 125, Loss: 1.184470, Accuracy: 58.41%\n",
      "Batch 126, Loss: 1.160907, Accuracy: 58.38%\n",
      "Batch 127, Loss: 1.110274, Accuracy: 58.40%\n",
      "Batch 128, Loss: 1.035436, Accuracy: 58.51%\n",
      "Batch 129, Loss: 1.118339, Accuracy: 58.55%\n",
      "Batch 130, Loss: 1.133047, Accuracy: 58.57%\n",
      "Batch 131, Loss: 1.157385, Accuracy: 58.58%\n",
      "Batch 132, Loss: 1.090714, Accuracy: 58.62%\n",
      "Batch 133, Loss: 1.138947, Accuracy: 58.65%\n",
      "Batch 134, Loss: 1.248866, Accuracy: 58.59%\n",
      "Batch 135, Loss: 1.138740, Accuracy: 58.61%\n",
      "Batch 136, Loss: 1.104673, Accuracy: 58.66%\n",
      "Batch 137, Loss: 1.156658, Accuracy: 58.68%\n",
      "Batch 138, Loss: 1.120258, Accuracy: 58.70%\n",
      "Batch 139, Loss: 1.086378, Accuracy: 58.75%\n",
      "Batch 140, Loss: 1.176910, Accuracy: 58.73%\n",
      "Batch 141, Loss: 1.131241, Accuracy: 58.75%\n",
      "Batch 142, Loss: 1.094699, Accuracy: 58.77%\n",
      "Batch 143, Loss: 1.136835, Accuracy: 58.76%\n",
      "Batch 144, Loss: 1.149278, Accuracy: 58.77%\n",
      "Batch 145, Loss: 1.217973, Accuracy: 58.69%\n",
      "Batch 146, Loss: 1.126531, Accuracy: 58.70%\n",
      "Batch 147, Loss: 1.085609, Accuracy: 58.75%\n",
      "Batch 148, Loss: 1.019584, Accuracy: 58.87%\n",
      "Batch 149, Loss: 1.108566, Accuracy: 58.88%\n",
      "Batch 150, Loss: 1.140405, Accuracy: 58.89%\n",
      "Batch 151, Loss: 1.217695, Accuracy: 58.84%\n",
      "Batch 152, Loss: 1.111668, Accuracy: 58.89%\n",
      "Batch 153, Loss: 1.114168, Accuracy: 58.94%\n",
      "Batch 154, Loss: 1.122230, Accuracy: 58.96%\n",
      "Batch 155, Loss: 1.149161, Accuracy: 58.95%\n",
      "Batch 156, Loss: 1.085917, Accuracy: 59.00%\n",
      "Batch 157, Loss: 1.069700, Accuracy: 59.06%\n",
      "Batch 158, Loss: 1.057895, Accuracy: 59.10%\n",
      "Batch 159, Loss: 1.010019, Accuracy: 59.22%\n",
      "Batch 160, Loss: 1.184955, Accuracy: 59.17%\n",
      "Batch 161, Loss: 1.137971, Accuracy: 59.15%\n",
      "Batch 162, Loss: 1.119337, Accuracy: 59.15%\n",
      "Batch 163, Loss: 1.249901, Accuracy: 59.07%\n",
      "Batch 164, Loss: 1.215772, Accuracy: 59.02%\n",
      "Batch 165, Loss: 1.138024, Accuracy: 59.03%\n",
      "Batch 166, Loss: 1.181158, Accuracy: 59.00%\n",
      "Batch 167, Loss: 1.118996, Accuracy: 59.02%\n",
      "Batch 168, Loss: 1.115737, Accuracy: 59.02%\n",
      "Batch 169, Loss: 1.171755, Accuracy: 59.00%\n",
      "Batch 170, Loss: 1.185472, Accuracy: 58.98%\n",
      "Batch 171, Loss: 1.193261, Accuracy: 58.95%\n",
      "Batch 172, Loss: 1.181828, Accuracy: 58.92%\n",
      "Batch 173, Loss: 1.147420, Accuracy: 58.92%\n",
      "Batch 174, Loss: 1.128588, Accuracy: 58.93%\n",
      "Batch 175, Loss: 1.118496, Accuracy: 58.94%\n",
      "Batch 176, Loss: 1.186964, Accuracy: 58.91%\n",
      "Batch 177, Loss: 1.165244, Accuracy: 58.90%\n",
      "Batch 178, Loss: 1.066487, Accuracy: 58.96%\n",
      "Batch 179, Loss: 1.114465, Accuracy: 58.99%\n",
      "Batch 180, Loss: 1.177241, Accuracy: 58.98%\n",
      "Batch 181, Loss: 1.148179, Accuracy: 59.00%\n",
      "Batch 182, Loss: 1.094827, Accuracy: 59.05%\n",
      "Batch 183, Loss: 1.175263, Accuracy: 59.04%\n",
      "Batch 184, Loss: 1.191567, Accuracy: 59.01%\n",
      "Batch 185, Loss: 1.085351, Accuracy: 59.05%\n",
      "Batch 186, Loss: 1.225625, Accuracy: 59.01%\n",
      "Batch 187, Loss: 1.032868, Accuracy: 59.09%\n",
      "Batch 188, Loss: 1.123795, Accuracy: 59.11%\n",
      "Batch 189, Loss: 1.090476, Accuracy: 59.14%\n",
      "Batch 190, Loss: 1.098083, Accuracy: 59.18%\n",
      "Batch 191, Loss: 1.149483, Accuracy: 59.16%\n",
      "Batch 192, Loss: 1.068581, Accuracy: 59.20%\n",
      "Batch 193, Loss: 1.084239, Accuracy: 59.25%\n",
      "Batch 194, Loss: 1.109924, Accuracy: 59.28%\n",
      "Batch 195, Loss: 1.089037, Accuracy: 59.30%\n",
      "Batch 196, Loss: 1.152762, Accuracy: 59.30%\n",
      "Batch 197, Loss: 1.266934, Accuracy: 59.23%\n",
      "Batch 198, Loss: 1.070425, Accuracy: 59.26%\n",
      "Batch 199, Loss: 1.239338, Accuracy: 59.22%\n",
      "Batch 200, Loss: 1.197965, Accuracy: 59.17%\n",
      "Batch 201, Loss: 1.176619, Accuracy: 59.15%\n",
      "Batch 202, Loss: 1.184761, Accuracy: 59.12%\n",
      "Batch 203, Loss: 1.092518, Accuracy: 59.16%\n",
      "Batch 204, Loss: 1.103863, Accuracy: 59.17%\n",
      "Batch 205, Loss: 1.135001, Accuracy: 59.15%\n",
      "Batch 206, Loss: 1.067788, Accuracy: 59.19%\n",
      "Batch 207, Loss: 1.168118, Accuracy: 59.18%\n",
      "Batch 208, Loss: 1.179659, Accuracy: 59.16%\n",
      "Batch 209, Loss: 1.182129, Accuracy: 59.14%\n",
      "Batch 210, Loss: 1.213614, Accuracy: 59.10%\n",
      "Batch 211, Loss: 1.269766, Accuracy: 59.06%\n",
      "Batch 212, Loss: 1.147134, Accuracy: 59.06%\n",
      "Batch 213, Loss: 1.204491, Accuracy: 59.02%\n",
      "Training - Epoch 4, Loss: 1.146761, Accuracy: 59.02%\n",
      "Validation Batch 1, Loss: 1.325186, Accuracy: 39.06%\n",
      "Validation Batch 2, Loss: 1.328598, Accuracy: 39.06%\n",
      "Validation Batch 3, Loss: 1.422488, Accuracy: 35.94%\n",
      "Validation Batch 4, Loss: 1.313841, Accuracy: 37.50%\n",
      "Validation Batch 5, Loss: 1.408396, Accuracy: 36.25%\n",
      "Validation Batch 6, Loss: 1.443286, Accuracy: 34.38%\n",
      "Validation Batch 7, Loss: 1.339631, Accuracy: 34.82%\n",
      "Validation Batch 8, Loss: 1.344718, Accuracy: 35.35%\n",
      "Validation Batch 9, Loss: 1.385199, Accuracy: 35.24%\n",
      "Validation Batch 10, Loss: 1.415875, Accuracy: 34.69%\n",
      "Validation Batch 11, Loss: 1.349280, Accuracy: 34.94%\n",
      "Validation Batch 12, Loss: 1.304665, Accuracy: 35.55%\n",
      "Validation Batch 13, Loss: 1.389944, Accuracy: 35.34%\n",
      "Validation Batch 14, Loss: 1.258981, Accuracy: 36.16%\n",
      "Validation Batch 15, Loss: 1.334907, Accuracy: 36.35%\n",
      "Validation Batch 16, Loss: 1.438898, Accuracy: 35.74%\n",
      "Validation Batch 17, Loss: 1.380038, Accuracy: 35.75%\n",
      "Validation Batch 18, Loss: 1.343816, Accuracy: 35.85%\n",
      "Validation Batch 19, Loss: 1.362470, Accuracy: 35.94%\n",
      "Validation Batch 20, Loss: 1.356873, Accuracy: 36.02%\n",
      "Validation Batch 21, Loss: 1.360783, Accuracy: 36.01%\n",
      "Validation Batch 22, Loss: 1.367112, Accuracy: 36.01%\n",
      "Validation Batch 23, Loss: 1.371891, Accuracy: 36.01%\n",
      "Validation Batch 24, Loss: 1.419114, Accuracy: 35.74%\n",
      "Validation Batch 25, Loss: 1.304181, Accuracy: 36.00%\n",
      "Validation Batch 26, Loss: 1.251960, Accuracy: 36.54%\n",
      "Validation Batch 27, Loss: 1.390806, Accuracy: 36.47%\n",
      "Validation - Epoch 4, Loss: 1.359738, Accuracy: 36.47%\n",
      "Patience—3\n",
      "Epoch 5\n",
      "Batch 1, Loss: 1.108346, Accuracy: 60.94%\n",
      "Batch 2, Loss: 1.105035, Accuracy: 62.50%\n",
      "Batch 3, Loss: 1.138492, Accuracy: 61.98%\n",
      "Batch 4, Loss: 1.143171, Accuracy: 60.55%\n",
      "Batch 5, Loss: 1.215907, Accuracy: 59.06%\n",
      "Batch 6, Loss: 1.125429, Accuracy: 58.59%\n",
      "Batch 7, Loss: 1.246110, Accuracy: 57.14%\n",
      "Batch 8, Loss: 1.068093, Accuracy: 58.59%\n",
      "Batch 9, Loss: 1.169697, Accuracy: 57.99%\n",
      "Batch 10, Loss: 1.091884, Accuracy: 59.06%\n",
      "Batch 11, Loss: 1.127939, Accuracy: 59.09%\n",
      "Batch 12, Loss: 1.142075, Accuracy: 59.24%\n",
      "Batch 13, Loss: 1.179199, Accuracy: 58.89%\n",
      "Batch 14, Loss: 1.084665, Accuracy: 59.60%\n",
      "Batch 15, Loss: 1.121781, Accuracy: 59.90%\n",
      "Batch 16, Loss: 1.140027, Accuracy: 59.77%\n",
      "Batch 17, Loss: 1.092089, Accuracy: 60.02%\n",
      "Batch 18, Loss: 1.082658, Accuracy: 60.24%\n",
      "Batch 19, Loss: 1.147856, Accuracy: 60.12%\n",
      "Batch 20, Loss: 1.113466, Accuracy: 60.39%\n",
      "Batch 21, Loss: 1.171233, Accuracy: 60.19%\n",
      "Batch 22, Loss: 1.098389, Accuracy: 60.30%\n",
      "Batch 23, Loss: 1.153950, Accuracy: 60.26%\n",
      "Batch 24, Loss: 1.171128, Accuracy: 60.16%\n",
      "Batch 25, Loss: 1.109012, Accuracy: 60.31%\n",
      "Batch 26, Loss: 1.129543, Accuracy: 60.40%\n",
      "Batch 27, Loss: 1.135815, Accuracy: 60.42%\n",
      "Batch 28, Loss: 1.179899, Accuracy: 60.21%\n",
      "Batch 29, Loss: 1.239933, Accuracy: 59.81%\n",
      "Batch 30, Loss: 1.157311, Accuracy: 59.64%\n",
      "Batch 31, Loss: 1.165330, Accuracy: 59.48%\n",
      "Batch 32, Loss: 1.128036, Accuracy: 59.42%\n",
      "Batch 33, Loss: 1.237743, Accuracy: 59.19%\n",
      "Batch 34, Loss: 1.125558, Accuracy: 59.24%\n",
      "Batch 35, Loss: 1.160774, Accuracy: 59.24%\n",
      "Batch 36, Loss: 1.068123, Accuracy: 59.51%\n",
      "Batch 37, Loss: 1.188197, Accuracy: 59.33%\n",
      "Batch 38, Loss: 1.165924, Accuracy: 59.33%\n",
      "Batch 39, Loss: 1.196990, Accuracy: 59.21%\n",
      "Batch 40, Loss: 1.150426, Accuracy: 59.26%\n",
      "Batch 41, Loss: 1.100222, Accuracy: 59.34%\n",
      "Batch 42, Loss: 1.086546, Accuracy: 59.41%\n",
      "Batch 43, Loss: 1.056207, Accuracy: 59.77%\n",
      "Batch 44, Loss: 1.137210, Accuracy: 59.73%\n",
      "Batch 45, Loss: 1.137499, Accuracy: 59.79%\n",
      "Batch 46, Loss: 1.102608, Accuracy: 59.92%\n",
      "Batch 47, Loss: 1.129884, Accuracy: 59.94%\n",
      "Batch 48, Loss: 1.086013, Accuracy: 60.09%\n",
      "Batch 49, Loss: 1.087452, Accuracy: 60.33%\n",
      "Batch 50, Loss: 1.154063, Accuracy: 60.28%\n",
      "Batch 51, Loss: 1.237572, Accuracy: 60.11%\n",
      "Batch 52, Loss: 1.103424, Accuracy: 60.22%\n",
      "Batch 53, Loss: 1.160497, Accuracy: 60.20%\n",
      "Batch 54, Loss: 1.090372, Accuracy: 60.30%\n",
      "Batch 55, Loss: 1.128733, Accuracy: 60.34%\n",
      "Batch 56, Loss: 1.165742, Accuracy: 60.27%\n",
      "Batch 57, Loss: 1.097301, Accuracy: 60.36%\n",
      "Batch 58, Loss: 1.193346, Accuracy: 60.26%\n",
      "Batch 59, Loss: 1.077828, Accuracy: 60.43%\n",
      "Batch 60, Loss: 1.097837, Accuracy: 60.55%\n",
      "Batch 61, Loss: 1.051975, Accuracy: 60.71%\n",
      "Batch 62, Loss: 1.143489, Accuracy: 60.74%\n",
      "Batch 63, Loss: 1.103648, Accuracy: 60.81%\n",
      "Batch 64, Loss: 1.049150, Accuracy: 60.94%\n",
      "Batch 65, Loss: 1.102198, Accuracy: 60.99%\n",
      "Batch 66, Loss: 1.165505, Accuracy: 60.89%\n",
      "Batch 67, Loss: 1.107092, Accuracy: 61.01%\n",
      "Batch 68, Loss: 1.097151, Accuracy: 61.08%\n",
      "Batch 69, Loss: 1.119892, Accuracy: 61.10%\n",
      "Batch 70, Loss: 1.086898, Accuracy: 61.16%\n",
      "Batch 71, Loss: 1.158962, Accuracy: 61.14%\n",
      "Batch 72, Loss: 1.156583, Accuracy: 61.07%\n",
      "Batch 73, Loss: 1.119641, Accuracy: 61.09%\n",
      "Batch 74, Loss: 1.162574, Accuracy: 61.02%\n",
      "Batch 75, Loss: 1.128194, Accuracy: 61.04%\n",
      "Batch 76, Loss: 1.133683, Accuracy: 61.00%\n",
      "Batch 77, Loss: 1.139772, Accuracy: 61.06%\n",
      "Batch 78, Loss: 1.045644, Accuracy: 61.20%\n",
      "Batch 79, Loss: 1.192142, Accuracy: 61.02%\n",
      "Batch 80, Loss: 1.149413, Accuracy: 61.04%\n",
      "Batch 81, Loss: 1.091232, Accuracy: 61.07%\n",
      "Batch 82, Loss: 1.099923, Accuracy: 61.19%\n",
      "Batch 83, Loss: 1.154984, Accuracy: 61.16%\n",
      "Batch 84, Loss: 1.144301, Accuracy: 61.12%\n",
      "Batch 85, Loss: 1.163774, Accuracy: 61.10%\n",
      "Batch 86, Loss: 1.057956, Accuracy: 61.21%\n",
      "Batch 87, Loss: 1.138189, Accuracy: 61.17%\n",
      "Batch 88, Loss: 1.196427, Accuracy: 61.08%\n",
      "Batch 89, Loss: 1.110961, Accuracy: 61.11%\n",
      "Batch 90, Loss: 1.158141, Accuracy: 61.11%\n",
      "Batch 91, Loss: 1.140898, Accuracy: 61.11%\n",
      "Batch 92, Loss: 1.083246, Accuracy: 61.18%\n",
      "Batch 93, Loss: 1.188745, Accuracy: 61.12%\n",
      "Batch 94, Loss: 1.208256, Accuracy: 61.02%\n",
      "Batch 95, Loss: 1.164378, Accuracy: 60.99%\n",
      "Batch 96, Loss: 1.137543, Accuracy: 60.97%\n",
      "Batch 97, Loss: 1.100641, Accuracy: 61.05%\n",
      "Batch 98, Loss: 1.068219, Accuracy: 61.11%\n",
      "Batch 99, Loss: 1.082976, Accuracy: 61.13%\n",
      "Batch 100, Loss: 1.179711, Accuracy: 61.03%\n",
      "Batch 101, Loss: 1.171268, Accuracy: 60.97%\n",
      "Batch 102, Loss: 1.070308, Accuracy: 61.03%\n",
      "Batch 103, Loss: 1.146635, Accuracy: 61.00%\n",
      "Batch 104, Loss: 1.045362, Accuracy: 61.07%\n",
      "Batch 105, Loss: 1.144578, Accuracy: 61.09%\n",
      "Batch 106, Loss: 1.121384, Accuracy: 61.10%\n",
      "Batch 107, Loss: 1.137177, Accuracy: 61.10%\n",
      "Batch 108, Loss: 1.179493, Accuracy: 61.01%\n",
      "Batch 109, Loss: 1.250443, Accuracy: 60.88%\n",
      "Batch 110, Loss: 1.100221, Accuracy: 60.92%\n",
      "Batch 111, Loss: 1.048083, Accuracy: 61.02%\n",
      "Batch 112, Loss: 1.033309, Accuracy: 61.12%\n",
      "Batch 113, Loss: 1.105306, Accuracy: 61.13%\n",
      "Batch 114, Loss: 1.155900, Accuracy: 61.09%\n",
      "Batch 115, Loss: 1.227872, Accuracy: 61.01%\n",
      "Batch 116, Loss: 1.172301, Accuracy: 60.95%\n",
      "Batch 117, Loss: 1.086237, Accuracy: 60.99%\n",
      "Batch 118, Loss: 1.190626, Accuracy: 60.94%\n",
      "Batch 119, Loss: 1.062592, Accuracy: 61.03%\n",
      "Batch 120, Loss: 1.106360, Accuracy: 61.05%\n",
      "Batch 121, Loss: 1.051062, Accuracy: 61.12%\n",
      "Batch 122, Loss: 1.119200, Accuracy: 61.12%\n",
      "Batch 123, Loss: 1.096217, Accuracy: 61.18%\n",
      "Batch 124, Loss: 1.111139, Accuracy: 61.19%\n",
      "Batch 125, Loss: 1.133319, Accuracy: 61.16%\n",
      "Batch 126, Loss: 1.100466, Accuracy: 61.22%\n",
      "Batch 127, Loss: 1.079076, Accuracy: 61.26%\n",
      "Batch 128, Loss: 1.169278, Accuracy: 61.22%\n",
      "Batch 129, Loss: 1.116298, Accuracy: 61.22%\n",
      "Batch 130, Loss: 1.137466, Accuracy: 61.19%\n",
      "Batch 131, Loss: 1.204968, Accuracy: 61.12%\n",
      "Batch 132, Loss: 1.124722, Accuracy: 61.14%\n",
      "Batch 133, Loss: 1.189314, Accuracy: 61.09%\n",
      "Batch 134, Loss: 1.055761, Accuracy: 61.15%\n",
      "Batch 135, Loss: 1.118735, Accuracy: 61.13%\n",
      "Batch 136, Loss: 1.060892, Accuracy: 61.19%\n",
      "Batch 137, Loss: 1.193540, Accuracy: 61.14%\n",
      "Batch 138, Loss: 1.094833, Accuracy: 61.18%\n",
      "Batch 139, Loss: 1.098634, Accuracy: 61.21%\n",
      "Batch 140, Loss: 1.126902, Accuracy: 61.21%\n",
      "Batch 141, Loss: 1.182188, Accuracy: 61.16%\n",
      "Batch 142, Loss: 1.177296, Accuracy: 61.11%\n",
      "Batch 143, Loss: 1.118881, Accuracy: 61.13%\n",
      "Batch 144, Loss: 1.109818, Accuracy: 61.14%\n",
      "Batch 145, Loss: 1.173034, Accuracy: 61.11%\n",
      "Batch 146, Loss: 1.124451, Accuracy: 61.08%\n",
      "Batch 147, Loss: 1.067262, Accuracy: 61.13%\n",
      "Batch 148, Loss: 1.082183, Accuracy: 61.15%\n",
      "Batch 149, Loss: 1.026624, Accuracy: 61.24%\n",
      "Batch 150, Loss: 1.172311, Accuracy: 61.21%\n",
      "Batch 151, Loss: 1.049589, Accuracy: 61.28%\n",
      "Batch 152, Loss: 1.099102, Accuracy: 61.31%\n",
      "Batch 153, Loss: 1.083172, Accuracy: 61.35%\n",
      "Batch 154, Loss: 1.149848, Accuracy: 61.33%\n",
      "Batch 155, Loss: 1.116343, Accuracy: 61.32%\n",
      "Batch 156, Loss: 1.159980, Accuracy: 61.29%\n",
      "Batch 157, Loss: 1.105944, Accuracy: 61.31%\n",
      "Batch 158, Loss: 1.035417, Accuracy: 61.37%\n",
      "Batch 159, Loss: 1.077886, Accuracy: 61.41%\n",
      "Batch 160, Loss: 1.106907, Accuracy: 61.42%\n",
      "Batch 161, Loss: 1.031855, Accuracy: 61.48%\n",
      "Batch 162, Loss: 1.041387, Accuracy: 61.53%\n",
      "Batch 163, Loss: 1.066661, Accuracy: 61.55%\n",
      "Batch 164, Loss: 1.141391, Accuracy: 61.55%\n",
      "Batch 165, Loss: 1.159017, Accuracy: 61.51%\n",
      "Batch 166, Loss: 1.051708, Accuracy: 61.56%\n",
      "Batch 167, Loss: 1.081366, Accuracy: 61.59%\n",
      "Batch 168, Loss: 1.175794, Accuracy: 61.55%\n",
      "Batch 169, Loss: 1.134422, Accuracy: 61.55%\n",
      "Batch 170, Loss: 1.125053, Accuracy: 61.55%\n",
      "Batch 171, Loss: 1.077397, Accuracy: 61.60%\n",
      "Batch 172, Loss: 1.070825, Accuracy: 61.61%\n",
      "Batch 173, Loss: 1.125856, Accuracy: 61.61%\n",
      "Batch 174, Loss: 1.114139, Accuracy: 61.61%\n",
      "Batch 175, Loss: 1.167816, Accuracy: 61.57%\n",
      "Batch 176, Loss: 1.118829, Accuracy: 61.59%\n",
      "Batch 177, Loss: 1.058361, Accuracy: 61.61%\n",
      "Batch 178, Loss: 1.125555, Accuracy: 61.60%\n",
      "Batch 179, Loss: 1.089387, Accuracy: 61.63%\n",
      "Batch 180, Loss: 1.064096, Accuracy: 61.66%\n",
      "Batch 181, Loss: 1.109711, Accuracy: 61.66%\n",
      "Batch 182, Loss: 1.110914, Accuracy: 61.68%\n",
      "Batch 183, Loss: 1.052853, Accuracy: 61.76%\n",
      "Batch 184, Loss: 1.138623, Accuracy: 61.74%\n",
      "Batch 185, Loss: 1.110547, Accuracy: 61.75%\n",
      "Batch 186, Loss: 1.056063, Accuracy: 61.79%\n",
      "Batch 187, Loss: 1.039720, Accuracy: 61.81%\n",
      "Batch 188, Loss: 1.086204, Accuracy: 61.83%\n",
      "Batch 189, Loss: 1.092950, Accuracy: 61.84%\n",
      "Batch 190, Loss: 1.102392, Accuracy: 61.84%\n",
      "Batch 191, Loss: 1.118868, Accuracy: 61.85%\n",
      "Batch 192, Loss: 1.135545, Accuracy: 61.85%\n",
      "Batch 193, Loss: 1.088474, Accuracy: 61.85%\n",
      "Batch 194, Loss: 1.047591, Accuracy: 61.89%\n",
      "Batch 195, Loss: 1.173396, Accuracy: 61.84%\n",
      "Batch 196, Loss: 1.166988, Accuracy: 61.81%\n",
      "Batch 197, Loss: 1.173225, Accuracy: 61.77%\n",
      "Batch 198, Loss: 1.107638, Accuracy: 61.77%\n",
      "Batch 199, Loss: 1.137695, Accuracy: 61.78%\n",
      "Batch 200, Loss: 1.101046, Accuracy: 61.77%\n",
      "Batch 201, Loss: 1.095631, Accuracy: 61.78%\n",
      "Batch 202, Loss: 1.128983, Accuracy: 61.78%\n",
      "Batch 203, Loss: 1.129135, Accuracy: 61.78%\n",
      "Batch 204, Loss: 1.136434, Accuracy: 61.76%\n",
      "Batch 205, Loss: 1.106406, Accuracy: 61.78%\n",
      "Batch 206, Loss: 1.158197, Accuracy: 61.76%\n",
      "Batch 207, Loss: 1.121468, Accuracy: 61.76%\n",
      "Batch 208, Loss: 1.138525, Accuracy: 61.76%\n",
      "Batch 209, Loss: 1.102543, Accuracy: 61.77%\n",
      "Batch 210, Loss: 1.113305, Accuracy: 61.78%\n",
      "Batch 211, Loss: 1.192804, Accuracy: 61.74%\n",
      "Batch 212, Loss: 1.119080, Accuracy: 61.74%\n",
      "Batch 213, Loss: 1.072683, Accuracy: 61.77%\n",
      "Training - Epoch 5, Loss: 1.123163, Accuracy: 61.77%\n",
      "Validation Batch 1, Loss: 1.300756, Accuracy: 43.75%\n",
      "Validation Batch 2, Loss: 1.309762, Accuracy: 42.19%\n",
      "Validation Batch 3, Loss: 1.410095, Accuracy: 38.54%\n",
      "Validation Batch 4, Loss: 1.304699, Accuracy: 39.45%\n",
      "Validation Batch 5, Loss: 1.394014, Accuracy: 37.81%\n",
      "Validation Batch 6, Loss: 1.403085, Accuracy: 36.46%\n",
      "Validation Batch 7, Loss: 1.314960, Accuracy: 37.05%\n",
      "Validation Batch 8, Loss: 1.325357, Accuracy: 37.30%\n",
      "Validation Batch 9, Loss: 1.371564, Accuracy: 36.98%\n",
      "Validation Batch 10, Loss: 1.396860, Accuracy: 36.41%\n",
      "Validation Batch 11, Loss: 1.336058, Accuracy: 36.51%\n",
      "Validation Batch 12, Loss: 1.279114, Accuracy: 36.98%\n",
      "Validation Batch 13, Loss: 1.370775, Accuracy: 36.78%\n",
      "Validation Batch 14, Loss: 1.247730, Accuracy: 37.50%\n",
      "Validation Batch 15, Loss: 1.318542, Accuracy: 37.71%\n",
      "Validation Batch 16, Loss: 1.415458, Accuracy: 37.11%\n",
      "Validation Batch 17, Loss: 1.373474, Accuracy: 37.04%\n",
      "Validation Batch 18, Loss: 1.317616, Accuracy: 37.24%\n",
      "Validation Batch 19, Loss: 1.351679, Accuracy: 37.25%\n",
      "Validation Batch 20, Loss: 1.334968, Accuracy: 37.34%\n",
      "Validation Batch 21, Loss: 1.349799, Accuracy: 37.28%\n",
      "Validation Batch 22, Loss: 1.348285, Accuracy: 37.22%\n",
      "Validation Batch 23, Loss: 1.364160, Accuracy: 37.16%\n",
      "Validation Batch 24, Loss: 1.394012, Accuracy: 36.91%\n",
      "Validation Batch 25, Loss: 1.281695, Accuracy: 37.25%\n",
      "Validation Batch 26, Loss: 1.229454, Accuracy: 37.80%\n",
      "Validation Batch 27, Loss: 1.382511, Accuracy: 37.70%\n",
      "Validation - Epoch 5, Loss: 1.341722, Accuracy: 37.70%\n",
      "Patience—0\n",
      "Epoch 6\n",
      "Batch 1, Loss: 1.119877, Accuracy: 62.50%\n",
      "Batch 2, Loss: 1.110865, Accuracy: 64.06%\n",
      "Batch 3, Loss: 1.206891, Accuracy: 60.94%\n",
      "Batch 4, Loss: 1.091563, Accuracy: 61.72%\n",
      "Batch 5, Loss: 1.114007, Accuracy: 61.25%\n",
      "Batch 6, Loss: 1.135661, Accuracy: 60.94%\n",
      "Batch 7, Loss: 1.036983, Accuracy: 62.50%\n",
      "Batch 8, Loss: 1.140117, Accuracy: 61.91%\n",
      "Batch 9, Loss: 1.106750, Accuracy: 62.15%\n",
      "Batch 10, Loss: 1.105510, Accuracy: 62.66%\n",
      "Batch 11, Loss: 1.126265, Accuracy: 62.78%\n",
      "Batch 12, Loss: 1.154397, Accuracy: 62.50%\n",
      "Batch 13, Loss: 1.093040, Accuracy: 62.74%\n",
      "Batch 14, Loss: 1.140488, Accuracy: 62.72%\n",
      "Batch 15, Loss: 1.112124, Accuracy: 62.50%\n",
      "Batch 16, Loss: 1.096219, Accuracy: 62.60%\n",
      "Batch 17, Loss: 1.137702, Accuracy: 62.32%\n",
      "Batch 18, Loss: 1.133154, Accuracy: 62.07%\n",
      "Batch 19, Loss: 1.132080, Accuracy: 62.09%\n",
      "Batch 20, Loss: 1.078656, Accuracy: 62.19%\n",
      "Batch 21, Loss: 1.134102, Accuracy: 62.13%\n",
      "Batch 22, Loss: 1.095474, Accuracy: 62.22%\n",
      "Batch 23, Loss: 1.188708, Accuracy: 61.89%\n",
      "Batch 24, Loss: 1.086024, Accuracy: 61.98%\n",
      "Batch 25, Loss: 1.051962, Accuracy: 62.38%\n",
      "Batch 26, Loss: 1.146971, Accuracy: 62.32%\n",
      "Batch 27, Loss: 1.058992, Accuracy: 62.56%\n",
      "Batch 28, Loss: 1.061911, Accuracy: 62.89%\n",
      "Batch 29, Loss: 1.105876, Accuracy: 62.88%\n",
      "Batch 30, Loss: 1.011399, Accuracy: 63.33%\n",
      "Batch 31, Loss: 1.115970, Accuracy: 63.26%\n",
      "Batch 32, Loss: 1.138684, Accuracy: 63.23%\n",
      "Batch 33, Loss: 1.078282, Accuracy: 63.26%\n",
      "Batch 34, Loss: 1.139787, Accuracy: 63.24%\n",
      "Batch 35, Loss: 1.108940, Accuracy: 63.30%\n",
      "Batch 36, Loss: 1.116218, Accuracy: 63.37%\n",
      "Batch 37, Loss: 1.021844, Accuracy: 63.68%\n",
      "Batch 38, Loss: 1.175702, Accuracy: 63.40%\n",
      "Batch 39, Loss: 1.144634, Accuracy: 63.26%\n",
      "Batch 40, Loss: 1.136748, Accuracy: 63.20%\n",
      "Batch 41, Loss: 1.046344, Accuracy: 63.34%\n",
      "Batch 42, Loss: 1.149873, Accuracy: 63.24%\n",
      "Batch 43, Loss: 1.111169, Accuracy: 63.23%\n",
      "Batch 44, Loss: 1.136344, Accuracy: 63.14%\n",
      "Batch 45, Loss: 1.205793, Accuracy: 62.81%\n",
      "Batch 46, Loss: 1.060556, Accuracy: 62.87%\n",
      "Batch 47, Loss: 1.134659, Accuracy: 62.77%\n",
      "Batch 48, Loss: 1.127378, Accuracy: 62.73%\n",
      "Batch 49, Loss: 1.033589, Accuracy: 62.88%\n",
      "Batch 50, Loss: 1.014920, Accuracy: 63.09%\n",
      "Batch 51, Loss: 1.094392, Accuracy: 63.08%\n",
      "Batch 52, Loss: 1.142631, Accuracy: 63.01%\n",
      "Batch 53, Loss: 1.089099, Accuracy: 63.06%\n",
      "Batch 54, Loss: 1.098464, Accuracy: 62.99%\n",
      "Batch 55, Loss: 1.129319, Accuracy: 62.95%\n",
      "Batch 56, Loss: 1.153257, Accuracy: 62.86%\n",
      "Batch 57, Loss: 1.109038, Accuracy: 62.86%\n",
      "Batch 58, Loss: 1.079744, Accuracy: 62.90%\n",
      "Batch 59, Loss: 1.144185, Accuracy: 62.84%\n",
      "Batch 60, Loss: 1.006171, Accuracy: 63.02%\n",
      "Batch 61, Loss: 1.013318, Accuracy: 63.22%\n",
      "Batch 62, Loss: 1.135562, Accuracy: 63.16%\n",
      "Batch 63, Loss: 1.175595, Accuracy: 63.05%\n",
      "Batch 64, Loss: 1.057937, Accuracy: 63.18%\n",
      "Batch 65, Loss: 1.143535, Accuracy: 63.12%\n",
      "Batch 66, Loss: 1.132509, Accuracy: 63.04%\n",
      "Batch 67, Loss: 1.118327, Accuracy: 63.01%\n",
      "Batch 68, Loss: 1.129069, Accuracy: 62.98%\n",
      "Batch 69, Loss: 1.144255, Accuracy: 62.95%\n",
      "Batch 70, Loss: 1.074664, Accuracy: 63.01%\n",
      "Batch 71, Loss: 1.124098, Accuracy: 63.03%\n",
      "Batch 72, Loss: 1.126940, Accuracy: 63.00%\n",
      "Batch 73, Loss: 1.186458, Accuracy: 62.84%\n",
      "Batch 74, Loss: 1.029832, Accuracy: 62.96%\n",
      "Batch 75, Loss: 1.127185, Accuracy: 62.98%\n",
      "Batch 76, Loss: 1.135219, Accuracy: 62.95%\n",
      "Batch 77, Loss: 1.093873, Accuracy: 62.97%\n",
      "Batch 78, Loss: 1.074267, Accuracy: 63.06%\n",
      "Batch 79, Loss: 1.188209, Accuracy: 62.97%\n",
      "Batch 80, Loss: 1.171506, Accuracy: 62.93%\n",
      "Batch 81, Loss: 1.130189, Accuracy: 62.92%\n",
      "Batch 82, Loss: 1.075396, Accuracy: 63.00%\n",
      "Batch 83, Loss: 1.123470, Accuracy: 62.99%\n",
      "Batch 84, Loss: 1.178962, Accuracy: 62.85%\n",
      "Batch 85, Loss: 1.115829, Accuracy: 62.83%\n",
      "Batch 86, Loss: 1.104729, Accuracy: 62.81%\n",
      "Batch 87, Loss: 1.220117, Accuracy: 62.72%\n",
      "Batch 88, Loss: 1.045997, Accuracy: 62.80%\n",
      "Batch 89, Loss: 1.131364, Accuracy: 62.80%\n",
      "Batch 90, Loss: 1.144549, Accuracy: 62.73%\n",
      "Batch 91, Loss: 1.107336, Accuracy: 62.74%\n",
      "Batch 92, Loss: 0.977062, Accuracy: 62.92%\n",
      "Batch 93, Loss: 1.068310, Accuracy: 62.99%\n",
      "Batch 94, Loss: 1.228186, Accuracy: 62.83%\n",
      "Batch 95, Loss: 1.167238, Accuracy: 62.76%\n",
      "Batch 96, Loss: 1.147292, Accuracy: 62.73%\n",
      "Batch 97, Loss: 1.122611, Accuracy: 62.71%\n",
      "Batch 98, Loss: 1.050204, Accuracy: 62.79%\n",
      "Batch 99, Loss: 1.108800, Accuracy: 62.75%\n",
      "Batch 100, Loss: 1.099570, Accuracy: 62.77%\n",
      "Batch 101, Loss: 1.022274, Accuracy: 62.84%\n",
      "Batch 102, Loss: 1.077781, Accuracy: 62.88%\n",
      "Batch 103, Loss: 1.116652, Accuracy: 62.88%\n",
      "Batch 104, Loss: 1.121221, Accuracy: 62.86%\n",
      "Batch 105, Loss: 1.138532, Accuracy: 62.86%\n",
      "Batch 106, Loss: 1.131101, Accuracy: 62.82%\n",
      "Batch 107, Loss: 1.161896, Accuracy: 62.76%\n",
      "Batch 108, Loss: 1.139226, Accuracy: 62.73%\n",
      "Batch 109, Loss: 1.125381, Accuracy: 62.70%\n",
      "Batch 110, Loss: 1.103161, Accuracy: 62.71%\n",
      "Batch 111, Loss: 1.081438, Accuracy: 62.75%\n",
      "Batch 112, Loss: 1.069685, Accuracy: 62.82%\n",
      "Batch 113, Loss: 1.089315, Accuracy: 62.86%\n",
      "Batch 114, Loss: 1.121446, Accuracy: 62.84%\n",
      "Batch 115, Loss: 1.040880, Accuracy: 62.91%\n",
      "Batch 116, Loss: 1.082861, Accuracy: 62.94%\n",
      "Batch 117, Loss: 1.112349, Accuracy: 62.94%\n",
      "Batch 118, Loss: 1.102828, Accuracy: 62.95%\n",
      "Batch 119, Loss: 1.081475, Accuracy: 62.95%\n",
      "Batch 120, Loss: 1.137963, Accuracy: 62.93%\n",
      "Batch 121, Loss: 1.074745, Accuracy: 62.99%\n",
      "Batch 122, Loss: 1.101836, Accuracy: 63.01%\n",
      "Batch 123, Loss: 1.121402, Accuracy: 63.02%\n",
      "Batch 124, Loss: 1.116920, Accuracy: 63.00%\n",
      "Batch 125, Loss: 1.154451, Accuracy: 62.98%\n",
      "Batch 126, Loss: 1.127485, Accuracy: 62.97%\n",
      "Batch 127, Loss: 1.104731, Accuracy: 62.96%\n",
      "Batch 128, Loss: 1.138279, Accuracy: 62.92%\n",
      "Batch 129, Loss: 1.120262, Accuracy: 62.91%\n",
      "Batch 130, Loss: 1.101140, Accuracy: 62.92%\n",
      "Batch 131, Loss: 1.120169, Accuracy: 62.89%\n",
      "Batch 132, Loss: 1.066711, Accuracy: 62.93%\n",
      "Batch 133, Loss: 1.243057, Accuracy: 62.82%\n",
      "Batch 134, Loss: 1.099243, Accuracy: 62.84%\n",
      "Batch 135, Loss: 1.112553, Accuracy: 62.81%\n",
      "Batch 136, Loss: 1.047035, Accuracy: 62.83%\n",
      "Batch 137, Loss: 1.194824, Accuracy: 62.74%\n",
      "Batch 138, Loss: 1.133188, Accuracy: 62.73%\n",
      "Batch 139, Loss: 1.056491, Accuracy: 62.77%\n",
      "Batch 140, Loss: 1.053446, Accuracy: 62.82%\n",
      "Batch 141, Loss: 1.116767, Accuracy: 62.83%\n",
      "Batch 142, Loss: 1.123041, Accuracy: 62.81%\n",
      "Batch 143, Loss: 1.048997, Accuracy: 62.86%\n",
      "Batch 144, Loss: 1.123477, Accuracy: 62.84%\n",
      "Batch 145, Loss: 1.141928, Accuracy: 62.82%\n",
      "Batch 146, Loss: 1.081337, Accuracy: 62.82%\n",
      "Batch 147, Loss: 1.118690, Accuracy: 62.82%\n",
      "Batch 148, Loss: 1.110974, Accuracy: 62.80%\n",
      "Batch 149, Loss: 1.144959, Accuracy: 62.75%\n",
      "Batch 150, Loss: 1.066597, Accuracy: 62.79%\n",
      "Batch 151, Loss: 1.132792, Accuracy: 62.77%\n",
      "Batch 152, Loss: 1.051450, Accuracy: 62.84%\n",
      "Batch 153, Loss: 1.039102, Accuracy: 62.89%\n",
      "Batch 154, Loss: 1.079768, Accuracy: 62.92%\n",
      "Batch 155, Loss: 1.062832, Accuracy: 62.96%\n",
      "Batch 156, Loss: 1.144560, Accuracy: 62.93%\n",
      "Batch 157, Loss: 1.034634, Accuracy: 62.97%\n",
      "Batch 158, Loss: 1.119177, Accuracy: 62.98%\n",
      "Batch 159, Loss: 1.127625, Accuracy: 62.99%\n",
      "Batch 160, Loss: 1.083432, Accuracy: 63.01%\n",
      "Batch 161, Loss: 1.005707, Accuracy: 63.09%\n",
      "Batch 162, Loss: 1.098567, Accuracy: 63.09%\n",
      "Batch 163, Loss: 1.053151, Accuracy: 63.13%\n",
      "Batch 164, Loss: 1.204354, Accuracy: 63.06%\n",
      "Batch 165, Loss: 1.094235, Accuracy: 63.08%\n",
      "Batch 166, Loss: 1.196702, Accuracy: 63.02%\n",
      "Batch 167, Loss: 1.125814, Accuracy: 63.01%\n",
      "Batch 168, Loss: 1.020230, Accuracy: 63.09%\n",
      "Batch 169, Loss: 1.130374, Accuracy: 63.07%\n",
      "Batch 170, Loss: 1.161412, Accuracy: 63.04%\n",
      "Batch 171, Loss: 1.120517, Accuracy: 63.03%\n",
      "Batch 172, Loss: 1.119307, Accuracy: 63.02%\n",
      "Batch 173, Loss: 1.077540, Accuracy: 63.05%\n",
      "Batch 174, Loss: 1.069491, Accuracy: 63.07%\n",
      "Batch 175, Loss: 1.143363, Accuracy: 63.04%\n",
      "Batch 176, Loss: 1.156207, Accuracy: 63.01%\n",
      "Batch 177, Loss: 1.108853, Accuracy: 63.02%\n",
      "Batch 178, Loss: 1.086785, Accuracy: 63.02%\n",
      "Batch 179, Loss: 1.101467, Accuracy: 63.03%\n",
      "Batch 180, Loss: 1.056352, Accuracy: 63.07%\n",
      "Batch 181, Loss: 1.094887, Accuracy: 63.06%\n",
      "Batch 182, Loss: 1.056460, Accuracy: 63.10%\n",
      "Batch 183, Loss: 1.107327, Accuracy: 63.11%\n",
      "Batch 184, Loss: 1.091026, Accuracy: 63.10%\n",
      "Batch 185, Loss: 1.070840, Accuracy: 63.11%\n",
      "Batch 186, Loss: 1.113592, Accuracy: 63.10%\n",
      "Batch 187, Loss: 1.039320, Accuracy: 63.12%\n",
      "Batch 188, Loss: 1.111109, Accuracy: 63.13%\n",
      "Batch 189, Loss: 1.132050, Accuracy: 63.11%\n",
      "Batch 190, Loss: 1.073744, Accuracy: 63.16%\n",
      "Batch 191, Loss: 1.111572, Accuracy: 63.15%\n",
      "Batch 192, Loss: 1.102900, Accuracy: 63.15%\n",
      "Batch 193, Loss: 1.087898, Accuracy: 63.16%\n",
      "Batch 194, Loss: 1.007927, Accuracy: 63.22%\n",
      "Batch 195, Loss: 1.099781, Accuracy: 63.24%\n",
      "Batch 196, Loss: 1.047489, Accuracy: 63.27%\n",
      "Batch 197, Loss: 1.105543, Accuracy: 63.26%\n",
      "Batch 198, Loss: 1.061027, Accuracy: 63.30%\n",
      "Batch 199, Loss: 1.102292, Accuracy: 63.29%\n",
      "Batch 200, Loss: 1.160653, Accuracy: 63.26%\n",
      "Batch 201, Loss: 1.060954, Accuracy: 63.29%\n",
      "Batch 202, Loss: 1.067769, Accuracy: 63.32%\n",
      "Batch 203, Loss: 1.146704, Accuracy: 63.30%\n",
      "Batch 204, Loss: 1.025291, Accuracy: 63.35%\n",
      "Batch 205, Loss: 1.060765, Accuracy: 63.37%\n",
      "Batch 206, Loss: 1.008027, Accuracy: 63.43%\n",
      "Batch 207, Loss: 1.145427, Accuracy: 63.40%\n",
      "Batch 208, Loss: 1.039727, Accuracy: 63.44%\n",
      "Batch 209, Loss: 1.114531, Accuracy: 63.43%\n",
      "Batch 210, Loss: 1.005403, Accuracy: 63.49%\n",
      "Batch 211, Loss: 1.057597, Accuracy: 63.51%\n",
      "Batch 212, Loss: 1.087909, Accuracy: 63.53%\n",
      "Batch 213, Loss: 1.119910, Accuracy: 63.53%\n",
      "Training - Epoch 6, Loss: 1.105317, Accuracy: 63.53%\n",
      "Validation Batch 1, Loss: 1.276903, Accuracy: 43.75%\n",
      "Validation Batch 2, Loss: 1.292244, Accuracy: 43.75%\n",
      "Validation Batch 3, Loss: 1.395112, Accuracy: 39.58%\n",
      "Validation Batch 4, Loss: 1.292070, Accuracy: 40.23%\n",
      "Validation Batch 5, Loss: 1.374183, Accuracy: 38.75%\n",
      "Validation Batch 6, Loss: 1.369123, Accuracy: 38.02%\n",
      "Validation Batch 7, Loss: 1.289590, Accuracy: 38.62%\n",
      "Validation Batch 8, Loss: 1.305898, Accuracy: 38.87%\n",
      "Validation Batch 9, Loss: 1.355302, Accuracy: 38.54%\n",
      "Validation Batch 10, Loss: 1.376645, Accuracy: 37.81%\n",
      "Validation Batch 11, Loss: 1.321861, Accuracy: 37.78%\n",
      "Validation Batch 12, Loss: 1.254789, Accuracy: 38.41%\n",
      "Validation Batch 13, Loss: 1.349458, Accuracy: 38.22%\n",
      "Validation Batch 14, Loss: 1.232916, Accuracy: 39.06%\n",
      "Validation Batch 15, Loss: 1.303215, Accuracy: 39.17%\n",
      "Validation Batch 16, Loss: 1.386724, Accuracy: 38.57%\n",
      "Validation Batch 17, Loss: 1.363517, Accuracy: 38.42%\n",
      "Validation Batch 18, Loss: 1.299319, Accuracy: 38.54%\n",
      "Validation Batch 19, Loss: 1.340831, Accuracy: 38.49%\n",
      "Validation Batch 20, Loss: 1.316455, Accuracy: 38.59%\n",
      "Validation Batch 21, Loss: 1.333562, Accuracy: 38.54%\n",
      "Validation Batch 22, Loss: 1.323771, Accuracy: 38.57%\n",
      "Validation Batch 23, Loss: 1.353768, Accuracy: 38.45%\n",
      "Validation Batch 24, Loss: 1.365279, Accuracy: 38.35%\n",
      "Validation Batch 25, Loss: 1.262597, Accuracy: 38.69%\n",
      "Validation Batch 26, Loss: 1.216161, Accuracy: 39.18%\n",
      "Validation Batch 27, Loss: 1.374699, Accuracy: 39.05%\n",
      "Validation - Epoch 6, Loss: 1.323185, Accuracy: 39.05%\n",
      "Patience—0\n",
      "Epoch 7\n",
      "Batch 1, Loss: 1.128775, Accuracy: 60.94%\n",
      "Batch 2, Loss: 1.061563, Accuracy: 66.41%\n",
      "Batch 3, Loss: 1.132638, Accuracy: 64.06%\n",
      "Batch 4, Loss: 1.102924, Accuracy: 63.67%\n",
      "Batch 5, Loss: 1.099033, Accuracy: 63.75%\n",
      "Batch 6, Loss: 1.144337, Accuracy: 63.02%\n",
      "Batch 7, Loss: 1.130128, Accuracy: 62.72%\n",
      "Batch 8, Loss: 1.062959, Accuracy: 63.28%\n",
      "Batch 9, Loss: 1.088709, Accuracy: 63.54%\n",
      "Batch 10, Loss: 1.088933, Accuracy: 63.75%\n",
      "Batch 11, Loss: 1.155385, Accuracy: 63.21%\n",
      "Batch 12, Loss: 1.070704, Accuracy: 63.54%\n",
      "Batch 13, Loss: 1.120212, Accuracy: 63.58%\n",
      "Batch 14, Loss: 1.134255, Accuracy: 63.39%\n",
      "Batch 15, Loss: 1.069896, Accuracy: 63.33%\n",
      "Batch 16, Loss: 1.099577, Accuracy: 63.28%\n",
      "Batch 17, Loss: 1.113692, Accuracy: 63.33%\n",
      "Batch 18, Loss: 1.077398, Accuracy: 63.72%\n",
      "Batch 19, Loss: 1.075680, Accuracy: 63.82%\n",
      "Batch 20, Loss: 1.040280, Accuracy: 64.14%\n",
      "Batch 21, Loss: 1.018334, Accuracy: 64.51%\n",
      "Batch 22, Loss: 1.122910, Accuracy: 64.35%\n",
      "Batch 23, Loss: 1.095814, Accuracy: 64.33%\n",
      "Batch 24, Loss: 1.034812, Accuracy: 64.65%\n",
      "Batch 25, Loss: 1.035095, Accuracy: 64.88%\n",
      "Batch 26, Loss: 1.109863, Accuracy: 64.84%\n",
      "Batch 27, Loss: 0.997273, Accuracy: 65.22%\n",
      "Batch 28, Loss: 1.089678, Accuracy: 65.29%\n",
      "Batch 29, Loss: 1.089491, Accuracy: 65.25%\n",
      "Batch 30, Loss: 1.067341, Accuracy: 65.26%\n",
      "Batch 31, Loss: 1.090292, Accuracy: 65.27%\n",
      "Batch 32, Loss: 1.078766, Accuracy: 65.43%\n",
      "Batch 33, Loss: 1.144158, Accuracy: 65.15%\n",
      "Batch 34, Loss: 1.081742, Accuracy: 65.21%\n",
      "Batch 35, Loss: 1.074409, Accuracy: 65.27%\n",
      "Batch 36, Loss: 1.164328, Accuracy: 65.06%\n",
      "Batch 37, Loss: 1.064003, Accuracy: 65.24%\n",
      "Batch 38, Loss: 1.106926, Accuracy: 65.17%\n",
      "Batch 39, Loss: 1.115580, Accuracy: 65.10%\n",
      "Batch 40, Loss: 1.215505, Accuracy: 64.84%\n",
      "Batch 41, Loss: 1.201037, Accuracy: 64.56%\n",
      "Batch 42, Loss: 1.070862, Accuracy: 64.66%\n",
      "Batch 43, Loss: 1.167925, Accuracy: 64.46%\n",
      "Batch 44, Loss: 1.036535, Accuracy: 64.63%\n",
      "Batch 45, Loss: 1.134469, Accuracy: 64.62%\n",
      "Batch 46, Loss: 1.091368, Accuracy: 64.67%\n",
      "Batch 47, Loss: 1.052084, Accuracy: 64.79%\n",
      "Batch 48, Loss: 1.183576, Accuracy: 64.62%\n",
      "Batch 49, Loss: 1.096123, Accuracy: 64.64%\n",
      "Batch 50, Loss: 1.148588, Accuracy: 64.50%\n",
      "Batch 51, Loss: 1.116675, Accuracy: 64.46%\n",
      "Batch 52, Loss: 1.078318, Accuracy: 64.51%\n",
      "Batch 53, Loss: 1.163422, Accuracy: 64.42%\n",
      "Batch 54, Loss: 1.143806, Accuracy: 64.32%\n",
      "Batch 55, Loss: 1.067795, Accuracy: 64.40%\n",
      "Batch 56, Loss: 1.090578, Accuracy: 64.37%\n",
      "Batch 57, Loss: 1.102025, Accuracy: 64.39%\n",
      "Batch 58, Loss: 1.103023, Accuracy: 64.41%\n",
      "Batch 59, Loss: 1.112745, Accuracy: 64.35%\n",
      "Batch 60, Loss: 1.085872, Accuracy: 64.40%\n",
      "Batch 61, Loss: 1.138872, Accuracy: 64.32%\n",
      "Batch 62, Loss: 1.181481, Accuracy: 64.11%\n",
      "Batch 63, Loss: 1.058919, Accuracy: 64.21%\n",
      "Batch 64, Loss: 1.024239, Accuracy: 64.33%\n",
      "Batch 65, Loss: 1.061946, Accuracy: 64.38%\n",
      "Batch 66, Loss: 1.028568, Accuracy: 64.49%\n",
      "Batch 67, Loss: 0.997490, Accuracy: 64.65%\n",
      "Batch 68, Loss: 1.126233, Accuracy: 64.57%\n",
      "Batch 69, Loss: 1.049486, Accuracy: 64.67%\n",
      "Batch 70, Loss: 1.111992, Accuracy: 64.62%\n",
      "Batch 71, Loss: 1.143759, Accuracy: 64.52%\n",
      "Batch 72, Loss: 1.055671, Accuracy: 64.58%\n",
      "Batch 73, Loss: 1.150442, Accuracy: 64.45%\n",
      "Batch 74, Loss: 1.071844, Accuracy: 64.44%\n",
      "Batch 75, Loss: 1.123031, Accuracy: 64.40%\n",
      "Batch 76, Loss: 1.117465, Accuracy: 64.35%\n",
      "Batch 77, Loss: 1.025003, Accuracy: 64.47%\n",
      "Batch 78, Loss: 1.086191, Accuracy: 64.52%\n",
      "Batch 79, Loss: 1.117939, Accuracy: 64.46%\n",
      "Batch 80, Loss: 1.116053, Accuracy: 64.43%\n",
      "Batch 81, Loss: 1.128218, Accuracy: 64.41%\n",
      "Batch 82, Loss: 1.129097, Accuracy: 64.37%\n",
      "Batch 83, Loss: 1.133369, Accuracy: 64.33%\n",
      "Batch 84, Loss: 1.097411, Accuracy: 64.32%\n",
      "Batch 85, Loss: 1.152106, Accuracy: 64.23%\n",
      "Batch 86, Loss: 1.130712, Accuracy: 64.21%\n",
      "Batch 87, Loss: 1.153821, Accuracy: 64.10%\n",
      "Batch 88, Loss: 1.056344, Accuracy: 64.15%\n",
      "Batch 89, Loss: 1.116316, Accuracy: 64.13%\n",
      "Batch 90, Loss: 1.128460, Accuracy: 64.13%\n",
      "Batch 91, Loss: 1.012900, Accuracy: 64.29%\n",
      "Batch 92, Loss: 1.056857, Accuracy: 64.39%\n",
      "Batch 93, Loss: 1.122484, Accuracy: 64.36%\n",
      "Batch 94, Loss: 1.071897, Accuracy: 64.38%\n",
      "Batch 95, Loss: 1.161420, Accuracy: 64.31%\n",
      "Batch 96, Loss: 1.112740, Accuracy: 64.32%\n",
      "Batch 97, Loss: 1.112511, Accuracy: 64.29%\n",
      "Batch 98, Loss: 1.104626, Accuracy: 64.27%\n",
      "Batch 99, Loss: 1.170593, Accuracy: 64.14%\n",
      "Batch 100, Loss: 0.992010, Accuracy: 64.25%\n",
      "Batch 101, Loss: 1.102793, Accuracy: 64.23%\n",
      "Batch 102, Loss: 1.082734, Accuracy: 64.28%\n",
      "Batch 103, Loss: 1.023292, Accuracy: 64.34%\n",
      "Batch 104, Loss: 1.169937, Accuracy: 64.24%\n",
      "Batch 105, Loss: 1.083046, Accuracy: 64.27%\n",
      "Batch 106, Loss: 1.048730, Accuracy: 64.33%\n",
      "Batch 107, Loss: 1.088887, Accuracy: 64.35%\n",
      "Batch 108, Loss: 1.069596, Accuracy: 64.38%\n",
      "Batch 109, Loss: 1.059416, Accuracy: 64.42%\n",
      "Batch 110, Loss: 1.097458, Accuracy: 64.43%\n",
      "Batch 111, Loss: 1.043641, Accuracy: 64.50%\n",
      "Batch 112, Loss: 1.064854, Accuracy: 64.55%\n",
      "Batch 113, Loss: 1.061254, Accuracy: 64.59%\n",
      "Batch 114, Loss: 1.147546, Accuracy: 64.51%\n",
      "Batch 115, Loss: 1.079521, Accuracy: 64.55%\n",
      "Batch 116, Loss: 1.028257, Accuracy: 64.60%\n",
      "Batch 117, Loss: 1.155300, Accuracy: 64.54%\n",
      "Batch 118, Loss: 1.010372, Accuracy: 64.62%\n",
      "Batch 119, Loss: 1.104004, Accuracy: 64.63%\n",
      "Batch 120, Loss: 1.055539, Accuracy: 64.64%\n",
      "Batch 121, Loss: 1.075188, Accuracy: 64.66%\n",
      "Batch 122, Loss: 1.092019, Accuracy: 64.64%\n",
      "Batch 123, Loss: 1.019303, Accuracy: 64.71%\n",
      "Batch 124, Loss: 1.078786, Accuracy: 64.71%\n",
      "Batch 125, Loss: 1.062935, Accuracy: 64.71%\n",
      "Batch 126, Loss: 1.200044, Accuracy: 64.63%\n",
      "Batch 127, Loss: 1.172550, Accuracy: 64.54%\n",
      "Batch 128, Loss: 1.075429, Accuracy: 64.55%\n",
      "Batch 129, Loss: 1.080247, Accuracy: 64.57%\n",
      "Batch 130, Loss: 1.127551, Accuracy: 64.56%\n",
      "Batch 131, Loss: 1.081975, Accuracy: 64.55%\n",
      "Batch 132, Loss: 1.087870, Accuracy: 64.55%\n",
      "Batch 133, Loss: 1.102152, Accuracy: 64.53%\n",
      "Batch 134, Loss: 1.021087, Accuracy: 64.60%\n",
      "Batch 135, Loss: 1.113991, Accuracy: 64.58%\n",
      "Batch 136, Loss: 1.021014, Accuracy: 64.66%\n",
      "Batch 137, Loss: 1.065653, Accuracy: 64.68%\n",
      "Batch 138, Loss: 1.064350, Accuracy: 64.69%\n",
      "Batch 139, Loss: 1.072798, Accuracy: 64.70%\n",
      "Batch 140, Loss: 1.071936, Accuracy: 64.71%\n",
      "Batch 141, Loss: 1.000170, Accuracy: 64.78%\n",
      "Batch 142, Loss: 0.989941, Accuracy: 64.89%\n",
      "Batch 143, Loss: 1.145070, Accuracy: 64.86%\n",
      "Batch 144, Loss: 1.163175, Accuracy: 64.80%\n",
      "Batch 145, Loss: 1.039379, Accuracy: 64.84%\n",
      "Batch 146, Loss: 1.071664, Accuracy: 64.88%\n",
      "Batch 147, Loss: 1.058702, Accuracy: 64.90%\n",
      "Batch 148, Loss: 1.156511, Accuracy: 64.84%\n",
      "Batch 149, Loss: 1.136777, Accuracy: 64.80%\n",
      "Batch 150, Loss: 1.104727, Accuracy: 64.78%\n",
      "Batch 151, Loss: 1.134352, Accuracy: 64.78%\n",
      "Batch 152, Loss: 1.144750, Accuracy: 64.72%\n",
      "Batch 153, Loss: 1.077183, Accuracy: 64.74%\n",
      "Batch 154, Loss: 1.104061, Accuracy: 64.73%\n",
      "Batch 155, Loss: 1.125264, Accuracy: 64.73%\n",
      "Batch 156, Loss: 1.108097, Accuracy: 64.71%\n",
      "Batch 157, Loss: 1.180193, Accuracy: 64.63%\n",
      "Batch 158, Loss: 1.080178, Accuracy: 64.63%\n",
      "Batch 159, Loss: 1.149830, Accuracy: 64.60%\n",
      "Batch 160, Loss: 1.060450, Accuracy: 64.65%\n",
      "Batch 161, Loss: 1.049756, Accuracy: 64.68%\n",
      "Batch 162, Loss: 1.042718, Accuracy: 64.72%\n",
      "Batch 163, Loss: 1.122142, Accuracy: 64.71%\n",
      "Batch 164, Loss: 1.004254, Accuracy: 64.78%\n",
      "Batch 165, Loss: 1.011066, Accuracy: 64.83%\n",
      "Batch 166, Loss: 1.147043, Accuracy: 64.79%\n",
      "Batch 167, Loss: 0.999503, Accuracy: 64.87%\n",
      "Batch 168, Loss: 1.072936, Accuracy: 64.87%\n",
      "Batch 169, Loss: 1.049735, Accuracy: 64.90%\n",
      "Batch 170, Loss: 1.117398, Accuracy: 64.88%\n",
      "Batch 171, Loss: 1.079249, Accuracy: 64.88%\n",
      "Batch 172, Loss: 1.093023, Accuracy: 64.90%\n",
      "Batch 173, Loss: 1.021457, Accuracy: 64.92%\n",
      "Batch 174, Loss: 1.101775, Accuracy: 64.92%\n",
      "Batch 175, Loss: 1.069335, Accuracy: 64.92%\n",
      "Batch 176, Loss: 1.050831, Accuracy: 64.94%\n",
      "Batch 177, Loss: 1.085433, Accuracy: 64.92%\n",
      "Batch 178, Loss: 1.083731, Accuracy: 64.91%\n",
      "Batch 179, Loss: 1.140364, Accuracy: 64.88%\n",
      "Batch 180, Loss: 1.151219, Accuracy: 64.85%\n",
      "Batch 181, Loss: 1.135821, Accuracy: 64.83%\n",
      "Batch 182, Loss: 1.059615, Accuracy: 64.84%\n",
      "Batch 183, Loss: 1.105169, Accuracy: 64.81%\n",
      "Batch 184, Loss: 1.133069, Accuracy: 64.78%\n",
      "Batch 185, Loss: 1.082487, Accuracy: 64.80%\n",
      "Batch 186, Loss: 1.081558, Accuracy: 64.80%\n",
      "Batch 187, Loss: 1.064132, Accuracy: 64.81%\n",
      "Batch 188, Loss: 1.124698, Accuracy: 64.78%\n",
      "Batch 189, Loss: 1.176019, Accuracy: 64.74%\n",
      "Batch 190, Loss: 1.137531, Accuracy: 64.72%\n",
      "Batch 191, Loss: 1.043492, Accuracy: 64.75%\n",
      "Batch 192, Loss: 1.067615, Accuracy: 64.75%\n",
      "Batch 193, Loss: 1.130193, Accuracy: 64.73%\n",
      "Batch 194, Loss: 1.113312, Accuracy: 64.71%\n",
      "Batch 195, Loss: 1.086397, Accuracy: 64.71%\n",
      "Batch 196, Loss: 1.022209, Accuracy: 64.76%\n",
      "Batch 197, Loss: 1.133938, Accuracy: 64.72%\n",
      "Batch 198, Loss: 1.136692, Accuracy: 64.69%\n",
      "Batch 199, Loss: 1.152555, Accuracy: 64.67%\n",
      "Batch 200, Loss: 1.024582, Accuracy: 64.71%\n",
      "Batch 201, Loss: 1.028151, Accuracy: 64.74%\n",
      "Batch 202, Loss: 1.113321, Accuracy: 64.72%\n",
      "Batch 203, Loss: 1.056356, Accuracy: 64.75%\n",
      "Batch 204, Loss: 1.081507, Accuracy: 64.75%\n",
      "Batch 205, Loss: 1.099806, Accuracy: 64.76%\n",
      "Batch 206, Loss: 1.049399, Accuracy: 64.77%\n",
      "Batch 207, Loss: 0.952016, Accuracy: 64.86%\n",
      "Batch 208, Loss: 1.074986, Accuracy: 64.87%\n",
      "Batch 209, Loss: 1.104790, Accuracy: 64.85%\n",
      "Batch 210, Loss: 0.995997, Accuracy: 64.90%\n",
      "Batch 211, Loss: 1.068338, Accuracy: 64.91%\n",
      "Batch 212, Loss: 1.157823, Accuracy: 64.88%\n",
      "Batch 213, Loss: 1.021416, Accuracy: 64.92%\n",
      "Training - Epoch 7, Loss: 1.091888, Accuracy: 64.92%\n",
      "Validation Batch 1, Loss: 1.250717, Accuracy: 43.75%\n",
      "Validation Batch 2, Loss: 1.275656, Accuracy: 43.75%\n",
      "Validation Batch 3, Loss: 1.372865, Accuracy: 39.58%\n",
      "Validation Batch 4, Loss: 1.268057, Accuracy: 40.62%\n",
      "Validation Batch 5, Loss: 1.349915, Accuracy: 39.38%\n",
      "Validation Batch 6, Loss: 1.334305, Accuracy: 38.80%\n",
      "Validation Batch 7, Loss: 1.267775, Accuracy: 39.51%\n",
      "Validation Batch 8, Loss: 1.284911, Accuracy: 40.04%\n",
      "Validation Batch 9, Loss: 1.330346, Accuracy: 39.58%\n",
      "Validation Batch 10, Loss: 1.340560, Accuracy: 39.22%\n",
      "Validation Batch 11, Loss: 1.296802, Accuracy: 39.20%\n",
      "Validation Batch 12, Loss: 1.227123, Accuracy: 40.10%\n",
      "Validation Batch 13, Loss: 1.319862, Accuracy: 39.78%\n",
      "Validation Batch 14, Loss: 1.213363, Accuracy: 40.62%\n",
      "Validation Batch 15, Loss: 1.281966, Accuracy: 40.83%\n",
      "Validation Batch 16, Loss: 1.353707, Accuracy: 40.43%\n",
      "Validation Batch 17, Loss: 1.351042, Accuracy: 40.17%\n",
      "Validation Batch 18, Loss: 1.281271, Accuracy: 40.36%\n",
      "Validation Batch 19, Loss: 1.323076, Accuracy: 40.21%\n",
      "Validation Batch 20, Loss: 1.294259, Accuracy: 40.31%\n",
      "Validation Batch 21, Loss: 1.312575, Accuracy: 40.25%\n",
      "Validation Batch 22, Loss: 1.296588, Accuracy: 40.41%\n",
      "Validation Batch 23, Loss: 1.337897, Accuracy: 40.29%\n",
      "Validation Batch 24, Loss: 1.332576, Accuracy: 40.23%\n",
      "Validation Batch 25, Loss: 1.242980, Accuracy: 40.56%\n",
      "Validation Batch 26, Loss: 1.202092, Accuracy: 41.05%\n",
      "Validation Batch 27, Loss: 1.352537, Accuracy: 40.87%\n",
      "Validation - Epoch 7, Loss: 1.299808, Accuracy: 40.87%\n",
      "Patience—0\n",
      "Epoch 8\n",
      "Batch 1, Loss: 1.121702, Accuracy: 60.94%\n",
      "Batch 2, Loss: 1.102939, Accuracy: 62.50%\n",
      "Batch 3, Loss: 1.005441, Accuracy: 65.62%\n",
      "Batch 4, Loss: 1.067407, Accuracy: 66.41%\n",
      "Batch 5, Loss: 1.077552, Accuracy: 66.25%\n",
      "Batch 6, Loss: 1.072541, Accuracy: 66.67%\n",
      "Batch 7, Loss: 1.167658, Accuracy: 65.62%\n",
      "Batch 8, Loss: 1.101964, Accuracy: 65.43%\n",
      "Batch 9, Loss: 1.127292, Accuracy: 64.76%\n",
      "Batch 10, Loss: 1.060315, Accuracy: 65.16%\n",
      "Batch 11, Loss: 1.150216, Accuracy: 64.49%\n",
      "Batch 12, Loss: 1.152740, Accuracy: 63.93%\n",
      "Batch 13, Loss: 1.101804, Accuracy: 63.70%\n",
      "Batch 14, Loss: 1.125010, Accuracy: 63.50%\n",
      "Batch 15, Loss: 1.044502, Accuracy: 64.06%\n",
      "Batch 16, Loss: 1.031146, Accuracy: 64.45%\n",
      "Batch 17, Loss: 1.113213, Accuracy: 64.34%\n",
      "Batch 18, Loss: 1.140272, Accuracy: 64.15%\n",
      "Batch 19, Loss: 1.068051, Accuracy: 64.14%\n",
      "Batch 20, Loss: 1.081974, Accuracy: 64.22%\n",
      "Batch 21, Loss: 1.008568, Accuracy: 64.51%\n",
      "Batch 22, Loss: 1.074154, Accuracy: 64.56%\n",
      "Batch 23, Loss: 1.062479, Accuracy: 64.67%\n",
      "Batch 24, Loss: 1.051383, Accuracy: 64.91%\n",
      "Batch 25, Loss: 0.959174, Accuracy: 65.44%\n",
      "Batch 26, Loss: 1.141767, Accuracy: 65.02%\n",
      "Batch 27, Loss: 1.058615, Accuracy: 65.16%\n",
      "Batch 28, Loss: 1.094523, Accuracy: 65.12%\n",
      "Batch 29, Loss: 1.041890, Accuracy: 65.41%\n",
      "Batch 30, Loss: 1.171989, Accuracy: 65.16%\n",
      "Batch 31, Loss: 1.083463, Accuracy: 65.27%\n",
      "Batch 32, Loss: 1.170522, Accuracy: 64.94%\n",
      "Batch 33, Loss: 1.089371, Accuracy: 65.01%\n",
      "Batch 34, Loss: 1.084462, Accuracy: 65.07%\n",
      "Batch 35, Loss: 1.076094, Accuracy: 65.04%\n",
      "Batch 36, Loss: 1.102929, Accuracy: 64.84%\n",
      "Batch 37, Loss: 1.144578, Accuracy: 64.61%\n",
      "Batch 38, Loss: 1.077786, Accuracy: 64.60%\n",
      "Batch 39, Loss: 1.159490, Accuracy: 64.38%\n",
      "Batch 40, Loss: 1.037931, Accuracy: 64.49%\n",
      "Batch 41, Loss: 1.143415, Accuracy: 64.37%\n",
      "Batch 42, Loss: 1.163455, Accuracy: 64.25%\n",
      "Batch 43, Loss: 1.033045, Accuracy: 64.43%\n",
      "Batch 44, Loss: 1.075599, Accuracy: 64.52%\n",
      "Batch 45, Loss: 1.114296, Accuracy: 64.44%\n",
      "Batch 46, Loss: 1.086187, Accuracy: 64.47%\n",
      "Batch 47, Loss: 1.091282, Accuracy: 64.53%\n",
      "Batch 48, Loss: 1.049573, Accuracy: 64.62%\n",
      "Batch 49, Loss: 1.003147, Accuracy: 64.89%\n",
      "Batch 50, Loss: 0.968038, Accuracy: 65.19%\n",
      "Batch 51, Loss: 1.058807, Accuracy: 65.26%\n",
      "Batch 52, Loss: 1.102470, Accuracy: 65.23%\n",
      "Batch 53, Loss: 1.095045, Accuracy: 65.24%\n",
      "Batch 54, Loss: 1.017167, Accuracy: 65.39%\n",
      "Batch 55, Loss: 1.085886, Accuracy: 65.40%\n",
      "Batch 56, Loss: 1.052702, Accuracy: 65.49%\n",
      "Batch 57, Loss: 1.180978, Accuracy: 65.30%\n",
      "Batch 58, Loss: 1.128099, Accuracy: 65.22%\n",
      "Batch 59, Loss: 1.094005, Accuracy: 65.12%\n",
      "Batch 60, Loss: 1.068820, Accuracy: 65.18%\n",
      "Batch 61, Loss: 1.085521, Accuracy: 65.16%\n",
      "Batch 62, Loss: 1.051716, Accuracy: 65.20%\n",
      "Batch 63, Loss: 1.111914, Accuracy: 65.10%\n",
      "Batch 64, Loss: 1.047099, Accuracy: 65.23%\n",
      "Batch 65, Loss: 1.063585, Accuracy: 65.26%\n",
      "Batch 66, Loss: 1.079636, Accuracy: 65.25%\n",
      "Batch 67, Loss: 1.099421, Accuracy: 65.23%\n",
      "Batch 68, Loss: 1.012992, Accuracy: 65.33%\n",
      "Batch 69, Loss: 1.082474, Accuracy: 65.35%\n",
      "Batch 70, Loss: 1.083058, Accuracy: 65.36%\n",
      "Batch 71, Loss: 1.092745, Accuracy: 65.34%\n",
      "Batch 72, Loss: 1.114536, Accuracy: 65.28%\n",
      "Batch 73, Loss: 1.117285, Accuracy: 65.24%\n",
      "Batch 74, Loss: 1.118894, Accuracy: 65.18%\n",
      "Batch 75, Loss: 1.071294, Accuracy: 65.21%\n",
      "Batch 76, Loss: 1.087894, Accuracy: 65.23%\n",
      "Batch 77, Loss: 1.064562, Accuracy: 65.26%\n",
      "Batch 78, Loss: 1.149450, Accuracy: 65.14%\n",
      "Batch 79, Loss: 1.167985, Accuracy: 65.03%\n",
      "Batch 80, Loss: 1.022130, Accuracy: 65.12%\n",
      "Batch 81, Loss: 1.091948, Accuracy: 65.07%\n",
      "Batch 82, Loss: 1.089934, Accuracy: 65.07%\n",
      "Batch 83, Loss: 1.173911, Accuracy: 65.02%\n",
      "Batch 84, Loss: 1.106170, Accuracy: 64.99%\n",
      "Batch 85, Loss: 1.060550, Accuracy: 65.02%\n",
      "Batch 86, Loss: 1.113088, Accuracy: 64.99%\n",
      "Batch 87, Loss: 1.068426, Accuracy: 65.03%\n",
      "Batch 88, Loss: 1.020111, Accuracy: 65.16%\n",
      "Batch 89, Loss: 1.067230, Accuracy: 65.19%\n",
      "Batch 90, Loss: 1.071932, Accuracy: 65.21%\n",
      "Batch 91, Loss: 1.046743, Accuracy: 65.28%\n",
      "Batch 92, Loss: 1.064782, Accuracy: 65.27%\n",
      "Batch 93, Loss: 1.051670, Accuracy: 65.29%\n",
      "Batch 94, Loss: 1.050754, Accuracy: 65.33%\n",
      "Batch 95, Loss: 1.083605, Accuracy: 65.35%\n",
      "Batch 96, Loss: 1.049587, Accuracy: 65.38%\n",
      "Batch 97, Loss: 1.139273, Accuracy: 65.37%\n",
      "Batch 98, Loss: 1.089095, Accuracy: 65.34%\n",
      "Batch 99, Loss: 1.093472, Accuracy: 65.36%\n",
      "Batch 100, Loss: 1.095573, Accuracy: 65.36%\n",
      "Batch 101, Loss: 1.011623, Accuracy: 65.45%\n",
      "Batch 102, Loss: 0.985581, Accuracy: 65.55%\n",
      "Batch 103, Loss: 1.095353, Accuracy: 65.53%\n",
      "Batch 104, Loss: 1.124068, Accuracy: 65.49%\n",
      "Batch 105, Loss: 1.045904, Accuracy: 65.54%\n",
      "Batch 106, Loss: 1.084877, Accuracy: 65.54%\n",
      "Batch 107, Loss: 1.075354, Accuracy: 65.54%\n",
      "Batch 108, Loss: 1.115317, Accuracy: 65.49%\n",
      "Batch 109, Loss: 1.070826, Accuracy: 65.50%\n",
      "Batch 110, Loss: 1.106473, Accuracy: 65.50%\n",
      "Batch 111, Loss: 1.095183, Accuracy: 65.47%\n",
      "Batch 112, Loss: 1.088805, Accuracy: 65.46%\n",
      "Batch 113, Loss: 1.050200, Accuracy: 65.50%\n",
      "Batch 114, Loss: 1.077858, Accuracy: 65.50%\n",
      "Batch 115, Loss: 1.077510, Accuracy: 65.53%\n",
      "Batch 116, Loss: 1.131212, Accuracy: 65.48%\n",
      "Batch 117, Loss: 1.165655, Accuracy: 65.41%\n",
      "Batch 118, Loss: 1.107803, Accuracy: 65.39%\n",
      "Batch 119, Loss: 1.120386, Accuracy: 65.35%\n",
      "Batch 120, Loss: 1.087427, Accuracy: 65.36%\n",
      "Batch 121, Loss: 1.028581, Accuracy: 65.43%\n",
      "Batch 122, Loss: 1.127670, Accuracy: 65.39%\n",
      "Batch 123, Loss: 1.008614, Accuracy: 65.45%\n",
      "Batch 124, Loss: 1.082384, Accuracy: 65.44%\n",
      "Batch 125, Loss: 1.110057, Accuracy: 65.44%\n",
      "Batch 126, Loss: 1.140922, Accuracy: 65.36%\n",
      "Batch 127, Loss: 1.111529, Accuracy: 65.34%\n",
      "Batch 128, Loss: 1.043057, Accuracy: 65.38%\n",
      "Batch 129, Loss: 1.081831, Accuracy: 65.39%\n",
      "Batch 130, Loss: 1.016205, Accuracy: 65.46%\n",
      "Batch 131, Loss: 1.082485, Accuracy: 65.47%\n",
      "Batch 132, Loss: 1.121508, Accuracy: 65.45%\n",
      "Batch 133, Loss: 1.079815, Accuracy: 65.46%\n",
      "Batch 134, Loss: 1.052181, Accuracy: 65.47%\n",
      "Batch 135, Loss: 0.995523, Accuracy: 65.56%\n",
      "Batch 136, Loss: 1.150293, Accuracy: 65.50%\n",
      "Batch 137, Loss: 1.113535, Accuracy: 65.48%\n",
      "Batch 138, Loss: 1.131777, Accuracy: 65.41%\n",
      "Batch 139, Loss: 1.090337, Accuracy: 65.41%\n",
      "Batch 140, Loss: 1.141205, Accuracy: 65.36%\n",
      "Batch 141, Loss: 1.056008, Accuracy: 65.39%\n",
      "Batch 142, Loss: 1.074076, Accuracy: 65.38%\n",
      "Batch 143, Loss: 1.013400, Accuracy: 65.46%\n",
      "Batch 144, Loss: 0.988056, Accuracy: 65.53%\n",
      "Batch 145, Loss: 1.124220, Accuracy: 65.50%\n",
      "Batch 146, Loss: 1.079420, Accuracy: 65.52%\n",
      "Batch 147, Loss: 1.129377, Accuracy: 65.47%\n",
      "Batch 148, Loss: 1.089590, Accuracy: 65.46%\n",
      "Batch 149, Loss: 1.045907, Accuracy: 65.49%\n",
      "Batch 150, Loss: 1.147931, Accuracy: 65.44%\n",
      "Batch 151, Loss: 1.075829, Accuracy: 65.44%\n",
      "Batch 152, Loss: 1.150777, Accuracy: 65.37%\n",
      "Batch 153, Loss: 1.046681, Accuracy: 65.38%\n",
      "Batch 154, Loss: 1.113925, Accuracy: 65.36%\n",
      "Batch 155, Loss: 1.039897, Accuracy: 65.38%\n",
      "Batch 156, Loss: 1.076705, Accuracy: 65.38%\n",
      "Batch 157, Loss: 1.118297, Accuracy: 65.37%\n",
      "Batch 158, Loss: 0.975616, Accuracy: 65.45%\n",
      "Batch 159, Loss: 1.113944, Accuracy: 65.43%\n",
      "Batch 160, Loss: 1.105178, Accuracy: 65.42%\n",
      "Batch 161, Loss: 1.039305, Accuracy: 65.46%\n",
      "Batch 162, Loss: 1.101192, Accuracy: 65.46%\n",
      "Batch 163, Loss: 1.159366, Accuracy: 65.41%\n",
      "Batch 164, Loss: 1.062481, Accuracy: 65.43%\n",
      "Batch 165, Loss: 1.042051, Accuracy: 65.47%\n",
      "Batch 166, Loss: 1.053574, Accuracy: 65.48%\n",
      "Batch 167, Loss: 1.078123, Accuracy: 65.49%\n",
      "Batch 168, Loss: 0.998126, Accuracy: 65.55%\n",
      "Batch 169, Loss: 1.024415, Accuracy: 65.59%\n",
      "Batch 170, Loss: 1.144701, Accuracy: 65.54%\n",
      "Batch 171, Loss: 1.049046, Accuracy: 65.56%\n",
      "Batch 172, Loss: 1.096270, Accuracy: 65.57%\n",
      "Batch 173, Loss: 1.080528, Accuracy: 65.56%\n",
      "Batch 174, Loss: 1.125838, Accuracy: 65.54%\n",
      "Batch 175, Loss: 1.037425, Accuracy: 65.59%\n",
      "Batch 176, Loss: 1.099206, Accuracy: 65.57%\n",
      "Batch 177, Loss: 1.085934, Accuracy: 65.56%\n",
      "Batch 178, Loss: 1.086807, Accuracy: 65.56%\n",
      "Batch 179, Loss: 1.038004, Accuracy: 65.59%\n",
      "Batch 180, Loss: 1.081668, Accuracy: 65.60%\n",
      "Batch 181, Loss: 1.075027, Accuracy: 65.62%\n",
      "Batch 182, Loss: 1.155286, Accuracy: 65.58%\n",
      "Batch 183, Loss: 1.042629, Accuracy: 65.60%\n",
      "Batch 184, Loss: 1.054755, Accuracy: 65.61%\n",
      "Batch 185, Loss: 1.112329, Accuracy: 65.59%\n",
      "Batch 186, Loss: 1.080812, Accuracy: 65.60%\n",
      "Batch 187, Loss: 1.119573, Accuracy: 65.57%\n",
      "Batch 188, Loss: 1.097181, Accuracy: 65.55%\n",
      "Batch 189, Loss: 1.171202, Accuracy: 65.48%\n",
      "Batch 190, Loss: 1.081267, Accuracy: 65.48%\n",
      "Batch 191, Loss: 1.041102, Accuracy: 65.49%\n",
      "Batch 192, Loss: 1.060537, Accuracy: 65.49%\n",
      "Batch 193, Loss: 1.100119, Accuracy: 65.49%\n",
      "Batch 194, Loss: 0.966646, Accuracy: 65.56%\n",
      "Batch 195, Loss: 1.129482, Accuracy: 65.53%\n",
      "Batch 196, Loss: 1.164148, Accuracy: 65.47%\n",
      "Batch 197, Loss: 1.134443, Accuracy: 65.43%\n",
      "Batch 198, Loss: 1.040011, Accuracy: 65.45%\n",
      "Batch 199, Loss: 1.062109, Accuracy: 65.47%\n",
      "Batch 200, Loss: 1.165609, Accuracy: 65.41%\n",
      "Batch 201, Loss: 1.056120, Accuracy: 65.42%\n",
      "Batch 202, Loss: 1.158442, Accuracy: 65.38%\n",
      "Batch 203, Loss: 1.063651, Accuracy: 65.38%\n",
      "Batch 204, Loss: 1.133276, Accuracy: 65.35%\n",
      "Batch 205, Loss: 1.064231, Accuracy: 65.37%\n",
      "Batch 206, Loss: 1.089004, Accuracy: 65.36%\n",
      "Batch 207, Loss: 1.038679, Accuracy: 65.40%\n",
      "Batch 208, Loss: 1.108495, Accuracy: 65.38%\n",
      "Batch 209, Loss: 1.080559, Accuracy: 65.40%\n",
      "Batch 210, Loss: 1.074141, Accuracy: 65.42%\n",
      "Batch 211, Loss: 1.055249, Accuracy: 65.43%\n",
      "Batch 212, Loss: 1.121654, Accuracy: 65.43%\n",
      "Batch 213, Loss: 1.117055, Accuracy: 65.42%\n",
      "Training - Epoch 8, Loss: 1.084787, Accuracy: 65.42%\n",
      "Validation Batch 1, Loss: 1.224270, Accuracy: 46.88%\n",
      "Validation Batch 2, Loss: 1.265356, Accuracy: 44.53%\n",
      "Validation Batch 3, Loss: 1.345296, Accuracy: 41.15%\n",
      "Validation Batch 4, Loss: 1.236434, Accuracy: 42.19%\n",
      "Validation Batch 5, Loss: 1.319461, Accuracy: 41.56%\n",
      "Validation Batch 6, Loss: 1.296376, Accuracy: 41.67%\n",
      "Validation Batch 7, Loss: 1.245957, Accuracy: 42.41%\n",
      "Validation Batch 8, Loss: 1.267473, Accuracy: 42.58%\n",
      "Validation Batch 9, Loss: 1.299714, Accuracy: 42.36%\n",
      "Validation Batch 10, Loss: 1.304711, Accuracy: 42.19%\n",
      "Validation Batch 11, Loss: 1.268564, Accuracy: 42.19%\n",
      "Validation Batch 12, Loss: 1.196852, Accuracy: 42.97%\n",
      "Validation Batch 13, Loss: 1.289159, Accuracy: 42.91%\n",
      "Validation Batch 14, Loss: 1.190706, Accuracy: 43.64%\n",
      "Validation Batch 15, Loss: 1.257288, Accuracy: 43.75%\n",
      "Validation Batch 16, Loss: 1.319657, Accuracy: 43.36%\n",
      "Validation Batch 17, Loss: 1.334330, Accuracy: 43.01%\n",
      "Validation Batch 18, Loss: 1.260766, Accuracy: 43.14%\n",
      "Validation Batch 19, Loss: 1.300743, Accuracy: 42.93%\n",
      "Validation Batch 20, Loss: 1.265913, Accuracy: 42.97%\n",
      "Validation Batch 21, Loss: 1.285087, Accuracy: 42.86%\n",
      "Validation Batch 22, Loss: 1.272819, Accuracy: 43.04%\n",
      "Validation Batch 23, Loss: 1.314438, Accuracy: 42.87%\n",
      "Validation Batch 24, Loss: 1.296994, Accuracy: 42.77%\n",
      "Validation Batch 25, Loss: 1.223998, Accuracy: 43.00%\n",
      "Validation Batch 26, Loss: 1.183957, Accuracy: 43.45%\n",
      "Validation Batch 27, Loss: 1.328079, Accuracy: 43.34%\n",
      "Validation - Epoch 8, Loss: 1.273867, Accuracy: 43.34%\n",
      "Patience—0\n",
      "Epoch 9\n",
      "Batch 1, Loss: 1.077456, Accuracy: 65.62%\n",
      "Batch 2, Loss: 1.057546, Accuracy: 67.97%\n",
      "Batch 3, Loss: 1.152200, Accuracy: 64.58%\n",
      "Batch 4, Loss: 1.105484, Accuracy: 64.84%\n",
      "Batch 5, Loss: 1.112954, Accuracy: 64.38%\n",
      "Batch 6, Loss: 1.101282, Accuracy: 63.54%\n",
      "Batch 7, Loss: 1.081809, Accuracy: 63.62%\n",
      "Batch 8, Loss: 1.025324, Accuracy: 64.84%\n",
      "Batch 9, Loss: 1.020265, Accuracy: 65.97%\n",
      "Batch 10, Loss: 1.067107, Accuracy: 66.25%\n",
      "Batch 11, Loss: 1.046861, Accuracy: 66.48%\n",
      "Batch 12, Loss: 1.074761, Accuracy: 66.54%\n",
      "Batch 13, Loss: 1.202034, Accuracy: 65.26%\n",
      "Batch 14, Loss: 1.238952, Accuracy: 64.06%\n",
      "Batch 15, Loss: 0.991781, Accuracy: 64.90%\n",
      "Batch 16, Loss: 1.100313, Accuracy: 64.84%\n",
      "Batch 17, Loss: 1.091541, Accuracy: 64.61%\n",
      "Batch 18, Loss: 1.032279, Accuracy: 65.02%\n",
      "Batch 19, Loss: 1.067751, Accuracy: 65.21%\n",
      "Batch 20, Loss: 1.073784, Accuracy: 65.16%\n",
      "Batch 21, Loss: 1.074407, Accuracy: 65.18%\n",
      "Batch 22, Loss: 1.018353, Accuracy: 65.48%\n",
      "Batch 23, Loss: 1.070204, Accuracy: 65.62%\n",
      "Batch 24, Loss: 1.192652, Accuracy: 65.17%\n",
      "Batch 25, Loss: 1.058777, Accuracy: 65.31%\n",
      "Batch 26, Loss: 1.034626, Accuracy: 65.62%\n",
      "Batch 27, Loss: 1.006123, Accuracy: 65.86%\n",
      "Batch 28, Loss: 1.090584, Accuracy: 65.74%\n",
      "Batch 29, Loss: 1.022787, Accuracy: 65.95%\n",
      "Batch 30, Loss: 1.209965, Accuracy: 65.42%\n",
      "Batch 31, Loss: 0.988158, Accuracy: 65.57%\n",
      "Batch 32, Loss: 1.072141, Accuracy: 65.48%\n",
      "Batch 33, Loss: 1.102483, Accuracy: 65.34%\n",
      "Batch 34, Loss: 1.069857, Accuracy: 65.44%\n",
      "Batch 35, Loss: 1.100864, Accuracy: 65.40%\n",
      "Batch 36, Loss: 1.115839, Accuracy: 65.36%\n",
      "Batch 37, Loss: 1.177764, Accuracy: 65.12%\n",
      "Batch 38, Loss: 1.151578, Accuracy: 64.93%\n",
      "Batch 39, Loss: 1.179822, Accuracy: 64.70%\n",
      "Batch 40, Loss: 1.049685, Accuracy: 64.84%\n",
      "Batch 41, Loss: 1.043799, Accuracy: 65.02%\n",
      "Batch 42, Loss: 1.105591, Accuracy: 64.99%\n",
      "Batch 43, Loss: 1.076953, Accuracy: 65.01%\n",
      "Batch 44, Loss: 1.147583, Accuracy: 64.88%\n",
      "Batch 45, Loss: 0.991771, Accuracy: 65.21%\n",
      "Batch 46, Loss: 1.149206, Accuracy: 65.15%\n",
      "Batch 47, Loss: 1.079167, Accuracy: 65.16%\n",
      "Batch 48, Loss: 1.053474, Accuracy: 65.23%\n",
      "Batch 49, Loss: 1.110920, Accuracy: 65.21%\n",
      "Batch 50, Loss: 1.049178, Accuracy: 65.28%\n",
      "Batch 51, Loss: 1.026880, Accuracy: 65.44%\n",
      "Batch 52, Loss: 1.126235, Accuracy: 65.35%\n",
      "Batch 53, Loss: 1.140187, Accuracy: 65.24%\n",
      "Batch 54, Loss: 1.112095, Accuracy: 65.22%\n",
      "Batch 55, Loss: 1.064991, Accuracy: 65.28%\n",
      "Batch 56, Loss: 1.126254, Accuracy: 65.15%\n",
      "Batch 57, Loss: 1.064573, Accuracy: 65.19%\n",
      "Batch 58, Loss: 1.058415, Accuracy: 65.22%\n",
      "Batch 59, Loss: 1.090362, Accuracy: 65.17%\n",
      "Batch 60, Loss: 1.208209, Accuracy: 64.95%\n",
      "Batch 61, Loss: 1.117866, Accuracy: 64.86%\n",
      "Batch 62, Loss: 1.099852, Accuracy: 64.82%\n",
      "Batch 63, Loss: 1.111887, Accuracy: 64.78%\n",
      "Batch 64, Loss: 1.128681, Accuracy: 64.72%\n",
      "Batch 65, Loss: 1.028065, Accuracy: 64.83%\n",
      "Batch 66, Loss: 1.187570, Accuracy: 64.68%\n",
      "Batch 67, Loss: 1.044534, Accuracy: 64.72%\n",
      "Batch 68, Loss: 1.016340, Accuracy: 64.84%\n",
      "Batch 69, Loss: 1.103429, Accuracy: 64.81%\n",
      "Batch 70, Loss: 1.094756, Accuracy: 64.84%\n",
      "Batch 71, Loss: 1.131307, Accuracy: 64.72%\n",
      "Batch 72, Loss: 1.029718, Accuracy: 64.84%\n",
      "Batch 73, Loss: 1.078958, Accuracy: 64.88%\n",
      "Batch 74, Loss: 1.034577, Accuracy: 64.99%\n",
      "Batch 75, Loss: 1.075319, Accuracy: 65.04%\n",
      "Batch 76, Loss: 1.045319, Accuracy: 65.07%\n",
      "Batch 77, Loss: 1.118841, Accuracy: 65.02%\n",
      "Batch 78, Loss: 1.019624, Accuracy: 65.06%\n",
      "Batch 79, Loss: 1.101429, Accuracy: 65.05%\n",
      "Batch 80, Loss: 1.071965, Accuracy: 65.06%\n",
      "Batch 81, Loss: 1.119537, Accuracy: 65.01%\n",
      "Batch 82, Loss: 1.107567, Accuracy: 65.00%\n",
      "Batch 83, Loss: 0.976740, Accuracy: 65.15%\n",
      "Batch 84, Loss: 1.051790, Accuracy: 65.18%\n",
      "Batch 85, Loss: 1.036436, Accuracy: 65.24%\n",
      "Batch 86, Loss: 1.038908, Accuracy: 65.30%\n",
      "Batch 87, Loss: 1.057072, Accuracy: 65.36%\n",
      "Batch 88, Loss: 1.191924, Accuracy: 65.22%\n",
      "Batch 89, Loss: 1.074632, Accuracy: 65.20%\n",
      "Batch 90, Loss: 1.073601, Accuracy: 65.19%\n",
      "Batch 91, Loss: 0.997774, Accuracy: 65.30%\n",
      "Batch 92, Loss: 1.122910, Accuracy: 65.25%\n",
      "Batch 93, Loss: 1.083528, Accuracy: 65.24%\n",
      "Batch 94, Loss: 1.127277, Accuracy: 65.14%\n",
      "Batch 95, Loss: 1.037297, Accuracy: 65.18%\n",
      "Batch 96, Loss: 1.120252, Accuracy: 65.15%\n",
      "Batch 97, Loss: 0.960885, Accuracy: 65.30%\n",
      "Batch 98, Loss: 1.020901, Accuracy: 65.40%\n",
      "Batch 99, Loss: 1.088932, Accuracy: 65.42%\n",
      "Batch 100, Loss: 0.954642, Accuracy: 65.55%\n",
      "Batch 101, Loss: 1.056198, Accuracy: 65.58%\n",
      "Batch 102, Loss: 1.083006, Accuracy: 65.59%\n",
      "Batch 103, Loss: 1.106814, Accuracy: 65.58%\n",
      "Batch 104, Loss: 1.110232, Accuracy: 65.53%\n",
      "Batch 105, Loss: 0.944174, Accuracy: 65.67%\n",
      "Batch 106, Loss: 1.138344, Accuracy: 65.62%\n",
      "Batch 107, Loss: 1.147475, Accuracy: 65.57%\n",
      "Batch 108, Loss: 1.081997, Accuracy: 65.58%\n",
      "Batch 109, Loss: 1.080839, Accuracy: 65.60%\n",
      "Batch 110, Loss: 1.052822, Accuracy: 65.62%\n",
      "Batch 111, Loss: 1.088613, Accuracy: 65.61%\n",
      "Batch 112, Loss: 1.073611, Accuracy: 65.64%\n",
      "Batch 113, Loss: 1.143578, Accuracy: 65.60%\n",
      "Batch 114, Loss: 1.039917, Accuracy: 65.64%\n",
      "Batch 115, Loss: 1.022397, Accuracy: 65.69%\n",
      "Batch 116, Loss: 1.052112, Accuracy: 65.72%\n",
      "Batch 117, Loss: 1.154025, Accuracy: 65.65%\n",
      "Batch 118, Loss: 1.131570, Accuracy: 65.59%\n",
      "Batch 119, Loss: 1.002561, Accuracy: 65.69%\n",
      "Batch 120, Loss: 1.104913, Accuracy: 65.68%\n",
      "Batch 121, Loss: 0.982734, Accuracy: 65.77%\n",
      "Batch 122, Loss: 1.104043, Accuracy: 65.73%\n",
      "Batch 123, Loss: 1.160812, Accuracy: 65.64%\n",
      "Batch 124, Loss: 1.078335, Accuracy: 65.66%\n",
      "Batch 125, Loss: 1.078233, Accuracy: 65.67%\n",
      "Batch 126, Loss: 1.068403, Accuracy: 65.67%\n",
      "Batch 127, Loss: 1.092836, Accuracy: 65.65%\n",
      "Batch 128, Loss: 1.080460, Accuracy: 65.65%\n",
      "Batch 129, Loss: 1.101083, Accuracy: 65.62%\n",
      "Batch 130, Loss: 1.028396, Accuracy: 65.66%\n",
      "Batch 131, Loss: 0.992939, Accuracy: 65.76%\n",
      "Batch 132, Loss: 1.014395, Accuracy: 65.81%\n",
      "Batch 133, Loss: 1.019072, Accuracy: 65.86%\n",
      "Batch 134, Loss: 1.123280, Accuracy: 65.83%\n",
      "Batch 135, Loss: 1.025889, Accuracy: 65.89%\n",
      "Batch 136, Loss: 1.026803, Accuracy: 65.94%\n",
      "Batch 137, Loss: 1.086452, Accuracy: 65.94%\n",
      "Batch 138, Loss: 1.085384, Accuracy: 65.95%\n",
      "Batch 139, Loss: 1.032331, Accuracy: 66.00%\n",
      "Batch 140, Loss: 1.076733, Accuracy: 66.00%\n",
      "Batch 141, Loss: 1.100479, Accuracy: 65.99%\n",
      "Batch 142, Loss: 1.040963, Accuracy: 66.03%\n",
      "Batch 143, Loss: 1.040952, Accuracy: 66.07%\n",
      "Batch 144, Loss: 1.059178, Accuracy: 66.08%\n",
      "Batch 145, Loss: 1.151592, Accuracy: 66.01%\n",
      "Batch 146, Loss: 0.985125, Accuracy: 66.10%\n",
      "Batch 147, Loss: 0.983441, Accuracy: 66.18%\n",
      "Batch 148, Loss: 1.059905, Accuracy: 66.16%\n",
      "Batch 149, Loss: 1.125383, Accuracy: 66.13%\n",
      "Batch 150, Loss: 1.107867, Accuracy: 66.10%\n",
      "Batch 151, Loss: 1.043308, Accuracy: 66.13%\n",
      "Batch 152, Loss: 1.027306, Accuracy: 66.16%\n",
      "Batch 153, Loss: 1.077085, Accuracy: 66.17%\n",
      "Batch 154, Loss: 1.135544, Accuracy: 66.12%\n",
      "Batch 155, Loss: 1.060417, Accuracy: 66.14%\n",
      "Batch 156, Loss: 1.087175, Accuracy: 66.15%\n",
      "Batch 157, Loss: 1.026910, Accuracy: 66.18%\n",
      "Batch 158, Loss: 1.004239, Accuracy: 66.24%\n",
      "Batch 159, Loss: 1.010318, Accuracy: 66.29%\n",
      "Batch 160, Loss: 1.126754, Accuracy: 66.27%\n",
      "Batch 161, Loss: 1.115045, Accuracy: 66.27%\n",
      "Batch 162, Loss: 1.099336, Accuracy: 66.25%\n",
      "Batch 163, Loss: 1.133381, Accuracy: 66.19%\n",
      "Batch 164, Loss: 1.062921, Accuracy: 66.21%\n",
      "Batch 165, Loss: 1.144747, Accuracy: 66.16%\n",
      "Batch 166, Loss: 1.044748, Accuracy: 66.19%\n",
      "Batch 167, Loss: 1.127822, Accuracy: 66.16%\n",
      "Batch 168, Loss: 1.011627, Accuracy: 66.19%\n",
      "Batch 169, Loss: 1.079406, Accuracy: 66.20%\n",
      "Batch 170, Loss: 1.020198, Accuracy: 66.23%\n",
      "Batch 171, Loss: 1.005085, Accuracy: 66.29%\n",
      "Batch 172, Loss: 1.072026, Accuracy: 66.30%\n",
      "Batch 173, Loss: 1.099107, Accuracy: 66.28%\n",
      "Batch 174, Loss: 1.089195, Accuracy: 66.25%\n",
      "Batch 175, Loss: 0.979576, Accuracy: 66.31%\n",
      "Batch 176, Loss: 1.028167, Accuracy: 66.33%\n",
      "Batch 177, Loss: 1.206260, Accuracy: 66.23%\n",
      "Batch 178, Loss: 1.114797, Accuracy: 66.21%\n",
      "Batch 179, Loss: 1.026841, Accuracy: 66.23%\n",
      "Batch 180, Loss: 1.027854, Accuracy: 66.26%\n",
      "Batch 181, Loss: 1.034502, Accuracy: 66.28%\n",
      "Batch 182, Loss: 1.101931, Accuracy: 66.25%\n",
      "Batch 183, Loss: 1.038144, Accuracy: 66.28%\n",
      "Batch 184, Loss: 1.141806, Accuracy: 66.25%\n",
      "Batch 185, Loss: 1.082958, Accuracy: 66.24%\n",
      "Batch 186, Loss: 1.064593, Accuracy: 66.26%\n",
      "Batch 187, Loss: 1.048427, Accuracy: 66.28%\n",
      "Batch 188, Loss: 1.043990, Accuracy: 66.30%\n",
      "Batch 189, Loss: 1.113222, Accuracy: 66.29%\n",
      "Batch 190, Loss: 1.058187, Accuracy: 66.30%\n",
      "Batch 191, Loss: 1.049558, Accuracy: 66.30%\n",
      "Batch 192, Loss: 1.054305, Accuracy: 66.32%\n",
      "Batch 193, Loss: 1.128369, Accuracy: 66.27%\n",
      "Batch 194, Loss: 1.048298, Accuracy: 66.29%\n",
      "Batch 195, Loss: 1.107927, Accuracy: 66.27%\n",
      "Batch 196, Loss: 1.018102, Accuracy: 66.32%\n",
      "Batch 197, Loss: 0.984478, Accuracy: 66.37%\n",
      "Batch 198, Loss: 1.081297, Accuracy: 66.37%\n",
      "Batch 199, Loss: 1.080562, Accuracy: 66.36%\n",
      "Batch 200, Loss: 1.118150, Accuracy: 66.33%\n",
      "Batch 201, Loss: 1.077225, Accuracy: 66.31%\n",
      "Batch 202, Loss: 1.047468, Accuracy: 66.32%\n",
      "Batch 203, Loss: 1.017787, Accuracy: 66.35%\n",
      "Batch 204, Loss: 1.080629, Accuracy: 66.34%\n",
      "Batch 205, Loss: 1.133524, Accuracy: 66.31%\n",
      "Batch 206, Loss: 1.089931, Accuracy: 66.30%\n",
      "Batch 207, Loss: 1.091272, Accuracy: 66.30%\n",
      "Batch 208, Loss: 1.079365, Accuracy: 66.28%\n",
      "Batch 209, Loss: 1.056395, Accuracy: 66.31%\n",
      "Batch 210, Loss: 1.077178, Accuracy: 66.30%\n",
      "Batch 211, Loss: 1.116069, Accuracy: 66.28%\n",
      "Batch 212, Loss: 1.128934, Accuracy: 66.26%\n",
      "Batch 213, Loss: 1.115616, Accuracy: 66.24%\n",
      "Training - Epoch 9, Loss: 1.076726, Accuracy: 66.24%\n",
      "Validation Batch 1, Loss: 1.210472, Accuracy: 50.00%\n",
      "Validation Batch 2, Loss: 1.259705, Accuracy: 47.66%\n",
      "Validation Batch 3, Loss: 1.329972, Accuracy: 43.23%\n",
      "Validation Batch 4, Loss: 1.216875, Accuracy: 44.53%\n",
      "Validation Batch 5, Loss: 1.305227, Accuracy: 43.75%\n",
      "Validation Batch 6, Loss: 1.279882, Accuracy: 43.49%\n",
      "Validation Batch 7, Loss: 1.238472, Accuracy: 43.97%\n",
      "Validation Batch 8, Loss: 1.259169, Accuracy: 44.14%\n",
      "Validation Batch 9, Loss: 1.282793, Accuracy: 43.75%\n",
      "Validation Batch 10, Loss: 1.280213, Accuracy: 43.91%\n",
      "Validation Batch 11, Loss: 1.255837, Accuracy: 43.89%\n",
      "Validation Batch 12, Loss: 1.181410, Accuracy: 44.53%\n",
      "Validation Batch 13, Loss: 1.272752, Accuracy: 44.35%\n",
      "Validation Batch 14, Loss: 1.179132, Accuracy: 45.09%\n",
      "Validation Batch 15, Loss: 1.246071, Accuracy: 45.21%\n",
      "Validation Batch 16, Loss: 1.306485, Accuracy: 44.73%\n",
      "Validation Batch 17, Loss: 1.327099, Accuracy: 44.30%\n",
      "Validation Batch 18, Loss: 1.252353, Accuracy: 44.36%\n",
      "Validation Batch 19, Loss: 1.288171, Accuracy: 44.16%\n",
      "Validation Batch 20, Loss: 1.246222, Accuracy: 44.14%\n",
      "Validation Batch 21, Loss: 1.273965, Accuracy: 44.12%\n",
      "Validation Batch 22, Loss: 1.259139, Accuracy: 44.25%\n",
      "Validation Batch 23, Loss: 1.303534, Accuracy: 44.09%\n",
      "Validation Batch 24, Loss: 1.279420, Accuracy: 44.01%\n",
      "Validation Batch 25, Loss: 1.212922, Accuracy: 44.31%\n",
      "Validation Batch 26, Loss: 1.177991, Accuracy: 44.71%\n",
      "Validation Batch 27, Loss: 1.315530, Accuracy: 44.57%\n",
      "Validation - Epoch 9, Loss: 1.260771, Accuracy: 44.57%\n",
      "Patience—0\n",
      "Epoch 10\n",
      "Batch 1, Loss: 1.005884, Accuracy: 76.56%\n",
      "Batch 2, Loss: 1.145154, Accuracy: 67.19%\n",
      "Batch 3, Loss: 0.985604, Accuracy: 70.31%\n",
      "Batch 4, Loss: 1.181822, Accuracy: 65.62%\n",
      "Batch 5, Loss: 1.066844, Accuracy: 66.25%\n",
      "Batch 6, Loss: 1.036944, Accuracy: 66.41%\n",
      "Batch 7, Loss: 1.156740, Accuracy: 64.51%\n",
      "Batch 8, Loss: 1.096480, Accuracy: 64.06%\n",
      "Batch 9, Loss: 1.093750, Accuracy: 64.41%\n",
      "Batch 10, Loss: 1.117028, Accuracy: 64.69%\n",
      "Batch 11, Loss: 1.075051, Accuracy: 64.77%\n",
      "Batch 12, Loss: 1.132748, Accuracy: 64.19%\n",
      "Batch 13, Loss: 1.045960, Accuracy: 64.78%\n",
      "Batch 14, Loss: 1.033609, Accuracy: 65.40%\n",
      "Batch 15, Loss: 1.175860, Accuracy: 64.48%\n",
      "Batch 16, Loss: 1.094227, Accuracy: 64.45%\n",
      "Batch 17, Loss: 1.054696, Accuracy: 64.61%\n",
      "Batch 18, Loss: 1.018340, Accuracy: 65.19%\n",
      "Batch 19, Loss: 1.021488, Accuracy: 65.62%\n",
      "Batch 20, Loss: 1.050757, Accuracy: 65.94%\n",
      "Batch 21, Loss: 1.161778, Accuracy: 65.62%\n",
      "Batch 22, Loss: 1.029468, Accuracy: 65.84%\n",
      "Batch 23, Loss: 1.062305, Accuracy: 65.96%\n",
      "Batch 24, Loss: 1.132019, Accuracy: 65.69%\n",
      "Batch 25, Loss: 1.076592, Accuracy: 65.69%\n",
      "Batch 26, Loss: 1.093269, Accuracy: 65.69%\n",
      "Batch 27, Loss: 1.063934, Accuracy: 65.86%\n",
      "Batch 28, Loss: 1.061290, Accuracy: 66.02%\n",
      "Batch 29, Loss: 0.939868, Accuracy: 66.59%\n",
      "Batch 30, Loss: 1.064745, Accuracy: 66.67%\n",
      "Batch 31, Loss: 1.079671, Accuracy: 66.73%\n",
      "Batch 32, Loss: 1.042957, Accuracy: 66.85%\n",
      "Batch 33, Loss: 1.071602, Accuracy: 66.86%\n",
      "Batch 34, Loss: 1.097914, Accuracy: 66.77%\n",
      "Batch 35, Loss: 1.058536, Accuracy: 66.88%\n",
      "Batch 36, Loss: 1.030927, Accuracy: 66.97%\n",
      "Batch 37, Loss: 1.038756, Accuracy: 67.15%\n",
      "Batch 38, Loss: 1.056904, Accuracy: 67.23%\n",
      "Batch 39, Loss: 1.118419, Accuracy: 67.07%\n",
      "Batch 40, Loss: 1.102943, Accuracy: 66.95%\n",
      "Batch 41, Loss: 1.127971, Accuracy: 66.69%\n",
      "Batch 42, Loss: 1.061838, Accuracy: 66.70%\n",
      "Batch 43, Loss: 1.083891, Accuracy: 66.75%\n",
      "Batch 44, Loss: 1.137646, Accuracy: 66.62%\n",
      "Batch 45, Loss: 1.057958, Accuracy: 66.63%\n",
      "Batch 46, Loss: 1.142657, Accuracy: 66.44%\n",
      "Batch 47, Loss: 1.013535, Accuracy: 66.52%\n",
      "Batch 48, Loss: 1.013738, Accuracy: 66.60%\n",
      "Batch 49, Loss: 1.132799, Accuracy: 66.49%\n",
      "Batch 50, Loss: 1.010086, Accuracy: 66.66%\n",
      "Batch 51, Loss: 1.067535, Accuracy: 66.67%\n",
      "Batch 52, Loss: 1.050155, Accuracy: 66.80%\n",
      "Batch 53, Loss: 1.064250, Accuracy: 66.86%\n",
      "Batch 54, Loss: 1.140043, Accuracy: 66.70%\n",
      "Batch 55, Loss: 1.010386, Accuracy: 66.82%\n",
      "Batch 56, Loss: 1.096393, Accuracy: 66.80%\n",
      "Batch 57, Loss: 1.084542, Accuracy: 66.78%\n",
      "Batch 58, Loss: 1.039504, Accuracy: 66.89%\n",
      "Batch 59, Loss: 1.045512, Accuracy: 66.92%\n",
      "Batch 60, Loss: 1.033016, Accuracy: 67.01%\n",
      "Batch 61, Loss: 1.074573, Accuracy: 67.01%\n",
      "Batch 62, Loss: 1.089603, Accuracy: 66.99%\n",
      "Batch 63, Loss: 1.116672, Accuracy: 66.91%\n",
      "Batch 64, Loss: 1.066400, Accuracy: 66.87%\n",
      "Batch 65, Loss: 1.063919, Accuracy: 66.92%\n",
      "Batch 66, Loss: 1.007010, Accuracy: 67.00%\n",
      "Batch 67, Loss: 1.107253, Accuracy: 66.98%\n",
      "Batch 68, Loss: 1.004504, Accuracy: 67.07%\n",
      "Batch 69, Loss: 1.081120, Accuracy: 67.07%\n",
      "Batch 70, Loss: 1.091346, Accuracy: 67.10%\n",
      "Batch 71, Loss: 1.071217, Accuracy: 67.08%\n",
      "Batch 72, Loss: 1.142076, Accuracy: 66.93%\n",
      "Batch 73, Loss: 1.082524, Accuracy: 66.89%\n",
      "Batch 74, Loss: 1.050794, Accuracy: 66.89%\n",
      "Batch 75, Loss: 1.141122, Accuracy: 66.77%\n",
      "Batch 76, Loss: 1.160566, Accuracy: 66.61%\n",
      "Batch 77, Loss: 1.095868, Accuracy: 66.58%\n",
      "Batch 78, Loss: 1.149212, Accuracy: 66.41%\n",
      "Batch 79, Loss: 1.066834, Accuracy: 66.42%\n",
      "Batch 80, Loss: 1.002445, Accuracy: 66.50%\n",
      "Batch 81, Loss: 1.013671, Accuracy: 66.59%\n",
      "Batch 82, Loss: 0.991111, Accuracy: 66.71%\n",
      "Batch 83, Loss: 1.154783, Accuracy: 66.60%\n",
      "Batch 84, Loss: 1.006408, Accuracy: 66.69%\n",
      "Batch 85, Loss: 0.982310, Accuracy: 66.82%\n",
      "Batch 86, Loss: 1.011738, Accuracy: 66.90%\n",
      "Batch 87, Loss: 1.050689, Accuracy: 66.92%\n",
      "Batch 88, Loss: 1.078189, Accuracy: 66.94%\n",
      "Batch 89, Loss: 1.036353, Accuracy: 66.98%\n",
      "Batch 90, Loss: 1.142165, Accuracy: 66.93%\n",
      "Batch 91, Loss: 1.077766, Accuracy: 66.91%\n",
      "Batch 92, Loss: 1.063235, Accuracy: 66.92%\n",
      "Batch 93, Loss: 1.013834, Accuracy: 66.99%\n",
      "Batch 94, Loss: 1.098187, Accuracy: 66.94%\n",
      "Batch 95, Loss: 1.046613, Accuracy: 66.92%\n",
      "Batch 96, Loss: 0.991969, Accuracy: 67.01%\n",
      "Batch 97, Loss: 1.078583, Accuracy: 66.99%\n",
      "Batch 98, Loss: 1.012168, Accuracy: 67.01%\n",
      "Batch 99, Loss: 1.105490, Accuracy: 66.95%\n",
      "Batch 100, Loss: 1.059601, Accuracy: 66.95%\n",
      "Batch 101, Loss: 1.102248, Accuracy: 66.88%\n",
      "Batch 102, Loss: 1.094905, Accuracy: 66.84%\n",
      "Batch 103, Loss: 1.090771, Accuracy: 66.82%\n",
      "Batch 104, Loss: 1.010682, Accuracy: 66.90%\n",
      "Batch 105, Loss: 1.007989, Accuracy: 66.95%\n",
      "Batch 106, Loss: 1.105668, Accuracy: 66.89%\n",
      "Batch 107, Loss: 1.146200, Accuracy: 66.82%\n",
      "Batch 108, Loss: 1.147536, Accuracy: 66.72%\n",
      "Batch 109, Loss: 1.125267, Accuracy: 66.67%\n",
      "Batch 110, Loss: 1.065423, Accuracy: 66.66%\n",
      "Batch 111, Loss: 1.183590, Accuracy: 66.51%\n",
      "Batch 112, Loss: 0.985486, Accuracy: 66.62%\n",
      "Batch 113, Loss: 1.026556, Accuracy: 66.68%\n",
      "Batch 114, Loss: 1.117092, Accuracy: 66.63%\n",
      "Batch 115, Loss: 1.083743, Accuracy: 66.63%\n",
      "Batch 116, Loss: 1.146556, Accuracy: 66.57%\n",
      "Batch 117, Loss: 1.045724, Accuracy: 66.63%\n",
      "Batch 118, Loss: 1.138420, Accuracy: 66.55%\n",
      "Batch 119, Loss: 1.052623, Accuracy: 66.56%\n",
      "Batch 120, Loss: 1.032508, Accuracy: 66.60%\n",
      "Batch 121, Loss: 1.051521, Accuracy: 66.62%\n",
      "Batch 122, Loss: 1.040754, Accuracy: 66.65%\n",
      "Batch 123, Loss: 1.113567, Accuracy: 66.63%\n",
      "Batch 124, Loss: 1.098161, Accuracy: 66.61%\n",
      "Batch 125, Loss: 1.047634, Accuracy: 66.61%\n",
      "Batch 126, Loss: 0.997978, Accuracy: 66.69%\n",
      "Batch 127, Loss: 1.049266, Accuracy: 66.72%\n",
      "Batch 128, Loss: 1.154481, Accuracy: 66.64%\n",
      "Batch 129, Loss: 1.080802, Accuracy: 66.64%\n",
      "Batch 130, Loss: 1.076353, Accuracy: 66.66%\n",
      "Batch 131, Loss: 0.953072, Accuracy: 66.78%\n",
      "Batch 132, Loss: 1.106199, Accuracy: 66.76%\n",
      "Batch 133, Loss: 1.022133, Accuracy: 66.80%\n",
      "Batch 134, Loss: 0.928992, Accuracy: 66.93%\n",
      "Batch 135, Loss: 1.000295, Accuracy: 66.99%\n",
      "Batch 136, Loss: 1.093202, Accuracy: 66.96%\n",
      "Batch 137, Loss: 1.105215, Accuracy: 66.94%\n",
      "Batch 138, Loss: 1.098445, Accuracy: 66.90%\n",
      "Batch 139, Loss: 1.014183, Accuracy: 66.94%\n",
      "Batch 140, Loss: 1.013577, Accuracy: 66.99%\n",
      "Batch 141, Loss: 1.018249, Accuracy: 67.02%\n",
      "Batch 142, Loss: 1.071998, Accuracy: 67.01%\n",
      "Batch 143, Loss: 1.099217, Accuracy: 66.98%\n",
      "Batch 144, Loss: 1.171866, Accuracy: 66.87%\n",
      "Batch 145, Loss: 1.103547, Accuracy: 66.84%\n",
      "Batch 146, Loss: 1.132346, Accuracy: 66.80%\n",
      "Batch 147, Loss: 1.034391, Accuracy: 66.82%\n",
      "Batch 148, Loss: 1.043052, Accuracy: 66.84%\n",
      "Batch 149, Loss: 1.106352, Accuracy: 66.81%\n",
      "Batch 150, Loss: 1.037544, Accuracy: 66.84%\n",
      "Batch 151, Loss: 1.048790, Accuracy: 66.87%\n",
      "Batch 152, Loss: 1.066479, Accuracy: 66.88%\n",
      "Batch 153, Loss: 1.006733, Accuracy: 66.95%\n",
      "Batch 154, Loss: 1.157914, Accuracy: 66.87%\n",
      "Batch 155, Loss: 1.079826, Accuracy: 66.84%\n",
      "Batch 156, Loss: 0.947951, Accuracy: 66.94%\n",
      "Batch 157, Loss: 1.040123, Accuracy: 66.98%\n",
      "Batch 158, Loss: 1.015108, Accuracy: 67.02%\n",
      "Batch 159, Loss: 1.062059, Accuracy: 67.02%\n",
      "Batch 160, Loss: 1.154345, Accuracy: 66.96%\n",
      "Batch 161, Loss: 1.119259, Accuracy: 66.93%\n",
      "Batch 162, Loss: 1.056682, Accuracy: 66.95%\n",
      "Batch 163, Loss: 1.114244, Accuracy: 66.90%\n",
      "Batch 164, Loss: 1.118270, Accuracy: 66.86%\n",
      "Batch 165, Loss: 1.173036, Accuracy: 66.80%\n",
      "Batch 166, Loss: 1.069544, Accuracy: 66.79%\n",
      "Batch 167, Loss: 1.070722, Accuracy: 66.79%\n",
      "Batch 168, Loss: 1.022303, Accuracy: 66.82%\n",
      "Batch 169, Loss: 0.998731, Accuracy: 66.87%\n",
      "Batch 170, Loss: 1.021250, Accuracy: 66.89%\n",
      "Batch 171, Loss: 1.007273, Accuracy: 66.92%\n",
      "Batch 172, Loss: 1.036569, Accuracy: 66.96%\n",
      "Batch 173, Loss: 1.083902, Accuracy: 66.95%\n",
      "Batch 174, Loss: 1.031283, Accuracy: 66.96%\n",
      "Batch 175, Loss: 1.050401, Accuracy: 66.98%\n",
      "Batch 176, Loss: 1.013092, Accuracy: 67.02%\n",
      "Batch 177, Loss: 1.108229, Accuracy: 67.02%\n",
      "Batch 178, Loss: 1.023710, Accuracy: 67.06%\n",
      "Batch 179, Loss: 1.044832, Accuracy: 67.08%\n",
      "Batch 180, Loss: 1.133489, Accuracy: 67.03%\n",
      "Batch 181, Loss: 1.066382, Accuracy: 67.03%\n",
      "Batch 182, Loss: 1.021501, Accuracy: 67.06%\n",
      "Batch 183, Loss: 1.015495, Accuracy: 67.09%\n",
      "Batch 184, Loss: 1.059172, Accuracy: 67.09%\n",
      "Batch 185, Loss: 1.087253, Accuracy: 67.08%\n",
      "Batch 186, Loss: 0.968047, Accuracy: 67.15%\n",
      "Batch 187, Loss: 1.054112, Accuracy: 67.17%\n",
      "Batch 188, Loss: 1.016735, Accuracy: 67.20%\n",
      "Batch 189, Loss: 1.046464, Accuracy: 67.21%\n",
      "Batch 190, Loss: 1.135636, Accuracy: 67.19%\n",
      "Batch 191, Loss: 1.134242, Accuracy: 67.15%\n",
      "Batch 192, Loss: 0.995681, Accuracy: 67.20%\n",
      "Batch 193, Loss: 1.046824, Accuracy: 67.23%\n",
      "Batch 194, Loss: 1.027698, Accuracy: 67.27%\n",
      "Batch 195, Loss: 1.048988, Accuracy: 67.29%\n",
      "Batch 196, Loss: 1.124014, Accuracy: 67.27%\n",
      "Batch 197, Loss: 1.134288, Accuracy: 67.24%\n",
      "Batch 198, Loss: 0.982877, Accuracy: 67.29%\n",
      "Batch 199, Loss: 1.159140, Accuracy: 67.23%\n",
      "Batch 200, Loss: 1.047666, Accuracy: 67.24%\n",
      "Batch 201, Loss: 1.026106, Accuracy: 67.27%\n",
      "Batch 202, Loss: 1.035773, Accuracy: 67.28%\n",
      "Batch 203, Loss: 1.018025, Accuracy: 67.31%\n",
      "Batch 204, Loss: 1.047805, Accuracy: 67.34%\n",
      "Batch 205, Loss: 1.028091, Accuracy: 67.35%\n",
      "Batch 206, Loss: 1.066977, Accuracy: 67.35%\n",
      "Batch 207, Loss: 1.066697, Accuracy: 67.35%\n",
      "Batch 208, Loss: 1.006990, Accuracy: 67.38%\n",
      "Batch 209, Loss: 1.020109, Accuracy: 67.41%\n",
      "Batch 210, Loss: 1.047100, Accuracy: 67.43%\n",
      "Batch 211, Loss: 1.084148, Accuracy: 67.42%\n",
      "Batch 212, Loss: 1.101731, Accuracy: 67.42%\n",
      "Batch 213, Loss: 1.071494, Accuracy: 67.41%\n",
      "Training - Epoch 10, Loss: 1.067179, Accuracy: 67.41%\n",
      "Validation Batch 1, Loss: 1.208340, Accuracy: 50.00%\n",
      "Validation Batch 2, Loss: 1.259440, Accuracy: 47.66%\n",
      "Validation Batch 3, Loss: 1.323829, Accuracy: 43.75%\n",
      "Validation Batch 4, Loss: 1.208413, Accuracy: 44.53%\n",
      "Validation Batch 5, Loss: 1.296136, Accuracy: 43.75%\n",
      "Validation Batch 6, Loss: 1.271701, Accuracy: 43.49%\n",
      "Validation Batch 7, Loss: 1.236782, Accuracy: 43.97%\n",
      "Validation Batch 8, Loss: 1.254589, Accuracy: 43.95%\n",
      "Validation Batch 9, Loss: 1.275937, Accuracy: 43.92%\n",
      "Validation Batch 10, Loss: 1.275752, Accuracy: 43.91%\n",
      "Validation Batch 11, Loss: 1.250370, Accuracy: 43.75%\n",
      "Validation Batch 12, Loss: 1.175697, Accuracy: 44.40%\n",
      "Validation Batch 13, Loss: 1.268643, Accuracy: 44.35%\n",
      "Validation Batch 14, Loss: 1.177636, Accuracy: 45.09%\n",
      "Validation Batch 15, Loss: 1.240445, Accuracy: 45.21%\n",
      "Validation Batch 16, Loss: 1.302251, Accuracy: 44.73%\n",
      "Validation Batch 17, Loss: 1.328246, Accuracy: 44.39%\n",
      "Validation Batch 18, Loss: 1.250899, Accuracy: 44.53%\n",
      "Validation Batch 19, Loss: 1.287574, Accuracy: 44.33%\n",
      "Validation Batch 20, Loss: 1.238684, Accuracy: 44.45%\n",
      "Validation Batch 21, Loss: 1.268953, Accuracy: 44.42%\n",
      "Validation Batch 22, Loss: 1.260473, Accuracy: 44.53%\n",
      "Validation Batch 23, Loss: 1.301472, Accuracy: 44.36%\n",
      "Validation Batch 24, Loss: 1.268756, Accuracy: 44.34%\n",
      "Validation Batch 25, Loss: 1.207857, Accuracy: 44.62%\n",
      "Validation Batch 26, Loss: 1.176020, Accuracy: 44.95%\n",
      "Validation Batch 27, Loss: 1.313121, Accuracy: 44.80%\n",
      "Validation - Epoch 10, Loss: 1.256593, Accuracy: 44.80%\n",
      "Patience—0\n",
      "Epoch 11\n",
      "Batch 1, Loss: 1.148197, Accuracy: 59.38%\n",
      "Batch 2, Loss: 1.104238, Accuracy: 61.72%\n",
      "Batch 3, Loss: 1.159637, Accuracy: 59.90%\n",
      "Batch 4, Loss: 1.031456, Accuracy: 62.11%\n",
      "Batch 5, Loss: 1.056608, Accuracy: 63.44%\n",
      "Batch 6, Loss: 1.039728, Accuracy: 65.10%\n",
      "Batch 7, Loss: 1.057620, Accuracy: 65.62%\n",
      "Batch 8, Loss: 1.033970, Accuracy: 66.21%\n",
      "Batch 9, Loss: 1.096687, Accuracy: 66.15%\n",
      "Batch 10, Loss: 0.969418, Accuracy: 67.34%\n",
      "Batch 11, Loss: 1.066528, Accuracy: 67.47%\n",
      "Batch 12, Loss: 1.038399, Accuracy: 67.84%\n",
      "Batch 13, Loss: 1.032702, Accuracy: 68.15%\n",
      "Batch 14, Loss: 1.021962, Accuracy: 68.53%\n",
      "Batch 15, Loss: 1.113648, Accuracy: 68.12%\n",
      "Batch 16, Loss: 1.080405, Accuracy: 68.07%\n",
      "Batch 17, Loss: 1.113282, Accuracy: 67.74%\n",
      "Batch 18, Loss: 1.008751, Accuracy: 67.97%\n",
      "Batch 19, Loss: 1.028149, Accuracy: 68.01%\n",
      "Batch 20, Loss: 1.038452, Accuracy: 68.12%\n",
      "Batch 21, Loss: 0.960240, Accuracy: 68.60%\n",
      "Batch 22, Loss: 1.141892, Accuracy: 68.32%\n",
      "Batch 23, Loss: 1.068046, Accuracy: 68.27%\n",
      "Batch 24, Loss: 1.033742, Accuracy: 68.42%\n",
      "Batch 25, Loss: 1.060051, Accuracy: 68.38%\n",
      "Batch 26, Loss: 1.070574, Accuracy: 68.33%\n",
      "Batch 27, Loss: 0.979108, Accuracy: 68.63%\n",
      "Batch 28, Loss: 1.102866, Accuracy: 68.42%\n",
      "Batch 29, Loss: 1.020894, Accuracy: 68.59%\n",
      "Batch 30, Loss: 1.021907, Accuracy: 68.75%\n",
      "Batch 31, Loss: 1.084166, Accuracy: 68.65%\n",
      "Batch 32, Loss: 1.049980, Accuracy: 68.65%\n",
      "Batch 33, Loss: 1.070945, Accuracy: 68.61%\n",
      "Batch 34, Loss: 1.057272, Accuracy: 68.70%\n",
      "Batch 35, Loss: 1.093204, Accuracy: 68.57%\n",
      "Batch 36, Loss: 0.969786, Accuracy: 68.79%\n",
      "Batch 37, Loss: 0.985484, Accuracy: 69.00%\n",
      "Batch 38, Loss: 1.013743, Accuracy: 69.12%\n",
      "Batch 39, Loss: 1.055333, Accuracy: 69.11%\n",
      "Batch 40, Loss: 1.016955, Accuracy: 69.22%\n",
      "Batch 41, Loss: 1.128466, Accuracy: 68.98%\n",
      "Batch 42, Loss: 1.141331, Accuracy: 68.79%\n",
      "Batch 43, Loss: 1.037862, Accuracy: 68.86%\n",
      "Batch 44, Loss: 1.020911, Accuracy: 68.96%\n",
      "Batch 45, Loss: 1.046584, Accuracy: 69.03%\n",
      "Batch 46, Loss: 1.023656, Accuracy: 69.09%\n",
      "Batch 47, Loss: 1.042524, Accuracy: 69.15%\n",
      "Batch 48, Loss: 1.087746, Accuracy: 69.14%\n",
      "Batch 49, Loss: 1.095908, Accuracy: 69.01%\n",
      "Batch 50, Loss: 1.082631, Accuracy: 68.94%\n",
      "Batch 51, Loss: 1.076216, Accuracy: 68.90%\n",
      "Batch 52, Loss: 1.113560, Accuracy: 68.69%\n",
      "Batch 53, Loss: 0.992542, Accuracy: 68.81%\n",
      "Batch 54, Loss: 0.990885, Accuracy: 68.92%\n",
      "Batch 55, Loss: 0.945800, Accuracy: 69.18%\n",
      "Batch 56, Loss: 1.124329, Accuracy: 69.03%\n",
      "Batch 57, Loss: 0.975091, Accuracy: 69.19%\n",
      "Batch 58, Loss: 1.081223, Accuracy: 69.10%\n",
      "Batch 59, Loss: 1.073136, Accuracy: 69.01%\n",
      "Batch 60, Loss: 1.038491, Accuracy: 68.96%\n",
      "Batch 61, Loss: 1.061897, Accuracy: 68.95%\n",
      "Batch 62, Loss: 1.121873, Accuracy: 68.80%\n",
      "Batch 63, Loss: 1.054807, Accuracy: 68.77%\n",
      "Batch 64, Loss: 1.050628, Accuracy: 68.80%\n",
      "Batch 65, Loss: 0.988652, Accuracy: 68.92%\n",
      "Batch 66, Loss: 1.057822, Accuracy: 68.92%\n",
      "Batch 67, Loss: 1.034308, Accuracy: 68.94%\n",
      "Batch 68, Loss: 1.097893, Accuracy: 68.89%\n",
      "Batch 69, Loss: 1.013668, Accuracy: 68.98%\n",
      "Batch 70, Loss: 1.036447, Accuracy: 68.97%\n",
      "Batch 71, Loss: 1.046931, Accuracy: 68.97%\n",
      "Batch 72, Loss: 1.084451, Accuracy: 68.92%\n",
      "Batch 73, Loss: 1.046071, Accuracy: 68.96%\n",
      "Batch 74, Loss: 1.033730, Accuracy: 68.98%\n",
      "Batch 75, Loss: 1.105659, Accuracy: 68.90%\n",
      "Batch 76, Loss: 1.125139, Accuracy: 68.79%\n",
      "Batch 77, Loss: 1.181421, Accuracy: 68.61%\n",
      "Batch 78, Loss: 1.127636, Accuracy: 68.51%\n",
      "Batch 79, Loss: 1.081860, Accuracy: 68.47%\n",
      "Batch 80, Loss: 1.017065, Accuracy: 68.52%\n",
      "Batch 81, Loss: 1.110777, Accuracy: 68.44%\n",
      "Batch 82, Loss: 1.115394, Accuracy: 68.35%\n",
      "Batch 83, Loss: 1.019753, Accuracy: 68.43%\n",
      "Batch 84, Loss: 1.102570, Accuracy: 68.38%\n",
      "Batch 85, Loss: 1.125295, Accuracy: 68.27%\n",
      "Batch 86, Loss: 1.090569, Accuracy: 68.20%\n",
      "Batch 87, Loss: 1.061612, Accuracy: 68.21%\n",
      "Batch 88, Loss: 1.045283, Accuracy: 68.20%\n",
      "Batch 89, Loss: 1.112810, Accuracy: 68.15%\n",
      "Batch 90, Loss: 0.938929, Accuracy: 68.32%\n",
      "Batch 91, Loss: 1.004993, Accuracy: 68.34%\n",
      "Batch 92, Loss: 1.048787, Accuracy: 68.36%\n",
      "Batch 93, Loss: 1.123770, Accuracy: 68.30%\n",
      "Batch 94, Loss: 1.091359, Accuracy: 68.25%\n",
      "Batch 95, Loss: 1.127401, Accuracy: 68.17%\n",
      "Batch 96, Loss: 1.012191, Accuracy: 68.21%\n",
      "Batch 97, Loss: 1.170434, Accuracy: 68.11%\n",
      "Batch 98, Loss: 1.083838, Accuracy: 68.10%\n",
      "Batch 99, Loss: 1.002979, Accuracy: 68.15%\n",
      "Batch 100, Loss: 0.995954, Accuracy: 68.22%\n",
      "Batch 101, Loss: 1.130698, Accuracy: 68.15%\n",
      "Batch 102, Loss: 1.100318, Accuracy: 68.09%\n",
      "Batch 103, Loss: 1.076486, Accuracy: 68.05%\n",
      "Batch 104, Loss: 0.995053, Accuracy: 68.15%\n",
      "Batch 105, Loss: 1.074373, Accuracy: 68.11%\n",
      "Batch 106, Loss: 1.045053, Accuracy: 68.13%\n",
      "Batch 107, Loss: 1.024771, Accuracy: 68.20%\n",
      "Batch 108, Loss: 1.035238, Accuracy: 68.24%\n",
      "Batch 109, Loss: 1.085950, Accuracy: 68.19%\n",
      "Batch 110, Loss: 1.106709, Accuracy: 68.11%\n",
      "Batch 111, Loss: 1.078132, Accuracy: 68.10%\n",
      "Batch 112, Loss: 1.053987, Accuracy: 68.12%\n",
      "Batch 113, Loss: 1.057669, Accuracy: 68.10%\n",
      "Batch 114, Loss: 1.027429, Accuracy: 68.12%\n",
      "Batch 115, Loss: 1.042800, Accuracy: 68.11%\n",
      "Batch 116, Loss: 1.033963, Accuracy: 68.10%\n",
      "Batch 117, Loss: 1.106488, Accuracy: 68.10%\n",
      "Batch 118, Loss: 1.080661, Accuracy: 68.07%\n",
      "Batch 119, Loss: 1.082802, Accuracy: 68.05%\n",
      "Batch 120, Loss: 1.025237, Accuracy: 68.07%\n",
      "Batch 121, Loss: 1.079441, Accuracy: 68.05%\n",
      "Batch 122, Loss: 1.046199, Accuracy: 68.07%\n",
      "Batch 123, Loss: 1.010004, Accuracy: 68.14%\n",
      "Batch 124, Loss: 1.024675, Accuracy: 68.16%\n",
      "Batch 125, Loss: 1.011729, Accuracy: 68.19%\n",
      "Batch 126, Loss: 1.117446, Accuracy: 68.14%\n",
      "Batch 127, Loss: 1.067121, Accuracy: 68.13%\n",
      "Batch 128, Loss: 1.056573, Accuracy: 68.15%\n",
      "Batch 129, Loss: 1.036813, Accuracy: 68.17%\n",
      "Batch 130, Loss: 1.126001, Accuracy: 68.10%\n",
      "Batch 131, Loss: 1.122419, Accuracy: 68.05%\n",
      "Batch 132, Loss: 1.091977, Accuracy: 68.04%\n",
      "Batch 133, Loss: 1.027605, Accuracy: 68.07%\n",
      "Batch 134, Loss: 1.102700, Accuracy: 68.05%\n",
      "Batch 135, Loss: 1.096973, Accuracy: 68.02%\n",
      "Batch 136, Loss: 1.043896, Accuracy: 68.04%\n",
      "Batch 137, Loss: 0.993025, Accuracy: 68.08%\n",
      "Batch 138, Loss: 1.032783, Accuracy: 68.09%\n",
      "Batch 139, Loss: 1.052447, Accuracy: 68.10%\n",
      "Batch 140, Loss: 1.024667, Accuracy: 68.11%\n",
      "Batch 141, Loss: 1.076856, Accuracy: 68.12%\n",
      "Batch 142, Loss: 1.131354, Accuracy: 68.06%\n",
      "Batch 143, Loss: 1.053880, Accuracy: 68.09%\n",
      "Batch 144, Loss: 1.049176, Accuracy: 68.11%\n",
      "Batch 145, Loss: 1.024465, Accuracy: 68.14%\n",
      "Batch 146, Loss: 1.135854, Accuracy: 68.08%\n",
      "Batch 147, Loss: 0.994470, Accuracy: 68.14%\n",
      "Batch 148, Loss: 1.010040, Accuracy: 68.18%\n",
      "Batch 149, Loss: 1.052703, Accuracy: 68.17%\n",
      "Batch 150, Loss: 1.011032, Accuracy: 68.23%\n",
      "Batch 151, Loss: 1.059220, Accuracy: 68.25%\n",
      "Batch 152, Loss: 1.101695, Accuracy: 68.22%\n",
      "Batch 153, Loss: 0.999094, Accuracy: 68.25%\n",
      "Batch 154, Loss: 1.018024, Accuracy: 68.26%\n",
      "Batch 155, Loss: 1.051761, Accuracy: 68.27%\n",
      "Batch 156, Loss: 1.058485, Accuracy: 68.27%\n",
      "Batch 157, Loss: 1.082699, Accuracy: 68.26%\n",
      "Batch 158, Loss: 1.055916, Accuracy: 68.27%\n",
      "Batch 159, Loss: 1.086237, Accuracy: 68.24%\n",
      "Batch 160, Loss: 1.054816, Accuracy: 68.24%\n",
      "Batch 161, Loss: 1.112821, Accuracy: 68.23%\n",
      "Batch 162, Loss: 1.080798, Accuracy: 68.21%\n",
      "Batch 163, Loss: 1.001355, Accuracy: 68.24%\n",
      "Batch 164, Loss: 1.021291, Accuracy: 68.26%\n",
      "Batch 165, Loss: 1.127599, Accuracy: 68.22%\n",
      "Batch 166, Loss: 1.103580, Accuracy: 68.19%\n",
      "Batch 167, Loss: 1.048629, Accuracy: 68.21%\n",
      "Batch 168, Loss: 1.003320, Accuracy: 68.24%\n",
      "Batch 169, Loss: 1.097141, Accuracy: 68.20%\n",
      "Batch 170, Loss: 1.088430, Accuracy: 68.19%\n",
      "Batch 171, Loss: 1.071782, Accuracy: 68.17%\n",
      "Batch 172, Loss: 1.071719, Accuracy: 68.16%\n",
      "Batch 173, Loss: 1.158962, Accuracy: 68.09%\n",
      "Batch 174, Loss: 1.080982, Accuracy: 68.09%\n",
      "Batch 175, Loss: 1.021725, Accuracy: 68.10%\n",
      "Batch 176, Loss: 1.085300, Accuracy: 68.08%\n",
      "Batch 177, Loss: 1.072159, Accuracy: 68.08%\n",
      "Batch 178, Loss: 0.993972, Accuracy: 68.11%\n",
      "Batch 179, Loss: 1.059050, Accuracy: 68.11%\n",
      "Batch 180, Loss: 1.026554, Accuracy: 68.15%\n",
      "Batch 181, Loss: 1.097723, Accuracy: 68.12%\n",
      "Batch 182, Loss: 1.112039, Accuracy: 68.09%\n",
      "Batch 183, Loss: 1.074757, Accuracy: 68.08%\n",
      "Batch 184, Loss: 1.057497, Accuracy: 68.08%\n",
      "Batch 185, Loss: 1.065594, Accuracy: 68.07%\n",
      "Batch 186, Loss: 1.104757, Accuracy: 68.04%\n",
      "Batch 187, Loss: 1.029276, Accuracy: 68.08%\n",
      "Batch 188, Loss: 0.991477, Accuracy: 68.11%\n",
      "Batch 189, Loss: 1.011148, Accuracy: 68.15%\n",
      "Batch 190, Loss: 1.092919, Accuracy: 68.13%\n",
      "Batch 191, Loss: 0.965139, Accuracy: 68.18%\n",
      "Batch 192, Loss: 1.027635, Accuracy: 68.20%\n",
      "Batch 193, Loss: 1.039780, Accuracy: 68.22%\n",
      "Batch 194, Loss: 1.071991, Accuracy: 68.21%\n",
      "Batch 195, Loss: 1.047193, Accuracy: 68.20%\n",
      "Batch 196, Loss: 1.019094, Accuracy: 68.22%\n",
      "Batch 197, Loss: 1.067963, Accuracy: 68.23%\n",
      "Batch 198, Loss: 1.149466, Accuracy: 68.17%\n",
      "Batch 199, Loss: 0.995877, Accuracy: 68.22%\n",
      "Batch 200, Loss: 1.146452, Accuracy: 68.16%\n",
      "Batch 201, Loss: 1.117680, Accuracy: 68.12%\n",
      "Batch 202, Loss: 1.098579, Accuracy: 68.10%\n",
      "Batch 203, Loss: 1.037485, Accuracy: 68.13%\n",
      "Batch 204, Loss: 1.052878, Accuracy: 68.12%\n",
      "Batch 205, Loss: 1.094761, Accuracy: 68.11%\n",
      "Batch 206, Loss: 1.074035, Accuracy: 68.11%\n",
      "Batch 207, Loss: 1.057025, Accuracy: 68.11%\n",
      "Batch 208, Loss: 1.096198, Accuracy: 68.10%\n",
      "Batch 209, Loss: 1.006847, Accuracy: 68.14%\n",
      "Batch 210, Loss: 1.052095, Accuracy: 68.14%\n",
      "Batch 211, Loss: 1.069696, Accuracy: 68.14%\n",
      "Batch 212, Loss: 1.018263, Accuracy: 68.15%\n",
      "Batch 213, Loss: 1.021076, Accuracy: 68.17%\n",
      "Training - Epoch 11, Loss: 1.060071, Accuracy: 68.17%\n",
      "Validation Batch 1, Loss: 1.173329, Accuracy: 53.12%\n",
      "Validation Batch 2, Loss: 1.243695, Accuracy: 49.22%\n",
      "Validation Batch 3, Loss: 1.293025, Accuracy: 46.88%\n",
      "Validation Batch 4, Loss: 1.173940, Accuracy: 48.83%\n",
      "Validation Batch 5, Loss: 1.264930, Accuracy: 47.81%\n",
      "Validation Batch 6, Loss: 1.231261, Accuracy: 47.66%\n",
      "Validation Batch 7, Loss: 1.206151, Accuracy: 48.21%\n",
      "Validation Batch 8, Loss: 1.230831, Accuracy: 48.05%\n",
      "Validation Batch 9, Loss: 1.242728, Accuracy: 48.09%\n",
      "Validation Batch 10, Loss: 1.237594, Accuracy: 47.97%\n",
      "Validation Batch 11, Loss: 1.208560, Accuracy: 47.87%\n",
      "Validation Batch 12, Loss: 1.145504, Accuracy: 48.57%\n",
      "Validation Batch 13, Loss: 1.243268, Accuracy: 48.68%\n",
      "Validation Batch 14, Loss: 1.159350, Accuracy: 49.11%\n",
      "Validation Batch 15, Loss: 1.208553, Accuracy: 49.27%\n",
      "Validation Batch 16, Loss: 1.260338, Accuracy: 49.12%\n",
      "Validation Batch 17, Loss: 1.300258, Accuracy: 48.71%\n",
      "Validation Batch 18, Loss: 1.221332, Accuracy: 48.70%\n",
      "Validation Batch 19, Loss: 1.257310, Accuracy: 48.60%\n",
      "Validation Batch 20, Loss: 1.206791, Accuracy: 48.67%\n",
      "Validation Batch 21, Loss: 1.230828, Accuracy: 48.59%\n",
      "Validation Batch 22, Loss: 1.242583, Accuracy: 48.58%\n",
      "Validation Batch 23, Loss: 1.284227, Accuracy: 48.37%\n",
      "Validation Batch 24, Loss: 1.232470, Accuracy: 48.31%\n",
      "Validation Batch 25, Loss: 1.190925, Accuracy: 48.56%\n",
      "Validation Batch 26, Loss: 1.158036, Accuracy: 48.74%\n",
      "Validation Batch 27, Loss: 1.284174, Accuracy: 48.50%\n",
      "Validation - Epoch 11, Loss: 1.227111, Accuracy: 48.50%\n",
      "Patience—0\n",
      "Epoch 12\n",
      "Batch 1, Loss: 1.057805, Accuracy: 65.62%\n",
      "Batch 2, Loss: 1.071947, Accuracy: 64.84%\n",
      "Batch 3, Loss: 1.041902, Accuracy: 66.15%\n",
      "Batch 4, Loss: 1.077832, Accuracy: 66.41%\n",
      "Batch 5, Loss: 1.040550, Accuracy: 67.81%\n",
      "Batch 6, Loss: 1.101137, Accuracy: 67.19%\n",
      "Batch 7, Loss: 0.992167, Accuracy: 68.75%\n",
      "Batch 8, Loss: 1.066101, Accuracy: 68.75%\n",
      "Batch 9, Loss: 1.015177, Accuracy: 68.92%\n",
      "Batch 10, Loss: 1.069454, Accuracy: 68.59%\n",
      "Batch 11, Loss: 0.994280, Accuracy: 69.18%\n",
      "Batch 12, Loss: 1.006536, Accuracy: 69.66%\n",
      "Batch 13, Loss: 1.038463, Accuracy: 69.59%\n",
      "Batch 14, Loss: 1.068823, Accuracy: 69.42%\n",
      "Batch 15, Loss: 1.143954, Accuracy: 68.75%\n",
      "Batch 16, Loss: 1.143314, Accuracy: 67.87%\n",
      "Batch 17, Loss: 1.084037, Accuracy: 67.74%\n",
      "Batch 18, Loss: 1.002716, Accuracy: 68.06%\n",
      "Batch 19, Loss: 1.184866, Accuracy: 67.43%\n",
      "Batch 20, Loss: 1.094604, Accuracy: 67.34%\n",
      "Batch 21, Loss: 1.083485, Accuracy: 67.11%\n",
      "Batch 22, Loss: 1.152448, Accuracy: 66.76%\n",
      "Batch 23, Loss: 1.140992, Accuracy: 66.30%\n",
      "Batch 24, Loss: 1.072133, Accuracy: 66.28%\n",
      "Batch 25, Loss: 1.050754, Accuracy: 66.38%\n",
      "Batch 26, Loss: 1.075876, Accuracy: 66.53%\n",
      "Batch 27, Loss: 1.059143, Accuracy: 66.61%\n",
      "Batch 28, Loss: 1.037317, Accuracy: 66.74%\n",
      "Batch 29, Loss: 1.063895, Accuracy: 66.81%\n",
      "Batch 30, Loss: 1.045743, Accuracy: 66.93%\n",
      "Batch 31, Loss: 1.094520, Accuracy: 66.78%\n",
      "Batch 32, Loss: 1.085962, Accuracy: 66.70%\n",
      "Batch 33, Loss: 1.073903, Accuracy: 66.62%\n",
      "Batch 34, Loss: 1.088153, Accuracy: 66.59%\n",
      "Batch 35, Loss: 1.098696, Accuracy: 66.52%\n",
      "Batch 36, Loss: 1.095502, Accuracy: 66.45%\n",
      "Batch 37, Loss: 1.149625, Accuracy: 66.26%\n",
      "Batch 38, Loss: 1.049390, Accuracy: 66.32%\n",
      "Batch 39, Loss: 1.119903, Accuracy: 66.19%\n",
      "Batch 40, Loss: 1.055927, Accuracy: 66.25%\n",
      "Batch 41, Loss: 1.046535, Accuracy: 66.31%\n",
      "Batch 42, Loss: 1.114910, Accuracy: 66.22%\n",
      "Batch 43, Loss: 1.054243, Accuracy: 66.32%\n",
      "Batch 44, Loss: 0.964081, Accuracy: 66.62%\n",
      "Batch 45, Loss: 1.185098, Accuracy: 66.28%\n",
      "Batch 46, Loss: 1.124005, Accuracy: 66.17%\n",
      "Batch 47, Loss: 1.034917, Accuracy: 66.26%\n",
      "Batch 48, Loss: 1.102445, Accuracy: 66.15%\n",
      "Batch 49, Loss: 0.980074, Accuracy: 66.36%\n",
      "Batch 50, Loss: 1.020503, Accuracy: 66.53%\n",
      "Batch 51, Loss: 1.067672, Accuracy: 66.54%\n",
      "Batch 52, Loss: 1.124514, Accuracy: 66.50%\n",
      "Batch 53, Loss: 1.027628, Accuracy: 66.63%\n",
      "Batch 54, Loss: 1.084778, Accuracy: 66.55%\n",
      "Batch 55, Loss: 0.984858, Accuracy: 66.73%\n",
      "Batch 56, Loss: 1.048018, Accuracy: 66.82%\n",
      "Batch 57, Loss: 1.031376, Accuracy: 66.91%\n",
      "Batch 58, Loss: 0.950476, Accuracy: 67.13%\n",
      "Batch 59, Loss: 0.988459, Accuracy: 67.27%\n",
      "Batch 60, Loss: 1.143314, Accuracy: 67.11%\n",
      "Batch 61, Loss: 1.096676, Accuracy: 67.06%\n",
      "Batch 62, Loss: 1.059794, Accuracy: 67.06%\n",
      "Batch 63, Loss: 1.026605, Accuracy: 67.14%\n",
      "Batch 64, Loss: 1.012547, Accuracy: 67.24%\n",
      "Batch 65, Loss: 1.054117, Accuracy: 67.26%\n",
      "Batch 66, Loss: 1.063117, Accuracy: 67.33%\n",
      "Batch 67, Loss: 1.069748, Accuracy: 67.35%\n",
      "Batch 68, Loss: 0.996804, Accuracy: 67.46%\n",
      "Batch 69, Loss: 0.999433, Accuracy: 67.55%\n",
      "Batch 70, Loss: 1.045219, Accuracy: 67.61%\n",
      "Batch 71, Loss: 1.081352, Accuracy: 67.58%\n",
      "Batch 72, Loss: 1.085274, Accuracy: 67.60%\n",
      "Batch 73, Loss: 1.044843, Accuracy: 67.68%\n",
      "Batch 74, Loss: 1.050065, Accuracy: 67.69%\n",
      "Batch 75, Loss: 1.000726, Accuracy: 67.77%\n",
      "Batch 76, Loss: 1.125131, Accuracy: 67.68%\n",
      "Batch 77, Loss: 1.112323, Accuracy: 67.61%\n",
      "Batch 78, Loss: 1.110735, Accuracy: 67.55%\n",
      "Batch 79, Loss: 0.983076, Accuracy: 67.66%\n",
      "Batch 80, Loss: 1.115431, Accuracy: 67.58%\n",
      "Batch 81, Loss: 1.000793, Accuracy: 67.67%\n",
      "Batch 82, Loss: 1.039787, Accuracy: 67.72%\n",
      "Batch 83, Loss: 1.068435, Accuracy: 67.73%\n",
      "Batch 84, Loss: 1.105039, Accuracy: 67.67%\n",
      "Batch 85, Loss: 0.989103, Accuracy: 67.74%\n",
      "Batch 86, Loss: 1.026497, Accuracy: 67.73%\n",
      "Batch 87, Loss: 0.993595, Accuracy: 67.83%\n",
      "Batch 88, Loss: 1.065259, Accuracy: 67.84%\n",
      "Batch 89, Loss: 0.982567, Accuracy: 67.94%\n",
      "Batch 90, Loss: 0.989889, Accuracy: 68.04%\n",
      "Batch 91, Loss: 1.150002, Accuracy: 67.93%\n",
      "Batch 92, Loss: 1.049896, Accuracy: 67.93%\n",
      "Batch 93, Loss: 1.057527, Accuracy: 67.98%\n",
      "Batch 94, Loss: 1.061241, Accuracy: 67.97%\n",
      "Batch 95, Loss: 1.046021, Accuracy: 68.01%\n",
      "Batch 96, Loss: 1.026314, Accuracy: 68.07%\n",
      "Batch 97, Loss: 1.033455, Accuracy: 68.09%\n",
      "Batch 98, Loss: 1.093517, Accuracy: 68.06%\n",
      "Batch 99, Loss: 1.058359, Accuracy: 68.07%\n",
      "Batch 100, Loss: 1.079003, Accuracy: 68.02%\n",
      "Batch 101, Loss: 0.973769, Accuracy: 68.13%\n",
      "Batch 102, Loss: 1.005731, Accuracy: 68.18%\n",
      "Batch 103, Loss: 0.971129, Accuracy: 68.28%\n",
      "Batch 104, Loss: 1.115278, Accuracy: 68.19%\n",
      "Batch 105, Loss: 1.022508, Accuracy: 68.23%\n",
      "Batch 106, Loss: 1.039820, Accuracy: 68.23%\n",
      "Batch 107, Loss: 1.045864, Accuracy: 68.22%\n",
      "Batch 108, Loss: 1.068716, Accuracy: 68.20%\n",
      "Batch 109, Loss: 1.101739, Accuracy: 68.15%\n",
      "Batch 110, Loss: 1.065211, Accuracy: 68.12%\n",
      "Batch 111, Loss: 1.141627, Accuracy: 68.06%\n",
      "Batch 112, Loss: 1.077945, Accuracy: 68.02%\n",
      "Batch 113, Loss: 0.999240, Accuracy: 68.06%\n",
      "Batch 114, Loss: 0.995659, Accuracy: 68.11%\n",
      "Batch 115, Loss: 1.106077, Accuracy: 68.06%\n",
      "Batch 116, Loss: 1.008441, Accuracy: 68.09%\n",
      "Batch 117, Loss: 0.999760, Accuracy: 68.14%\n",
      "Batch 118, Loss: 1.099748, Accuracy: 68.10%\n",
      "Batch 119, Loss: 0.976202, Accuracy: 68.19%\n",
      "Batch 120, Loss: 0.990945, Accuracy: 68.24%\n",
      "Batch 121, Loss: 1.095871, Accuracy: 68.18%\n",
      "Batch 122, Loss: 1.124120, Accuracy: 68.12%\n",
      "Batch 123, Loss: 1.105963, Accuracy: 68.09%\n",
      "Batch 124, Loss: 1.006557, Accuracy: 68.15%\n",
      "Batch 125, Loss: 1.037890, Accuracy: 68.16%\n",
      "Batch 126, Loss: 1.062464, Accuracy: 68.18%\n",
      "Batch 127, Loss: 1.108566, Accuracy: 68.13%\n",
      "Batch 128, Loss: 1.037597, Accuracy: 68.14%\n",
      "Batch 129, Loss: 0.964481, Accuracy: 68.22%\n",
      "Batch 130, Loss: 1.053974, Accuracy: 68.23%\n",
      "Batch 131, Loss: 1.101534, Accuracy: 68.19%\n",
      "Batch 132, Loss: 1.015708, Accuracy: 68.21%\n",
      "Batch 133, Loss: 1.076168, Accuracy: 68.19%\n",
      "Batch 134, Loss: 1.082856, Accuracy: 68.16%\n",
      "Batch 135, Loss: 1.027998, Accuracy: 68.18%\n",
      "Batch 136, Loss: 1.087688, Accuracy: 68.15%\n",
      "Batch 137, Loss: 1.012953, Accuracy: 68.18%\n",
      "Batch 138, Loss: 1.083749, Accuracy: 68.15%\n",
      "Batch 139, Loss: 1.019003, Accuracy: 68.20%\n",
      "Batch 140, Loss: 1.038270, Accuracy: 68.21%\n",
      "Batch 141, Loss: 1.010880, Accuracy: 68.27%\n",
      "Batch 142, Loss: 1.057172, Accuracy: 68.27%\n",
      "Batch 143, Loss: 1.038430, Accuracy: 68.30%\n",
      "Batch 144, Loss: 1.056936, Accuracy: 68.31%\n",
      "Batch 145, Loss: 1.098404, Accuracy: 68.25%\n",
      "Batch 146, Loss: 1.040449, Accuracy: 68.26%\n",
      "Batch 147, Loss: 1.099841, Accuracy: 68.23%\n",
      "Batch 148, Loss: 1.096078, Accuracy: 68.20%\n",
      "Batch 149, Loss: 1.092723, Accuracy: 68.15%\n",
      "Batch 150, Loss: 1.067339, Accuracy: 68.17%\n",
      "Batch 151, Loss: 1.107681, Accuracy: 68.14%\n",
      "Batch 152, Loss: 1.041218, Accuracy: 68.16%\n",
      "Batch 153, Loss: 1.053743, Accuracy: 68.17%\n",
      "Batch 154, Loss: 1.110218, Accuracy: 68.12%\n",
      "Batch 155, Loss: 1.049917, Accuracy: 68.14%\n",
      "Batch 156, Loss: 0.995128, Accuracy: 68.17%\n",
      "Batch 157, Loss: 1.079436, Accuracy: 68.16%\n",
      "Batch 158, Loss: 1.031126, Accuracy: 68.16%\n",
      "Batch 159, Loss: 1.016736, Accuracy: 68.18%\n",
      "Batch 160, Loss: 1.077848, Accuracy: 68.17%\n",
      "Batch 161, Loss: 0.994117, Accuracy: 68.22%\n",
      "Batch 162, Loss: 1.084275, Accuracy: 68.18%\n",
      "Batch 163, Loss: 1.082590, Accuracy: 68.16%\n",
      "Batch 164, Loss: 1.126793, Accuracy: 68.10%\n",
      "Batch 165, Loss: 1.123788, Accuracy: 68.05%\n",
      "Batch 166, Loss: 1.042746, Accuracy: 68.04%\n",
      "Batch 167, Loss: 1.075527, Accuracy: 68.04%\n",
      "Batch 168, Loss: 1.014401, Accuracy: 68.06%\n",
      "Batch 169, Loss: 1.021620, Accuracy: 68.08%\n",
      "Batch 170, Loss: 0.994208, Accuracy: 68.13%\n",
      "Batch 171, Loss: 1.120970, Accuracy: 68.09%\n",
      "Batch 172, Loss: 1.019697, Accuracy: 68.11%\n",
      "Batch 173, Loss: 1.047835, Accuracy: 68.13%\n",
      "Batch 174, Loss: 1.032616, Accuracy: 68.15%\n",
      "Batch 175, Loss: 0.970249, Accuracy: 68.21%\n",
      "Batch 176, Loss: 1.078968, Accuracy: 68.19%\n",
      "Batch 177, Loss: 1.020655, Accuracy: 68.22%\n",
      "Batch 178, Loss: 1.109616, Accuracy: 68.20%\n",
      "Batch 179, Loss: 1.017699, Accuracy: 68.23%\n",
      "Batch 180, Loss: 1.004803, Accuracy: 68.26%\n",
      "Batch 181, Loss: 1.065987, Accuracy: 68.25%\n",
      "Batch 182, Loss: 0.983134, Accuracy: 68.29%\n",
      "Batch 183, Loss: 0.962859, Accuracy: 68.34%\n",
      "Batch 184, Loss: 1.032184, Accuracy: 68.34%\n",
      "Batch 185, Loss: 1.096720, Accuracy: 68.34%\n",
      "Batch 186, Loss: 1.064687, Accuracy: 68.33%\n",
      "Batch 187, Loss: 0.978256, Accuracy: 68.37%\n",
      "Batch 188, Loss: 1.116876, Accuracy: 68.33%\n",
      "Batch 189, Loss: 1.066287, Accuracy: 68.33%\n",
      "Batch 190, Loss: 1.007980, Accuracy: 68.34%\n",
      "Batch 191, Loss: 1.037751, Accuracy: 68.35%\n",
      "Batch 192, Loss: 1.113720, Accuracy: 68.32%\n",
      "Batch 193, Loss: 0.958387, Accuracy: 68.38%\n",
      "Batch 194, Loss: 1.052676, Accuracy: 68.38%\n",
      "Batch 195, Loss: 1.018342, Accuracy: 68.40%\n",
      "Batch 196, Loss: 1.098848, Accuracy: 68.38%\n",
      "Batch 197, Loss: 0.976924, Accuracy: 68.42%\n",
      "Batch 198, Loss: 1.081668, Accuracy: 68.39%\n",
      "Batch 199, Loss: 1.084452, Accuracy: 68.39%\n",
      "Batch 200, Loss: 1.098963, Accuracy: 68.38%\n",
      "Batch 201, Loss: 1.035343, Accuracy: 68.38%\n",
      "Batch 202, Loss: 1.100185, Accuracy: 68.36%\n",
      "Batch 203, Loss: 1.056523, Accuracy: 68.37%\n",
      "Batch 204, Loss: 1.048497, Accuracy: 68.36%\n",
      "Batch 205, Loss: 1.060260, Accuracy: 68.36%\n",
      "Batch 206, Loss: 1.012636, Accuracy: 68.39%\n",
      "Batch 207, Loss: 1.026962, Accuracy: 68.40%\n",
      "Batch 208, Loss: 1.038922, Accuracy: 68.40%\n",
      "Batch 209, Loss: 1.115026, Accuracy: 68.37%\n",
      "Batch 210, Loss: 0.976095, Accuracy: 68.42%\n",
      "Batch 211, Loss: 1.081645, Accuracy: 68.42%\n",
      "Batch 212, Loss: 1.100284, Accuracy: 68.38%\n",
      "Batch 213, Loss: 1.034705, Accuracy: 68.40%\n",
      "Training - Epoch 12, Loss: 1.055649, Accuracy: 68.40%\n",
      "Validation Batch 1, Loss: 1.139434, Accuracy: 56.25%\n",
      "Validation Batch 2, Loss: 1.219746, Accuracy: 53.12%\n",
      "Validation Batch 3, Loss: 1.255836, Accuracy: 50.52%\n",
      "Validation Batch 4, Loss: 1.140249, Accuracy: 52.34%\n",
      "Validation Batch 5, Loss: 1.230569, Accuracy: 51.56%\n",
      "Validation Batch 6, Loss: 1.190445, Accuracy: 51.56%\n",
      "Validation Batch 7, Loss: 1.180197, Accuracy: 51.79%\n",
      "Validation Batch 8, Loss: 1.204062, Accuracy: 51.37%\n",
      "Validation Batch 9, Loss: 1.207422, Accuracy: 51.22%\n",
      "Validation Batch 10, Loss: 1.190411, Accuracy: 51.09%\n",
      "Validation Batch 11, Loss: 1.165169, Accuracy: 51.42%\n",
      "Validation Batch 12, Loss: 1.113293, Accuracy: 52.21%\n",
      "Validation Batch 13, Loss: 1.216057, Accuracy: 52.16%\n",
      "Validation Batch 14, Loss: 1.133009, Accuracy: 52.90%\n",
      "Validation Batch 15, Loss: 1.180193, Accuracy: 52.81%\n",
      "Validation Batch 16, Loss: 1.219495, Accuracy: 52.54%\n",
      "Validation Batch 17, Loss: 1.269688, Accuracy: 52.02%\n",
      "Validation Batch 18, Loss: 1.190568, Accuracy: 51.91%\n",
      "Validation Batch 19, Loss: 1.225279, Accuracy: 51.81%\n",
      "Validation Batch 20, Loss: 1.177623, Accuracy: 51.88%\n",
      "Validation Batch 21, Loss: 1.196230, Accuracy: 51.86%\n",
      "Validation Batch 22, Loss: 1.220495, Accuracy: 51.70%\n",
      "Validation Batch 23, Loss: 1.256936, Accuracy: 51.43%\n",
      "Validation Batch 24, Loss: 1.195014, Accuracy: 51.43%\n",
      "Validation Batch 25, Loss: 1.166147, Accuracy: 51.56%\n",
      "Validation Batch 26, Loss: 1.137502, Accuracy: 51.80%\n",
      "Validation Batch 27, Loss: 1.239909, Accuracy: 51.67%\n",
      "Validation - Epoch 12, Loss: 1.194851, Accuracy: 51.67%\n",
      "Patience—0\n",
      "Epoch 13\n",
      "Batch 1, Loss: 1.060893, Accuracy: 68.75%\n",
      "Batch 2, Loss: 1.115489, Accuracy: 64.06%\n",
      "Batch 3, Loss: 0.994756, Accuracy: 67.71%\n",
      "Batch 4, Loss: 1.104563, Accuracy: 65.62%\n",
      "Batch 5, Loss: 1.105105, Accuracy: 65.00%\n",
      "Batch 6, Loss: 1.117355, Accuracy: 65.10%\n",
      "Batch 7, Loss: 0.973444, Accuracy: 67.19%\n",
      "Batch 8, Loss: 1.076125, Accuracy: 67.19%\n",
      "Batch 9, Loss: 1.013017, Accuracy: 67.71%\n",
      "Batch 10, Loss: 1.096164, Accuracy: 67.34%\n",
      "Batch 11, Loss: 1.023828, Accuracy: 67.76%\n",
      "Batch 12, Loss: 1.020139, Accuracy: 67.97%\n",
      "Batch 13, Loss: 1.055373, Accuracy: 67.91%\n",
      "Batch 14, Loss: 0.987678, Accuracy: 68.42%\n",
      "Batch 15, Loss: 1.003481, Accuracy: 68.96%\n",
      "Batch 16, Loss: 1.079307, Accuracy: 68.75%\n",
      "Batch 17, Loss: 1.049684, Accuracy: 68.75%\n",
      "Batch 18, Loss: 1.044518, Accuracy: 68.84%\n",
      "Batch 19, Loss: 0.988646, Accuracy: 69.33%\n",
      "Batch 20, Loss: 1.113372, Accuracy: 68.91%\n",
      "Batch 21, Loss: 1.079271, Accuracy: 68.53%\n",
      "Batch 22, Loss: 1.063507, Accuracy: 68.54%\n",
      "Batch 23, Loss: 1.135693, Accuracy: 68.14%\n",
      "Batch 24, Loss: 1.136399, Accuracy: 67.90%\n",
      "Batch 25, Loss: 1.082076, Accuracy: 67.81%\n",
      "Batch 26, Loss: 1.071579, Accuracy: 67.85%\n",
      "Batch 27, Loss: 1.035662, Accuracy: 68.00%\n",
      "Batch 28, Loss: 0.980751, Accuracy: 68.36%\n",
      "Batch 29, Loss: 1.031718, Accuracy: 68.37%\n",
      "Batch 30, Loss: 1.164838, Accuracy: 68.02%\n",
      "Batch 31, Loss: 1.000784, Accuracy: 68.20%\n",
      "Batch 32, Loss: 1.051494, Accuracy: 68.26%\n",
      "Batch 33, Loss: 1.037543, Accuracy: 68.37%\n",
      "Batch 34, Loss: 1.118227, Accuracy: 68.15%\n",
      "Batch 35, Loss: 1.008395, Accuracy: 68.35%\n",
      "Batch 36, Loss: 1.016500, Accuracy: 68.53%\n",
      "Batch 37, Loss: 1.036400, Accuracy: 68.58%\n",
      "Batch 38, Loss: 0.972370, Accuracy: 68.83%\n",
      "Batch 39, Loss: 0.982217, Accuracy: 69.03%\n",
      "Batch 40, Loss: 1.085618, Accuracy: 68.95%\n",
      "Batch 41, Loss: 1.014149, Accuracy: 69.02%\n",
      "Batch 42, Loss: 1.033839, Accuracy: 69.05%\n",
      "Batch 43, Loss: 1.149340, Accuracy: 68.79%\n",
      "Batch 44, Loss: 1.049222, Accuracy: 68.79%\n",
      "Batch 45, Loss: 1.003147, Accuracy: 68.92%\n",
      "Batch 46, Loss: 1.081430, Accuracy: 68.82%\n",
      "Batch 47, Loss: 1.082998, Accuracy: 68.78%\n",
      "Batch 48, Loss: 1.058281, Accuracy: 68.78%\n",
      "Batch 49, Loss: 1.009107, Accuracy: 68.88%\n",
      "Batch 50, Loss: 1.080260, Accuracy: 68.78%\n",
      "Batch 51, Loss: 1.074558, Accuracy: 68.75%\n",
      "Batch 52, Loss: 1.091272, Accuracy: 68.72%\n",
      "Batch 53, Loss: 0.982662, Accuracy: 68.84%\n",
      "Batch 54, Loss: 1.028689, Accuracy: 68.92%\n",
      "Batch 55, Loss: 0.983623, Accuracy: 69.09%\n",
      "Batch 56, Loss: 1.044039, Accuracy: 69.14%\n",
      "Batch 57, Loss: 1.024068, Accuracy: 69.13%\n",
      "Batch 58, Loss: 1.022777, Accuracy: 69.23%\n",
      "Batch 59, Loss: 1.047174, Accuracy: 69.25%\n",
      "Batch 60, Loss: 1.020831, Accuracy: 69.30%\n",
      "Batch 61, Loss: 1.108496, Accuracy: 69.16%\n",
      "Batch 62, Loss: 1.036276, Accuracy: 69.13%\n",
      "Batch 63, Loss: 1.115748, Accuracy: 69.05%\n",
      "Batch 64, Loss: 1.034332, Accuracy: 69.04%\n",
      "Batch 65, Loss: 1.071019, Accuracy: 68.97%\n",
      "Batch 66, Loss: 0.995834, Accuracy: 69.08%\n",
      "Batch 67, Loss: 1.086806, Accuracy: 69.01%\n",
      "Batch 68, Loss: 1.017115, Accuracy: 69.09%\n",
      "Batch 69, Loss: 1.089345, Accuracy: 69.02%\n",
      "Batch 70, Loss: 1.022036, Accuracy: 69.06%\n",
      "Batch 71, Loss: 0.994526, Accuracy: 69.12%\n",
      "Batch 72, Loss: 1.068914, Accuracy: 69.08%\n",
      "Batch 73, Loss: 1.001459, Accuracy: 69.14%\n",
      "Batch 74, Loss: 0.972942, Accuracy: 69.26%\n",
      "Batch 75, Loss: 1.049581, Accuracy: 69.23%\n",
      "Batch 76, Loss: 1.032067, Accuracy: 69.26%\n",
      "Batch 77, Loss: 1.095048, Accuracy: 69.18%\n",
      "Batch 78, Loss: 1.097212, Accuracy: 69.09%\n",
      "Batch 79, Loss: 1.031362, Accuracy: 69.07%\n",
      "Batch 80, Loss: 0.935672, Accuracy: 69.22%\n",
      "Batch 81, Loss: 0.974238, Accuracy: 69.27%\n",
      "Batch 82, Loss: 1.133987, Accuracy: 69.15%\n",
      "Batch 83, Loss: 1.083930, Accuracy: 69.09%\n",
      "Batch 84, Loss: 1.038174, Accuracy: 69.10%\n",
      "Batch 85, Loss: 1.023733, Accuracy: 69.12%\n",
      "Batch 86, Loss: 1.082897, Accuracy: 69.08%\n",
      "Batch 87, Loss: 1.101880, Accuracy: 69.00%\n",
      "Batch 88, Loss: 1.058987, Accuracy: 68.93%\n",
      "Batch 89, Loss: 1.014831, Accuracy: 68.98%\n",
      "Batch 90, Loss: 1.015487, Accuracy: 69.03%\n",
      "Batch 91, Loss: 1.038820, Accuracy: 69.04%\n",
      "Batch 92, Loss: 1.027982, Accuracy: 69.07%\n",
      "Batch 93, Loss: 0.983180, Accuracy: 69.15%\n",
      "Batch 94, Loss: 1.067264, Accuracy: 69.13%\n",
      "Batch 95, Loss: 1.105081, Accuracy: 69.06%\n",
      "Batch 96, Loss: 1.030497, Accuracy: 69.06%\n",
      "Batch 97, Loss: 0.979683, Accuracy: 69.12%\n",
      "Batch 98, Loss: 1.007676, Accuracy: 69.15%\n",
      "Batch 99, Loss: 0.994485, Accuracy: 69.21%\n",
      "Batch 100, Loss: 0.996816, Accuracy: 69.27%\n",
      "Batch 101, Loss: 1.050248, Accuracy: 69.25%\n",
      "Batch 102, Loss: 0.982409, Accuracy: 69.32%\n",
      "Batch 103, Loss: 1.043686, Accuracy: 69.31%\n",
      "Batch 104, Loss: 1.004945, Accuracy: 69.32%\n",
      "Batch 105, Loss: 1.085079, Accuracy: 69.26%\n",
      "Batch 106, Loss: 1.087495, Accuracy: 69.21%\n",
      "Batch 107, Loss: 1.089114, Accuracy: 69.19%\n",
      "Batch 108, Loss: 1.030499, Accuracy: 69.21%\n",
      "Batch 109, Loss: 1.169974, Accuracy: 69.11%\n",
      "Batch 110, Loss: 1.009355, Accuracy: 69.15%\n",
      "Batch 111, Loss: 1.128793, Accuracy: 69.06%\n",
      "Batch 112, Loss: 1.057547, Accuracy: 69.06%\n",
      "Batch 113, Loss: 1.006624, Accuracy: 69.12%\n",
      "Batch 114, Loss: 0.949178, Accuracy: 69.23%\n",
      "Batch 115, Loss: 1.107516, Accuracy: 69.20%\n",
      "Batch 116, Loss: 1.071913, Accuracy: 69.22%\n",
      "Batch 117, Loss: 0.991005, Accuracy: 69.28%\n",
      "Batch 118, Loss: 1.047201, Accuracy: 69.29%\n",
      "Batch 119, Loss: 1.042830, Accuracy: 69.29%\n",
      "Batch 120, Loss: 1.051716, Accuracy: 69.28%\n",
      "Batch 121, Loss: 1.030262, Accuracy: 69.31%\n",
      "Batch 122, Loss: 0.994793, Accuracy: 69.35%\n",
      "Batch 123, Loss: 1.069888, Accuracy: 69.35%\n",
      "Batch 124, Loss: 1.071353, Accuracy: 69.34%\n",
      "Batch 125, Loss: 1.080568, Accuracy: 69.33%\n",
      "Batch 126, Loss: 1.078926, Accuracy: 69.31%\n",
      "Batch 127, Loss: 0.954238, Accuracy: 69.41%\n",
      "Batch 128, Loss: 1.109275, Accuracy: 69.35%\n",
      "Batch 129, Loss: 0.976910, Accuracy: 69.42%\n",
      "Batch 130, Loss: 1.141359, Accuracy: 69.34%\n",
      "Batch 131, Loss: 1.051492, Accuracy: 69.36%\n",
      "Batch 132, Loss: 1.128181, Accuracy: 69.29%\n",
      "Batch 133, Loss: 1.062400, Accuracy: 69.28%\n",
      "Batch 134, Loss: 1.009952, Accuracy: 69.32%\n",
      "Batch 135, Loss: 1.034077, Accuracy: 69.34%\n",
      "Batch 136, Loss: 0.998559, Accuracy: 69.38%\n",
      "Batch 137, Loss: 1.089836, Accuracy: 69.33%\n",
      "Batch 138, Loss: 1.058380, Accuracy: 69.34%\n",
      "Batch 139, Loss: 1.143774, Accuracy: 69.27%\n",
      "Batch 140, Loss: 1.129621, Accuracy: 69.20%\n",
      "Batch 141, Loss: 1.049200, Accuracy: 69.19%\n",
      "Batch 142, Loss: 1.070918, Accuracy: 69.20%\n",
      "Batch 143, Loss: 1.020786, Accuracy: 69.23%\n",
      "Batch 144, Loss: 1.034316, Accuracy: 69.24%\n",
      "Batch 145, Loss: 1.049666, Accuracy: 69.25%\n",
      "Batch 146, Loss: 1.099086, Accuracy: 69.21%\n",
      "Batch 147, Loss: 0.983495, Accuracy: 69.25%\n",
      "Batch 148, Loss: 0.974408, Accuracy: 69.30%\n",
      "Batch 149, Loss: 1.066729, Accuracy: 69.31%\n",
      "Batch 150, Loss: 1.054311, Accuracy: 69.29%\n",
      "Batch 151, Loss: 1.084186, Accuracy: 69.26%\n",
      "Batch 152, Loss: 1.059831, Accuracy: 69.25%\n",
      "Batch 153, Loss: 1.019817, Accuracy: 69.27%\n",
      "Batch 154, Loss: 1.049108, Accuracy: 69.27%\n",
      "Batch 155, Loss: 1.057001, Accuracy: 69.26%\n",
      "Batch 156, Loss: 1.038996, Accuracy: 69.24%\n",
      "Batch 157, Loss: 1.053722, Accuracy: 69.24%\n",
      "Batch 158, Loss: 1.017384, Accuracy: 69.27%\n",
      "Batch 159, Loss: 1.192866, Accuracy: 69.17%\n",
      "Batch 160, Loss: 1.076842, Accuracy: 69.16%\n",
      "Batch 161, Loss: 1.082087, Accuracy: 69.12%\n",
      "Batch 162, Loss: 1.085541, Accuracy: 69.08%\n",
      "Batch 163, Loss: 1.016592, Accuracy: 69.11%\n",
      "Batch 164, Loss: 1.025919, Accuracy: 69.12%\n",
      "Batch 165, Loss: 1.056379, Accuracy: 69.09%\n",
      "Batch 166, Loss: 1.057492, Accuracy: 69.09%\n",
      "Batch 167, Loss: 1.099361, Accuracy: 69.05%\n",
      "Batch 168, Loss: 1.101681, Accuracy: 69.02%\n",
      "Batch 169, Loss: 1.107817, Accuracy: 68.98%\n",
      "Batch 170, Loss: 1.046639, Accuracy: 68.99%\n",
      "Batch 171, Loss: 1.070083, Accuracy: 68.98%\n",
      "Batch 172, Loss: 1.071904, Accuracy: 68.98%\n",
      "Batch 173, Loss: 0.991244, Accuracy: 69.02%\n",
      "Batch 174, Loss: 1.040838, Accuracy: 69.02%\n",
      "Batch 175, Loss: 0.990929, Accuracy: 69.06%\n",
      "Batch 176, Loss: 1.074221, Accuracy: 69.04%\n",
      "Batch 177, Loss: 0.982550, Accuracy: 69.08%\n",
      "Batch 178, Loss: 1.090544, Accuracy: 69.03%\n",
      "Batch 179, Loss: 0.976725, Accuracy: 69.07%\n",
      "Batch 180, Loss: 1.065435, Accuracy: 69.05%\n",
      "Batch 181, Loss: 0.982972, Accuracy: 69.10%\n",
      "Batch 182, Loss: 1.039210, Accuracy: 69.13%\n",
      "Batch 183, Loss: 1.119401, Accuracy: 69.07%\n",
      "Batch 184, Loss: 1.074120, Accuracy: 69.05%\n",
      "Batch 185, Loss: 1.014811, Accuracy: 69.05%\n",
      "Batch 186, Loss: 1.065231, Accuracy: 69.04%\n",
      "Batch 187, Loss: 1.011915, Accuracy: 69.07%\n",
      "Batch 188, Loss: 1.022757, Accuracy: 69.09%\n",
      "Batch 189, Loss: 0.982448, Accuracy: 69.13%\n",
      "Batch 190, Loss: 1.021483, Accuracy: 69.15%\n",
      "Batch 191, Loss: 1.025004, Accuracy: 69.16%\n",
      "Batch 192, Loss: 1.084305, Accuracy: 69.16%\n",
      "Batch 193, Loss: 1.089405, Accuracy: 69.11%\n",
      "Batch 194, Loss: 1.140509, Accuracy: 69.05%\n",
      "Batch 195, Loss: 1.045519, Accuracy: 69.04%\n",
      "Batch 196, Loss: 1.043462, Accuracy: 69.04%\n",
      "Batch 197, Loss: 1.024538, Accuracy: 69.06%\n",
      "Batch 198, Loss: 1.057068, Accuracy: 69.05%\n",
      "Batch 199, Loss: 1.031881, Accuracy: 69.07%\n",
      "Batch 200, Loss: 1.090984, Accuracy: 69.05%\n",
      "Batch 201, Loss: 1.060468, Accuracy: 69.05%\n",
      "Batch 202, Loss: 1.154030, Accuracy: 68.99%\n",
      "Batch 203, Loss: 1.007317, Accuracy: 69.01%\n",
      "Batch 204, Loss: 1.037468, Accuracy: 69.02%\n",
      "Batch 205, Loss: 0.973155, Accuracy: 69.05%\n",
      "Batch 206, Loss: 0.976036, Accuracy: 69.09%\n",
      "Batch 207, Loss: 1.042355, Accuracy: 69.08%\n",
      "Batch 208, Loss: 1.017656, Accuracy: 69.10%\n",
      "Batch 209, Loss: 1.026367, Accuracy: 69.09%\n",
      "Batch 210, Loss: 1.008680, Accuracy: 69.11%\n",
      "Batch 211, Loss: 1.006981, Accuracy: 69.13%\n",
      "Batch 212, Loss: 1.032189, Accuracy: 69.13%\n",
      "Batch 213, Loss: 0.987515, Accuracy: 69.15%\n",
      "Training - Epoch 13, Loss: 1.047961, Accuracy: 69.15%\n",
      "Validation Batch 1, Loss: 1.142197, Accuracy: 56.25%\n",
      "Validation Batch 2, Loss: 1.218249, Accuracy: 53.12%\n",
      "Validation Batch 3, Loss: 1.259745, Accuracy: 50.52%\n",
      "Validation Batch 4, Loss: 1.135839, Accuracy: 53.12%\n",
      "Validation Batch 5, Loss: 1.222412, Accuracy: 52.19%\n",
      "Validation Batch 6, Loss: 1.189808, Accuracy: 52.08%\n",
      "Validation Batch 7, Loss: 1.182814, Accuracy: 52.23%\n",
      "Validation Batch 8, Loss: 1.199817, Accuracy: 51.95%\n",
      "Validation Batch 9, Loss: 1.205150, Accuracy: 52.08%\n",
      "Validation Batch 10, Loss: 1.188946, Accuracy: 52.03%\n",
      "Validation Batch 11, Loss: 1.166320, Accuracy: 52.56%\n",
      "Validation Batch 12, Loss: 1.108765, Accuracy: 53.52%\n",
      "Validation Batch 13, Loss: 1.211797, Accuracy: 53.37%\n",
      "Validation Batch 14, Loss: 1.137496, Accuracy: 53.79%\n",
      "Validation Batch 15, Loss: 1.175672, Accuracy: 53.85%\n",
      "Validation Batch 16, Loss: 1.223072, Accuracy: 53.61%\n",
      "Validation Batch 17, Loss: 1.275491, Accuracy: 53.03%\n",
      "Validation Batch 18, Loss: 1.188034, Accuracy: 52.95%\n",
      "Validation Batch 19, Loss: 1.223033, Accuracy: 52.80%\n",
      "Validation Batch 20, Loss: 1.175098, Accuracy: 52.89%\n",
      "Validation Batch 21, Loss: 1.195329, Accuracy: 52.98%\n",
      "Validation Batch 22, Loss: 1.221135, Accuracy: 52.77%\n",
      "Validation Batch 23, Loss: 1.259154, Accuracy: 52.51%\n",
      "Validation Batch 24, Loss: 1.192487, Accuracy: 52.54%\n",
      "Validation Batch 25, Loss: 1.162096, Accuracy: 52.69%\n",
      "Validation Batch 26, Loss: 1.139571, Accuracy: 52.76%\n",
      "Validation Batch 27, Loss: 1.246423, Accuracy: 52.55%\n",
      "Validation - Epoch 13, Loss: 1.194294, Accuracy: 52.55%\n",
      "Patience—0\n",
      "Epoch 14\n",
      "Batch 1, Loss: 1.034220, Accuracy: 71.88%\n",
      "Batch 2, Loss: 1.150283, Accuracy: 64.06%\n",
      "Batch 3, Loss: 1.108822, Accuracy: 63.02%\n",
      "Batch 4, Loss: 1.096645, Accuracy: 62.89%\n",
      "Batch 5, Loss: 0.962285, Accuracy: 66.56%\n",
      "Batch 6, Loss: 1.001293, Accuracy: 68.23%\n",
      "Batch 7, Loss: 1.052426, Accuracy: 68.53%\n",
      "Batch 8, Loss: 0.960502, Accuracy: 70.12%\n",
      "Batch 9, Loss: 1.066602, Accuracy: 69.44%\n",
      "Batch 10, Loss: 1.069512, Accuracy: 69.22%\n",
      "Batch 11, Loss: 1.075513, Accuracy: 68.89%\n",
      "Batch 12, Loss: 1.023719, Accuracy: 69.27%\n",
      "Batch 13, Loss: 1.081038, Accuracy: 69.11%\n",
      "Batch 14, Loss: 0.991697, Accuracy: 69.64%\n",
      "Batch 15, Loss: 1.104227, Accuracy: 69.17%\n",
      "Batch 16, Loss: 0.984679, Accuracy: 69.63%\n",
      "Batch 17, Loss: 1.055180, Accuracy: 69.58%\n",
      "Batch 18, Loss: 1.017352, Accuracy: 69.70%\n",
      "Batch 19, Loss: 1.007207, Accuracy: 69.90%\n",
      "Batch 20, Loss: 0.956786, Accuracy: 70.55%\n",
      "Batch 21, Loss: 1.078709, Accuracy: 70.39%\n",
      "Batch 22, Loss: 1.090947, Accuracy: 70.31%\n",
      "Batch 23, Loss: 1.092410, Accuracy: 70.18%\n",
      "Batch 24, Loss: 1.103624, Accuracy: 69.92%\n",
      "Batch 25, Loss: 1.167760, Accuracy: 69.31%\n",
      "Batch 26, Loss: 1.024880, Accuracy: 69.35%\n",
      "Batch 27, Loss: 1.056734, Accuracy: 69.27%\n",
      "Batch 28, Loss: 1.032161, Accuracy: 69.36%\n",
      "Batch 29, Loss: 1.104885, Accuracy: 69.18%\n",
      "Batch 30, Loss: 1.049633, Accuracy: 69.11%\n",
      "Batch 31, Loss: 1.042756, Accuracy: 69.10%\n",
      "Batch 32, Loss: 1.105454, Accuracy: 68.95%\n",
      "Batch 33, Loss: 1.110140, Accuracy: 68.89%\n",
      "Batch 34, Loss: 0.970257, Accuracy: 69.16%\n",
      "Batch 35, Loss: 1.037163, Accuracy: 69.24%\n",
      "Batch 36, Loss: 1.033004, Accuracy: 69.40%\n",
      "Batch 37, Loss: 1.035761, Accuracy: 69.47%\n",
      "Batch 38, Loss: 0.934703, Accuracy: 69.78%\n",
      "Batch 39, Loss: 1.081875, Accuracy: 69.63%\n",
      "Batch 40, Loss: 0.977834, Accuracy: 69.77%\n",
      "Batch 41, Loss: 1.068336, Accuracy: 69.70%\n",
      "Batch 42, Loss: 0.976126, Accuracy: 69.94%\n",
      "Batch 43, Loss: 0.959459, Accuracy: 70.09%\n",
      "Batch 44, Loss: 0.972951, Accuracy: 70.24%\n",
      "Batch 45, Loss: 1.046714, Accuracy: 70.24%\n",
      "Batch 46, Loss: 1.013568, Accuracy: 70.31%\n",
      "Batch 47, Loss: 1.037873, Accuracy: 70.31%\n",
      "Batch 48, Loss: 0.952914, Accuracy: 70.54%\n",
      "Batch 49, Loss: 1.010161, Accuracy: 70.63%\n",
      "Batch 50, Loss: 1.032698, Accuracy: 70.62%\n",
      "Batch 51, Loss: 0.991178, Accuracy: 70.74%\n",
      "Batch 52, Loss: 0.971712, Accuracy: 70.88%\n",
      "Batch 53, Loss: 0.973525, Accuracy: 71.05%\n",
      "Batch 54, Loss: 1.034074, Accuracy: 71.04%\n",
      "Batch 55, Loss: 1.022456, Accuracy: 71.05%\n",
      "Batch 56, Loss: 0.997428, Accuracy: 71.09%\n",
      "Batch 57, Loss: 1.123001, Accuracy: 70.92%\n",
      "Batch 58, Loss: 1.149678, Accuracy: 70.72%\n",
      "Batch 59, Loss: 1.045864, Accuracy: 70.66%\n",
      "Batch 60, Loss: 1.033670, Accuracy: 70.68%\n",
      "Batch 61, Loss: 1.082493, Accuracy: 70.57%\n",
      "Batch 62, Loss: 1.002884, Accuracy: 70.64%\n",
      "Batch 63, Loss: 1.057841, Accuracy: 70.59%\n",
      "Batch 64, Loss: 1.038700, Accuracy: 70.63%\n",
      "Batch 65, Loss: 0.918866, Accuracy: 70.79%\n",
      "Batch 66, Loss: 1.056752, Accuracy: 70.74%\n",
      "Batch 67, Loss: 1.050293, Accuracy: 70.69%\n",
      "Batch 68, Loss: 1.072998, Accuracy: 70.61%\n",
      "Batch 69, Loss: 1.070902, Accuracy: 70.54%\n",
      "Batch 70, Loss: 1.057321, Accuracy: 70.54%\n",
      "Batch 71, Loss: 1.000070, Accuracy: 70.60%\n",
      "Batch 72, Loss: 0.958541, Accuracy: 70.75%\n",
      "Batch 73, Loss: 0.974681, Accuracy: 70.85%\n",
      "Batch 74, Loss: 1.007282, Accuracy: 70.86%\n",
      "Batch 75, Loss: 1.086311, Accuracy: 70.79%\n",
      "Batch 76, Loss: 1.128069, Accuracy: 70.66%\n",
      "Batch 77, Loss: 1.117125, Accuracy: 70.54%\n",
      "Batch 78, Loss: 1.024241, Accuracy: 70.55%\n",
      "Batch 79, Loss: 1.080824, Accuracy: 70.47%\n",
      "Batch 80, Loss: 1.110020, Accuracy: 70.35%\n",
      "Batch 81, Loss: 1.063113, Accuracy: 70.33%\n",
      "Batch 82, Loss: 1.041370, Accuracy: 70.35%\n",
      "Batch 83, Loss: 1.088378, Accuracy: 70.29%\n",
      "Batch 84, Loss: 1.031170, Accuracy: 70.33%\n",
      "Batch 85, Loss: 1.159370, Accuracy: 70.17%\n",
      "Batch 86, Loss: 1.022359, Accuracy: 70.19%\n",
      "Batch 87, Loss: 1.080536, Accuracy: 70.15%\n",
      "Batch 88, Loss: 1.125450, Accuracy: 70.03%\n",
      "Batch 89, Loss: 1.055505, Accuracy: 70.00%\n",
      "Batch 90, Loss: 1.026466, Accuracy: 70.00%\n",
      "Batch 91, Loss: 1.028318, Accuracy: 70.02%\n",
      "Batch 92, Loss: 1.016514, Accuracy: 70.04%\n",
      "Batch 93, Loss: 1.029714, Accuracy: 70.04%\n",
      "Batch 94, Loss: 1.085866, Accuracy: 70.00%\n",
      "Batch 95, Loss: 1.073446, Accuracy: 69.98%\n",
      "Batch 96, Loss: 1.062515, Accuracy: 69.97%\n",
      "Batch 97, Loss: 1.136760, Accuracy: 69.85%\n",
      "Batch 98, Loss: 1.006130, Accuracy: 69.87%\n",
      "Batch 99, Loss: 0.951780, Accuracy: 69.97%\n",
      "Batch 100, Loss: 1.028918, Accuracy: 69.94%\n",
      "Batch 101, Loss: 1.124329, Accuracy: 69.85%\n",
      "Batch 102, Loss: 0.962831, Accuracy: 69.93%\n",
      "Batch 103, Loss: 1.028203, Accuracy: 69.95%\n",
      "Batch 104, Loss: 1.106800, Accuracy: 69.91%\n",
      "Batch 105, Loss: 1.030478, Accuracy: 69.94%\n",
      "Batch 106, Loss: 1.079757, Accuracy: 69.89%\n",
      "Batch 107, Loss: 0.962576, Accuracy: 69.96%\n",
      "Batch 108, Loss: 1.044118, Accuracy: 69.97%\n",
      "Batch 109, Loss: 1.068739, Accuracy: 69.95%\n",
      "Batch 110, Loss: 1.024577, Accuracy: 69.96%\n",
      "Batch 111, Loss: 0.979907, Accuracy: 70.02%\n",
      "Batch 112, Loss: 1.080412, Accuracy: 69.98%\n",
      "Batch 113, Loss: 1.068850, Accuracy: 69.97%\n",
      "Batch 114, Loss: 1.050260, Accuracy: 69.97%\n",
      "Batch 115, Loss: 0.976252, Accuracy: 70.03%\n",
      "Batch 116, Loss: 0.982812, Accuracy: 70.07%\n",
      "Batch 117, Loss: 1.032248, Accuracy: 70.07%\n",
      "Batch 118, Loss: 1.066453, Accuracy: 70.06%\n",
      "Batch 119, Loss: 1.061126, Accuracy: 70.05%\n",
      "Batch 120, Loss: 1.003737, Accuracy: 70.10%\n",
      "Batch 121, Loss: 0.980528, Accuracy: 70.18%\n",
      "Batch 122, Loss: 1.001593, Accuracy: 70.25%\n",
      "Batch 123, Loss: 1.015189, Accuracy: 70.26%\n",
      "Batch 124, Loss: 1.151493, Accuracy: 70.15%\n",
      "Batch 125, Loss: 1.061724, Accuracy: 70.11%\n",
      "Batch 126, Loss: 1.041557, Accuracy: 70.11%\n",
      "Batch 127, Loss: 1.046356, Accuracy: 70.10%\n",
      "Batch 128, Loss: 1.060257, Accuracy: 70.08%\n",
      "Batch 129, Loss: 1.028738, Accuracy: 70.11%\n",
      "Batch 130, Loss: 1.023798, Accuracy: 70.13%\n",
      "Batch 131, Loss: 0.949264, Accuracy: 70.21%\n",
      "Batch 132, Loss: 1.070734, Accuracy: 70.19%\n",
      "Batch 133, Loss: 1.000075, Accuracy: 70.23%\n",
      "Batch 134, Loss: 1.077451, Accuracy: 70.18%\n",
      "Batch 135, Loss: 1.043013, Accuracy: 70.17%\n",
      "Batch 136, Loss: 0.969063, Accuracy: 70.22%\n",
      "Batch 137, Loss: 0.998332, Accuracy: 70.26%\n",
      "Batch 138, Loss: 1.081468, Accuracy: 70.22%\n",
      "Batch 139, Loss: 0.978275, Accuracy: 70.27%\n",
      "Batch 140, Loss: 1.092637, Accuracy: 70.21%\n",
      "Batch 141, Loss: 1.093683, Accuracy: 70.18%\n",
      "Batch 142, Loss: 1.052044, Accuracy: 70.16%\n",
      "Batch 143, Loss: 1.027522, Accuracy: 70.17%\n",
      "Batch 144, Loss: 1.028929, Accuracy: 70.16%\n",
      "Batch 145, Loss: 0.972149, Accuracy: 70.19%\n",
      "Batch 146, Loss: 1.109187, Accuracy: 70.14%\n",
      "Batch 147, Loss: 1.052451, Accuracy: 70.13%\n",
      "Batch 148, Loss: 0.980618, Accuracy: 70.19%\n",
      "Batch 149, Loss: 1.120193, Accuracy: 70.12%\n",
      "Batch 150, Loss: 1.001520, Accuracy: 70.16%\n",
      "Batch 151, Loss: 0.938407, Accuracy: 70.23%\n",
      "Batch 152, Loss: 1.109305, Accuracy: 70.18%\n",
      "Batch 153, Loss: 1.051267, Accuracy: 70.19%\n",
      "Batch 154, Loss: 0.965379, Accuracy: 70.25%\n",
      "Batch 155, Loss: 1.027265, Accuracy: 70.26%\n",
      "Batch 156, Loss: 1.049169, Accuracy: 70.26%\n",
      "Batch 157, Loss: 1.071703, Accuracy: 70.24%\n",
      "Batch 158, Loss: 0.992572, Accuracy: 70.26%\n",
      "Batch 159, Loss: 1.042681, Accuracy: 70.28%\n",
      "Batch 160, Loss: 0.983832, Accuracy: 70.31%\n",
      "Batch 161, Loss: 1.088500, Accuracy: 70.27%\n",
      "Batch 162, Loss: 0.983536, Accuracy: 70.33%\n",
      "Batch 163, Loss: 1.044302, Accuracy: 70.32%\n",
      "Batch 164, Loss: 1.073989, Accuracy: 70.30%\n",
      "Batch 165, Loss: 0.996562, Accuracy: 70.32%\n",
      "Batch 166, Loss: 1.097088, Accuracy: 70.29%\n",
      "Batch 167, Loss: 1.097629, Accuracy: 70.25%\n",
      "Batch 168, Loss: 1.103859, Accuracy: 70.21%\n",
      "Batch 169, Loss: 1.027817, Accuracy: 70.19%\n",
      "Batch 170, Loss: 1.003548, Accuracy: 70.22%\n",
      "Batch 171, Loss: 1.026237, Accuracy: 70.23%\n",
      "Batch 172, Loss: 1.096147, Accuracy: 70.18%\n",
      "Batch 173, Loss: 1.059244, Accuracy: 70.17%\n",
      "Batch 174, Loss: 1.111108, Accuracy: 70.11%\n",
      "Batch 175, Loss: 1.071332, Accuracy: 70.09%\n",
      "Batch 176, Loss: 1.044886, Accuracy: 70.08%\n",
      "Batch 177, Loss: 1.032467, Accuracy: 70.08%\n",
      "Batch 178, Loss: 0.979345, Accuracy: 70.12%\n",
      "Batch 179, Loss: 1.058847, Accuracy: 70.11%\n",
      "Batch 180, Loss: 1.017002, Accuracy: 70.13%\n",
      "Batch 181, Loss: 1.083826, Accuracy: 70.09%\n",
      "Batch 182, Loss: 1.050842, Accuracy: 70.08%\n",
      "Batch 183, Loss: 0.947934, Accuracy: 70.13%\n",
      "Batch 184, Loss: 1.207336, Accuracy: 70.03%\n",
      "Batch 185, Loss: 1.107694, Accuracy: 70.01%\n",
      "Batch 186, Loss: 1.003242, Accuracy: 70.04%\n",
      "Batch 187, Loss: 1.046895, Accuracy: 70.05%\n",
      "Batch 188, Loss: 0.982147, Accuracy: 70.09%\n",
      "Batch 189, Loss: 1.044718, Accuracy: 70.10%\n",
      "Batch 190, Loss: 1.118026, Accuracy: 70.03%\n",
      "Batch 191, Loss: 1.059745, Accuracy: 70.02%\n",
      "Batch 192, Loss: 0.970626, Accuracy: 70.06%\n",
      "Batch 193, Loss: 1.012000, Accuracy: 70.07%\n",
      "Batch 194, Loss: 1.031596, Accuracy: 70.07%\n",
      "Batch 195, Loss: 1.044199, Accuracy: 70.07%\n",
      "Batch 196, Loss: 1.019467, Accuracy: 70.08%\n",
      "Batch 197, Loss: 1.101308, Accuracy: 70.05%\n",
      "Batch 198, Loss: 1.079340, Accuracy: 70.04%\n",
      "Batch 199, Loss: 0.997900, Accuracy: 70.08%\n",
      "Batch 200, Loss: 1.063253, Accuracy: 70.06%\n",
      "Batch 201, Loss: 1.006809, Accuracy: 70.09%\n",
      "Batch 202, Loss: 1.028854, Accuracy: 70.09%\n",
      "Batch 203, Loss: 1.167445, Accuracy: 70.03%\n",
      "Batch 204, Loss: 1.106955, Accuracy: 69.99%\n",
      "Batch 205, Loss: 1.055813, Accuracy: 69.98%\n",
      "Batch 206, Loss: 0.975093, Accuracy: 70.03%\n",
      "Batch 207, Loss: 1.054104, Accuracy: 70.04%\n",
      "Batch 208, Loss: 1.045848, Accuracy: 70.03%\n",
      "Batch 209, Loss: 1.076702, Accuracy: 70.01%\n",
      "Batch 210, Loss: 1.048710, Accuracy: 70.01%\n",
      "Batch 211, Loss: 1.060762, Accuracy: 70.01%\n",
      "Batch 212, Loss: 1.024441, Accuracy: 70.01%\n",
      "Batch 213, Loss: 1.050020, Accuracy: 70.00%\n",
      "Training - Epoch 14, Loss: 1.042803, Accuracy: 70.00%\n",
      "Validation Batch 1, Loss: 1.127575, Accuracy: 57.81%\n",
      "Validation Batch 2, Loss: 1.215262, Accuracy: 54.69%\n",
      "Validation Batch 3, Loss: 1.242331, Accuracy: 52.08%\n",
      "Validation Batch 4, Loss: 1.119902, Accuracy: 54.69%\n",
      "Validation Batch 5, Loss: 1.203868, Accuracy: 53.44%\n",
      "Validation Batch 6, Loss: 1.172677, Accuracy: 53.12%\n",
      "Validation Batch 7, Loss: 1.171980, Accuracy: 53.12%\n",
      "Validation Batch 8, Loss: 1.184021, Accuracy: 53.32%\n",
      "Validation Batch 9, Loss: 1.192681, Accuracy: 53.65%\n",
      "Validation Batch 10, Loss: 1.171499, Accuracy: 53.59%\n",
      "Validation Batch 11, Loss: 1.148046, Accuracy: 54.12%\n",
      "Validation Batch 12, Loss: 1.093297, Accuracy: 55.08%\n",
      "Validation Batch 13, Loss: 1.197469, Accuracy: 54.81%\n",
      "Validation Batch 14, Loss: 1.130409, Accuracy: 55.25%\n",
      "Validation Batch 15, Loss: 1.157238, Accuracy: 55.21%\n",
      "Validation Batch 16, Loss: 1.201008, Accuracy: 54.98%\n",
      "Validation Batch 17, Loss: 1.261336, Accuracy: 54.50%\n",
      "Validation Batch 18, Loss: 1.166629, Accuracy: 54.51%\n",
      "Validation Batch 19, Loss: 1.207281, Accuracy: 54.36%\n",
      "Validation Batch 20, Loss: 1.163197, Accuracy: 54.53%\n",
      "Validation Batch 21, Loss: 1.174078, Accuracy: 54.69%\n",
      "Validation Batch 22, Loss: 1.211231, Accuracy: 54.40%\n",
      "Validation Batch 23, Loss: 1.250502, Accuracy: 54.08%\n",
      "Validation Batch 24, Loss: 1.177012, Accuracy: 54.17%\n",
      "Validation Batch 25, Loss: 1.150700, Accuracy: 54.25%\n",
      "Validation Batch 26, Loss: 1.128530, Accuracy: 54.45%\n",
      "Validation Batch 27, Loss: 1.222646, Accuracy: 54.26%\n",
      "Validation - Epoch 14, Loss: 1.179348, Accuracy: 54.26%\n",
      "Patience—0\n",
      "Epoch 15\n",
      "Batch 1, Loss: 1.006428, Accuracy: 75.00%\n",
      "Batch 2, Loss: 1.059363, Accuracy: 71.09%\n",
      "Batch 3, Loss: 1.050722, Accuracy: 70.31%\n",
      "Batch 4, Loss: 1.045965, Accuracy: 69.92%\n",
      "Batch 5, Loss: 1.090268, Accuracy: 68.44%\n",
      "Batch 6, Loss: 0.970341, Accuracy: 69.53%\n",
      "Batch 7, Loss: 0.969532, Accuracy: 70.54%\n",
      "Batch 8, Loss: 1.011876, Accuracy: 70.90%\n",
      "Batch 9, Loss: 1.021661, Accuracy: 71.18%\n",
      "Batch 10, Loss: 1.068010, Accuracy: 71.09%\n",
      "Batch 11, Loss: 1.073500, Accuracy: 70.60%\n",
      "Batch 12, Loss: 1.011023, Accuracy: 70.57%\n",
      "Batch 13, Loss: 1.133338, Accuracy: 69.71%\n",
      "Batch 14, Loss: 1.010756, Accuracy: 70.09%\n",
      "Batch 15, Loss: 0.936486, Accuracy: 71.04%\n",
      "Batch 16, Loss: 1.106276, Accuracy: 70.51%\n",
      "Batch 17, Loss: 1.001950, Accuracy: 70.77%\n",
      "Batch 18, Loss: 1.072022, Accuracy: 70.57%\n",
      "Batch 19, Loss: 1.120170, Accuracy: 70.07%\n",
      "Batch 20, Loss: 0.965717, Accuracy: 70.55%\n",
      "Batch 21, Loss: 1.093958, Accuracy: 70.24%\n",
      "Batch 22, Loss: 1.130520, Accuracy: 69.89%\n",
      "Batch 23, Loss: 1.056495, Accuracy: 69.84%\n",
      "Batch 24, Loss: 1.040143, Accuracy: 69.92%\n",
      "Batch 25, Loss: 1.043860, Accuracy: 69.94%\n",
      "Batch 26, Loss: 1.051477, Accuracy: 69.83%\n",
      "Batch 27, Loss: 1.161308, Accuracy: 69.33%\n",
      "Batch 28, Loss: 1.066685, Accuracy: 69.25%\n",
      "Batch 29, Loss: 0.979925, Accuracy: 69.45%\n",
      "Batch 30, Loss: 1.060895, Accuracy: 69.48%\n",
      "Batch 31, Loss: 1.061785, Accuracy: 69.35%\n",
      "Batch 32, Loss: 1.127651, Accuracy: 69.04%\n",
      "Batch 33, Loss: 1.005263, Accuracy: 69.18%\n",
      "Batch 34, Loss: 1.108961, Accuracy: 68.98%\n",
      "Batch 35, Loss: 1.021361, Accuracy: 69.02%\n",
      "Batch 36, Loss: 0.991850, Accuracy: 69.23%\n",
      "Batch 37, Loss: 1.057138, Accuracy: 69.05%\n",
      "Batch 38, Loss: 1.104698, Accuracy: 68.87%\n",
      "Batch 39, Loss: 1.074365, Accuracy: 68.75%\n",
      "Batch 40, Loss: 1.119444, Accuracy: 68.59%\n",
      "Batch 41, Loss: 1.007407, Accuracy: 68.71%\n",
      "Batch 42, Loss: 0.998826, Accuracy: 68.90%\n",
      "Batch 43, Loss: 0.996510, Accuracy: 69.04%\n",
      "Batch 44, Loss: 1.112439, Accuracy: 68.86%\n",
      "Batch 45, Loss: 1.203973, Accuracy: 68.51%\n",
      "Batch 46, Loss: 1.081300, Accuracy: 68.48%\n",
      "Batch 47, Loss: 1.062121, Accuracy: 68.48%\n",
      "Batch 48, Loss: 1.038764, Accuracy: 68.52%\n",
      "Batch 49, Loss: 1.046025, Accuracy: 68.56%\n",
      "Batch 50, Loss: 1.094255, Accuracy: 68.47%\n",
      "Batch 51, Loss: 0.979150, Accuracy: 68.66%\n",
      "Batch 52, Loss: 1.084188, Accuracy: 68.60%\n",
      "Batch 53, Loss: 1.015926, Accuracy: 68.69%\n",
      "Batch 54, Loss: 0.933448, Accuracy: 68.95%\n",
      "Batch 55, Loss: 1.091816, Accuracy: 68.86%\n",
      "Batch 56, Loss: 1.014296, Accuracy: 68.95%\n",
      "Batch 57, Loss: 1.052671, Accuracy: 68.94%\n",
      "Batch 58, Loss: 1.127279, Accuracy: 68.80%\n",
      "Batch 59, Loss: 1.056559, Accuracy: 68.80%\n",
      "Batch 60, Loss: 0.980155, Accuracy: 68.91%\n",
      "Batch 61, Loss: 1.057622, Accuracy: 68.85%\n",
      "Batch 62, Loss: 1.043123, Accuracy: 68.88%\n",
      "Batch 63, Loss: 0.981218, Accuracy: 69.02%\n",
      "Batch 64, Loss: 1.073229, Accuracy: 68.99%\n",
      "Batch 65, Loss: 0.968674, Accuracy: 69.13%\n",
      "Batch 66, Loss: 0.994015, Accuracy: 69.20%\n",
      "Batch 67, Loss: 1.032009, Accuracy: 69.22%\n",
      "Batch 68, Loss: 0.887913, Accuracy: 69.49%\n",
      "Batch 69, Loss: 1.161959, Accuracy: 69.29%\n",
      "Batch 70, Loss: 0.990574, Accuracy: 69.38%\n",
      "Batch 71, Loss: 1.021809, Accuracy: 69.43%\n",
      "Batch 72, Loss: 1.077113, Accuracy: 69.38%\n",
      "Batch 73, Loss: 1.001368, Accuracy: 69.43%\n",
      "Batch 74, Loss: 1.117414, Accuracy: 69.32%\n",
      "Batch 75, Loss: 0.959849, Accuracy: 69.42%\n",
      "Batch 76, Loss: 1.002844, Accuracy: 69.49%\n",
      "Batch 77, Loss: 1.121540, Accuracy: 69.36%\n",
      "Batch 78, Loss: 1.029460, Accuracy: 69.35%\n",
      "Batch 79, Loss: 1.030034, Accuracy: 69.42%\n",
      "Batch 80, Loss: 1.136477, Accuracy: 69.30%\n",
      "Batch 81, Loss: 1.046609, Accuracy: 69.29%\n",
      "Batch 82, Loss: 1.101351, Accuracy: 69.21%\n",
      "Batch 83, Loss: 1.144239, Accuracy: 69.07%\n",
      "Batch 84, Loss: 0.953957, Accuracy: 69.22%\n",
      "Batch 85, Loss: 1.071453, Accuracy: 69.21%\n",
      "Batch 86, Loss: 1.025796, Accuracy: 69.24%\n",
      "Batch 87, Loss: 0.995006, Accuracy: 69.31%\n",
      "Batch 88, Loss: 1.036386, Accuracy: 69.34%\n",
      "Batch 89, Loss: 0.990189, Accuracy: 69.40%\n",
      "Batch 90, Loss: 1.032552, Accuracy: 69.43%\n",
      "Batch 91, Loss: 1.008687, Accuracy: 69.47%\n",
      "Batch 92, Loss: 1.065063, Accuracy: 69.46%\n",
      "Batch 93, Loss: 0.971648, Accuracy: 69.56%\n",
      "Batch 94, Loss: 1.078178, Accuracy: 69.50%\n",
      "Batch 95, Loss: 0.988634, Accuracy: 69.57%\n",
      "Batch 96, Loss: 1.016094, Accuracy: 69.58%\n",
      "Batch 97, Loss: 1.101036, Accuracy: 69.54%\n",
      "Batch 98, Loss: 1.168695, Accuracy: 69.39%\n",
      "Batch 99, Loss: 1.080760, Accuracy: 69.32%\n",
      "Batch 100, Loss: 0.972999, Accuracy: 69.42%\n",
      "Batch 101, Loss: 1.045282, Accuracy: 69.43%\n",
      "Batch 102, Loss: 1.033410, Accuracy: 69.44%\n",
      "Batch 103, Loss: 0.984746, Accuracy: 69.54%\n",
      "Batch 104, Loss: 1.010350, Accuracy: 69.59%\n",
      "Batch 105, Loss: 1.048620, Accuracy: 69.60%\n",
      "Batch 106, Loss: 1.021701, Accuracy: 69.62%\n",
      "Batch 107, Loss: 0.992044, Accuracy: 69.68%\n",
      "Batch 108, Loss: 1.032601, Accuracy: 69.69%\n",
      "Batch 109, Loss: 1.043306, Accuracy: 69.70%\n",
      "Batch 110, Loss: 0.987563, Accuracy: 69.77%\n",
      "Batch 111, Loss: 1.032949, Accuracy: 69.76%\n",
      "Batch 112, Loss: 1.019757, Accuracy: 69.78%\n",
      "Batch 113, Loss: 0.993716, Accuracy: 69.84%\n",
      "Batch 114, Loss: 1.042732, Accuracy: 69.85%\n",
      "Batch 115, Loss: 1.063407, Accuracy: 69.81%\n",
      "Batch 116, Loss: 0.901403, Accuracy: 69.95%\n",
      "Batch 117, Loss: 1.029023, Accuracy: 69.98%\n",
      "Batch 118, Loss: 1.051213, Accuracy: 69.95%\n",
      "Batch 119, Loss: 1.122950, Accuracy: 69.85%\n",
      "Batch 120, Loss: 1.086796, Accuracy: 69.80%\n",
      "Batch 121, Loss: 0.967002, Accuracy: 69.86%\n",
      "Batch 122, Loss: 1.001522, Accuracy: 69.89%\n",
      "Batch 123, Loss: 1.023988, Accuracy: 69.92%\n",
      "Batch 124, Loss: 1.073436, Accuracy: 69.90%\n",
      "Batch 125, Loss: 1.105760, Accuracy: 69.83%\n",
      "Batch 126, Loss: 1.055178, Accuracy: 69.83%\n",
      "Batch 127, Loss: 1.005512, Accuracy: 69.88%\n",
      "Batch 128, Loss: 1.101429, Accuracy: 69.81%\n",
      "Batch 129, Loss: 1.029090, Accuracy: 69.82%\n",
      "Batch 130, Loss: 1.125375, Accuracy: 69.70%\n",
      "Batch 131, Loss: 1.029735, Accuracy: 69.70%\n",
      "Batch 132, Loss: 1.034811, Accuracy: 69.71%\n",
      "Batch 133, Loss: 1.057657, Accuracy: 69.69%\n",
      "Batch 134, Loss: 0.978368, Accuracy: 69.74%\n",
      "Batch 135, Loss: 1.037257, Accuracy: 69.75%\n",
      "Batch 136, Loss: 1.077038, Accuracy: 69.73%\n",
      "Batch 137, Loss: 0.998817, Accuracy: 69.77%\n",
      "Batch 138, Loss: 1.053010, Accuracy: 69.77%\n",
      "Batch 139, Loss: 1.000673, Accuracy: 69.80%\n",
      "Batch 140, Loss: 1.027571, Accuracy: 69.81%\n",
      "Batch 141, Loss: 1.052213, Accuracy: 69.81%\n",
      "Batch 142, Loss: 0.998134, Accuracy: 69.85%\n",
      "Batch 143, Loss: 1.023022, Accuracy: 69.89%\n",
      "Batch 144, Loss: 1.002924, Accuracy: 69.94%\n",
      "Batch 145, Loss: 1.018402, Accuracy: 69.96%\n",
      "Batch 146, Loss: 1.051011, Accuracy: 69.95%\n",
      "Batch 147, Loss: 1.026348, Accuracy: 69.96%\n",
      "Batch 148, Loss: 1.057552, Accuracy: 69.94%\n",
      "Batch 149, Loss: 1.014998, Accuracy: 69.97%\n",
      "Batch 150, Loss: 1.022007, Accuracy: 69.96%\n",
      "Batch 151, Loss: 1.061548, Accuracy: 69.96%\n",
      "Batch 152, Loss: 1.025168, Accuracy: 69.96%\n",
      "Batch 153, Loss: 1.004809, Accuracy: 69.99%\n",
      "Batch 154, Loss: 1.028229, Accuracy: 70.00%\n",
      "Batch 155, Loss: 0.964710, Accuracy: 70.06%\n",
      "Batch 156, Loss: 1.063567, Accuracy: 70.03%\n",
      "Batch 157, Loss: 0.965255, Accuracy: 70.08%\n",
      "Batch 158, Loss: 1.100821, Accuracy: 70.06%\n",
      "Batch 159, Loss: 1.035637, Accuracy: 70.07%\n",
      "Batch 160, Loss: 1.029212, Accuracy: 70.07%\n",
      "Batch 161, Loss: 1.069346, Accuracy: 70.06%\n",
      "Batch 162, Loss: 0.935439, Accuracy: 70.13%\n",
      "Batch 163, Loss: 1.052992, Accuracy: 70.12%\n",
      "Batch 164, Loss: 1.073786, Accuracy: 70.11%\n",
      "Batch 165, Loss: 0.962958, Accuracy: 70.17%\n",
      "Batch 166, Loss: 1.017786, Accuracy: 70.18%\n",
      "Batch 167, Loss: 1.000605, Accuracy: 70.19%\n",
      "Batch 168, Loss: 1.031464, Accuracy: 70.20%\n",
      "Batch 169, Loss: 1.010758, Accuracy: 70.23%\n",
      "Batch 170, Loss: 1.025373, Accuracy: 70.25%\n",
      "Batch 171, Loss: 1.090124, Accuracy: 70.21%\n",
      "Batch 172, Loss: 1.148834, Accuracy: 70.15%\n",
      "Batch 173, Loss: 0.959210, Accuracy: 70.20%\n",
      "Batch 174, Loss: 1.026220, Accuracy: 70.20%\n",
      "Batch 175, Loss: 1.058749, Accuracy: 70.19%\n",
      "Batch 176, Loss: 1.018760, Accuracy: 70.21%\n",
      "Batch 177, Loss: 1.026229, Accuracy: 70.19%\n",
      "Batch 178, Loss: 0.980513, Accuracy: 70.22%\n",
      "Batch 179, Loss: 1.044113, Accuracy: 70.20%\n",
      "Batch 180, Loss: 1.021580, Accuracy: 70.21%\n",
      "Batch 181, Loss: 1.024368, Accuracy: 70.23%\n",
      "Batch 182, Loss: 1.005990, Accuracy: 70.26%\n",
      "Batch 183, Loss: 0.979444, Accuracy: 70.30%\n",
      "Batch 184, Loss: 1.091076, Accuracy: 70.26%\n",
      "Batch 185, Loss: 1.021395, Accuracy: 70.27%\n",
      "Batch 186, Loss: 1.103706, Accuracy: 70.21%\n",
      "Batch 187, Loss: 1.002470, Accuracy: 70.23%\n",
      "Batch 188, Loss: 1.072028, Accuracy: 70.21%\n",
      "Batch 189, Loss: 0.985025, Accuracy: 70.24%\n",
      "Batch 190, Loss: 1.012504, Accuracy: 70.26%\n",
      "Batch 191, Loss: 1.041136, Accuracy: 70.26%\n",
      "Batch 192, Loss: 1.114758, Accuracy: 70.21%\n",
      "Batch 193, Loss: 1.075764, Accuracy: 70.21%\n",
      "Batch 194, Loss: 1.009217, Accuracy: 70.22%\n",
      "Batch 195, Loss: 1.032251, Accuracy: 70.23%\n",
      "Batch 196, Loss: 0.984028, Accuracy: 70.26%\n",
      "Batch 197, Loss: 1.086813, Accuracy: 70.22%\n",
      "Batch 198, Loss: 1.033594, Accuracy: 70.23%\n",
      "Batch 199, Loss: 1.107498, Accuracy: 70.18%\n",
      "Batch 200, Loss: 0.994948, Accuracy: 70.20%\n",
      "Batch 201, Loss: 1.001533, Accuracy: 70.23%\n",
      "Batch 202, Loss: 0.979432, Accuracy: 70.27%\n",
      "Batch 203, Loss: 1.093473, Accuracy: 70.26%\n",
      "Batch 204, Loss: 1.083088, Accuracy: 70.23%\n",
      "Batch 205, Loss: 0.981572, Accuracy: 70.26%\n",
      "Batch 206, Loss: 1.085038, Accuracy: 70.24%\n",
      "Batch 207, Loss: 1.056037, Accuracy: 70.23%\n",
      "Batch 208, Loss: 1.049320, Accuracy: 70.23%\n",
      "Batch 209, Loss: 1.049325, Accuracy: 70.22%\n",
      "Batch 210, Loss: 1.102149, Accuracy: 70.17%\n",
      "Batch 211, Loss: 0.945127, Accuracy: 70.22%\n",
      "Batch 212, Loss: 1.046811, Accuracy: 70.22%\n",
      "Batch 213, Loss: 1.002467, Accuracy: 70.23%\n",
      "Training - Epoch 15, Loss: 1.038866, Accuracy: 70.23%\n",
      "Validation Batch 1, Loss: 1.106876, Accuracy: 62.50%\n",
      "Validation Batch 2, Loss: 1.196895, Accuracy: 58.59%\n",
      "Validation Batch 3, Loss: 1.217184, Accuracy: 55.73%\n",
      "Validation Batch 4, Loss: 1.096735, Accuracy: 57.81%\n",
      "Validation Batch 5, Loss: 1.185476, Accuracy: 56.56%\n",
      "Validation Batch 6, Loss: 1.147220, Accuracy: 56.77%\n",
      "Validation Batch 7, Loss: 1.150864, Accuracy: 56.47%\n",
      "Validation Batch 8, Loss: 1.174827, Accuracy: 56.45%\n",
      "Validation Batch 9, Loss: 1.176859, Accuracy: 56.25%\n",
      "Validation Batch 10, Loss: 1.144683, Accuracy: 56.41%\n",
      "Validation Batch 11, Loss: 1.120940, Accuracy: 56.96%\n",
      "Validation Batch 12, Loss: 1.076899, Accuracy: 57.81%\n",
      "Validation Batch 13, Loss: 1.184629, Accuracy: 57.57%\n",
      "Validation Batch 14, Loss: 1.115571, Accuracy: 57.92%\n",
      "Validation Batch 15, Loss: 1.137928, Accuracy: 57.92%\n",
      "Validation Batch 16, Loss: 1.180093, Accuracy: 57.52%\n",
      "Validation Batch 17, Loss: 1.241073, Accuracy: 56.89%\n",
      "Validation Batch 18, Loss: 1.145043, Accuracy: 56.77%\n",
      "Validation Batch 19, Loss: 1.186768, Accuracy: 56.58%\n",
      "Validation Batch 20, Loss: 1.140461, Accuracy: 56.80%\n",
      "Validation Batch 21, Loss: 1.155137, Accuracy: 56.85%\n",
      "Validation Batch 22, Loss: 1.195649, Accuracy: 56.61%\n",
      "Validation Batch 23, Loss: 1.230771, Accuracy: 56.32%\n",
      "Validation Batch 24, Loss: 1.150640, Accuracy: 56.45%\n",
      "Validation Batch 25, Loss: 1.136768, Accuracy: 56.44%\n",
      "Validation Batch 26, Loss: 1.114914, Accuracy: 56.55%\n",
      "Validation Batch 27, Loss: 1.197067, Accuracy: 56.31%\n",
      "Validation - Epoch 15, Loss: 1.159554, Accuracy: 56.31%\n",
      "Patience—0\n",
      "Epoch 16\n",
      "Batch 1, Loss: 0.968033, Accuracy: 75.00%\n",
      "Batch 2, Loss: 1.007579, Accuracy: 73.44%\n",
      "Batch 3, Loss: 0.996840, Accuracy: 73.96%\n",
      "Batch 4, Loss: 1.026861, Accuracy: 73.83%\n",
      "Batch 5, Loss: 0.984972, Accuracy: 74.69%\n",
      "Batch 6, Loss: 1.096451, Accuracy: 72.92%\n",
      "Batch 7, Loss: 0.985030, Accuracy: 73.44%\n",
      "Batch 8, Loss: 1.009212, Accuracy: 73.05%\n",
      "Batch 9, Loss: 1.133094, Accuracy: 71.35%\n",
      "Batch 10, Loss: 1.023303, Accuracy: 71.25%\n",
      "Batch 11, Loss: 1.059232, Accuracy: 70.74%\n",
      "Batch 12, Loss: 0.997100, Accuracy: 71.22%\n",
      "Batch 13, Loss: 1.070159, Accuracy: 70.67%\n",
      "Batch 14, Loss: 0.983619, Accuracy: 71.09%\n",
      "Batch 15, Loss: 1.097924, Accuracy: 70.62%\n",
      "Batch 16, Loss: 1.070595, Accuracy: 70.41%\n",
      "Batch 17, Loss: 1.063011, Accuracy: 70.22%\n",
      "Batch 18, Loss: 1.023987, Accuracy: 70.49%\n",
      "Batch 19, Loss: 0.999504, Accuracy: 70.81%\n",
      "Batch 20, Loss: 1.036276, Accuracy: 70.78%\n",
      "Batch 21, Loss: 1.046004, Accuracy: 70.76%\n",
      "Batch 22, Loss: 1.054272, Accuracy: 70.60%\n",
      "Batch 23, Loss: 0.989211, Accuracy: 70.86%\n",
      "Batch 24, Loss: 1.035363, Accuracy: 70.83%\n",
      "Batch 25, Loss: 1.057343, Accuracy: 70.75%\n",
      "Batch 26, Loss: 1.069116, Accuracy: 70.61%\n",
      "Batch 27, Loss: 0.998097, Accuracy: 70.78%\n",
      "Batch 28, Loss: 1.013822, Accuracy: 70.87%\n",
      "Batch 29, Loss: 1.124680, Accuracy: 70.47%\n",
      "Batch 30, Loss: 1.059040, Accuracy: 70.36%\n",
      "Batch 31, Loss: 1.067489, Accuracy: 70.21%\n",
      "Batch 32, Loss: 0.981554, Accuracy: 70.46%\n",
      "Batch 33, Loss: 1.024006, Accuracy: 70.50%\n",
      "Batch 34, Loss: 1.003163, Accuracy: 70.59%\n",
      "Batch 35, Loss: 1.033534, Accuracy: 70.54%\n",
      "Batch 36, Loss: 1.087182, Accuracy: 70.31%\n",
      "Batch 37, Loss: 1.033023, Accuracy: 70.40%\n",
      "Batch 38, Loss: 0.959555, Accuracy: 70.64%\n",
      "Batch 39, Loss: 0.991575, Accuracy: 70.75%\n",
      "Batch 40, Loss: 0.987003, Accuracy: 70.90%\n",
      "Batch 41, Loss: 1.093375, Accuracy: 70.73%\n",
      "Batch 42, Loss: 0.941768, Accuracy: 70.91%\n",
      "Batch 43, Loss: 1.025002, Accuracy: 70.93%\n",
      "Batch 44, Loss: 1.007412, Accuracy: 71.02%\n",
      "Batch 45, Loss: 1.089599, Accuracy: 70.83%\n",
      "Batch 46, Loss: 1.126161, Accuracy: 70.58%\n",
      "Batch 47, Loss: 0.957413, Accuracy: 70.71%\n",
      "Batch 48, Loss: 1.119651, Accuracy: 70.54%\n",
      "Batch 49, Loss: 1.075974, Accuracy: 70.44%\n",
      "Batch 50, Loss: 0.967668, Accuracy: 70.56%\n",
      "Batch 51, Loss: 1.010465, Accuracy: 70.59%\n",
      "Batch 52, Loss: 1.075844, Accuracy: 70.49%\n",
      "Batch 53, Loss: 1.055305, Accuracy: 70.49%\n",
      "Batch 54, Loss: 1.024497, Accuracy: 70.52%\n",
      "Batch 55, Loss: 1.102893, Accuracy: 70.40%\n",
      "Batch 56, Loss: 1.087117, Accuracy: 70.31%\n",
      "Batch 57, Loss: 1.091131, Accuracy: 70.23%\n",
      "Batch 58, Loss: 1.067626, Accuracy: 70.12%\n",
      "Batch 59, Loss: 0.946419, Accuracy: 70.29%\n",
      "Batch 60, Loss: 1.007849, Accuracy: 70.31%\n",
      "Batch 61, Loss: 1.132123, Accuracy: 70.16%\n",
      "Batch 62, Loss: 1.017909, Accuracy: 70.16%\n",
      "Batch 63, Loss: 0.999413, Accuracy: 70.24%\n",
      "Batch 64, Loss: 1.078023, Accuracy: 70.17%\n",
      "Batch 65, Loss: 1.033062, Accuracy: 70.17%\n",
      "Batch 66, Loss: 1.065307, Accuracy: 70.12%\n",
      "Batch 67, Loss: 0.979030, Accuracy: 70.27%\n",
      "Batch 68, Loss: 1.082202, Accuracy: 70.22%\n",
      "Batch 69, Loss: 1.066910, Accuracy: 70.18%\n",
      "Batch 70, Loss: 1.054733, Accuracy: 70.18%\n",
      "Batch 71, Loss: 1.115394, Accuracy: 70.05%\n",
      "Batch 72, Loss: 1.006451, Accuracy: 70.10%\n",
      "Batch 73, Loss: 0.990045, Accuracy: 70.21%\n",
      "Batch 74, Loss: 1.031589, Accuracy: 70.23%\n",
      "Batch 75, Loss: 1.011610, Accuracy: 70.29%\n",
      "Batch 76, Loss: 1.039288, Accuracy: 70.31%\n",
      "Batch 77, Loss: 1.008565, Accuracy: 70.35%\n",
      "Batch 78, Loss: 1.068888, Accuracy: 70.31%\n",
      "Batch 79, Loss: 1.052969, Accuracy: 70.33%\n",
      "Batch 80, Loss: 0.975292, Accuracy: 70.45%\n",
      "Batch 81, Loss: 1.050535, Accuracy: 70.41%\n",
      "Batch 82, Loss: 1.047093, Accuracy: 70.41%\n",
      "Batch 83, Loss: 1.006295, Accuracy: 70.46%\n",
      "Batch 84, Loss: 1.107700, Accuracy: 70.39%\n",
      "Batch 85, Loss: 1.171624, Accuracy: 70.22%\n",
      "Batch 86, Loss: 0.946391, Accuracy: 70.35%\n",
      "Batch 87, Loss: 1.044023, Accuracy: 70.33%\n",
      "Batch 88, Loss: 1.026048, Accuracy: 70.37%\n",
      "Batch 89, Loss: 0.967020, Accuracy: 70.44%\n",
      "Batch 90, Loss: 1.053274, Accuracy: 70.42%\n",
      "Batch 91, Loss: 0.967640, Accuracy: 70.52%\n",
      "Batch 92, Loss: 0.996395, Accuracy: 70.55%\n",
      "Batch 93, Loss: 1.027984, Accuracy: 70.56%\n",
      "Batch 94, Loss: 1.009152, Accuracy: 70.56%\n",
      "Batch 95, Loss: 1.017848, Accuracy: 70.61%\n",
      "Batch 96, Loss: 1.100067, Accuracy: 70.52%\n",
      "Batch 97, Loss: 1.037651, Accuracy: 70.52%\n",
      "Batch 98, Loss: 1.047425, Accuracy: 70.47%\n",
      "Batch 99, Loss: 1.061544, Accuracy: 70.44%\n",
      "Batch 100, Loss: 0.985740, Accuracy: 70.50%\n",
      "Batch 101, Loss: 0.990941, Accuracy: 70.54%\n",
      "Batch 102, Loss: 1.064550, Accuracy: 70.54%\n",
      "Batch 103, Loss: 0.986799, Accuracy: 70.62%\n",
      "Batch 104, Loss: 1.003858, Accuracy: 70.63%\n",
      "Batch 105, Loss: 0.965769, Accuracy: 70.70%\n",
      "Batch 106, Loss: 1.100333, Accuracy: 70.64%\n",
      "Batch 107, Loss: 1.020974, Accuracy: 70.65%\n",
      "Batch 108, Loss: 1.025024, Accuracy: 70.63%\n",
      "Batch 109, Loss: 1.082470, Accuracy: 70.58%\n",
      "Batch 110, Loss: 1.028744, Accuracy: 70.58%\n",
      "Batch 111, Loss: 0.980982, Accuracy: 70.64%\n",
      "Batch 112, Loss: 1.052082, Accuracy: 70.63%\n",
      "Batch 113, Loss: 1.057175, Accuracy: 70.63%\n",
      "Batch 114, Loss: 1.002183, Accuracy: 70.63%\n",
      "Batch 115, Loss: 1.069533, Accuracy: 70.60%\n",
      "Batch 116, Loss: 1.062148, Accuracy: 70.57%\n",
      "Batch 117, Loss: 1.002689, Accuracy: 70.59%\n",
      "Batch 118, Loss: 1.034225, Accuracy: 70.59%\n",
      "Batch 119, Loss: 1.054557, Accuracy: 70.54%\n",
      "Batch 120, Loss: 0.949680, Accuracy: 70.60%\n",
      "Batch 121, Loss: 0.965629, Accuracy: 70.66%\n",
      "Batch 122, Loss: 1.088312, Accuracy: 70.62%\n",
      "Batch 123, Loss: 0.964952, Accuracy: 70.66%\n",
      "Batch 124, Loss: 1.081478, Accuracy: 70.63%\n",
      "Batch 125, Loss: 1.006001, Accuracy: 70.65%\n",
      "Batch 126, Loss: 1.055272, Accuracy: 70.63%\n",
      "Batch 127, Loss: 1.015532, Accuracy: 70.66%\n",
      "Batch 128, Loss: 1.042297, Accuracy: 70.65%\n",
      "Batch 129, Loss: 1.030530, Accuracy: 70.66%\n",
      "Batch 130, Loss: 1.012270, Accuracy: 70.69%\n",
      "Batch 131, Loss: 1.024163, Accuracy: 70.69%\n",
      "Batch 132, Loss: 1.033659, Accuracy: 70.68%\n",
      "Batch 133, Loss: 1.039667, Accuracy: 70.70%\n",
      "Batch 134, Loss: 1.070719, Accuracy: 70.67%\n",
      "Batch 135, Loss: 1.009996, Accuracy: 70.67%\n",
      "Batch 136, Loss: 1.043330, Accuracy: 70.69%\n",
      "Batch 137, Loss: 1.011784, Accuracy: 70.72%\n",
      "Batch 138, Loss: 1.034697, Accuracy: 70.72%\n",
      "Batch 139, Loss: 1.025774, Accuracy: 70.73%\n",
      "Batch 140, Loss: 1.080051, Accuracy: 70.69%\n",
      "Batch 141, Loss: 1.085764, Accuracy: 70.66%\n",
      "Batch 142, Loss: 0.923714, Accuracy: 70.73%\n",
      "Batch 143, Loss: 0.914914, Accuracy: 70.83%\n",
      "Batch 144, Loss: 1.095777, Accuracy: 70.79%\n",
      "Batch 145, Loss: 0.976379, Accuracy: 70.84%\n",
      "Batch 146, Loss: 1.068936, Accuracy: 70.82%\n",
      "Batch 147, Loss: 0.988729, Accuracy: 70.84%\n",
      "Batch 148, Loss: 1.022936, Accuracy: 70.86%\n",
      "Batch 149, Loss: 1.096868, Accuracy: 70.84%\n",
      "Batch 150, Loss: 1.001843, Accuracy: 70.90%\n",
      "Batch 151, Loss: 0.981169, Accuracy: 70.94%\n",
      "Batch 152, Loss: 1.049859, Accuracy: 70.92%\n",
      "Batch 153, Loss: 1.067150, Accuracy: 70.88%\n",
      "Batch 154, Loss: 1.004535, Accuracy: 70.89%\n",
      "Batch 155, Loss: 1.041056, Accuracy: 70.88%\n",
      "Batch 156, Loss: 1.069114, Accuracy: 70.85%\n",
      "Batch 157, Loss: 1.050693, Accuracy: 70.84%\n",
      "Batch 158, Loss: 1.102993, Accuracy: 70.80%\n",
      "Batch 159, Loss: 1.082340, Accuracy: 70.77%\n",
      "Batch 160, Loss: 0.960588, Accuracy: 70.83%\n",
      "Batch 161, Loss: 1.043883, Accuracy: 70.82%\n",
      "Batch 162, Loss: 0.997681, Accuracy: 70.83%\n",
      "Batch 163, Loss: 0.951108, Accuracy: 70.91%\n",
      "Batch 164, Loss: 1.067963, Accuracy: 70.88%\n",
      "Batch 165, Loss: 1.047118, Accuracy: 70.88%\n",
      "Batch 166, Loss: 1.048598, Accuracy: 70.88%\n",
      "Batch 167, Loss: 1.102514, Accuracy: 70.83%\n",
      "Batch 168, Loss: 1.042932, Accuracy: 70.81%\n",
      "Batch 169, Loss: 1.089550, Accuracy: 70.77%\n",
      "Batch 170, Loss: 1.027649, Accuracy: 70.78%\n",
      "Batch 171, Loss: 1.036497, Accuracy: 70.79%\n",
      "Batch 172, Loss: 0.979372, Accuracy: 70.82%\n",
      "Batch 173, Loss: 1.016890, Accuracy: 70.85%\n",
      "Batch 174, Loss: 1.105978, Accuracy: 70.80%\n",
      "Batch 175, Loss: 1.033242, Accuracy: 70.79%\n",
      "Batch 176, Loss: 0.991284, Accuracy: 70.79%\n",
      "Batch 177, Loss: 1.100190, Accuracy: 70.75%\n",
      "Batch 178, Loss: 0.986944, Accuracy: 70.78%\n",
      "Batch 179, Loss: 1.094908, Accuracy: 70.74%\n",
      "Batch 180, Loss: 1.014322, Accuracy: 70.76%\n",
      "Batch 181, Loss: 1.077828, Accuracy: 70.73%\n",
      "Batch 182, Loss: 1.046472, Accuracy: 70.72%\n",
      "Batch 183, Loss: 1.008112, Accuracy: 70.72%\n",
      "Batch 184, Loss: 1.046698, Accuracy: 70.70%\n",
      "Batch 185, Loss: 0.974350, Accuracy: 70.74%\n",
      "Batch 186, Loss: 1.020030, Accuracy: 70.75%\n",
      "Batch 187, Loss: 0.939661, Accuracy: 70.80%\n",
      "Batch 188, Loss: 1.155533, Accuracy: 70.71%\n",
      "Batch 189, Loss: 0.992554, Accuracy: 70.75%\n",
      "Batch 190, Loss: 0.944907, Accuracy: 70.80%\n",
      "Batch 191, Loss: 1.020948, Accuracy: 70.80%\n",
      "Batch 192, Loss: 1.064140, Accuracy: 70.79%\n",
      "Batch 193, Loss: 1.075802, Accuracy: 70.77%\n",
      "Batch 194, Loss: 0.996749, Accuracy: 70.78%\n",
      "Batch 195, Loss: 0.973110, Accuracy: 70.83%\n",
      "Batch 196, Loss: 0.948369, Accuracy: 70.88%\n",
      "Batch 197, Loss: 1.000371, Accuracy: 70.89%\n",
      "Batch 198, Loss: 1.060119, Accuracy: 70.88%\n",
      "Batch 199, Loss: 1.002904, Accuracy: 70.90%\n",
      "Batch 200, Loss: 0.983500, Accuracy: 70.93%\n",
      "Batch 201, Loss: 1.019929, Accuracy: 70.93%\n",
      "Batch 202, Loss: 1.000935, Accuracy: 70.95%\n",
      "Batch 203, Loss: 1.088425, Accuracy: 70.93%\n",
      "Batch 204, Loss: 1.024178, Accuracy: 70.93%\n",
      "Batch 205, Loss: 1.091381, Accuracy: 70.91%\n",
      "Batch 206, Loss: 1.004746, Accuracy: 70.93%\n",
      "Batch 207, Loss: 0.938487, Accuracy: 71.01%\n",
      "Batch 208, Loss: 1.081665, Accuracy: 70.97%\n",
      "Batch 209, Loss: 1.102005, Accuracy: 70.94%\n",
      "Batch 210, Loss: 1.016184, Accuracy: 70.94%\n",
      "Batch 211, Loss: 0.965857, Accuracy: 70.96%\n",
      "Batch 212, Loss: 1.039425, Accuracy: 70.96%\n",
      "Batch 213, Loss: 1.026168, Accuracy: 70.97%\n",
      "Training - Epoch 16, Loss: 1.032242, Accuracy: 70.97%\n",
      "Validation Batch 1, Loss: 1.091149, Accuracy: 65.62%\n",
      "Validation Batch 2, Loss: 1.184844, Accuracy: 60.94%\n",
      "Validation Batch 3, Loss: 1.196567, Accuracy: 58.33%\n",
      "Validation Batch 4, Loss: 1.086369, Accuracy: 60.16%\n",
      "Validation Batch 5, Loss: 1.169616, Accuracy: 58.75%\n",
      "Validation Batch 6, Loss: 1.128771, Accuracy: 58.59%\n",
      "Validation Batch 7, Loss: 1.136470, Accuracy: 58.26%\n",
      "Validation Batch 8, Loss: 1.163403, Accuracy: 58.59%\n",
      "Validation Batch 9, Loss: 1.159441, Accuracy: 58.68%\n",
      "Validation Batch 10, Loss: 1.123262, Accuracy: 58.59%\n",
      "Validation Batch 11, Loss: 1.100036, Accuracy: 59.09%\n",
      "Validation Batch 12, Loss: 1.061757, Accuracy: 59.77%\n",
      "Validation Batch 13, Loss: 1.173885, Accuracy: 59.50%\n",
      "Validation Batch 14, Loss: 1.102108, Accuracy: 59.71%\n",
      "Validation Batch 15, Loss: 1.120391, Accuracy: 59.69%\n",
      "Validation Batch 16, Loss: 1.158705, Accuracy: 59.38%\n",
      "Validation Batch 17, Loss: 1.219807, Accuracy: 58.73%\n",
      "Validation Batch 18, Loss: 1.127290, Accuracy: 58.59%\n",
      "Validation Batch 19, Loss: 1.174022, Accuracy: 58.39%\n",
      "Validation Batch 20, Loss: 1.126369, Accuracy: 58.52%\n",
      "Validation Batch 21, Loss: 1.138936, Accuracy: 58.48%\n",
      "Validation Batch 22, Loss: 1.185687, Accuracy: 58.17%\n",
      "Validation Batch 23, Loss: 1.217117, Accuracy: 57.81%\n",
      "Validation Batch 24, Loss: 1.132609, Accuracy: 58.07%\n",
      "Validation Batch 25, Loss: 1.125011, Accuracy: 58.00%\n",
      "Validation Batch 26, Loss: 1.101463, Accuracy: 58.17%\n",
      "Validation Batch 27, Loss: 1.163800, Accuracy: 58.25%\n",
      "Validation - Epoch 16, Loss: 1.143292, Accuracy: 58.25%\n",
      "Patience—0\n",
      "Epoch 17\n",
      "Batch 1, Loss: 1.015418, Accuracy: 73.44%\n",
      "Batch 2, Loss: 1.058106, Accuracy: 71.09%\n",
      "Batch 3, Loss: 1.093231, Accuracy: 68.75%\n",
      "Batch 4, Loss: 1.016527, Accuracy: 70.31%\n",
      "Batch 5, Loss: 1.070917, Accuracy: 69.69%\n",
      "Batch 6, Loss: 1.099201, Accuracy: 68.49%\n",
      "Batch 7, Loss: 1.057079, Accuracy: 68.53%\n",
      "Batch 8, Loss: 0.997415, Accuracy: 68.95%\n",
      "Batch 9, Loss: 0.984131, Accuracy: 69.97%\n",
      "Batch 10, Loss: 1.010986, Accuracy: 70.47%\n",
      "Batch 11, Loss: 1.051347, Accuracy: 70.45%\n",
      "Batch 12, Loss: 1.080896, Accuracy: 69.92%\n",
      "Batch 13, Loss: 1.022706, Accuracy: 69.95%\n",
      "Batch 14, Loss: 0.999265, Accuracy: 70.31%\n",
      "Batch 15, Loss: 1.024837, Accuracy: 70.31%\n",
      "Batch 16, Loss: 1.049822, Accuracy: 70.21%\n",
      "Batch 17, Loss: 1.035381, Accuracy: 70.22%\n",
      "Batch 18, Loss: 1.004730, Accuracy: 70.66%\n",
      "Batch 19, Loss: 0.941505, Accuracy: 71.13%\n",
      "Batch 20, Loss: 0.971052, Accuracy: 71.56%\n",
      "Batch 21, Loss: 0.940859, Accuracy: 71.95%\n",
      "Batch 22, Loss: 0.898320, Accuracy: 72.59%\n",
      "Batch 23, Loss: 0.982132, Accuracy: 72.76%\n",
      "Batch 24, Loss: 1.028497, Accuracy: 72.53%\n",
      "Batch 25, Loss: 1.017541, Accuracy: 72.62%\n",
      "Batch 26, Loss: 0.972192, Accuracy: 72.84%\n",
      "Batch 27, Loss: 1.029993, Accuracy: 72.92%\n",
      "Batch 28, Loss: 1.072043, Accuracy: 72.60%\n",
      "Batch 29, Loss: 1.068571, Accuracy: 72.36%\n",
      "Batch 30, Loss: 1.001773, Accuracy: 72.45%\n",
      "Batch 31, Loss: 1.027068, Accuracy: 72.43%\n",
      "Batch 32, Loss: 1.025186, Accuracy: 72.46%\n",
      "Batch 33, Loss: 1.041439, Accuracy: 72.40%\n",
      "Batch 34, Loss: 1.030423, Accuracy: 72.47%\n",
      "Batch 35, Loss: 1.070246, Accuracy: 72.28%\n",
      "Batch 36, Loss: 1.030458, Accuracy: 72.27%\n",
      "Batch 37, Loss: 0.982222, Accuracy: 72.38%\n",
      "Batch 38, Loss: 0.961050, Accuracy: 72.57%\n",
      "Batch 39, Loss: 1.115055, Accuracy: 72.32%\n",
      "Batch 40, Loss: 1.035181, Accuracy: 72.27%\n",
      "Batch 41, Loss: 0.993139, Accuracy: 72.33%\n",
      "Batch 42, Loss: 0.950237, Accuracy: 72.47%\n",
      "Batch 43, Loss: 1.050883, Accuracy: 72.42%\n",
      "Batch 44, Loss: 1.038421, Accuracy: 72.37%\n",
      "Batch 45, Loss: 0.988580, Accuracy: 72.47%\n",
      "Batch 46, Loss: 1.126507, Accuracy: 72.15%\n",
      "Batch 47, Loss: 1.065229, Accuracy: 72.04%\n",
      "Batch 48, Loss: 1.002304, Accuracy: 72.10%\n",
      "Batch 49, Loss: 1.057531, Accuracy: 72.03%\n",
      "Batch 50, Loss: 1.041040, Accuracy: 72.00%\n",
      "Batch 51, Loss: 0.995095, Accuracy: 72.03%\n",
      "Batch 52, Loss: 1.039469, Accuracy: 72.00%\n",
      "Batch 53, Loss: 1.094526, Accuracy: 71.85%\n",
      "Batch 54, Loss: 1.080520, Accuracy: 71.76%\n",
      "Batch 55, Loss: 1.052772, Accuracy: 71.70%\n",
      "Batch 56, Loss: 1.018207, Accuracy: 71.68%\n",
      "Batch 57, Loss: 0.982633, Accuracy: 71.77%\n",
      "Batch 58, Loss: 1.036994, Accuracy: 71.77%\n",
      "Batch 59, Loss: 0.999438, Accuracy: 71.80%\n",
      "Batch 60, Loss: 1.018957, Accuracy: 71.74%\n",
      "Batch 61, Loss: 0.945508, Accuracy: 71.93%\n",
      "Batch 62, Loss: 1.019520, Accuracy: 71.98%\n",
      "Batch 63, Loss: 0.999571, Accuracy: 72.02%\n",
      "Batch 64, Loss: 1.055481, Accuracy: 71.97%\n",
      "Batch 65, Loss: 1.068594, Accuracy: 71.88%\n",
      "Batch 66, Loss: 1.026576, Accuracy: 71.85%\n",
      "Batch 67, Loss: 1.078832, Accuracy: 71.76%\n",
      "Batch 68, Loss: 1.088230, Accuracy: 71.65%\n",
      "Batch 69, Loss: 1.092535, Accuracy: 71.54%\n",
      "Batch 70, Loss: 1.051135, Accuracy: 71.47%\n",
      "Batch 71, Loss: 1.060197, Accuracy: 71.39%\n",
      "Batch 72, Loss: 0.995721, Accuracy: 71.44%\n",
      "Batch 73, Loss: 0.993428, Accuracy: 71.49%\n",
      "Batch 74, Loss: 0.946114, Accuracy: 71.62%\n",
      "Batch 75, Loss: 1.085638, Accuracy: 71.50%\n",
      "Batch 76, Loss: 1.119278, Accuracy: 71.38%\n",
      "Batch 77, Loss: 1.025143, Accuracy: 71.43%\n",
      "Batch 78, Loss: 1.062404, Accuracy: 71.39%\n",
      "Batch 79, Loss: 1.092024, Accuracy: 71.30%\n",
      "Batch 80, Loss: 1.118114, Accuracy: 71.19%\n",
      "Batch 81, Loss: 1.057323, Accuracy: 71.16%\n",
      "Batch 82, Loss: 0.992002, Accuracy: 71.23%\n",
      "Batch 83, Loss: 1.009132, Accuracy: 71.22%\n",
      "Batch 84, Loss: 1.017573, Accuracy: 71.24%\n",
      "Batch 85, Loss: 1.001291, Accuracy: 71.23%\n",
      "Batch 86, Loss: 0.962704, Accuracy: 71.31%\n",
      "Batch 87, Loss: 1.023911, Accuracy: 71.34%\n",
      "Batch 88, Loss: 1.009243, Accuracy: 71.38%\n",
      "Batch 89, Loss: 1.050888, Accuracy: 71.38%\n",
      "Batch 90, Loss: 0.960925, Accuracy: 71.44%\n",
      "Batch 91, Loss: 0.992995, Accuracy: 71.48%\n",
      "Batch 92, Loss: 1.038943, Accuracy: 71.47%\n",
      "Batch 93, Loss: 0.959514, Accuracy: 71.54%\n",
      "Batch 94, Loss: 0.996465, Accuracy: 71.59%\n",
      "Batch 95, Loss: 1.088289, Accuracy: 71.51%\n",
      "Batch 96, Loss: 0.900960, Accuracy: 71.66%\n",
      "Batch 97, Loss: 1.008334, Accuracy: 71.65%\n",
      "Batch 98, Loss: 1.059208, Accuracy: 71.59%\n",
      "Batch 99, Loss: 1.032454, Accuracy: 71.56%\n",
      "Batch 100, Loss: 1.104917, Accuracy: 71.50%\n",
      "Batch 101, Loss: 1.085314, Accuracy: 71.44%\n",
      "Batch 102, Loss: 1.008535, Accuracy: 71.45%\n",
      "Batch 103, Loss: 1.094771, Accuracy: 71.37%\n",
      "Batch 104, Loss: 1.081411, Accuracy: 71.32%\n",
      "Batch 105, Loss: 0.984398, Accuracy: 71.34%\n",
      "Batch 106, Loss: 1.091399, Accuracy: 71.29%\n",
      "Batch 107, Loss: 1.039891, Accuracy: 71.25%\n",
      "Batch 108, Loss: 1.092960, Accuracy: 71.18%\n",
      "Batch 109, Loss: 1.102771, Accuracy: 71.13%\n",
      "Batch 110, Loss: 0.994160, Accuracy: 71.18%\n",
      "Batch 111, Loss: 1.050109, Accuracy: 71.17%\n",
      "Batch 112, Loss: 1.007818, Accuracy: 71.19%\n",
      "Batch 113, Loss: 1.003137, Accuracy: 71.21%\n",
      "Batch 114, Loss: 0.974296, Accuracy: 71.26%\n",
      "Batch 115, Loss: 0.986053, Accuracy: 71.32%\n",
      "Batch 116, Loss: 1.036748, Accuracy: 71.30%\n",
      "Batch 117, Loss: 1.011190, Accuracy: 71.33%\n",
      "Batch 118, Loss: 1.063458, Accuracy: 71.29%\n",
      "Batch 119, Loss: 1.169125, Accuracy: 71.17%\n",
      "Batch 120, Loss: 0.964521, Accuracy: 71.21%\n",
      "Batch 121, Loss: 0.990625, Accuracy: 71.23%\n",
      "Batch 122, Loss: 1.009433, Accuracy: 71.25%\n",
      "Batch 123, Loss: 1.023332, Accuracy: 71.27%\n",
      "Batch 124, Loss: 1.073515, Accuracy: 71.21%\n",
      "Batch 125, Loss: 1.064409, Accuracy: 71.19%\n",
      "Batch 126, Loss: 0.950648, Accuracy: 71.27%\n",
      "Batch 127, Loss: 0.965067, Accuracy: 71.33%\n",
      "Batch 128, Loss: 1.088176, Accuracy: 71.26%\n",
      "Batch 129, Loss: 1.015665, Accuracy: 71.28%\n",
      "Batch 130, Loss: 1.056249, Accuracy: 71.25%\n",
      "Batch 131, Loss: 1.040793, Accuracy: 71.23%\n",
      "Batch 132, Loss: 1.069145, Accuracy: 71.20%\n",
      "Batch 133, Loss: 0.995028, Accuracy: 71.23%\n",
      "Batch 134, Loss: 0.955914, Accuracy: 71.30%\n",
      "Batch 135, Loss: 1.003086, Accuracy: 71.34%\n",
      "Batch 136, Loss: 0.939299, Accuracy: 71.42%\n",
      "Batch 137, Loss: 0.953016, Accuracy: 71.48%\n",
      "Batch 138, Loss: 1.002996, Accuracy: 71.50%\n",
      "Batch 139, Loss: 1.096803, Accuracy: 71.44%\n",
      "Batch 140, Loss: 0.978939, Accuracy: 71.50%\n",
      "Batch 141, Loss: 1.043106, Accuracy: 71.50%\n",
      "Batch 142, Loss: 0.934456, Accuracy: 71.58%\n",
      "Batch 143, Loss: 1.049663, Accuracy: 71.56%\n",
      "Batch 144, Loss: 1.062676, Accuracy: 71.54%\n",
      "Batch 145, Loss: 1.030802, Accuracy: 71.53%\n",
      "Batch 146, Loss: 0.979515, Accuracy: 71.54%\n",
      "Batch 147, Loss: 1.011334, Accuracy: 71.55%\n",
      "Batch 148, Loss: 1.053213, Accuracy: 71.53%\n",
      "Batch 149, Loss: 1.039183, Accuracy: 71.53%\n",
      "Batch 150, Loss: 1.004365, Accuracy: 71.54%\n",
      "Batch 151, Loss: 1.020532, Accuracy: 71.55%\n",
      "Batch 152, Loss: 1.074778, Accuracy: 71.53%\n",
      "Batch 153, Loss: 1.092633, Accuracy: 71.46%\n",
      "Batch 154, Loss: 1.047949, Accuracy: 71.43%\n",
      "Batch 155, Loss: 0.987548, Accuracy: 71.46%\n",
      "Batch 156, Loss: 1.071242, Accuracy: 71.41%\n",
      "Batch 157, Loss: 0.999006, Accuracy: 71.43%\n",
      "Batch 158, Loss: 1.055695, Accuracy: 71.42%\n",
      "Batch 159, Loss: 1.056949, Accuracy: 71.40%\n",
      "Batch 160, Loss: 0.984721, Accuracy: 71.44%\n",
      "Batch 161, Loss: 1.066266, Accuracy: 71.40%\n",
      "Batch 162, Loss: 1.050771, Accuracy: 71.39%\n",
      "Batch 163, Loss: 1.089970, Accuracy: 71.34%\n",
      "Batch 164, Loss: 0.979747, Accuracy: 71.36%\n",
      "Batch 165, Loss: 1.001099, Accuracy: 71.37%\n",
      "Batch 166, Loss: 1.167468, Accuracy: 71.29%\n",
      "Batch 167, Loss: 0.967418, Accuracy: 71.34%\n",
      "Batch 168, Loss: 1.048218, Accuracy: 71.34%\n",
      "Batch 169, Loss: 0.998583, Accuracy: 71.36%\n",
      "Batch 170, Loss: 1.012703, Accuracy: 71.39%\n",
      "Batch 171, Loss: 1.022685, Accuracy: 71.40%\n",
      "Batch 172, Loss: 1.059993, Accuracy: 71.38%\n",
      "Batch 173, Loss: 1.029061, Accuracy: 71.40%\n",
      "Batch 174, Loss: 1.002899, Accuracy: 71.41%\n",
      "Batch 175, Loss: 0.944053, Accuracy: 71.46%\n",
      "Batch 176, Loss: 1.029146, Accuracy: 71.45%\n",
      "Batch 177, Loss: 0.963673, Accuracy: 71.49%\n",
      "Batch 178, Loss: 1.123754, Accuracy: 71.43%\n",
      "Batch 179, Loss: 0.995652, Accuracy: 71.44%\n",
      "Batch 180, Loss: 0.946959, Accuracy: 71.49%\n",
      "Batch 181, Loss: 0.920683, Accuracy: 71.55%\n",
      "Batch 182, Loss: 0.989408, Accuracy: 71.55%\n",
      "Batch 183, Loss: 1.037247, Accuracy: 71.55%\n",
      "Batch 184, Loss: 0.988244, Accuracy: 71.58%\n",
      "Batch 185, Loss: 0.988685, Accuracy: 71.60%\n",
      "Batch 186, Loss: 0.994434, Accuracy: 71.61%\n",
      "Batch 187, Loss: 0.977540, Accuracy: 71.64%\n",
      "Batch 188, Loss: 1.027673, Accuracy: 71.64%\n",
      "Batch 189, Loss: 0.982077, Accuracy: 71.65%\n",
      "Batch 190, Loss: 1.119795, Accuracy: 71.60%\n",
      "Batch 191, Loss: 0.941660, Accuracy: 71.64%\n",
      "Batch 192, Loss: 0.997723, Accuracy: 71.64%\n",
      "Batch 193, Loss: 0.968328, Accuracy: 71.67%\n",
      "Batch 194, Loss: 1.112856, Accuracy: 71.62%\n",
      "Batch 195, Loss: 1.055894, Accuracy: 71.61%\n",
      "Batch 196, Loss: 0.951602, Accuracy: 71.65%\n",
      "Batch 197, Loss: 0.989848, Accuracy: 71.68%\n",
      "Batch 198, Loss: 1.114020, Accuracy: 71.63%\n",
      "Batch 199, Loss: 1.079925, Accuracy: 71.61%\n",
      "Batch 200, Loss: 1.089009, Accuracy: 71.56%\n",
      "Batch 201, Loss: 1.062722, Accuracy: 71.56%\n",
      "Batch 202, Loss: 1.095605, Accuracy: 71.53%\n",
      "Batch 203, Loss: 0.974942, Accuracy: 71.55%\n",
      "Batch 204, Loss: 1.025590, Accuracy: 71.55%\n",
      "Batch 205, Loss: 1.105465, Accuracy: 71.52%\n",
      "Batch 206, Loss: 1.079612, Accuracy: 71.48%\n",
      "Batch 207, Loss: 0.932190, Accuracy: 71.53%\n",
      "Batch 208, Loss: 1.016624, Accuracy: 71.54%\n",
      "Batch 209, Loss: 1.033218, Accuracy: 71.55%\n",
      "Batch 210, Loss: 1.070177, Accuracy: 71.53%\n",
      "Batch 211, Loss: 1.081879, Accuracy: 71.51%\n",
      "Batch 212, Loss: 1.016530, Accuracy: 71.51%\n",
      "Batch 213, Loss: 1.054783, Accuracy: 71.49%\n",
      "Training - Epoch 17, Loss: 1.027362, Accuracy: 71.49%\n",
      "Validation Batch 1, Loss: 1.094150, Accuracy: 62.50%\n",
      "Validation Batch 2, Loss: 1.191043, Accuracy: 57.81%\n",
      "Validation Batch 3, Loss: 1.202098, Accuracy: 56.77%\n",
      "Validation Batch 4, Loss: 1.085075, Accuracy: 59.38%\n",
      "Validation Batch 5, Loss: 1.160969, Accuracy: 58.44%\n",
      "Validation Batch 6, Loss: 1.120151, Accuracy: 58.85%\n",
      "Validation Batch 7, Loss: 1.142702, Accuracy: 58.26%\n",
      "Validation Batch 8, Loss: 1.154131, Accuracy: 58.59%\n",
      "Validation Batch 9, Loss: 1.166676, Accuracy: 58.33%\n",
      "Validation Batch 10, Loss: 1.133578, Accuracy: 58.44%\n",
      "Validation Batch 11, Loss: 1.109982, Accuracy: 58.95%\n",
      "Validation Batch 12, Loss: 1.063147, Accuracy: 59.77%\n",
      "Validation Batch 13, Loss: 1.178413, Accuracy: 59.38%\n",
      "Validation Batch 14, Loss: 1.114382, Accuracy: 59.49%\n",
      "Validation Batch 15, Loss: 1.117965, Accuracy: 59.69%\n",
      "Validation Batch 16, Loss: 1.150275, Accuracy: 59.38%\n",
      "Validation Batch 17, Loss: 1.234022, Accuracy: 58.64%\n",
      "Validation Batch 18, Loss: 1.122352, Accuracy: 58.59%\n",
      "Validation Batch 19, Loss: 1.188203, Accuracy: 58.31%\n",
      "Validation Batch 20, Loss: 1.142839, Accuracy: 58.36%\n",
      "Validation Batch 21, Loss: 1.132677, Accuracy: 58.48%\n",
      "Validation Batch 22, Loss: 1.197857, Accuracy: 58.10%\n",
      "Validation Batch 23, Loss: 1.224149, Accuracy: 57.74%\n",
      "Validation Batch 24, Loss: 1.137381, Accuracy: 57.81%\n",
      "Validation Batch 25, Loss: 1.119952, Accuracy: 57.81%\n",
      "Validation Batch 26, Loss: 1.105219, Accuracy: 57.87%\n",
      "Validation Batch 27, Loss: 1.167413, Accuracy: 57.72%\n",
      "Validation - Epoch 17, Loss: 1.146548, Accuracy: 57.72%\n",
      "Patience—1\n",
      "Epoch 18\n",
      "Batch 1, Loss: 1.043268, Accuracy: 70.31%\n",
      "Batch 2, Loss: 1.045863, Accuracy: 68.75%\n",
      "Batch 3, Loss: 1.076089, Accuracy: 67.71%\n",
      "Batch 4, Loss: 1.090883, Accuracy: 67.19%\n",
      "Batch 5, Loss: 1.002357, Accuracy: 68.75%\n",
      "Batch 6, Loss: 1.043959, Accuracy: 69.01%\n",
      "Batch 7, Loss: 1.067390, Accuracy: 68.97%\n",
      "Batch 8, Loss: 1.094132, Accuracy: 68.55%\n",
      "Batch 9, Loss: 1.069564, Accuracy: 68.40%\n",
      "Batch 10, Loss: 1.094391, Accuracy: 67.97%\n",
      "Batch 11, Loss: 1.061900, Accuracy: 67.90%\n",
      "Batch 12, Loss: 0.913763, Accuracy: 69.27%\n",
      "Batch 13, Loss: 1.030325, Accuracy: 69.59%\n",
      "Batch 14, Loss: 1.051309, Accuracy: 69.53%\n",
      "Batch 15, Loss: 0.985338, Accuracy: 70.00%\n",
      "Batch 16, Loss: 0.988950, Accuracy: 70.12%\n",
      "Batch 17, Loss: 0.980107, Accuracy: 70.50%\n",
      "Batch 18, Loss: 1.034654, Accuracy: 70.49%\n",
      "Batch 19, Loss: 1.036869, Accuracy: 70.39%\n",
      "Batch 20, Loss: 1.079889, Accuracy: 70.16%\n",
      "Batch 21, Loss: 1.027923, Accuracy: 70.24%\n",
      "Batch 22, Loss: 1.038974, Accuracy: 70.17%\n",
      "Batch 23, Loss: 1.137656, Accuracy: 69.77%\n",
      "Batch 24, Loss: 1.041215, Accuracy: 69.73%\n",
      "Batch 25, Loss: 0.912815, Accuracy: 70.19%\n",
      "Batch 26, Loss: 1.046136, Accuracy: 70.07%\n",
      "Batch 27, Loss: 1.027258, Accuracy: 70.08%\n",
      "Batch 28, Loss: 1.014057, Accuracy: 70.20%\n",
      "Batch 29, Loss: 1.094651, Accuracy: 70.04%\n",
      "Batch 30, Loss: 1.032376, Accuracy: 70.00%\n",
      "Batch 31, Loss: 1.059844, Accuracy: 69.91%\n",
      "Batch 32, Loss: 0.974953, Accuracy: 70.21%\n",
      "Batch 33, Loss: 0.986573, Accuracy: 70.50%\n",
      "Batch 34, Loss: 1.017867, Accuracy: 70.63%\n",
      "Batch 35, Loss: 1.010770, Accuracy: 70.80%\n",
      "Batch 36, Loss: 1.078337, Accuracy: 70.62%\n",
      "Batch 37, Loss: 1.077341, Accuracy: 70.52%\n",
      "Batch 38, Loss: 1.054683, Accuracy: 70.48%\n",
      "Batch 39, Loss: 1.070121, Accuracy: 70.39%\n",
      "Batch 40, Loss: 1.090541, Accuracy: 70.23%\n",
      "Batch 41, Loss: 1.031650, Accuracy: 70.27%\n",
      "Batch 42, Loss: 0.956783, Accuracy: 70.50%\n",
      "Batch 43, Loss: 1.004694, Accuracy: 70.53%\n",
      "Batch 44, Loss: 1.037644, Accuracy: 70.42%\n",
      "Batch 45, Loss: 1.003831, Accuracy: 70.49%\n",
      "Batch 46, Loss: 1.016824, Accuracy: 70.52%\n",
      "Batch 47, Loss: 0.993777, Accuracy: 70.61%\n",
      "Batch 48, Loss: 0.973037, Accuracy: 70.77%\n",
      "Batch 49, Loss: 1.010291, Accuracy: 70.82%\n",
      "Batch 50, Loss: 1.080837, Accuracy: 70.69%\n",
      "Batch 51, Loss: 1.091074, Accuracy: 70.59%\n",
      "Batch 52, Loss: 0.976786, Accuracy: 70.67%\n",
      "Batch 53, Loss: 1.038716, Accuracy: 70.64%\n",
      "Batch 54, Loss: 0.983441, Accuracy: 70.75%\n",
      "Batch 55, Loss: 0.968407, Accuracy: 70.88%\n",
      "Batch 56, Loss: 1.037446, Accuracy: 70.95%\n",
      "Batch 57, Loss: 0.993484, Accuracy: 71.00%\n",
      "Batch 58, Loss: 1.050809, Accuracy: 70.99%\n",
      "Batch 59, Loss: 1.020760, Accuracy: 71.03%\n",
      "Batch 60, Loss: 0.988041, Accuracy: 71.12%\n",
      "Batch 61, Loss: 1.029593, Accuracy: 71.08%\n",
      "Batch 62, Loss: 1.084255, Accuracy: 70.94%\n",
      "Batch 63, Loss: 0.972266, Accuracy: 70.98%\n",
      "Batch 64, Loss: 0.934897, Accuracy: 71.14%\n",
      "Batch 65, Loss: 0.963381, Accuracy: 71.25%\n",
      "Batch 66, Loss: 0.967054, Accuracy: 71.35%\n",
      "Batch 67, Loss: 1.057525, Accuracy: 71.34%\n",
      "Batch 68, Loss: 0.964656, Accuracy: 71.46%\n",
      "Batch 69, Loss: 1.025670, Accuracy: 71.44%\n",
      "Batch 70, Loss: 1.020116, Accuracy: 71.45%\n",
      "Batch 71, Loss: 1.043629, Accuracy: 71.39%\n",
      "Batch 72, Loss: 1.004430, Accuracy: 71.38%\n",
      "Batch 73, Loss: 0.957622, Accuracy: 71.47%\n",
      "Batch 74, Loss: 1.038386, Accuracy: 71.45%\n",
      "Batch 75, Loss: 0.976729, Accuracy: 71.52%\n",
      "Batch 76, Loss: 1.087616, Accuracy: 71.42%\n",
      "Batch 77, Loss: 0.995658, Accuracy: 71.43%\n",
      "Batch 78, Loss: 1.081353, Accuracy: 71.33%\n",
      "Batch 79, Loss: 1.067328, Accuracy: 71.30%\n",
      "Batch 80, Loss: 0.979516, Accuracy: 71.37%\n",
      "Batch 81, Loss: 1.008788, Accuracy: 71.37%\n",
      "Batch 82, Loss: 1.003806, Accuracy: 71.44%\n",
      "Batch 83, Loss: 0.982403, Accuracy: 71.50%\n",
      "Batch 84, Loss: 1.017650, Accuracy: 71.52%\n",
      "Batch 85, Loss: 0.944398, Accuracy: 71.65%\n",
      "Batch 86, Loss: 0.928924, Accuracy: 71.78%\n",
      "Batch 87, Loss: 1.034246, Accuracy: 71.80%\n",
      "Batch 88, Loss: 1.055542, Accuracy: 71.77%\n",
      "Batch 89, Loss: 1.061592, Accuracy: 71.72%\n",
      "Batch 90, Loss: 1.135514, Accuracy: 71.61%\n",
      "Batch 91, Loss: 1.019071, Accuracy: 71.67%\n",
      "Batch 92, Loss: 1.029157, Accuracy: 71.65%\n",
      "Batch 93, Loss: 0.981212, Accuracy: 71.69%\n",
      "Batch 94, Loss: 1.051160, Accuracy: 71.64%\n",
      "Batch 95, Loss: 0.995570, Accuracy: 71.69%\n",
      "Batch 96, Loss: 0.988694, Accuracy: 71.73%\n",
      "Batch 97, Loss: 0.972355, Accuracy: 71.78%\n",
      "Batch 98, Loss: 1.067320, Accuracy: 71.73%\n",
      "Batch 99, Loss: 0.976585, Accuracy: 71.78%\n",
      "Batch 100, Loss: 1.010022, Accuracy: 71.81%\n",
      "Batch 101, Loss: 0.983861, Accuracy: 71.86%\n",
      "Batch 102, Loss: 1.075043, Accuracy: 71.78%\n",
      "Batch 103, Loss: 0.964358, Accuracy: 71.83%\n",
      "Batch 104, Loss: 1.162088, Accuracy: 71.66%\n",
      "Batch 105, Loss: 0.985097, Accuracy: 71.71%\n",
      "Batch 106, Loss: 1.001644, Accuracy: 71.76%\n",
      "Batch 107, Loss: 1.032279, Accuracy: 71.74%\n",
      "Batch 108, Loss: 0.951355, Accuracy: 71.80%\n",
      "Batch 109, Loss: 1.005455, Accuracy: 71.83%\n",
      "Batch 110, Loss: 1.074667, Accuracy: 71.79%\n",
      "Batch 111, Loss: 1.083843, Accuracy: 71.71%\n",
      "Batch 112, Loss: 1.175532, Accuracy: 71.54%\n",
      "Batch 113, Loss: 0.963485, Accuracy: 71.60%\n",
      "Batch 114, Loss: 1.009498, Accuracy: 71.63%\n",
      "Batch 115, Loss: 1.055301, Accuracy: 71.62%\n",
      "Batch 116, Loss: 0.966249, Accuracy: 71.69%\n",
      "Batch 117, Loss: 0.961141, Accuracy: 71.73%\n",
      "Batch 118, Loss: 1.044935, Accuracy: 71.72%\n",
      "Batch 119, Loss: 0.885562, Accuracy: 71.85%\n",
      "Batch 120, Loss: 1.000296, Accuracy: 71.86%\n",
      "Batch 121, Loss: 1.078058, Accuracy: 71.80%\n",
      "Batch 122, Loss: 1.066489, Accuracy: 71.75%\n",
      "Batch 123, Loss: 1.051190, Accuracy: 71.72%\n",
      "Batch 124, Loss: 1.039666, Accuracy: 71.71%\n",
      "Batch 125, Loss: 0.941731, Accuracy: 71.78%\n",
      "Batch 126, Loss: 0.993549, Accuracy: 71.80%\n",
      "Batch 127, Loss: 1.043046, Accuracy: 71.75%\n",
      "Batch 128, Loss: 1.037412, Accuracy: 71.72%\n",
      "Batch 129, Loss: 0.975828, Accuracy: 71.77%\n",
      "Batch 130, Loss: 0.964876, Accuracy: 71.84%\n",
      "Batch 131, Loss: 1.075101, Accuracy: 71.82%\n",
      "Batch 132, Loss: 1.126422, Accuracy: 71.73%\n",
      "Batch 133, Loss: 1.033031, Accuracy: 71.72%\n",
      "Batch 134, Loss: 0.995578, Accuracy: 71.72%\n",
      "Batch 135, Loss: 1.047031, Accuracy: 71.72%\n",
      "Batch 136, Loss: 1.006269, Accuracy: 71.75%\n",
      "Batch 137, Loss: 1.091690, Accuracy: 71.69%\n",
      "Batch 138, Loss: 1.119114, Accuracy: 71.61%\n",
      "Batch 139, Loss: 0.970302, Accuracy: 71.66%\n",
      "Batch 140, Loss: 0.993320, Accuracy: 71.67%\n",
      "Batch 141, Loss: 1.046252, Accuracy: 71.63%\n",
      "Batch 142, Loss: 1.041694, Accuracy: 71.61%\n",
      "Batch 143, Loss: 0.968279, Accuracy: 71.65%\n",
      "Batch 144, Loss: 0.965298, Accuracy: 71.69%\n",
      "Batch 145, Loss: 1.040412, Accuracy: 71.67%\n",
      "Batch 146, Loss: 1.013694, Accuracy: 71.68%\n",
      "Batch 147, Loss: 0.991444, Accuracy: 71.70%\n",
      "Batch 148, Loss: 1.007008, Accuracy: 71.73%\n",
      "Batch 149, Loss: 0.920512, Accuracy: 71.79%\n",
      "Batch 150, Loss: 0.984649, Accuracy: 71.82%\n",
      "Batch 151, Loss: 1.105828, Accuracy: 71.77%\n",
      "Batch 152, Loss: 0.968559, Accuracy: 71.81%\n",
      "Batch 153, Loss: 1.003454, Accuracy: 71.82%\n",
      "Batch 154, Loss: 1.013073, Accuracy: 71.83%\n",
      "Batch 155, Loss: 0.991236, Accuracy: 71.85%\n",
      "Batch 156, Loss: 1.042741, Accuracy: 71.85%\n",
      "Batch 157, Loss: 1.077808, Accuracy: 71.84%\n",
      "Batch 158, Loss: 1.021731, Accuracy: 71.83%\n",
      "Batch 159, Loss: 1.131006, Accuracy: 71.76%\n",
      "Batch 160, Loss: 1.030630, Accuracy: 71.76%\n",
      "Batch 161, Loss: 1.075539, Accuracy: 71.72%\n",
      "Batch 162, Loss: 0.983079, Accuracy: 71.75%\n",
      "Batch 163, Loss: 0.992827, Accuracy: 71.80%\n",
      "Batch 164, Loss: 1.108357, Accuracy: 71.74%\n",
      "Batch 165, Loss: 1.055147, Accuracy: 71.71%\n",
      "Batch 166, Loss: 1.007964, Accuracy: 71.73%\n",
      "Batch 167, Loss: 1.069413, Accuracy: 71.72%\n",
      "Batch 168, Loss: 1.006729, Accuracy: 71.73%\n",
      "Batch 169, Loss: 1.006359, Accuracy: 71.73%\n",
      "Batch 170, Loss: 1.067531, Accuracy: 71.70%\n",
      "Batch 171, Loss: 1.063095, Accuracy: 71.69%\n",
      "Batch 172, Loss: 1.039163, Accuracy: 71.68%\n",
      "Batch 173, Loss: 1.035463, Accuracy: 71.67%\n",
      "Batch 174, Loss: 1.052860, Accuracy: 71.65%\n",
      "Batch 175, Loss: 1.014037, Accuracy: 71.65%\n",
      "Batch 176, Loss: 1.032940, Accuracy: 71.64%\n",
      "Batch 177, Loss: 1.077711, Accuracy: 71.62%\n",
      "Batch 178, Loss: 0.997051, Accuracy: 71.65%\n",
      "Batch 179, Loss: 1.130795, Accuracy: 71.59%\n",
      "Batch 180, Loss: 0.992170, Accuracy: 71.62%\n",
      "Batch 181, Loss: 1.049327, Accuracy: 71.62%\n",
      "Batch 182, Loss: 0.944520, Accuracy: 71.66%\n",
      "Batch 183, Loss: 1.080027, Accuracy: 71.61%\n",
      "Batch 184, Loss: 0.944392, Accuracy: 71.66%\n",
      "Batch 185, Loss: 1.019208, Accuracy: 71.67%\n",
      "Batch 186, Loss: 1.040148, Accuracy: 71.66%\n",
      "Batch 187, Loss: 0.955206, Accuracy: 71.71%\n",
      "Batch 188, Loss: 1.121846, Accuracy: 71.65%\n",
      "Batch 189, Loss: 1.010494, Accuracy: 71.66%\n",
      "Batch 190, Loss: 1.090779, Accuracy: 71.62%\n",
      "Batch 191, Loss: 0.956034, Accuracy: 71.65%\n",
      "Batch 192, Loss: 1.120150, Accuracy: 71.61%\n",
      "Batch 193, Loss: 1.067810, Accuracy: 71.58%\n",
      "Batch 194, Loss: 1.079664, Accuracy: 71.55%\n",
      "Batch 195, Loss: 0.968833, Accuracy: 71.59%\n",
      "Batch 196, Loss: 1.043934, Accuracy: 71.59%\n",
      "Batch 197, Loss: 1.067850, Accuracy: 71.57%\n",
      "Batch 198, Loss: 1.053741, Accuracy: 71.57%\n",
      "Batch 199, Loss: 0.981138, Accuracy: 71.59%\n",
      "Batch 200, Loss: 0.993768, Accuracy: 71.60%\n",
      "Batch 201, Loss: 1.058040, Accuracy: 71.58%\n",
      "Batch 202, Loss: 0.993004, Accuracy: 71.61%\n",
      "Batch 203, Loss: 0.975779, Accuracy: 71.65%\n",
      "Batch 204, Loss: 0.974806, Accuracy: 71.68%\n",
      "Batch 205, Loss: 0.949911, Accuracy: 71.72%\n",
      "Batch 206, Loss: 1.064687, Accuracy: 71.70%\n",
      "Batch 207, Loss: 0.997923, Accuracy: 71.72%\n",
      "Batch 208, Loss: 1.058517, Accuracy: 71.70%\n",
      "Batch 209, Loss: 0.946317, Accuracy: 71.75%\n",
      "Batch 210, Loss: 1.058526, Accuracy: 71.73%\n",
      "Batch 211, Loss: 1.092017, Accuracy: 71.69%\n",
      "Batch 212, Loss: 1.042183, Accuracy: 71.69%\n",
      "Batch 213, Loss: 0.966277, Accuracy: 71.72%\n",
      "Training - Epoch 18, Loss: 1.025028, Accuracy: 71.72%\n",
      "Validation Batch 1, Loss: 1.066038, Accuracy: 68.75%\n",
      "Validation Batch 2, Loss: 1.162390, Accuracy: 62.50%\n",
      "Validation Batch 3, Loss: 1.167969, Accuracy: 60.94%\n",
      "Validation Batch 4, Loss: 1.059455, Accuracy: 62.89%\n",
      "Validation Batch 5, Loss: 1.139338, Accuracy: 61.56%\n",
      "Validation Batch 6, Loss: 1.093589, Accuracy: 61.72%\n",
      "Validation Batch 7, Loss: 1.110632, Accuracy: 61.83%\n",
      "Validation Batch 8, Loss: 1.140184, Accuracy: 61.52%\n",
      "Validation Batch 9, Loss: 1.141251, Accuracy: 61.46%\n",
      "Validation Batch 10, Loss: 1.098196, Accuracy: 61.72%\n",
      "Validation Batch 11, Loss: 1.074086, Accuracy: 62.22%\n",
      "Validation Batch 12, Loss: 1.039392, Accuracy: 62.89%\n",
      "Validation Batch 13, Loss: 1.157660, Accuracy: 62.38%\n",
      "Validation Batch 14, Loss: 1.088016, Accuracy: 62.39%\n",
      "Validation Batch 15, Loss: 1.091856, Accuracy: 62.50%\n",
      "Validation Batch 16, Loss: 1.122796, Accuracy: 62.21%\n",
      "Validation Batch 17, Loss: 1.195577, Accuracy: 61.67%\n",
      "Validation Batch 18, Loss: 1.090913, Accuracy: 61.72%\n",
      "Validation Batch 19, Loss: 1.147118, Accuracy: 61.43%\n",
      "Validation Batch 20, Loss: 1.108335, Accuracy: 61.41%\n",
      "Validation Batch 21, Loss: 1.106857, Accuracy: 61.46%\n",
      "Validation Batch 22, Loss: 1.167171, Accuracy: 61.01%\n",
      "Validation Batch 23, Loss: 1.198902, Accuracy: 60.53%\n",
      "Validation Batch 24, Loss: 1.108325, Accuracy: 60.68%\n",
      "Validation Batch 25, Loss: 1.099453, Accuracy: 60.69%\n",
      "Validation Batch 26, Loss: 1.085269, Accuracy: 60.76%\n",
      "Validation Batch 27, Loss: 1.127178, Accuracy: 60.83%\n",
      "Validation - Epoch 18, Loss: 1.118072, Accuracy: 60.83%\n",
      "Patience—0\n",
      "Epoch 19\n",
      "Batch 1, Loss: 1.003489, Accuracy: 73.44%\n",
      "Batch 2, Loss: 1.010910, Accuracy: 73.44%\n",
      "Batch 3, Loss: 1.043763, Accuracy: 71.88%\n",
      "Batch 4, Loss: 1.128007, Accuracy: 68.36%\n",
      "Batch 5, Loss: 0.981081, Accuracy: 70.31%\n",
      "Batch 6, Loss: 1.007849, Accuracy: 70.83%\n",
      "Batch 7, Loss: 1.013089, Accuracy: 70.98%\n",
      "Batch 8, Loss: 0.975465, Accuracy: 71.48%\n",
      "Batch 9, Loss: 0.920825, Accuracy: 72.74%\n",
      "Batch 10, Loss: 1.079376, Accuracy: 71.88%\n",
      "Batch 11, Loss: 0.932288, Accuracy: 72.87%\n",
      "Batch 12, Loss: 0.948723, Accuracy: 73.57%\n",
      "Batch 13, Loss: 0.972491, Accuracy: 73.80%\n",
      "Batch 14, Loss: 1.079326, Accuracy: 73.33%\n",
      "Batch 15, Loss: 1.025737, Accuracy: 73.33%\n",
      "Batch 16, Loss: 1.033465, Accuracy: 73.14%\n",
      "Batch 17, Loss: 0.981607, Accuracy: 73.35%\n",
      "Batch 18, Loss: 1.030294, Accuracy: 73.00%\n",
      "Batch 19, Loss: 0.998642, Accuracy: 73.03%\n",
      "Batch 20, Loss: 1.026183, Accuracy: 73.05%\n",
      "Batch 21, Loss: 1.060118, Accuracy: 72.92%\n",
      "Batch 22, Loss: 1.009518, Accuracy: 72.87%\n",
      "Batch 23, Loss: 0.985678, Accuracy: 73.03%\n",
      "Batch 24, Loss: 0.964628, Accuracy: 73.24%\n",
      "Batch 25, Loss: 0.951635, Accuracy: 73.62%\n",
      "Batch 26, Loss: 1.004086, Accuracy: 73.74%\n",
      "Batch 27, Loss: 1.017662, Accuracy: 73.73%\n",
      "Batch 28, Loss: 0.964474, Accuracy: 73.88%\n",
      "Batch 29, Loss: 0.983059, Accuracy: 73.98%\n",
      "Batch 30, Loss: 1.097911, Accuracy: 73.70%\n",
      "Batch 31, Loss: 1.050094, Accuracy: 73.64%\n",
      "Batch 32, Loss: 0.978858, Accuracy: 73.78%\n",
      "Batch 33, Loss: 1.020545, Accuracy: 73.67%\n",
      "Batch 34, Loss: 0.965616, Accuracy: 73.85%\n",
      "Batch 35, Loss: 0.992539, Accuracy: 73.84%\n",
      "Batch 36, Loss: 1.060508, Accuracy: 73.57%\n",
      "Batch 37, Loss: 1.081728, Accuracy: 73.35%\n",
      "Batch 38, Loss: 1.011311, Accuracy: 73.36%\n",
      "Batch 39, Loss: 1.078095, Accuracy: 73.12%\n",
      "Batch 40, Loss: 1.119738, Accuracy: 72.85%\n",
      "Batch 41, Loss: 0.966245, Accuracy: 72.98%\n",
      "Batch 42, Loss: 1.118094, Accuracy: 72.73%\n",
      "Batch 43, Loss: 1.108713, Accuracy: 72.49%\n",
      "Batch 44, Loss: 1.005735, Accuracy: 72.55%\n",
      "Batch 45, Loss: 1.093503, Accuracy: 72.36%\n",
      "Batch 46, Loss: 1.141133, Accuracy: 72.01%\n",
      "Batch 47, Loss: 1.036376, Accuracy: 71.97%\n",
      "Batch 48, Loss: 1.037770, Accuracy: 71.97%\n",
      "Batch 49, Loss: 1.078587, Accuracy: 71.81%\n",
      "Batch 50, Loss: 1.024498, Accuracy: 71.81%\n",
      "Batch 51, Loss: 1.009210, Accuracy: 71.84%\n",
      "Batch 52, Loss: 1.036703, Accuracy: 71.81%\n",
      "Batch 53, Loss: 1.077754, Accuracy: 71.70%\n",
      "Batch 54, Loss: 0.977227, Accuracy: 71.79%\n",
      "Batch 55, Loss: 0.985643, Accuracy: 71.88%\n",
      "Batch 56, Loss: 1.023090, Accuracy: 71.90%\n",
      "Batch 57, Loss: 1.048376, Accuracy: 71.88%\n",
      "Batch 58, Loss: 1.089835, Accuracy: 71.74%\n",
      "Batch 59, Loss: 1.110571, Accuracy: 71.58%\n",
      "Batch 60, Loss: 0.999698, Accuracy: 71.64%\n",
      "Batch 61, Loss: 1.065318, Accuracy: 71.57%\n",
      "Batch 62, Loss: 1.006792, Accuracy: 71.60%\n",
      "Batch 63, Loss: 1.031667, Accuracy: 71.60%\n",
      "Batch 64, Loss: 1.013195, Accuracy: 71.63%\n",
      "Batch 65, Loss: 1.106030, Accuracy: 71.54%\n",
      "Batch 66, Loss: 1.060461, Accuracy: 71.47%\n",
      "Batch 67, Loss: 0.949153, Accuracy: 71.60%\n",
      "Batch 68, Loss: 1.061195, Accuracy: 71.53%\n",
      "Batch 69, Loss: 1.029945, Accuracy: 71.51%\n",
      "Batch 70, Loss: 1.002273, Accuracy: 71.50%\n",
      "Batch 71, Loss: 1.064530, Accuracy: 71.46%\n",
      "Batch 72, Loss: 0.999824, Accuracy: 71.48%\n",
      "Batch 73, Loss: 0.890429, Accuracy: 71.68%\n",
      "Batch 74, Loss: 1.031968, Accuracy: 71.66%\n",
      "Batch 75, Loss: 0.955946, Accuracy: 71.75%\n",
      "Batch 76, Loss: 1.054592, Accuracy: 71.75%\n",
      "Batch 77, Loss: 0.997529, Accuracy: 71.79%\n",
      "Batch 78, Loss: 1.064078, Accuracy: 71.75%\n",
      "Batch 79, Loss: 1.035232, Accuracy: 71.76%\n",
      "Batch 80, Loss: 1.043653, Accuracy: 71.74%\n",
      "Batch 81, Loss: 1.045451, Accuracy: 71.70%\n",
      "Batch 82, Loss: 1.004748, Accuracy: 71.74%\n",
      "Batch 83, Loss: 0.938919, Accuracy: 71.86%\n",
      "Batch 84, Loss: 1.029238, Accuracy: 71.86%\n",
      "Batch 85, Loss: 1.006121, Accuracy: 71.88%\n",
      "Batch 86, Loss: 0.966324, Accuracy: 71.95%\n",
      "Batch 87, Loss: 0.985775, Accuracy: 71.98%\n",
      "Batch 88, Loss: 1.008671, Accuracy: 72.00%\n",
      "Batch 89, Loss: 0.961401, Accuracy: 72.07%\n",
      "Batch 90, Loss: 1.023316, Accuracy: 72.08%\n",
      "Batch 91, Loss: 1.012192, Accuracy: 72.12%\n",
      "Batch 92, Loss: 0.952901, Accuracy: 72.16%\n",
      "Batch 93, Loss: 1.069755, Accuracy: 72.11%\n",
      "Batch 94, Loss: 1.089152, Accuracy: 72.02%\n",
      "Batch 95, Loss: 1.034781, Accuracy: 71.99%\n",
      "Batch 96, Loss: 1.038189, Accuracy: 71.97%\n",
      "Batch 97, Loss: 1.095310, Accuracy: 71.89%\n",
      "Batch 98, Loss: 0.959207, Accuracy: 71.97%\n",
      "Batch 99, Loss: 1.094116, Accuracy: 71.89%\n",
      "Batch 100, Loss: 1.102417, Accuracy: 71.81%\n",
      "Batch 101, Loss: 0.950319, Accuracy: 71.91%\n",
      "Batch 102, Loss: 1.011966, Accuracy: 71.91%\n",
      "Batch 103, Loss: 1.045828, Accuracy: 71.89%\n",
      "Batch 104, Loss: 0.964674, Accuracy: 71.92%\n",
      "Batch 105, Loss: 1.015292, Accuracy: 71.95%\n",
      "Batch 106, Loss: 1.034075, Accuracy: 71.96%\n",
      "Batch 107, Loss: 0.999246, Accuracy: 71.98%\n",
      "Batch 108, Loss: 1.054612, Accuracy: 71.95%\n",
      "Batch 109, Loss: 1.130930, Accuracy: 71.83%\n",
      "Batch 110, Loss: 1.058689, Accuracy: 71.79%\n",
      "Batch 111, Loss: 0.967054, Accuracy: 71.85%\n",
      "Batch 112, Loss: 0.932849, Accuracy: 71.93%\n",
      "Batch 113, Loss: 0.946821, Accuracy: 72.00%\n",
      "Batch 114, Loss: 1.000751, Accuracy: 72.03%\n",
      "Batch 115, Loss: 0.979937, Accuracy: 72.05%\n",
      "Batch 116, Loss: 0.975443, Accuracy: 72.09%\n",
      "Batch 117, Loss: 1.085851, Accuracy: 72.05%\n",
      "Batch 118, Loss: 0.989055, Accuracy: 72.07%\n",
      "Batch 119, Loss: 1.088220, Accuracy: 71.99%\n",
      "Batch 120, Loss: 1.017456, Accuracy: 71.98%\n",
      "Batch 121, Loss: 1.036862, Accuracy: 71.97%\n",
      "Batch 122, Loss: 1.006517, Accuracy: 71.98%\n",
      "Batch 123, Loss: 1.043040, Accuracy: 71.96%\n",
      "Batch 124, Loss: 0.985847, Accuracy: 71.99%\n",
      "Batch 125, Loss: 0.934671, Accuracy: 72.05%\n",
      "Batch 126, Loss: 1.055357, Accuracy: 72.04%\n",
      "Batch 127, Loss: 0.963591, Accuracy: 72.10%\n",
      "Batch 128, Loss: 0.949786, Accuracy: 72.17%\n",
      "Batch 129, Loss: 1.017139, Accuracy: 72.17%\n",
      "Batch 130, Loss: 1.035903, Accuracy: 72.16%\n",
      "Batch 131, Loss: 1.137591, Accuracy: 72.09%\n",
      "Batch 132, Loss: 0.940307, Accuracy: 72.16%\n",
      "Batch 133, Loss: 0.966855, Accuracy: 72.24%\n",
      "Batch 134, Loss: 1.053296, Accuracy: 72.22%\n",
      "Batch 135, Loss: 1.013397, Accuracy: 72.25%\n",
      "Batch 136, Loss: 1.069453, Accuracy: 72.20%\n",
      "Batch 137, Loss: 0.990387, Accuracy: 72.23%\n",
      "Batch 138, Loss: 0.940965, Accuracy: 72.29%\n",
      "Batch 139, Loss: 1.081102, Accuracy: 72.25%\n",
      "Batch 140, Loss: 1.018659, Accuracy: 72.24%\n",
      "Batch 141, Loss: 1.034968, Accuracy: 72.25%\n",
      "Batch 142, Loss: 1.177573, Accuracy: 72.13%\n",
      "Batch 143, Loss: 0.983376, Accuracy: 72.16%\n",
      "Batch 144, Loss: 1.038759, Accuracy: 72.16%\n",
      "Batch 145, Loss: 0.986380, Accuracy: 72.18%\n",
      "Batch 146, Loss: 0.982326, Accuracy: 72.20%\n",
      "Batch 147, Loss: 1.089706, Accuracy: 72.12%\n",
      "Batch 148, Loss: 0.942745, Accuracy: 72.18%\n",
      "Batch 149, Loss: 0.998318, Accuracy: 72.18%\n",
      "Batch 150, Loss: 0.997481, Accuracy: 72.20%\n",
      "Batch 151, Loss: 1.023642, Accuracy: 72.21%\n",
      "Batch 152, Loss: 0.995993, Accuracy: 72.21%\n",
      "Batch 153, Loss: 1.027713, Accuracy: 72.21%\n",
      "Batch 154, Loss: 1.125743, Accuracy: 72.13%\n",
      "Batch 155, Loss: 1.088331, Accuracy: 72.08%\n",
      "Batch 156, Loss: 1.001789, Accuracy: 72.09%\n",
      "Batch 157, Loss: 1.000051, Accuracy: 72.12%\n",
      "Batch 158, Loss: 1.079770, Accuracy: 72.08%\n",
      "Batch 159, Loss: 0.988286, Accuracy: 72.11%\n",
      "Batch 160, Loss: 0.992101, Accuracy: 72.14%\n",
      "Batch 161, Loss: 0.972061, Accuracy: 72.17%\n",
      "Batch 162, Loss: 0.992812, Accuracy: 72.20%\n",
      "Batch 163, Loss: 1.012962, Accuracy: 72.20%\n",
      "Batch 164, Loss: 1.174697, Accuracy: 72.08%\n",
      "Batch 165, Loss: 0.898116, Accuracy: 72.18%\n",
      "Batch 166, Loss: 1.002585, Accuracy: 72.19%\n",
      "Batch 167, Loss: 0.957864, Accuracy: 72.21%\n",
      "Batch 168, Loss: 0.934847, Accuracy: 72.29%\n",
      "Batch 169, Loss: 0.902678, Accuracy: 72.37%\n",
      "Batch 170, Loss: 1.019138, Accuracy: 72.35%\n",
      "Batch 171, Loss: 0.985675, Accuracy: 72.36%\n",
      "Batch 172, Loss: 0.948451, Accuracy: 72.40%\n",
      "Batch 173, Loss: 1.005294, Accuracy: 72.41%\n",
      "Batch 174, Loss: 0.992961, Accuracy: 72.43%\n",
      "Batch 175, Loss: 0.971729, Accuracy: 72.46%\n",
      "Batch 176, Loss: 1.077441, Accuracy: 72.43%\n",
      "Batch 177, Loss: 1.024419, Accuracy: 72.41%\n",
      "Batch 178, Loss: 1.032009, Accuracy: 72.40%\n",
      "Batch 179, Loss: 1.030321, Accuracy: 72.39%\n",
      "Batch 180, Loss: 1.031146, Accuracy: 72.38%\n",
      "Batch 181, Loss: 0.995509, Accuracy: 72.40%\n",
      "Batch 182, Loss: 1.080252, Accuracy: 72.36%\n",
      "Batch 183, Loss: 1.076026, Accuracy: 72.35%\n",
      "Batch 184, Loss: 1.042448, Accuracy: 72.34%\n",
      "Batch 185, Loss: 1.076898, Accuracy: 72.31%\n",
      "Batch 186, Loss: 1.049142, Accuracy: 72.30%\n",
      "Batch 187, Loss: 0.940408, Accuracy: 72.35%\n",
      "Batch 188, Loss: 0.974307, Accuracy: 72.38%\n",
      "Batch 189, Loss: 1.053428, Accuracy: 72.35%\n",
      "Batch 190, Loss: 1.031082, Accuracy: 72.34%\n",
      "Batch 191, Loss: 1.047396, Accuracy: 72.33%\n",
      "Batch 192, Loss: 0.966449, Accuracy: 72.36%\n",
      "Batch 193, Loss: 1.078363, Accuracy: 72.32%\n",
      "Batch 194, Loss: 0.958774, Accuracy: 72.36%\n",
      "Batch 195, Loss: 1.089061, Accuracy: 72.31%\n",
      "Batch 196, Loss: 1.002046, Accuracy: 72.31%\n",
      "Batch 197, Loss: 1.014549, Accuracy: 72.32%\n",
      "Batch 198, Loss: 1.069644, Accuracy: 72.31%\n",
      "Batch 199, Loss: 0.978745, Accuracy: 72.35%\n",
      "Batch 200, Loss: 0.985861, Accuracy: 72.35%\n",
      "Batch 201, Loss: 0.898563, Accuracy: 72.40%\n",
      "Batch 202, Loss: 1.105837, Accuracy: 72.35%\n",
      "Batch 203, Loss: 1.000624, Accuracy: 72.36%\n",
      "Batch 204, Loss: 1.114049, Accuracy: 72.31%\n",
      "Batch 205, Loss: 0.965057, Accuracy: 72.34%\n",
      "Batch 206, Loss: 1.122508, Accuracy: 72.28%\n",
      "Batch 207, Loss: 1.004135, Accuracy: 72.30%\n",
      "Batch 208, Loss: 1.005165, Accuracy: 72.30%\n",
      "Batch 209, Loss: 0.963652, Accuracy: 72.32%\n",
      "Batch 210, Loss: 1.056858, Accuracy: 72.31%\n",
      "Batch 211, Loss: 0.974713, Accuracy: 72.33%\n",
      "Batch 212, Loss: 1.067661, Accuracy: 72.31%\n",
      "Batch 213, Loss: 0.982618, Accuracy: 72.33%\n",
      "Training - Epoch 19, Loss: 1.019835, Accuracy: 72.33%\n",
      "Validation Batch 1, Loss: 1.074385, Accuracy: 67.19%\n",
      "Validation Batch 2, Loss: 1.166415, Accuracy: 61.72%\n",
      "Validation Batch 3, Loss: 1.169698, Accuracy: 60.42%\n",
      "Validation Batch 4, Loss: 1.063721, Accuracy: 62.50%\n",
      "Validation Batch 5, Loss: 1.149299, Accuracy: 61.25%\n",
      "Validation Batch 6, Loss: 1.108282, Accuracy: 61.20%\n",
      "Validation Batch 7, Loss: 1.115427, Accuracy: 61.16%\n",
      "Validation Batch 8, Loss: 1.151891, Accuracy: 60.74%\n",
      "Validation Batch 9, Loss: 1.145149, Accuracy: 60.76%\n",
      "Validation Batch 10, Loss: 1.102163, Accuracy: 60.94%\n",
      "Validation Batch 11, Loss: 1.079684, Accuracy: 61.51%\n",
      "Validation Batch 12, Loss: 1.046142, Accuracy: 62.11%\n",
      "Validation Batch 13, Loss: 1.156461, Accuracy: 61.90%\n",
      "Validation Batch 14, Loss: 1.089901, Accuracy: 61.94%\n",
      "Validation Batch 15, Loss: 1.096142, Accuracy: 62.08%\n",
      "Validation Batch 16, Loss: 1.137117, Accuracy: 61.82%\n",
      "Validation Batch 17, Loss: 1.201109, Accuracy: 61.21%\n",
      "Validation Batch 18, Loss: 1.094889, Accuracy: 61.28%\n",
      "Validation Batch 19, Loss: 1.151043, Accuracy: 61.10%\n",
      "Validation Batch 20, Loss: 1.109289, Accuracy: 61.09%\n",
      "Validation Batch 21, Loss: 1.121548, Accuracy: 61.09%\n",
      "Validation Batch 22, Loss: 1.171155, Accuracy: 60.72%\n",
      "Validation Batch 23, Loss: 1.200552, Accuracy: 60.33%\n",
      "Validation Batch 24, Loss: 1.109498, Accuracy: 60.55%\n",
      "Validation Batch 25, Loss: 1.106578, Accuracy: 60.50%\n",
      "Validation Batch 26, Loss: 1.091927, Accuracy: 60.52%\n",
      "Validation Batch 27, Loss: 1.127825, Accuracy: 60.60%\n",
      "Validation - Epoch 19, Loss: 1.123603, Accuracy: 60.60%\n",
      "Patience—1\n",
      "Epoch 20\n",
      "Batch 1, Loss: 1.009409, Accuracy: 75.00%\n",
      "Batch 2, Loss: 0.988367, Accuracy: 76.56%\n",
      "Batch 3, Loss: 0.993616, Accuracy: 75.52%\n",
      "Batch 4, Loss: 1.076718, Accuracy: 72.66%\n",
      "Batch 5, Loss: 0.988212, Accuracy: 73.12%\n",
      "Batch 6, Loss: 1.089854, Accuracy: 71.88%\n",
      "Batch 7, Loss: 1.008248, Accuracy: 72.10%\n",
      "Batch 8, Loss: 1.108711, Accuracy: 71.29%\n",
      "Batch 9, Loss: 0.988901, Accuracy: 71.70%\n",
      "Batch 10, Loss: 1.008028, Accuracy: 71.72%\n",
      "Batch 11, Loss: 1.052258, Accuracy: 71.31%\n",
      "Batch 12, Loss: 0.966799, Accuracy: 71.74%\n",
      "Batch 13, Loss: 1.052348, Accuracy: 71.51%\n",
      "Batch 14, Loss: 0.989622, Accuracy: 71.65%\n",
      "Batch 15, Loss: 1.055516, Accuracy: 71.46%\n",
      "Batch 16, Loss: 1.032985, Accuracy: 71.39%\n",
      "Batch 17, Loss: 1.017452, Accuracy: 71.51%\n",
      "Batch 18, Loss: 1.017697, Accuracy: 71.53%\n",
      "Batch 19, Loss: 0.947864, Accuracy: 72.04%\n",
      "Batch 20, Loss: 0.982697, Accuracy: 72.34%\n",
      "Batch 21, Loss: 1.030242, Accuracy: 72.25%\n",
      "Batch 22, Loss: 1.024018, Accuracy: 72.30%\n",
      "Batch 23, Loss: 1.042165, Accuracy: 72.21%\n",
      "Batch 24, Loss: 1.163371, Accuracy: 71.55%\n",
      "Batch 25, Loss: 1.010760, Accuracy: 71.56%\n",
      "Batch 26, Loss: 0.959037, Accuracy: 71.88%\n",
      "Batch 27, Loss: 1.005513, Accuracy: 71.93%\n",
      "Batch 28, Loss: 0.959416, Accuracy: 72.15%\n",
      "Batch 29, Loss: 1.029562, Accuracy: 72.09%\n",
      "Batch 30, Loss: 1.085458, Accuracy: 71.77%\n",
      "Batch 31, Loss: 1.059544, Accuracy: 71.67%\n",
      "Batch 32, Loss: 1.091228, Accuracy: 71.53%\n",
      "Batch 33, Loss: 0.983120, Accuracy: 71.78%\n",
      "Batch 34, Loss: 0.972696, Accuracy: 71.92%\n",
      "Batch 35, Loss: 1.037600, Accuracy: 71.92%\n",
      "Batch 36, Loss: 0.938764, Accuracy: 72.18%\n",
      "Batch 37, Loss: 0.953476, Accuracy: 72.34%\n",
      "Batch 38, Loss: 1.007902, Accuracy: 72.37%\n",
      "Batch 39, Loss: 1.036002, Accuracy: 72.24%\n",
      "Batch 40, Loss: 1.031710, Accuracy: 72.11%\n",
      "Batch 41, Loss: 1.079856, Accuracy: 71.95%\n",
      "Batch 42, Loss: 0.972903, Accuracy: 72.06%\n",
      "Batch 43, Loss: 1.061023, Accuracy: 71.91%\n",
      "Batch 44, Loss: 1.050029, Accuracy: 71.84%\n",
      "Batch 45, Loss: 0.991482, Accuracy: 71.94%\n",
      "Batch 46, Loss: 1.013137, Accuracy: 72.01%\n",
      "Batch 47, Loss: 1.066629, Accuracy: 71.91%\n",
      "Batch 48, Loss: 1.006749, Accuracy: 72.01%\n",
      "Batch 49, Loss: 1.084562, Accuracy: 71.91%\n",
      "Batch 50, Loss: 0.919768, Accuracy: 72.12%\n",
      "Batch 51, Loss: 1.074304, Accuracy: 72.03%\n",
      "Batch 52, Loss: 1.022933, Accuracy: 71.97%\n",
      "Batch 53, Loss: 0.946100, Accuracy: 72.14%\n",
      "Batch 54, Loss: 0.965684, Accuracy: 72.28%\n",
      "Batch 55, Loss: 0.992550, Accuracy: 72.30%\n",
      "Batch 56, Loss: 0.964452, Accuracy: 72.43%\n",
      "Batch 57, Loss: 1.114673, Accuracy: 72.26%\n",
      "Batch 58, Loss: 1.002273, Accuracy: 72.25%\n",
      "Batch 59, Loss: 1.015814, Accuracy: 72.27%\n",
      "Batch 60, Loss: 1.008989, Accuracy: 72.24%\n",
      "Batch 61, Loss: 0.987699, Accuracy: 72.26%\n",
      "Batch 62, Loss: 0.964219, Accuracy: 72.33%\n",
      "Batch 63, Loss: 1.052868, Accuracy: 72.25%\n",
      "Batch 64, Loss: 0.981930, Accuracy: 72.34%\n",
      "Batch 65, Loss: 0.961528, Accuracy: 72.40%\n",
      "Batch 66, Loss: 1.057944, Accuracy: 72.32%\n",
      "Batch 67, Loss: 0.972376, Accuracy: 72.41%\n",
      "Batch 68, Loss: 1.027658, Accuracy: 72.38%\n",
      "Batch 69, Loss: 0.992042, Accuracy: 72.46%\n",
      "Batch 70, Loss: 1.062599, Accuracy: 72.39%\n",
      "Batch 71, Loss: 0.978808, Accuracy: 72.47%\n",
      "Batch 72, Loss: 0.959707, Accuracy: 72.59%\n",
      "Batch 73, Loss: 0.987607, Accuracy: 72.65%\n",
      "Batch 74, Loss: 1.006998, Accuracy: 72.68%\n",
      "Batch 75, Loss: 0.980326, Accuracy: 72.73%\n",
      "Batch 76, Loss: 0.957999, Accuracy: 72.80%\n",
      "Batch 77, Loss: 0.971945, Accuracy: 72.85%\n",
      "Batch 78, Loss: 1.043380, Accuracy: 72.76%\n",
      "Batch 79, Loss: 1.016081, Accuracy: 72.78%\n",
      "Batch 80, Loss: 0.994845, Accuracy: 72.81%\n",
      "Batch 81, Loss: 1.088459, Accuracy: 72.72%\n",
      "Batch 82, Loss: 0.988600, Accuracy: 72.77%\n",
      "Batch 83, Loss: 0.996011, Accuracy: 72.78%\n",
      "Batch 84, Loss: 0.984752, Accuracy: 72.82%\n",
      "Batch 85, Loss: 1.037235, Accuracy: 72.79%\n",
      "Batch 86, Loss: 1.012538, Accuracy: 72.80%\n",
      "Batch 87, Loss: 0.957486, Accuracy: 72.90%\n",
      "Batch 88, Loss: 0.951581, Accuracy: 72.99%\n",
      "Batch 89, Loss: 1.128205, Accuracy: 72.86%\n",
      "Batch 90, Loss: 1.092466, Accuracy: 72.78%\n",
      "Batch 91, Loss: 1.063479, Accuracy: 72.72%\n",
      "Batch 92, Loss: 1.010860, Accuracy: 72.74%\n",
      "Batch 93, Loss: 1.034016, Accuracy: 72.72%\n",
      "Batch 94, Loss: 1.030243, Accuracy: 72.71%\n",
      "Batch 95, Loss: 1.038878, Accuracy: 72.71%\n",
      "Batch 96, Loss: 1.117980, Accuracy: 72.57%\n",
      "Batch 97, Loss: 1.051995, Accuracy: 72.52%\n",
      "Batch 98, Loss: 0.965023, Accuracy: 72.54%\n",
      "Batch 99, Loss: 1.108935, Accuracy: 72.43%\n",
      "Batch 100, Loss: 0.956379, Accuracy: 72.50%\n",
      "Batch 101, Loss: 1.012350, Accuracy: 72.51%\n",
      "Batch 102, Loss: 1.026608, Accuracy: 72.47%\n",
      "Batch 103, Loss: 1.082833, Accuracy: 72.39%\n",
      "Batch 104, Loss: 0.960211, Accuracy: 72.45%\n",
      "Batch 105, Loss: 0.998776, Accuracy: 72.47%\n",
      "Batch 106, Loss: 0.961126, Accuracy: 72.54%\n",
      "Batch 107, Loss: 1.031692, Accuracy: 72.52%\n",
      "Batch 108, Loss: 1.037022, Accuracy: 72.51%\n",
      "Batch 109, Loss: 0.997648, Accuracy: 72.53%\n",
      "Batch 110, Loss: 1.018940, Accuracy: 72.54%\n",
      "Batch 111, Loss: 1.046352, Accuracy: 72.47%\n",
      "Batch 112, Loss: 0.966321, Accuracy: 72.50%\n",
      "Batch 113, Loss: 0.974913, Accuracy: 72.55%\n",
      "Batch 114, Loss: 0.953113, Accuracy: 72.62%\n",
      "Batch 115, Loss: 1.003124, Accuracy: 72.64%\n",
      "Batch 116, Loss: 1.009360, Accuracy: 72.66%\n",
      "Batch 117, Loss: 1.012082, Accuracy: 72.64%\n",
      "Batch 118, Loss: 0.973705, Accuracy: 72.68%\n",
      "Batch 119, Loss: 1.017075, Accuracy: 72.69%\n",
      "Batch 120, Loss: 1.046062, Accuracy: 72.67%\n",
      "Batch 121, Loss: 0.898441, Accuracy: 72.75%\n",
      "Batch 122, Loss: 1.116337, Accuracy: 72.64%\n",
      "Batch 123, Loss: 1.095598, Accuracy: 72.59%\n",
      "Batch 124, Loss: 1.064274, Accuracy: 72.54%\n",
      "Batch 125, Loss: 1.024910, Accuracy: 72.54%\n",
      "Batch 126, Loss: 1.070123, Accuracy: 72.51%\n",
      "Batch 127, Loss: 1.064894, Accuracy: 72.47%\n",
      "Batch 128, Loss: 0.995501, Accuracy: 72.46%\n",
      "Batch 129, Loss: 1.022770, Accuracy: 72.47%\n",
      "Batch 130, Loss: 1.042623, Accuracy: 72.43%\n",
      "Batch 131, Loss: 1.009861, Accuracy: 72.45%\n",
      "Batch 132, Loss: 1.000757, Accuracy: 72.47%\n",
      "Batch 133, Loss: 0.943486, Accuracy: 72.52%\n",
      "Batch 134, Loss: 1.041966, Accuracy: 72.50%\n",
      "Batch 135, Loss: 1.021749, Accuracy: 72.49%\n",
      "Batch 136, Loss: 1.051637, Accuracy: 72.45%\n",
      "Batch 137, Loss: 1.050354, Accuracy: 72.42%\n",
      "Batch 138, Loss: 0.988089, Accuracy: 72.43%\n",
      "Batch 139, Loss: 1.025301, Accuracy: 72.43%\n",
      "Batch 140, Loss: 1.026432, Accuracy: 72.43%\n",
      "Batch 141, Loss: 1.014980, Accuracy: 72.45%\n",
      "Batch 142, Loss: 1.021281, Accuracy: 72.44%\n",
      "Batch 143, Loss: 0.965279, Accuracy: 72.50%\n",
      "Batch 144, Loss: 1.028062, Accuracy: 72.48%\n",
      "Batch 145, Loss: 1.018187, Accuracy: 72.48%\n",
      "Batch 146, Loss: 1.093249, Accuracy: 72.41%\n",
      "Batch 147, Loss: 1.016663, Accuracy: 72.42%\n",
      "Batch 148, Loss: 0.925343, Accuracy: 72.49%\n",
      "Batch 149, Loss: 1.058745, Accuracy: 72.47%\n",
      "Batch 150, Loss: 1.068909, Accuracy: 72.45%\n",
      "Batch 151, Loss: 1.039775, Accuracy: 72.43%\n",
      "Batch 152, Loss: 1.073373, Accuracy: 72.38%\n",
      "Batch 153, Loss: 1.056354, Accuracy: 72.34%\n",
      "Batch 154, Loss: 0.954736, Accuracy: 72.39%\n",
      "Batch 155, Loss: 1.024954, Accuracy: 72.39%\n",
      "Batch 156, Loss: 1.056169, Accuracy: 72.36%\n",
      "Batch 157, Loss: 1.081185, Accuracy: 72.32%\n",
      "Batch 158, Loss: 1.011500, Accuracy: 72.33%\n",
      "Batch 159, Loss: 0.930181, Accuracy: 72.40%\n",
      "Batch 160, Loss: 1.034321, Accuracy: 72.40%\n",
      "Batch 161, Loss: 1.013082, Accuracy: 72.42%\n",
      "Batch 162, Loss: 1.024052, Accuracy: 72.41%\n",
      "Batch 163, Loss: 0.977156, Accuracy: 72.44%\n",
      "Batch 164, Loss: 0.960233, Accuracy: 72.47%\n",
      "Batch 165, Loss: 0.952739, Accuracy: 72.51%\n",
      "Batch 166, Loss: 0.978530, Accuracy: 72.54%\n",
      "Batch 167, Loss: 1.076709, Accuracy: 72.50%\n",
      "Batch 168, Loss: 0.971353, Accuracy: 72.54%\n",
      "Batch 169, Loss: 0.959549, Accuracy: 72.56%\n",
      "Batch 170, Loss: 1.045725, Accuracy: 72.55%\n",
      "Batch 171, Loss: 1.049898, Accuracy: 72.53%\n",
      "Batch 172, Loss: 1.134002, Accuracy: 72.47%\n",
      "Batch 173, Loss: 1.039802, Accuracy: 72.45%\n",
      "Batch 174, Loss: 1.001981, Accuracy: 72.45%\n",
      "Batch 175, Loss: 1.086939, Accuracy: 72.38%\n",
      "Batch 176, Loss: 1.004308, Accuracy: 72.40%\n",
      "Batch 177, Loss: 1.048403, Accuracy: 72.40%\n",
      "Batch 178, Loss: 1.035740, Accuracy: 72.39%\n",
      "Batch 179, Loss: 1.092552, Accuracy: 72.36%\n",
      "Batch 180, Loss: 1.071829, Accuracy: 72.31%\n",
      "Batch 181, Loss: 0.997360, Accuracy: 72.32%\n",
      "Batch 182, Loss: 0.977830, Accuracy: 72.34%\n",
      "Batch 183, Loss: 1.095223, Accuracy: 72.30%\n",
      "Batch 184, Loss: 0.966827, Accuracy: 72.33%\n",
      "Batch 185, Loss: 1.024161, Accuracy: 72.32%\n",
      "Batch 186, Loss: 0.948032, Accuracy: 72.36%\n",
      "Batch 187, Loss: 0.958869, Accuracy: 72.42%\n",
      "Batch 188, Loss: 0.989843, Accuracy: 72.42%\n",
      "Batch 189, Loss: 0.949878, Accuracy: 72.46%\n",
      "Batch 190, Loss: 0.949376, Accuracy: 72.51%\n",
      "Batch 191, Loss: 0.999342, Accuracy: 72.51%\n",
      "Batch 192, Loss: 1.011970, Accuracy: 72.51%\n",
      "Batch 193, Loss: 1.060156, Accuracy: 72.49%\n",
      "Batch 194, Loss: 0.998801, Accuracy: 72.50%\n",
      "Batch 195, Loss: 0.994869, Accuracy: 72.50%\n",
      "Batch 196, Loss: 0.993006, Accuracy: 72.50%\n",
      "Batch 197, Loss: 0.956801, Accuracy: 72.53%\n",
      "Batch 198, Loss: 0.969692, Accuracy: 72.55%\n",
      "Batch 199, Loss: 1.070657, Accuracy: 72.52%\n",
      "Batch 200, Loss: 1.030464, Accuracy: 72.51%\n",
      "Batch 201, Loss: 1.018439, Accuracy: 72.51%\n",
      "Batch 202, Loss: 1.028335, Accuracy: 72.52%\n",
      "Batch 203, Loss: 1.056281, Accuracy: 72.51%\n",
      "Batch 204, Loss: 0.931938, Accuracy: 72.55%\n",
      "Batch 205, Loss: 1.039727, Accuracy: 72.53%\n",
      "Batch 206, Loss: 0.940942, Accuracy: 72.57%\n",
      "Batch 207, Loss: 1.014099, Accuracy: 72.56%\n",
      "Batch 208, Loss: 0.978727, Accuracy: 72.58%\n",
      "Batch 209, Loss: 0.939515, Accuracy: 72.61%\n",
      "Batch 210, Loss: 0.934609, Accuracy: 72.65%\n",
      "Batch 211, Loss: 0.949395, Accuracy: 72.67%\n",
      "Batch 212, Loss: 0.966624, Accuracy: 72.70%\n",
      "Batch 213, Loss: 1.015176, Accuracy: 72.70%\n",
      "Training - Epoch 20, Loss: 1.014853, Accuracy: 72.70%\n",
      "Validation Batch 1, Loss: 1.061446, Accuracy: 67.19%\n",
      "Validation Batch 2, Loss: 1.155154, Accuracy: 61.72%\n",
      "Validation Batch 3, Loss: 1.157164, Accuracy: 60.42%\n",
      "Validation Batch 4, Loss: 1.049446, Accuracy: 62.50%\n",
      "Validation Batch 5, Loss: 1.128812, Accuracy: 61.25%\n",
      "Validation Batch 6, Loss: 1.081850, Accuracy: 61.46%\n",
      "Validation Batch 7, Loss: 1.100278, Accuracy: 61.83%\n",
      "Validation Batch 8, Loss: 1.131807, Accuracy: 61.72%\n",
      "Validation Batch 9, Loss: 1.135178, Accuracy: 61.63%\n",
      "Validation Batch 10, Loss: 1.090918, Accuracy: 62.03%\n",
      "Validation Batch 11, Loss: 1.071885, Accuracy: 62.64%\n",
      "Validation Batch 12, Loss: 1.040504, Accuracy: 63.02%\n",
      "Validation Batch 13, Loss: 1.150358, Accuracy: 62.62%\n",
      "Validation Batch 14, Loss: 1.085829, Accuracy: 62.61%\n",
      "Validation Batch 15, Loss: 1.079848, Accuracy: 62.71%\n",
      "Validation Batch 16, Loss: 1.111083, Accuracy: 62.50%\n",
      "Validation Batch 17, Loss: 1.190423, Accuracy: 61.95%\n",
      "Validation Batch 18, Loss: 1.080946, Accuracy: 62.07%\n",
      "Validation Batch 19, Loss: 1.140183, Accuracy: 61.84%\n",
      "Validation Batch 20, Loss: 1.106995, Accuracy: 61.72%\n",
      "Validation Batch 21, Loss: 1.100568, Accuracy: 61.83%\n",
      "Validation Batch 22, Loss: 1.165675, Accuracy: 61.36%\n",
      "Validation Batch 23, Loss: 1.195055, Accuracy: 60.87%\n",
      "Validation Batch 24, Loss: 1.100009, Accuracy: 61.07%\n",
      "Validation Batch 25, Loss: 1.096713, Accuracy: 61.06%\n",
      "Validation Batch 26, Loss: 1.081393, Accuracy: 61.24%\n",
      "Validation Batch 27, Loss: 1.113955, Accuracy: 61.36%\n",
      "Validation - Epoch 20, Loss: 1.111240, Accuracy: 61.36%\n",
      "Patience—0\n",
      "Epoch 21\n",
      "Batch 1, Loss: 1.021012, Accuracy: 73.44%\n",
      "Batch 2, Loss: 1.081835, Accuracy: 69.53%\n",
      "Batch 3, Loss: 1.001295, Accuracy: 71.35%\n",
      "Batch 4, Loss: 0.989497, Accuracy: 72.66%\n",
      "Batch 5, Loss: 0.989642, Accuracy: 73.44%\n",
      "Batch 6, Loss: 1.096783, Accuracy: 71.88%\n",
      "Batch 7, Loss: 0.942428, Accuracy: 73.21%\n",
      "Batch 8, Loss: 1.013862, Accuracy: 73.24%\n",
      "Batch 9, Loss: 1.060742, Accuracy: 72.74%\n",
      "Batch 10, Loss: 0.938630, Accuracy: 73.59%\n",
      "Batch 11, Loss: 1.049447, Accuracy: 73.15%\n",
      "Batch 12, Loss: 1.039870, Accuracy: 72.79%\n",
      "Batch 13, Loss: 1.035893, Accuracy: 72.60%\n",
      "Batch 14, Loss: 1.041067, Accuracy: 72.54%\n",
      "Batch 15, Loss: 1.073004, Accuracy: 72.08%\n",
      "Batch 16, Loss: 0.952820, Accuracy: 72.46%\n",
      "Batch 17, Loss: 1.009558, Accuracy: 72.52%\n",
      "Batch 18, Loss: 0.987567, Accuracy: 72.83%\n",
      "Batch 19, Loss: 1.024209, Accuracy: 72.78%\n",
      "Batch 20, Loss: 1.070108, Accuracy: 72.42%\n",
      "Batch 21, Loss: 0.999793, Accuracy: 72.62%\n",
      "Batch 22, Loss: 1.027501, Accuracy: 72.59%\n",
      "Batch 23, Loss: 0.981349, Accuracy: 72.89%\n",
      "Batch 24, Loss: 1.116277, Accuracy: 72.46%\n",
      "Batch 25, Loss: 1.018023, Accuracy: 72.44%\n",
      "Batch 26, Loss: 1.003477, Accuracy: 72.54%\n",
      "Batch 27, Loss: 1.052170, Accuracy: 72.34%\n",
      "Batch 28, Loss: 1.042763, Accuracy: 72.15%\n",
      "Batch 29, Loss: 0.995187, Accuracy: 72.20%\n",
      "Batch 30, Loss: 1.024236, Accuracy: 72.19%\n",
      "Batch 31, Loss: 1.008379, Accuracy: 72.18%\n",
      "Batch 32, Loss: 0.963541, Accuracy: 72.36%\n",
      "Batch 33, Loss: 1.041178, Accuracy: 72.40%\n",
      "Batch 34, Loss: 1.048956, Accuracy: 72.24%\n",
      "Batch 35, Loss: 1.036866, Accuracy: 72.19%\n",
      "Batch 36, Loss: 1.030489, Accuracy: 72.22%\n",
      "Batch 37, Loss: 1.076819, Accuracy: 72.00%\n",
      "Batch 38, Loss: 1.020255, Accuracy: 72.00%\n",
      "Batch 39, Loss: 1.081953, Accuracy: 71.79%\n",
      "Batch 40, Loss: 0.999805, Accuracy: 71.88%\n",
      "Batch 41, Loss: 0.954999, Accuracy: 72.07%\n",
      "Batch 42, Loss: 0.981294, Accuracy: 72.17%\n",
      "Batch 43, Loss: 1.003960, Accuracy: 72.24%\n",
      "Batch 44, Loss: 1.045386, Accuracy: 72.16%\n",
      "Batch 45, Loss: 1.028063, Accuracy: 72.15%\n",
      "Batch 46, Loss: 1.044810, Accuracy: 72.11%\n",
      "Batch 47, Loss: 1.002715, Accuracy: 72.21%\n",
      "Batch 48, Loss: 0.977987, Accuracy: 72.27%\n",
      "Batch 49, Loss: 1.020518, Accuracy: 72.23%\n",
      "Batch 50, Loss: 1.033692, Accuracy: 72.19%\n",
      "Batch 51, Loss: 1.048413, Accuracy: 72.12%\n",
      "Batch 52, Loss: 1.073259, Accuracy: 72.00%\n",
      "Batch 53, Loss: 1.007152, Accuracy: 72.05%\n",
      "Batch 54, Loss: 1.068479, Accuracy: 71.96%\n",
      "Batch 55, Loss: 0.946728, Accuracy: 72.16%\n",
      "Batch 56, Loss: 1.076584, Accuracy: 72.10%\n",
      "Batch 57, Loss: 1.049500, Accuracy: 72.07%\n",
      "Batch 58, Loss: 1.076027, Accuracy: 71.98%\n",
      "Batch 59, Loss: 1.031108, Accuracy: 71.98%\n",
      "Batch 60, Loss: 1.016758, Accuracy: 72.01%\n",
      "Batch 61, Loss: 1.108170, Accuracy: 71.88%\n",
      "Batch 62, Loss: 0.938239, Accuracy: 72.03%\n",
      "Batch 63, Loss: 0.999004, Accuracy: 72.05%\n",
      "Batch 64, Loss: 0.957453, Accuracy: 72.12%\n",
      "Batch 65, Loss: 1.057560, Accuracy: 72.02%\n",
      "Batch 66, Loss: 1.003039, Accuracy: 72.06%\n",
      "Batch 67, Loss: 0.923233, Accuracy: 72.25%\n",
      "Batch 68, Loss: 0.987632, Accuracy: 72.31%\n",
      "Batch 69, Loss: 1.025995, Accuracy: 72.31%\n",
      "Batch 70, Loss: 0.990549, Accuracy: 72.34%\n",
      "Batch 71, Loss: 1.013108, Accuracy: 72.36%\n",
      "Batch 72, Loss: 1.051081, Accuracy: 72.31%\n",
      "Batch 73, Loss: 0.989530, Accuracy: 72.37%\n",
      "Batch 74, Loss: 1.011675, Accuracy: 72.38%\n",
      "Batch 75, Loss: 0.989759, Accuracy: 72.46%\n",
      "Batch 76, Loss: 0.988481, Accuracy: 72.51%\n",
      "Batch 77, Loss: 0.981286, Accuracy: 72.56%\n",
      "Batch 78, Loss: 1.068399, Accuracy: 72.54%\n",
      "Batch 79, Loss: 1.015162, Accuracy: 72.55%\n",
      "Batch 80, Loss: 0.989224, Accuracy: 72.58%\n",
      "Batch 81, Loss: 0.921724, Accuracy: 72.70%\n",
      "Batch 82, Loss: 1.017438, Accuracy: 72.71%\n",
      "Batch 83, Loss: 0.935827, Accuracy: 72.82%\n",
      "Batch 84, Loss: 1.026308, Accuracy: 72.81%\n",
      "Batch 85, Loss: 0.950487, Accuracy: 72.90%\n",
      "Batch 86, Loss: 0.987980, Accuracy: 72.97%\n",
      "Batch 87, Loss: 1.015272, Accuracy: 72.95%\n",
      "Batch 88, Loss: 0.943450, Accuracy: 73.05%\n",
      "Batch 89, Loss: 1.003937, Accuracy: 73.07%\n",
      "Batch 90, Loss: 0.933276, Accuracy: 73.19%\n",
      "Batch 91, Loss: 1.020352, Accuracy: 73.15%\n",
      "Batch 92, Loss: 1.059817, Accuracy: 73.11%\n",
      "Batch 93, Loss: 0.912024, Accuracy: 73.22%\n",
      "Batch 94, Loss: 0.945882, Accuracy: 73.27%\n",
      "Batch 95, Loss: 1.011149, Accuracy: 73.26%\n",
      "Batch 96, Loss: 1.017339, Accuracy: 73.24%\n",
      "Batch 97, Loss: 1.078380, Accuracy: 73.20%\n",
      "Batch 98, Loss: 0.969610, Accuracy: 73.23%\n",
      "Batch 99, Loss: 0.973976, Accuracy: 73.30%\n",
      "Batch 100, Loss: 0.977153, Accuracy: 73.31%\n",
      "Batch 101, Loss: 0.933122, Accuracy: 73.41%\n",
      "Batch 102, Loss: 0.958156, Accuracy: 73.47%\n",
      "Batch 103, Loss: 0.994507, Accuracy: 73.48%\n",
      "Batch 104, Loss: 1.025896, Accuracy: 73.45%\n",
      "Batch 105, Loss: 0.945559, Accuracy: 73.51%\n",
      "Batch 106, Loss: 1.017339, Accuracy: 73.51%\n",
      "Batch 107, Loss: 1.086288, Accuracy: 73.42%\n",
      "Batch 108, Loss: 0.999149, Accuracy: 73.42%\n",
      "Batch 109, Loss: 0.993751, Accuracy: 73.45%\n",
      "Batch 110, Loss: 1.021786, Accuracy: 73.44%\n",
      "Batch 111, Loss: 0.933140, Accuracy: 73.51%\n",
      "Batch 112, Loss: 1.130487, Accuracy: 73.41%\n",
      "Batch 113, Loss: 0.944142, Accuracy: 73.48%\n",
      "Batch 114, Loss: 1.066490, Accuracy: 73.42%\n",
      "Batch 115, Loss: 0.955064, Accuracy: 73.46%\n",
      "Batch 116, Loss: 1.051568, Accuracy: 73.44%\n",
      "Batch 117, Loss: 1.017272, Accuracy: 73.44%\n",
      "Batch 118, Loss: 1.099856, Accuracy: 73.34%\n",
      "Batch 119, Loss: 0.998856, Accuracy: 73.36%\n",
      "Batch 120, Loss: 0.973684, Accuracy: 73.39%\n",
      "Batch 121, Loss: 1.078737, Accuracy: 73.32%\n",
      "Batch 122, Loss: 1.023491, Accuracy: 73.31%\n",
      "Batch 123, Loss: 0.975295, Accuracy: 73.34%\n",
      "Batch 124, Loss: 1.043446, Accuracy: 73.30%\n",
      "Batch 125, Loss: 1.038373, Accuracy: 73.30%\n",
      "Batch 126, Loss: 0.947389, Accuracy: 73.35%\n",
      "Batch 127, Loss: 1.054954, Accuracy: 73.33%\n",
      "Batch 128, Loss: 1.127872, Accuracy: 73.22%\n",
      "Batch 129, Loss: 1.059838, Accuracy: 73.18%\n",
      "Batch 130, Loss: 1.080278, Accuracy: 73.14%\n",
      "Batch 131, Loss: 0.921563, Accuracy: 73.20%\n",
      "Batch 132, Loss: 0.960564, Accuracy: 73.25%\n",
      "Batch 133, Loss: 0.994514, Accuracy: 73.25%\n",
      "Batch 134, Loss: 1.004185, Accuracy: 73.25%\n",
      "Batch 135, Loss: 0.985099, Accuracy: 73.28%\n",
      "Batch 136, Loss: 1.057968, Accuracy: 73.23%\n",
      "Batch 137, Loss: 1.022462, Accuracy: 73.22%\n",
      "Batch 138, Loss: 1.073923, Accuracy: 73.18%\n",
      "Batch 139, Loss: 0.903547, Accuracy: 73.27%\n",
      "Batch 140, Loss: 1.050902, Accuracy: 73.24%\n",
      "Batch 141, Loss: 1.060404, Accuracy: 73.18%\n",
      "Batch 142, Loss: 0.952303, Accuracy: 73.23%\n",
      "Batch 143, Loss: 1.025925, Accuracy: 73.22%\n",
      "Batch 144, Loss: 0.984416, Accuracy: 73.23%\n",
      "Batch 145, Loss: 1.043341, Accuracy: 73.20%\n",
      "Batch 146, Loss: 0.950354, Accuracy: 73.24%\n",
      "Batch 147, Loss: 1.071285, Accuracy: 73.20%\n",
      "Batch 148, Loss: 1.028275, Accuracy: 73.18%\n",
      "Batch 149, Loss: 1.011135, Accuracy: 73.16%\n",
      "Batch 150, Loss: 0.950098, Accuracy: 73.20%\n",
      "Batch 151, Loss: 0.920107, Accuracy: 73.25%\n",
      "Batch 152, Loss: 0.993094, Accuracy: 73.26%\n",
      "Batch 153, Loss: 1.018431, Accuracy: 73.26%\n",
      "Batch 154, Loss: 1.031790, Accuracy: 73.25%\n",
      "Batch 155, Loss: 1.050704, Accuracy: 73.24%\n",
      "Batch 156, Loss: 0.996871, Accuracy: 73.25%\n",
      "Batch 157, Loss: 1.137258, Accuracy: 73.16%\n",
      "Batch 158, Loss: 1.012319, Accuracy: 73.17%\n",
      "Batch 159, Loss: 1.059867, Accuracy: 73.14%\n",
      "Batch 160, Loss: 0.990842, Accuracy: 73.16%\n",
      "Batch 161, Loss: 0.997248, Accuracy: 73.17%\n",
      "Batch 162, Loss: 1.006543, Accuracy: 73.18%\n",
      "Batch 163, Loss: 0.988702, Accuracy: 73.19%\n",
      "Batch 164, Loss: 1.007122, Accuracy: 73.19%\n",
      "Batch 165, Loss: 1.058374, Accuracy: 73.14%\n",
      "Batch 166, Loss: 1.010632, Accuracy: 73.16%\n",
      "Batch 167, Loss: 0.979814, Accuracy: 73.18%\n",
      "Batch 168, Loss: 1.023007, Accuracy: 73.16%\n",
      "Batch 169, Loss: 1.009768, Accuracy: 73.16%\n",
      "Batch 170, Loss: 0.971240, Accuracy: 73.18%\n",
      "Batch 171, Loss: 0.994292, Accuracy: 73.19%\n",
      "Batch 172, Loss: 1.018899, Accuracy: 73.19%\n",
      "Batch 173, Loss: 0.963302, Accuracy: 73.20%\n",
      "Batch 174, Loss: 0.974336, Accuracy: 73.23%\n",
      "Batch 175, Loss: 1.094668, Accuracy: 73.18%\n",
      "Batch 176, Loss: 0.954642, Accuracy: 73.21%\n",
      "Batch 177, Loss: 1.017354, Accuracy: 73.22%\n",
      "Batch 178, Loss: 1.030831, Accuracy: 73.18%\n",
      "Batch 179, Loss: 1.017589, Accuracy: 73.18%\n",
      "Batch 180, Loss: 0.930992, Accuracy: 73.22%\n",
      "Batch 181, Loss: 0.951642, Accuracy: 73.25%\n",
      "Batch 182, Loss: 0.992367, Accuracy: 73.27%\n",
      "Batch 183, Loss: 0.958312, Accuracy: 73.28%\n",
      "Batch 184, Loss: 0.964756, Accuracy: 73.31%\n",
      "Batch 185, Loss: 0.989051, Accuracy: 73.33%\n",
      "Batch 186, Loss: 0.934354, Accuracy: 73.38%\n",
      "Batch 187, Loss: 0.990076, Accuracy: 73.40%\n",
      "Batch 188, Loss: 1.089850, Accuracy: 73.35%\n",
      "Batch 189, Loss: 1.009192, Accuracy: 73.33%\n",
      "Batch 190, Loss: 0.992547, Accuracy: 73.35%\n",
      "Batch 191, Loss: 1.097821, Accuracy: 73.31%\n",
      "Batch 192, Loss: 0.984399, Accuracy: 73.33%\n",
      "Batch 193, Loss: 1.034380, Accuracy: 73.32%\n",
      "Batch 194, Loss: 0.956603, Accuracy: 73.35%\n",
      "Batch 195, Loss: 1.014430, Accuracy: 73.35%\n",
      "Batch 196, Loss: 0.943870, Accuracy: 73.38%\n",
      "Batch 197, Loss: 1.034486, Accuracy: 73.36%\n",
      "Batch 198, Loss: 0.962426, Accuracy: 73.38%\n",
      "Batch 199, Loss: 1.038044, Accuracy: 73.35%\n",
      "Batch 200, Loss: 0.903246, Accuracy: 73.41%\n",
      "Batch 201, Loss: 0.996732, Accuracy: 73.41%\n",
      "Batch 202, Loss: 1.002165, Accuracy: 73.42%\n",
      "Batch 203, Loss: 0.995977, Accuracy: 73.43%\n",
      "Batch 204, Loss: 0.943517, Accuracy: 73.48%\n",
      "Batch 205, Loss: 1.040610, Accuracy: 73.45%\n",
      "Batch 206, Loss: 1.030840, Accuracy: 73.45%\n",
      "Batch 207, Loss: 1.063458, Accuracy: 73.42%\n",
      "Batch 208, Loss: 1.007313, Accuracy: 73.41%\n",
      "Batch 209, Loss: 0.925924, Accuracy: 73.47%\n",
      "Batch 210, Loss: 0.951159, Accuracy: 73.50%\n",
      "Batch 211, Loss: 0.991405, Accuracy: 73.50%\n",
      "Batch 212, Loss: 0.989467, Accuracy: 73.52%\n",
      "Batch 213, Loss: 1.073562, Accuracy: 73.48%\n",
      "Training - Epoch 21, Loss: 1.009429, Accuracy: 73.48%\n",
      "Validation Batch 1, Loss: 1.027112, Accuracy: 73.44%\n",
      "Validation Batch 2, Loss: 1.124934, Accuracy: 64.84%\n",
      "Validation Batch 3, Loss: 1.133530, Accuracy: 63.54%\n",
      "Validation Batch 4, Loss: 1.029712, Accuracy: 65.23%\n",
      "Validation Batch 5, Loss: 1.081393, Accuracy: 65.31%\n",
      "Validation Batch 6, Loss: 1.033631, Accuracy: 65.62%\n",
      "Validation Batch 7, Loss: 1.067634, Accuracy: 66.29%\n",
      "Validation Batch 8, Loss: 1.103928, Accuracy: 66.21%\n",
      "Validation Batch 9, Loss: 1.109466, Accuracy: 65.62%\n",
      "Validation Batch 10, Loss: 1.056267, Accuracy: 65.78%\n",
      "Validation Batch 11, Loss: 1.040040, Accuracy: 66.05%\n",
      "Validation Batch 12, Loss: 1.021169, Accuracy: 66.41%\n",
      "Validation Batch 13, Loss: 1.132646, Accuracy: 65.87%\n",
      "Validation Batch 14, Loss: 1.066299, Accuracy: 65.74%\n",
      "Validation Batch 15, Loss: 1.053435, Accuracy: 65.73%\n",
      "Validation Batch 16, Loss: 1.068106, Accuracy: 65.72%\n",
      "Validation Batch 17, Loss: 1.156136, Accuracy: 65.35%\n",
      "Validation Batch 18, Loss: 1.041269, Accuracy: 65.54%\n",
      "Validation Batch 19, Loss: 1.103324, Accuracy: 65.46%\n",
      "Validation Batch 20, Loss: 1.088514, Accuracy: 65.47%\n",
      "Validation Batch 21, Loss: 1.065360, Accuracy: 65.55%\n",
      "Validation Batch 22, Loss: 1.135946, Accuracy: 65.27%\n",
      "Validation Batch 23, Loss: 1.174803, Accuracy: 64.74%\n",
      "Validation Batch 24, Loss: 1.076848, Accuracy: 64.91%\n",
      "Validation Batch 25, Loss: 1.074727, Accuracy: 64.94%\n",
      "Validation Batch 26, Loss: 1.061438, Accuracy: 65.02%\n",
      "Validation Batch 27, Loss: 1.079629, Accuracy: 65.12%\n",
      "Validation - Epoch 21, Loss: 1.081752, Accuracy: 65.12%\n",
      "Patience—0\n",
      "Epoch 22\n",
      "Batch 1, Loss: 1.036622, Accuracy: 70.31%\n",
      "Batch 2, Loss: 1.071450, Accuracy: 67.97%\n",
      "Batch 3, Loss: 0.946414, Accuracy: 71.88%\n",
      "Batch 4, Loss: 0.995890, Accuracy: 72.27%\n",
      "Batch 5, Loss: 1.015616, Accuracy: 73.12%\n",
      "Batch 6, Loss: 1.072725, Accuracy: 72.40%\n",
      "Batch 7, Loss: 1.023526, Accuracy: 72.54%\n",
      "Batch 8, Loss: 0.992422, Accuracy: 72.66%\n",
      "Batch 9, Loss: 1.006857, Accuracy: 72.92%\n",
      "Batch 10, Loss: 0.992202, Accuracy: 73.44%\n",
      "Batch 11, Loss: 1.031262, Accuracy: 73.01%\n",
      "Batch 12, Loss: 1.011980, Accuracy: 73.18%\n",
      "Batch 13, Loss: 1.020257, Accuracy: 73.20%\n",
      "Batch 14, Loss: 1.085693, Accuracy: 72.66%\n",
      "Batch 15, Loss: 1.030586, Accuracy: 72.50%\n",
      "Batch 16, Loss: 0.997665, Accuracy: 72.66%\n",
      "Batch 17, Loss: 0.986850, Accuracy: 72.79%\n",
      "Batch 18, Loss: 1.009464, Accuracy: 72.92%\n",
      "Batch 19, Loss: 1.018271, Accuracy: 72.94%\n",
      "Batch 20, Loss: 0.967434, Accuracy: 73.20%\n",
      "Batch 21, Loss: 1.049328, Accuracy: 72.92%\n",
      "Batch 22, Loss: 0.995206, Accuracy: 73.01%\n",
      "Batch 23, Loss: 0.976900, Accuracy: 73.23%\n",
      "Batch 24, Loss: 1.027198, Accuracy: 73.11%\n",
      "Batch 25, Loss: 1.013748, Accuracy: 73.12%\n",
      "Batch 26, Loss: 1.006136, Accuracy: 73.14%\n",
      "Batch 27, Loss: 1.003315, Accuracy: 73.21%\n",
      "Batch 28, Loss: 0.929430, Accuracy: 73.49%\n",
      "Batch 29, Loss: 0.991978, Accuracy: 73.71%\n",
      "Batch 30, Loss: 0.976750, Accuracy: 73.80%\n",
      "Batch 31, Loss: 0.978865, Accuracy: 73.89%\n",
      "Batch 32, Loss: 1.052570, Accuracy: 73.73%\n",
      "Batch 33, Loss: 0.925255, Accuracy: 74.05%\n",
      "Batch 34, Loss: 0.969371, Accuracy: 74.08%\n",
      "Batch 35, Loss: 1.004559, Accuracy: 74.15%\n",
      "Batch 36, Loss: 0.949303, Accuracy: 74.26%\n",
      "Batch 37, Loss: 1.101999, Accuracy: 73.99%\n",
      "Batch 38, Loss: 1.069842, Accuracy: 73.81%\n",
      "Batch 39, Loss: 1.004044, Accuracy: 73.80%\n",
      "Batch 40, Loss: 0.928096, Accuracy: 73.98%\n",
      "Batch 41, Loss: 0.956013, Accuracy: 74.12%\n",
      "Batch 42, Loss: 0.983168, Accuracy: 74.18%\n",
      "Batch 43, Loss: 1.072684, Accuracy: 73.98%\n",
      "Batch 44, Loss: 0.974345, Accuracy: 74.04%\n",
      "Batch 45, Loss: 1.029037, Accuracy: 73.96%\n",
      "Batch 46, Loss: 1.019430, Accuracy: 73.91%\n",
      "Batch 47, Loss: 0.997145, Accuracy: 73.90%\n",
      "Batch 48, Loss: 1.008880, Accuracy: 73.89%\n",
      "Batch 49, Loss: 1.015516, Accuracy: 73.95%\n",
      "Batch 50, Loss: 0.959617, Accuracy: 74.06%\n",
      "Batch 51, Loss: 1.031958, Accuracy: 73.99%\n",
      "Batch 52, Loss: 1.065947, Accuracy: 73.89%\n",
      "Batch 53, Loss: 0.977723, Accuracy: 73.91%\n",
      "Batch 54, Loss: 1.026537, Accuracy: 73.84%\n",
      "Batch 55, Loss: 1.025468, Accuracy: 73.78%\n",
      "Batch 56, Loss: 1.015932, Accuracy: 73.69%\n",
      "Batch 57, Loss: 0.993572, Accuracy: 73.74%\n",
      "Batch 58, Loss: 1.038210, Accuracy: 73.73%\n",
      "Batch 59, Loss: 1.015843, Accuracy: 73.70%\n",
      "Batch 60, Loss: 1.056816, Accuracy: 73.59%\n",
      "Batch 61, Loss: 1.058289, Accuracy: 73.51%\n",
      "Batch 62, Loss: 1.017116, Accuracy: 73.56%\n",
      "Batch 63, Loss: 0.912420, Accuracy: 73.74%\n",
      "Batch 64, Loss: 1.118226, Accuracy: 73.56%\n",
      "Batch 65, Loss: 1.035006, Accuracy: 73.56%\n",
      "Batch 66, Loss: 0.997193, Accuracy: 73.56%\n",
      "Batch 67, Loss: 0.993596, Accuracy: 73.55%\n",
      "Batch 68, Loss: 0.947362, Accuracy: 73.62%\n",
      "Batch 69, Loss: 1.005513, Accuracy: 73.60%\n",
      "Batch 70, Loss: 0.953170, Accuracy: 73.71%\n",
      "Batch 71, Loss: 1.123898, Accuracy: 73.50%\n",
      "Batch 72, Loss: 0.975056, Accuracy: 73.57%\n",
      "Batch 73, Loss: 1.003337, Accuracy: 73.59%\n",
      "Batch 74, Loss: 0.992055, Accuracy: 73.56%\n",
      "Batch 75, Loss: 0.963241, Accuracy: 73.62%\n",
      "Batch 76, Loss: 0.957141, Accuracy: 73.73%\n",
      "Batch 77, Loss: 0.949078, Accuracy: 73.78%\n",
      "Batch 78, Loss: 0.988961, Accuracy: 73.84%\n",
      "Batch 79, Loss: 1.060092, Accuracy: 73.77%\n",
      "Batch 80, Loss: 0.995331, Accuracy: 73.81%\n",
      "Batch 81, Loss: 0.939841, Accuracy: 73.90%\n",
      "Batch 82, Loss: 0.983305, Accuracy: 73.91%\n",
      "Batch 83, Loss: 0.981426, Accuracy: 73.95%\n",
      "Batch 84, Loss: 0.987158, Accuracy: 73.96%\n",
      "Batch 85, Loss: 1.023336, Accuracy: 73.90%\n",
      "Batch 86, Loss: 0.977098, Accuracy: 73.93%\n",
      "Batch 87, Loss: 1.046761, Accuracy: 73.85%\n",
      "Batch 88, Loss: 0.986088, Accuracy: 73.86%\n",
      "Batch 89, Loss: 0.931282, Accuracy: 73.93%\n",
      "Batch 90, Loss: 0.902233, Accuracy: 74.06%\n",
      "Batch 91, Loss: 1.024828, Accuracy: 74.06%\n",
      "Batch 92, Loss: 0.988097, Accuracy: 74.08%\n",
      "Batch 93, Loss: 1.047536, Accuracy: 74.06%\n",
      "Batch 94, Loss: 1.045271, Accuracy: 73.99%\n",
      "Batch 95, Loss: 1.062978, Accuracy: 73.91%\n",
      "Batch 96, Loss: 0.975669, Accuracy: 73.99%\n",
      "Batch 97, Loss: 1.053481, Accuracy: 73.94%\n",
      "Batch 98, Loss: 1.018124, Accuracy: 73.92%\n",
      "Batch 99, Loss: 1.000306, Accuracy: 73.90%\n",
      "Batch 100, Loss: 0.961615, Accuracy: 73.94%\n",
      "Batch 101, Loss: 1.022256, Accuracy: 73.92%\n",
      "Batch 102, Loss: 1.036147, Accuracy: 73.87%\n",
      "Batch 103, Loss: 0.986737, Accuracy: 73.88%\n",
      "Batch 104, Loss: 0.939420, Accuracy: 73.96%\n",
      "Batch 105, Loss: 0.928734, Accuracy: 74.05%\n",
      "Batch 106, Loss: 1.053470, Accuracy: 73.98%\n",
      "Batch 107, Loss: 0.986765, Accuracy: 74.02%\n",
      "Batch 108, Loss: 1.044568, Accuracy: 73.97%\n",
      "Batch 109, Loss: 1.026559, Accuracy: 73.94%\n",
      "Batch 110, Loss: 1.022435, Accuracy: 73.93%\n",
      "Batch 111, Loss: 0.875957, Accuracy: 74.06%\n",
      "Batch 112, Loss: 1.061359, Accuracy: 74.00%\n",
      "Batch 113, Loss: 0.899322, Accuracy: 74.12%\n",
      "Batch 114, Loss: 1.004997, Accuracy: 74.12%\n",
      "Batch 115, Loss: 0.973532, Accuracy: 74.14%\n",
      "Batch 116, Loss: 1.022510, Accuracy: 74.12%\n",
      "Batch 117, Loss: 1.030376, Accuracy: 74.11%\n",
      "Batch 118, Loss: 1.030987, Accuracy: 74.05%\n",
      "Batch 119, Loss: 1.062574, Accuracy: 73.99%\n",
      "Batch 120, Loss: 1.046084, Accuracy: 73.95%\n",
      "Batch 121, Loss: 1.015960, Accuracy: 73.94%\n",
      "Batch 122, Loss: 1.006811, Accuracy: 73.94%\n",
      "Batch 123, Loss: 0.985431, Accuracy: 73.97%\n",
      "Batch 124, Loss: 0.981001, Accuracy: 73.99%\n",
      "Batch 125, Loss: 1.001192, Accuracy: 74.00%\n",
      "Batch 126, Loss: 0.971141, Accuracy: 74.02%\n",
      "Batch 127, Loss: 1.013177, Accuracy: 74.00%\n",
      "Batch 128, Loss: 0.978418, Accuracy: 74.01%\n",
      "Batch 129, Loss: 1.011800, Accuracy: 74.01%\n",
      "Batch 130, Loss: 0.936116, Accuracy: 74.06%\n",
      "Batch 131, Loss: 1.029241, Accuracy: 74.05%\n",
      "Batch 132, Loss: 0.957367, Accuracy: 74.10%\n",
      "Batch 133, Loss: 1.030379, Accuracy: 74.07%\n",
      "Batch 134, Loss: 0.955087, Accuracy: 74.11%\n",
      "Batch 135, Loss: 1.032770, Accuracy: 74.09%\n",
      "Batch 136, Loss: 0.915917, Accuracy: 74.16%\n",
      "Batch 137, Loss: 1.031113, Accuracy: 74.13%\n",
      "Batch 138, Loss: 1.048142, Accuracy: 74.08%\n",
      "Batch 139, Loss: 1.036805, Accuracy: 74.04%\n",
      "Batch 140, Loss: 1.046695, Accuracy: 74.03%\n",
      "Batch 141, Loss: 1.002331, Accuracy: 74.02%\n",
      "Batch 142, Loss: 1.009814, Accuracy: 74.02%\n",
      "Batch 143, Loss: 1.020883, Accuracy: 74.03%\n",
      "Batch 144, Loss: 0.976127, Accuracy: 74.03%\n",
      "Batch 145, Loss: 0.957336, Accuracy: 74.07%\n",
      "Batch 146, Loss: 1.035463, Accuracy: 74.05%\n",
      "Batch 147, Loss: 1.035285, Accuracy: 74.03%\n",
      "Batch 148, Loss: 1.022969, Accuracy: 74.03%\n",
      "Batch 149, Loss: 0.951818, Accuracy: 74.06%\n",
      "Batch 150, Loss: 0.949466, Accuracy: 74.10%\n",
      "Batch 151, Loss: 1.027466, Accuracy: 74.08%\n",
      "Batch 152, Loss: 1.046930, Accuracy: 74.04%\n",
      "Batch 153, Loss: 1.012747, Accuracy: 74.03%\n",
      "Batch 154, Loss: 0.993773, Accuracy: 74.05%\n",
      "Batch 155, Loss: 1.055440, Accuracy: 74.01%\n",
      "Batch 156, Loss: 1.040044, Accuracy: 73.99%\n",
      "Batch 157, Loss: 0.977613, Accuracy: 74.02%\n",
      "Batch 158, Loss: 1.008430, Accuracy: 74.03%\n",
      "Batch 159, Loss: 1.030584, Accuracy: 74.02%\n",
      "Batch 160, Loss: 1.002985, Accuracy: 74.02%\n",
      "Batch 161, Loss: 1.107795, Accuracy: 73.94%\n",
      "Batch 162, Loss: 1.002032, Accuracy: 73.96%\n",
      "Batch 163, Loss: 1.044910, Accuracy: 73.93%\n",
      "Batch 164, Loss: 0.993904, Accuracy: 73.93%\n",
      "Batch 165, Loss: 0.987191, Accuracy: 73.96%\n",
      "Batch 166, Loss: 1.045814, Accuracy: 73.95%\n",
      "Batch 167, Loss: 0.983745, Accuracy: 73.95%\n",
      "Batch 168, Loss: 1.038823, Accuracy: 73.93%\n",
      "Batch 169, Loss: 1.023526, Accuracy: 73.93%\n",
      "Batch 170, Loss: 1.012884, Accuracy: 73.93%\n",
      "Batch 171, Loss: 1.005530, Accuracy: 73.94%\n",
      "Batch 172, Loss: 0.939179, Accuracy: 73.98%\n",
      "Batch 173, Loss: 1.091810, Accuracy: 73.93%\n",
      "Batch 174, Loss: 1.096791, Accuracy: 73.87%\n",
      "Batch 175, Loss: 1.122764, Accuracy: 73.81%\n",
      "Batch 176, Loss: 1.111884, Accuracy: 73.75%\n",
      "Batch 177, Loss: 1.015661, Accuracy: 73.74%\n",
      "Batch 178, Loss: 1.073744, Accuracy: 73.69%\n",
      "Batch 179, Loss: 1.024939, Accuracy: 73.66%\n",
      "Batch 180, Loss: 0.979520, Accuracy: 73.69%\n",
      "Batch 181, Loss: 1.042278, Accuracy: 73.67%\n",
      "Batch 182, Loss: 0.978428, Accuracy: 73.70%\n",
      "Batch 183, Loss: 0.978325, Accuracy: 73.71%\n",
      "Batch 184, Loss: 1.008644, Accuracy: 73.72%\n",
      "Batch 185, Loss: 1.124917, Accuracy: 73.65%\n",
      "Batch 186, Loss: 1.036015, Accuracy: 73.64%\n",
      "Batch 187, Loss: 0.945984, Accuracy: 73.67%\n",
      "Batch 188, Loss: 0.997956, Accuracy: 73.68%\n",
      "Batch 189, Loss: 1.033467, Accuracy: 73.64%\n",
      "Batch 190, Loss: 1.068386, Accuracy: 73.61%\n",
      "Batch 191, Loss: 0.896455, Accuracy: 73.67%\n",
      "Batch 192, Loss: 1.000964, Accuracy: 73.67%\n",
      "Batch 193, Loss: 1.041108, Accuracy: 73.66%\n",
      "Batch 194, Loss: 0.944034, Accuracy: 73.68%\n",
      "Batch 195, Loss: 0.972267, Accuracy: 73.70%\n",
      "Batch 196, Loss: 1.052435, Accuracy: 73.68%\n",
      "Batch 197, Loss: 1.061407, Accuracy: 73.65%\n",
      "Batch 198, Loss: 1.065726, Accuracy: 73.63%\n",
      "Batch 199, Loss: 0.941844, Accuracy: 73.67%\n",
      "Batch 200, Loss: 1.013053, Accuracy: 73.67%\n",
      "Batch 201, Loss: 0.992131, Accuracy: 73.68%\n",
      "Batch 202, Loss: 0.905221, Accuracy: 73.74%\n",
      "Batch 203, Loss: 0.927036, Accuracy: 73.79%\n",
      "Batch 204, Loss: 1.056963, Accuracy: 73.76%\n",
      "Batch 205, Loss: 1.059453, Accuracy: 73.73%\n",
      "Batch 206, Loss: 1.012685, Accuracy: 73.73%\n",
      "Batch 207, Loss: 1.003505, Accuracy: 73.73%\n",
      "Batch 208, Loss: 1.053839, Accuracy: 73.69%\n",
      "Batch 209, Loss: 1.032787, Accuracy: 73.68%\n",
      "Batch 210, Loss: 1.034110, Accuracy: 73.67%\n",
      "Batch 211, Loss: 1.065420, Accuracy: 73.63%\n",
      "Batch 212, Loss: 0.986423, Accuracy: 73.65%\n",
      "Batch 213, Loss: 1.030434, Accuracy: 73.65%\n",
      "Training - Epoch 22, Loss: 1.008340, Accuracy: 73.65%\n",
      "Validation Batch 1, Loss: 1.006149, Accuracy: 71.88%\n",
      "Validation Batch 2, Loss: 1.109175, Accuracy: 64.84%\n",
      "Validation Batch 3, Loss: 1.114102, Accuracy: 64.06%\n",
      "Validation Batch 4, Loss: 1.017300, Accuracy: 66.02%\n",
      "Validation Batch 5, Loss: 1.049543, Accuracy: 66.25%\n",
      "Validation Batch 6, Loss: 0.993959, Accuracy: 67.97%\n",
      "Validation Batch 7, Loss: 1.051543, Accuracy: 68.53%\n",
      "Validation Batch 8, Loss: 1.081915, Accuracy: 68.55%\n",
      "Validation Batch 9, Loss: 1.089571, Accuracy: 68.06%\n",
      "Validation Batch 10, Loss: 1.031841, Accuracy: 68.44%\n",
      "Validation Batch 11, Loss: 1.023671, Accuracy: 68.47%\n",
      "Validation Batch 12, Loss: 1.001746, Accuracy: 69.01%\n",
      "Validation Batch 13, Loss: 1.103582, Accuracy: 68.63%\n",
      "Validation Batch 14, Loss: 1.050315, Accuracy: 68.64%\n",
      "Validation Batch 15, Loss: 1.032212, Accuracy: 68.75%\n",
      "Validation Batch 16, Loss: 1.027324, Accuracy: 69.14%\n",
      "Validation Batch 17, Loss: 1.122698, Accuracy: 68.75%\n",
      "Validation Batch 18, Loss: 1.022844, Accuracy: 69.01%\n",
      "Validation Batch 19, Loss: 1.077541, Accuracy: 68.83%\n",
      "Validation Batch 20, Loss: 1.074712, Accuracy: 68.67%\n",
      "Validation Batch 21, Loss: 1.039938, Accuracy: 68.53%\n",
      "Validation Batch 22, Loss: 1.109497, Accuracy: 68.11%\n",
      "Validation Batch 23, Loss: 1.142953, Accuracy: 67.73%\n",
      "Validation Batch 24, Loss: 1.055490, Accuracy: 67.90%\n",
      "Validation Batch 25, Loss: 1.046720, Accuracy: 67.88%\n",
      "Validation Batch 26, Loss: 1.047749, Accuracy: 67.91%\n",
      "Validation Batch 27, Loss: 1.045168, Accuracy: 67.94%\n",
      "Validation - Epoch 22, Loss: 1.058121, Accuracy: 67.94%\n",
      "Patience—0\n",
      "Epoch 23\n",
      "Batch 1, Loss: 1.066657, Accuracy: 68.75%\n",
      "Batch 2, Loss: 1.026847, Accuracy: 70.31%\n",
      "Batch 3, Loss: 1.015149, Accuracy: 71.35%\n",
      "Batch 4, Loss: 1.032786, Accuracy: 71.88%\n",
      "Batch 5, Loss: 0.981734, Accuracy: 73.44%\n",
      "Batch 6, Loss: 1.128990, Accuracy: 72.14%\n",
      "Batch 7, Loss: 0.972058, Accuracy: 72.77%\n",
      "Batch 8, Loss: 0.941712, Accuracy: 73.63%\n",
      "Batch 9, Loss: 0.999583, Accuracy: 73.78%\n",
      "Batch 10, Loss: 1.024504, Accuracy: 73.59%\n",
      "Batch 11, Loss: 0.986279, Accuracy: 73.58%\n",
      "Batch 12, Loss: 1.007864, Accuracy: 73.70%\n",
      "Batch 13, Loss: 0.969164, Accuracy: 74.04%\n",
      "Batch 14, Loss: 0.929273, Accuracy: 74.55%\n",
      "Batch 15, Loss: 1.024367, Accuracy: 74.38%\n",
      "Batch 16, Loss: 1.013632, Accuracy: 74.22%\n",
      "Batch 17, Loss: 0.949604, Accuracy: 74.63%\n",
      "Batch 18, Loss: 1.047017, Accuracy: 74.31%\n",
      "Batch 19, Loss: 1.043458, Accuracy: 74.10%\n",
      "Batch 20, Loss: 0.972325, Accuracy: 74.22%\n",
      "Batch 21, Loss: 1.030416, Accuracy: 74.03%\n",
      "Batch 22, Loss: 1.105342, Accuracy: 73.44%\n",
      "Batch 23, Loss: 1.040641, Accuracy: 73.37%\n",
      "Batch 24, Loss: 1.028805, Accuracy: 73.31%\n",
      "Batch 25, Loss: 0.978212, Accuracy: 73.44%\n",
      "Batch 26, Loss: 0.933826, Accuracy: 73.68%\n",
      "Batch 27, Loss: 0.994360, Accuracy: 73.73%\n",
      "Batch 28, Loss: 0.902040, Accuracy: 74.11%\n",
      "Batch 29, Loss: 0.915269, Accuracy: 74.46%\n",
      "Batch 30, Loss: 0.976035, Accuracy: 74.64%\n",
      "Batch 31, Loss: 1.026319, Accuracy: 74.60%\n",
      "Batch 32, Loss: 0.960741, Accuracy: 74.66%\n",
      "Batch 33, Loss: 0.993062, Accuracy: 74.67%\n",
      "Batch 34, Loss: 0.999860, Accuracy: 74.68%\n",
      "Batch 35, Loss: 0.984689, Accuracy: 74.73%\n",
      "Batch 36, Loss: 0.986246, Accuracy: 74.78%\n",
      "Batch 37, Loss: 1.062317, Accuracy: 74.58%\n",
      "Batch 38, Loss: 0.974677, Accuracy: 74.59%\n",
      "Batch 39, Loss: 1.008898, Accuracy: 74.48%\n",
      "Batch 40, Loss: 0.968050, Accuracy: 74.53%\n",
      "Batch 41, Loss: 1.051799, Accuracy: 74.39%\n",
      "Batch 42, Loss: 1.049616, Accuracy: 74.29%\n",
      "Batch 43, Loss: 0.995765, Accuracy: 74.27%\n",
      "Batch 44, Loss: 0.969077, Accuracy: 74.36%\n",
      "Batch 45, Loss: 0.963141, Accuracy: 74.41%\n",
      "Batch 46, Loss: 1.054912, Accuracy: 74.35%\n",
      "Batch 47, Loss: 1.112269, Accuracy: 74.04%\n",
      "Batch 48, Loss: 0.987482, Accuracy: 74.06%\n",
      "Batch 49, Loss: 0.945310, Accuracy: 74.17%\n",
      "Batch 50, Loss: 1.050462, Accuracy: 74.09%\n",
      "Batch 51, Loss: 0.954811, Accuracy: 74.23%\n",
      "Batch 52, Loss: 0.943058, Accuracy: 74.40%\n",
      "Batch 53, Loss: 1.049553, Accuracy: 74.26%\n",
      "Batch 54, Loss: 0.976635, Accuracy: 74.36%\n",
      "Batch 55, Loss: 1.011772, Accuracy: 74.32%\n",
      "Batch 56, Loss: 1.007730, Accuracy: 74.27%\n",
      "Batch 57, Loss: 0.937538, Accuracy: 74.45%\n",
      "Batch 58, Loss: 1.087909, Accuracy: 74.33%\n",
      "Batch 59, Loss: 0.988491, Accuracy: 74.34%\n",
      "Batch 60, Loss: 0.913624, Accuracy: 74.53%\n",
      "Batch 61, Loss: 0.999302, Accuracy: 74.49%\n",
      "Batch 62, Loss: 1.044845, Accuracy: 74.45%\n",
      "Batch 63, Loss: 1.030241, Accuracy: 74.36%\n",
      "Batch 64, Loss: 1.016222, Accuracy: 74.32%\n",
      "Batch 65, Loss: 1.068867, Accuracy: 74.23%\n",
      "Batch 66, Loss: 1.034324, Accuracy: 74.20%\n",
      "Batch 67, Loss: 0.848597, Accuracy: 74.49%\n",
      "Batch 68, Loss: 0.948557, Accuracy: 74.56%\n",
      "Batch 69, Loss: 1.126233, Accuracy: 74.39%\n",
      "Batch 70, Loss: 0.946842, Accuracy: 74.49%\n",
      "Batch 71, Loss: 1.094464, Accuracy: 74.38%\n",
      "Batch 72, Loss: 0.977712, Accuracy: 74.39%\n",
      "Batch 73, Loss: 0.888619, Accuracy: 74.55%\n",
      "Batch 74, Loss: 1.015095, Accuracy: 74.51%\n",
      "Batch 75, Loss: 0.981264, Accuracy: 74.50%\n",
      "Batch 76, Loss: 0.955921, Accuracy: 74.57%\n",
      "Batch 77, Loss: 0.930802, Accuracy: 74.68%\n",
      "Batch 78, Loss: 0.978469, Accuracy: 74.72%\n",
      "Batch 79, Loss: 0.961718, Accuracy: 74.76%\n",
      "Batch 80, Loss: 0.949807, Accuracy: 74.84%\n",
      "Batch 81, Loss: 0.984769, Accuracy: 74.85%\n",
      "Batch 82, Loss: 1.055282, Accuracy: 74.77%\n",
      "Batch 83, Loss: 1.016106, Accuracy: 74.74%\n",
      "Batch 84, Loss: 1.064549, Accuracy: 74.67%\n",
      "Batch 85, Loss: 0.998964, Accuracy: 74.67%\n",
      "Batch 86, Loss: 0.990129, Accuracy: 74.65%\n",
      "Batch 87, Loss: 1.103407, Accuracy: 74.52%\n",
      "Batch 88, Loss: 0.960556, Accuracy: 74.57%\n",
      "Batch 89, Loss: 0.999931, Accuracy: 74.58%\n",
      "Batch 90, Loss: 0.984318, Accuracy: 74.60%\n",
      "Batch 91, Loss: 0.988163, Accuracy: 74.62%\n",
      "Batch 92, Loss: 1.034307, Accuracy: 74.58%\n",
      "Batch 93, Loss: 1.041062, Accuracy: 74.55%\n",
      "Batch 94, Loss: 1.010443, Accuracy: 74.48%\n",
      "Batch 95, Loss: 1.072800, Accuracy: 74.41%\n",
      "Batch 96, Loss: 1.002608, Accuracy: 74.40%\n",
      "Batch 97, Loss: 0.978777, Accuracy: 74.40%\n",
      "Batch 98, Loss: 1.006446, Accuracy: 74.39%\n",
      "Batch 99, Loss: 1.043719, Accuracy: 74.31%\n",
      "Batch 100, Loss: 1.041632, Accuracy: 74.27%\n",
      "Batch 101, Loss: 1.065182, Accuracy: 74.21%\n",
      "Batch 102, Loss: 1.075021, Accuracy: 74.11%\n",
      "Batch 103, Loss: 0.991610, Accuracy: 74.14%\n",
      "Batch 104, Loss: 1.059263, Accuracy: 74.08%\n",
      "Batch 105, Loss: 0.959961, Accuracy: 74.12%\n",
      "Batch 106, Loss: 1.034715, Accuracy: 74.09%\n",
      "Batch 107, Loss: 1.035592, Accuracy: 74.05%\n",
      "Batch 108, Loss: 1.042117, Accuracy: 74.03%\n",
      "Batch 109, Loss: 1.003538, Accuracy: 74.01%\n",
      "Batch 110, Loss: 0.987573, Accuracy: 74.03%\n",
      "Batch 111, Loss: 0.932102, Accuracy: 74.10%\n",
      "Batch 112, Loss: 1.064740, Accuracy: 74.07%\n",
      "Batch 113, Loss: 0.949723, Accuracy: 74.13%\n",
      "Batch 114, Loss: 0.996824, Accuracy: 74.15%\n",
      "Batch 115, Loss: 0.994843, Accuracy: 74.14%\n",
      "Batch 116, Loss: 0.996304, Accuracy: 74.15%\n",
      "Batch 117, Loss: 0.959201, Accuracy: 74.20%\n",
      "Batch 118, Loss: 1.091714, Accuracy: 74.11%\n",
      "Batch 119, Loss: 0.977194, Accuracy: 74.16%\n",
      "Batch 120, Loss: 1.067025, Accuracy: 74.10%\n",
      "Batch 121, Loss: 0.996638, Accuracy: 74.11%\n",
      "Batch 122, Loss: 1.048409, Accuracy: 74.07%\n",
      "Batch 123, Loss: 0.995568, Accuracy: 74.05%\n",
      "Batch 124, Loss: 0.968778, Accuracy: 74.08%\n",
      "Batch 125, Loss: 1.038154, Accuracy: 74.06%\n",
      "Batch 126, Loss: 0.956331, Accuracy: 74.11%\n",
      "Batch 127, Loss: 1.010090, Accuracy: 74.08%\n",
      "Batch 128, Loss: 0.988113, Accuracy: 74.08%\n",
      "Batch 129, Loss: 0.988145, Accuracy: 74.08%\n",
      "Batch 130, Loss: 0.975233, Accuracy: 74.11%\n",
      "Batch 131, Loss: 1.011365, Accuracy: 74.09%\n",
      "Batch 132, Loss: 0.968250, Accuracy: 74.11%\n",
      "Batch 133, Loss: 0.951074, Accuracy: 74.15%\n",
      "Batch 134, Loss: 0.969772, Accuracy: 74.17%\n",
      "Batch 135, Loss: 0.877455, Accuracy: 74.28%\n",
      "Batch 136, Loss: 0.999320, Accuracy: 74.28%\n",
      "Batch 137, Loss: 1.006819, Accuracy: 74.27%\n",
      "Batch 138, Loss: 1.038080, Accuracy: 74.24%\n",
      "Batch 139, Loss: 1.069806, Accuracy: 74.19%\n",
      "Batch 140, Loss: 0.985251, Accuracy: 74.20%\n",
      "Batch 141, Loss: 0.976493, Accuracy: 74.22%\n",
      "Batch 142, Loss: 1.063739, Accuracy: 74.17%\n",
      "Batch 143, Loss: 1.031485, Accuracy: 74.13%\n",
      "Batch 144, Loss: 1.014786, Accuracy: 74.09%\n",
      "Batch 145, Loss: 0.903749, Accuracy: 74.17%\n",
      "Batch 146, Loss: 1.011755, Accuracy: 74.18%\n",
      "Batch 147, Loss: 1.061606, Accuracy: 74.14%\n",
      "Batch 148, Loss: 1.000934, Accuracy: 74.14%\n",
      "Batch 149, Loss: 1.116162, Accuracy: 74.09%\n",
      "Batch 150, Loss: 1.058273, Accuracy: 74.06%\n",
      "Batch 151, Loss: 1.121922, Accuracy: 73.98%\n",
      "Batch 152, Loss: 0.965688, Accuracy: 73.99%\n",
      "Batch 153, Loss: 1.004760, Accuracy: 74.00%\n",
      "Batch 154, Loss: 0.898212, Accuracy: 74.09%\n",
      "Batch 155, Loss: 1.044403, Accuracy: 74.07%\n",
      "Batch 156, Loss: 0.963636, Accuracy: 74.10%\n",
      "Batch 157, Loss: 0.953198, Accuracy: 74.13%\n",
      "Batch 158, Loss: 1.021351, Accuracy: 74.12%\n",
      "Batch 159, Loss: 0.971411, Accuracy: 74.14%\n",
      "Batch 160, Loss: 0.979968, Accuracy: 74.16%\n",
      "Batch 161, Loss: 1.025831, Accuracy: 74.15%\n",
      "Batch 162, Loss: 0.972609, Accuracy: 74.18%\n",
      "Batch 163, Loss: 1.049886, Accuracy: 74.15%\n",
      "Batch 164, Loss: 0.981910, Accuracy: 74.15%\n",
      "Batch 165, Loss: 1.024393, Accuracy: 74.14%\n",
      "Batch 166, Loss: 1.013910, Accuracy: 74.13%\n",
      "Batch 167, Loss: 1.004718, Accuracy: 74.13%\n",
      "Batch 168, Loss: 1.000839, Accuracy: 74.12%\n",
      "Batch 169, Loss: 0.987929, Accuracy: 74.13%\n",
      "Batch 170, Loss: 1.008010, Accuracy: 74.13%\n",
      "Batch 171, Loss: 0.954381, Accuracy: 74.15%\n",
      "Batch 172, Loss: 1.042812, Accuracy: 74.12%\n",
      "Batch 173, Loss: 1.056190, Accuracy: 74.10%\n",
      "Batch 174, Loss: 0.920677, Accuracy: 74.15%\n",
      "Batch 175, Loss: 0.966556, Accuracy: 74.17%\n",
      "Batch 176, Loss: 0.972838, Accuracy: 74.17%\n",
      "Batch 177, Loss: 1.044692, Accuracy: 74.16%\n",
      "Batch 178, Loss: 1.076501, Accuracy: 74.10%\n",
      "Batch 179, Loss: 1.063189, Accuracy: 74.07%\n",
      "Batch 180, Loss: 0.960347, Accuracy: 74.09%\n",
      "Batch 181, Loss: 1.131896, Accuracy: 74.02%\n",
      "Batch 182, Loss: 1.070158, Accuracy: 73.98%\n",
      "Batch 183, Loss: 1.066162, Accuracy: 73.95%\n",
      "Batch 184, Loss: 0.944698, Accuracy: 73.99%\n",
      "Batch 185, Loss: 1.002745, Accuracy: 73.99%\n",
      "Batch 186, Loss: 1.002567, Accuracy: 73.98%\n",
      "Batch 187, Loss: 0.954132, Accuracy: 74.01%\n",
      "Batch 188, Loss: 0.959295, Accuracy: 74.04%\n",
      "Batch 189, Loss: 0.965676, Accuracy: 74.06%\n",
      "Batch 190, Loss: 1.006161, Accuracy: 74.08%\n",
      "Batch 191, Loss: 1.019608, Accuracy: 74.07%\n",
      "Batch 192, Loss: 0.939119, Accuracy: 74.10%\n",
      "Batch 193, Loss: 0.914691, Accuracy: 74.15%\n",
      "Batch 194, Loss: 0.935861, Accuracy: 74.19%\n",
      "Batch 195, Loss: 1.016717, Accuracy: 74.19%\n",
      "Batch 196, Loss: 0.980117, Accuracy: 74.19%\n",
      "Batch 197, Loss: 1.022684, Accuracy: 74.18%\n",
      "Batch 198, Loss: 0.990272, Accuracy: 74.20%\n",
      "Batch 199, Loss: 1.048134, Accuracy: 74.17%\n",
      "Batch 200, Loss: 1.019789, Accuracy: 74.16%\n",
      "Batch 201, Loss: 1.019436, Accuracy: 74.14%\n",
      "Batch 202, Loss: 0.947587, Accuracy: 74.18%\n",
      "Batch 203, Loss: 1.057112, Accuracy: 74.15%\n",
      "Batch 204, Loss: 0.985854, Accuracy: 74.17%\n",
      "Batch 205, Loss: 1.016414, Accuracy: 74.16%\n",
      "Batch 206, Loss: 1.047342, Accuracy: 74.14%\n",
      "Batch 207, Loss: 1.028916, Accuracy: 74.12%\n",
      "Batch 208, Loss: 0.995738, Accuracy: 74.13%\n",
      "Batch 209, Loss: 1.046205, Accuracy: 74.11%\n",
      "Batch 210, Loss: 0.980712, Accuracy: 74.11%\n",
      "Batch 211, Loss: 1.030696, Accuracy: 74.10%\n",
      "Batch 212, Loss: 0.962777, Accuracy: 74.12%\n",
      "Batch 213, Loss: 1.003238, Accuracy: 74.12%\n",
      "Training - Epoch 23, Loss: 1.003572, Accuracy: 74.12%\n",
      "Validation Batch 1, Loss: 1.012835, Accuracy: 71.88%\n",
      "Validation Batch 2, Loss: 1.107340, Accuracy: 65.62%\n",
      "Validation Batch 3, Loss: 1.117633, Accuracy: 64.58%\n",
      "Validation Batch 4, Loss: 1.019920, Accuracy: 66.02%\n",
      "Validation Batch 5, Loss: 1.059264, Accuracy: 66.25%\n",
      "Validation Batch 6, Loss: 1.003757, Accuracy: 67.19%\n",
      "Validation Batch 7, Loss: 1.055311, Accuracy: 68.08%\n",
      "Validation Batch 8, Loss: 1.085554, Accuracy: 67.97%\n",
      "Validation Batch 9, Loss: 1.093058, Accuracy: 67.53%\n",
      "Validation Batch 10, Loss: 1.036654, Accuracy: 67.97%\n",
      "Validation Batch 11, Loss: 1.026729, Accuracy: 68.04%\n",
      "Validation Batch 12, Loss: 1.004234, Accuracy: 68.36%\n",
      "Validation Batch 13, Loss: 1.106983, Accuracy: 68.03%\n",
      "Validation Batch 14, Loss: 1.055053, Accuracy: 68.19%\n",
      "Validation Batch 15, Loss: 1.036450, Accuracy: 68.33%\n",
      "Validation Batch 16, Loss: 1.041185, Accuracy: 68.65%\n",
      "Validation Batch 17, Loss: 1.133256, Accuracy: 68.29%\n",
      "Validation Batch 18, Loss: 1.027267, Accuracy: 68.49%\n",
      "Validation Batch 19, Loss: 1.090906, Accuracy: 68.34%\n",
      "Validation Batch 20, Loss: 1.079544, Accuracy: 68.12%\n",
      "Validation Batch 21, Loss: 1.046384, Accuracy: 68.01%\n",
      "Validation Batch 22, Loss: 1.122119, Accuracy: 67.61%\n",
      "Validation Batch 23, Loss: 1.152843, Accuracy: 67.26%\n",
      "Validation Batch 24, Loss: 1.062260, Accuracy: 67.38%\n",
      "Validation Batch 25, Loss: 1.061337, Accuracy: 67.31%\n",
      "Validation Batch 26, Loss: 1.052489, Accuracy: 67.31%\n",
      "Validation Batch 27, Loss: 1.047930, Accuracy: 67.35%\n",
      "Validation - Epoch 23, Loss: 1.064381, Accuracy: 67.35%\n",
      "Patience—1\n",
      "Epoch 24\n",
      "Batch 1, Loss: 0.920284, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.981744, Accuracy: 80.47%\n",
      "Batch 3, Loss: 0.991472, Accuracy: 78.65%\n",
      "Batch 4, Loss: 1.038282, Accuracy: 76.56%\n",
      "Batch 5, Loss: 1.006480, Accuracy: 75.62%\n",
      "Batch 6, Loss: 0.996479, Accuracy: 75.78%\n",
      "Batch 7, Loss: 0.956204, Accuracy: 76.34%\n",
      "Batch 8, Loss: 0.946134, Accuracy: 76.76%\n",
      "Batch 9, Loss: 1.111923, Accuracy: 75.00%\n",
      "Batch 10, Loss: 0.956621, Accuracy: 75.47%\n",
      "Batch 11, Loss: 0.959625, Accuracy: 75.99%\n",
      "Batch 12, Loss: 0.962145, Accuracy: 76.17%\n",
      "Batch 13, Loss: 1.015185, Accuracy: 75.84%\n",
      "Batch 14, Loss: 0.929643, Accuracy: 76.23%\n",
      "Batch 15, Loss: 0.955890, Accuracy: 76.35%\n",
      "Batch 16, Loss: 0.944105, Accuracy: 76.76%\n",
      "Batch 17, Loss: 0.964392, Accuracy: 76.93%\n",
      "Batch 18, Loss: 1.095062, Accuracy: 76.22%\n",
      "Batch 19, Loss: 1.027591, Accuracy: 75.90%\n",
      "Batch 20, Loss: 0.974228, Accuracy: 75.86%\n",
      "Batch 21, Loss: 0.927962, Accuracy: 76.26%\n",
      "Batch 22, Loss: 0.974573, Accuracy: 76.21%\n",
      "Batch 23, Loss: 0.983970, Accuracy: 76.22%\n",
      "Batch 24, Loss: 1.003241, Accuracy: 75.98%\n",
      "Batch 25, Loss: 1.013931, Accuracy: 75.75%\n",
      "Batch 26, Loss: 1.052701, Accuracy: 75.48%\n",
      "Batch 27, Loss: 0.999094, Accuracy: 75.46%\n",
      "Batch 28, Loss: 1.055580, Accuracy: 75.33%\n",
      "Batch 29, Loss: 0.891445, Accuracy: 75.75%\n",
      "Batch 30, Loss: 0.992331, Accuracy: 75.78%\n",
      "Batch 31, Loss: 0.924209, Accuracy: 75.96%\n",
      "Batch 32, Loss: 0.976717, Accuracy: 75.98%\n",
      "Batch 33, Loss: 1.036088, Accuracy: 75.76%\n",
      "Batch 34, Loss: 0.916280, Accuracy: 76.01%\n",
      "Batch 35, Loss: 0.951349, Accuracy: 76.21%\n",
      "Batch 36, Loss: 0.986160, Accuracy: 76.17%\n",
      "Batch 37, Loss: 1.017338, Accuracy: 76.06%\n",
      "Batch 38, Loss: 1.000865, Accuracy: 76.03%\n",
      "Batch 39, Loss: 0.943257, Accuracy: 76.20%\n",
      "Batch 40, Loss: 0.985826, Accuracy: 76.17%\n",
      "Batch 41, Loss: 0.971689, Accuracy: 76.18%\n",
      "Batch 42, Loss: 1.050854, Accuracy: 76.00%\n",
      "Batch 43, Loss: 0.992683, Accuracy: 75.98%\n",
      "Batch 44, Loss: 0.950699, Accuracy: 76.07%\n",
      "Batch 45, Loss: 1.037104, Accuracy: 75.90%\n",
      "Batch 46, Loss: 1.034577, Accuracy: 75.82%\n",
      "Batch 47, Loss: 0.970890, Accuracy: 75.83%\n",
      "Batch 48, Loss: 1.130447, Accuracy: 75.55%\n",
      "Batch 49, Loss: 0.978318, Accuracy: 75.61%\n",
      "Batch 50, Loss: 1.010982, Accuracy: 75.56%\n",
      "Batch 51, Loss: 0.950511, Accuracy: 75.61%\n",
      "Batch 52, Loss: 0.942851, Accuracy: 75.72%\n",
      "Batch 53, Loss: 1.022833, Accuracy: 75.68%\n",
      "Batch 54, Loss: 1.056137, Accuracy: 75.52%\n",
      "Batch 55, Loss: 1.091913, Accuracy: 75.34%\n",
      "Batch 56, Loss: 0.953209, Accuracy: 75.39%\n",
      "Batch 57, Loss: 0.996818, Accuracy: 75.36%\n",
      "Batch 58, Loss: 1.025160, Accuracy: 75.30%\n",
      "Batch 59, Loss: 1.014468, Accuracy: 75.21%\n",
      "Batch 60, Loss: 0.918607, Accuracy: 75.34%\n",
      "Batch 61, Loss: 0.914135, Accuracy: 75.51%\n",
      "Batch 62, Loss: 1.028425, Accuracy: 75.45%\n",
      "Batch 63, Loss: 0.996289, Accuracy: 75.45%\n",
      "Batch 64, Loss: 0.931024, Accuracy: 75.51%\n",
      "Batch 65, Loss: 0.990676, Accuracy: 75.53%\n",
      "Batch 66, Loss: 1.061183, Accuracy: 75.40%\n",
      "Batch 67, Loss: 1.026937, Accuracy: 75.37%\n",
      "Batch 68, Loss: 0.911683, Accuracy: 75.51%\n",
      "Batch 69, Loss: 0.903945, Accuracy: 75.63%\n",
      "Batch 70, Loss: 1.039771, Accuracy: 75.54%\n",
      "Batch 71, Loss: 0.982070, Accuracy: 75.51%\n",
      "Batch 72, Loss: 0.984670, Accuracy: 75.50%\n",
      "Batch 73, Loss: 1.072096, Accuracy: 75.36%\n",
      "Batch 74, Loss: 0.997969, Accuracy: 75.36%\n",
      "Batch 75, Loss: 1.016128, Accuracy: 75.33%\n",
      "Batch 76, Loss: 0.944561, Accuracy: 75.37%\n",
      "Batch 77, Loss: 0.959893, Accuracy: 75.45%\n",
      "Batch 78, Loss: 1.073530, Accuracy: 75.32%\n",
      "Batch 79, Loss: 0.993306, Accuracy: 75.32%\n",
      "Batch 80, Loss: 1.072882, Accuracy: 75.20%\n",
      "Batch 81, Loss: 1.074239, Accuracy: 75.12%\n",
      "Batch 82, Loss: 0.992752, Accuracy: 75.13%\n",
      "Batch 83, Loss: 1.050415, Accuracy: 75.08%\n",
      "Batch 84, Loss: 0.959910, Accuracy: 75.13%\n",
      "Batch 85, Loss: 0.917221, Accuracy: 75.22%\n",
      "Batch 86, Loss: 1.006769, Accuracy: 75.20%\n",
      "Batch 87, Loss: 0.993415, Accuracy: 75.18%\n",
      "Batch 88, Loss: 0.995462, Accuracy: 75.16%\n",
      "Batch 89, Loss: 1.024053, Accuracy: 75.11%\n",
      "Batch 90, Loss: 1.029758, Accuracy: 75.07%\n",
      "Batch 91, Loss: 1.009382, Accuracy: 75.03%\n",
      "Batch 92, Loss: 0.998998, Accuracy: 75.02%\n",
      "Batch 93, Loss: 0.991846, Accuracy: 75.02%\n",
      "Batch 94, Loss: 0.924302, Accuracy: 75.08%\n",
      "Batch 95, Loss: 0.965407, Accuracy: 75.13%\n",
      "Batch 96, Loss: 1.100419, Accuracy: 75.02%\n",
      "Batch 97, Loss: 0.959776, Accuracy: 75.06%\n",
      "Batch 98, Loss: 1.001321, Accuracy: 75.03%\n",
      "Batch 99, Loss: 1.025367, Accuracy: 75.00%\n",
      "Batch 100, Loss: 1.056274, Accuracy: 74.95%\n",
      "Batch 101, Loss: 0.947605, Accuracy: 75.02%\n",
      "Batch 102, Loss: 1.030201, Accuracy: 74.97%\n",
      "Batch 103, Loss: 0.987882, Accuracy: 74.98%\n",
      "Batch 104, Loss: 0.992573, Accuracy: 74.98%\n",
      "Batch 105, Loss: 0.972059, Accuracy: 75.01%\n",
      "Batch 106, Loss: 0.972573, Accuracy: 75.03%\n",
      "Batch 107, Loss: 0.993487, Accuracy: 75.04%\n",
      "Batch 108, Loss: 1.021172, Accuracy: 75.03%\n",
      "Batch 109, Loss: 1.023042, Accuracy: 75.01%\n",
      "Batch 110, Loss: 0.996173, Accuracy: 75.01%\n",
      "Batch 111, Loss: 0.983702, Accuracy: 75.03%\n",
      "Batch 112, Loss: 1.016549, Accuracy: 75.00%\n",
      "Batch 113, Loss: 1.054955, Accuracy: 74.92%\n",
      "Batch 114, Loss: 1.026733, Accuracy: 74.90%\n",
      "Batch 115, Loss: 0.964155, Accuracy: 74.93%\n",
      "Batch 116, Loss: 0.998375, Accuracy: 74.95%\n",
      "Batch 117, Loss: 0.954430, Accuracy: 74.97%\n",
      "Batch 118, Loss: 0.931019, Accuracy: 75.03%\n",
      "Batch 119, Loss: 1.084316, Accuracy: 74.93%\n",
      "Batch 120, Loss: 0.946219, Accuracy: 74.99%\n",
      "Batch 121, Loss: 0.930739, Accuracy: 75.04%\n",
      "Batch 122, Loss: 0.978069, Accuracy: 75.08%\n",
      "Batch 123, Loss: 1.043171, Accuracy: 75.04%\n",
      "Batch 124, Loss: 1.056447, Accuracy: 74.99%\n",
      "Batch 125, Loss: 1.015663, Accuracy: 74.95%\n",
      "Batch 126, Loss: 1.099966, Accuracy: 74.86%\n",
      "Batch 127, Loss: 1.033509, Accuracy: 74.83%\n",
      "Batch 128, Loss: 0.996511, Accuracy: 74.83%\n",
      "Batch 129, Loss: 1.044596, Accuracy: 74.81%\n",
      "Batch 130, Loss: 0.914218, Accuracy: 74.89%\n",
      "Batch 131, Loss: 1.035125, Accuracy: 74.87%\n",
      "Batch 132, Loss: 1.053999, Accuracy: 74.82%\n",
      "Batch 133, Loss: 1.043789, Accuracy: 74.79%\n",
      "Batch 134, Loss: 1.022737, Accuracy: 74.78%\n",
      "Batch 135, Loss: 0.962784, Accuracy: 74.81%\n",
      "Batch 136, Loss: 1.037162, Accuracy: 74.78%\n",
      "Batch 137, Loss: 0.883059, Accuracy: 74.89%\n",
      "Batch 138, Loss: 0.961953, Accuracy: 74.92%\n",
      "Batch 139, Loss: 1.030155, Accuracy: 74.91%\n",
      "Batch 140, Loss: 1.005755, Accuracy: 74.89%\n",
      "Batch 141, Loss: 0.923250, Accuracy: 74.96%\n",
      "Batch 142, Loss: 0.886423, Accuracy: 75.03%\n",
      "Batch 143, Loss: 1.021328, Accuracy: 75.01%\n",
      "Batch 144, Loss: 0.982618, Accuracy: 75.03%\n",
      "Batch 145, Loss: 0.952487, Accuracy: 75.08%\n",
      "Batch 146, Loss: 1.057482, Accuracy: 75.02%\n",
      "Batch 147, Loss: 1.051763, Accuracy: 74.98%\n",
      "Batch 148, Loss: 0.960579, Accuracy: 75.00%\n",
      "Batch 149, Loss: 0.995980, Accuracy: 75.01%\n",
      "Batch 150, Loss: 0.994231, Accuracy: 75.01%\n",
      "Batch 151, Loss: 0.937110, Accuracy: 75.05%\n",
      "Batch 152, Loss: 1.037693, Accuracy: 75.00%\n",
      "Batch 153, Loss: 0.980087, Accuracy: 75.02%\n",
      "Batch 154, Loss: 1.022063, Accuracy: 74.99%\n",
      "Batch 155, Loss: 1.068065, Accuracy: 74.95%\n",
      "Batch 156, Loss: 0.970460, Accuracy: 74.96%\n",
      "Batch 157, Loss: 1.074679, Accuracy: 74.90%\n",
      "Batch 158, Loss: 1.173220, Accuracy: 74.78%\n",
      "Batch 159, Loss: 0.997572, Accuracy: 74.80%\n",
      "Batch 160, Loss: 0.985875, Accuracy: 74.81%\n",
      "Batch 161, Loss: 1.033637, Accuracy: 74.79%\n",
      "Batch 162, Loss: 0.984959, Accuracy: 74.79%\n",
      "Batch 163, Loss: 0.959206, Accuracy: 74.80%\n",
      "Batch 164, Loss: 1.078114, Accuracy: 74.72%\n",
      "Batch 165, Loss: 0.972595, Accuracy: 74.73%\n",
      "Batch 166, Loss: 1.016772, Accuracy: 74.72%\n",
      "Batch 167, Loss: 0.954392, Accuracy: 74.74%\n",
      "Batch 168, Loss: 0.961955, Accuracy: 74.77%\n",
      "Batch 169, Loss: 1.083505, Accuracy: 74.71%\n",
      "Batch 170, Loss: 0.998236, Accuracy: 74.72%\n",
      "Batch 171, Loss: 0.972010, Accuracy: 74.73%\n",
      "Batch 172, Loss: 0.939475, Accuracy: 74.77%\n",
      "Batch 173, Loss: 0.926485, Accuracy: 74.83%\n",
      "Batch 174, Loss: 1.020583, Accuracy: 74.80%\n",
      "Batch 175, Loss: 1.031035, Accuracy: 74.79%\n",
      "Batch 176, Loss: 1.009443, Accuracy: 74.80%\n",
      "Batch 177, Loss: 0.977492, Accuracy: 74.81%\n",
      "Batch 178, Loss: 1.042530, Accuracy: 74.78%\n",
      "Batch 179, Loss: 0.928218, Accuracy: 74.82%\n",
      "Batch 180, Loss: 0.939685, Accuracy: 74.85%\n",
      "Batch 181, Loss: 0.990982, Accuracy: 74.84%\n",
      "Batch 182, Loss: 1.097509, Accuracy: 74.79%\n",
      "Batch 183, Loss: 1.038444, Accuracy: 74.76%\n",
      "Batch 184, Loss: 0.928168, Accuracy: 74.80%\n",
      "Batch 185, Loss: 0.945059, Accuracy: 74.83%\n",
      "Batch 186, Loss: 0.952137, Accuracy: 74.85%\n",
      "Batch 187, Loss: 1.045881, Accuracy: 74.82%\n",
      "Batch 188, Loss: 1.058880, Accuracy: 74.78%\n",
      "Batch 189, Loss: 0.926236, Accuracy: 74.82%\n",
      "Batch 190, Loss: 1.026945, Accuracy: 74.79%\n",
      "Batch 191, Loss: 1.083014, Accuracy: 74.75%\n",
      "Batch 192, Loss: 1.040171, Accuracy: 74.72%\n",
      "Batch 193, Loss: 1.024331, Accuracy: 74.71%\n",
      "Batch 194, Loss: 0.962599, Accuracy: 74.72%\n",
      "Batch 195, Loss: 0.932936, Accuracy: 74.75%\n",
      "Batch 196, Loss: 1.045853, Accuracy: 74.72%\n",
      "Batch 197, Loss: 1.063371, Accuracy: 74.69%\n",
      "Batch 198, Loss: 1.147465, Accuracy: 74.60%\n",
      "Batch 199, Loss: 1.002689, Accuracy: 74.62%\n",
      "Batch 200, Loss: 1.020758, Accuracy: 74.62%\n",
      "Batch 201, Loss: 0.948861, Accuracy: 74.64%\n",
      "Batch 202, Loss: 0.969714, Accuracy: 74.65%\n",
      "Batch 203, Loss: 1.111233, Accuracy: 74.58%\n",
      "Batch 204, Loss: 1.091777, Accuracy: 74.53%\n",
      "Batch 205, Loss: 0.944448, Accuracy: 74.56%\n",
      "Batch 206, Loss: 1.045395, Accuracy: 74.53%\n",
      "Batch 207, Loss: 1.065332, Accuracy: 74.51%\n",
      "Batch 208, Loss: 0.983355, Accuracy: 74.51%\n",
      "Batch 209, Loss: 0.946682, Accuracy: 74.54%\n",
      "Batch 210, Loss: 1.010513, Accuracy: 74.54%\n",
      "Batch 211, Loss: 1.051382, Accuracy: 74.53%\n",
      "Batch 212, Loss: 1.000201, Accuracy: 74.53%\n",
      "Batch 213, Loss: 0.938638, Accuracy: 74.55%\n",
      "Training - Epoch 24, Loss: 0.999231, Accuracy: 74.55%\n",
      "Validation Batch 1, Loss: 0.994166, Accuracy: 76.56%\n",
      "Validation Batch 2, Loss: 1.089038, Accuracy: 67.97%\n",
      "Validation Batch 3, Loss: 1.090900, Accuracy: 67.19%\n",
      "Validation Batch 4, Loss: 0.999599, Accuracy: 68.75%\n",
      "Validation Batch 5, Loss: 1.019050, Accuracy: 69.69%\n",
      "Validation Batch 6, Loss: 0.971042, Accuracy: 71.09%\n",
      "Validation Batch 7, Loss: 1.031159, Accuracy: 71.43%\n",
      "Validation Batch 8, Loss: 1.068533, Accuracy: 70.90%\n",
      "Validation Batch 9, Loss: 1.070081, Accuracy: 70.66%\n",
      "Validation Batch 10, Loss: 1.012473, Accuracy: 70.94%\n",
      "Validation Batch 11, Loss: 0.999653, Accuracy: 71.45%\n",
      "Validation Batch 12, Loss: 0.987073, Accuracy: 71.74%\n",
      "Validation Batch 13, Loss: 1.086856, Accuracy: 71.27%\n",
      "Validation Batch 14, Loss: 1.032833, Accuracy: 71.09%\n",
      "Validation Batch 15, Loss: 1.017142, Accuracy: 71.15%\n",
      "Validation Batch 16, Loss: 1.017619, Accuracy: 71.39%\n",
      "Validation Batch 17, Loss: 1.103484, Accuracy: 70.96%\n",
      "Validation Batch 18, Loss: 1.005631, Accuracy: 71.27%\n",
      "Validation Batch 19, Loss: 1.052885, Accuracy: 71.05%\n",
      "Validation Batch 20, Loss: 1.042481, Accuracy: 71.02%\n",
      "Validation Batch 21, Loss: 1.023132, Accuracy: 71.06%\n",
      "Validation Batch 22, Loss: 1.088382, Accuracy: 70.74%\n",
      "Validation Batch 23, Loss: 1.127653, Accuracy: 70.18%\n",
      "Validation Batch 24, Loss: 1.043221, Accuracy: 70.18%\n",
      "Validation Batch 25, Loss: 1.041814, Accuracy: 70.00%\n",
      "Validation Batch 26, Loss: 1.035713, Accuracy: 70.07%\n",
      "Validation Batch 27, Loss: 1.015837, Accuracy: 70.11%\n",
      "Validation - Epoch 24, Loss: 1.039535, Accuracy: 70.11%\n",
      "Patience—0\n",
      "Epoch 25\n",
      "Batch 1, Loss: 1.011690, Accuracy: 73.44%\n",
      "Batch 2, Loss: 1.072878, Accuracy: 70.31%\n",
      "Batch 3, Loss: 1.023972, Accuracy: 70.83%\n",
      "Batch 4, Loss: 1.046537, Accuracy: 70.70%\n",
      "Batch 5, Loss: 1.033638, Accuracy: 71.25%\n",
      "Batch 6, Loss: 1.012316, Accuracy: 71.61%\n",
      "Batch 7, Loss: 0.943025, Accuracy: 72.77%\n",
      "Batch 8, Loss: 1.029930, Accuracy: 72.46%\n",
      "Batch 9, Loss: 0.952511, Accuracy: 73.44%\n",
      "Batch 10, Loss: 1.075160, Accuracy: 72.66%\n",
      "Batch 11, Loss: 0.963221, Accuracy: 73.15%\n",
      "Batch 12, Loss: 0.959959, Accuracy: 73.57%\n",
      "Batch 13, Loss: 0.905103, Accuracy: 74.40%\n",
      "Batch 14, Loss: 0.970020, Accuracy: 74.67%\n",
      "Batch 15, Loss: 0.970606, Accuracy: 74.90%\n",
      "Batch 16, Loss: 0.946587, Accuracy: 75.20%\n",
      "Batch 17, Loss: 0.996789, Accuracy: 75.09%\n",
      "Batch 18, Loss: 0.930748, Accuracy: 75.43%\n",
      "Batch 19, Loss: 0.930423, Accuracy: 75.74%\n",
      "Batch 20, Loss: 0.936313, Accuracy: 76.02%\n",
      "Batch 21, Loss: 1.058462, Accuracy: 75.74%\n",
      "Batch 22, Loss: 0.994856, Accuracy: 75.78%\n",
      "Batch 23, Loss: 1.038574, Accuracy: 75.48%\n",
      "Batch 24, Loss: 1.027048, Accuracy: 75.33%\n",
      "Batch 25, Loss: 0.911914, Accuracy: 75.62%\n",
      "Batch 26, Loss: 1.017614, Accuracy: 75.48%\n",
      "Batch 27, Loss: 1.028485, Accuracy: 75.29%\n",
      "Batch 28, Loss: 0.994002, Accuracy: 75.28%\n",
      "Batch 29, Loss: 1.003760, Accuracy: 75.27%\n",
      "Batch 30, Loss: 0.978899, Accuracy: 75.26%\n",
      "Batch 31, Loss: 0.983382, Accuracy: 75.30%\n",
      "Batch 32, Loss: 0.992719, Accuracy: 75.20%\n",
      "Batch 33, Loss: 1.028378, Accuracy: 75.14%\n",
      "Batch 34, Loss: 1.000669, Accuracy: 75.09%\n",
      "Batch 35, Loss: 1.057449, Accuracy: 74.91%\n",
      "Batch 36, Loss: 0.937750, Accuracy: 75.17%\n",
      "Batch 37, Loss: 1.029626, Accuracy: 75.00%\n",
      "Batch 38, Loss: 0.997831, Accuracy: 75.00%\n",
      "Batch 39, Loss: 0.932319, Accuracy: 75.16%\n",
      "Batch 40, Loss: 0.976008, Accuracy: 75.16%\n",
      "Batch 41, Loss: 0.952158, Accuracy: 75.30%\n",
      "Batch 42, Loss: 0.995103, Accuracy: 75.37%\n",
      "Batch 43, Loss: 0.979504, Accuracy: 75.44%\n",
      "Batch 44, Loss: 0.956056, Accuracy: 75.50%\n",
      "Batch 45, Loss: 1.037069, Accuracy: 75.38%\n",
      "Batch 46, Loss: 0.961787, Accuracy: 75.44%\n",
      "Batch 47, Loss: 0.980807, Accuracy: 75.43%\n",
      "Batch 48, Loss: 1.020292, Accuracy: 75.26%\n",
      "Batch 49, Loss: 0.928728, Accuracy: 75.41%\n",
      "Batch 50, Loss: 0.941146, Accuracy: 75.53%\n",
      "Batch 51, Loss: 0.902129, Accuracy: 75.74%\n",
      "Batch 52, Loss: 1.054468, Accuracy: 75.60%\n",
      "Batch 53, Loss: 0.900970, Accuracy: 75.77%\n",
      "Batch 54, Loss: 0.972425, Accuracy: 75.81%\n",
      "Batch 55, Loss: 1.026513, Accuracy: 75.71%\n",
      "Batch 56, Loss: 1.114437, Accuracy: 75.45%\n",
      "Batch 57, Loss: 1.007923, Accuracy: 75.38%\n",
      "Batch 58, Loss: 0.924131, Accuracy: 75.51%\n",
      "Batch 59, Loss: 1.030285, Accuracy: 75.42%\n",
      "Batch 60, Loss: 0.947109, Accuracy: 75.52%\n",
      "Batch 61, Loss: 1.032739, Accuracy: 75.41%\n",
      "Batch 62, Loss: 0.983726, Accuracy: 75.40%\n",
      "Batch 63, Loss: 1.013383, Accuracy: 75.35%\n",
      "Batch 64, Loss: 1.072176, Accuracy: 75.20%\n",
      "Batch 65, Loss: 1.026701, Accuracy: 75.17%\n",
      "Batch 66, Loss: 0.994695, Accuracy: 75.17%\n",
      "Batch 67, Loss: 1.072113, Accuracy: 75.02%\n",
      "Batch 68, Loss: 1.036978, Accuracy: 74.95%\n",
      "Batch 69, Loss: 1.029351, Accuracy: 74.89%\n",
      "Batch 70, Loss: 0.966182, Accuracy: 74.93%\n",
      "Batch 71, Loss: 1.009231, Accuracy: 74.91%\n",
      "Batch 72, Loss: 0.965248, Accuracy: 74.96%\n",
      "Batch 73, Loss: 0.954056, Accuracy: 75.04%\n",
      "Batch 74, Loss: 1.086435, Accuracy: 74.92%\n",
      "Batch 75, Loss: 1.000767, Accuracy: 74.88%\n",
      "Batch 76, Loss: 1.064019, Accuracy: 74.84%\n",
      "Batch 77, Loss: 0.971542, Accuracy: 74.88%\n",
      "Batch 78, Loss: 1.043471, Accuracy: 74.86%\n",
      "Batch 79, Loss: 0.993636, Accuracy: 74.88%\n",
      "Batch 80, Loss: 0.932742, Accuracy: 74.94%\n",
      "Batch 81, Loss: 0.926480, Accuracy: 75.04%\n",
      "Batch 82, Loss: 1.054944, Accuracy: 74.96%\n",
      "Batch 83, Loss: 1.018171, Accuracy: 74.94%\n",
      "Batch 84, Loss: 0.998517, Accuracy: 74.94%\n",
      "Batch 85, Loss: 1.041005, Accuracy: 74.89%\n",
      "Batch 86, Loss: 0.980117, Accuracy: 74.91%\n",
      "Batch 87, Loss: 1.089365, Accuracy: 74.80%\n",
      "Batch 88, Loss: 1.054290, Accuracy: 74.70%\n",
      "Batch 89, Loss: 1.033152, Accuracy: 74.63%\n",
      "Batch 90, Loss: 1.000793, Accuracy: 74.65%\n",
      "Batch 91, Loss: 0.986737, Accuracy: 74.69%\n",
      "Batch 92, Loss: 1.013485, Accuracy: 74.66%\n",
      "Batch 93, Loss: 1.084549, Accuracy: 74.55%\n",
      "Batch 94, Loss: 0.957754, Accuracy: 74.62%\n",
      "Batch 95, Loss: 1.016416, Accuracy: 74.56%\n",
      "Batch 96, Loss: 0.998809, Accuracy: 74.58%\n",
      "Batch 97, Loss: 0.979715, Accuracy: 74.60%\n",
      "Batch 98, Loss: 0.930017, Accuracy: 74.68%\n",
      "Batch 99, Loss: 0.981809, Accuracy: 74.68%\n",
      "Batch 100, Loss: 0.956221, Accuracy: 74.73%\n",
      "Batch 101, Loss: 1.031007, Accuracy: 74.71%\n",
      "Batch 102, Loss: 1.068585, Accuracy: 74.62%\n",
      "Batch 103, Loss: 1.096879, Accuracy: 74.51%\n",
      "Batch 104, Loss: 0.992657, Accuracy: 74.53%\n",
      "Batch 105, Loss: 1.013006, Accuracy: 74.52%\n",
      "Batch 106, Loss: 1.053263, Accuracy: 74.50%\n",
      "Batch 107, Loss: 1.059050, Accuracy: 74.45%\n",
      "Batch 108, Loss: 1.061393, Accuracy: 74.38%\n",
      "Batch 109, Loss: 1.010669, Accuracy: 74.37%\n",
      "Batch 110, Loss: 0.978568, Accuracy: 74.39%\n",
      "Batch 111, Loss: 1.007212, Accuracy: 74.38%\n",
      "Batch 112, Loss: 0.896965, Accuracy: 74.50%\n",
      "Batch 113, Loss: 1.000277, Accuracy: 74.50%\n",
      "Batch 114, Loss: 0.904037, Accuracy: 74.59%\n",
      "Batch 115, Loss: 0.946418, Accuracy: 74.66%\n",
      "Batch 116, Loss: 1.055195, Accuracy: 74.62%\n",
      "Batch 117, Loss: 1.070573, Accuracy: 74.56%\n",
      "Batch 118, Loss: 0.868423, Accuracy: 74.68%\n",
      "Batch 119, Loss: 1.032497, Accuracy: 74.66%\n",
      "Batch 120, Loss: 0.960688, Accuracy: 74.67%\n",
      "Batch 121, Loss: 1.084448, Accuracy: 74.60%\n",
      "Batch 122, Loss: 1.101667, Accuracy: 74.50%\n",
      "Batch 123, Loss: 0.983840, Accuracy: 74.50%\n",
      "Batch 124, Loss: 0.984154, Accuracy: 74.51%\n",
      "Batch 125, Loss: 0.991593, Accuracy: 74.50%\n",
      "Batch 126, Loss: 0.930744, Accuracy: 74.58%\n",
      "Batch 127, Loss: 1.002337, Accuracy: 74.58%\n",
      "Batch 128, Loss: 0.900383, Accuracy: 74.67%\n",
      "Batch 129, Loss: 0.924816, Accuracy: 74.73%\n",
      "Batch 130, Loss: 0.952983, Accuracy: 74.77%\n",
      "Batch 131, Loss: 1.019050, Accuracy: 74.75%\n",
      "Batch 132, Loss: 0.910260, Accuracy: 74.81%\n",
      "Batch 133, Loss: 0.981898, Accuracy: 74.81%\n",
      "Batch 134, Loss: 0.899879, Accuracy: 74.88%\n",
      "Batch 135, Loss: 0.935891, Accuracy: 74.92%\n",
      "Batch 136, Loss: 0.936317, Accuracy: 74.97%\n",
      "Batch 137, Loss: 0.917951, Accuracy: 75.02%\n",
      "Batch 138, Loss: 0.931585, Accuracy: 75.06%\n",
      "Batch 139, Loss: 0.980975, Accuracy: 75.06%\n",
      "Batch 140, Loss: 0.878896, Accuracy: 75.12%\n",
      "Batch 141, Loss: 0.948803, Accuracy: 75.16%\n",
      "Batch 142, Loss: 0.991206, Accuracy: 75.17%\n",
      "Batch 143, Loss: 1.013605, Accuracy: 75.13%\n",
      "Batch 144, Loss: 1.026695, Accuracy: 75.11%\n",
      "Batch 145, Loss: 1.047243, Accuracy: 75.05%\n",
      "Batch 146, Loss: 1.003341, Accuracy: 75.03%\n",
      "Batch 147, Loss: 1.020883, Accuracy: 75.00%\n",
      "Batch 148, Loss: 0.949756, Accuracy: 75.03%\n",
      "Batch 149, Loss: 0.938112, Accuracy: 75.08%\n",
      "Batch 150, Loss: 1.023792, Accuracy: 75.05%\n",
      "Batch 151, Loss: 1.042832, Accuracy: 75.00%\n",
      "Batch 152, Loss: 1.100730, Accuracy: 74.93%\n",
      "Batch 153, Loss: 1.051247, Accuracy: 74.91%\n",
      "Batch 154, Loss: 1.073350, Accuracy: 74.85%\n",
      "Batch 155, Loss: 0.969015, Accuracy: 74.85%\n",
      "Batch 156, Loss: 1.035922, Accuracy: 74.83%\n",
      "Batch 157, Loss: 1.023699, Accuracy: 74.80%\n",
      "Batch 158, Loss: 1.061229, Accuracy: 74.75%\n",
      "Batch 159, Loss: 1.040748, Accuracy: 74.74%\n",
      "Batch 160, Loss: 0.921289, Accuracy: 74.79%\n",
      "Batch 161, Loss: 0.969348, Accuracy: 74.81%\n",
      "Batch 162, Loss: 0.951847, Accuracy: 74.85%\n",
      "Batch 163, Loss: 1.001402, Accuracy: 74.85%\n",
      "Batch 164, Loss: 0.932604, Accuracy: 74.90%\n",
      "Batch 165, Loss: 0.978288, Accuracy: 74.90%\n",
      "Batch 166, Loss: 1.020546, Accuracy: 74.87%\n",
      "Batch 167, Loss: 0.998317, Accuracy: 74.87%\n",
      "Batch 168, Loss: 1.021175, Accuracy: 74.86%\n",
      "Batch 169, Loss: 1.007872, Accuracy: 74.84%\n",
      "Batch 170, Loss: 1.002441, Accuracy: 74.86%\n",
      "Batch 171, Loss: 0.972848, Accuracy: 74.88%\n",
      "Batch 172, Loss: 0.971922, Accuracy: 74.89%\n",
      "Batch 173, Loss: 1.060745, Accuracy: 74.86%\n",
      "Batch 174, Loss: 1.025234, Accuracy: 74.84%\n",
      "Batch 175, Loss: 1.058406, Accuracy: 74.79%\n",
      "Batch 176, Loss: 1.065917, Accuracy: 74.75%\n",
      "Batch 177, Loss: 1.030031, Accuracy: 74.73%\n",
      "Batch 178, Loss: 0.944412, Accuracy: 74.75%\n",
      "Batch 179, Loss: 1.024408, Accuracy: 74.73%\n",
      "Batch 180, Loss: 1.044424, Accuracy: 74.71%\n",
      "Batch 181, Loss: 0.996902, Accuracy: 74.72%\n",
      "Batch 182, Loss: 0.955366, Accuracy: 74.75%\n",
      "Batch 183, Loss: 1.042755, Accuracy: 74.71%\n",
      "Batch 184, Loss: 1.017325, Accuracy: 74.69%\n",
      "Batch 185, Loss: 0.989707, Accuracy: 74.70%\n",
      "Batch 186, Loss: 0.976419, Accuracy: 74.71%\n",
      "Batch 187, Loss: 0.978508, Accuracy: 74.72%\n",
      "Batch 188, Loss: 1.014321, Accuracy: 74.70%\n",
      "Batch 189, Loss: 0.980243, Accuracy: 74.71%\n",
      "Batch 190, Loss: 1.023948, Accuracy: 74.69%\n",
      "Batch 191, Loss: 0.998465, Accuracy: 74.69%\n",
      "Batch 192, Loss: 1.024796, Accuracy: 74.68%\n",
      "Batch 193, Loss: 0.995189, Accuracy: 74.69%\n",
      "Batch 194, Loss: 0.909599, Accuracy: 74.74%\n",
      "Batch 195, Loss: 0.961808, Accuracy: 74.76%\n",
      "Batch 196, Loss: 0.993395, Accuracy: 74.76%\n",
      "Batch 197, Loss: 0.965228, Accuracy: 74.77%\n",
      "Batch 198, Loss: 1.048001, Accuracy: 74.75%\n",
      "Batch 199, Loss: 1.083447, Accuracy: 74.69%\n",
      "Batch 200, Loss: 0.990521, Accuracy: 74.70%\n",
      "Batch 201, Loss: 1.014826, Accuracy: 74.69%\n",
      "Batch 202, Loss: 0.925979, Accuracy: 74.73%\n",
      "Batch 203, Loss: 1.019058, Accuracy: 74.72%\n",
      "Batch 204, Loss: 1.013685, Accuracy: 74.71%\n",
      "Batch 205, Loss: 0.964521, Accuracy: 74.71%\n",
      "Batch 206, Loss: 0.996716, Accuracy: 74.70%\n",
      "Batch 207, Loss: 1.050897, Accuracy: 74.68%\n",
      "Batch 208, Loss: 1.054049, Accuracy: 74.65%\n",
      "Batch 209, Loss: 1.024046, Accuracy: 74.63%\n",
      "Batch 210, Loss: 0.992411, Accuracy: 74.63%\n",
      "Batch 211, Loss: 0.990198, Accuracy: 74.64%\n",
      "Batch 212, Loss: 0.960314, Accuracy: 74.65%\n",
      "Batch 213, Loss: 1.040866, Accuracy: 74.61%\n",
      "Training - Epoch 25, Loss: 0.997192, Accuracy: 74.61%\n",
      "Validation Batch 1, Loss: 0.996383, Accuracy: 75.00%\n",
      "Validation Batch 2, Loss: 1.085608, Accuracy: 67.19%\n",
      "Validation Batch 3, Loss: 1.102968, Accuracy: 65.62%\n",
      "Validation Batch 4, Loss: 1.009043, Accuracy: 67.58%\n",
      "Validation Batch 5, Loss: 1.024472, Accuracy: 68.75%\n",
      "Validation Batch 6, Loss: 0.963857, Accuracy: 70.57%\n",
      "Validation Batch 7, Loss: 1.040816, Accuracy: 70.76%\n",
      "Validation Batch 8, Loss: 1.059530, Accuracy: 70.31%\n",
      "Validation Batch 9, Loss: 1.072647, Accuracy: 70.14%\n",
      "Validation Batch 10, Loss: 1.013722, Accuracy: 70.62%\n",
      "Validation Batch 11, Loss: 1.010619, Accuracy: 70.88%\n",
      "Validation Batch 12, Loss: 0.988559, Accuracy: 71.48%\n",
      "Validation Batch 13, Loss: 1.086065, Accuracy: 71.03%\n",
      "Validation Batch 14, Loss: 1.047195, Accuracy: 70.76%\n",
      "Validation Batch 15, Loss: 1.024032, Accuracy: 70.83%\n",
      "Validation Batch 16, Loss: 1.015888, Accuracy: 71.09%\n",
      "Validation Batch 17, Loss: 1.109270, Accuracy: 70.68%\n",
      "Validation Batch 18, Loss: 1.011039, Accuracy: 70.92%\n",
      "Validation Batch 19, Loss: 1.058371, Accuracy: 70.81%\n",
      "Validation Batch 20, Loss: 1.056514, Accuracy: 70.78%\n",
      "Validation Batch 21, Loss: 1.026580, Accuracy: 70.83%\n",
      "Validation Batch 22, Loss: 1.099459, Accuracy: 70.31%\n",
      "Validation Batch 23, Loss: 1.135193, Accuracy: 69.84%\n",
      "Validation Batch 24, Loss: 1.041675, Accuracy: 69.92%\n",
      "Validation Batch 25, Loss: 1.043274, Accuracy: 69.75%\n",
      "Validation Batch 26, Loss: 1.041299, Accuracy: 69.71%\n",
      "Validation Batch 27, Loss: 1.020485, Accuracy: 69.70%\n",
      "Validation - Epoch 25, Loss: 1.043873, Accuracy: 69.70%\n",
      "Patience—1\n",
      "Epoch 26\n",
      "Batch 1, Loss: 1.053012, Accuracy: 68.75%\n",
      "Batch 2, Loss: 1.033771, Accuracy: 70.31%\n",
      "Batch 3, Loss: 0.970718, Accuracy: 72.92%\n",
      "Batch 4, Loss: 0.996148, Accuracy: 73.44%\n",
      "Batch 5, Loss: 0.996373, Accuracy: 73.44%\n",
      "Batch 6, Loss: 0.948630, Accuracy: 74.22%\n",
      "Batch 7, Loss: 1.001088, Accuracy: 74.11%\n",
      "Batch 8, Loss: 1.015569, Accuracy: 73.83%\n",
      "Batch 9, Loss: 1.005169, Accuracy: 73.78%\n",
      "Batch 10, Loss: 0.924091, Accuracy: 74.69%\n",
      "Batch 11, Loss: 0.999530, Accuracy: 74.57%\n",
      "Batch 12, Loss: 1.002099, Accuracy: 74.35%\n",
      "Batch 13, Loss: 1.004505, Accuracy: 74.40%\n",
      "Batch 14, Loss: 0.899782, Accuracy: 75.11%\n",
      "Batch 15, Loss: 1.024388, Accuracy: 75.00%\n",
      "Batch 16, Loss: 1.058074, Accuracy: 74.71%\n",
      "Batch 17, Loss: 0.893880, Accuracy: 75.28%\n",
      "Batch 18, Loss: 0.994460, Accuracy: 75.26%\n",
      "Batch 19, Loss: 0.938411, Accuracy: 75.58%\n",
      "Batch 20, Loss: 1.077529, Accuracy: 75.08%\n",
      "Batch 21, Loss: 0.963431, Accuracy: 75.30%\n",
      "Batch 22, Loss: 1.022478, Accuracy: 75.14%\n",
      "Batch 23, Loss: 1.056454, Accuracy: 74.80%\n",
      "Batch 24, Loss: 0.998296, Accuracy: 74.80%\n",
      "Batch 25, Loss: 1.088097, Accuracy: 74.44%\n",
      "Batch 26, Loss: 1.020572, Accuracy: 74.28%\n",
      "Batch 27, Loss: 1.085845, Accuracy: 73.84%\n",
      "Batch 28, Loss: 1.006488, Accuracy: 73.83%\n",
      "Batch 29, Loss: 1.000363, Accuracy: 73.81%\n",
      "Batch 30, Loss: 0.936237, Accuracy: 74.11%\n",
      "Batch 31, Loss: 0.973547, Accuracy: 74.09%\n",
      "Batch 32, Loss: 1.024856, Accuracy: 73.97%\n",
      "Batch 33, Loss: 1.022539, Accuracy: 73.96%\n",
      "Batch 34, Loss: 0.974442, Accuracy: 74.08%\n",
      "Batch 35, Loss: 0.933459, Accuracy: 74.33%\n",
      "Batch 36, Loss: 0.964728, Accuracy: 74.39%\n",
      "Batch 37, Loss: 0.927629, Accuracy: 74.66%\n",
      "Batch 38, Loss: 1.017512, Accuracy: 74.59%\n",
      "Batch 39, Loss: 0.996625, Accuracy: 74.56%\n",
      "Batch 40, Loss: 1.068609, Accuracy: 74.38%\n",
      "Batch 41, Loss: 1.064425, Accuracy: 74.28%\n",
      "Batch 42, Loss: 1.000497, Accuracy: 74.26%\n",
      "Batch 43, Loss: 1.014643, Accuracy: 74.24%\n",
      "Batch 44, Loss: 1.001278, Accuracy: 74.15%\n",
      "Batch 45, Loss: 0.968330, Accuracy: 74.24%\n",
      "Batch 46, Loss: 1.070408, Accuracy: 74.05%\n",
      "Batch 47, Loss: 0.984999, Accuracy: 74.07%\n",
      "Batch 48, Loss: 1.044254, Accuracy: 73.99%\n",
      "Batch 49, Loss: 1.055492, Accuracy: 73.85%\n",
      "Batch 50, Loss: 1.064657, Accuracy: 73.75%\n",
      "Batch 51, Loss: 1.180082, Accuracy: 73.44%\n",
      "Batch 52, Loss: 0.966524, Accuracy: 73.59%\n",
      "Batch 53, Loss: 0.938815, Accuracy: 73.73%\n",
      "Batch 54, Loss: 1.002902, Accuracy: 73.76%\n",
      "Batch 55, Loss: 0.987910, Accuracy: 73.75%\n",
      "Batch 56, Loss: 0.924336, Accuracy: 73.88%\n",
      "Batch 57, Loss: 1.060386, Accuracy: 73.82%\n",
      "Batch 58, Loss: 1.013406, Accuracy: 73.79%\n",
      "Batch 59, Loss: 0.967395, Accuracy: 73.81%\n",
      "Batch 60, Loss: 0.975283, Accuracy: 73.83%\n",
      "Batch 61, Loss: 0.935827, Accuracy: 73.92%\n",
      "Batch 62, Loss: 0.969033, Accuracy: 73.97%\n",
      "Batch 63, Loss: 0.980248, Accuracy: 74.03%\n",
      "Batch 64, Loss: 1.037657, Accuracy: 73.97%\n",
      "Batch 65, Loss: 1.047524, Accuracy: 73.89%\n",
      "Batch 66, Loss: 0.886021, Accuracy: 74.05%\n",
      "Batch 67, Loss: 0.955761, Accuracy: 74.14%\n",
      "Batch 68, Loss: 0.914234, Accuracy: 74.31%\n",
      "Batch 69, Loss: 0.925697, Accuracy: 74.41%\n",
      "Batch 70, Loss: 0.962760, Accuracy: 74.46%\n",
      "Batch 71, Loss: 0.958830, Accuracy: 74.54%\n",
      "Batch 72, Loss: 0.974320, Accuracy: 74.52%\n",
      "Batch 73, Loss: 0.989801, Accuracy: 74.55%\n",
      "Batch 74, Loss: 1.025988, Accuracy: 74.56%\n",
      "Batch 75, Loss: 0.938287, Accuracy: 74.67%\n",
      "Batch 76, Loss: 0.983414, Accuracy: 74.71%\n",
      "Batch 77, Loss: 0.896540, Accuracy: 74.86%\n",
      "Batch 78, Loss: 0.981288, Accuracy: 74.88%\n",
      "Batch 79, Loss: 0.970994, Accuracy: 74.92%\n",
      "Batch 80, Loss: 0.957183, Accuracy: 74.96%\n",
      "Batch 81, Loss: 0.938794, Accuracy: 75.02%\n",
      "Batch 82, Loss: 0.987451, Accuracy: 75.04%\n",
      "Batch 83, Loss: 1.045695, Accuracy: 74.96%\n",
      "Batch 84, Loss: 1.011534, Accuracy: 74.94%\n",
      "Batch 85, Loss: 0.886008, Accuracy: 75.07%\n",
      "Batch 86, Loss: 1.014195, Accuracy: 75.02%\n",
      "Batch 87, Loss: 0.993768, Accuracy: 75.04%\n",
      "Batch 88, Loss: 0.996624, Accuracy: 75.02%\n",
      "Batch 89, Loss: 1.049140, Accuracy: 74.96%\n",
      "Batch 90, Loss: 1.002422, Accuracy: 74.97%\n",
      "Batch 91, Loss: 1.044925, Accuracy: 74.91%\n",
      "Batch 92, Loss: 0.899112, Accuracy: 75.03%\n",
      "Batch 93, Loss: 1.004293, Accuracy: 75.02%\n",
      "Batch 94, Loss: 0.989497, Accuracy: 75.00%\n",
      "Batch 95, Loss: 1.017477, Accuracy: 74.97%\n",
      "Batch 96, Loss: 0.958117, Accuracy: 75.03%\n",
      "Batch 97, Loss: 1.027505, Accuracy: 75.00%\n",
      "Batch 98, Loss: 1.000730, Accuracy: 74.97%\n",
      "Batch 99, Loss: 1.014423, Accuracy: 74.95%\n",
      "Batch 100, Loss: 1.011958, Accuracy: 74.95%\n",
      "Batch 101, Loss: 1.052637, Accuracy: 74.89%\n",
      "Batch 102, Loss: 0.965193, Accuracy: 74.91%\n",
      "Batch 103, Loss: 1.020494, Accuracy: 74.86%\n",
      "Batch 104, Loss: 1.011560, Accuracy: 74.83%\n",
      "Batch 105, Loss: 1.061468, Accuracy: 74.78%\n",
      "Batch 106, Loss: 0.980916, Accuracy: 74.79%\n",
      "Batch 107, Loss: 0.919826, Accuracy: 74.87%\n",
      "Batch 108, Loss: 1.043341, Accuracy: 74.83%\n",
      "Batch 109, Loss: 1.033750, Accuracy: 74.77%\n",
      "Batch 110, Loss: 0.946953, Accuracy: 74.82%\n",
      "Batch 111, Loss: 0.970297, Accuracy: 74.82%\n",
      "Batch 112, Loss: 0.902635, Accuracy: 74.92%\n",
      "Batch 113, Loss: 0.862395, Accuracy: 75.04%\n",
      "Batch 114, Loss: 0.962080, Accuracy: 75.07%\n",
      "Batch 115, Loss: 0.995533, Accuracy: 75.08%\n",
      "Batch 116, Loss: 0.964197, Accuracy: 75.12%\n",
      "Batch 117, Loss: 0.901000, Accuracy: 75.20%\n",
      "Batch 118, Loss: 1.039600, Accuracy: 75.16%\n",
      "Batch 119, Loss: 1.068764, Accuracy: 75.08%\n",
      "Batch 120, Loss: 0.972009, Accuracy: 75.10%\n",
      "Batch 121, Loss: 0.943472, Accuracy: 75.14%\n",
      "Batch 122, Loss: 0.994780, Accuracy: 75.12%\n",
      "Batch 123, Loss: 1.050798, Accuracy: 75.06%\n",
      "Batch 124, Loss: 0.943601, Accuracy: 75.11%\n",
      "Batch 125, Loss: 1.034968, Accuracy: 75.06%\n",
      "Batch 126, Loss: 1.027386, Accuracy: 75.02%\n",
      "Batch 127, Loss: 1.022498, Accuracy: 75.00%\n",
      "Batch 128, Loss: 1.037280, Accuracy: 74.96%\n",
      "Batch 129, Loss: 0.957618, Accuracy: 75.00%\n",
      "Batch 130, Loss: 0.984284, Accuracy: 75.01%\n",
      "Batch 131, Loss: 0.944426, Accuracy: 75.02%\n",
      "Batch 132, Loss: 1.065096, Accuracy: 74.98%\n",
      "Batch 133, Loss: 0.960502, Accuracy: 75.01%\n",
      "Batch 134, Loss: 0.986158, Accuracy: 75.01%\n",
      "Batch 135, Loss: 0.967865, Accuracy: 75.03%\n",
      "Batch 136, Loss: 0.999812, Accuracy: 75.03%\n",
      "Batch 137, Loss: 1.093898, Accuracy: 74.98%\n",
      "Batch 138, Loss: 0.980530, Accuracy: 75.00%\n",
      "Batch 139, Loss: 1.041769, Accuracy: 74.96%\n",
      "Batch 140, Loss: 0.938074, Accuracy: 75.00%\n",
      "Batch 141, Loss: 0.970499, Accuracy: 75.02%\n",
      "Batch 142, Loss: 0.999855, Accuracy: 75.04%\n",
      "Batch 143, Loss: 0.986932, Accuracy: 75.03%\n",
      "Batch 144, Loss: 1.056973, Accuracy: 74.98%\n",
      "Batch 145, Loss: 1.014227, Accuracy: 74.95%\n",
      "Batch 146, Loss: 0.966236, Accuracy: 74.97%\n",
      "Batch 147, Loss: 0.961387, Accuracy: 75.00%\n",
      "Batch 148, Loss: 0.964577, Accuracy: 75.01%\n",
      "Batch 149, Loss: 0.982342, Accuracy: 75.02%\n",
      "Batch 150, Loss: 1.031799, Accuracy: 75.00%\n",
      "Batch 151, Loss: 1.037108, Accuracy: 74.98%\n",
      "Batch 152, Loss: 0.957895, Accuracy: 75.02%\n",
      "Batch 153, Loss: 1.000758, Accuracy: 75.02%\n",
      "Batch 154, Loss: 0.938164, Accuracy: 75.05%\n",
      "Batch 155, Loss: 1.104394, Accuracy: 74.97%\n",
      "Batch 156, Loss: 0.964335, Accuracy: 75.00%\n",
      "Batch 157, Loss: 1.037368, Accuracy: 74.97%\n",
      "Batch 158, Loss: 0.954879, Accuracy: 75.00%\n",
      "Batch 159, Loss: 1.016841, Accuracy: 74.96%\n",
      "Batch 160, Loss: 0.954614, Accuracy: 74.97%\n",
      "Batch 161, Loss: 1.028759, Accuracy: 74.94%\n",
      "Batch 162, Loss: 0.943112, Accuracy: 74.97%\n",
      "Batch 163, Loss: 1.048045, Accuracy: 74.93%\n",
      "Batch 164, Loss: 0.930328, Accuracy: 74.97%\n",
      "Batch 165, Loss: 1.022395, Accuracy: 74.96%\n",
      "Batch 166, Loss: 0.922614, Accuracy: 75.02%\n",
      "Batch 167, Loss: 0.989460, Accuracy: 75.02%\n",
      "Batch 168, Loss: 1.107231, Accuracy: 74.94%\n",
      "Batch 169, Loss: 1.056153, Accuracy: 74.91%\n",
      "Batch 170, Loss: 0.923319, Accuracy: 74.97%\n",
      "Batch 171, Loss: 1.039537, Accuracy: 74.95%\n",
      "Batch 172, Loss: 1.019573, Accuracy: 74.93%\n",
      "Batch 173, Loss: 1.036764, Accuracy: 74.90%\n",
      "Batch 174, Loss: 1.053153, Accuracy: 74.87%\n",
      "Batch 175, Loss: 1.026282, Accuracy: 74.84%\n",
      "Batch 176, Loss: 0.925353, Accuracy: 74.88%\n",
      "Batch 177, Loss: 1.008906, Accuracy: 74.88%\n",
      "Batch 178, Loss: 1.026597, Accuracy: 74.85%\n",
      "Batch 179, Loss: 1.035212, Accuracy: 74.83%\n",
      "Batch 180, Loss: 1.077110, Accuracy: 74.79%\n",
      "Batch 181, Loss: 0.828306, Accuracy: 74.90%\n",
      "Batch 182, Loss: 1.113025, Accuracy: 74.84%\n",
      "Batch 183, Loss: 0.939167, Accuracy: 74.87%\n",
      "Batch 184, Loss: 0.991605, Accuracy: 74.87%\n",
      "Batch 185, Loss: 0.988283, Accuracy: 74.88%\n",
      "Batch 186, Loss: 0.996223, Accuracy: 74.87%\n",
      "Batch 187, Loss: 1.112373, Accuracy: 74.82%\n",
      "Batch 188, Loss: 0.950366, Accuracy: 74.84%\n",
      "Batch 189, Loss: 1.004071, Accuracy: 74.83%\n",
      "Batch 190, Loss: 0.992710, Accuracy: 74.83%\n",
      "Batch 191, Loss: 0.994770, Accuracy: 74.83%\n",
      "Batch 192, Loss: 1.011353, Accuracy: 74.83%\n",
      "Batch 193, Loss: 0.940136, Accuracy: 74.85%\n",
      "Batch 194, Loss: 0.943413, Accuracy: 74.88%\n",
      "Batch 195, Loss: 0.990854, Accuracy: 74.90%\n",
      "Batch 196, Loss: 1.031928, Accuracy: 74.88%\n",
      "Batch 197, Loss: 1.009148, Accuracy: 74.87%\n",
      "Batch 198, Loss: 0.973159, Accuracy: 74.87%\n",
      "Batch 199, Loss: 0.963289, Accuracy: 74.89%\n",
      "Batch 200, Loss: 0.962646, Accuracy: 74.91%\n",
      "Batch 201, Loss: 0.981002, Accuracy: 74.91%\n",
      "Batch 202, Loss: 1.012309, Accuracy: 74.91%\n",
      "Batch 203, Loss: 1.031182, Accuracy: 74.91%\n",
      "Batch 204, Loss: 0.927930, Accuracy: 74.93%\n",
      "Batch 205, Loss: 1.034165, Accuracy: 74.91%\n",
      "Batch 206, Loss: 0.894029, Accuracy: 74.96%\n",
      "Batch 207, Loss: 0.963856, Accuracy: 74.98%\n",
      "Batch 208, Loss: 1.089090, Accuracy: 74.93%\n",
      "Batch 209, Loss: 1.054363, Accuracy: 74.90%\n",
      "Batch 210, Loss: 1.131870, Accuracy: 74.84%\n",
      "Batch 211, Loss: 0.969951, Accuracy: 74.84%\n",
      "Batch 212, Loss: 0.965100, Accuracy: 74.86%\n",
      "Batch 213, Loss: 1.041298, Accuracy: 74.84%\n",
      "Training - Epoch 26, Loss: 0.995277, Accuracy: 74.84%\n",
      "Validation Batch 1, Loss: 0.998485, Accuracy: 78.12%\n",
      "Validation Batch 2, Loss: 1.094835, Accuracy: 67.97%\n",
      "Validation Batch 3, Loss: 1.108185, Accuracy: 66.15%\n",
      "Validation Batch 4, Loss: 1.016256, Accuracy: 67.58%\n",
      "Validation Batch 5, Loss: 1.042562, Accuracy: 67.81%\n",
      "Validation Batch 6, Loss: 0.982234, Accuracy: 69.27%\n",
      "Validation Batch 7, Loss: 1.047956, Accuracy: 69.64%\n",
      "Validation Batch 8, Loss: 1.069637, Accuracy: 69.34%\n",
      "Validation Batch 9, Loss: 1.079749, Accuracy: 68.92%\n",
      "Validation Batch 10, Loss: 1.027898, Accuracy: 69.22%\n",
      "Validation Batch 11, Loss: 1.013549, Accuracy: 69.46%\n",
      "Validation Batch 12, Loss: 0.996208, Accuracy: 69.79%\n",
      "Validation Batch 13, Loss: 1.094176, Accuracy: 69.47%\n",
      "Validation Batch 14, Loss: 1.049884, Accuracy: 69.42%\n",
      "Validation Batch 15, Loss: 1.026392, Accuracy: 69.48%\n",
      "Validation Batch 16, Loss: 1.028128, Accuracy: 69.73%\n",
      "Validation Batch 17, Loss: 1.121628, Accuracy: 69.21%\n",
      "Validation Batch 18, Loss: 1.010757, Accuracy: 69.62%\n",
      "Validation Batch 19, Loss: 1.068656, Accuracy: 69.41%\n",
      "Validation Batch 20, Loss: 1.065994, Accuracy: 69.22%\n",
      "Validation Batch 21, Loss: 1.042316, Accuracy: 69.20%\n",
      "Validation Batch 22, Loss: 1.110679, Accuracy: 68.82%\n",
      "Validation Batch 23, Loss: 1.146227, Accuracy: 68.34%\n",
      "Validation Batch 24, Loss: 1.046662, Accuracy: 68.49%\n",
      "Validation Batch 25, Loss: 1.061010, Accuracy: 68.38%\n",
      "Validation Batch 26, Loss: 1.045611, Accuracy: 68.33%\n",
      "Validation Batch 27, Loss: 1.028760, Accuracy: 68.35%\n",
      "Validation - Epoch 26, Loss: 1.052757, Accuracy: 68.35%\n",
      "Patience—2\n",
      "Epoch 27\n",
      "Batch 1, Loss: 1.014504, Accuracy: 73.44%\n",
      "Batch 2, Loss: 1.025506, Accuracy: 72.66%\n",
      "Batch 3, Loss: 1.019414, Accuracy: 72.92%\n",
      "Batch 4, Loss: 1.017632, Accuracy: 72.27%\n",
      "Batch 5, Loss: 1.066157, Accuracy: 71.56%\n",
      "Batch 6, Loss: 0.889401, Accuracy: 73.96%\n",
      "Batch 7, Loss: 0.946085, Accuracy: 74.55%\n",
      "Batch 8, Loss: 1.026006, Accuracy: 74.02%\n",
      "Batch 9, Loss: 0.893163, Accuracy: 75.52%\n",
      "Batch 10, Loss: 1.026133, Accuracy: 75.00%\n",
      "Batch 11, Loss: 0.946929, Accuracy: 75.28%\n",
      "Batch 12, Loss: 0.929465, Accuracy: 75.91%\n",
      "Batch 13, Loss: 0.996674, Accuracy: 75.72%\n",
      "Batch 14, Loss: 0.930743, Accuracy: 76.12%\n",
      "Batch 15, Loss: 0.987361, Accuracy: 76.25%\n",
      "Batch 16, Loss: 0.998622, Accuracy: 76.07%\n",
      "Batch 17, Loss: 1.016985, Accuracy: 75.92%\n",
      "Batch 18, Loss: 0.984165, Accuracy: 76.04%\n",
      "Batch 19, Loss: 0.957663, Accuracy: 76.07%\n",
      "Batch 20, Loss: 1.047805, Accuracy: 75.62%\n",
      "Batch 21, Loss: 0.917322, Accuracy: 76.12%\n",
      "Batch 22, Loss: 1.042814, Accuracy: 75.71%\n",
      "Batch 23, Loss: 1.012761, Accuracy: 75.54%\n",
      "Batch 24, Loss: 1.029870, Accuracy: 75.33%\n",
      "Batch 25, Loss: 1.050866, Accuracy: 75.06%\n",
      "Batch 26, Loss: 0.974776, Accuracy: 75.12%\n",
      "Batch 27, Loss: 1.057878, Accuracy: 74.94%\n",
      "Batch 28, Loss: 0.991548, Accuracy: 74.89%\n",
      "Batch 29, Loss: 1.049774, Accuracy: 74.68%\n",
      "Batch 30, Loss: 0.978791, Accuracy: 74.74%\n",
      "Batch 31, Loss: 0.924319, Accuracy: 75.00%\n",
      "Batch 32, Loss: 0.984861, Accuracy: 75.00%\n",
      "Batch 33, Loss: 0.950974, Accuracy: 75.19%\n",
      "Batch 34, Loss: 0.993625, Accuracy: 75.18%\n",
      "Batch 35, Loss: 0.936093, Accuracy: 75.36%\n",
      "Batch 36, Loss: 0.970823, Accuracy: 75.43%\n",
      "Batch 37, Loss: 1.000226, Accuracy: 75.38%\n",
      "Batch 38, Loss: 0.992621, Accuracy: 75.37%\n",
      "Batch 39, Loss: 0.967066, Accuracy: 75.48%\n",
      "Batch 40, Loss: 1.022847, Accuracy: 75.43%\n",
      "Batch 41, Loss: 0.999528, Accuracy: 75.42%\n",
      "Batch 42, Loss: 1.092918, Accuracy: 75.07%\n",
      "Batch 43, Loss: 0.909024, Accuracy: 75.25%\n",
      "Batch 44, Loss: 1.011219, Accuracy: 75.21%\n",
      "Batch 45, Loss: 1.000448, Accuracy: 75.21%\n",
      "Batch 46, Loss: 1.065771, Accuracy: 75.03%\n",
      "Batch 47, Loss: 1.053820, Accuracy: 74.80%\n",
      "Batch 48, Loss: 1.010084, Accuracy: 74.71%\n",
      "Batch 49, Loss: 1.052504, Accuracy: 74.62%\n",
      "Batch 50, Loss: 1.009452, Accuracy: 74.53%\n",
      "Batch 51, Loss: 0.947530, Accuracy: 74.66%\n",
      "Batch 52, Loss: 0.995921, Accuracy: 74.70%\n",
      "Batch 53, Loss: 0.972955, Accuracy: 74.79%\n",
      "Batch 54, Loss: 1.091925, Accuracy: 74.59%\n",
      "Batch 55, Loss: 0.908741, Accuracy: 74.74%\n",
      "Batch 56, Loss: 0.917020, Accuracy: 74.92%\n",
      "Batch 57, Loss: 0.975913, Accuracy: 74.95%\n",
      "Batch 58, Loss: 0.950872, Accuracy: 75.05%\n",
      "Batch 59, Loss: 0.905842, Accuracy: 75.19%\n",
      "Batch 60, Loss: 1.016194, Accuracy: 75.16%\n",
      "Batch 61, Loss: 1.116065, Accuracy: 74.95%\n",
      "Batch 62, Loss: 0.856542, Accuracy: 75.18%\n",
      "Batch 63, Loss: 0.981994, Accuracy: 75.22%\n",
      "Batch 64, Loss: 0.901751, Accuracy: 75.37%\n",
      "Batch 65, Loss: 0.909592, Accuracy: 75.50%\n",
      "Batch 66, Loss: 1.000801, Accuracy: 75.47%\n",
      "Batch 67, Loss: 0.989420, Accuracy: 75.44%\n",
      "Batch 68, Loss: 0.973155, Accuracy: 75.41%\n",
      "Batch 69, Loss: 1.022453, Accuracy: 75.34%\n",
      "Batch 70, Loss: 0.980246, Accuracy: 75.38%\n",
      "Batch 71, Loss: 0.974883, Accuracy: 75.42%\n",
      "Batch 72, Loss: 0.996330, Accuracy: 75.39%\n",
      "Batch 73, Loss: 1.014727, Accuracy: 75.34%\n",
      "Batch 74, Loss: 1.050566, Accuracy: 75.27%\n",
      "Batch 75, Loss: 1.023697, Accuracy: 75.23%\n",
      "Batch 76, Loss: 0.987648, Accuracy: 75.25%\n",
      "Batch 77, Loss: 1.006497, Accuracy: 75.22%\n",
      "Batch 78, Loss: 1.067999, Accuracy: 75.12%\n",
      "Batch 79, Loss: 1.041981, Accuracy: 75.06%\n",
      "Batch 80, Loss: 0.970418, Accuracy: 75.04%\n",
      "Batch 81, Loss: 0.956107, Accuracy: 75.08%\n",
      "Batch 82, Loss: 1.011873, Accuracy: 75.04%\n",
      "Batch 83, Loss: 0.969768, Accuracy: 75.06%\n",
      "Batch 84, Loss: 0.943540, Accuracy: 75.11%\n",
      "Batch 85, Loss: 0.997502, Accuracy: 75.11%\n",
      "Batch 86, Loss: 0.934083, Accuracy: 75.20%\n",
      "Batch 87, Loss: 1.045100, Accuracy: 75.11%\n",
      "Batch 88, Loss: 0.891135, Accuracy: 75.25%\n",
      "Batch 89, Loss: 0.971101, Accuracy: 75.26%\n",
      "Batch 90, Loss: 0.999430, Accuracy: 75.24%\n",
      "Batch 91, Loss: 0.988249, Accuracy: 75.26%\n",
      "Batch 92, Loss: 0.934496, Accuracy: 75.32%\n",
      "Batch 93, Loss: 1.011806, Accuracy: 75.30%\n",
      "Batch 94, Loss: 0.904338, Accuracy: 75.40%\n",
      "Batch 95, Loss: 0.945962, Accuracy: 75.46%\n",
      "Batch 96, Loss: 1.004486, Accuracy: 75.42%\n",
      "Batch 97, Loss: 0.919860, Accuracy: 75.52%\n",
      "Batch 98, Loss: 1.007992, Accuracy: 75.49%\n",
      "Batch 99, Loss: 1.059516, Accuracy: 75.41%\n",
      "Batch 100, Loss: 0.989749, Accuracy: 75.41%\n",
      "Batch 101, Loss: 1.028272, Accuracy: 75.37%\n",
      "Batch 102, Loss: 1.008021, Accuracy: 75.35%\n",
      "Batch 103, Loss: 0.950928, Accuracy: 75.41%\n",
      "Batch 104, Loss: 0.948433, Accuracy: 75.45%\n",
      "Batch 105, Loss: 0.872327, Accuracy: 75.58%\n",
      "Batch 106, Loss: 0.997354, Accuracy: 75.55%\n",
      "Batch 107, Loss: 1.029868, Accuracy: 75.53%\n",
      "Batch 108, Loss: 1.053250, Accuracy: 75.46%\n",
      "Batch 109, Loss: 0.990634, Accuracy: 75.44%\n",
      "Batch 110, Loss: 0.951361, Accuracy: 75.48%\n",
      "Batch 111, Loss: 0.995191, Accuracy: 75.48%\n",
      "Batch 112, Loss: 0.971552, Accuracy: 75.52%\n",
      "Batch 113, Loss: 0.888585, Accuracy: 75.62%\n",
      "Batch 114, Loss: 0.968331, Accuracy: 75.62%\n",
      "Batch 115, Loss: 0.972857, Accuracy: 75.62%\n",
      "Batch 116, Loss: 1.034612, Accuracy: 75.59%\n",
      "Batch 117, Loss: 0.938336, Accuracy: 75.64%\n",
      "Batch 118, Loss: 0.978956, Accuracy: 75.65%\n",
      "Batch 119, Loss: 0.928702, Accuracy: 75.71%\n",
      "Batch 120, Loss: 1.035429, Accuracy: 75.68%\n",
      "Batch 121, Loss: 1.018886, Accuracy: 75.66%\n",
      "Batch 122, Loss: 1.016194, Accuracy: 75.65%\n",
      "Batch 123, Loss: 1.003936, Accuracy: 75.64%\n",
      "Batch 124, Loss: 0.999788, Accuracy: 75.64%\n",
      "Batch 125, Loss: 1.048353, Accuracy: 75.58%\n",
      "Batch 126, Loss: 1.069141, Accuracy: 75.52%\n",
      "Batch 127, Loss: 0.886652, Accuracy: 75.62%\n",
      "Batch 128, Loss: 1.078075, Accuracy: 75.52%\n",
      "Batch 129, Loss: 1.111282, Accuracy: 75.40%\n",
      "Batch 130, Loss: 0.949827, Accuracy: 75.43%\n",
      "Batch 131, Loss: 0.964657, Accuracy: 75.45%\n",
      "Batch 132, Loss: 0.987887, Accuracy: 75.45%\n",
      "Batch 133, Loss: 0.906548, Accuracy: 75.49%\n",
      "Batch 134, Loss: 1.033782, Accuracy: 75.48%\n",
      "Batch 135, Loss: 1.045601, Accuracy: 75.44%\n",
      "Batch 136, Loss: 1.033156, Accuracy: 75.41%\n",
      "Batch 137, Loss: 1.028980, Accuracy: 75.39%\n",
      "Batch 138, Loss: 0.981108, Accuracy: 75.41%\n",
      "Batch 139, Loss: 0.958608, Accuracy: 75.42%\n",
      "Batch 140, Loss: 1.034038, Accuracy: 75.37%\n",
      "Batch 141, Loss: 0.978410, Accuracy: 75.38%\n",
      "Batch 142, Loss: 0.995381, Accuracy: 75.37%\n",
      "Batch 143, Loss: 1.008613, Accuracy: 75.36%\n",
      "Batch 144, Loss: 1.042638, Accuracy: 75.30%\n",
      "Batch 145, Loss: 0.933033, Accuracy: 75.33%\n",
      "Batch 146, Loss: 1.042007, Accuracy: 75.30%\n",
      "Batch 147, Loss: 0.990833, Accuracy: 75.31%\n",
      "Batch 148, Loss: 0.902123, Accuracy: 75.39%\n",
      "Batch 149, Loss: 0.996404, Accuracy: 75.40%\n",
      "Batch 150, Loss: 0.904926, Accuracy: 75.45%\n",
      "Batch 151, Loss: 0.949250, Accuracy: 75.47%\n",
      "Batch 152, Loss: 0.980167, Accuracy: 75.46%\n",
      "Batch 153, Loss: 0.969950, Accuracy: 75.49%\n",
      "Batch 154, Loss: 1.055024, Accuracy: 75.44%\n",
      "Batch 155, Loss: 0.991990, Accuracy: 75.43%\n",
      "Batch 156, Loss: 0.955428, Accuracy: 75.48%\n",
      "Batch 157, Loss: 1.037941, Accuracy: 75.44%\n",
      "Batch 158, Loss: 0.956364, Accuracy: 75.45%\n",
      "Batch 159, Loss: 0.982869, Accuracy: 75.46%\n",
      "Batch 160, Loss: 1.052716, Accuracy: 75.43%\n",
      "Batch 161, Loss: 1.018820, Accuracy: 75.40%\n",
      "Batch 162, Loss: 0.936839, Accuracy: 75.44%\n",
      "Batch 163, Loss: 0.924871, Accuracy: 75.50%\n",
      "Batch 164, Loss: 0.962353, Accuracy: 75.50%\n",
      "Batch 165, Loss: 0.979838, Accuracy: 75.53%\n",
      "Batch 166, Loss: 0.991048, Accuracy: 75.53%\n",
      "Batch 167, Loss: 1.009473, Accuracy: 75.51%\n",
      "Batch 168, Loss: 1.001308, Accuracy: 75.51%\n",
      "Batch 169, Loss: 0.989834, Accuracy: 75.52%\n",
      "Batch 170, Loss: 0.968672, Accuracy: 75.52%\n",
      "Batch 171, Loss: 1.014566, Accuracy: 75.51%\n",
      "Batch 172, Loss: 0.978497, Accuracy: 75.52%\n",
      "Batch 173, Loss: 0.969227, Accuracy: 75.52%\n",
      "Batch 174, Loss: 0.989790, Accuracy: 75.54%\n",
      "Batch 175, Loss: 1.000850, Accuracy: 75.54%\n",
      "Batch 176, Loss: 1.003823, Accuracy: 75.53%\n",
      "Batch 177, Loss: 0.966704, Accuracy: 75.54%\n",
      "Batch 178, Loss: 0.973701, Accuracy: 75.52%\n",
      "Batch 179, Loss: 0.972649, Accuracy: 75.52%\n",
      "Batch 180, Loss: 0.933462, Accuracy: 75.56%\n",
      "Batch 181, Loss: 0.929865, Accuracy: 75.60%\n",
      "Batch 182, Loss: 0.908093, Accuracy: 75.64%\n",
      "Batch 183, Loss: 1.061150, Accuracy: 75.59%\n",
      "Batch 184, Loss: 0.982844, Accuracy: 75.59%\n",
      "Batch 185, Loss: 0.994204, Accuracy: 75.59%\n",
      "Batch 186, Loss: 1.034421, Accuracy: 75.57%\n",
      "Batch 187, Loss: 0.972703, Accuracy: 75.58%\n",
      "Batch 188, Loss: 0.974594, Accuracy: 75.59%\n",
      "Batch 189, Loss: 0.967972, Accuracy: 75.60%\n",
      "Batch 190, Loss: 1.006060, Accuracy: 75.57%\n",
      "Batch 191, Loss: 1.045483, Accuracy: 75.53%\n",
      "Batch 192, Loss: 0.888808, Accuracy: 75.59%\n",
      "Batch 193, Loss: 1.036257, Accuracy: 75.55%\n",
      "Batch 194, Loss: 1.013797, Accuracy: 75.54%\n",
      "Batch 195, Loss: 1.044788, Accuracy: 75.50%\n",
      "Batch 196, Loss: 1.047231, Accuracy: 75.47%\n",
      "Batch 197, Loss: 0.925687, Accuracy: 75.51%\n",
      "Batch 198, Loss: 1.005561, Accuracy: 75.48%\n",
      "Batch 199, Loss: 0.948695, Accuracy: 75.50%\n",
      "Batch 200, Loss: 0.900702, Accuracy: 75.56%\n",
      "Batch 201, Loss: 1.066917, Accuracy: 75.52%\n",
      "Batch 202, Loss: 1.065037, Accuracy: 75.47%\n",
      "Batch 203, Loss: 1.015111, Accuracy: 75.46%\n",
      "Batch 204, Loss: 1.094332, Accuracy: 75.41%\n",
      "Batch 205, Loss: 0.906989, Accuracy: 75.46%\n",
      "Batch 206, Loss: 0.967296, Accuracy: 75.47%\n",
      "Batch 207, Loss: 0.979570, Accuracy: 75.47%\n",
      "Batch 208, Loss: 0.961755, Accuracy: 75.48%\n",
      "Batch 209, Loss: 1.006665, Accuracy: 75.49%\n",
      "Batch 210, Loss: 0.962852, Accuracy: 75.50%\n",
      "Batch 211, Loss: 0.959717, Accuracy: 75.51%\n",
      "Batch 212, Loss: 1.035918, Accuracy: 75.50%\n",
      "Batch 213, Loss: 0.959931, Accuracy: 75.52%\n",
      "Training - Epoch 27, Loss: 0.988233, Accuracy: 75.52%\n",
      "Validation Batch 1, Loss: 0.981201, Accuracy: 78.12%\n",
      "Validation Batch 2, Loss: 1.076445, Accuracy: 70.31%\n",
      "Validation Batch 3, Loss: 1.086626, Accuracy: 69.27%\n",
      "Validation Batch 4, Loss: 0.997251, Accuracy: 70.70%\n",
      "Validation Batch 5, Loss: 1.009523, Accuracy: 70.94%\n",
      "Validation Batch 6, Loss: 0.949177, Accuracy: 72.66%\n",
      "Validation Batch 7, Loss: 1.027201, Accuracy: 72.77%\n",
      "Validation Batch 8, Loss: 1.049766, Accuracy: 72.46%\n",
      "Validation Batch 9, Loss: 1.056121, Accuracy: 72.22%\n",
      "Validation Batch 10, Loss: 1.005643, Accuracy: 72.50%\n",
      "Validation Batch 11, Loss: 0.994165, Accuracy: 72.87%\n",
      "Validation Batch 12, Loss: 0.974134, Accuracy: 73.57%\n",
      "Validation Batch 13, Loss: 1.071590, Accuracy: 72.96%\n",
      "Validation Batch 14, Loss: 1.031322, Accuracy: 72.66%\n",
      "Validation Batch 15, Loss: 1.006907, Accuracy: 72.81%\n",
      "Validation Batch 16, Loss: 1.000284, Accuracy: 73.05%\n",
      "Validation Batch 17, Loss: 1.093454, Accuracy: 72.52%\n",
      "Validation Batch 18, Loss: 0.994161, Accuracy: 72.83%\n",
      "Validation Batch 19, Loss: 1.040584, Accuracy: 72.62%\n",
      "Validation Batch 20, Loss: 1.042776, Accuracy: 72.58%\n",
      "Validation Batch 21, Loss: 1.018943, Accuracy: 72.62%\n",
      "Validation Batch 22, Loss: 1.082522, Accuracy: 72.30%\n",
      "Validation Batch 23, Loss: 1.119221, Accuracy: 71.74%\n",
      "Validation Batch 24, Loss: 1.030635, Accuracy: 71.74%\n",
      "Validation Batch 25, Loss: 1.035164, Accuracy: 71.50%\n",
      "Validation Batch 26, Loss: 1.031433, Accuracy: 71.39%\n",
      "Validation Batch 27, Loss: 0.996317, Accuracy: 71.40%\n",
      "Validation - Epoch 27, Loss: 1.029725, Accuracy: 71.40%\n",
      "Patience—0\n",
      "Epoch 28\n",
      "Batch 1, Loss: 1.063045, Accuracy: 65.62%\n",
      "Batch 2, Loss: 1.028657, Accuracy: 69.53%\n",
      "Batch 3, Loss: 1.002187, Accuracy: 70.83%\n",
      "Batch 4, Loss: 0.901051, Accuracy: 73.83%\n",
      "Batch 5, Loss: 0.928598, Accuracy: 75.31%\n",
      "Batch 6, Loss: 0.925062, Accuracy: 76.56%\n",
      "Batch 7, Loss: 0.983440, Accuracy: 76.56%\n",
      "Batch 8, Loss: 1.006305, Accuracy: 76.37%\n",
      "Batch 9, Loss: 1.036101, Accuracy: 75.87%\n",
      "Batch 10, Loss: 0.992987, Accuracy: 75.78%\n",
      "Batch 11, Loss: 1.087258, Accuracy: 75.00%\n",
      "Batch 12, Loss: 1.067988, Accuracy: 74.48%\n",
      "Batch 13, Loss: 0.974596, Accuracy: 74.76%\n",
      "Batch 14, Loss: 0.922115, Accuracy: 75.33%\n",
      "Batch 15, Loss: 0.990202, Accuracy: 75.31%\n",
      "Batch 16, Loss: 0.959455, Accuracy: 75.59%\n",
      "Batch 17, Loss: 0.973411, Accuracy: 75.64%\n",
      "Batch 18, Loss: 1.041768, Accuracy: 75.43%\n",
      "Batch 19, Loss: 0.947320, Accuracy: 75.66%\n",
      "Batch 20, Loss: 0.976429, Accuracy: 75.62%\n",
      "Batch 21, Loss: 0.988555, Accuracy: 75.67%\n",
      "Batch 22, Loss: 1.028045, Accuracy: 75.50%\n",
      "Batch 23, Loss: 0.941877, Accuracy: 75.75%\n",
      "Batch 24, Loss: 1.053740, Accuracy: 75.33%\n",
      "Batch 25, Loss: 0.917919, Accuracy: 75.62%\n",
      "Batch 26, Loss: 0.961087, Accuracy: 75.72%\n",
      "Batch 27, Loss: 0.977161, Accuracy: 75.75%\n",
      "Batch 28, Loss: 0.957101, Accuracy: 75.78%\n",
      "Batch 29, Loss: 0.949455, Accuracy: 76.02%\n",
      "Batch 30, Loss: 1.017310, Accuracy: 75.83%\n",
      "Batch 31, Loss: 1.028201, Accuracy: 75.71%\n",
      "Batch 32, Loss: 0.951186, Accuracy: 75.88%\n",
      "Batch 33, Loss: 1.010373, Accuracy: 75.80%\n",
      "Batch 34, Loss: 1.059768, Accuracy: 75.60%\n",
      "Batch 35, Loss: 1.051893, Accuracy: 75.36%\n",
      "Batch 36, Loss: 0.966300, Accuracy: 75.52%\n",
      "Batch 37, Loss: 1.043244, Accuracy: 75.38%\n",
      "Batch 38, Loss: 0.913215, Accuracy: 75.58%\n",
      "Batch 39, Loss: 0.959849, Accuracy: 75.64%\n",
      "Batch 40, Loss: 0.945946, Accuracy: 75.78%\n",
      "Batch 41, Loss: 1.067636, Accuracy: 75.57%\n",
      "Batch 42, Loss: 1.125067, Accuracy: 75.22%\n",
      "Batch 43, Loss: 1.010840, Accuracy: 75.15%\n",
      "Batch 44, Loss: 1.013850, Accuracy: 75.11%\n",
      "Batch 45, Loss: 1.012158, Accuracy: 75.00%\n",
      "Batch 46, Loss: 1.081229, Accuracy: 74.76%\n",
      "Batch 47, Loss: 0.990944, Accuracy: 74.77%\n",
      "Batch 48, Loss: 0.974870, Accuracy: 74.77%\n",
      "Batch 49, Loss: 1.026893, Accuracy: 74.74%\n",
      "Batch 50, Loss: 1.003623, Accuracy: 74.75%\n",
      "Batch 51, Loss: 1.015980, Accuracy: 74.72%\n",
      "Batch 52, Loss: 1.044843, Accuracy: 74.64%\n",
      "Batch 53, Loss: 0.965528, Accuracy: 74.73%\n",
      "Batch 54, Loss: 0.992383, Accuracy: 74.74%\n",
      "Batch 55, Loss: 1.036549, Accuracy: 74.72%\n",
      "Batch 56, Loss: 1.060246, Accuracy: 74.64%\n",
      "Batch 57, Loss: 1.037327, Accuracy: 74.56%\n",
      "Batch 58, Loss: 0.976342, Accuracy: 74.60%\n",
      "Batch 59, Loss: 0.985047, Accuracy: 74.66%\n",
      "Batch 60, Loss: 0.939458, Accuracy: 74.74%\n",
      "Batch 61, Loss: 0.989515, Accuracy: 74.74%\n",
      "Batch 62, Loss: 0.966617, Accuracy: 74.82%\n",
      "Batch 63, Loss: 0.943785, Accuracy: 74.88%\n",
      "Batch 64, Loss: 0.958282, Accuracy: 74.95%\n",
      "Batch 65, Loss: 0.973019, Accuracy: 75.02%\n",
      "Batch 66, Loss: 1.032697, Accuracy: 74.93%\n",
      "Batch 67, Loss: 1.009139, Accuracy: 74.91%\n",
      "Batch 68, Loss: 1.075389, Accuracy: 74.79%\n",
      "Batch 69, Loss: 0.932438, Accuracy: 74.89%\n",
      "Batch 70, Loss: 0.944218, Accuracy: 74.93%\n",
      "Batch 71, Loss: 1.027225, Accuracy: 74.87%\n",
      "Batch 72, Loss: 0.996648, Accuracy: 74.87%\n",
      "Batch 73, Loss: 0.989080, Accuracy: 74.85%\n",
      "Batch 74, Loss: 0.984080, Accuracy: 74.87%\n",
      "Batch 75, Loss: 0.986592, Accuracy: 74.88%\n",
      "Batch 76, Loss: 0.960488, Accuracy: 74.98%\n",
      "Batch 77, Loss: 1.042755, Accuracy: 74.90%\n",
      "Batch 78, Loss: 1.036883, Accuracy: 74.86%\n",
      "Batch 79, Loss: 1.021625, Accuracy: 74.84%\n",
      "Batch 80, Loss: 0.943596, Accuracy: 74.90%\n",
      "Batch 81, Loss: 0.981803, Accuracy: 74.92%\n",
      "Batch 82, Loss: 0.965496, Accuracy: 74.94%\n",
      "Batch 83, Loss: 0.963962, Accuracy: 74.98%\n",
      "Batch 84, Loss: 0.975220, Accuracy: 75.00%\n",
      "Batch 85, Loss: 0.986910, Accuracy: 75.02%\n",
      "Batch 86, Loss: 1.045414, Accuracy: 74.95%\n",
      "Batch 87, Loss: 0.952730, Accuracy: 75.02%\n",
      "Batch 88, Loss: 0.993545, Accuracy: 75.04%\n",
      "Batch 89, Loss: 0.940517, Accuracy: 75.11%\n",
      "Batch 90, Loss: 0.979620, Accuracy: 75.12%\n",
      "Batch 91, Loss: 1.032189, Accuracy: 75.09%\n",
      "Batch 92, Loss: 0.976237, Accuracy: 75.10%\n",
      "Batch 93, Loss: 0.991834, Accuracy: 75.10%\n",
      "Batch 94, Loss: 1.010268, Accuracy: 75.08%\n",
      "Batch 95, Loss: 0.959120, Accuracy: 75.13%\n",
      "Batch 96, Loss: 1.018448, Accuracy: 75.10%\n",
      "Batch 97, Loss: 0.911976, Accuracy: 75.18%\n",
      "Batch 98, Loss: 1.053839, Accuracy: 75.11%\n",
      "Batch 99, Loss: 0.960960, Accuracy: 75.13%\n",
      "Batch 100, Loss: 0.990520, Accuracy: 75.14%\n",
      "Batch 101, Loss: 1.041192, Accuracy: 75.09%\n",
      "Batch 102, Loss: 0.983766, Accuracy: 75.08%\n",
      "Batch 103, Loss: 1.047916, Accuracy: 75.03%\n",
      "Batch 104, Loss: 0.930972, Accuracy: 75.09%\n",
      "Batch 105, Loss: 0.976143, Accuracy: 75.09%\n",
      "Batch 106, Loss: 0.993184, Accuracy: 75.09%\n",
      "Batch 107, Loss: 1.036059, Accuracy: 75.07%\n",
      "Batch 108, Loss: 1.091953, Accuracy: 74.97%\n",
      "Batch 109, Loss: 0.991236, Accuracy: 74.99%\n",
      "Batch 110, Loss: 1.011296, Accuracy: 74.96%\n",
      "Batch 111, Loss: 1.009128, Accuracy: 74.96%\n",
      "Batch 112, Loss: 0.974883, Accuracy: 74.96%\n",
      "Batch 113, Loss: 0.942672, Accuracy: 75.01%\n",
      "Batch 114, Loss: 1.014903, Accuracy: 75.01%\n",
      "Batch 115, Loss: 1.033886, Accuracy: 74.97%\n",
      "Batch 116, Loss: 0.980294, Accuracy: 74.99%\n",
      "Batch 117, Loss: 0.941421, Accuracy: 75.04%\n",
      "Batch 118, Loss: 0.940041, Accuracy: 75.08%\n",
      "Batch 119, Loss: 1.099158, Accuracy: 74.97%\n",
      "Batch 120, Loss: 0.997333, Accuracy: 74.96%\n",
      "Batch 121, Loss: 0.943414, Accuracy: 75.01%\n",
      "Batch 122, Loss: 0.910143, Accuracy: 75.09%\n",
      "Batch 123, Loss: 0.974993, Accuracy: 75.09%\n",
      "Batch 124, Loss: 1.003642, Accuracy: 75.09%\n",
      "Batch 125, Loss: 0.956931, Accuracy: 75.11%\n",
      "Batch 126, Loss: 1.007862, Accuracy: 75.10%\n",
      "Batch 127, Loss: 1.006705, Accuracy: 75.10%\n",
      "Batch 128, Loss: 0.974398, Accuracy: 75.12%\n",
      "Batch 129, Loss: 1.020389, Accuracy: 75.11%\n",
      "Batch 130, Loss: 0.976485, Accuracy: 75.13%\n",
      "Batch 131, Loss: 0.970574, Accuracy: 75.16%\n",
      "Batch 132, Loss: 0.867587, Accuracy: 75.25%\n",
      "Batch 133, Loss: 0.896996, Accuracy: 75.33%\n",
      "Batch 134, Loss: 0.971695, Accuracy: 75.34%\n",
      "Batch 135, Loss: 0.975996, Accuracy: 75.35%\n",
      "Batch 136, Loss: 0.912876, Accuracy: 75.40%\n",
      "Batch 137, Loss: 1.022800, Accuracy: 75.38%\n",
      "Batch 138, Loss: 1.022669, Accuracy: 75.34%\n",
      "Batch 139, Loss: 1.028119, Accuracy: 75.31%\n",
      "Batch 140, Loss: 0.951093, Accuracy: 75.33%\n",
      "Batch 141, Loss: 1.011461, Accuracy: 75.31%\n",
      "Batch 142, Loss: 0.970851, Accuracy: 75.32%\n",
      "Batch 143, Loss: 1.020315, Accuracy: 75.30%\n",
      "Batch 144, Loss: 0.971751, Accuracy: 75.31%\n",
      "Batch 145, Loss: 1.021943, Accuracy: 75.27%\n",
      "Batch 146, Loss: 0.945246, Accuracy: 75.30%\n",
      "Batch 147, Loss: 1.033747, Accuracy: 75.28%\n",
      "Batch 148, Loss: 0.916888, Accuracy: 75.34%\n",
      "Batch 149, Loss: 0.902369, Accuracy: 75.38%\n",
      "Batch 150, Loss: 1.069633, Accuracy: 75.32%\n",
      "Batch 151, Loss: 0.933174, Accuracy: 75.36%\n",
      "Batch 152, Loss: 1.008279, Accuracy: 75.36%\n",
      "Batch 153, Loss: 0.972063, Accuracy: 75.39%\n",
      "Batch 154, Loss: 0.928335, Accuracy: 75.43%\n",
      "Batch 155, Loss: 1.018688, Accuracy: 75.40%\n",
      "Batch 156, Loss: 0.991715, Accuracy: 75.40%\n",
      "Batch 157, Loss: 1.030215, Accuracy: 75.38%\n",
      "Batch 158, Loss: 1.000825, Accuracy: 75.36%\n",
      "Batch 159, Loss: 1.037731, Accuracy: 75.33%\n",
      "Batch 160, Loss: 0.923666, Accuracy: 75.37%\n",
      "Batch 161, Loss: 0.935172, Accuracy: 75.41%\n",
      "Batch 162, Loss: 0.979088, Accuracy: 75.41%\n",
      "Batch 163, Loss: 0.912389, Accuracy: 75.46%\n",
      "Batch 164, Loss: 0.926762, Accuracy: 75.50%\n",
      "Batch 165, Loss: 0.948696, Accuracy: 75.52%\n",
      "Batch 166, Loss: 1.069308, Accuracy: 75.46%\n",
      "Batch 167, Loss: 1.002290, Accuracy: 75.47%\n",
      "Batch 168, Loss: 0.906681, Accuracy: 75.52%\n",
      "Batch 169, Loss: 0.983647, Accuracy: 75.54%\n",
      "Batch 170, Loss: 0.940787, Accuracy: 75.57%\n",
      "Batch 171, Loss: 0.984208, Accuracy: 75.56%\n",
      "Batch 172, Loss: 0.946052, Accuracy: 75.57%\n",
      "Batch 173, Loss: 1.021361, Accuracy: 75.56%\n",
      "Batch 174, Loss: 1.011447, Accuracy: 75.55%\n",
      "Batch 175, Loss: 0.974780, Accuracy: 75.55%\n",
      "Batch 176, Loss: 1.094377, Accuracy: 75.51%\n",
      "Batch 177, Loss: 0.982575, Accuracy: 75.50%\n",
      "Batch 178, Loss: 0.954750, Accuracy: 75.53%\n",
      "Batch 179, Loss: 0.933735, Accuracy: 75.52%\n",
      "Batch 180, Loss: 0.937835, Accuracy: 75.56%\n",
      "Batch 181, Loss: 0.984214, Accuracy: 75.54%\n",
      "Batch 182, Loss: 0.980868, Accuracy: 75.54%\n",
      "Batch 183, Loss: 0.876111, Accuracy: 75.60%\n",
      "Batch 184, Loss: 0.964955, Accuracy: 75.61%\n",
      "Batch 185, Loss: 0.950834, Accuracy: 75.62%\n",
      "Batch 186, Loss: 0.945269, Accuracy: 75.66%\n",
      "Batch 187, Loss: 1.067402, Accuracy: 75.62%\n",
      "Batch 188, Loss: 0.915227, Accuracy: 75.65%\n",
      "Batch 189, Loss: 0.942078, Accuracy: 75.68%\n",
      "Batch 190, Loss: 0.982143, Accuracy: 75.67%\n",
      "Batch 191, Loss: 0.929883, Accuracy: 75.70%\n",
      "Batch 192, Loss: 1.029548, Accuracy: 75.67%\n",
      "Batch 193, Loss: 0.994056, Accuracy: 75.66%\n",
      "Batch 194, Loss: 0.961731, Accuracy: 75.68%\n",
      "Batch 195, Loss: 1.051140, Accuracy: 75.63%\n",
      "Batch 196, Loss: 1.023272, Accuracy: 75.60%\n",
      "Batch 197, Loss: 1.040579, Accuracy: 75.56%\n",
      "Batch 198, Loss: 0.979264, Accuracy: 75.55%\n",
      "Batch 199, Loss: 0.997017, Accuracy: 75.55%\n",
      "Batch 200, Loss: 0.988341, Accuracy: 75.55%\n",
      "Batch 201, Loss: 1.015309, Accuracy: 75.52%\n",
      "Batch 202, Loss: 0.946557, Accuracy: 75.54%\n",
      "Batch 203, Loss: 1.009748, Accuracy: 75.52%\n",
      "Batch 204, Loss: 1.027921, Accuracy: 75.50%\n",
      "Batch 205, Loss: 1.008602, Accuracy: 75.48%\n",
      "Batch 206, Loss: 0.963586, Accuracy: 75.49%\n",
      "Batch 207, Loss: 0.954497, Accuracy: 75.50%\n",
      "Batch 208, Loss: 0.976877, Accuracy: 75.50%\n",
      "Batch 209, Loss: 0.996252, Accuracy: 75.51%\n",
      "Batch 210, Loss: 0.975530, Accuracy: 75.52%\n",
      "Batch 211, Loss: 1.022266, Accuracy: 75.50%\n",
      "Batch 212, Loss: 0.934130, Accuracy: 75.53%\n",
      "Batch 213, Loss: 0.987452, Accuracy: 75.53%\n",
      "Training - Epoch 28, Loss: 0.987822, Accuracy: 75.53%\n",
      "Validation Batch 1, Loss: 0.970521, Accuracy: 78.12%\n",
      "Validation Batch 2, Loss: 1.059583, Accuracy: 70.31%\n",
      "Validation Batch 3, Loss: 1.080517, Accuracy: 68.75%\n",
      "Validation Batch 4, Loss: 0.992222, Accuracy: 70.31%\n",
      "Validation Batch 5, Loss: 0.994503, Accuracy: 71.88%\n",
      "Validation Batch 6, Loss: 0.934123, Accuracy: 73.70%\n",
      "Validation Batch 7, Loss: 1.017143, Accuracy: 73.66%\n",
      "Validation Batch 8, Loss: 1.045487, Accuracy: 73.05%\n",
      "Validation Batch 9, Loss: 1.047916, Accuracy: 72.74%\n",
      "Validation Batch 10, Loss: 0.991761, Accuracy: 73.28%\n",
      "Validation Batch 11, Loss: 0.981689, Accuracy: 73.72%\n",
      "Validation Batch 12, Loss: 0.965689, Accuracy: 74.35%\n",
      "Validation Batch 13, Loss: 1.065655, Accuracy: 73.68%\n",
      "Validation Batch 14, Loss: 1.021215, Accuracy: 73.55%\n",
      "Validation Batch 15, Loss: 0.997931, Accuracy: 73.65%\n",
      "Validation Batch 16, Loss: 0.990394, Accuracy: 73.83%\n",
      "Validation Batch 17, Loss: 1.080928, Accuracy: 73.35%\n",
      "Validation Batch 18, Loss: 0.985470, Accuracy: 73.52%\n",
      "Validation Batch 19, Loss: 1.033571, Accuracy: 73.36%\n",
      "Validation Batch 20, Loss: 1.034939, Accuracy: 73.28%\n",
      "Validation Batch 21, Loss: 1.007008, Accuracy: 73.36%\n",
      "Validation Batch 22, Loss: 1.066702, Accuracy: 73.01%\n",
      "Validation Batch 23, Loss: 1.106227, Accuracy: 72.55%\n",
      "Validation Batch 24, Loss: 1.023348, Accuracy: 72.53%\n",
      "Validation Batch 25, Loss: 1.018867, Accuracy: 72.44%\n",
      "Validation Batch 26, Loss: 1.024139, Accuracy: 72.42%\n",
      "Validation Batch 27, Loss: 0.987161, Accuracy: 72.46%\n",
      "Validation - Epoch 28, Loss: 1.019434, Accuracy: 72.46%\n",
      "Patience—0\n",
      "Epoch 29\n",
      "Batch 1, Loss: 1.011396, Accuracy: 71.88%\n",
      "Batch 2, Loss: 0.925218, Accuracy: 77.34%\n",
      "Batch 3, Loss: 1.012777, Accuracy: 76.04%\n",
      "Batch 4, Loss: 1.010382, Accuracy: 75.00%\n",
      "Batch 5, Loss: 0.981961, Accuracy: 74.69%\n",
      "Batch 6, Loss: 1.020443, Accuracy: 73.96%\n",
      "Batch 7, Loss: 0.918873, Accuracy: 75.22%\n",
      "Batch 8, Loss: 0.932675, Accuracy: 75.78%\n",
      "Batch 9, Loss: 0.908348, Accuracy: 76.56%\n",
      "Batch 10, Loss: 0.937318, Accuracy: 77.03%\n",
      "Batch 11, Loss: 1.005638, Accuracy: 76.56%\n",
      "Batch 12, Loss: 0.976098, Accuracy: 76.56%\n",
      "Batch 13, Loss: 0.961424, Accuracy: 76.80%\n",
      "Batch 14, Loss: 1.000062, Accuracy: 76.79%\n",
      "Batch 15, Loss: 1.061351, Accuracy: 76.25%\n",
      "Batch 16, Loss: 1.001077, Accuracy: 76.17%\n",
      "Batch 17, Loss: 1.040593, Accuracy: 75.92%\n",
      "Batch 18, Loss: 1.017958, Accuracy: 75.69%\n",
      "Batch 19, Loss: 1.096050, Accuracy: 75.00%\n",
      "Batch 20, Loss: 1.012169, Accuracy: 74.92%\n",
      "Batch 21, Loss: 0.953202, Accuracy: 75.15%\n",
      "Batch 22, Loss: 0.926548, Accuracy: 75.43%\n",
      "Batch 23, Loss: 0.985167, Accuracy: 75.34%\n",
      "Batch 24, Loss: 0.920465, Accuracy: 75.65%\n",
      "Batch 25, Loss: 1.027289, Accuracy: 75.44%\n",
      "Batch 26, Loss: 1.027698, Accuracy: 75.30%\n",
      "Batch 27, Loss: 1.057960, Accuracy: 74.88%\n",
      "Batch 28, Loss: 1.039016, Accuracy: 74.72%\n",
      "Batch 29, Loss: 0.920822, Accuracy: 75.00%\n",
      "Batch 30, Loss: 0.930214, Accuracy: 75.21%\n",
      "Batch 31, Loss: 0.997703, Accuracy: 75.25%\n",
      "Batch 32, Loss: 1.045341, Accuracy: 75.10%\n",
      "Batch 33, Loss: 1.038490, Accuracy: 74.91%\n",
      "Batch 34, Loss: 1.000565, Accuracy: 74.91%\n",
      "Batch 35, Loss: 0.950078, Accuracy: 75.00%\n",
      "Batch 36, Loss: 1.043054, Accuracy: 74.91%\n",
      "Batch 37, Loss: 0.900059, Accuracy: 75.21%\n",
      "Batch 38, Loss: 1.025455, Accuracy: 75.16%\n",
      "Batch 39, Loss: 0.845055, Accuracy: 75.56%\n",
      "Batch 40, Loss: 0.981936, Accuracy: 75.59%\n",
      "Batch 41, Loss: 0.911222, Accuracy: 75.72%\n",
      "Batch 42, Loss: 0.907611, Accuracy: 75.93%\n",
      "Batch 43, Loss: 0.980122, Accuracy: 75.94%\n",
      "Batch 44, Loss: 0.973285, Accuracy: 75.96%\n",
      "Batch 45, Loss: 1.035405, Accuracy: 75.87%\n",
      "Batch 46, Loss: 0.952139, Accuracy: 75.99%\n",
      "Batch 47, Loss: 1.073311, Accuracy: 75.76%\n",
      "Batch 48, Loss: 0.971298, Accuracy: 75.75%\n",
      "Batch 49, Loss: 0.957687, Accuracy: 75.80%\n",
      "Batch 50, Loss: 0.937564, Accuracy: 75.91%\n",
      "Batch 51, Loss: 1.072673, Accuracy: 75.74%\n",
      "Batch 52, Loss: 0.929826, Accuracy: 75.90%\n",
      "Batch 53, Loss: 0.907583, Accuracy: 76.06%\n",
      "Batch 54, Loss: 1.057505, Accuracy: 75.93%\n",
      "Batch 55, Loss: 0.997651, Accuracy: 75.91%\n",
      "Batch 56, Loss: 0.938918, Accuracy: 75.98%\n",
      "Batch 57, Loss: 1.103018, Accuracy: 75.79%\n",
      "Batch 58, Loss: 0.947064, Accuracy: 75.92%\n",
      "Batch 59, Loss: 1.093419, Accuracy: 75.74%\n",
      "Batch 60, Loss: 0.928438, Accuracy: 75.83%\n",
      "Batch 61, Loss: 0.971395, Accuracy: 75.87%\n",
      "Batch 62, Loss: 0.906592, Accuracy: 76.01%\n",
      "Batch 63, Loss: 0.965524, Accuracy: 76.04%\n",
      "Batch 64, Loss: 1.052486, Accuracy: 75.90%\n",
      "Batch 65, Loss: 1.005958, Accuracy: 75.87%\n",
      "Batch 66, Loss: 1.006539, Accuracy: 75.85%\n",
      "Batch 67, Loss: 0.975683, Accuracy: 75.89%\n",
      "Batch 68, Loss: 0.967237, Accuracy: 75.92%\n",
      "Batch 69, Loss: 1.058578, Accuracy: 75.79%\n",
      "Batch 70, Loss: 1.029227, Accuracy: 75.71%\n",
      "Batch 71, Loss: 1.044130, Accuracy: 75.62%\n",
      "Batch 72, Loss: 0.933760, Accuracy: 75.69%\n",
      "Batch 73, Loss: 0.954717, Accuracy: 75.73%\n",
      "Batch 74, Loss: 0.950495, Accuracy: 75.78%\n",
      "Batch 75, Loss: 1.115755, Accuracy: 75.58%\n",
      "Batch 76, Loss: 1.035373, Accuracy: 75.51%\n",
      "Batch 77, Loss: 0.915345, Accuracy: 75.63%\n",
      "Batch 78, Loss: 0.919799, Accuracy: 75.74%\n",
      "Batch 79, Loss: 0.935162, Accuracy: 75.79%\n",
      "Batch 80, Loss: 0.972033, Accuracy: 75.80%\n",
      "Batch 81, Loss: 1.055020, Accuracy: 75.68%\n",
      "Batch 82, Loss: 0.925616, Accuracy: 75.76%\n",
      "Batch 83, Loss: 1.028626, Accuracy: 75.72%\n",
      "Batch 84, Loss: 0.929089, Accuracy: 75.80%\n",
      "Batch 85, Loss: 0.965612, Accuracy: 75.83%\n",
      "Batch 86, Loss: 0.925114, Accuracy: 75.91%\n",
      "Batch 87, Loss: 0.997985, Accuracy: 75.90%\n",
      "Batch 88, Loss: 0.979985, Accuracy: 75.91%\n",
      "Batch 89, Loss: 0.996041, Accuracy: 75.88%\n",
      "Batch 90, Loss: 0.935986, Accuracy: 75.95%\n",
      "Batch 91, Loss: 0.960199, Accuracy: 75.98%\n",
      "Batch 92, Loss: 0.994583, Accuracy: 75.93%\n",
      "Batch 93, Loss: 0.907159, Accuracy: 76.02%\n",
      "Batch 94, Loss: 1.053988, Accuracy: 75.93%\n",
      "Batch 95, Loss: 0.992520, Accuracy: 75.92%\n",
      "Batch 96, Loss: 1.070831, Accuracy: 75.83%\n",
      "Batch 97, Loss: 1.019942, Accuracy: 75.79%\n",
      "Batch 98, Loss: 0.971393, Accuracy: 75.80%\n",
      "Batch 99, Loss: 0.954288, Accuracy: 75.82%\n",
      "Batch 100, Loss: 1.031079, Accuracy: 75.77%\n",
      "Batch 101, Loss: 0.999056, Accuracy: 75.77%\n",
      "Batch 102, Loss: 0.959182, Accuracy: 75.80%\n",
      "Batch 103, Loss: 0.982530, Accuracy: 75.79%\n",
      "Batch 104, Loss: 1.005557, Accuracy: 75.77%\n",
      "Batch 105, Loss: 0.976570, Accuracy: 75.79%\n",
      "Batch 106, Loss: 1.035447, Accuracy: 75.72%\n",
      "Batch 107, Loss: 1.006558, Accuracy: 75.70%\n",
      "Batch 108, Loss: 0.915300, Accuracy: 75.80%\n",
      "Batch 109, Loss: 0.991732, Accuracy: 75.77%\n",
      "Batch 110, Loss: 0.925850, Accuracy: 75.84%\n",
      "Batch 111, Loss: 0.921037, Accuracy: 75.90%\n",
      "Batch 112, Loss: 0.910904, Accuracy: 75.98%\n",
      "Batch 113, Loss: 1.013561, Accuracy: 75.95%\n",
      "Batch 114, Loss: 0.970476, Accuracy: 75.97%\n",
      "Batch 115, Loss: 0.895537, Accuracy: 76.06%\n",
      "Batch 116, Loss: 0.995177, Accuracy: 76.04%\n",
      "Batch 117, Loss: 1.130475, Accuracy: 75.89%\n",
      "Batch 118, Loss: 1.087376, Accuracy: 75.81%\n",
      "Batch 119, Loss: 0.970245, Accuracy: 75.83%\n",
      "Batch 120, Loss: 0.988960, Accuracy: 75.82%\n",
      "Batch 121, Loss: 1.024102, Accuracy: 75.81%\n",
      "Batch 122, Loss: 0.971039, Accuracy: 75.82%\n",
      "Batch 123, Loss: 0.991510, Accuracy: 75.81%\n",
      "Batch 124, Loss: 0.985390, Accuracy: 75.81%\n",
      "Batch 125, Loss: 1.029936, Accuracy: 75.79%\n",
      "Batch 126, Loss: 1.066077, Accuracy: 75.71%\n",
      "Batch 127, Loss: 0.993869, Accuracy: 75.70%\n",
      "Batch 128, Loss: 0.980814, Accuracy: 75.71%\n",
      "Batch 129, Loss: 1.014373, Accuracy: 75.68%\n",
      "Batch 130, Loss: 0.985685, Accuracy: 75.67%\n",
      "Batch 131, Loss: 1.050743, Accuracy: 75.62%\n",
      "Batch 132, Loss: 0.947943, Accuracy: 75.65%\n",
      "Batch 133, Loss: 0.883952, Accuracy: 75.72%\n",
      "Batch 134, Loss: 0.918721, Accuracy: 75.78%\n",
      "Batch 135, Loss: 0.982071, Accuracy: 75.80%\n",
      "Batch 136, Loss: 0.915761, Accuracy: 75.86%\n",
      "Batch 137, Loss: 0.954016, Accuracy: 75.88%\n",
      "Batch 138, Loss: 0.985045, Accuracy: 75.88%\n",
      "Batch 139, Loss: 1.071250, Accuracy: 75.81%\n",
      "Batch 140, Loss: 0.921892, Accuracy: 75.86%\n",
      "Batch 141, Loss: 0.938452, Accuracy: 75.91%\n",
      "Batch 142, Loss: 0.990677, Accuracy: 75.92%\n",
      "Batch 143, Loss: 1.042557, Accuracy: 75.87%\n",
      "Batch 144, Loss: 1.006412, Accuracy: 75.87%\n",
      "Batch 145, Loss: 1.082606, Accuracy: 75.80%\n",
      "Batch 146, Loss: 0.987886, Accuracy: 75.80%\n",
      "Batch 147, Loss: 1.014442, Accuracy: 75.79%\n",
      "Batch 148, Loss: 0.948453, Accuracy: 75.81%\n",
      "Batch 149, Loss: 0.984923, Accuracy: 75.82%\n",
      "Batch 150, Loss: 1.070559, Accuracy: 75.75%\n",
      "Batch 151, Loss: 0.976283, Accuracy: 75.76%\n",
      "Batch 152, Loss: 0.940018, Accuracy: 75.79%\n",
      "Batch 153, Loss: 1.023021, Accuracy: 75.77%\n",
      "Batch 154, Loss: 1.010959, Accuracy: 75.75%\n",
      "Batch 155, Loss: 0.991370, Accuracy: 75.74%\n",
      "Batch 156, Loss: 0.978652, Accuracy: 75.75%\n",
      "Batch 157, Loss: 0.939737, Accuracy: 75.78%\n",
      "Batch 158, Loss: 1.025865, Accuracy: 75.75%\n",
      "Batch 159, Loss: 0.897930, Accuracy: 75.81%\n",
      "Batch 160, Loss: 0.989517, Accuracy: 75.80%\n",
      "Batch 161, Loss: 1.011562, Accuracy: 75.78%\n",
      "Batch 162, Loss: 1.034111, Accuracy: 75.73%\n",
      "Batch 163, Loss: 1.032824, Accuracy: 75.70%\n",
      "Batch 164, Loss: 0.972397, Accuracy: 75.71%\n",
      "Batch 165, Loss: 0.960952, Accuracy: 75.72%\n",
      "Batch 166, Loss: 0.994386, Accuracy: 75.72%\n",
      "Batch 167, Loss: 0.988051, Accuracy: 75.72%\n",
      "Batch 168, Loss: 0.988569, Accuracy: 75.73%\n",
      "Batch 169, Loss: 0.969453, Accuracy: 75.74%\n",
      "Batch 170, Loss: 0.953389, Accuracy: 75.76%\n",
      "Batch 171, Loss: 0.932863, Accuracy: 75.80%\n",
      "Batch 172, Loss: 1.053577, Accuracy: 75.76%\n",
      "Batch 173, Loss: 1.039345, Accuracy: 75.71%\n",
      "Batch 174, Loss: 1.025946, Accuracy: 75.69%\n",
      "Batch 175, Loss: 0.984712, Accuracy: 75.69%\n",
      "Batch 176, Loss: 1.026452, Accuracy: 75.67%\n",
      "Batch 177, Loss: 0.924712, Accuracy: 75.70%\n",
      "Batch 178, Loss: 1.009294, Accuracy: 75.68%\n",
      "Batch 179, Loss: 0.969580, Accuracy: 75.68%\n",
      "Batch 180, Loss: 0.967633, Accuracy: 75.70%\n",
      "Batch 181, Loss: 0.985605, Accuracy: 75.71%\n",
      "Batch 182, Loss: 0.965482, Accuracy: 75.74%\n",
      "Batch 183, Loss: 0.991502, Accuracy: 75.73%\n",
      "Batch 184, Loss: 1.101979, Accuracy: 75.69%\n",
      "Batch 185, Loss: 0.951846, Accuracy: 75.72%\n",
      "Batch 186, Loss: 0.990258, Accuracy: 75.71%\n",
      "Batch 187, Loss: 0.996458, Accuracy: 75.69%\n",
      "Batch 188, Loss: 0.976870, Accuracy: 75.71%\n",
      "Batch 189, Loss: 0.962723, Accuracy: 75.72%\n",
      "Batch 190, Loss: 0.911363, Accuracy: 75.76%\n",
      "Batch 191, Loss: 1.006412, Accuracy: 75.76%\n",
      "Batch 192, Loss: 1.025103, Accuracy: 75.72%\n",
      "Batch 193, Loss: 0.953525, Accuracy: 75.73%\n",
      "Batch 194, Loss: 1.038154, Accuracy: 75.69%\n",
      "Batch 195, Loss: 0.971799, Accuracy: 75.70%\n",
      "Batch 196, Loss: 1.046215, Accuracy: 75.67%\n",
      "Batch 197, Loss: 0.997899, Accuracy: 75.67%\n",
      "Batch 198, Loss: 0.990473, Accuracy: 75.65%\n",
      "Batch 199, Loss: 0.886984, Accuracy: 75.70%\n",
      "Batch 200, Loss: 1.043132, Accuracy: 75.67%\n",
      "Batch 201, Loss: 0.970844, Accuracy: 75.67%\n",
      "Batch 202, Loss: 0.964822, Accuracy: 75.67%\n",
      "Batch 203, Loss: 0.962294, Accuracy: 75.67%\n",
      "Batch 204, Loss: 1.015900, Accuracy: 75.65%\n",
      "Batch 205, Loss: 1.008461, Accuracy: 75.62%\n",
      "Batch 206, Loss: 1.007110, Accuracy: 75.61%\n",
      "Batch 207, Loss: 1.010496, Accuracy: 75.60%\n",
      "Batch 208, Loss: 1.045441, Accuracy: 75.57%\n",
      "Batch 209, Loss: 0.905428, Accuracy: 75.61%\n",
      "Batch 210, Loss: 1.064074, Accuracy: 75.59%\n",
      "Batch 211, Loss: 0.956213, Accuracy: 75.59%\n",
      "Batch 212, Loss: 1.033385, Accuracy: 75.57%\n",
      "Batch 213, Loss: 0.938768, Accuracy: 75.59%\n",
      "Training - Epoch 29, Loss: 0.987077, Accuracy: 75.59%\n",
      "Validation Batch 1, Loss: 0.960840, Accuracy: 79.69%\n",
      "Validation Batch 2, Loss: 1.036621, Accuracy: 75.78%\n",
      "Validation Batch 3, Loss: 1.067010, Accuracy: 72.40%\n",
      "Validation Batch 4, Loss: 0.974949, Accuracy: 73.44%\n",
      "Validation Batch 5, Loss: 0.978216, Accuracy: 74.69%\n",
      "Validation Batch 6, Loss: 0.919143, Accuracy: 76.30%\n",
      "Validation Batch 7, Loss: 1.006933, Accuracy: 75.89%\n",
      "Validation Batch 8, Loss: 1.038424, Accuracy: 75.20%\n",
      "Validation Batch 9, Loss: 1.036223, Accuracy: 74.65%\n",
      "Validation Batch 10, Loss: 0.981387, Accuracy: 75.16%\n",
      "Validation Batch 11, Loss: 0.967299, Accuracy: 75.57%\n",
      "Validation Batch 12, Loss: 0.954500, Accuracy: 76.04%\n",
      "Validation Batch 13, Loss: 1.052128, Accuracy: 75.36%\n",
      "Validation Batch 14, Loss: 1.004560, Accuracy: 75.22%\n",
      "Validation Batch 15, Loss: 0.988295, Accuracy: 75.42%\n",
      "Validation Batch 16, Loss: 0.978321, Accuracy: 75.68%\n",
      "Validation Batch 17, Loss: 1.068459, Accuracy: 75.18%\n",
      "Validation Batch 18, Loss: 0.976468, Accuracy: 75.26%\n",
      "Validation Batch 19, Loss: 1.028500, Accuracy: 75.00%\n",
      "Validation Batch 20, Loss: 1.020333, Accuracy: 74.84%\n",
      "Validation Batch 21, Loss: 0.996699, Accuracy: 74.78%\n",
      "Validation Batch 22, Loss: 1.049583, Accuracy: 74.43%\n",
      "Validation Batch 23, Loss: 1.088314, Accuracy: 74.05%\n",
      "Validation Batch 24, Loss: 1.013045, Accuracy: 74.02%\n",
      "Validation Batch 25, Loss: 0.993216, Accuracy: 74.00%\n",
      "Validation Batch 26, Loss: 1.012999, Accuracy: 74.04%\n",
      "Validation Batch 27, Loss: 0.967555, Accuracy: 74.16%\n",
      "Validation - Epoch 29, Loss: 1.005927, Accuracy: 74.16%\n",
      "Patience—0\n",
      "Epoch 30\n",
      "Batch 1, Loss: 0.956805, Accuracy: 79.69%\n",
      "Batch 2, Loss: 1.037145, Accuracy: 75.00%\n",
      "Batch 3, Loss: 1.002826, Accuracy: 73.96%\n",
      "Batch 4, Loss: 1.041855, Accuracy: 72.66%\n",
      "Batch 5, Loss: 1.029379, Accuracy: 72.19%\n",
      "Batch 6, Loss: 0.965112, Accuracy: 73.18%\n",
      "Batch 7, Loss: 0.981533, Accuracy: 73.66%\n",
      "Batch 8, Loss: 1.021005, Accuracy: 73.63%\n",
      "Batch 9, Loss: 0.952816, Accuracy: 74.31%\n",
      "Batch 10, Loss: 0.982360, Accuracy: 74.22%\n",
      "Batch 11, Loss: 0.907726, Accuracy: 75.14%\n",
      "Batch 12, Loss: 0.960774, Accuracy: 75.52%\n",
      "Batch 13, Loss: 0.994043, Accuracy: 75.36%\n",
      "Batch 14, Loss: 0.858536, Accuracy: 76.34%\n",
      "Batch 15, Loss: 0.996001, Accuracy: 76.25%\n",
      "Batch 16, Loss: 1.038109, Accuracy: 75.98%\n",
      "Batch 17, Loss: 0.960477, Accuracy: 75.92%\n",
      "Batch 18, Loss: 1.002555, Accuracy: 75.87%\n",
      "Batch 19, Loss: 0.932691, Accuracy: 76.15%\n",
      "Batch 20, Loss: 0.928409, Accuracy: 76.41%\n",
      "Batch 21, Loss: 1.011497, Accuracy: 76.19%\n",
      "Batch 22, Loss: 0.949409, Accuracy: 76.28%\n",
      "Batch 23, Loss: 1.037974, Accuracy: 76.02%\n",
      "Batch 24, Loss: 0.989247, Accuracy: 75.98%\n",
      "Batch 25, Loss: 0.997104, Accuracy: 75.81%\n",
      "Batch 26, Loss: 0.970572, Accuracy: 75.84%\n",
      "Batch 27, Loss: 0.988517, Accuracy: 75.81%\n",
      "Batch 28, Loss: 0.946030, Accuracy: 76.00%\n",
      "Batch 29, Loss: 0.996421, Accuracy: 75.86%\n",
      "Batch 30, Loss: 0.987584, Accuracy: 75.83%\n",
      "Batch 31, Loss: 0.988910, Accuracy: 75.86%\n",
      "Batch 32, Loss: 1.028304, Accuracy: 75.73%\n",
      "Batch 33, Loss: 1.004075, Accuracy: 75.71%\n",
      "Batch 34, Loss: 0.976748, Accuracy: 75.69%\n",
      "Batch 35, Loss: 0.986979, Accuracy: 75.67%\n",
      "Batch 36, Loss: 1.013436, Accuracy: 75.65%\n",
      "Batch 37, Loss: 0.985743, Accuracy: 75.63%\n",
      "Batch 38, Loss: 0.987198, Accuracy: 75.62%\n",
      "Batch 39, Loss: 1.012534, Accuracy: 75.48%\n",
      "Batch 40, Loss: 0.923798, Accuracy: 75.62%\n",
      "Batch 41, Loss: 0.927446, Accuracy: 75.76%\n",
      "Batch 42, Loss: 1.012333, Accuracy: 75.67%\n",
      "Batch 43, Loss: 0.992457, Accuracy: 75.69%\n",
      "Batch 44, Loss: 0.969065, Accuracy: 75.71%\n",
      "Batch 45, Loss: 0.955716, Accuracy: 75.76%\n",
      "Batch 46, Loss: 0.890168, Accuracy: 75.99%\n",
      "Batch 47, Loss: 1.003584, Accuracy: 76.00%\n",
      "Batch 48, Loss: 0.956799, Accuracy: 76.07%\n",
      "Batch 49, Loss: 1.024882, Accuracy: 75.99%\n",
      "Batch 50, Loss: 1.035905, Accuracy: 75.88%\n",
      "Batch 51, Loss: 0.964389, Accuracy: 75.95%\n",
      "Batch 52, Loss: 1.044976, Accuracy: 75.84%\n",
      "Batch 53, Loss: 0.980105, Accuracy: 75.83%\n",
      "Batch 54, Loss: 1.015136, Accuracy: 75.75%\n",
      "Batch 55, Loss: 0.911769, Accuracy: 75.91%\n",
      "Batch 56, Loss: 1.154193, Accuracy: 75.59%\n",
      "Batch 57, Loss: 1.063452, Accuracy: 75.47%\n",
      "Batch 58, Loss: 0.986521, Accuracy: 75.43%\n",
      "Batch 59, Loss: 0.965227, Accuracy: 75.45%\n",
      "Batch 60, Loss: 0.942675, Accuracy: 75.55%\n",
      "Batch 61, Loss: 0.983324, Accuracy: 75.54%\n",
      "Batch 62, Loss: 1.021976, Accuracy: 75.48%\n",
      "Batch 63, Loss: 1.003711, Accuracy: 75.47%\n",
      "Batch 64, Loss: 0.997519, Accuracy: 75.46%\n",
      "Batch 65, Loss: 0.940341, Accuracy: 75.53%\n",
      "Batch 66, Loss: 1.041400, Accuracy: 75.45%\n",
      "Batch 67, Loss: 0.960079, Accuracy: 75.49%\n",
      "Batch 68, Loss: 0.968892, Accuracy: 75.53%\n",
      "Batch 69, Loss: 1.082111, Accuracy: 75.41%\n",
      "Batch 70, Loss: 0.876211, Accuracy: 75.58%\n",
      "Batch 71, Loss: 0.948896, Accuracy: 75.62%\n",
      "Batch 72, Loss: 1.007001, Accuracy: 75.59%\n",
      "Batch 73, Loss: 1.033645, Accuracy: 75.54%\n",
      "Batch 74, Loss: 0.956415, Accuracy: 75.59%\n",
      "Batch 75, Loss: 0.940027, Accuracy: 75.69%\n",
      "Batch 76, Loss: 0.972328, Accuracy: 75.72%\n",
      "Batch 77, Loss: 0.923623, Accuracy: 75.79%\n",
      "Batch 78, Loss: 0.913064, Accuracy: 75.90%\n",
      "Batch 79, Loss: 0.966454, Accuracy: 75.95%\n",
      "Batch 80, Loss: 0.968432, Accuracy: 75.96%\n",
      "Batch 81, Loss: 1.020521, Accuracy: 75.93%\n",
      "Batch 82, Loss: 1.011083, Accuracy: 75.90%\n",
      "Batch 83, Loss: 1.023607, Accuracy: 75.87%\n",
      "Batch 84, Loss: 0.981231, Accuracy: 75.89%\n",
      "Batch 85, Loss: 1.077268, Accuracy: 75.75%\n",
      "Batch 86, Loss: 0.951303, Accuracy: 75.80%\n",
      "Batch 87, Loss: 1.000252, Accuracy: 75.81%\n",
      "Batch 88, Loss: 1.022508, Accuracy: 75.76%\n",
      "Batch 89, Loss: 1.029268, Accuracy: 75.72%\n",
      "Batch 90, Loss: 0.881624, Accuracy: 75.85%\n",
      "Batch 91, Loss: 0.919869, Accuracy: 75.93%\n",
      "Batch 92, Loss: 1.038808, Accuracy: 75.87%\n",
      "Batch 93, Loss: 1.088276, Accuracy: 75.74%\n",
      "Batch 94, Loss: 0.997761, Accuracy: 75.73%\n",
      "Batch 95, Loss: 0.982128, Accuracy: 75.71%\n",
      "Batch 96, Loss: 1.011944, Accuracy: 75.68%\n",
      "Batch 97, Loss: 1.042508, Accuracy: 75.63%\n",
      "Batch 98, Loss: 0.999680, Accuracy: 75.61%\n",
      "Batch 99, Loss: 0.974321, Accuracy: 75.63%\n",
      "Batch 100, Loss: 0.936261, Accuracy: 75.70%\n",
      "Batch 101, Loss: 1.016572, Accuracy: 75.68%\n",
      "Batch 102, Loss: 0.980851, Accuracy: 75.69%\n",
      "Batch 103, Loss: 1.099140, Accuracy: 75.56%\n",
      "Batch 104, Loss: 0.934880, Accuracy: 75.60%\n",
      "Batch 105, Loss: 0.914084, Accuracy: 75.67%\n",
      "Batch 106, Loss: 0.992250, Accuracy: 75.68%\n",
      "Batch 107, Loss: 0.901593, Accuracy: 75.77%\n",
      "Batch 108, Loss: 0.996519, Accuracy: 75.78%\n",
      "Batch 109, Loss: 0.991708, Accuracy: 75.79%\n",
      "Batch 110, Loss: 0.913132, Accuracy: 75.85%\n",
      "Batch 111, Loss: 0.962340, Accuracy: 75.86%\n",
      "Batch 112, Loss: 0.902259, Accuracy: 75.95%\n",
      "Batch 113, Loss: 1.096030, Accuracy: 75.83%\n",
      "Batch 114, Loss: 0.941853, Accuracy: 75.88%\n",
      "Batch 115, Loss: 1.021369, Accuracy: 75.84%\n",
      "Batch 116, Loss: 1.030517, Accuracy: 75.79%\n",
      "Batch 117, Loss: 0.968392, Accuracy: 75.80%\n",
      "Batch 118, Loss: 0.968542, Accuracy: 75.82%\n",
      "Batch 119, Loss: 0.955786, Accuracy: 75.85%\n",
      "Batch 120, Loss: 0.985590, Accuracy: 75.85%\n",
      "Batch 121, Loss: 0.961141, Accuracy: 75.88%\n",
      "Batch 122, Loss: 1.015620, Accuracy: 75.83%\n",
      "Batch 123, Loss: 0.934837, Accuracy: 75.89%\n",
      "Batch 124, Loss: 0.910046, Accuracy: 75.96%\n",
      "Batch 125, Loss: 1.036661, Accuracy: 75.92%\n",
      "Batch 126, Loss: 0.975084, Accuracy: 75.92%\n",
      "Batch 127, Loss: 0.933687, Accuracy: 75.96%\n",
      "Batch 128, Loss: 0.996806, Accuracy: 75.94%\n",
      "Batch 129, Loss: 0.935257, Accuracy: 75.98%\n",
      "Batch 130, Loss: 0.938895, Accuracy: 76.00%\n",
      "Batch 131, Loss: 0.986970, Accuracy: 75.98%\n",
      "Batch 132, Loss: 1.038367, Accuracy: 75.92%\n",
      "Batch 133, Loss: 0.994424, Accuracy: 75.90%\n",
      "Batch 134, Loss: 1.080277, Accuracy: 75.82%\n",
      "Batch 135, Loss: 0.971879, Accuracy: 75.82%\n",
      "Batch 136, Loss: 0.996531, Accuracy: 75.80%\n",
      "Batch 137, Loss: 0.954809, Accuracy: 75.82%\n",
      "Batch 138, Loss: 1.021876, Accuracy: 75.79%\n",
      "Batch 139, Loss: 0.981191, Accuracy: 75.81%\n",
      "Batch 140, Loss: 1.037618, Accuracy: 75.76%\n",
      "Batch 141, Loss: 0.914171, Accuracy: 75.83%\n",
      "Batch 142, Loss: 0.954371, Accuracy: 75.87%\n",
      "Batch 143, Loss: 1.005632, Accuracy: 75.85%\n",
      "Batch 144, Loss: 1.048499, Accuracy: 75.81%\n",
      "Batch 145, Loss: 0.903185, Accuracy: 75.89%\n",
      "Batch 146, Loss: 1.038395, Accuracy: 75.85%\n",
      "Batch 147, Loss: 1.011332, Accuracy: 75.82%\n",
      "Batch 148, Loss: 0.986126, Accuracy: 75.81%\n",
      "Batch 149, Loss: 0.975375, Accuracy: 75.82%\n",
      "Batch 150, Loss: 0.954284, Accuracy: 75.84%\n",
      "Batch 151, Loss: 0.939832, Accuracy: 75.89%\n",
      "Batch 152, Loss: 1.023426, Accuracy: 75.87%\n",
      "Batch 153, Loss: 0.999072, Accuracy: 75.87%\n",
      "Batch 154, Loss: 0.956360, Accuracy: 75.88%\n",
      "Batch 155, Loss: 0.926531, Accuracy: 75.93%\n",
      "Batch 156, Loss: 1.002570, Accuracy: 75.91%\n",
      "Batch 157, Loss: 0.919980, Accuracy: 75.96%\n",
      "Batch 158, Loss: 0.934854, Accuracy: 75.99%\n",
      "Batch 159, Loss: 0.958499, Accuracy: 76.00%\n",
      "Batch 160, Loss: 0.961160, Accuracy: 76.03%\n",
      "Batch 161, Loss: 0.979510, Accuracy: 76.04%\n",
      "Batch 162, Loss: 1.003173, Accuracy: 76.04%\n",
      "Batch 163, Loss: 0.961595, Accuracy: 76.05%\n",
      "Batch 164, Loss: 0.869194, Accuracy: 76.13%\n",
      "Batch 165, Loss: 0.943501, Accuracy: 76.16%\n",
      "Batch 166, Loss: 0.941188, Accuracy: 76.19%\n",
      "Batch 167, Loss: 1.035527, Accuracy: 76.16%\n",
      "Batch 168, Loss: 0.984165, Accuracy: 76.15%\n",
      "Batch 169, Loss: 0.985070, Accuracy: 76.15%\n",
      "Batch 170, Loss: 0.969123, Accuracy: 76.17%\n",
      "Batch 171, Loss: 0.971734, Accuracy: 76.17%\n",
      "Batch 172, Loss: 0.932795, Accuracy: 76.21%\n",
      "Batch 173, Loss: 1.048622, Accuracy: 76.17%\n",
      "Batch 174, Loss: 0.974685, Accuracy: 76.17%\n",
      "Batch 175, Loss: 0.958411, Accuracy: 76.20%\n",
      "Batch 176, Loss: 1.011085, Accuracy: 76.17%\n",
      "Batch 177, Loss: 1.000088, Accuracy: 76.17%\n",
      "Batch 178, Loss: 0.951942, Accuracy: 76.19%\n",
      "Batch 179, Loss: 1.028032, Accuracy: 76.17%\n",
      "Batch 180, Loss: 0.946984, Accuracy: 76.20%\n",
      "Batch 181, Loss: 1.011935, Accuracy: 76.18%\n",
      "Batch 182, Loss: 0.840360, Accuracy: 76.27%\n",
      "Batch 183, Loss: 1.105653, Accuracy: 76.20%\n",
      "Batch 184, Loss: 1.137717, Accuracy: 76.11%\n",
      "Batch 185, Loss: 0.971519, Accuracy: 76.11%\n",
      "Batch 186, Loss: 0.921803, Accuracy: 76.15%\n",
      "Batch 187, Loss: 0.948780, Accuracy: 76.17%\n",
      "Batch 188, Loss: 1.043807, Accuracy: 76.13%\n",
      "Batch 189, Loss: 0.965131, Accuracy: 76.14%\n",
      "Batch 190, Loss: 0.996270, Accuracy: 76.14%\n",
      "Batch 191, Loss: 1.005279, Accuracy: 76.13%\n",
      "Batch 192, Loss: 0.897820, Accuracy: 76.17%\n",
      "Batch 193, Loss: 0.961136, Accuracy: 76.18%\n",
      "Batch 194, Loss: 1.022211, Accuracy: 76.16%\n",
      "Batch 195, Loss: 0.956096, Accuracy: 76.17%\n",
      "Batch 196, Loss: 0.999348, Accuracy: 76.16%\n",
      "Batch 197, Loss: 0.942658, Accuracy: 76.17%\n",
      "Batch 198, Loss: 0.962789, Accuracy: 76.18%\n",
      "Batch 199, Loss: 1.023975, Accuracy: 76.15%\n",
      "Batch 200, Loss: 1.029437, Accuracy: 76.12%\n",
      "Batch 201, Loss: 1.036275, Accuracy: 76.10%\n",
      "Batch 202, Loss: 1.044096, Accuracy: 76.07%\n",
      "Batch 203, Loss: 1.117365, Accuracy: 76.00%\n",
      "Batch 204, Loss: 0.923444, Accuracy: 76.03%\n",
      "Batch 205, Loss: 0.935374, Accuracy: 76.04%\n",
      "Batch 206, Loss: 0.956040, Accuracy: 76.06%\n",
      "Batch 207, Loss: 1.008928, Accuracy: 76.05%\n",
      "Batch 208, Loss: 0.967363, Accuracy: 76.05%\n",
      "Batch 209, Loss: 0.995846, Accuracy: 76.05%\n",
      "Batch 210, Loss: 0.952859, Accuracy: 76.06%\n",
      "Batch 211, Loss: 0.969616, Accuracy: 76.07%\n",
      "Batch 212, Loss: 0.979398, Accuracy: 76.07%\n",
      "Batch 213, Loss: 0.980233, Accuracy: 76.07%\n",
      "Training - Epoch 30, Loss: 0.983196, Accuracy: 76.07%\n",
      "Validation Batch 1, Loss: 0.962934, Accuracy: 78.12%\n",
      "Validation Batch 2, Loss: 1.032581, Accuracy: 73.44%\n",
      "Validation Batch 3, Loss: 1.069846, Accuracy: 70.83%\n",
      "Validation Batch 4, Loss: 0.983839, Accuracy: 72.27%\n",
      "Validation Batch 5, Loss: 0.980972, Accuracy: 72.81%\n",
      "Validation Batch 6, Loss: 0.924444, Accuracy: 74.48%\n",
      "Validation Batch 7, Loss: 1.015131, Accuracy: 74.33%\n",
      "Validation Batch 8, Loss: 1.038562, Accuracy: 74.02%\n",
      "Validation Batch 9, Loss: 1.039940, Accuracy: 73.61%\n",
      "Validation Batch 10, Loss: 0.987949, Accuracy: 73.91%\n",
      "Validation Batch 11, Loss: 0.972475, Accuracy: 74.43%\n",
      "Validation Batch 12, Loss: 0.961960, Accuracy: 75.00%\n",
      "Validation Batch 13, Loss: 1.058853, Accuracy: 74.28%\n",
      "Validation Batch 14, Loss: 1.014018, Accuracy: 74.11%\n",
      "Validation Batch 15, Loss: 0.994019, Accuracy: 74.38%\n",
      "Validation Batch 16, Loss: 0.979121, Accuracy: 74.71%\n",
      "Validation Batch 17, Loss: 1.074512, Accuracy: 74.26%\n",
      "Validation Batch 18, Loss: 0.976881, Accuracy: 74.48%\n",
      "Validation Batch 19, Loss: 1.031950, Accuracy: 74.34%\n",
      "Validation Batch 20, Loss: 1.032638, Accuracy: 74.22%\n",
      "Validation Batch 21, Loss: 0.995668, Accuracy: 74.26%\n",
      "Validation Batch 22, Loss: 1.055624, Accuracy: 74.01%\n",
      "Validation Batch 23, Loss: 1.091099, Accuracy: 73.71%\n",
      "Validation Batch 24, Loss: 1.018449, Accuracy: 73.63%\n",
      "Validation Batch 25, Loss: 1.001337, Accuracy: 73.50%\n",
      "Validation Batch 26, Loss: 1.018899, Accuracy: 73.44%\n",
      "Validation Batch 27, Loss: 0.977114, Accuracy: 73.46%\n",
      "Validation - Epoch 30, Loss: 1.010771, Accuracy: 73.46%\n",
      "Patience—1\n",
      "Epoch 31\n",
      "Batch 1, Loss: 0.966086, Accuracy: 78.12%\n",
      "Batch 2, Loss: 1.032828, Accuracy: 75.00%\n",
      "Batch 3, Loss: 1.040944, Accuracy: 73.44%\n",
      "Batch 4, Loss: 0.986057, Accuracy: 73.83%\n",
      "Batch 5, Loss: 0.986583, Accuracy: 74.06%\n",
      "Batch 6, Loss: 0.956126, Accuracy: 74.74%\n",
      "Batch 7, Loss: 0.954929, Accuracy: 75.00%\n",
      "Batch 8, Loss: 0.944627, Accuracy: 75.78%\n",
      "Batch 9, Loss: 0.988075, Accuracy: 75.69%\n",
      "Batch 10, Loss: 0.890888, Accuracy: 76.72%\n",
      "Batch 11, Loss: 0.924919, Accuracy: 77.13%\n",
      "Batch 12, Loss: 0.963150, Accuracy: 77.21%\n",
      "Batch 13, Loss: 1.030548, Accuracy: 76.68%\n",
      "Batch 14, Loss: 0.983477, Accuracy: 76.79%\n",
      "Batch 15, Loss: 0.960126, Accuracy: 77.08%\n",
      "Batch 16, Loss: 0.908470, Accuracy: 77.54%\n",
      "Batch 17, Loss: 0.987960, Accuracy: 77.39%\n",
      "Batch 18, Loss: 0.955016, Accuracy: 77.60%\n",
      "Batch 19, Loss: 0.978538, Accuracy: 77.71%\n",
      "Batch 20, Loss: 0.973825, Accuracy: 77.66%\n",
      "Batch 21, Loss: 0.965856, Accuracy: 77.68%\n",
      "Batch 22, Loss: 0.988221, Accuracy: 77.56%\n",
      "Batch 23, Loss: 1.004104, Accuracy: 77.45%\n",
      "Batch 24, Loss: 1.071013, Accuracy: 77.08%\n",
      "Batch 25, Loss: 0.999699, Accuracy: 77.00%\n",
      "Batch 26, Loss: 0.976985, Accuracy: 76.98%\n",
      "Batch 27, Loss: 0.978284, Accuracy: 76.91%\n",
      "Batch 28, Loss: 0.964067, Accuracy: 77.01%\n",
      "Batch 29, Loss: 0.987281, Accuracy: 76.99%\n",
      "Batch 30, Loss: 0.994201, Accuracy: 76.93%\n",
      "Batch 31, Loss: 0.931185, Accuracy: 77.02%\n",
      "Batch 32, Loss: 0.997522, Accuracy: 77.00%\n",
      "Batch 33, Loss: 1.106279, Accuracy: 76.56%\n",
      "Batch 34, Loss: 0.913979, Accuracy: 76.70%\n",
      "Batch 35, Loss: 1.002777, Accuracy: 76.61%\n",
      "Batch 36, Loss: 0.992047, Accuracy: 76.52%\n",
      "Batch 37, Loss: 1.028333, Accuracy: 76.39%\n",
      "Batch 38, Loss: 0.999782, Accuracy: 76.36%\n",
      "Batch 39, Loss: 1.002866, Accuracy: 76.28%\n",
      "Batch 40, Loss: 1.081563, Accuracy: 76.09%\n",
      "Batch 41, Loss: 0.983144, Accuracy: 76.14%\n",
      "Batch 42, Loss: 0.983247, Accuracy: 76.12%\n",
      "Batch 43, Loss: 0.969565, Accuracy: 76.16%\n",
      "Batch 44, Loss: 0.926452, Accuracy: 76.28%\n",
      "Batch 45, Loss: 0.931518, Accuracy: 76.39%\n",
      "Batch 46, Loss: 0.957499, Accuracy: 76.46%\n",
      "Batch 47, Loss: 0.970224, Accuracy: 76.50%\n",
      "Batch 48, Loss: 1.052492, Accuracy: 76.33%\n",
      "Batch 49, Loss: 0.951450, Accuracy: 76.40%\n",
      "Batch 50, Loss: 0.953110, Accuracy: 76.47%\n",
      "Batch 51, Loss: 0.980078, Accuracy: 76.41%\n",
      "Batch 52, Loss: 0.871965, Accuracy: 76.62%\n",
      "Batch 53, Loss: 0.998906, Accuracy: 76.59%\n",
      "Batch 54, Loss: 0.980559, Accuracy: 76.56%\n",
      "Batch 55, Loss: 0.915773, Accuracy: 76.68%\n",
      "Batch 56, Loss: 0.988687, Accuracy: 76.67%\n",
      "Batch 57, Loss: 0.913467, Accuracy: 76.78%\n",
      "Batch 58, Loss: 1.015158, Accuracy: 76.70%\n",
      "Batch 59, Loss: 0.969735, Accuracy: 76.72%\n",
      "Batch 60, Loss: 0.991855, Accuracy: 76.67%\n",
      "Batch 61, Loss: 1.006716, Accuracy: 76.64%\n",
      "Batch 62, Loss: 0.939201, Accuracy: 76.69%\n",
      "Batch 63, Loss: 0.984486, Accuracy: 76.69%\n",
      "Batch 64, Loss: 0.904521, Accuracy: 76.83%\n",
      "Batch 65, Loss: 0.930953, Accuracy: 76.90%\n",
      "Batch 66, Loss: 0.940860, Accuracy: 76.96%\n",
      "Batch 67, Loss: 0.992774, Accuracy: 76.94%\n",
      "Batch 68, Loss: 0.950694, Accuracy: 77.00%\n",
      "Batch 69, Loss: 0.957262, Accuracy: 77.04%\n",
      "Batch 70, Loss: 1.011768, Accuracy: 76.99%\n",
      "Batch 71, Loss: 1.045813, Accuracy: 76.83%\n",
      "Batch 72, Loss: 0.876509, Accuracy: 76.95%\n",
      "Batch 73, Loss: 0.992821, Accuracy: 76.93%\n",
      "Batch 74, Loss: 0.979003, Accuracy: 76.90%\n",
      "Batch 75, Loss: 0.924127, Accuracy: 76.98%\n",
      "Batch 76, Loss: 1.017522, Accuracy: 76.91%\n",
      "Batch 77, Loss: 0.886242, Accuracy: 77.05%\n",
      "Batch 78, Loss: 0.985505, Accuracy: 77.04%\n",
      "Batch 79, Loss: 1.098063, Accuracy: 76.88%\n",
      "Batch 80, Loss: 0.960989, Accuracy: 76.89%\n",
      "Batch 81, Loss: 0.954175, Accuracy: 76.97%\n",
      "Batch 82, Loss: 0.960438, Accuracy: 76.96%\n",
      "Batch 83, Loss: 0.989385, Accuracy: 76.94%\n",
      "Batch 84, Loss: 1.027964, Accuracy: 76.86%\n",
      "Batch 85, Loss: 1.107780, Accuracy: 76.69%\n",
      "Batch 86, Loss: 0.974028, Accuracy: 76.71%\n",
      "Batch 87, Loss: 0.968880, Accuracy: 76.71%\n",
      "Batch 88, Loss: 0.964281, Accuracy: 76.74%\n",
      "Batch 89, Loss: 1.045251, Accuracy: 76.65%\n",
      "Batch 90, Loss: 1.017026, Accuracy: 76.61%\n",
      "Batch 91, Loss: 1.084654, Accuracy: 76.49%\n",
      "Batch 92, Loss: 0.951595, Accuracy: 76.55%\n",
      "Batch 93, Loss: 0.964133, Accuracy: 76.53%\n",
      "Batch 94, Loss: 0.945026, Accuracy: 76.60%\n",
      "Batch 95, Loss: 1.108661, Accuracy: 76.48%\n",
      "Batch 96, Loss: 1.014104, Accuracy: 76.42%\n",
      "Batch 97, Loss: 0.881818, Accuracy: 76.51%\n",
      "Batch 98, Loss: 0.990454, Accuracy: 76.51%\n",
      "Batch 99, Loss: 0.987155, Accuracy: 76.52%\n",
      "Batch 100, Loss: 0.987847, Accuracy: 76.50%\n",
      "Batch 101, Loss: 1.032853, Accuracy: 76.47%\n",
      "Batch 102, Loss: 1.014196, Accuracy: 76.42%\n",
      "Batch 103, Loss: 1.018052, Accuracy: 76.38%\n",
      "Batch 104, Loss: 0.979753, Accuracy: 76.37%\n",
      "Batch 105, Loss: 0.969948, Accuracy: 76.40%\n",
      "Batch 106, Loss: 1.017311, Accuracy: 76.36%\n",
      "Batch 107, Loss: 0.987882, Accuracy: 76.34%\n",
      "Batch 108, Loss: 1.053730, Accuracy: 76.26%\n",
      "Batch 109, Loss: 1.008999, Accuracy: 76.22%\n",
      "Batch 110, Loss: 1.003537, Accuracy: 76.19%\n",
      "Batch 111, Loss: 0.894625, Accuracy: 76.27%\n",
      "Batch 112, Loss: 1.034827, Accuracy: 76.20%\n",
      "Batch 113, Loss: 0.943331, Accuracy: 76.23%\n",
      "Batch 114, Loss: 1.053020, Accuracy: 76.19%\n",
      "Batch 115, Loss: 1.016809, Accuracy: 76.17%\n",
      "Batch 116, Loss: 0.968270, Accuracy: 76.19%\n",
      "Batch 117, Loss: 0.934694, Accuracy: 76.23%\n",
      "Batch 118, Loss: 0.957710, Accuracy: 76.24%\n",
      "Batch 119, Loss: 0.948267, Accuracy: 76.29%\n",
      "Batch 120, Loss: 0.904942, Accuracy: 76.35%\n",
      "Batch 121, Loss: 0.993754, Accuracy: 76.33%\n",
      "Batch 122, Loss: 0.961037, Accuracy: 76.34%\n",
      "Batch 123, Loss: 0.924655, Accuracy: 76.40%\n",
      "Batch 124, Loss: 0.994834, Accuracy: 76.39%\n",
      "Batch 125, Loss: 1.054963, Accuracy: 76.34%\n",
      "Batch 126, Loss: 0.974533, Accuracy: 76.34%\n",
      "Batch 127, Loss: 0.987940, Accuracy: 76.33%\n",
      "Batch 128, Loss: 1.118897, Accuracy: 76.21%\n",
      "Batch 129, Loss: 0.988828, Accuracy: 76.21%\n",
      "Batch 130, Loss: 1.013840, Accuracy: 76.19%\n",
      "Batch 131, Loss: 0.955081, Accuracy: 76.20%\n",
      "Batch 132, Loss: 1.096571, Accuracy: 76.10%\n",
      "Batch 133, Loss: 1.027934, Accuracy: 76.06%\n",
      "Batch 134, Loss: 0.952500, Accuracy: 76.08%\n",
      "Batch 135, Loss: 1.015229, Accuracy: 76.05%\n",
      "Batch 136, Loss: 1.015292, Accuracy: 76.02%\n",
      "Batch 137, Loss: 0.970944, Accuracy: 76.04%\n",
      "Batch 138, Loss: 0.998357, Accuracy: 76.03%\n",
      "Batch 139, Loss: 0.921335, Accuracy: 76.08%\n",
      "Batch 140, Loss: 0.936016, Accuracy: 76.12%\n",
      "Batch 141, Loss: 1.013491, Accuracy: 76.10%\n",
      "Batch 142, Loss: 0.980910, Accuracy: 76.10%\n",
      "Batch 143, Loss: 1.004210, Accuracy: 76.08%\n",
      "Batch 144, Loss: 0.981664, Accuracy: 76.07%\n",
      "Batch 145, Loss: 1.061635, Accuracy: 76.02%\n",
      "Batch 146, Loss: 0.997212, Accuracy: 76.03%\n",
      "Batch 147, Loss: 1.061954, Accuracy: 75.97%\n",
      "Batch 148, Loss: 0.926373, Accuracy: 76.00%\n",
      "Batch 149, Loss: 0.998806, Accuracy: 75.99%\n",
      "Batch 150, Loss: 0.945531, Accuracy: 76.01%\n",
      "Batch 151, Loss: 0.994789, Accuracy: 76.00%\n",
      "Batch 152, Loss: 0.889552, Accuracy: 76.07%\n",
      "Batch 153, Loss: 0.931441, Accuracy: 76.10%\n",
      "Batch 154, Loss: 0.941247, Accuracy: 76.13%\n",
      "Batch 155, Loss: 0.941960, Accuracy: 76.15%\n",
      "Batch 156, Loss: 0.986564, Accuracy: 76.14%\n",
      "Batch 157, Loss: 1.041233, Accuracy: 76.11%\n",
      "Batch 158, Loss: 0.991783, Accuracy: 76.12%\n",
      "Batch 159, Loss: 0.968567, Accuracy: 76.13%\n",
      "Batch 160, Loss: 1.001594, Accuracy: 76.12%\n",
      "Batch 161, Loss: 0.924171, Accuracy: 76.17%\n",
      "Batch 162, Loss: 1.018080, Accuracy: 76.15%\n",
      "Batch 163, Loss: 0.940358, Accuracy: 76.17%\n",
      "Batch 164, Loss: 1.015927, Accuracy: 76.12%\n",
      "Batch 165, Loss: 0.940530, Accuracy: 76.13%\n",
      "Batch 166, Loss: 1.058416, Accuracy: 76.07%\n",
      "Batch 167, Loss: 1.039749, Accuracy: 76.04%\n",
      "Batch 168, Loss: 0.972636, Accuracy: 76.06%\n",
      "Batch 169, Loss: 0.996889, Accuracy: 76.07%\n",
      "Batch 170, Loss: 1.053133, Accuracy: 76.02%\n",
      "Batch 171, Loss: 0.891599, Accuracy: 76.08%\n",
      "Batch 172, Loss: 0.975458, Accuracy: 76.08%\n",
      "Batch 173, Loss: 0.974855, Accuracy: 76.07%\n",
      "Batch 174, Loss: 0.913513, Accuracy: 76.11%\n",
      "Batch 175, Loss: 1.067098, Accuracy: 76.05%\n",
      "Batch 176, Loss: 1.085961, Accuracy: 75.99%\n",
      "Batch 177, Loss: 1.022815, Accuracy: 75.96%\n",
      "Batch 178, Loss: 0.947517, Accuracy: 75.98%\n",
      "Batch 179, Loss: 0.982151, Accuracy: 76.00%\n",
      "Batch 180, Loss: 0.968037, Accuracy: 76.02%\n",
      "Batch 181, Loss: 1.046036, Accuracy: 75.98%\n",
      "Batch 182, Loss: 0.989737, Accuracy: 75.97%\n",
      "Batch 183, Loss: 0.962692, Accuracy: 76.00%\n",
      "Batch 184, Loss: 0.935616, Accuracy: 76.03%\n",
      "Batch 185, Loss: 0.990774, Accuracy: 76.01%\n",
      "Batch 186, Loss: 0.997992, Accuracy: 76.00%\n",
      "Batch 187, Loss: 0.906588, Accuracy: 76.04%\n",
      "Batch 188, Loss: 1.024751, Accuracy: 76.01%\n",
      "Batch 189, Loss: 0.939250, Accuracy: 76.03%\n",
      "Batch 190, Loss: 1.071265, Accuracy: 75.98%\n",
      "Batch 191, Loss: 0.980164, Accuracy: 75.98%\n",
      "Batch 192, Loss: 0.970625, Accuracy: 75.98%\n",
      "Batch 193, Loss: 0.901462, Accuracy: 76.04%\n",
      "Batch 194, Loss: 0.988349, Accuracy: 76.03%\n",
      "Batch 195, Loss: 1.058843, Accuracy: 75.99%\n",
      "Batch 196, Loss: 0.960002, Accuracy: 76.00%\n",
      "Batch 197, Loss: 0.896184, Accuracy: 76.04%\n",
      "Batch 198, Loss: 0.910506, Accuracy: 76.09%\n",
      "Batch 199, Loss: 0.971913, Accuracy: 76.09%\n",
      "Batch 200, Loss: 0.981666, Accuracy: 76.11%\n",
      "Batch 201, Loss: 0.925126, Accuracy: 76.14%\n",
      "Batch 202, Loss: 0.895684, Accuracy: 76.18%\n",
      "Batch 203, Loss: 0.971899, Accuracy: 76.19%\n",
      "Batch 204, Loss: 0.963288, Accuracy: 76.20%\n",
      "Batch 205, Loss: 0.926351, Accuracy: 76.22%\n",
      "Batch 206, Loss: 0.961022, Accuracy: 76.23%\n",
      "Batch 207, Loss: 1.064445, Accuracy: 76.20%\n",
      "Batch 208, Loss: 0.947093, Accuracy: 76.21%\n",
      "Batch 209, Loss: 1.052231, Accuracy: 76.17%\n",
      "Batch 210, Loss: 0.929308, Accuracy: 76.21%\n",
      "Batch 211, Loss: 0.897430, Accuracy: 76.24%\n",
      "Batch 212, Loss: 1.023994, Accuracy: 76.22%\n",
      "Batch 213, Loss: 0.939049, Accuracy: 76.24%\n",
      "Training - Epoch 31, Loss: 0.981646, Accuracy: 76.24%\n",
      "Validation Batch 1, Loss: 0.954730, Accuracy: 81.25%\n",
      "Validation Batch 2, Loss: 1.026980, Accuracy: 76.56%\n",
      "Validation Batch 3, Loss: 1.076093, Accuracy: 72.40%\n",
      "Validation Batch 4, Loss: 0.986113, Accuracy: 73.44%\n",
      "Validation Batch 5, Loss: 0.973696, Accuracy: 74.69%\n",
      "Validation Batch 6, Loss: 0.914242, Accuracy: 76.30%\n",
      "Validation Batch 7, Loss: 1.013573, Accuracy: 75.67%\n",
      "Validation Batch 8, Loss: 1.032751, Accuracy: 75.39%\n",
      "Validation Batch 9, Loss: 1.028750, Accuracy: 75.00%\n",
      "Validation Batch 10, Loss: 0.981912, Accuracy: 75.16%\n",
      "Validation Batch 11, Loss: 0.972020, Accuracy: 75.43%\n",
      "Validation Batch 12, Loss: 0.962167, Accuracy: 75.78%\n",
      "Validation Batch 13, Loss: 1.046871, Accuracy: 75.36%\n",
      "Validation Batch 14, Loss: 1.015429, Accuracy: 75.11%\n",
      "Validation Batch 15, Loss: 0.989536, Accuracy: 75.31%\n",
      "Validation Batch 16, Loss: 0.977964, Accuracy: 75.49%\n",
      "Validation Batch 17, Loss: 1.069666, Accuracy: 75.00%\n",
      "Validation Batch 18, Loss: 0.979391, Accuracy: 75.09%\n",
      "Validation Batch 19, Loss: 1.026521, Accuracy: 75.08%\n",
      "Validation Batch 20, Loss: 1.031953, Accuracy: 74.92%\n",
      "Validation Batch 21, Loss: 0.995313, Accuracy: 74.93%\n",
      "Validation Batch 22, Loss: 1.044347, Accuracy: 74.72%\n",
      "Validation Batch 23, Loss: 1.083753, Accuracy: 74.39%\n",
      "Validation Batch 24, Loss: 1.008466, Accuracy: 74.41%\n",
      "Validation Batch 25, Loss: 1.000220, Accuracy: 74.38%\n",
      "Validation Batch 26, Loss: 1.018292, Accuracy: 74.28%\n",
      "Validation Batch 27, Loss: 0.970166, Accuracy: 74.34%\n",
      "Validation - Epoch 31, Loss: 1.006701, Accuracy: 74.34%\n",
      "Patience—2\n",
      "Epoch 32\n",
      "Batch 1, Loss: 0.927910, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.900314, Accuracy: 83.59%\n",
      "Batch 3, Loss: 0.920304, Accuracy: 83.33%\n",
      "Batch 4, Loss: 0.953406, Accuracy: 82.03%\n",
      "Batch 5, Loss: 1.039676, Accuracy: 80.00%\n",
      "Batch 6, Loss: 0.831646, Accuracy: 82.03%\n",
      "Batch 7, Loss: 1.019672, Accuracy: 80.58%\n",
      "Batch 8, Loss: 1.000496, Accuracy: 79.69%\n",
      "Batch 9, Loss: 0.948910, Accuracy: 79.51%\n",
      "Batch 10, Loss: 1.072556, Accuracy: 78.28%\n",
      "Batch 11, Loss: 1.104231, Accuracy: 76.99%\n",
      "Batch 12, Loss: 0.953050, Accuracy: 77.21%\n",
      "Batch 13, Loss: 0.987675, Accuracy: 77.04%\n",
      "Batch 14, Loss: 0.943087, Accuracy: 77.23%\n",
      "Batch 15, Loss: 0.911176, Accuracy: 77.71%\n",
      "Batch 16, Loss: 0.929006, Accuracy: 78.03%\n",
      "Batch 17, Loss: 0.903329, Accuracy: 78.40%\n",
      "Batch 18, Loss: 0.980324, Accuracy: 78.04%\n",
      "Batch 19, Loss: 0.958699, Accuracy: 77.96%\n",
      "Batch 20, Loss: 0.900328, Accuracy: 78.28%\n",
      "Batch 21, Loss: 1.052438, Accuracy: 77.68%\n",
      "Batch 22, Loss: 1.015267, Accuracy: 77.56%\n",
      "Batch 23, Loss: 0.946482, Accuracy: 77.58%\n",
      "Batch 24, Loss: 0.940555, Accuracy: 77.67%\n",
      "Batch 25, Loss: 0.975547, Accuracy: 77.62%\n",
      "Batch 26, Loss: 0.908679, Accuracy: 77.88%\n",
      "Batch 27, Loss: 0.908114, Accuracy: 78.12%\n",
      "Batch 28, Loss: 0.982801, Accuracy: 78.01%\n",
      "Batch 29, Loss: 0.924001, Accuracy: 78.18%\n",
      "Batch 30, Loss: 1.013890, Accuracy: 78.02%\n",
      "Batch 31, Loss: 1.008050, Accuracy: 77.87%\n",
      "Batch 32, Loss: 0.998152, Accuracy: 77.78%\n",
      "Batch 33, Loss: 1.061252, Accuracy: 77.51%\n",
      "Batch 34, Loss: 0.933181, Accuracy: 77.62%\n",
      "Batch 35, Loss: 0.975326, Accuracy: 77.59%\n",
      "Batch 36, Loss: 0.948387, Accuracy: 77.65%\n",
      "Batch 37, Loss: 0.936710, Accuracy: 77.79%\n",
      "Batch 38, Loss: 1.021475, Accuracy: 77.67%\n",
      "Batch 39, Loss: 0.926754, Accuracy: 77.80%\n",
      "Batch 40, Loss: 0.946508, Accuracy: 77.89%\n",
      "Batch 41, Loss: 0.947417, Accuracy: 77.93%\n",
      "Batch 42, Loss: 1.013426, Accuracy: 77.79%\n",
      "Batch 43, Loss: 1.019903, Accuracy: 77.65%\n",
      "Batch 44, Loss: 0.951375, Accuracy: 77.70%\n",
      "Batch 45, Loss: 0.970716, Accuracy: 77.71%\n",
      "Batch 46, Loss: 0.997921, Accuracy: 77.58%\n",
      "Batch 47, Loss: 0.968750, Accuracy: 77.59%\n",
      "Batch 48, Loss: 0.950689, Accuracy: 77.67%\n",
      "Batch 49, Loss: 1.004786, Accuracy: 77.65%\n",
      "Batch 50, Loss: 0.994645, Accuracy: 77.59%\n",
      "Batch 51, Loss: 1.018374, Accuracy: 77.45%\n",
      "Batch 52, Loss: 1.045858, Accuracy: 77.28%\n",
      "Batch 53, Loss: 1.048573, Accuracy: 77.18%\n",
      "Batch 54, Loss: 1.041385, Accuracy: 77.05%\n",
      "Batch 55, Loss: 0.916190, Accuracy: 77.13%\n",
      "Batch 56, Loss: 0.964800, Accuracy: 77.15%\n",
      "Batch 57, Loss: 1.016059, Accuracy: 77.03%\n",
      "Batch 58, Loss: 0.975047, Accuracy: 77.05%\n",
      "Batch 59, Loss: 0.946833, Accuracy: 77.07%\n",
      "Batch 60, Loss: 1.017102, Accuracy: 76.98%\n",
      "Batch 61, Loss: 0.942467, Accuracy: 77.02%\n",
      "Batch 62, Loss: 1.067502, Accuracy: 76.86%\n",
      "Batch 63, Loss: 1.009544, Accuracy: 76.81%\n",
      "Batch 64, Loss: 1.007952, Accuracy: 76.78%\n",
      "Batch 65, Loss: 1.008730, Accuracy: 76.73%\n",
      "Batch 66, Loss: 0.967034, Accuracy: 76.73%\n",
      "Batch 67, Loss: 1.022384, Accuracy: 76.61%\n",
      "Batch 68, Loss: 0.999035, Accuracy: 76.59%\n",
      "Batch 69, Loss: 0.932409, Accuracy: 76.68%\n",
      "Batch 70, Loss: 1.022594, Accuracy: 76.61%\n",
      "Batch 71, Loss: 0.942445, Accuracy: 76.65%\n",
      "Batch 72, Loss: 0.913890, Accuracy: 76.78%\n",
      "Batch 73, Loss: 0.909268, Accuracy: 76.86%\n",
      "Batch 74, Loss: 1.010117, Accuracy: 76.82%\n",
      "Batch 75, Loss: 1.049473, Accuracy: 76.71%\n",
      "Batch 76, Loss: 0.915532, Accuracy: 76.81%\n",
      "Batch 77, Loss: 0.999009, Accuracy: 76.77%\n",
      "Batch 78, Loss: 1.047520, Accuracy: 76.68%\n",
      "Batch 79, Loss: 0.957389, Accuracy: 76.66%\n",
      "Batch 80, Loss: 0.856617, Accuracy: 76.82%\n",
      "Batch 81, Loss: 0.947924, Accuracy: 76.87%\n",
      "Batch 82, Loss: 1.019104, Accuracy: 76.81%\n",
      "Batch 83, Loss: 0.903260, Accuracy: 76.90%\n",
      "Batch 84, Loss: 0.975677, Accuracy: 76.90%\n",
      "Batch 85, Loss: 0.954168, Accuracy: 76.93%\n",
      "Batch 86, Loss: 0.986392, Accuracy: 76.93%\n",
      "Batch 87, Loss: 1.019432, Accuracy: 76.87%\n",
      "Batch 88, Loss: 0.883859, Accuracy: 76.99%\n",
      "Batch 89, Loss: 0.988924, Accuracy: 76.97%\n",
      "Batch 90, Loss: 0.988198, Accuracy: 76.94%\n",
      "Batch 91, Loss: 1.023964, Accuracy: 76.89%\n",
      "Batch 92, Loss: 0.924926, Accuracy: 76.97%\n",
      "Batch 93, Loss: 0.985264, Accuracy: 76.92%\n",
      "Batch 94, Loss: 1.005350, Accuracy: 76.86%\n",
      "Batch 95, Loss: 1.016730, Accuracy: 76.83%\n",
      "Batch 96, Loss: 0.938384, Accuracy: 76.86%\n",
      "Batch 97, Loss: 0.924836, Accuracy: 76.90%\n",
      "Batch 98, Loss: 1.025281, Accuracy: 76.88%\n",
      "Batch 99, Loss: 0.978878, Accuracy: 76.88%\n",
      "Batch 100, Loss: 1.011794, Accuracy: 76.83%\n",
      "Batch 101, Loss: 1.057716, Accuracy: 76.73%\n",
      "Batch 102, Loss: 0.902936, Accuracy: 76.82%\n",
      "Batch 103, Loss: 0.948396, Accuracy: 76.84%\n",
      "Batch 104, Loss: 1.057588, Accuracy: 76.76%\n",
      "Batch 105, Loss: 1.026908, Accuracy: 76.73%\n",
      "Batch 106, Loss: 1.010061, Accuracy: 76.67%\n",
      "Batch 107, Loss: 0.892251, Accuracy: 76.75%\n",
      "Batch 108, Loss: 1.051329, Accuracy: 76.68%\n",
      "Batch 109, Loss: 1.020718, Accuracy: 76.66%\n",
      "Batch 110, Loss: 0.958788, Accuracy: 76.66%\n",
      "Batch 111, Loss: 0.903435, Accuracy: 76.73%\n",
      "Batch 112, Loss: 0.977824, Accuracy: 76.73%\n",
      "Batch 113, Loss: 1.013138, Accuracy: 76.71%\n",
      "Batch 114, Loss: 0.930392, Accuracy: 76.77%\n",
      "Batch 115, Loss: 0.973470, Accuracy: 76.75%\n",
      "Batch 116, Loss: 0.941413, Accuracy: 76.79%\n",
      "Batch 117, Loss: 0.931995, Accuracy: 76.84%\n",
      "Batch 118, Loss: 0.938268, Accuracy: 76.85%\n",
      "Batch 119, Loss: 0.982108, Accuracy: 76.86%\n",
      "Batch 120, Loss: 0.935552, Accuracy: 76.90%\n",
      "Batch 121, Loss: 0.989771, Accuracy: 76.90%\n",
      "Batch 122, Loss: 1.033468, Accuracy: 76.86%\n",
      "Batch 123, Loss: 1.020296, Accuracy: 76.82%\n",
      "Batch 124, Loss: 0.968190, Accuracy: 76.83%\n",
      "Batch 125, Loss: 0.897311, Accuracy: 76.89%\n",
      "Batch 126, Loss: 1.076502, Accuracy: 76.80%\n",
      "Batch 127, Loss: 0.955073, Accuracy: 76.81%\n",
      "Batch 128, Loss: 0.960425, Accuracy: 76.82%\n",
      "Batch 129, Loss: 0.937714, Accuracy: 76.87%\n",
      "Batch 130, Loss: 0.977868, Accuracy: 76.88%\n",
      "Batch 131, Loss: 0.960579, Accuracy: 76.88%\n",
      "Batch 132, Loss: 1.016119, Accuracy: 76.85%\n",
      "Batch 133, Loss: 0.908580, Accuracy: 76.90%\n",
      "Batch 134, Loss: 0.989542, Accuracy: 76.89%\n",
      "Batch 135, Loss: 0.889628, Accuracy: 76.94%\n",
      "Batch 136, Loss: 1.063807, Accuracy: 76.88%\n",
      "Batch 137, Loss: 0.982335, Accuracy: 76.88%\n",
      "Batch 138, Loss: 0.946011, Accuracy: 76.90%\n",
      "Batch 139, Loss: 0.906652, Accuracy: 76.96%\n",
      "Batch 140, Loss: 0.927284, Accuracy: 77.00%\n",
      "Batch 141, Loss: 1.016600, Accuracy: 76.98%\n",
      "Batch 142, Loss: 0.904546, Accuracy: 77.05%\n",
      "Batch 143, Loss: 0.961589, Accuracy: 77.05%\n",
      "Batch 144, Loss: 0.972619, Accuracy: 77.05%\n",
      "Batch 145, Loss: 0.986600, Accuracy: 77.04%\n",
      "Batch 146, Loss: 1.071039, Accuracy: 76.96%\n",
      "Batch 147, Loss: 0.999531, Accuracy: 76.95%\n",
      "Batch 148, Loss: 0.963886, Accuracy: 76.94%\n",
      "Batch 149, Loss: 0.982802, Accuracy: 76.94%\n",
      "Batch 150, Loss: 0.992393, Accuracy: 76.92%\n",
      "Batch 151, Loss: 0.986247, Accuracy: 76.91%\n",
      "Batch 152, Loss: 1.001066, Accuracy: 76.90%\n",
      "Batch 153, Loss: 1.051957, Accuracy: 76.86%\n",
      "Batch 154, Loss: 0.950495, Accuracy: 76.88%\n",
      "Batch 155, Loss: 0.961664, Accuracy: 76.89%\n",
      "Batch 156, Loss: 0.973990, Accuracy: 76.89%\n",
      "Batch 157, Loss: 0.937417, Accuracy: 76.91%\n",
      "Batch 158, Loss: 0.975961, Accuracy: 76.91%\n",
      "Batch 159, Loss: 0.926911, Accuracy: 76.94%\n",
      "Batch 160, Loss: 0.985977, Accuracy: 76.92%\n",
      "Batch 161, Loss: 0.951800, Accuracy: 76.93%\n",
      "Batch 162, Loss: 0.959576, Accuracy: 76.95%\n",
      "Batch 163, Loss: 1.011760, Accuracy: 76.94%\n",
      "Batch 164, Loss: 0.972668, Accuracy: 76.93%\n",
      "Batch 165, Loss: 0.974458, Accuracy: 76.92%\n",
      "Batch 166, Loss: 1.002174, Accuracy: 76.91%\n",
      "Batch 167, Loss: 0.970118, Accuracy: 76.93%\n",
      "Batch 168, Loss: 0.972368, Accuracy: 76.93%\n",
      "Batch 169, Loss: 0.998715, Accuracy: 76.90%\n",
      "Batch 170, Loss: 1.059651, Accuracy: 76.87%\n",
      "Batch 171, Loss: 0.988886, Accuracy: 76.86%\n",
      "Batch 172, Loss: 1.032760, Accuracy: 76.84%\n",
      "Batch 173, Loss: 0.984101, Accuracy: 76.82%\n",
      "Batch 174, Loss: 0.978163, Accuracy: 76.83%\n",
      "Batch 175, Loss: 1.091434, Accuracy: 76.76%\n",
      "Batch 176, Loss: 0.952068, Accuracy: 76.78%\n",
      "Batch 177, Loss: 0.977963, Accuracy: 76.78%\n",
      "Batch 178, Loss: 1.045587, Accuracy: 76.73%\n",
      "Batch 179, Loss: 1.040108, Accuracy: 76.69%\n",
      "Batch 180, Loss: 1.060617, Accuracy: 76.64%\n",
      "Batch 181, Loss: 0.945095, Accuracy: 76.66%\n",
      "Batch 182, Loss: 1.014423, Accuracy: 76.64%\n",
      "Batch 183, Loss: 0.971530, Accuracy: 76.65%\n",
      "Batch 184, Loss: 0.937795, Accuracy: 76.67%\n",
      "Batch 185, Loss: 1.039556, Accuracy: 76.64%\n",
      "Batch 186, Loss: 1.067457, Accuracy: 76.60%\n",
      "Batch 187, Loss: 1.082849, Accuracy: 76.55%\n",
      "Batch 188, Loss: 0.968129, Accuracy: 76.55%\n",
      "Batch 189, Loss: 0.922911, Accuracy: 76.57%\n",
      "Batch 190, Loss: 0.986686, Accuracy: 76.57%\n",
      "Batch 191, Loss: 1.052369, Accuracy: 76.52%\n",
      "Batch 192, Loss: 0.952723, Accuracy: 76.54%\n",
      "Batch 193, Loss: 1.000420, Accuracy: 76.52%\n",
      "Batch 194, Loss: 1.004190, Accuracy: 76.51%\n",
      "Batch 195, Loss: 0.970131, Accuracy: 76.51%\n",
      "Batch 196, Loss: 0.945387, Accuracy: 76.53%\n",
      "Batch 197, Loss: 0.975336, Accuracy: 76.53%\n",
      "Batch 198, Loss: 0.983535, Accuracy: 76.54%\n",
      "Batch 199, Loss: 0.994264, Accuracy: 76.53%\n",
      "Batch 200, Loss: 0.995648, Accuracy: 76.52%\n",
      "Batch 201, Loss: 0.967200, Accuracy: 76.52%\n",
      "Batch 202, Loss: 0.972193, Accuracy: 76.53%\n",
      "Batch 203, Loss: 1.001815, Accuracy: 76.52%\n",
      "Batch 204, Loss: 0.909366, Accuracy: 76.56%\n",
      "Batch 205, Loss: 0.999072, Accuracy: 76.55%\n",
      "Batch 206, Loss: 0.914945, Accuracy: 76.59%\n",
      "Batch 207, Loss: 0.945382, Accuracy: 76.59%\n",
      "Batch 208, Loss: 0.917698, Accuracy: 76.63%\n",
      "Batch 209, Loss: 0.959978, Accuracy: 76.64%\n",
      "Batch 210, Loss: 0.986552, Accuracy: 76.64%\n",
      "Batch 211, Loss: 0.931155, Accuracy: 76.67%\n",
      "Batch 212, Loss: 0.908266, Accuracy: 76.70%\n",
      "Batch 213, Loss: 0.966751, Accuracy: 76.69%\n",
      "Training - Epoch 32, Loss: 0.977385, Accuracy: 76.69%\n",
      "Validation Batch 1, Loss: 0.947727, Accuracy: 82.81%\n",
      "Validation Batch 2, Loss: 1.011605, Accuracy: 78.91%\n",
      "Validation Batch 3, Loss: 1.060878, Accuracy: 74.48%\n",
      "Validation Batch 4, Loss: 0.975118, Accuracy: 75.39%\n",
      "Validation Batch 5, Loss: 0.964755, Accuracy: 77.19%\n",
      "Validation Batch 6, Loss: 0.908762, Accuracy: 78.39%\n",
      "Validation Batch 7, Loss: 1.002448, Accuracy: 77.68%\n",
      "Validation Batch 8, Loss: 1.028886, Accuracy: 76.95%\n",
      "Validation Batch 9, Loss: 1.020781, Accuracy: 76.39%\n",
      "Validation Batch 10, Loss: 0.978249, Accuracy: 76.41%\n",
      "Validation Batch 11, Loss: 0.966298, Accuracy: 76.70%\n",
      "Validation Batch 12, Loss: 0.957468, Accuracy: 76.95%\n",
      "Validation Batch 13, Loss: 1.040579, Accuracy: 76.44%\n",
      "Validation Batch 14, Loss: 1.008972, Accuracy: 76.12%\n",
      "Validation Batch 15, Loss: 0.985092, Accuracy: 76.25%\n",
      "Validation Batch 16, Loss: 0.970828, Accuracy: 76.46%\n",
      "Validation Batch 17, Loss: 1.059052, Accuracy: 75.92%\n",
      "Validation Batch 18, Loss: 0.972531, Accuracy: 76.04%\n",
      "Validation Batch 19, Loss: 1.020082, Accuracy: 75.99%\n",
      "Validation Batch 20, Loss: 1.023623, Accuracy: 75.78%\n",
      "Validation Batch 21, Loss: 0.991939, Accuracy: 75.67%\n",
      "Validation Batch 22, Loss: 1.037245, Accuracy: 75.43%\n",
      "Validation Batch 23, Loss: 1.073184, Accuracy: 75.14%\n",
      "Validation Batch 24, Loss: 1.004135, Accuracy: 75.13%\n",
      "Validation Batch 25, Loss: 0.993154, Accuracy: 75.12%\n",
      "Validation Batch 26, Loss: 1.011385, Accuracy: 75.06%\n",
      "Validation Batch 27, Loss: 0.953298, Accuracy: 75.16%\n",
      "Validation - Epoch 32, Loss: 0.998818, Accuracy: 75.16%\n",
      "Patience—0\n",
      "Epoch 33\n",
      "Batch 1, Loss: 0.956334, Accuracy: 78.12%\n",
      "Batch 2, Loss: 0.951729, Accuracy: 79.69%\n",
      "Batch 3, Loss: 1.005102, Accuracy: 77.60%\n",
      "Batch 4, Loss: 0.936161, Accuracy: 78.12%\n",
      "Batch 5, Loss: 0.973119, Accuracy: 78.12%\n",
      "Batch 6, Loss: 0.931507, Accuracy: 78.65%\n",
      "Batch 7, Loss: 0.937202, Accuracy: 78.79%\n",
      "Batch 8, Loss: 1.028144, Accuracy: 78.12%\n",
      "Batch 9, Loss: 0.970415, Accuracy: 78.12%\n",
      "Batch 10, Loss: 1.041867, Accuracy: 76.88%\n",
      "Batch 11, Loss: 0.868219, Accuracy: 77.84%\n",
      "Batch 12, Loss: 1.105322, Accuracy: 76.56%\n",
      "Batch 13, Loss: 1.003601, Accuracy: 76.44%\n",
      "Batch 14, Loss: 0.960321, Accuracy: 76.67%\n",
      "Batch 15, Loss: 0.951099, Accuracy: 76.88%\n",
      "Batch 16, Loss: 0.907090, Accuracy: 77.44%\n",
      "Batch 17, Loss: 0.985159, Accuracy: 77.39%\n",
      "Batch 18, Loss: 0.928791, Accuracy: 77.60%\n",
      "Batch 19, Loss: 0.878353, Accuracy: 78.12%\n",
      "Batch 20, Loss: 0.981020, Accuracy: 78.12%\n",
      "Batch 21, Loss: 1.034628, Accuracy: 77.75%\n",
      "Batch 22, Loss: 0.938443, Accuracy: 77.98%\n",
      "Batch 23, Loss: 0.951939, Accuracy: 77.99%\n",
      "Batch 24, Loss: 1.014028, Accuracy: 77.73%\n",
      "Batch 25, Loss: 0.991254, Accuracy: 77.62%\n",
      "Batch 26, Loss: 1.054646, Accuracy: 77.28%\n",
      "Batch 27, Loss: 1.001334, Accuracy: 77.20%\n",
      "Batch 28, Loss: 1.015086, Accuracy: 77.06%\n",
      "Batch 29, Loss: 0.987336, Accuracy: 77.05%\n",
      "Batch 30, Loss: 0.966607, Accuracy: 77.08%\n",
      "Batch 31, Loss: 1.009938, Accuracy: 77.02%\n",
      "Batch 32, Loss: 0.937392, Accuracy: 77.20%\n",
      "Batch 33, Loss: 1.000763, Accuracy: 77.08%\n",
      "Batch 34, Loss: 1.038443, Accuracy: 76.88%\n",
      "Batch 35, Loss: 0.939695, Accuracy: 77.01%\n",
      "Batch 36, Loss: 0.954880, Accuracy: 77.04%\n",
      "Batch 37, Loss: 1.076832, Accuracy: 76.73%\n",
      "Batch 38, Loss: 0.926857, Accuracy: 76.89%\n",
      "Batch 39, Loss: 1.042881, Accuracy: 76.72%\n",
      "Batch 40, Loss: 0.990232, Accuracy: 76.68%\n",
      "Batch 41, Loss: 0.994840, Accuracy: 76.68%\n",
      "Batch 42, Loss: 0.969353, Accuracy: 76.71%\n",
      "Batch 43, Loss: 1.064164, Accuracy: 76.49%\n",
      "Batch 44, Loss: 0.975287, Accuracy: 76.49%\n",
      "Batch 45, Loss: 0.992189, Accuracy: 76.46%\n",
      "Batch 46, Loss: 0.944203, Accuracy: 76.49%\n",
      "Batch 47, Loss: 0.967089, Accuracy: 76.53%\n",
      "Batch 48, Loss: 0.965395, Accuracy: 76.53%\n",
      "Batch 49, Loss: 0.966748, Accuracy: 76.53%\n",
      "Batch 50, Loss: 1.061309, Accuracy: 76.34%\n",
      "Batch 51, Loss: 1.016847, Accuracy: 76.26%\n",
      "Batch 52, Loss: 1.018155, Accuracy: 76.20%\n",
      "Batch 53, Loss: 0.970087, Accuracy: 76.18%\n",
      "Batch 54, Loss: 1.030913, Accuracy: 76.10%\n",
      "Batch 55, Loss: 1.022740, Accuracy: 76.02%\n",
      "Batch 56, Loss: 1.027558, Accuracy: 75.92%\n",
      "Batch 57, Loss: 0.969373, Accuracy: 75.93%\n",
      "Batch 58, Loss: 0.969220, Accuracy: 75.94%\n",
      "Batch 59, Loss: 0.948306, Accuracy: 76.01%\n",
      "Batch 60, Loss: 0.974266, Accuracy: 76.04%\n",
      "Batch 61, Loss: 0.959467, Accuracy: 76.08%\n",
      "Batch 62, Loss: 1.013698, Accuracy: 75.98%\n",
      "Batch 63, Loss: 1.003768, Accuracy: 75.94%\n",
      "Batch 64, Loss: 1.031242, Accuracy: 75.85%\n",
      "Batch 65, Loss: 0.950999, Accuracy: 75.91%\n",
      "Batch 66, Loss: 0.916135, Accuracy: 76.04%\n",
      "Batch 67, Loss: 1.060258, Accuracy: 75.91%\n",
      "Batch 68, Loss: 0.980530, Accuracy: 75.87%\n",
      "Batch 69, Loss: 1.024634, Accuracy: 75.79%\n",
      "Batch 70, Loss: 0.973324, Accuracy: 75.78%\n",
      "Batch 71, Loss: 1.034119, Accuracy: 75.70%\n",
      "Batch 72, Loss: 0.900552, Accuracy: 75.87%\n",
      "Batch 73, Loss: 1.005154, Accuracy: 75.86%\n",
      "Batch 74, Loss: 0.983666, Accuracy: 75.89%\n",
      "Batch 75, Loss: 0.945661, Accuracy: 75.94%\n",
      "Batch 76, Loss: 0.967637, Accuracy: 75.95%\n",
      "Batch 77, Loss: 0.963953, Accuracy: 75.97%\n",
      "Batch 78, Loss: 0.940205, Accuracy: 76.02%\n",
      "Batch 79, Loss: 0.987132, Accuracy: 75.99%\n",
      "Batch 80, Loss: 0.955255, Accuracy: 76.04%\n",
      "Batch 81, Loss: 1.048099, Accuracy: 75.95%\n",
      "Batch 82, Loss: 0.995386, Accuracy: 75.93%\n",
      "Batch 83, Loss: 1.033943, Accuracy: 75.85%\n",
      "Batch 84, Loss: 1.026585, Accuracy: 75.80%\n",
      "Batch 85, Loss: 0.979986, Accuracy: 75.81%\n",
      "Batch 86, Loss: 1.014671, Accuracy: 75.78%\n",
      "Batch 87, Loss: 0.860115, Accuracy: 75.93%\n",
      "Batch 88, Loss: 0.906096, Accuracy: 76.03%\n",
      "Batch 89, Loss: 0.987006, Accuracy: 76.05%\n",
      "Batch 90, Loss: 0.980521, Accuracy: 76.06%\n",
      "Batch 91, Loss: 0.970804, Accuracy: 76.08%\n",
      "Batch 92, Loss: 1.007322, Accuracy: 76.05%\n",
      "Batch 93, Loss: 0.972729, Accuracy: 76.06%\n",
      "Batch 94, Loss: 0.961671, Accuracy: 76.08%\n",
      "Batch 95, Loss: 0.983146, Accuracy: 76.09%\n",
      "Batch 96, Loss: 1.049434, Accuracy: 76.03%\n",
      "Batch 97, Loss: 0.931720, Accuracy: 76.08%\n",
      "Batch 98, Loss: 0.938770, Accuracy: 76.15%\n",
      "Batch 99, Loss: 1.036051, Accuracy: 76.06%\n",
      "Batch 100, Loss: 1.029477, Accuracy: 76.00%\n",
      "Batch 101, Loss: 0.960634, Accuracy: 76.02%\n",
      "Batch 102, Loss: 0.983815, Accuracy: 76.01%\n",
      "Batch 103, Loss: 0.981405, Accuracy: 76.00%\n",
      "Batch 104, Loss: 0.889284, Accuracy: 76.11%\n",
      "Batch 105, Loss: 0.985789, Accuracy: 76.10%\n",
      "Batch 106, Loss: 1.016749, Accuracy: 76.08%\n",
      "Batch 107, Loss: 1.015315, Accuracy: 76.07%\n",
      "Batch 108, Loss: 0.985061, Accuracy: 76.06%\n",
      "Batch 109, Loss: 0.973651, Accuracy: 76.08%\n",
      "Batch 110, Loss: 1.079563, Accuracy: 75.97%\n",
      "Batch 111, Loss: 0.944601, Accuracy: 76.01%\n",
      "Batch 112, Loss: 0.874497, Accuracy: 76.12%\n",
      "Batch 113, Loss: 0.961270, Accuracy: 76.13%\n",
      "Batch 114, Loss: 0.940800, Accuracy: 76.17%\n",
      "Batch 115, Loss: 0.940559, Accuracy: 76.21%\n",
      "Batch 116, Loss: 1.017347, Accuracy: 76.19%\n",
      "Batch 117, Loss: 1.042207, Accuracy: 76.12%\n",
      "Batch 118, Loss: 0.998146, Accuracy: 76.11%\n",
      "Batch 119, Loss: 0.873584, Accuracy: 76.21%\n",
      "Batch 120, Loss: 0.980312, Accuracy: 76.22%\n",
      "Batch 121, Loss: 0.970017, Accuracy: 76.23%\n",
      "Batch 122, Loss: 0.905876, Accuracy: 76.28%\n",
      "Batch 123, Loss: 0.922260, Accuracy: 76.35%\n",
      "Batch 124, Loss: 1.039950, Accuracy: 76.30%\n",
      "Batch 125, Loss: 1.068873, Accuracy: 76.22%\n",
      "Batch 126, Loss: 0.999152, Accuracy: 76.20%\n",
      "Batch 127, Loss: 0.982207, Accuracy: 76.21%\n",
      "Batch 128, Loss: 1.032951, Accuracy: 76.17%\n",
      "Batch 129, Loss: 0.911454, Accuracy: 76.22%\n",
      "Batch 130, Loss: 1.033857, Accuracy: 76.19%\n",
      "Batch 131, Loss: 0.935753, Accuracy: 76.23%\n",
      "Batch 132, Loss: 1.010941, Accuracy: 76.20%\n",
      "Batch 133, Loss: 1.005916, Accuracy: 76.17%\n",
      "Batch 134, Loss: 0.972848, Accuracy: 76.18%\n",
      "Batch 135, Loss: 0.951822, Accuracy: 76.20%\n",
      "Batch 136, Loss: 0.987378, Accuracy: 76.19%\n",
      "Batch 137, Loss: 0.973052, Accuracy: 76.20%\n",
      "Batch 138, Loss: 1.005415, Accuracy: 76.18%\n",
      "Batch 139, Loss: 1.020675, Accuracy: 76.15%\n",
      "Batch 140, Loss: 0.959703, Accuracy: 76.17%\n",
      "Batch 141, Loss: 1.004510, Accuracy: 76.16%\n",
      "Batch 142, Loss: 1.031284, Accuracy: 76.13%\n",
      "Batch 143, Loss: 1.035384, Accuracy: 76.09%\n",
      "Batch 144, Loss: 0.958383, Accuracy: 76.11%\n",
      "Batch 145, Loss: 1.053722, Accuracy: 76.08%\n",
      "Batch 146, Loss: 1.018093, Accuracy: 76.04%\n",
      "Batch 147, Loss: 0.975568, Accuracy: 76.05%\n",
      "Batch 148, Loss: 0.968096, Accuracy: 76.07%\n",
      "Batch 149, Loss: 1.005828, Accuracy: 76.04%\n",
      "Batch 150, Loss: 1.000769, Accuracy: 76.04%\n",
      "Batch 151, Loss: 0.993137, Accuracy: 76.05%\n",
      "Batch 152, Loss: 1.011883, Accuracy: 76.02%\n",
      "Batch 153, Loss: 0.993787, Accuracy: 76.01%\n",
      "Batch 154, Loss: 1.038875, Accuracy: 75.96%\n",
      "Batch 155, Loss: 0.939442, Accuracy: 75.98%\n",
      "Batch 156, Loss: 0.999889, Accuracy: 75.96%\n",
      "Batch 157, Loss: 0.940548, Accuracy: 76.01%\n",
      "Batch 158, Loss: 1.065845, Accuracy: 75.93%\n",
      "Batch 159, Loss: 1.016170, Accuracy: 75.90%\n",
      "Batch 160, Loss: 1.019690, Accuracy: 75.89%\n",
      "Batch 161, Loss: 0.919267, Accuracy: 75.93%\n",
      "Batch 162, Loss: 0.940262, Accuracy: 75.95%\n",
      "Batch 163, Loss: 0.942626, Accuracy: 75.99%\n",
      "Batch 164, Loss: 1.042261, Accuracy: 75.94%\n",
      "Batch 165, Loss: 0.979666, Accuracy: 75.95%\n",
      "Batch 166, Loss: 0.955416, Accuracy: 75.97%\n",
      "Batch 167, Loss: 1.032034, Accuracy: 75.95%\n",
      "Batch 168, Loss: 0.906695, Accuracy: 76.00%\n",
      "Batch 169, Loss: 0.942524, Accuracy: 76.04%\n",
      "Batch 170, Loss: 0.906527, Accuracy: 76.08%\n",
      "Batch 171, Loss: 0.953773, Accuracy: 76.11%\n",
      "Batch 172, Loss: 0.916081, Accuracy: 76.15%\n",
      "Batch 173, Loss: 0.957515, Accuracy: 76.17%\n",
      "Batch 174, Loss: 0.907552, Accuracy: 76.21%\n",
      "Batch 175, Loss: 0.960538, Accuracy: 76.22%\n",
      "Batch 176, Loss: 0.931114, Accuracy: 76.24%\n",
      "Batch 177, Loss: 1.002260, Accuracy: 76.24%\n",
      "Batch 178, Loss: 0.955095, Accuracy: 76.25%\n",
      "Batch 179, Loss: 0.962237, Accuracy: 76.27%\n",
      "Batch 180, Loss: 1.015567, Accuracy: 76.24%\n",
      "Batch 181, Loss: 0.971403, Accuracy: 76.24%\n",
      "Batch 182, Loss: 0.982638, Accuracy: 76.24%\n",
      "Batch 183, Loss: 0.963852, Accuracy: 76.25%\n",
      "Batch 184, Loss: 1.014199, Accuracy: 76.22%\n",
      "Batch 185, Loss: 0.923462, Accuracy: 76.26%\n",
      "Batch 186, Loss: 1.009318, Accuracy: 76.23%\n",
      "Batch 187, Loss: 1.079967, Accuracy: 76.18%\n",
      "Batch 188, Loss: 0.964668, Accuracy: 76.19%\n",
      "Batch 189, Loss: 0.986793, Accuracy: 76.17%\n",
      "Batch 190, Loss: 1.014342, Accuracy: 76.17%\n",
      "Batch 191, Loss: 0.974933, Accuracy: 76.18%\n",
      "Batch 192, Loss: 0.942604, Accuracy: 76.20%\n",
      "Batch 193, Loss: 1.109992, Accuracy: 76.13%\n",
      "Batch 194, Loss: 0.989470, Accuracy: 76.14%\n",
      "Batch 195, Loss: 0.951440, Accuracy: 76.15%\n",
      "Batch 196, Loss: 0.911749, Accuracy: 76.18%\n",
      "Batch 197, Loss: 0.952660, Accuracy: 76.20%\n",
      "Batch 198, Loss: 0.968693, Accuracy: 76.21%\n",
      "Batch 199, Loss: 0.992483, Accuracy: 76.20%\n",
      "Batch 200, Loss: 1.048472, Accuracy: 76.16%\n",
      "Batch 201, Loss: 0.892014, Accuracy: 76.21%\n",
      "Batch 202, Loss: 1.006781, Accuracy: 76.20%\n",
      "Batch 203, Loss: 0.994223, Accuracy: 76.19%\n",
      "Batch 204, Loss: 0.941032, Accuracy: 76.20%\n",
      "Batch 205, Loss: 1.034037, Accuracy: 76.18%\n",
      "Batch 206, Loss: 1.046823, Accuracy: 76.15%\n",
      "Batch 207, Loss: 0.974961, Accuracy: 76.15%\n",
      "Batch 208, Loss: 0.968559, Accuracy: 76.15%\n",
      "Batch 209, Loss: 0.924810, Accuracy: 76.19%\n",
      "Batch 210, Loss: 0.998406, Accuracy: 76.17%\n",
      "Batch 211, Loss: 0.968740, Accuracy: 76.17%\n",
      "Batch 212, Loss: 0.886713, Accuracy: 76.23%\n",
      "Batch 213, Loss: 0.912177, Accuracy: 76.27%\n",
      "Training - Epoch 33, Loss: 0.981079, Accuracy: 76.27%\n",
      "Validation Batch 1, Loss: 0.945349, Accuracy: 82.81%\n",
      "Validation Batch 2, Loss: 1.012056, Accuracy: 78.12%\n",
      "Validation Batch 3, Loss: 1.062496, Accuracy: 74.48%\n",
      "Validation Batch 4, Loss: 0.972632, Accuracy: 75.00%\n",
      "Validation Batch 5, Loss: 0.960756, Accuracy: 76.25%\n",
      "Validation Batch 6, Loss: 0.905146, Accuracy: 77.86%\n",
      "Validation Batch 7, Loss: 0.997284, Accuracy: 77.46%\n",
      "Validation Batch 8, Loss: 1.027200, Accuracy: 76.76%\n",
      "Validation Batch 9, Loss: 1.017201, Accuracy: 76.04%\n",
      "Validation Batch 10, Loss: 0.975212, Accuracy: 76.25%\n",
      "Validation Batch 11, Loss: 0.961257, Accuracy: 76.42%\n",
      "Validation Batch 12, Loss: 0.955810, Accuracy: 76.69%\n",
      "Validation Batch 13, Loss: 1.037627, Accuracy: 76.20%\n",
      "Validation Batch 14, Loss: 1.003628, Accuracy: 76.00%\n",
      "Validation Batch 15, Loss: 0.981695, Accuracy: 76.15%\n",
      "Validation Batch 16, Loss: 0.971928, Accuracy: 76.37%\n",
      "Validation Batch 17, Loss: 1.060292, Accuracy: 75.74%\n",
      "Validation Batch 18, Loss: 0.969530, Accuracy: 75.87%\n",
      "Validation Batch 19, Loss: 1.022455, Accuracy: 75.82%\n",
      "Validation Batch 20, Loss: 1.019589, Accuracy: 75.62%\n",
      "Validation Batch 21, Loss: 0.992687, Accuracy: 75.52%\n",
      "Validation Batch 22, Loss: 1.036001, Accuracy: 75.28%\n",
      "Validation Batch 23, Loss: 1.072603, Accuracy: 75.00%\n",
      "Validation Batch 24, Loss: 1.002563, Accuracy: 75.00%\n",
      "Validation Batch 25, Loss: 0.996503, Accuracy: 75.06%\n",
      "Validation Batch 26, Loss: 1.003068, Accuracy: 75.06%\n",
      "Validation Batch 27, Loss: 0.958806, Accuracy: 75.16%\n",
      "Validation - Epoch 33, Loss: 0.997088, Accuracy: 75.16%\n",
      "Patience—0\n",
      "Epoch 34\n",
      "Batch 1, Loss: 1.028516, Accuracy: 73.44%\n",
      "Batch 2, Loss: 0.998835, Accuracy: 73.44%\n",
      "Batch 3, Loss: 1.021615, Accuracy: 72.40%\n",
      "Batch 4, Loss: 0.995575, Accuracy: 72.66%\n",
      "Batch 5, Loss: 0.945991, Accuracy: 74.06%\n",
      "Batch 6, Loss: 0.914552, Accuracy: 75.52%\n",
      "Batch 7, Loss: 0.957536, Accuracy: 75.89%\n",
      "Batch 8, Loss: 0.972616, Accuracy: 75.98%\n",
      "Batch 9, Loss: 0.975935, Accuracy: 76.22%\n",
      "Batch 10, Loss: 1.058294, Accuracy: 75.47%\n",
      "Batch 11, Loss: 0.962680, Accuracy: 75.57%\n",
      "Batch 12, Loss: 0.986426, Accuracy: 75.52%\n",
      "Batch 13, Loss: 0.890595, Accuracy: 76.32%\n",
      "Batch 14, Loss: 1.019441, Accuracy: 76.00%\n",
      "Batch 15, Loss: 1.104086, Accuracy: 75.00%\n",
      "Batch 16, Loss: 1.048077, Accuracy: 74.71%\n",
      "Batch 17, Loss: 0.986679, Accuracy: 74.72%\n",
      "Batch 18, Loss: 1.034688, Accuracy: 74.48%\n",
      "Batch 19, Loss: 0.954827, Accuracy: 74.75%\n",
      "Batch 20, Loss: 0.987548, Accuracy: 74.84%\n",
      "Batch 21, Loss: 0.955643, Accuracy: 75.00%\n",
      "Batch 22, Loss: 1.007185, Accuracy: 74.93%\n",
      "Batch 23, Loss: 1.041720, Accuracy: 74.66%\n",
      "Batch 24, Loss: 0.946542, Accuracy: 74.87%\n",
      "Batch 25, Loss: 0.914879, Accuracy: 75.19%\n",
      "Batch 26, Loss: 0.941690, Accuracy: 75.42%\n",
      "Batch 27, Loss: 0.976086, Accuracy: 75.52%\n",
      "Batch 28, Loss: 1.001564, Accuracy: 75.39%\n",
      "Batch 29, Loss: 0.948772, Accuracy: 75.54%\n",
      "Batch 30, Loss: 0.999265, Accuracy: 75.57%\n",
      "Batch 31, Loss: 0.953663, Accuracy: 75.71%\n",
      "Batch 32, Loss: 0.926129, Accuracy: 75.93%\n",
      "Batch 33, Loss: 0.987576, Accuracy: 75.85%\n",
      "Batch 34, Loss: 0.959306, Accuracy: 75.87%\n",
      "Batch 35, Loss: 0.999303, Accuracy: 75.80%\n",
      "Batch 36, Loss: 1.046525, Accuracy: 75.65%\n",
      "Batch 37, Loss: 0.978210, Accuracy: 75.68%\n",
      "Batch 38, Loss: 0.901802, Accuracy: 75.90%\n",
      "Batch 39, Loss: 0.947622, Accuracy: 76.00%\n",
      "Batch 40, Loss: 0.949240, Accuracy: 76.09%\n",
      "Batch 41, Loss: 1.025193, Accuracy: 75.95%\n",
      "Batch 42, Loss: 0.940944, Accuracy: 76.12%\n",
      "Batch 43, Loss: 0.934640, Accuracy: 76.20%\n",
      "Batch 44, Loss: 0.988911, Accuracy: 76.17%\n",
      "Batch 45, Loss: 0.965175, Accuracy: 76.22%\n",
      "Batch 46, Loss: 1.003876, Accuracy: 76.12%\n",
      "Batch 47, Loss: 0.911997, Accuracy: 76.26%\n",
      "Batch 48, Loss: 0.945881, Accuracy: 76.30%\n",
      "Batch 49, Loss: 0.928070, Accuracy: 76.43%\n",
      "Batch 50, Loss: 1.048526, Accuracy: 76.34%\n",
      "Batch 51, Loss: 0.922270, Accuracy: 76.44%\n",
      "Batch 52, Loss: 1.003042, Accuracy: 76.35%\n",
      "Batch 53, Loss: 0.911372, Accuracy: 76.50%\n",
      "Batch 54, Loss: 1.043744, Accuracy: 76.33%\n",
      "Batch 55, Loss: 0.958442, Accuracy: 76.42%\n",
      "Batch 56, Loss: 1.073279, Accuracy: 76.23%\n",
      "Batch 57, Loss: 0.989361, Accuracy: 76.23%\n",
      "Batch 58, Loss: 0.915901, Accuracy: 76.35%\n",
      "Batch 59, Loss: 0.941988, Accuracy: 76.43%\n",
      "Batch 60, Loss: 0.945538, Accuracy: 76.48%\n",
      "Batch 61, Loss: 1.036355, Accuracy: 76.38%\n",
      "Batch 62, Loss: 1.081142, Accuracy: 76.21%\n",
      "Batch 63, Loss: 0.980751, Accuracy: 76.22%\n",
      "Batch 64, Loss: 0.948816, Accuracy: 76.22%\n",
      "Batch 65, Loss: 0.966867, Accuracy: 76.25%\n",
      "Batch 66, Loss: 1.011362, Accuracy: 76.21%\n",
      "Batch 67, Loss: 0.934977, Accuracy: 76.28%\n",
      "Batch 68, Loss: 1.007047, Accuracy: 76.24%\n",
      "Batch 69, Loss: 1.033956, Accuracy: 76.15%\n",
      "Batch 70, Loss: 0.946635, Accuracy: 76.18%\n",
      "Batch 71, Loss: 1.053607, Accuracy: 76.08%\n",
      "Batch 72, Loss: 1.102274, Accuracy: 75.87%\n",
      "Batch 73, Loss: 0.947517, Accuracy: 75.92%\n",
      "Batch 74, Loss: 0.965489, Accuracy: 75.93%\n",
      "Batch 75, Loss: 0.879853, Accuracy: 76.08%\n",
      "Batch 76, Loss: 0.912824, Accuracy: 76.15%\n",
      "Batch 77, Loss: 1.030592, Accuracy: 76.06%\n",
      "Batch 78, Loss: 0.939079, Accuracy: 76.12%\n",
      "Batch 79, Loss: 0.931975, Accuracy: 76.19%\n",
      "Batch 80, Loss: 0.963029, Accuracy: 76.19%\n",
      "Batch 81, Loss: 0.905511, Accuracy: 76.31%\n",
      "Batch 82, Loss: 1.024244, Accuracy: 76.26%\n",
      "Batch 83, Loss: 1.009321, Accuracy: 76.20%\n",
      "Batch 84, Loss: 0.971707, Accuracy: 76.19%\n",
      "Batch 85, Loss: 1.056873, Accuracy: 76.08%\n",
      "Batch 86, Loss: 1.023447, Accuracy: 76.05%\n",
      "Batch 87, Loss: 0.962982, Accuracy: 76.10%\n",
      "Batch 88, Loss: 1.030966, Accuracy: 76.01%\n",
      "Batch 89, Loss: 1.016406, Accuracy: 75.97%\n",
      "Batch 90, Loss: 0.959058, Accuracy: 76.01%\n",
      "Batch 91, Loss: 0.903803, Accuracy: 76.10%\n",
      "Batch 92, Loss: 0.906125, Accuracy: 76.19%\n",
      "Batch 93, Loss: 0.965293, Accuracy: 76.21%\n",
      "Batch 94, Loss: 0.965269, Accuracy: 76.21%\n",
      "Batch 95, Loss: 0.919150, Accuracy: 76.28%\n",
      "Batch 96, Loss: 0.953042, Accuracy: 76.32%\n",
      "Batch 97, Loss: 0.993843, Accuracy: 76.30%\n",
      "Batch 98, Loss: 0.927933, Accuracy: 76.36%\n",
      "Batch 99, Loss: 0.964248, Accuracy: 76.39%\n",
      "Batch 100, Loss: 1.023992, Accuracy: 76.34%\n",
      "Batch 101, Loss: 0.938674, Accuracy: 76.39%\n",
      "Batch 102, Loss: 0.938430, Accuracy: 76.44%\n",
      "Batch 103, Loss: 0.946549, Accuracy: 76.49%\n",
      "Batch 104, Loss: 0.924530, Accuracy: 76.53%\n",
      "Batch 105, Loss: 0.954961, Accuracy: 76.58%\n",
      "Batch 106, Loss: 0.944366, Accuracy: 76.61%\n",
      "Batch 107, Loss: 1.059268, Accuracy: 76.52%\n",
      "Batch 108, Loss: 0.922166, Accuracy: 76.58%\n",
      "Batch 109, Loss: 0.994217, Accuracy: 76.56%\n",
      "Batch 110, Loss: 0.862875, Accuracy: 76.66%\n",
      "Batch 111, Loss: 0.881685, Accuracy: 76.76%\n",
      "Batch 112, Loss: 0.992181, Accuracy: 76.74%\n",
      "Batch 113, Loss: 0.944416, Accuracy: 76.76%\n",
      "Batch 114, Loss: 0.932442, Accuracy: 76.77%\n",
      "Batch 115, Loss: 0.995290, Accuracy: 76.74%\n",
      "Batch 116, Loss: 1.040605, Accuracy: 76.70%\n",
      "Batch 117, Loss: 0.985285, Accuracy: 76.68%\n",
      "Batch 118, Loss: 0.959628, Accuracy: 76.69%\n",
      "Batch 119, Loss: 1.016687, Accuracy: 76.64%\n",
      "Batch 120, Loss: 0.958296, Accuracy: 76.64%\n",
      "Batch 121, Loss: 0.896269, Accuracy: 76.70%\n",
      "Batch 122, Loss: 0.982564, Accuracy: 76.69%\n",
      "Batch 123, Loss: 0.921788, Accuracy: 76.73%\n",
      "Batch 124, Loss: 0.990956, Accuracy: 76.71%\n",
      "Batch 125, Loss: 1.039689, Accuracy: 76.65%\n",
      "Batch 126, Loss: 1.008476, Accuracy: 76.62%\n",
      "Batch 127, Loss: 0.873249, Accuracy: 76.71%\n",
      "Batch 128, Loss: 0.887278, Accuracy: 76.78%\n",
      "Batch 129, Loss: 0.985815, Accuracy: 76.78%\n",
      "Batch 130, Loss: 1.016167, Accuracy: 76.73%\n",
      "Batch 131, Loss: 0.960189, Accuracy: 76.75%\n",
      "Batch 132, Loss: 0.947452, Accuracy: 76.79%\n",
      "Batch 133, Loss: 1.051301, Accuracy: 76.74%\n",
      "Batch 134, Loss: 1.009240, Accuracy: 76.73%\n",
      "Batch 135, Loss: 1.070824, Accuracy: 76.64%\n",
      "Batch 136, Loss: 0.999542, Accuracy: 76.63%\n",
      "Batch 137, Loss: 1.048410, Accuracy: 76.56%\n",
      "Batch 138, Loss: 0.960801, Accuracy: 76.57%\n",
      "Batch 139, Loss: 0.946683, Accuracy: 76.58%\n",
      "Batch 140, Loss: 0.981855, Accuracy: 76.57%\n",
      "Batch 141, Loss: 0.985873, Accuracy: 76.56%\n",
      "Batch 142, Loss: 0.919939, Accuracy: 76.61%\n",
      "Batch 143, Loss: 1.079815, Accuracy: 76.52%\n",
      "Batch 144, Loss: 0.991240, Accuracy: 76.50%\n",
      "Batch 145, Loss: 0.973880, Accuracy: 76.50%\n",
      "Batch 146, Loss: 1.000809, Accuracy: 76.49%\n",
      "Batch 147, Loss: 0.977287, Accuracy: 76.50%\n",
      "Batch 148, Loss: 0.969490, Accuracy: 76.50%\n",
      "Batch 149, Loss: 1.006225, Accuracy: 76.48%\n",
      "Batch 150, Loss: 1.052276, Accuracy: 76.43%\n",
      "Batch 151, Loss: 0.901832, Accuracy: 76.49%\n",
      "Batch 152, Loss: 0.971856, Accuracy: 76.49%\n",
      "Batch 153, Loss: 0.895783, Accuracy: 76.55%\n",
      "Batch 154, Loss: 1.029349, Accuracy: 76.51%\n",
      "Batch 155, Loss: 0.960447, Accuracy: 76.52%\n",
      "Batch 156, Loss: 0.975977, Accuracy: 76.51%\n",
      "Batch 157, Loss: 0.960820, Accuracy: 76.51%\n",
      "Batch 158, Loss: 0.995074, Accuracy: 76.48%\n",
      "Batch 159, Loss: 1.082635, Accuracy: 76.42%\n",
      "Batch 160, Loss: 0.928713, Accuracy: 76.45%\n",
      "Batch 161, Loss: 1.007969, Accuracy: 76.43%\n",
      "Batch 162, Loss: 0.868357, Accuracy: 76.49%\n",
      "Batch 163, Loss: 1.045522, Accuracy: 76.45%\n",
      "Batch 164, Loss: 0.919427, Accuracy: 76.51%\n",
      "Batch 165, Loss: 0.984317, Accuracy: 76.50%\n",
      "Batch 166, Loss: 0.946476, Accuracy: 76.52%\n",
      "Batch 167, Loss: 1.049438, Accuracy: 76.48%\n",
      "Batch 168, Loss: 0.975118, Accuracy: 76.49%\n",
      "Batch 169, Loss: 0.901019, Accuracy: 76.54%\n",
      "Batch 170, Loss: 0.927925, Accuracy: 76.57%\n",
      "Batch 171, Loss: 1.034256, Accuracy: 76.53%\n",
      "Batch 172, Loss: 0.993257, Accuracy: 76.53%\n",
      "Batch 173, Loss: 0.945830, Accuracy: 76.54%\n",
      "Batch 174, Loss: 1.042464, Accuracy: 76.50%\n",
      "Batch 175, Loss: 0.950697, Accuracy: 76.49%\n",
      "Batch 176, Loss: 0.999818, Accuracy: 76.46%\n",
      "Batch 177, Loss: 1.017728, Accuracy: 76.45%\n",
      "Batch 178, Loss: 0.957291, Accuracy: 76.45%\n",
      "Batch 179, Loss: 0.887873, Accuracy: 76.51%\n",
      "Batch 180, Loss: 0.939985, Accuracy: 76.53%\n",
      "Batch 181, Loss: 0.929022, Accuracy: 76.55%\n",
      "Batch 182, Loss: 1.018817, Accuracy: 76.53%\n",
      "Batch 183, Loss: 0.982407, Accuracy: 76.54%\n",
      "Batch 184, Loss: 0.979262, Accuracy: 76.53%\n",
      "Batch 185, Loss: 1.022584, Accuracy: 76.52%\n",
      "Batch 186, Loss: 0.960679, Accuracy: 76.52%\n",
      "Batch 187, Loss: 0.982810, Accuracy: 76.52%\n",
      "Batch 188, Loss: 0.982740, Accuracy: 76.51%\n",
      "Batch 189, Loss: 1.022038, Accuracy: 76.48%\n",
      "Batch 190, Loss: 1.078881, Accuracy: 76.42%\n",
      "Batch 191, Loss: 0.966287, Accuracy: 76.44%\n",
      "Batch 192, Loss: 0.989372, Accuracy: 76.44%\n",
      "Batch 193, Loss: 1.009390, Accuracy: 76.42%\n",
      "Batch 194, Loss: 0.958968, Accuracy: 76.43%\n",
      "Batch 195, Loss: 0.931528, Accuracy: 76.45%\n",
      "Batch 196, Loss: 1.079434, Accuracy: 76.40%\n",
      "Batch 197, Loss: 0.963032, Accuracy: 76.40%\n",
      "Batch 198, Loss: 0.965841, Accuracy: 76.41%\n",
      "Batch 199, Loss: 0.967389, Accuracy: 76.42%\n",
      "Batch 200, Loss: 0.892526, Accuracy: 76.45%\n",
      "Batch 201, Loss: 0.979376, Accuracy: 76.46%\n",
      "Batch 202, Loss: 1.002529, Accuracy: 76.45%\n",
      "Batch 203, Loss: 0.902996, Accuracy: 76.49%\n",
      "Batch 204, Loss: 1.002187, Accuracy: 76.48%\n",
      "Batch 205, Loss: 0.824254, Accuracy: 76.55%\n",
      "Batch 206, Loss: 0.967889, Accuracy: 76.56%\n",
      "Batch 207, Loss: 0.949199, Accuracy: 76.58%\n",
      "Batch 208, Loss: 0.948806, Accuracy: 76.58%\n",
      "Batch 209, Loss: 0.879413, Accuracy: 76.64%\n",
      "Batch 210, Loss: 0.973271, Accuracy: 76.63%\n",
      "Batch 211, Loss: 1.073451, Accuracy: 76.58%\n",
      "Batch 212, Loss: 1.014114, Accuracy: 76.56%\n",
      "Batch 213, Loss: 0.967843, Accuracy: 76.56%\n",
      "Training - Epoch 34, Loss: 0.975475, Accuracy: 76.56%\n",
      "Validation Batch 1, Loss: 0.946372, Accuracy: 82.81%\n",
      "Validation Batch 2, Loss: 1.010973, Accuracy: 78.91%\n",
      "Validation Batch 3, Loss: 1.068967, Accuracy: 75.00%\n",
      "Validation Batch 4, Loss: 0.974008, Accuracy: 75.78%\n",
      "Validation Batch 5, Loss: 0.962688, Accuracy: 77.19%\n",
      "Validation Batch 6, Loss: 0.907963, Accuracy: 78.39%\n",
      "Validation Batch 7, Loss: 1.002877, Accuracy: 77.90%\n",
      "Validation Batch 8, Loss: 1.025778, Accuracy: 77.15%\n",
      "Validation Batch 9, Loss: 1.018124, Accuracy: 76.56%\n",
      "Validation Batch 10, Loss: 0.973828, Accuracy: 76.56%\n",
      "Validation Batch 11, Loss: 0.966512, Accuracy: 76.70%\n",
      "Validation Batch 12, Loss: 0.955277, Accuracy: 76.95%\n",
      "Validation Batch 13, Loss: 1.034936, Accuracy: 76.44%\n",
      "Validation Batch 14, Loss: 1.006315, Accuracy: 76.12%\n",
      "Validation Batch 15, Loss: 0.984301, Accuracy: 76.25%\n",
      "Validation Batch 16, Loss: 0.975074, Accuracy: 76.37%\n",
      "Validation Batch 17, Loss: 1.066707, Accuracy: 75.74%\n",
      "Validation Batch 18, Loss: 0.971942, Accuracy: 75.78%\n",
      "Validation Batch 19, Loss: 1.023450, Accuracy: 75.66%\n",
      "Validation Batch 20, Loss: 1.020888, Accuracy: 75.47%\n",
      "Validation Batch 21, Loss: 0.993010, Accuracy: 75.45%\n",
      "Validation Batch 22, Loss: 1.038758, Accuracy: 75.21%\n",
      "Validation Batch 23, Loss: 1.072282, Accuracy: 74.93%\n",
      "Validation Batch 24, Loss: 1.000094, Accuracy: 74.93%\n",
      "Validation Batch 25, Loss: 0.994077, Accuracy: 74.94%\n",
      "Validation Batch 26, Loss: 1.002784, Accuracy: 74.94%\n",
      "Validation Batch 27, Loss: 0.963049, Accuracy: 74.99%\n",
      "Validation - Epoch 34, Loss: 0.998557, Accuracy: 74.99%\n",
      "Patience—1\n",
      "Epoch 35\n",
      "Batch 1, Loss: 0.949652, Accuracy: 78.12%\n",
      "Batch 2, Loss: 0.971791, Accuracy: 77.34%\n",
      "Batch 3, Loss: 0.966062, Accuracy: 77.60%\n",
      "Batch 4, Loss: 0.955626, Accuracy: 78.52%\n",
      "Batch 5, Loss: 0.984496, Accuracy: 77.81%\n",
      "Batch 6, Loss: 1.052024, Accuracy: 75.78%\n",
      "Batch 7, Loss: 0.976781, Accuracy: 75.89%\n",
      "Batch 8, Loss: 1.026437, Accuracy: 75.39%\n",
      "Batch 9, Loss: 0.930384, Accuracy: 76.04%\n",
      "Batch 10, Loss: 0.912034, Accuracy: 76.72%\n",
      "Batch 11, Loss: 0.872485, Accuracy: 77.70%\n",
      "Batch 12, Loss: 0.941243, Accuracy: 77.73%\n",
      "Batch 13, Loss: 1.008193, Accuracy: 77.28%\n",
      "Batch 14, Loss: 0.934662, Accuracy: 77.57%\n",
      "Batch 15, Loss: 0.938317, Accuracy: 77.71%\n",
      "Batch 16, Loss: 1.019742, Accuracy: 77.34%\n",
      "Batch 17, Loss: 0.983045, Accuracy: 77.30%\n",
      "Batch 18, Loss: 0.920197, Accuracy: 77.60%\n",
      "Batch 19, Loss: 0.976469, Accuracy: 77.63%\n",
      "Batch 20, Loss: 0.963583, Accuracy: 77.58%\n",
      "Batch 21, Loss: 0.961631, Accuracy: 77.68%\n",
      "Batch 22, Loss: 0.921251, Accuracy: 77.84%\n",
      "Batch 23, Loss: 0.982303, Accuracy: 77.58%\n",
      "Batch 24, Loss: 0.972205, Accuracy: 77.54%\n",
      "Batch 25, Loss: 0.939916, Accuracy: 77.69%\n",
      "Batch 26, Loss: 1.022632, Accuracy: 77.46%\n",
      "Batch 27, Loss: 0.923442, Accuracy: 77.66%\n",
      "Batch 28, Loss: 0.929798, Accuracy: 77.79%\n",
      "Batch 29, Loss: 0.986119, Accuracy: 77.69%\n",
      "Batch 30, Loss: 0.901609, Accuracy: 77.97%\n",
      "Batch 31, Loss: 1.023316, Accuracy: 77.77%\n",
      "Batch 32, Loss: 0.948183, Accuracy: 77.88%\n",
      "Batch 33, Loss: 1.020660, Accuracy: 77.75%\n",
      "Batch 34, Loss: 0.932045, Accuracy: 77.85%\n",
      "Batch 35, Loss: 0.978018, Accuracy: 77.77%\n",
      "Batch 36, Loss: 0.936389, Accuracy: 77.82%\n",
      "Batch 37, Loss: 1.029739, Accuracy: 77.66%\n",
      "Batch 38, Loss: 0.981076, Accuracy: 77.63%\n",
      "Batch 39, Loss: 0.940095, Accuracy: 77.72%\n",
      "Batch 40, Loss: 0.974164, Accuracy: 77.66%\n",
      "Batch 41, Loss: 0.993579, Accuracy: 77.63%\n",
      "Batch 42, Loss: 0.907598, Accuracy: 77.79%\n",
      "Batch 43, Loss: 0.969576, Accuracy: 77.76%\n",
      "Batch 44, Loss: 0.970165, Accuracy: 77.73%\n",
      "Batch 45, Loss: 0.940051, Accuracy: 77.81%\n",
      "Batch 46, Loss: 0.955681, Accuracy: 77.82%\n",
      "Batch 47, Loss: 0.964190, Accuracy: 77.86%\n",
      "Batch 48, Loss: 1.010998, Accuracy: 77.77%\n",
      "Batch 49, Loss: 0.939440, Accuracy: 77.81%\n",
      "Batch 50, Loss: 1.035437, Accuracy: 77.66%\n",
      "Batch 51, Loss: 0.937921, Accuracy: 77.73%\n",
      "Batch 52, Loss: 0.928788, Accuracy: 77.82%\n",
      "Batch 53, Loss: 0.937008, Accuracy: 77.89%\n",
      "Batch 54, Loss: 0.957507, Accuracy: 77.92%\n",
      "Batch 55, Loss: 1.002330, Accuracy: 77.84%\n",
      "Batch 56, Loss: 0.955573, Accuracy: 77.85%\n",
      "Batch 57, Loss: 0.982445, Accuracy: 77.85%\n",
      "Batch 58, Loss: 0.930839, Accuracy: 77.94%\n",
      "Batch 59, Loss: 0.973963, Accuracy: 77.97%\n",
      "Batch 60, Loss: 0.955686, Accuracy: 77.97%\n",
      "Batch 61, Loss: 0.996021, Accuracy: 77.89%\n",
      "Batch 62, Loss: 0.981764, Accuracy: 77.85%\n",
      "Batch 63, Loss: 0.913715, Accuracy: 77.95%\n",
      "Batch 64, Loss: 0.937808, Accuracy: 78.03%\n",
      "Batch 65, Loss: 1.033558, Accuracy: 77.88%\n",
      "Batch 66, Loss: 1.008148, Accuracy: 77.79%\n",
      "Batch 67, Loss: 0.932812, Accuracy: 77.80%\n",
      "Batch 68, Loss: 0.955845, Accuracy: 77.85%\n",
      "Batch 69, Loss: 0.932770, Accuracy: 77.92%\n",
      "Batch 70, Loss: 1.067924, Accuracy: 77.77%\n",
      "Batch 71, Loss: 0.942642, Accuracy: 77.82%\n",
      "Batch 72, Loss: 1.051376, Accuracy: 77.69%\n",
      "Batch 73, Loss: 0.999983, Accuracy: 77.61%\n",
      "Batch 74, Loss: 0.917957, Accuracy: 77.70%\n",
      "Batch 75, Loss: 1.043188, Accuracy: 77.56%\n",
      "Batch 76, Loss: 1.032459, Accuracy: 77.49%\n",
      "Batch 77, Loss: 0.977153, Accuracy: 77.48%\n",
      "Batch 78, Loss: 1.006487, Accuracy: 77.44%\n",
      "Batch 79, Loss: 0.948737, Accuracy: 77.45%\n",
      "Batch 80, Loss: 0.948605, Accuracy: 77.46%\n",
      "Batch 81, Loss: 0.879955, Accuracy: 77.58%\n",
      "Batch 82, Loss: 0.964239, Accuracy: 77.57%\n",
      "Batch 83, Loss: 0.968237, Accuracy: 77.58%\n",
      "Batch 84, Loss: 0.913910, Accuracy: 77.64%\n",
      "Batch 85, Loss: 1.018443, Accuracy: 77.56%\n",
      "Batch 86, Loss: 0.942720, Accuracy: 77.60%\n",
      "Batch 87, Loss: 0.976537, Accuracy: 77.55%\n",
      "Batch 88, Loss: 0.963218, Accuracy: 77.57%\n",
      "Batch 89, Loss: 0.890336, Accuracy: 77.69%\n",
      "Batch 90, Loss: 0.978610, Accuracy: 77.66%\n",
      "Batch 91, Loss: 0.916241, Accuracy: 77.73%\n",
      "Batch 92, Loss: 0.908805, Accuracy: 77.80%\n",
      "Batch 93, Loss: 1.002589, Accuracy: 77.76%\n",
      "Batch 94, Loss: 1.019991, Accuracy: 77.73%\n",
      "Batch 95, Loss: 0.918909, Accuracy: 77.78%\n",
      "Batch 96, Loss: 0.997166, Accuracy: 77.75%\n",
      "Batch 97, Loss: 0.935667, Accuracy: 77.79%\n",
      "Batch 98, Loss: 0.963229, Accuracy: 77.79%\n",
      "Batch 99, Loss: 0.930021, Accuracy: 77.83%\n",
      "Batch 100, Loss: 1.002813, Accuracy: 77.77%\n",
      "Batch 101, Loss: 0.925308, Accuracy: 77.82%\n",
      "Batch 102, Loss: 0.993933, Accuracy: 77.79%\n",
      "Batch 103, Loss: 0.962911, Accuracy: 77.76%\n",
      "Batch 104, Loss: 0.924519, Accuracy: 77.79%\n",
      "Batch 105, Loss: 0.940660, Accuracy: 77.81%\n",
      "Batch 106, Loss: 0.964142, Accuracy: 77.82%\n",
      "Batch 107, Loss: 1.035588, Accuracy: 77.72%\n",
      "Batch 108, Loss: 1.021127, Accuracy: 77.65%\n",
      "Batch 109, Loss: 0.994676, Accuracy: 77.62%\n",
      "Batch 110, Loss: 0.991575, Accuracy: 77.63%\n",
      "Batch 111, Loss: 0.967110, Accuracy: 77.63%\n",
      "Batch 112, Loss: 0.968774, Accuracy: 77.62%\n",
      "Batch 113, Loss: 0.893041, Accuracy: 77.68%\n",
      "Batch 114, Loss: 1.007895, Accuracy: 77.63%\n",
      "Batch 115, Loss: 0.987729, Accuracy: 77.61%\n",
      "Batch 116, Loss: 0.995612, Accuracy: 77.60%\n",
      "Batch 117, Loss: 0.910404, Accuracy: 77.66%\n",
      "Batch 118, Loss: 0.944998, Accuracy: 77.69%\n",
      "Batch 119, Loss: 0.973861, Accuracy: 77.68%\n",
      "Batch 120, Loss: 0.966471, Accuracy: 77.66%\n",
      "Batch 121, Loss: 0.973431, Accuracy: 77.67%\n",
      "Batch 122, Loss: 1.021128, Accuracy: 77.60%\n",
      "Batch 123, Loss: 0.960959, Accuracy: 77.60%\n",
      "Batch 124, Loss: 0.950107, Accuracy: 77.61%\n",
      "Batch 125, Loss: 0.999225, Accuracy: 77.59%\n",
      "Batch 126, Loss: 1.063129, Accuracy: 77.52%\n",
      "Batch 127, Loss: 0.991446, Accuracy: 77.52%\n",
      "Batch 128, Loss: 0.861718, Accuracy: 77.62%\n",
      "Batch 129, Loss: 0.921340, Accuracy: 77.66%\n",
      "Batch 130, Loss: 1.033720, Accuracy: 77.63%\n",
      "Batch 131, Loss: 0.931472, Accuracy: 77.66%\n",
      "Batch 132, Loss: 0.973565, Accuracy: 77.66%\n",
      "Batch 133, Loss: 0.969113, Accuracy: 77.67%\n",
      "Batch 134, Loss: 0.980709, Accuracy: 77.65%\n",
      "Batch 135, Loss: 0.960259, Accuracy: 77.65%\n",
      "Batch 136, Loss: 0.915406, Accuracy: 77.68%\n",
      "Batch 137, Loss: 0.960624, Accuracy: 77.67%\n",
      "Batch 138, Loss: 1.011450, Accuracy: 77.64%\n",
      "Batch 139, Loss: 0.957418, Accuracy: 77.64%\n",
      "Batch 140, Loss: 0.987027, Accuracy: 77.62%\n",
      "Batch 141, Loss: 0.969195, Accuracy: 77.62%\n",
      "Batch 142, Loss: 0.931922, Accuracy: 77.64%\n",
      "Batch 143, Loss: 0.912513, Accuracy: 77.68%\n",
      "Batch 144, Loss: 1.059795, Accuracy: 77.59%\n",
      "Batch 145, Loss: 0.935743, Accuracy: 77.62%\n",
      "Batch 146, Loss: 0.993622, Accuracy: 77.60%\n",
      "Batch 147, Loss: 0.937246, Accuracy: 77.63%\n",
      "Batch 148, Loss: 0.987505, Accuracy: 77.61%\n",
      "Batch 149, Loss: 0.958403, Accuracy: 77.63%\n",
      "Batch 150, Loss: 1.008123, Accuracy: 77.60%\n",
      "Batch 151, Loss: 0.964159, Accuracy: 77.60%\n",
      "Batch 152, Loss: 1.048595, Accuracy: 77.55%\n",
      "Batch 153, Loss: 0.901050, Accuracy: 77.60%\n",
      "Batch 154, Loss: 0.983364, Accuracy: 77.61%\n",
      "Batch 155, Loss: 0.999154, Accuracy: 77.59%\n",
      "Batch 156, Loss: 1.061514, Accuracy: 77.51%\n",
      "Batch 157, Loss: 0.919660, Accuracy: 77.56%\n",
      "Batch 158, Loss: 0.986770, Accuracy: 77.55%\n",
      "Batch 159, Loss: 1.003353, Accuracy: 77.55%\n",
      "Batch 160, Loss: 0.992428, Accuracy: 77.54%\n",
      "Batch 161, Loss: 0.960897, Accuracy: 77.55%\n",
      "Batch 162, Loss: 1.007609, Accuracy: 77.54%\n",
      "Batch 163, Loss: 0.919984, Accuracy: 77.58%\n",
      "Batch 164, Loss: 1.087427, Accuracy: 77.50%\n",
      "Batch 165, Loss: 0.946297, Accuracy: 77.51%\n",
      "Batch 166, Loss: 1.018986, Accuracy: 77.48%\n",
      "Batch 167, Loss: 0.976603, Accuracy: 77.47%\n",
      "Batch 168, Loss: 0.935233, Accuracy: 77.48%\n",
      "Batch 169, Loss: 1.062251, Accuracy: 77.43%\n",
      "Batch 170, Loss: 1.020864, Accuracy: 77.40%\n",
      "Batch 171, Loss: 1.009820, Accuracy: 77.38%\n",
      "Batch 172, Loss: 0.936575, Accuracy: 77.40%\n",
      "Batch 173, Loss: 1.034038, Accuracy: 77.36%\n",
      "Batch 174, Loss: 1.017839, Accuracy: 77.33%\n",
      "Batch 175, Loss: 0.937039, Accuracy: 77.36%\n",
      "Batch 176, Loss: 1.005272, Accuracy: 77.33%\n",
      "Batch 177, Loss: 1.009689, Accuracy: 77.30%\n",
      "Batch 178, Loss: 0.999861, Accuracy: 77.27%\n",
      "Batch 179, Loss: 1.018746, Accuracy: 77.24%\n",
      "Batch 180, Loss: 1.014888, Accuracy: 77.20%\n",
      "Batch 181, Loss: 0.980603, Accuracy: 77.19%\n",
      "Batch 182, Loss: 0.931271, Accuracy: 77.22%\n",
      "Batch 183, Loss: 1.000105, Accuracy: 77.19%\n",
      "Batch 184, Loss: 1.058714, Accuracy: 77.15%\n",
      "Batch 185, Loss: 0.847033, Accuracy: 77.21%\n",
      "Batch 186, Loss: 0.935250, Accuracy: 77.23%\n",
      "Batch 187, Loss: 0.933673, Accuracy: 77.25%\n",
      "Batch 188, Loss: 0.971664, Accuracy: 77.24%\n",
      "Batch 189, Loss: 1.001081, Accuracy: 77.23%\n",
      "Batch 190, Loss: 1.004776, Accuracy: 77.21%\n",
      "Batch 191, Loss: 0.901327, Accuracy: 77.24%\n",
      "Batch 192, Loss: 1.022822, Accuracy: 77.21%\n",
      "Batch 193, Loss: 0.984097, Accuracy: 77.20%\n",
      "Batch 194, Loss: 0.988389, Accuracy: 77.18%\n",
      "Batch 195, Loss: 0.965058, Accuracy: 77.19%\n",
      "Batch 196, Loss: 0.928996, Accuracy: 77.23%\n",
      "Batch 197, Loss: 1.077744, Accuracy: 77.19%\n",
      "Batch 198, Loss: 0.934968, Accuracy: 77.20%\n",
      "Batch 199, Loss: 1.055205, Accuracy: 77.17%\n",
      "Batch 200, Loss: 1.047317, Accuracy: 77.14%\n",
      "Batch 201, Loss: 0.933172, Accuracy: 77.16%\n",
      "Batch 202, Loss: 0.900126, Accuracy: 77.20%\n",
      "Batch 203, Loss: 1.014444, Accuracy: 77.19%\n",
      "Batch 204, Loss: 0.940493, Accuracy: 77.21%\n",
      "Batch 205, Loss: 0.974502, Accuracy: 77.20%\n",
      "Batch 206, Loss: 0.969648, Accuracy: 77.20%\n",
      "Batch 207, Loss: 0.952764, Accuracy: 77.21%\n",
      "Batch 208, Loss: 0.997652, Accuracy: 77.19%\n",
      "Batch 209, Loss: 0.961353, Accuracy: 77.21%\n",
      "Batch 210, Loss: 1.032528, Accuracy: 77.17%\n",
      "Batch 211, Loss: 0.971393, Accuracy: 77.18%\n",
      "Batch 212, Loss: 0.884686, Accuracy: 77.23%\n",
      "Batch 213, Loss: 1.008730, Accuracy: 77.21%\n",
      "Training - Epoch 35, Loss: 0.971532, Accuracy: 77.21%\n",
      "Validation Batch 1, Loss: 0.944754, Accuracy: 81.25%\n",
      "Validation Batch 2, Loss: 1.009311, Accuracy: 78.12%\n",
      "Validation Batch 3, Loss: 1.067161, Accuracy: 74.48%\n",
      "Validation Batch 4, Loss: 0.971979, Accuracy: 75.39%\n",
      "Validation Batch 5, Loss: 0.955283, Accuracy: 76.88%\n",
      "Validation Batch 6, Loss: 0.904083, Accuracy: 78.12%\n",
      "Validation Batch 7, Loss: 0.995736, Accuracy: 77.68%\n",
      "Validation Batch 8, Loss: 1.025774, Accuracy: 76.95%\n",
      "Validation Batch 9, Loss: 1.014734, Accuracy: 76.22%\n",
      "Validation Batch 10, Loss: 0.971526, Accuracy: 76.56%\n",
      "Validation Batch 11, Loss: 0.958010, Accuracy: 76.70%\n",
      "Validation Batch 12, Loss: 0.959828, Accuracy: 76.69%\n",
      "Validation Batch 13, Loss: 1.026416, Accuracy: 76.32%\n",
      "Validation Batch 14, Loss: 1.011262, Accuracy: 76.00%\n",
      "Validation Batch 15, Loss: 0.980788, Accuracy: 76.15%\n",
      "Validation Batch 16, Loss: 0.973887, Accuracy: 76.37%\n",
      "Validation Batch 17, Loss: 1.066172, Accuracy: 75.64%\n",
      "Validation Batch 18, Loss: 0.967128, Accuracy: 75.78%\n",
      "Validation Batch 19, Loss: 1.023736, Accuracy: 75.58%\n",
      "Validation Batch 20, Loss: 1.015584, Accuracy: 75.39%\n",
      "Validation Batch 21, Loss: 0.995128, Accuracy: 75.30%\n",
      "Validation Batch 22, Loss: 1.037222, Accuracy: 75.00%\n",
      "Validation Batch 23, Loss: 1.072439, Accuracy: 74.59%\n",
      "Validation Batch 24, Loss: 0.998325, Accuracy: 74.61%\n",
      "Validation Batch 25, Loss: 0.999053, Accuracy: 74.56%\n",
      "Validation Batch 26, Loss: 0.997100, Accuracy: 74.58%\n",
      "Validation Batch 27, Loss: 0.955868, Accuracy: 74.63%\n",
      "Validation - Epoch 35, Loss: 0.996233, Accuracy: 74.63%\n",
      "Patience—0\n",
      "Epoch 36\n",
      "Batch 1, Loss: 0.970182, Accuracy: 75.00%\n",
      "Batch 2, Loss: 0.961520, Accuracy: 78.12%\n",
      "Batch 3, Loss: 0.989195, Accuracy: 77.08%\n",
      "Batch 4, Loss: 0.946706, Accuracy: 77.73%\n",
      "Batch 5, Loss: 0.901959, Accuracy: 79.38%\n",
      "Batch 6, Loss: 1.046493, Accuracy: 77.86%\n",
      "Batch 7, Loss: 0.926561, Accuracy: 78.57%\n",
      "Batch 8, Loss: 0.969066, Accuracy: 78.32%\n",
      "Batch 9, Loss: 0.966609, Accuracy: 78.30%\n",
      "Batch 10, Loss: 0.943142, Accuracy: 78.28%\n",
      "Batch 11, Loss: 1.060791, Accuracy: 77.41%\n",
      "Batch 12, Loss: 0.941289, Accuracy: 77.73%\n",
      "Batch 13, Loss: 0.985544, Accuracy: 77.64%\n",
      "Batch 14, Loss: 1.040311, Accuracy: 77.12%\n",
      "Batch 15, Loss: 0.905442, Accuracy: 77.50%\n",
      "Batch 16, Loss: 0.993833, Accuracy: 77.25%\n",
      "Batch 17, Loss: 0.942941, Accuracy: 77.48%\n",
      "Batch 18, Loss: 0.933424, Accuracy: 77.69%\n",
      "Batch 19, Loss: 0.913266, Accuracy: 78.04%\n",
      "Batch 20, Loss: 0.877299, Accuracy: 78.52%\n",
      "Batch 21, Loss: 0.891461, Accuracy: 78.87%\n",
      "Batch 22, Loss: 0.935145, Accuracy: 79.05%\n",
      "Batch 23, Loss: 0.953609, Accuracy: 79.01%\n",
      "Batch 24, Loss: 0.996638, Accuracy: 78.91%\n",
      "Batch 25, Loss: 0.903562, Accuracy: 79.06%\n",
      "Batch 26, Loss: 0.941126, Accuracy: 79.15%\n",
      "Batch 27, Loss: 0.951908, Accuracy: 79.11%\n",
      "Batch 28, Loss: 0.914096, Accuracy: 79.30%\n",
      "Batch 29, Loss: 0.953086, Accuracy: 79.26%\n",
      "Batch 30, Loss: 1.011158, Accuracy: 79.01%\n",
      "Batch 31, Loss: 0.884903, Accuracy: 79.23%\n",
      "Batch 32, Loss: 0.985050, Accuracy: 79.10%\n",
      "Batch 33, Loss: 1.012461, Accuracy: 78.93%\n",
      "Batch 34, Loss: 1.116844, Accuracy: 78.45%\n",
      "Batch 35, Loss: 0.897106, Accuracy: 78.66%\n",
      "Batch 36, Loss: 0.918775, Accuracy: 78.78%\n",
      "Batch 37, Loss: 0.984213, Accuracy: 78.76%\n",
      "Batch 38, Loss: 1.028075, Accuracy: 78.54%\n",
      "Batch 39, Loss: 0.955837, Accuracy: 78.49%\n",
      "Batch 40, Loss: 0.959456, Accuracy: 78.52%\n",
      "Batch 41, Loss: 0.944255, Accuracy: 78.43%\n",
      "Batch 42, Loss: 1.001555, Accuracy: 78.31%\n",
      "Batch 43, Loss: 0.999986, Accuracy: 78.23%\n",
      "Batch 44, Loss: 1.028411, Accuracy: 78.09%\n",
      "Batch 45, Loss: 0.965397, Accuracy: 78.12%\n",
      "Batch 46, Loss: 0.967822, Accuracy: 78.12%\n",
      "Batch 47, Loss: 0.964378, Accuracy: 78.09%\n",
      "Batch 48, Loss: 0.908264, Accuracy: 78.22%\n",
      "Batch 49, Loss: 1.036861, Accuracy: 78.06%\n",
      "Batch 50, Loss: 0.914470, Accuracy: 78.16%\n",
      "Batch 51, Loss: 1.012622, Accuracy: 78.06%\n",
      "Batch 52, Loss: 0.982082, Accuracy: 78.03%\n",
      "Batch 53, Loss: 0.982442, Accuracy: 77.98%\n",
      "Batch 54, Loss: 0.900625, Accuracy: 78.10%\n",
      "Batch 55, Loss: 0.976802, Accuracy: 78.07%\n",
      "Batch 56, Loss: 0.908986, Accuracy: 78.15%\n",
      "Batch 57, Loss: 0.928034, Accuracy: 78.26%\n",
      "Batch 58, Loss: 0.951230, Accuracy: 78.31%\n",
      "Batch 59, Loss: 0.989426, Accuracy: 78.23%\n",
      "Batch 60, Loss: 0.965010, Accuracy: 78.23%\n",
      "Batch 61, Loss: 0.976806, Accuracy: 78.20%\n",
      "Batch 62, Loss: 1.009764, Accuracy: 78.10%\n",
      "Batch 63, Loss: 0.939854, Accuracy: 78.12%\n",
      "Batch 64, Loss: 0.917692, Accuracy: 78.22%\n",
      "Batch 65, Loss: 0.967114, Accuracy: 78.22%\n",
      "Batch 66, Loss: 0.893840, Accuracy: 78.34%\n",
      "Batch 67, Loss: 0.983065, Accuracy: 78.31%\n",
      "Batch 68, Loss: 1.007612, Accuracy: 78.22%\n",
      "Batch 69, Loss: 0.970345, Accuracy: 78.17%\n",
      "Batch 70, Loss: 0.972128, Accuracy: 78.15%\n",
      "Batch 71, Loss: 1.042627, Accuracy: 77.99%\n",
      "Batch 72, Loss: 1.050431, Accuracy: 77.86%\n",
      "Batch 73, Loss: 0.943408, Accuracy: 77.91%\n",
      "Batch 74, Loss: 0.938433, Accuracy: 77.98%\n",
      "Batch 75, Loss: 0.908828, Accuracy: 78.04%\n",
      "Batch 76, Loss: 0.932151, Accuracy: 78.06%\n",
      "Batch 77, Loss: 1.005586, Accuracy: 78.02%\n",
      "Batch 78, Loss: 0.941868, Accuracy: 78.04%\n",
      "Batch 79, Loss: 0.971585, Accuracy: 78.05%\n",
      "Batch 80, Loss: 1.050348, Accuracy: 77.95%\n",
      "Batch 81, Loss: 0.970531, Accuracy: 77.95%\n",
      "Batch 82, Loss: 0.912948, Accuracy: 78.03%\n",
      "Batch 83, Loss: 0.936441, Accuracy: 78.07%\n",
      "Batch 84, Loss: 0.985417, Accuracy: 78.03%\n",
      "Batch 85, Loss: 0.918895, Accuracy: 78.09%\n",
      "Batch 86, Loss: 0.872492, Accuracy: 78.20%\n",
      "Batch 87, Loss: 0.967967, Accuracy: 78.16%\n",
      "Batch 88, Loss: 1.045139, Accuracy: 78.07%\n",
      "Batch 89, Loss: 0.918560, Accuracy: 78.14%\n",
      "Batch 90, Loss: 0.970596, Accuracy: 78.12%\n",
      "Batch 91, Loss: 1.033776, Accuracy: 78.04%\n",
      "Batch 92, Loss: 1.003839, Accuracy: 77.99%\n",
      "Batch 93, Loss: 1.017671, Accuracy: 77.92%\n",
      "Batch 94, Loss: 0.923451, Accuracy: 77.96%\n",
      "Batch 95, Loss: 0.892983, Accuracy: 78.04%\n",
      "Batch 96, Loss: 0.999990, Accuracy: 77.99%\n",
      "Batch 97, Loss: 1.011062, Accuracy: 77.95%\n",
      "Batch 98, Loss: 0.984144, Accuracy: 77.95%\n",
      "Batch 99, Loss: 0.922906, Accuracy: 78.00%\n",
      "Batch 100, Loss: 1.075857, Accuracy: 77.91%\n",
      "Batch 101, Loss: 0.956310, Accuracy: 77.91%\n",
      "Batch 102, Loss: 0.907819, Accuracy: 77.96%\n",
      "Batch 103, Loss: 0.955215, Accuracy: 77.96%\n",
      "Batch 104, Loss: 1.018406, Accuracy: 77.91%\n",
      "Batch 105, Loss: 0.978153, Accuracy: 77.92%\n",
      "Batch 106, Loss: 1.023069, Accuracy: 77.86%\n",
      "Batch 107, Loss: 0.895788, Accuracy: 77.91%\n",
      "Batch 108, Loss: 1.023472, Accuracy: 77.85%\n",
      "Batch 109, Loss: 0.919080, Accuracy: 77.90%\n",
      "Batch 110, Loss: 0.934740, Accuracy: 77.93%\n",
      "Batch 111, Loss: 0.952207, Accuracy: 77.93%\n",
      "Batch 112, Loss: 0.910458, Accuracy: 77.99%\n",
      "Batch 113, Loss: 1.071891, Accuracy: 77.88%\n",
      "Batch 114, Loss: 1.040544, Accuracy: 77.81%\n",
      "Batch 115, Loss: 0.944651, Accuracy: 77.83%\n",
      "Batch 116, Loss: 0.972906, Accuracy: 77.83%\n",
      "Batch 117, Loss: 0.990960, Accuracy: 77.80%\n",
      "Batch 118, Loss: 0.954988, Accuracy: 77.82%\n",
      "Batch 119, Loss: 0.998336, Accuracy: 77.81%\n",
      "Batch 120, Loss: 0.925085, Accuracy: 77.85%\n",
      "Batch 121, Loss: 1.001534, Accuracy: 77.80%\n",
      "Batch 122, Loss: 0.901938, Accuracy: 77.88%\n",
      "Batch 123, Loss: 0.953932, Accuracy: 77.88%\n",
      "Batch 124, Loss: 0.995703, Accuracy: 77.85%\n",
      "Batch 125, Loss: 0.995484, Accuracy: 77.83%\n",
      "Batch 126, Loss: 0.977416, Accuracy: 77.80%\n",
      "Batch 127, Loss: 0.991490, Accuracy: 77.77%\n",
      "Batch 128, Loss: 0.995183, Accuracy: 77.72%\n",
      "Batch 129, Loss: 0.990767, Accuracy: 77.69%\n",
      "Batch 130, Loss: 0.967217, Accuracy: 77.68%\n",
      "Batch 131, Loss: 1.021616, Accuracy: 77.62%\n",
      "Batch 132, Loss: 0.964374, Accuracy: 77.62%\n",
      "Batch 133, Loss: 0.960646, Accuracy: 77.61%\n",
      "Batch 134, Loss: 0.962937, Accuracy: 77.61%\n",
      "Batch 135, Loss: 1.052258, Accuracy: 77.55%\n",
      "Batch 136, Loss: 0.944359, Accuracy: 77.56%\n",
      "Batch 137, Loss: 0.968905, Accuracy: 77.55%\n",
      "Batch 138, Loss: 0.964941, Accuracy: 77.55%\n",
      "Batch 139, Loss: 0.942495, Accuracy: 77.59%\n",
      "Batch 140, Loss: 0.999202, Accuracy: 77.54%\n",
      "Batch 141, Loss: 1.092019, Accuracy: 77.46%\n",
      "Batch 142, Loss: 1.001348, Accuracy: 77.43%\n",
      "Batch 143, Loss: 1.028261, Accuracy: 77.39%\n",
      "Batch 144, Loss: 1.008147, Accuracy: 77.38%\n",
      "Batch 145, Loss: 0.966942, Accuracy: 77.38%\n",
      "Batch 146, Loss: 0.923473, Accuracy: 77.42%\n",
      "Batch 147, Loss: 0.933898, Accuracy: 77.41%\n",
      "Batch 148, Loss: 1.016459, Accuracy: 77.39%\n",
      "Batch 149, Loss: 0.915632, Accuracy: 77.43%\n",
      "Batch 150, Loss: 1.002886, Accuracy: 77.42%\n",
      "Batch 151, Loss: 0.951329, Accuracy: 77.42%\n",
      "Batch 152, Loss: 0.956478, Accuracy: 77.44%\n",
      "Batch 153, Loss: 0.971143, Accuracy: 77.44%\n",
      "Batch 154, Loss: 0.962133, Accuracy: 77.46%\n",
      "Batch 155, Loss: 0.966753, Accuracy: 77.45%\n",
      "Batch 156, Loss: 0.905284, Accuracy: 77.49%\n",
      "Batch 157, Loss: 0.999043, Accuracy: 77.49%\n",
      "Batch 158, Loss: 0.993874, Accuracy: 77.47%\n",
      "Batch 159, Loss: 0.914861, Accuracy: 77.52%\n",
      "Batch 160, Loss: 0.964628, Accuracy: 77.52%\n",
      "Batch 161, Loss: 0.930039, Accuracy: 77.55%\n",
      "Batch 162, Loss: 0.996206, Accuracy: 77.52%\n",
      "Batch 163, Loss: 1.025671, Accuracy: 77.47%\n",
      "Batch 164, Loss: 0.967983, Accuracy: 77.48%\n",
      "Batch 165, Loss: 1.024029, Accuracy: 77.44%\n",
      "Batch 166, Loss: 1.011304, Accuracy: 77.41%\n",
      "Batch 167, Loss: 0.965887, Accuracy: 77.41%\n",
      "Batch 168, Loss: 0.998731, Accuracy: 77.38%\n",
      "Batch 169, Loss: 0.916871, Accuracy: 77.41%\n",
      "Batch 170, Loss: 1.010238, Accuracy: 77.38%\n",
      "Batch 171, Loss: 0.947877, Accuracy: 77.39%\n",
      "Batch 172, Loss: 0.859878, Accuracy: 77.46%\n",
      "Batch 173, Loss: 1.020563, Accuracy: 77.43%\n",
      "Batch 174, Loss: 0.952697, Accuracy: 77.42%\n",
      "Batch 175, Loss: 1.057013, Accuracy: 77.38%\n",
      "Batch 176, Loss: 0.925055, Accuracy: 77.41%\n",
      "Batch 177, Loss: 1.008553, Accuracy: 77.38%\n",
      "Batch 178, Loss: 1.059754, Accuracy: 77.33%\n",
      "Batch 179, Loss: 0.945807, Accuracy: 77.33%\n",
      "Batch 180, Loss: 0.960954, Accuracy: 77.34%\n",
      "Batch 181, Loss: 1.098452, Accuracy: 77.28%\n",
      "Batch 182, Loss: 1.028244, Accuracy: 77.24%\n",
      "Batch 183, Loss: 1.022278, Accuracy: 77.22%\n",
      "Batch 184, Loss: 0.919469, Accuracy: 77.24%\n",
      "Batch 185, Loss: 1.008926, Accuracy: 77.22%\n",
      "Batch 186, Loss: 1.047080, Accuracy: 77.18%\n",
      "Batch 187, Loss: 0.942035, Accuracy: 77.18%\n",
      "Batch 188, Loss: 0.950593, Accuracy: 77.19%\n",
      "Batch 189, Loss: 0.921727, Accuracy: 77.22%\n",
      "Batch 190, Loss: 1.033840, Accuracy: 77.19%\n",
      "Batch 191, Loss: 1.021987, Accuracy: 77.16%\n",
      "Batch 192, Loss: 0.931876, Accuracy: 77.17%\n",
      "Batch 193, Loss: 0.993409, Accuracy: 77.17%\n",
      "Batch 194, Loss: 0.932031, Accuracy: 77.19%\n",
      "Batch 195, Loss: 0.834864, Accuracy: 77.27%\n",
      "Batch 196, Loss: 0.984471, Accuracy: 77.26%\n",
      "Batch 197, Loss: 0.939134, Accuracy: 77.28%\n",
      "Batch 198, Loss: 0.938666, Accuracy: 77.30%\n",
      "Batch 199, Loss: 1.010928, Accuracy: 77.28%\n",
      "Batch 200, Loss: 0.937697, Accuracy: 77.29%\n",
      "Batch 201, Loss: 1.038081, Accuracy: 77.25%\n",
      "Batch 202, Loss: 1.031298, Accuracy: 77.22%\n",
      "Batch 203, Loss: 1.053327, Accuracy: 77.17%\n",
      "Batch 204, Loss: 0.933791, Accuracy: 77.18%\n",
      "Batch 205, Loss: 0.949376, Accuracy: 77.19%\n",
      "Batch 206, Loss: 0.976826, Accuracy: 77.19%\n",
      "Batch 207, Loss: 0.963403, Accuracy: 77.20%\n",
      "Batch 208, Loss: 0.978788, Accuracy: 77.19%\n",
      "Batch 209, Loss: 0.967472, Accuracy: 77.20%\n",
      "Batch 210, Loss: 0.927230, Accuracy: 77.22%\n",
      "Batch 211, Loss: 0.925442, Accuracy: 77.25%\n",
      "Batch 212, Loss: 0.906959, Accuracy: 77.28%\n",
      "Batch 213, Loss: 0.963554, Accuracy: 77.28%\n",
      "Training - Epoch 36, Loss: 0.970077, Accuracy: 77.28%\n",
      "Validation Batch 1, Loss: 0.932101, Accuracy: 81.25%\n",
      "Validation Batch 2, Loss: 0.983025, Accuracy: 81.25%\n",
      "Validation Batch 3, Loss: 1.054198, Accuracy: 77.08%\n",
      "Validation Batch 4, Loss: 0.958727, Accuracy: 77.73%\n",
      "Validation Batch 5, Loss: 0.936090, Accuracy: 79.06%\n",
      "Validation Batch 6, Loss: 0.878886, Accuracy: 80.73%\n",
      "Validation Batch 7, Loss: 0.982305, Accuracy: 80.36%\n",
      "Validation Batch 8, Loss: 1.018284, Accuracy: 79.30%\n",
      "Validation Batch 9, Loss: 0.994190, Accuracy: 78.99%\n",
      "Validation Batch 10, Loss: 0.957699, Accuracy: 78.91%\n",
      "Validation Batch 11, Loss: 0.949984, Accuracy: 79.12%\n",
      "Validation Batch 12, Loss: 0.943484, Accuracy: 79.17%\n",
      "Validation Batch 13, Loss: 1.007106, Accuracy: 78.73%\n",
      "Validation Batch 14, Loss: 0.988356, Accuracy: 78.46%\n",
      "Validation Batch 15, Loss: 0.967108, Accuracy: 78.44%\n",
      "Validation Batch 16, Loss: 0.954284, Accuracy: 78.52%\n",
      "Validation Batch 17, Loss: 1.027066, Accuracy: 78.31%\n",
      "Validation Batch 18, Loss: 0.957596, Accuracy: 78.21%\n",
      "Validation Batch 19, Loss: 1.001487, Accuracy: 78.12%\n",
      "Validation Batch 20, Loss: 1.002790, Accuracy: 77.89%\n",
      "Validation Batch 21, Loss: 0.980242, Accuracy: 77.68%\n",
      "Validation Batch 22, Loss: 1.001066, Accuracy: 77.56%\n",
      "Validation Batch 23, Loss: 1.043673, Accuracy: 77.17%\n",
      "Validation Batch 24, Loss: 0.981541, Accuracy: 77.15%\n",
      "Validation Batch 25, Loss: 0.961847, Accuracy: 77.12%\n",
      "Validation Batch 26, Loss: 0.980443, Accuracy: 77.10%\n",
      "Validation Batch 27, Loss: 0.933441, Accuracy: 77.22%\n",
      "Validation - Epoch 36, Loss: 0.976927, Accuracy: 77.22%\n",
      "Patience—0\n",
      "Epoch 37\n",
      "Batch 1, Loss: 1.022082, Accuracy: 70.31%\n",
      "Batch 2, Loss: 0.910758, Accuracy: 76.56%\n",
      "Batch 3, Loss: 0.992455, Accuracy: 76.04%\n",
      "Batch 4, Loss: 1.048363, Accuracy: 74.61%\n",
      "Batch 5, Loss: 0.995585, Accuracy: 74.38%\n",
      "Batch 6, Loss: 0.956526, Accuracy: 75.26%\n",
      "Batch 7, Loss: 0.927201, Accuracy: 76.34%\n",
      "Batch 8, Loss: 0.904936, Accuracy: 77.34%\n",
      "Batch 9, Loss: 0.967758, Accuracy: 77.43%\n",
      "Batch 10, Loss: 0.997299, Accuracy: 77.03%\n",
      "Batch 11, Loss: 0.996804, Accuracy: 76.99%\n",
      "Batch 12, Loss: 1.017110, Accuracy: 76.43%\n",
      "Batch 13, Loss: 1.036687, Accuracy: 75.84%\n",
      "Batch 14, Loss: 0.947035, Accuracy: 76.12%\n",
      "Batch 15, Loss: 1.007370, Accuracy: 75.83%\n",
      "Batch 16, Loss: 1.007603, Accuracy: 75.68%\n",
      "Batch 17, Loss: 0.912316, Accuracy: 76.19%\n",
      "Batch 18, Loss: 1.001077, Accuracy: 76.13%\n",
      "Batch 19, Loss: 0.937291, Accuracy: 76.40%\n",
      "Batch 20, Loss: 1.066149, Accuracy: 75.94%\n",
      "Batch 21, Loss: 0.899162, Accuracy: 76.34%\n",
      "Batch 22, Loss: 0.956536, Accuracy: 76.42%\n",
      "Batch 23, Loss: 0.991214, Accuracy: 76.36%\n",
      "Batch 24, Loss: 1.045318, Accuracy: 76.04%\n",
      "Batch 25, Loss: 0.917233, Accuracy: 76.31%\n",
      "Batch 26, Loss: 0.940028, Accuracy: 76.44%\n",
      "Batch 27, Loss: 0.993288, Accuracy: 76.33%\n",
      "Batch 28, Loss: 0.971491, Accuracy: 76.34%\n",
      "Batch 29, Loss: 0.940062, Accuracy: 76.51%\n",
      "Batch 30, Loss: 0.980060, Accuracy: 76.46%\n",
      "Batch 31, Loss: 0.949792, Accuracy: 76.51%\n",
      "Batch 32, Loss: 0.951743, Accuracy: 76.61%\n",
      "Batch 33, Loss: 0.958737, Accuracy: 76.66%\n",
      "Batch 34, Loss: 0.914741, Accuracy: 76.88%\n",
      "Batch 35, Loss: 0.937633, Accuracy: 76.96%\n",
      "Batch 36, Loss: 0.944527, Accuracy: 77.04%\n",
      "Batch 37, Loss: 0.953522, Accuracy: 77.07%\n",
      "Batch 38, Loss: 0.995781, Accuracy: 76.97%\n",
      "Batch 39, Loss: 0.997343, Accuracy: 76.96%\n",
      "Batch 40, Loss: 0.914732, Accuracy: 77.11%\n",
      "Batch 41, Loss: 0.944565, Accuracy: 77.10%\n",
      "Batch 42, Loss: 0.918530, Accuracy: 77.27%\n",
      "Batch 43, Loss: 1.018512, Accuracy: 77.14%\n",
      "Batch 44, Loss: 1.012468, Accuracy: 77.06%\n",
      "Batch 45, Loss: 1.027997, Accuracy: 76.94%\n",
      "Batch 46, Loss: 0.965785, Accuracy: 76.94%\n",
      "Batch 47, Loss: 1.015857, Accuracy: 76.89%\n",
      "Batch 48, Loss: 0.924874, Accuracy: 77.02%\n",
      "Batch 49, Loss: 0.953350, Accuracy: 77.04%\n",
      "Batch 50, Loss: 1.006245, Accuracy: 76.94%\n",
      "Batch 51, Loss: 0.963223, Accuracy: 76.93%\n",
      "Batch 52, Loss: 0.938685, Accuracy: 77.01%\n",
      "Batch 53, Loss: 0.975197, Accuracy: 77.00%\n",
      "Batch 54, Loss: 0.967623, Accuracy: 77.00%\n",
      "Batch 55, Loss: 0.977478, Accuracy: 77.02%\n",
      "Batch 56, Loss: 0.927742, Accuracy: 77.09%\n",
      "Batch 57, Loss: 0.999522, Accuracy: 77.08%\n",
      "Batch 58, Loss: 0.909282, Accuracy: 77.16%\n",
      "Batch 59, Loss: 0.943681, Accuracy: 77.20%\n",
      "Batch 60, Loss: 0.922121, Accuracy: 77.27%\n",
      "Batch 61, Loss: 0.930983, Accuracy: 77.33%\n",
      "Batch 62, Loss: 1.029519, Accuracy: 77.19%\n",
      "Batch 63, Loss: 0.968775, Accuracy: 77.18%\n",
      "Batch 64, Loss: 1.051732, Accuracy: 77.05%\n",
      "Batch 65, Loss: 0.942101, Accuracy: 77.07%\n",
      "Batch 66, Loss: 0.993700, Accuracy: 77.04%\n",
      "Batch 67, Loss: 0.925918, Accuracy: 77.12%\n",
      "Batch 68, Loss: 1.030731, Accuracy: 77.00%\n",
      "Batch 69, Loss: 1.076313, Accuracy: 76.86%\n",
      "Batch 70, Loss: 0.888290, Accuracy: 76.96%\n",
      "Batch 71, Loss: 0.978137, Accuracy: 76.96%\n",
      "Batch 72, Loss: 0.918377, Accuracy: 77.06%\n",
      "Batch 73, Loss: 0.934110, Accuracy: 77.12%\n",
      "Batch 74, Loss: 1.036553, Accuracy: 77.01%\n",
      "Batch 75, Loss: 0.928197, Accuracy: 77.08%\n",
      "Batch 76, Loss: 0.992551, Accuracy: 77.08%\n",
      "Batch 77, Loss: 0.960085, Accuracy: 77.11%\n",
      "Batch 78, Loss: 1.002551, Accuracy: 77.06%\n",
      "Batch 79, Loss: 0.914034, Accuracy: 77.16%\n",
      "Batch 80, Loss: 0.979427, Accuracy: 77.11%\n",
      "Batch 81, Loss: 1.056590, Accuracy: 76.99%\n",
      "Batch 82, Loss: 1.003114, Accuracy: 76.94%\n",
      "Batch 83, Loss: 1.030469, Accuracy: 76.88%\n",
      "Batch 84, Loss: 0.932012, Accuracy: 76.93%\n",
      "Batch 85, Loss: 0.934748, Accuracy: 76.99%\n",
      "Batch 86, Loss: 0.980196, Accuracy: 76.98%\n",
      "Batch 87, Loss: 0.947701, Accuracy: 76.99%\n",
      "Batch 88, Loss: 0.901365, Accuracy: 77.10%\n",
      "Batch 89, Loss: 1.058584, Accuracy: 76.98%\n",
      "Batch 90, Loss: 0.989590, Accuracy: 76.96%\n",
      "Batch 91, Loss: 0.902454, Accuracy: 77.04%\n",
      "Batch 92, Loss: 0.968595, Accuracy: 77.07%\n",
      "Batch 93, Loss: 1.027079, Accuracy: 77.02%\n",
      "Batch 94, Loss: 0.937347, Accuracy: 77.06%\n",
      "Batch 95, Loss: 0.947639, Accuracy: 77.11%\n",
      "Batch 96, Loss: 0.972579, Accuracy: 77.10%\n",
      "Batch 97, Loss: 0.942802, Accuracy: 77.16%\n",
      "Batch 98, Loss: 0.975591, Accuracy: 77.14%\n",
      "Batch 99, Loss: 0.893545, Accuracy: 77.24%\n",
      "Batch 100, Loss: 0.969277, Accuracy: 77.23%\n",
      "Batch 101, Loss: 0.960261, Accuracy: 77.26%\n",
      "Batch 102, Loss: 1.052121, Accuracy: 77.18%\n",
      "Batch 103, Loss: 0.965713, Accuracy: 77.18%\n",
      "Batch 104, Loss: 0.935411, Accuracy: 77.22%\n",
      "Batch 105, Loss: 0.987969, Accuracy: 77.20%\n",
      "Batch 106, Loss: 0.923885, Accuracy: 77.23%\n",
      "Batch 107, Loss: 0.950895, Accuracy: 77.23%\n",
      "Batch 108, Loss: 0.972542, Accuracy: 77.23%\n",
      "Batch 109, Loss: 0.982963, Accuracy: 77.22%\n",
      "Batch 110, Loss: 0.956077, Accuracy: 77.22%\n",
      "Batch 111, Loss: 0.894619, Accuracy: 77.29%\n",
      "Batch 112, Loss: 0.925535, Accuracy: 77.34%\n",
      "Batch 113, Loss: 0.972597, Accuracy: 77.35%\n",
      "Batch 114, Loss: 1.080780, Accuracy: 77.25%\n",
      "Batch 115, Loss: 0.949630, Accuracy: 77.26%\n",
      "Batch 116, Loss: 0.981885, Accuracy: 77.24%\n",
      "Batch 117, Loss: 0.922265, Accuracy: 77.27%\n",
      "Batch 118, Loss: 0.965385, Accuracy: 77.28%\n",
      "Batch 119, Loss: 0.985437, Accuracy: 77.25%\n",
      "Batch 120, Loss: 0.880431, Accuracy: 77.32%\n",
      "Batch 121, Loss: 1.005512, Accuracy: 77.29%\n",
      "Batch 122, Loss: 1.074796, Accuracy: 77.19%\n",
      "Batch 123, Loss: 0.988188, Accuracy: 77.18%\n",
      "Batch 124, Loss: 0.895469, Accuracy: 77.24%\n",
      "Batch 125, Loss: 0.959465, Accuracy: 77.25%\n",
      "Batch 126, Loss: 0.915018, Accuracy: 77.31%\n",
      "Batch 127, Loss: 0.927867, Accuracy: 77.34%\n",
      "Batch 128, Loss: 0.990126, Accuracy: 77.31%\n",
      "Batch 129, Loss: 0.925382, Accuracy: 77.35%\n",
      "Batch 130, Loss: 0.968895, Accuracy: 77.34%\n",
      "Batch 131, Loss: 1.031490, Accuracy: 77.30%\n",
      "Batch 132, Loss: 0.900316, Accuracy: 77.37%\n",
      "Batch 133, Loss: 0.953772, Accuracy: 77.38%\n",
      "Batch 134, Loss: 0.993819, Accuracy: 77.37%\n",
      "Batch 135, Loss: 0.901222, Accuracy: 77.42%\n",
      "Batch 136, Loss: 0.954838, Accuracy: 77.42%\n",
      "Batch 137, Loss: 1.009960, Accuracy: 77.40%\n",
      "Batch 138, Loss: 0.908270, Accuracy: 77.43%\n",
      "Batch 139, Loss: 0.961641, Accuracy: 77.45%\n",
      "Batch 140, Loss: 0.998402, Accuracy: 77.42%\n",
      "Batch 141, Loss: 0.928055, Accuracy: 77.45%\n",
      "Batch 142, Loss: 0.892300, Accuracy: 77.50%\n",
      "Batch 143, Loss: 0.949026, Accuracy: 77.50%\n",
      "Batch 144, Loss: 1.010222, Accuracy: 77.47%\n",
      "Batch 145, Loss: 0.952316, Accuracy: 77.49%\n",
      "Batch 146, Loss: 0.991442, Accuracy: 77.47%\n",
      "Batch 147, Loss: 1.017312, Accuracy: 77.43%\n",
      "Batch 148, Loss: 0.972341, Accuracy: 77.43%\n",
      "Batch 149, Loss: 1.084687, Accuracy: 77.35%\n",
      "Batch 150, Loss: 0.919726, Accuracy: 77.38%\n",
      "Batch 151, Loss: 1.000686, Accuracy: 77.36%\n",
      "Batch 152, Loss: 0.974161, Accuracy: 77.35%\n",
      "Batch 153, Loss: 0.949098, Accuracy: 77.37%\n",
      "Batch 154, Loss: 0.890228, Accuracy: 77.41%\n",
      "Batch 155, Loss: 0.960634, Accuracy: 77.42%\n",
      "Batch 156, Loss: 1.043805, Accuracy: 77.36%\n",
      "Batch 157, Loss: 0.950707, Accuracy: 77.38%\n",
      "Batch 158, Loss: 0.959594, Accuracy: 77.38%\n",
      "Batch 159, Loss: 0.912568, Accuracy: 77.43%\n",
      "Batch 160, Loss: 1.034600, Accuracy: 77.39%\n",
      "Batch 161, Loss: 0.899608, Accuracy: 77.44%\n",
      "Batch 162, Loss: 0.971869, Accuracy: 77.44%\n",
      "Batch 163, Loss: 0.932663, Accuracy: 77.46%\n",
      "Batch 164, Loss: 0.946109, Accuracy: 77.49%\n",
      "Batch 165, Loss: 0.951586, Accuracy: 77.51%\n",
      "Batch 166, Loss: 0.903942, Accuracy: 77.55%\n",
      "Batch 167, Loss: 1.026354, Accuracy: 77.51%\n",
      "Batch 168, Loss: 0.919624, Accuracy: 77.54%\n",
      "Batch 169, Loss: 0.992231, Accuracy: 77.52%\n",
      "Batch 170, Loss: 0.976445, Accuracy: 77.51%\n",
      "Batch 171, Loss: 0.998068, Accuracy: 77.49%\n",
      "Batch 172, Loss: 1.041820, Accuracy: 77.45%\n",
      "Batch 173, Loss: 1.011117, Accuracy: 77.41%\n",
      "Batch 174, Loss: 1.051165, Accuracy: 77.35%\n",
      "Batch 175, Loss: 0.948227, Accuracy: 77.38%\n",
      "Batch 176, Loss: 1.000857, Accuracy: 77.37%\n",
      "Batch 177, Loss: 1.028454, Accuracy: 77.33%\n",
      "Batch 178, Loss: 0.907905, Accuracy: 77.38%\n",
      "Batch 179, Loss: 1.011932, Accuracy: 77.36%\n",
      "Batch 180, Loss: 0.970686, Accuracy: 77.35%\n",
      "Batch 181, Loss: 1.001293, Accuracy: 77.34%\n",
      "Batch 182, Loss: 0.937602, Accuracy: 77.35%\n",
      "Batch 183, Loss: 1.017163, Accuracy: 77.33%\n",
      "Batch 184, Loss: 0.948480, Accuracy: 77.34%\n",
      "Batch 185, Loss: 0.971041, Accuracy: 77.35%\n",
      "Batch 186, Loss: 0.989388, Accuracy: 77.33%\n",
      "Batch 187, Loss: 0.940291, Accuracy: 77.35%\n",
      "Batch 188, Loss: 0.991425, Accuracy: 77.34%\n",
      "Batch 189, Loss: 0.946802, Accuracy: 77.35%\n",
      "Batch 190, Loss: 1.012789, Accuracy: 77.34%\n",
      "Batch 191, Loss: 0.990358, Accuracy: 77.32%\n",
      "Batch 192, Loss: 0.925610, Accuracy: 77.35%\n",
      "Batch 193, Loss: 0.984127, Accuracy: 77.34%\n",
      "Batch 194, Loss: 0.985013, Accuracy: 77.33%\n",
      "Batch 195, Loss: 0.972628, Accuracy: 77.32%\n",
      "Batch 196, Loss: 0.900097, Accuracy: 77.37%\n",
      "Batch 197, Loss: 1.036635, Accuracy: 77.32%\n",
      "Batch 198, Loss: 0.997900, Accuracy: 77.30%\n",
      "Batch 199, Loss: 0.934850, Accuracy: 77.32%\n",
      "Batch 200, Loss: 0.913475, Accuracy: 77.35%\n",
      "Batch 201, Loss: 0.945303, Accuracy: 77.36%\n",
      "Batch 202, Loss: 0.995405, Accuracy: 77.35%\n",
      "Batch 203, Loss: 1.010740, Accuracy: 77.34%\n",
      "Batch 204, Loss: 0.936990, Accuracy: 77.35%\n",
      "Batch 205, Loss: 0.996472, Accuracy: 77.33%\n",
      "Batch 206, Loss: 0.926293, Accuracy: 77.37%\n",
      "Batch 207, Loss: 1.015693, Accuracy: 77.35%\n",
      "Batch 208, Loss: 0.961251, Accuracy: 77.35%\n",
      "Batch 209, Loss: 0.975437, Accuracy: 77.35%\n",
      "Batch 210, Loss: 1.029491, Accuracy: 77.33%\n",
      "Batch 211, Loss: 1.051784, Accuracy: 77.28%\n",
      "Batch 212, Loss: 0.971129, Accuracy: 77.29%\n",
      "Batch 213, Loss: 1.064308, Accuracy: 77.25%\n",
      "Training - Epoch 37, Loss: 0.970158, Accuracy: 77.25%\n",
      "Validation Batch 1, Loss: 0.925216, Accuracy: 82.81%\n",
      "Validation Batch 2, Loss: 0.977615, Accuracy: 81.25%\n",
      "Validation Batch 3, Loss: 1.040236, Accuracy: 77.60%\n",
      "Validation Batch 4, Loss: 0.955339, Accuracy: 78.52%\n",
      "Validation Batch 5, Loss: 0.929128, Accuracy: 79.69%\n",
      "Validation Batch 6, Loss: 0.875298, Accuracy: 81.25%\n",
      "Validation Batch 7, Loss: 0.977298, Accuracy: 81.03%\n",
      "Validation Batch 8, Loss: 1.014691, Accuracy: 79.88%\n",
      "Validation Batch 9, Loss: 0.990789, Accuracy: 79.51%\n",
      "Validation Batch 10, Loss: 0.952314, Accuracy: 79.69%\n",
      "Validation Batch 11, Loss: 0.937666, Accuracy: 79.69%\n",
      "Validation Batch 12, Loss: 0.935795, Accuracy: 79.69%\n",
      "Validation Batch 13, Loss: 1.000218, Accuracy: 79.21%\n",
      "Validation Batch 14, Loss: 0.980073, Accuracy: 79.13%\n",
      "Validation Batch 15, Loss: 0.959006, Accuracy: 79.06%\n",
      "Validation Batch 16, Loss: 0.951343, Accuracy: 79.10%\n",
      "Validation Batch 17, Loss: 1.018571, Accuracy: 78.77%\n",
      "Validation Batch 18, Loss: 0.949374, Accuracy: 78.73%\n",
      "Validation Batch 19, Loss: 0.993203, Accuracy: 78.54%\n",
      "Validation Batch 20, Loss: 0.991851, Accuracy: 78.28%\n",
      "Validation Batch 21, Loss: 0.969909, Accuracy: 78.27%\n",
      "Validation Batch 22, Loss: 0.989124, Accuracy: 78.20%\n",
      "Validation Batch 23, Loss: 1.029233, Accuracy: 77.79%\n",
      "Validation Batch 24, Loss: 0.977526, Accuracy: 77.73%\n",
      "Validation Batch 25, Loss: 0.962241, Accuracy: 77.69%\n",
      "Validation Batch 26, Loss: 0.972364, Accuracy: 77.70%\n",
      "Validation Batch 27, Loss: 0.919169, Accuracy: 77.80%\n",
      "Validation - Epoch 37, Loss: 0.969429, Accuracy: 77.80%\n",
      "Patience—0\n",
      "Epoch 38\n",
      "Batch 1, Loss: 0.931489, Accuracy: 81.25%\n",
      "Batch 2, Loss: 1.066077, Accuracy: 73.44%\n",
      "Batch 3, Loss: 0.985915, Accuracy: 73.96%\n",
      "Batch 4, Loss: 0.918438, Accuracy: 75.78%\n",
      "Batch 5, Loss: 1.015639, Accuracy: 74.69%\n",
      "Batch 6, Loss: 1.050011, Accuracy: 73.70%\n",
      "Batch 7, Loss: 0.953775, Accuracy: 74.33%\n",
      "Batch 8, Loss: 0.994671, Accuracy: 74.41%\n",
      "Batch 9, Loss: 1.010539, Accuracy: 74.13%\n",
      "Batch 10, Loss: 1.013702, Accuracy: 74.06%\n",
      "Batch 11, Loss: 0.946027, Accuracy: 74.57%\n",
      "Batch 12, Loss: 1.081017, Accuracy: 73.83%\n",
      "Batch 13, Loss: 1.128282, Accuracy: 72.84%\n",
      "Batch 14, Loss: 0.995651, Accuracy: 72.99%\n",
      "Batch 15, Loss: 0.957644, Accuracy: 73.33%\n",
      "Batch 16, Loss: 1.036767, Accuracy: 73.14%\n",
      "Batch 17, Loss: 0.957794, Accuracy: 73.53%\n",
      "Batch 18, Loss: 0.992079, Accuracy: 73.70%\n",
      "Batch 19, Loss: 0.935976, Accuracy: 74.01%\n",
      "Batch 20, Loss: 0.882083, Accuracy: 74.69%\n",
      "Batch 21, Loss: 0.939331, Accuracy: 75.00%\n",
      "Batch 22, Loss: 1.012499, Accuracy: 75.00%\n",
      "Batch 23, Loss: 0.934610, Accuracy: 75.27%\n",
      "Batch 24, Loss: 0.913947, Accuracy: 75.65%\n",
      "Batch 25, Loss: 0.955502, Accuracy: 75.75%\n",
      "Batch 26, Loss: 0.976089, Accuracy: 75.84%\n",
      "Batch 27, Loss: 1.016114, Accuracy: 75.75%\n",
      "Batch 28, Loss: 0.903953, Accuracy: 76.12%\n",
      "Batch 29, Loss: 0.932385, Accuracy: 76.40%\n",
      "Batch 30, Loss: 1.040149, Accuracy: 76.09%\n",
      "Batch 31, Loss: 0.975885, Accuracy: 76.16%\n",
      "Batch 32, Loss: 1.035333, Accuracy: 75.93%\n",
      "Batch 33, Loss: 0.936323, Accuracy: 76.09%\n",
      "Batch 34, Loss: 0.985679, Accuracy: 76.06%\n",
      "Batch 35, Loss: 1.018591, Accuracy: 75.94%\n",
      "Batch 36, Loss: 0.948351, Accuracy: 76.04%\n",
      "Batch 37, Loss: 0.959141, Accuracy: 76.14%\n",
      "Batch 38, Loss: 0.964556, Accuracy: 76.19%\n",
      "Batch 39, Loss: 0.899664, Accuracy: 76.44%\n",
      "Batch 40, Loss: 0.961332, Accuracy: 76.45%\n",
      "Batch 41, Loss: 0.936598, Accuracy: 76.56%\n",
      "Batch 42, Loss: 0.918659, Accuracy: 76.75%\n",
      "Batch 43, Loss: 0.946506, Accuracy: 76.78%\n",
      "Batch 44, Loss: 0.972760, Accuracy: 76.78%\n",
      "Batch 45, Loss: 0.924376, Accuracy: 76.91%\n",
      "Batch 46, Loss: 0.933981, Accuracy: 77.00%\n",
      "Batch 47, Loss: 0.990875, Accuracy: 76.99%\n",
      "Batch 48, Loss: 0.919381, Accuracy: 77.08%\n",
      "Batch 49, Loss: 1.050119, Accuracy: 76.91%\n",
      "Batch 50, Loss: 1.026697, Accuracy: 76.78%\n",
      "Batch 51, Loss: 0.993840, Accuracy: 76.69%\n",
      "Batch 52, Loss: 0.906459, Accuracy: 76.77%\n",
      "Batch 53, Loss: 0.974687, Accuracy: 76.77%\n",
      "Batch 54, Loss: 0.939184, Accuracy: 76.79%\n",
      "Batch 55, Loss: 0.951186, Accuracy: 76.85%\n",
      "Batch 56, Loss: 1.020206, Accuracy: 76.76%\n",
      "Batch 57, Loss: 0.958811, Accuracy: 76.78%\n",
      "Batch 58, Loss: 0.926134, Accuracy: 76.83%\n",
      "Batch 59, Loss: 0.963068, Accuracy: 76.85%\n",
      "Batch 60, Loss: 0.876602, Accuracy: 77.03%\n",
      "Batch 61, Loss: 0.961047, Accuracy: 77.02%\n",
      "Batch 62, Loss: 0.969692, Accuracy: 77.02%\n",
      "Batch 63, Loss: 1.010564, Accuracy: 76.96%\n",
      "Batch 64, Loss: 0.955051, Accuracy: 77.00%\n",
      "Batch 65, Loss: 0.999343, Accuracy: 76.97%\n",
      "Batch 66, Loss: 0.895417, Accuracy: 77.08%\n",
      "Batch 67, Loss: 0.922738, Accuracy: 77.15%\n",
      "Batch 68, Loss: 0.926223, Accuracy: 77.23%\n",
      "Batch 69, Loss: 0.933072, Accuracy: 77.29%\n",
      "Batch 70, Loss: 0.888094, Accuracy: 77.41%\n",
      "Batch 71, Loss: 0.967441, Accuracy: 77.42%\n",
      "Batch 72, Loss: 1.021005, Accuracy: 77.34%\n",
      "Batch 73, Loss: 0.922174, Accuracy: 77.40%\n",
      "Batch 74, Loss: 1.017109, Accuracy: 77.32%\n",
      "Batch 75, Loss: 1.017340, Accuracy: 77.27%\n",
      "Batch 76, Loss: 0.856763, Accuracy: 77.43%\n",
      "Batch 77, Loss: 0.997770, Accuracy: 77.37%\n",
      "Batch 78, Loss: 0.958474, Accuracy: 77.38%\n",
      "Batch 79, Loss: 0.911201, Accuracy: 77.45%\n",
      "Batch 80, Loss: 0.969334, Accuracy: 77.44%\n",
      "Batch 81, Loss: 1.008017, Accuracy: 77.41%\n",
      "Batch 82, Loss: 0.942997, Accuracy: 77.44%\n",
      "Batch 83, Loss: 0.983326, Accuracy: 77.45%\n",
      "Batch 84, Loss: 0.966876, Accuracy: 77.46%\n",
      "Batch 85, Loss: 0.899241, Accuracy: 77.52%\n",
      "Batch 86, Loss: 0.967798, Accuracy: 77.53%\n",
      "Batch 87, Loss: 1.032628, Accuracy: 77.46%\n",
      "Batch 88, Loss: 0.940922, Accuracy: 77.50%\n",
      "Batch 89, Loss: 0.883385, Accuracy: 77.63%\n",
      "Batch 90, Loss: 0.885959, Accuracy: 77.71%\n",
      "Batch 91, Loss: 0.942996, Accuracy: 77.75%\n",
      "Batch 92, Loss: 0.943122, Accuracy: 77.79%\n",
      "Batch 93, Loss: 0.950630, Accuracy: 77.77%\n",
      "Batch 94, Loss: 1.019644, Accuracy: 77.71%\n",
      "Batch 95, Loss: 1.073411, Accuracy: 77.58%\n",
      "Batch 96, Loss: 0.898833, Accuracy: 77.65%\n",
      "Batch 97, Loss: 0.979860, Accuracy: 77.64%\n",
      "Batch 98, Loss: 0.960827, Accuracy: 77.61%\n",
      "Batch 99, Loss: 0.899819, Accuracy: 77.70%\n",
      "Batch 100, Loss: 0.964415, Accuracy: 77.72%\n",
      "Batch 101, Loss: 0.951014, Accuracy: 77.74%\n",
      "Batch 102, Loss: 0.983712, Accuracy: 77.71%\n",
      "Batch 103, Loss: 0.897218, Accuracy: 77.78%\n",
      "Batch 104, Loss: 0.862578, Accuracy: 77.88%\n",
      "Batch 105, Loss: 0.913637, Accuracy: 77.95%\n",
      "Batch 106, Loss: 1.004124, Accuracy: 77.87%\n",
      "Batch 107, Loss: 0.960260, Accuracy: 77.91%\n",
      "Batch 108, Loss: 0.918025, Accuracy: 77.95%\n",
      "Batch 109, Loss: 0.929502, Accuracy: 77.98%\n",
      "Batch 110, Loss: 0.984540, Accuracy: 77.95%\n",
      "Batch 111, Loss: 0.926647, Accuracy: 77.98%\n",
      "Batch 112, Loss: 1.105408, Accuracy: 77.85%\n",
      "Batch 113, Loss: 1.086661, Accuracy: 77.74%\n",
      "Batch 114, Loss: 1.030784, Accuracy: 77.67%\n",
      "Batch 115, Loss: 0.938226, Accuracy: 77.69%\n",
      "Batch 116, Loss: 0.935966, Accuracy: 77.72%\n",
      "Batch 117, Loss: 1.040515, Accuracy: 77.66%\n",
      "Batch 118, Loss: 0.913149, Accuracy: 77.69%\n",
      "Batch 119, Loss: 0.965112, Accuracy: 77.70%\n",
      "Batch 120, Loss: 0.988514, Accuracy: 77.70%\n",
      "Batch 121, Loss: 0.957451, Accuracy: 77.70%\n",
      "Batch 122, Loss: 0.963314, Accuracy: 77.70%\n",
      "Batch 123, Loss: 0.991744, Accuracy: 77.67%\n",
      "Batch 124, Loss: 0.912097, Accuracy: 77.72%\n",
      "Batch 125, Loss: 0.930437, Accuracy: 77.76%\n",
      "Batch 126, Loss: 0.924513, Accuracy: 77.79%\n",
      "Batch 127, Loss: 0.940661, Accuracy: 77.82%\n",
      "Batch 128, Loss: 0.980677, Accuracy: 77.82%\n",
      "Batch 129, Loss: 0.939754, Accuracy: 77.83%\n",
      "Batch 130, Loss: 0.943672, Accuracy: 77.84%\n",
      "Batch 131, Loss: 0.893811, Accuracy: 77.90%\n",
      "Batch 132, Loss: 0.947730, Accuracy: 77.91%\n",
      "Batch 133, Loss: 0.976457, Accuracy: 77.91%\n",
      "Batch 134, Loss: 0.895755, Accuracy: 77.96%\n",
      "Batch 135, Loss: 0.995330, Accuracy: 77.93%\n",
      "Batch 136, Loss: 0.944696, Accuracy: 77.95%\n",
      "Batch 137, Loss: 0.932884, Accuracy: 77.99%\n",
      "Batch 138, Loss: 0.979049, Accuracy: 77.96%\n",
      "Batch 139, Loss: 1.055809, Accuracy: 77.88%\n",
      "Batch 140, Loss: 1.013189, Accuracy: 77.83%\n",
      "Batch 141, Loss: 1.035584, Accuracy: 77.78%\n",
      "Batch 142, Loss: 0.988236, Accuracy: 77.76%\n",
      "Batch 143, Loss: 0.940665, Accuracy: 77.80%\n",
      "Batch 144, Loss: 0.983896, Accuracy: 77.80%\n",
      "Batch 145, Loss: 0.935111, Accuracy: 77.83%\n",
      "Batch 146, Loss: 0.941292, Accuracy: 77.85%\n",
      "Batch 147, Loss: 0.946423, Accuracy: 77.86%\n",
      "Batch 148, Loss: 0.940196, Accuracy: 77.88%\n",
      "Batch 149, Loss: 0.918552, Accuracy: 77.92%\n",
      "Batch 150, Loss: 0.949876, Accuracy: 77.94%\n",
      "Batch 151, Loss: 0.868039, Accuracy: 78.01%\n",
      "Batch 152, Loss: 1.052147, Accuracy: 77.95%\n",
      "Batch 153, Loss: 1.038849, Accuracy: 77.90%\n",
      "Batch 154, Loss: 0.943916, Accuracy: 77.89%\n",
      "Batch 155, Loss: 0.984478, Accuracy: 77.88%\n",
      "Batch 156, Loss: 1.004106, Accuracy: 77.84%\n",
      "Batch 157, Loss: 1.025411, Accuracy: 77.80%\n",
      "Batch 158, Loss: 0.958775, Accuracy: 77.80%\n",
      "Batch 159, Loss: 0.965297, Accuracy: 77.80%\n",
      "Batch 160, Loss: 0.960933, Accuracy: 77.80%\n",
      "Batch 161, Loss: 0.931856, Accuracy: 77.83%\n",
      "Batch 162, Loss: 0.960732, Accuracy: 77.84%\n",
      "Batch 163, Loss: 0.897414, Accuracy: 77.88%\n",
      "Batch 164, Loss: 0.987006, Accuracy: 77.87%\n",
      "Batch 165, Loss: 0.980379, Accuracy: 77.86%\n",
      "Batch 166, Loss: 0.951830, Accuracy: 77.86%\n",
      "Batch 167, Loss: 1.009530, Accuracy: 77.83%\n",
      "Batch 168, Loss: 1.013659, Accuracy: 77.79%\n",
      "Batch 169, Loss: 0.967502, Accuracy: 77.80%\n",
      "Batch 170, Loss: 0.937007, Accuracy: 77.81%\n",
      "Batch 171, Loss: 0.995048, Accuracy: 77.80%\n",
      "Batch 172, Loss: 0.864700, Accuracy: 77.86%\n",
      "Batch 173, Loss: 1.030936, Accuracy: 77.82%\n",
      "Batch 174, Loss: 0.973268, Accuracy: 77.78%\n",
      "Batch 175, Loss: 0.991558, Accuracy: 77.78%\n",
      "Batch 176, Loss: 0.952289, Accuracy: 77.79%\n",
      "Batch 177, Loss: 0.951286, Accuracy: 77.79%\n",
      "Batch 178, Loss: 0.982310, Accuracy: 77.77%\n",
      "Batch 179, Loss: 0.967884, Accuracy: 77.78%\n",
      "Batch 180, Loss: 1.003880, Accuracy: 77.76%\n",
      "Batch 181, Loss: 0.952488, Accuracy: 77.75%\n",
      "Batch 182, Loss: 1.036518, Accuracy: 77.70%\n",
      "Batch 183, Loss: 0.991352, Accuracy: 77.69%\n",
      "Batch 184, Loss: 0.941608, Accuracy: 77.69%\n",
      "Batch 185, Loss: 0.917619, Accuracy: 77.73%\n",
      "Batch 186, Loss: 0.966520, Accuracy: 77.73%\n",
      "Batch 187, Loss: 0.938896, Accuracy: 77.75%\n",
      "Batch 188, Loss: 0.957747, Accuracy: 77.76%\n",
      "Batch 189, Loss: 1.025058, Accuracy: 77.72%\n",
      "Batch 190, Loss: 0.842257, Accuracy: 77.80%\n",
      "Batch 191, Loss: 1.012155, Accuracy: 77.79%\n",
      "Batch 192, Loss: 0.949047, Accuracy: 77.79%\n",
      "Batch 193, Loss: 1.045774, Accuracy: 77.76%\n",
      "Batch 194, Loss: 1.093859, Accuracy: 77.70%\n",
      "Batch 195, Loss: 0.945487, Accuracy: 77.71%\n",
      "Batch 196, Loss: 0.949189, Accuracy: 77.72%\n",
      "Batch 197, Loss: 1.010424, Accuracy: 77.69%\n",
      "Batch 198, Loss: 0.942266, Accuracy: 77.70%\n",
      "Batch 199, Loss: 1.051423, Accuracy: 77.66%\n",
      "Batch 200, Loss: 1.036556, Accuracy: 77.63%\n",
      "Batch 201, Loss: 0.992697, Accuracy: 77.62%\n",
      "Batch 202, Loss: 0.954595, Accuracy: 77.63%\n",
      "Batch 203, Loss: 1.071573, Accuracy: 77.57%\n",
      "Batch 204, Loss: 1.012909, Accuracy: 77.54%\n",
      "Batch 205, Loss: 0.993161, Accuracy: 77.52%\n",
      "Batch 206, Loss: 0.912133, Accuracy: 77.56%\n",
      "Batch 207, Loss: 0.943789, Accuracy: 77.58%\n",
      "Batch 208, Loss: 0.936315, Accuracy: 77.59%\n",
      "Batch 209, Loss: 0.929740, Accuracy: 77.61%\n",
      "Batch 210, Loss: 0.980894, Accuracy: 77.60%\n",
      "Batch 211, Loss: 0.990278, Accuracy: 77.59%\n",
      "Batch 212, Loss: 0.953468, Accuracy: 77.60%\n",
      "Batch 213, Loss: 0.941952, Accuracy: 77.62%\n",
      "Training - Epoch 38, Loss: 0.966873, Accuracy: 77.62%\n",
      "Validation Batch 1, Loss: 0.928647, Accuracy: 82.81%\n",
      "Validation Batch 2, Loss: 0.980485, Accuracy: 80.47%\n",
      "Validation Batch 3, Loss: 1.047518, Accuracy: 77.08%\n",
      "Validation Batch 4, Loss: 0.956761, Accuracy: 78.12%\n",
      "Validation Batch 5, Loss: 0.937521, Accuracy: 79.38%\n",
      "Validation Batch 6, Loss: 0.882029, Accuracy: 80.73%\n",
      "Validation Batch 7, Loss: 0.972426, Accuracy: 80.36%\n",
      "Validation Batch 8, Loss: 1.017333, Accuracy: 79.30%\n",
      "Validation Batch 9, Loss: 0.999052, Accuracy: 78.99%\n",
      "Validation Batch 10, Loss: 0.962353, Accuracy: 78.91%\n",
      "Validation Batch 11, Loss: 0.941246, Accuracy: 79.12%\n",
      "Validation Batch 12, Loss: 0.944505, Accuracy: 79.17%\n",
      "Validation Batch 13, Loss: 1.008325, Accuracy: 78.61%\n",
      "Validation Batch 14, Loss: 0.985520, Accuracy: 78.57%\n",
      "Validation Batch 15, Loss: 0.965202, Accuracy: 78.54%\n",
      "Validation Batch 16, Loss: 0.956190, Accuracy: 78.61%\n",
      "Validation Batch 17, Loss: 1.030992, Accuracy: 77.94%\n",
      "Validation Batch 18, Loss: 0.946356, Accuracy: 77.95%\n",
      "Validation Batch 19, Loss: 1.000497, Accuracy: 77.88%\n",
      "Validation Batch 20, Loss: 0.996573, Accuracy: 77.66%\n",
      "Validation Batch 21, Loss: 0.979415, Accuracy: 77.38%\n",
      "Validation Batch 22, Loss: 1.001946, Accuracy: 77.27%\n",
      "Validation Batch 23, Loss: 1.033485, Accuracy: 76.97%\n",
      "Validation Batch 24, Loss: 0.981585, Accuracy: 76.95%\n",
      "Validation Batch 25, Loss: 0.969890, Accuracy: 76.88%\n",
      "Validation Batch 26, Loss: 0.974286, Accuracy: 76.92%\n",
      "Validation Batch 27, Loss: 0.931915, Accuracy: 77.04%\n",
      "Validation - Epoch 38, Loss: 0.975261, Accuracy: 77.04%\n",
      "Patience—1\n",
      "Epoch 39\n",
      "Batch 1, Loss: 0.981926, Accuracy: 76.56%\n",
      "Batch 2, Loss: 0.997339, Accuracy: 75.78%\n",
      "Batch 3, Loss: 0.923514, Accuracy: 77.60%\n",
      "Batch 4, Loss: 0.845399, Accuracy: 80.86%\n",
      "Batch 5, Loss: 0.976178, Accuracy: 79.69%\n",
      "Batch 6, Loss: 0.968420, Accuracy: 79.17%\n",
      "Batch 7, Loss: 0.952314, Accuracy: 79.24%\n",
      "Batch 8, Loss: 0.922603, Accuracy: 79.69%\n",
      "Batch 9, Loss: 1.007110, Accuracy: 78.82%\n",
      "Batch 10, Loss: 1.052568, Accuracy: 77.66%\n",
      "Batch 11, Loss: 0.996084, Accuracy: 77.27%\n",
      "Batch 12, Loss: 0.912252, Accuracy: 77.73%\n",
      "Batch 13, Loss: 0.950804, Accuracy: 78.00%\n",
      "Batch 14, Loss: 0.948834, Accuracy: 78.12%\n",
      "Batch 15, Loss: 0.894215, Accuracy: 78.54%\n",
      "Batch 16, Loss: 0.982906, Accuracy: 78.32%\n",
      "Batch 17, Loss: 1.018488, Accuracy: 77.94%\n",
      "Batch 18, Loss: 0.928354, Accuracy: 78.21%\n",
      "Batch 19, Loss: 0.900347, Accuracy: 78.62%\n",
      "Batch 20, Loss: 0.989869, Accuracy: 78.52%\n",
      "Batch 21, Loss: 0.951831, Accuracy: 78.57%\n",
      "Batch 22, Loss: 0.963832, Accuracy: 78.62%\n",
      "Batch 23, Loss: 0.952498, Accuracy: 78.67%\n",
      "Batch 24, Loss: 0.961062, Accuracy: 78.78%\n",
      "Batch 25, Loss: 0.901666, Accuracy: 79.06%\n",
      "Batch 26, Loss: 1.032194, Accuracy: 78.73%\n",
      "Batch 27, Loss: 0.907890, Accuracy: 78.88%\n",
      "Batch 28, Loss: 0.938722, Accuracy: 78.91%\n",
      "Batch 29, Loss: 0.963106, Accuracy: 78.88%\n",
      "Batch 30, Loss: 0.862221, Accuracy: 79.17%\n",
      "Batch 31, Loss: 0.973589, Accuracy: 79.08%\n",
      "Batch 32, Loss: 0.945280, Accuracy: 79.10%\n",
      "Batch 33, Loss: 0.961008, Accuracy: 79.02%\n",
      "Batch 34, Loss: 0.961108, Accuracy: 79.00%\n",
      "Batch 35, Loss: 0.904861, Accuracy: 79.15%\n",
      "Batch 36, Loss: 0.952819, Accuracy: 79.17%\n",
      "Batch 37, Loss: 0.874671, Accuracy: 79.39%\n",
      "Batch 38, Loss: 0.949483, Accuracy: 79.40%\n",
      "Batch 39, Loss: 0.935995, Accuracy: 79.49%\n",
      "Batch 40, Loss: 0.996000, Accuracy: 79.34%\n",
      "Batch 41, Loss: 1.080452, Accuracy: 79.00%\n",
      "Batch 42, Loss: 0.991227, Accuracy: 78.87%\n",
      "Batch 43, Loss: 1.016862, Accuracy: 78.74%\n",
      "Batch 44, Loss: 1.005643, Accuracy: 78.66%\n",
      "Batch 45, Loss: 1.074630, Accuracy: 78.37%\n",
      "Batch 46, Loss: 0.894927, Accuracy: 78.50%\n",
      "Batch 47, Loss: 0.950968, Accuracy: 78.52%\n",
      "Batch 48, Loss: 0.977223, Accuracy: 78.52%\n",
      "Batch 49, Loss: 0.904837, Accuracy: 78.64%\n",
      "Batch 50, Loss: 0.953364, Accuracy: 78.62%\n",
      "Batch 51, Loss: 0.934916, Accuracy: 78.71%\n",
      "Batch 52, Loss: 0.967418, Accuracy: 78.70%\n",
      "Batch 53, Loss: 0.950377, Accuracy: 78.71%\n",
      "Batch 54, Loss: 0.985878, Accuracy: 78.65%\n",
      "Batch 55, Loss: 0.914834, Accuracy: 78.72%\n",
      "Batch 56, Loss: 0.954470, Accuracy: 78.74%\n",
      "Batch 57, Loss: 0.990585, Accuracy: 78.67%\n",
      "Batch 58, Loss: 0.883813, Accuracy: 78.83%\n",
      "Batch 59, Loss: 0.988309, Accuracy: 78.73%\n",
      "Batch 60, Loss: 0.970321, Accuracy: 78.72%\n",
      "Batch 61, Loss: 0.987348, Accuracy: 78.69%\n",
      "Batch 62, Loss: 0.987701, Accuracy: 78.63%\n",
      "Batch 63, Loss: 1.037352, Accuracy: 78.50%\n",
      "Batch 64, Loss: 0.973462, Accuracy: 78.47%\n",
      "Batch 65, Loss: 0.886621, Accuracy: 78.58%\n",
      "Batch 66, Loss: 0.944185, Accuracy: 78.60%\n",
      "Batch 67, Loss: 0.987678, Accuracy: 78.54%\n",
      "Batch 68, Loss: 0.954037, Accuracy: 78.56%\n",
      "Batch 69, Loss: 0.898887, Accuracy: 78.65%\n",
      "Batch 70, Loss: 0.935129, Accuracy: 78.68%\n",
      "Batch 71, Loss: 0.880585, Accuracy: 78.79%\n",
      "Batch 72, Loss: 0.927277, Accuracy: 78.82%\n",
      "Batch 73, Loss: 1.004918, Accuracy: 78.77%\n",
      "Batch 74, Loss: 0.947777, Accuracy: 78.80%\n",
      "Batch 75, Loss: 0.986070, Accuracy: 78.75%\n",
      "Batch 76, Loss: 1.015745, Accuracy: 78.66%\n",
      "Batch 77, Loss: 0.991512, Accuracy: 78.63%\n",
      "Batch 78, Loss: 1.048770, Accuracy: 78.53%\n",
      "Batch 79, Loss: 0.958620, Accuracy: 78.50%\n",
      "Batch 80, Loss: 0.918211, Accuracy: 78.55%\n",
      "Batch 81, Loss: 0.968763, Accuracy: 78.55%\n",
      "Batch 82, Loss: 1.022977, Accuracy: 78.43%\n",
      "Batch 83, Loss: 0.862814, Accuracy: 78.56%\n",
      "Batch 84, Loss: 0.990617, Accuracy: 78.50%\n",
      "Batch 85, Loss: 0.935768, Accuracy: 78.53%\n",
      "Batch 86, Loss: 0.922209, Accuracy: 78.58%\n",
      "Batch 87, Loss: 0.965808, Accuracy: 78.59%\n",
      "Batch 88, Loss: 0.961735, Accuracy: 78.57%\n",
      "Batch 89, Loss: 0.888960, Accuracy: 78.65%\n",
      "Batch 90, Loss: 1.026899, Accuracy: 78.54%\n",
      "Batch 91, Loss: 0.974654, Accuracy: 78.50%\n",
      "Batch 92, Loss: 1.034843, Accuracy: 78.43%\n",
      "Batch 93, Loss: 1.077726, Accuracy: 78.29%\n",
      "Batch 94, Loss: 0.971464, Accuracy: 78.27%\n",
      "Batch 95, Loss: 0.955762, Accuracy: 78.27%\n",
      "Batch 96, Loss: 0.928569, Accuracy: 78.32%\n",
      "Batch 97, Loss: 0.869097, Accuracy: 78.41%\n",
      "Batch 98, Loss: 0.915733, Accuracy: 78.48%\n",
      "Batch 99, Loss: 0.979017, Accuracy: 78.46%\n",
      "Batch 100, Loss: 0.980062, Accuracy: 78.44%\n",
      "Batch 101, Loss: 0.978063, Accuracy: 78.42%\n",
      "Batch 102, Loss: 0.972934, Accuracy: 78.40%\n",
      "Batch 103, Loss: 0.959777, Accuracy: 78.40%\n",
      "Batch 104, Loss: 1.023221, Accuracy: 78.32%\n",
      "Batch 105, Loss: 0.986360, Accuracy: 78.30%\n",
      "Batch 106, Loss: 1.007970, Accuracy: 78.26%\n",
      "Batch 107, Loss: 0.979502, Accuracy: 78.24%\n",
      "Batch 108, Loss: 1.031548, Accuracy: 78.15%\n",
      "Batch 109, Loss: 1.007068, Accuracy: 78.10%\n",
      "Batch 110, Loss: 1.042887, Accuracy: 78.01%\n",
      "Batch 111, Loss: 0.987934, Accuracy: 77.98%\n",
      "Batch 112, Loss: 0.901178, Accuracy: 78.04%\n",
      "Batch 113, Loss: 0.956561, Accuracy: 78.06%\n",
      "Batch 114, Loss: 0.906026, Accuracy: 78.11%\n",
      "Batch 115, Loss: 0.957358, Accuracy: 78.11%\n",
      "Batch 116, Loss: 1.006053, Accuracy: 78.08%\n",
      "Batch 117, Loss: 0.954212, Accuracy: 78.10%\n",
      "Batch 118, Loss: 0.987361, Accuracy: 78.09%\n",
      "Batch 119, Loss: 0.936858, Accuracy: 78.11%\n",
      "Batch 120, Loss: 0.885109, Accuracy: 78.18%\n",
      "Batch 121, Loss: 0.987261, Accuracy: 78.15%\n",
      "Batch 122, Loss: 0.993612, Accuracy: 78.10%\n",
      "Batch 123, Loss: 0.903884, Accuracy: 78.16%\n",
      "Batch 124, Loss: 1.055608, Accuracy: 78.10%\n",
      "Batch 125, Loss: 0.917025, Accuracy: 78.14%\n",
      "Batch 126, Loss: 1.003613, Accuracy: 78.10%\n",
      "Batch 127, Loss: 0.863428, Accuracy: 78.17%\n",
      "Batch 128, Loss: 0.953733, Accuracy: 78.19%\n",
      "Batch 129, Loss: 0.933274, Accuracy: 78.20%\n",
      "Batch 130, Loss: 1.012133, Accuracy: 78.17%\n",
      "Batch 131, Loss: 0.978161, Accuracy: 78.17%\n",
      "Batch 132, Loss: 0.977888, Accuracy: 78.17%\n",
      "Batch 133, Loss: 0.900456, Accuracy: 78.22%\n",
      "Batch 134, Loss: 0.981823, Accuracy: 78.22%\n",
      "Batch 135, Loss: 0.957521, Accuracy: 78.23%\n",
      "Batch 136, Loss: 0.998833, Accuracy: 78.22%\n",
      "Batch 137, Loss: 1.017246, Accuracy: 78.17%\n",
      "Batch 138, Loss: 0.986435, Accuracy: 78.16%\n",
      "Batch 139, Loss: 1.010901, Accuracy: 78.12%\n",
      "Batch 140, Loss: 0.945699, Accuracy: 78.14%\n",
      "Batch 141, Loss: 1.085772, Accuracy: 78.05%\n",
      "Batch 142, Loss: 0.945215, Accuracy: 78.06%\n",
      "Batch 143, Loss: 0.883859, Accuracy: 78.12%\n",
      "Batch 144, Loss: 0.964218, Accuracy: 78.12%\n",
      "Batch 145, Loss: 1.019357, Accuracy: 78.09%\n",
      "Batch 146, Loss: 1.002022, Accuracy: 78.06%\n",
      "Batch 147, Loss: 0.923652, Accuracy: 78.09%\n",
      "Batch 148, Loss: 0.997949, Accuracy: 78.07%\n",
      "Batch 149, Loss: 0.961195, Accuracy: 78.06%\n",
      "Batch 150, Loss: 0.956362, Accuracy: 78.07%\n",
      "Batch 151, Loss: 0.957759, Accuracy: 78.08%\n",
      "Batch 152, Loss: 0.913980, Accuracy: 78.11%\n",
      "Batch 153, Loss: 1.016985, Accuracy: 78.07%\n",
      "Batch 154, Loss: 0.962159, Accuracy: 78.07%\n",
      "Batch 155, Loss: 0.946505, Accuracy: 78.08%\n",
      "Batch 156, Loss: 0.979841, Accuracy: 78.09%\n",
      "Batch 157, Loss: 0.976395, Accuracy: 78.09%\n",
      "Batch 158, Loss: 0.971372, Accuracy: 78.08%\n",
      "Batch 159, Loss: 0.893772, Accuracy: 78.12%\n",
      "Batch 160, Loss: 1.053778, Accuracy: 78.06%\n",
      "Batch 161, Loss: 0.907406, Accuracy: 78.09%\n",
      "Batch 162, Loss: 1.013025, Accuracy: 78.05%\n",
      "Batch 163, Loss: 0.985545, Accuracy: 78.04%\n",
      "Batch 164, Loss: 0.945390, Accuracy: 78.05%\n",
      "Batch 165, Loss: 0.958525, Accuracy: 78.04%\n",
      "Batch 166, Loss: 0.984756, Accuracy: 78.03%\n",
      "Batch 167, Loss: 0.972773, Accuracy: 78.03%\n",
      "Batch 168, Loss: 1.030656, Accuracy: 77.99%\n",
      "Batch 169, Loss: 1.000345, Accuracy: 77.97%\n",
      "Batch 170, Loss: 0.994889, Accuracy: 77.95%\n",
      "Batch 171, Loss: 0.899288, Accuracy: 77.99%\n",
      "Batch 172, Loss: 0.941078, Accuracy: 78.00%\n",
      "Batch 173, Loss: 0.918205, Accuracy: 78.02%\n",
      "Batch 174, Loss: 0.941947, Accuracy: 78.04%\n",
      "Batch 175, Loss: 0.999570, Accuracy: 78.02%\n",
      "Batch 176, Loss: 0.964796, Accuracy: 78.02%\n",
      "Batch 177, Loss: 1.021874, Accuracy: 77.99%\n",
      "Batch 178, Loss: 1.028067, Accuracy: 77.95%\n",
      "Batch 179, Loss: 1.047018, Accuracy: 77.90%\n",
      "Batch 180, Loss: 0.905836, Accuracy: 77.93%\n",
      "Batch 181, Loss: 1.009692, Accuracy: 77.91%\n",
      "Batch 182, Loss: 0.927313, Accuracy: 77.93%\n",
      "Batch 183, Loss: 1.007723, Accuracy: 77.89%\n",
      "Batch 184, Loss: 0.994362, Accuracy: 77.88%\n",
      "Batch 185, Loss: 0.996909, Accuracy: 77.86%\n",
      "Batch 186, Loss: 0.953339, Accuracy: 77.86%\n",
      "Batch 187, Loss: 0.969058, Accuracy: 77.87%\n",
      "Batch 188, Loss: 1.009763, Accuracy: 77.84%\n",
      "Batch 189, Loss: 0.939850, Accuracy: 77.86%\n",
      "Batch 190, Loss: 0.958712, Accuracy: 77.87%\n",
      "Batch 191, Loss: 0.915011, Accuracy: 77.90%\n",
      "Batch 192, Loss: 0.986489, Accuracy: 77.88%\n",
      "Batch 193, Loss: 0.932442, Accuracy: 77.90%\n",
      "Batch 194, Loss: 0.959189, Accuracy: 77.91%\n",
      "Batch 195, Loss: 0.936344, Accuracy: 77.92%\n",
      "Batch 196, Loss: 0.996376, Accuracy: 77.90%\n",
      "Batch 197, Loss: 0.996505, Accuracy: 77.88%\n",
      "Batch 198, Loss: 1.024530, Accuracy: 77.84%\n",
      "Batch 199, Loss: 1.013829, Accuracy: 77.81%\n",
      "Batch 200, Loss: 0.948432, Accuracy: 77.82%\n",
      "Batch 201, Loss: 0.962703, Accuracy: 77.81%\n",
      "Batch 202, Loss: 0.979814, Accuracy: 77.82%\n",
      "Batch 203, Loss: 0.994740, Accuracy: 77.80%\n",
      "Batch 204, Loss: 0.951053, Accuracy: 77.82%\n",
      "Batch 205, Loss: 0.963218, Accuracy: 77.81%\n",
      "Batch 206, Loss: 0.988124, Accuracy: 77.80%\n",
      "Batch 207, Loss: 1.088639, Accuracy: 77.73%\n",
      "Batch 208, Loss: 0.918057, Accuracy: 77.76%\n",
      "Batch 209, Loss: 1.024898, Accuracy: 77.74%\n",
      "Batch 210, Loss: 1.014307, Accuracy: 77.72%\n",
      "Batch 211, Loss: 0.996404, Accuracy: 77.71%\n",
      "Batch 212, Loss: 0.977647, Accuracy: 77.70%\n",
      "Batch 213, Loss: 0.928289, Accuracy: 77.73%\n",
      "Training - Epoch 39, Loss: 0.966757, Accuracy: 77.73%\n",
      "Validation Batch 1, Loss: 0.913604, Accuracy: 82.81%\n",
      "Validation Batch 2, Loss: 0.957355, Accuracy: 82.03%\n",
      "Validation Batch 3, Loss: 1.014315, Accuracy: 79.17%\n",
      "Validation Batch 4, Loss: 0.933949, Accuracy: 80.08%\n",
      "Validation Batch 5, Loss: 0.911060, Accuracy: 80.94%\n",
      "Validation Batch 6, Loss: 0.857763, Accuracy: 82.29%\n",
      "Validation Batch 7, Loss: 0.957766, Accuracy: 82.14%\n",
      "Validation Batch 8, Loss: 1.006780, Accuracy: 80.86%\n",
      "Validation Batch 9, Loss: 0.974051, Accuracy: 80.56%\n",
      "Validation Batch 10, Loss: 0.944684, Accuracy: 80.47%\n",
      "Validation Batch 11, Loss: 0.913580, Accuracy: 80.68%\n",
      "Validation Batch 12, Loss: 0.917617, Accuracy: 80.99%\n",
      "Validation Batch 13, Loss: 0.980206, Accuracy: 80.53%\n",
      "Validation Batch 14, Loss: 0.952423, Accuracy: 80.47%\n",
      "Validation Batch 15, Loss: 0.940327, Accuracy: 80.42%\n",
      "Validation Batch 16, Loss: 0.937891, Accuracy: 80.47%\n",
      "Validation Batch 17, Loss: 0.992464, Accuracy: 80.33%\n",
      "Validation Batch 18, Loss: 0.931546, Accuracy: 80.21%\n",
      "Validation Batch 19, Loss: 0.977583, Accuracy: 80.10%\n",
      "Validation Batch 20, Loss: 0.958810, Accuracy: 79.84%\n",
      "Validation Batch 21, Loss: 0.948540, Accuracy: 79.84%\n",
      "Validation Batch 22, Loss: 0.961712, Accuracy: 79.83%\n",
      "Validation Batch 23, Loss: 0.994081, Accuracy: 79.62%\n",
      "Validation Batch 24, Loss: 0.965125, Accuracy: 79.56%\n",
      "Validation Batch 25, Loss: 0.924434, Accuracy: 79.62%\n",
      "Validation Batch 26, Loss: 0.950283, Accuracy: 79.69%\n",
      "Validation Batch 27, Loss: 0.888377, Accuracy: 79.86%\n",
      "Validation - Epoch 39, Loss: 0.948382, Accuracy: 79.86%\n",
      "Patience—0\n",
      "Epoch 40\n",
      "Batch 1, Loss: 0.924231, Accuracy: 81.25%\n",
      "Batch 2, Loss: 0.949333, Accuracy: 80.47%\n",
      "Batch 3, Loss: 0.917333, Accuracy: 81.25%\n",
      "Batch 4, Loss: 0.973043, Accuracy: 80.08%\n",
      "Batch 5, Loss: 1.007421, Accuracy: 78.75%\n",
      "Batch 6, Loss: 1.044039, Accuracy: 77.34%\n",
      "Batch 7, Loss: 0.969169, Accuracy: 77.01%\n",
      "Batch 8, Loss: 0.932032, Accuracy: 77.73%\n",
      "Batch 9, Loss: 1.050842, Accuracy: 76.91%\n",
      "Batch 10, Loss: 0.959094, Accuracy: 77.03%\n",
      "Batch 11, Loss: 0.964133, Accuracy: 76.99%\n",
      "Batch 12, Loss: 0.903480, Accuracy: 77.60%\n",
      "Batch 13, Loss: 0.948541, Accuracy: 77.64%\n",
      "Batch 14, Loss: 0.941487, Accuracy: 77.90%\n",
      "Batch 15, Loss: 1.022524, Accuracy: 77.50%\n",
      "Batch 16, Loss: 0.995837, Accuracy: 77.34%\n",
      "Batch 17, Loss: 0.948936, Accuracy: 77.48%\n",
      "Batch 18, Loss: 0.937373, Accuracy: 77.69%\n",
      "Batch 19, Loss: 0.961847, Accuracy: 77.80%\n",
      "Batch 20, Loss: 0.936221, Accuracy: 78.05%\n",
      "Batch 21, Loss: 1.073100, Accuracy: 77.53%\n",
      "Batch 22, Loss: 0.967763, Accuracy: 77.56%\n",
      "Batch 23, Loss: 0.994151, Accuracy: 77.45%\n",
      "Batch 24, Loss: 0.937951, Accuracy: 77.60%\n",
      "Batch 25, Loss: 0.922077, Accuracy: 77.81%\n",
      "Batch 26, Loss: 0.894752, Accuracy: 77.94%\n",
      "Batch 27, Loss: 1.018252, Accuracy: 77.84%\n",
      "Batch 28, Loss: 0.950025, Accuracy: 77.90%\n",
      "Batch 29, Loss: 0.998633, Accuracy: 77.80%\n",
      "Batch 30, Loss: 1.028641, Accuracy: 77.55%\n",
      "Batch 31, Loss: 0.983035, Accuracy: 77.52%\n",
      "Batch 32, Loss: 1.029806, Accuracy: 77.34%\n",
      "Batch 33, Loss: 0.882961, Accuracy: 77.60%\n",
      "Batch 34, Loss: 0.906765, Accuracy: 77.85%\n",
      "Batch 35, Loss: 0.892184, Accuracy: 78.12%\n",
      "Batch 36, Loss: 0.893579, Accuracy: 78.34%\n",
      "Batch 37, Loss: 0.868376, Accuracy: 78.63%\n",
      "Batch 38, Loss: 0.939548, Accuracy: 78.70%\n",
      "Batch 39, Loss: 0.983829, Accuracy: 78.65%\n",
      "Batch 40, Loss: 0.920559, Accuracy: 78.75%\n",
      "Batch 41, Loss: 0.935088, Accuracy: 78.81%\n",
      "Batch 42, Loss: 0.896838, Accuracy: 78.94%\n",
      "Batch 43, Loss: 0.904770, Accuracy: 79.11%\n",
      "Batch 44, Loss: 0.970915, Accuracy: 79.05%\n",
      "Batch 45, Loss: 0.932507, Accuracy: 79.06%\n",
      "Batch 46, Loss: 0.896341, Accuracy: 79.21%\n",
      "Batch 47, Loss: 0.964725, Accuracy: 79.19%\n",
      "Batch 48, Loss: 1.004674, Accuracy: 79.07%\n",
      "Batch 49, Loss: 0.947609, Accuracy: 78.99%\n",
      "Batch 50, Loss: 0.986018, Accuracy: 78.94%\n",
      "Batch 51, Loss: 0.923593, Accuracy: 78.98%\n",
      "Batch 52, Loss: 0.869845, Accuracy: 79.15%\n",
      "Batch 53, Loss: 0.937474, Accuracy: 79.19%\n",
      "Batch 54, Loss: 0.911618, Accuracy: 79.28%\n",
      "Batch 55, Loss: 0.992022, Accuracy: 79.18%\n",
      "Batch 56, Loss: 1.008368, Accuracy: 79.10%\n",
      "Batch 57, Loss: 1.027412, Accuracy: 78.97%\n",
      "Batch 58, Loss: 1.014075, Accuracy: 78.85%\n",
      "Batch 59, Loss: 0.940341, Accuracy: 78.89%\n",
      "Batch 60, Loss: 1.007903, Accuracy: 78.80%\n",
      "Batch 61, Loss: 0.985542, Accuracy: 78.74%\n",
      "Batch 62, Loss: 0.931765, Accuracy: 78.81%\n",
      "Batch 63, Loss: 0.930756, Accuracy: 78.84%\n",
      "Batch 64, Loss: 1.024077, Accuracy: 78.71%\n",
      "Batch 65, Loss: 0.891833, Accuracy: 78.82%\n",
      "Batch 66, Loss: 0.995009, Accuracy: 78.72%\n",
      "Batch 67, Loss: 1.050846, Accuracy: 78.57%\n",
      "Batch 68, Loss: 1.022453, Accuracy: 78.45%\n",
      "Batch 69, Loss: 0.946875, Accuracy: 78.49%\n",
      "Batch 70, Loss: 0.991823, Accuracy: 78.44%\n",
      "Batch 71, Loss: 0.948642, Accuracy: 78.46%\n",
      "Batch 72, Loss: 0.980326, Accuracy: 78.43%\n",
      "Batch 73, Loss: 0.913593, Accuracy: 78.51%\n",
      "Batch 74, Loss: 1.053048, Accuracy: 78.40%\n",
      "Batch 75, Loss: 0.981554, Accuracy: 78.35%\n",
      "Batch 76, Loss: 0.964634, Accuracy: 78.33%\n",
      "Batch 77, Loss: 1.040343, Accuracy: 78.21%\n",
      "Batch 78, Loss: 0.980884, Accuracy: 78.19%\n",
      "Batch 79, Loss: 0.984307, Accuracy: 78.14%\n",
      "Batch 80, Loss: 0.877262, Accuracy: 78.24%\n",
      "Batch 81, Loss: 0.950516, Accuracy: 78.28%\n",
      "Batch 82, Loss: 0.981061, Accuracy: 78.26%\n",
      "Batch 83, Loss: 0.988150, Accuracy: 78.22%\n",
      "Batch 84, Loss: 0.914751, Accuracy: 78.27%\n",
      "Batch 85, Loss: 1.072699, Accuracy: 78.12%\n",
      "Batch 86, Loss: 0.963737, Accuracy: 78.11%\n",
      "Batch 87, Loss: 0.939672, Accuracy: 78.14%\n",
      "Batch 88, Loss: 0.870784, Accuracy: 78.25%\n",
      "Batch 89, Loss: 0.929101, Accuracy: 78.30%\n",
      "Batch 90, Loss: 0.941333, Accuracy: 78.32%\n",
      "Batch 91, Loss: 0.863581, Accuracy: 78.43%\n",
      "Batch 92, Loss: 0.916807, Accuracy: 78.48%\n",
      "Batch 93, Loss: 1.091156, Accuracy: 78.34%\n",
      "Batch 94, Loss: 0.984917, Accuracy: 78.32%\n",
      "Batch 95, Loss: 0.954525, Accuracy: 78.34%\n",
      "Batch 96, Loss: 0.960694, Accuracy: 78.34%\n",
      "Batch 97, Loss: 0.955404, Accuracy: 78.38%\n",
      "Batch 98, Loss: 1.010814, Accuracy: 78.33%\n",
      "Batch 99, Loss: 0.929311, Accuracy: 78.38%\n",
      "Batch 100, Loss: 0.935234, Accuracy: 78.41%\n",
      "Batch 101, Loss: 0.951403, Accuracy: 78.42%\n",
      "Batch 102, Loss: 0.923388, Accuracy: 78.45%\n",
      "Batch 103, Loss: 0.985275, Accuracy: 78.38%\n",
      "Batch 104, Loss: 0.947289, Accuracy: 78.40%\n",
      "Batch 105, Loss: 1.001475, Accuracy: 78.33%\n",
      "Batch 106, Loss: 0.994907, Accuracy: 78.30%\n",
      "Batch 107, Loss: 0.981530, Accuracy: 78.29%\n",
      "Batch 108, Loss: 1.088823, Accuracy: 78.17%\n",
      "Batch 109, Loss: 0.901576, Accuracy: 78.21%\n",
      "Batch 110, Loss: 0.983892, Accuracy: 78.20%\n",
      "Batch 111, Loss: 0.963241, Accuracy: 78.22%\n",
      "Batch 112, Loss: 1.064094, Accuracy: 78.12%\n",
      "Batch 113, Loss: 0.957315, Accuracy: 78.12%\n",
      "Batch 114, Loss: 0.891477, Accuracy: 78.18%\n",
      "Batch 115, Loss: 0.878164, Accuracy: 78.26%\n",
      "Batch 116, Loss: 0.949783, Accuracy: 78.27%\n",
      "Batch 117, Loss: 0.922183, Accuracy: 78.31%\n",
      "Batch 118, Loss: 0.948190, Accuracy: 78.31%\n",
      "Batch 119, Loss: 0.947380, Accuracy: 78.32%\n",
      "Batch 120, Loss: 0.952611, Accuracy: 78.31%\n",
      "Batch 121, Loss: 1.017262, Accuracy: 78.25%\n",
      "Batch 122, Loss: 0.874661, Accuracy: 78.32%\n",
      "Batch 123, Loss: 0.946119, Accuracy: 78.33%\n",
      "Batch 124, Loss: 1.082551, Accuracy: 78.21%\n",
      "Batch 125, Loss: 0.950697, Accuracy: 78.21%\n",
      "Batch 126, Loss: 0.877888, Accuracy: 78.27%\n",
      "Batch 127, Loss: 0.871109, Accuracy: 78.35%\n",
      "Batch 128, Loss: 0.978733, Accuracy: 78.33%\n",
      "Batch 129, Loss: 1.034492, Accuracy: 78.27%\n",
      "Batch 130, Loss: 1.012487, Accuracy: 78.22%\n",
      "Batch 131, Loss: 0.989804, Accuracy: 78.20%\n",
      "Batch 132, Loss: 0.955258, Accuracy: 78.21%\n",
      "Batch 133, Loss: 0.933799, Accuracy: 78.22%\n",
      "Batch 134, Loss: 0.865756, Accuracy: 78.29%\n",
      "Batch 135, Loss: 0.920034, Accuracy: 78.32%\n",
      "Batch 136, Loss: 0.966822, Accuracy: 78.32%\n",
      "Batch 137, Loss: 0.977943, Accuracy: 78.31%\n",
      "Batch 138, Loss: 0.955003, Accuracy: 78.31%\n",
      "Batch 139, Loss: 0.876012, Accuracy: 78.37%\n",
      "Batch 140, Loss: 0.995059, Accuracy: 78.34%\n",
      "Batch 141, Loss: 1.009077, Accuracy: 78.29%\n",
      "Batch 142, Loss: 0.946576, Accuracy: 78.29%\n",
      "Batch 143, Loss: 0.941688, Accuracy: 78.30%\n",
      "Batch 144, Loss: 0.911408, Accuracy: 78.34%\n",
      "Batch 145, Loss: 0.964106, Accuracy: 78.34%\n",
      "Batch 146, Loss: 1.003306, Accuracy: 78.32%\n",
      "Batch 147, Loss: 0.897278, Accuracy: 78.36%\n",
      "Batch 148, Loss: 0.959708, Accuracy: 78.36%\n",
      "Batch 149, Loss: 1.031557, Accuracy: 78.29%\n",
      "Batch 150, Loss: 0.961660, Accuracy: 78.30%\n",
      "Batch 151, Loss: 0.943443, Accuracy: 78.30%\n",
      "Batch 152, Loss: 0.969682, Accuracy: 78.29%\n",
      "Batch 153, Loss: 1.069605, Accuracy: 78.22%\n",
      "Batch 154, Loss: 0.958535, Accuracy: 78.23%\n",
      "Batch 155, Loss: 0.932673, Accuracy: 78.24%\n",
      "Batch 156, Loss: 0.969295, Accuracy: 78.25%\n",
      "Batch 157, Loss: 0.938319, Accuracy: 78.25%\n",
      "Batch 158, Loss: 0.963561, Accuracy: 78.25%\n",
      "Batch 159, Loss: 0.939936, Accuracy: 78.25%\n",
      "Batch 160, Loss: 1.002319, Accuracy: 78.22%\n",
      "Batch 161, Loss: 0.974706, Accuracy: 78.21%\n",
      "Batch 162, Loss: 0.930574, Accuracy: 78.22%\n",
      "Batch 163, Loss: 1.052959, Accuracy: 78.15%\n",
      "Batch 164, Loss: 1.063388, Accuracy: 78.10%\n",
      "Batch 165, Loss: 0.983503, Accuracy: 78.08%\n",
      "Batch 166, Loss: 1.041725, Accuracy: 78.03%\n",
      "Batch 167, Loss: 1.021582, Accuracy: 77.98%\n",
      "Batch 168, Loss: 0.963365, Accuracy: 77.99%\n",
      "Batch 169, Loss: 0.959666, Accuracy: 77.99%\n",
      "Batch 170, Loss: 1.018742, Accuracy: 77.94%\n",
      "Batch 171, Loss: 0.992649, Accuracy: 77.92%\n",
      "Batch 172, Loss: 0.915532, Accuracy: 77.96%\n",
      "Batch 173, Loss: 0.918735, Accuracy: 77.99%\n",
      "Batch 174, Loss: 1.016798, Accuracy: 77.95%\n",
      "Batch 175, Loss: 0.957290, Accuracy: 77.95%\n",
      "Batch 176, Loss: 0.924422, Accuracy: 77.97%\n",
      "Batch 177, Loss: 0.942007, Accuracy: 77.98%\n",
      "Batch 178, Loss: 0.987250, Accuracy: 77.96%\n",
      "Batch 179, Loss: 0.969614, Accuracy: 77.95%\n",
      "Batch 180, Loss: 1.027344, Accuracy: 77.92%\n",
      "Batch 181, Loss: 0.946814, Accuracy: 77.92%\n",
      "Batch 182, Loss: 0.899261, Accuracy: 77.97%\n",
      "Batch 183, Loss: 0.986475, Accuracy: 77.95%\n",
      "Batch 184, Loss: 0.963111, Accuracy: 77.96%\n",
      "Batch 185, Loss: 0.965526, Accuracy: 77.95%\n",
      "Batch 186, Loss: 0.956665, Accuracy: 77.96%\n",
      "Batch 187, Loss: 0.896060, Accuracy: 78.00%\n",
      "Batch 188, Loss: 0.969678, Accuracy: 78.00%\n",
      "Batch 189, Loss: 0.982289, Accuracy: 77.99%\n",
      "Batch 190, Loss: 0.965170, Accuracy: 78.00%\n",
      "Batch 191, Loss: 0.956048, Accuracy: 78.00%\n",
      "Batch 192, Loss: 0.969380, Accuracy: 78.00%\n",
      "Batch 193, Loss: 0.890492, Accuracy: 78.04%\n",
      "Batch 194, Loss: 0.982635, Accuracy: 78.04%\n",
      "Batch 195, Loss: 0.954632, Accuracy: 78.04%\n",
      "Batch 196, Loss: 0.921027, Accuracy: 78.06%\n",
      "Batch 197, Loss: 0.951545, Accuracy: 78.07%\n",
      "Batch 198, Loss: 1.032314, Accuracy: 78.05%\n",
      "Batch 199, Loss: 0.926105, Accuracy: 78.06%\n",
      "Batch 200, Loss: 0.931556, Accuracy: 78.08%\n",
      "Batch 201, Loss: 0.991287, Accuracy: 78.07%\n",
      "Batch 202, Loss: 0.982573, Accuracy: 78.07%\n",
      "Batch 203, Loss: 0.966425, Accuracy: 78.06%\n",
      "Batch 204, Loss: 1.073671, Accuracy: 78.01%\n",
      "Batch 205, Loss: 0.952057, Accuracy: 78.03%\n",
      "Batch 206, Loss: 0.901095, Accuracy: 78.06%\n",
      "Batch 207, Loss: 1.018084, Accuracy: 78.04%\n",
      "Batch 208, Loss: 1.006174, Accuracy: 78.03%\n",
      "Batch 209, Loss: 0.926215, Accuracy: 78.05%\n",
      "Batch 210, Loss: 0.969766, Accuracy: 78.04%\n",
      "Batch 211, Loss: 0.925208, Accuracy: 78.06%\n",
      "Batch 212, Loss: 1.004393, Accuracy: 78.04%\n",
      "Batch 213, Loss: 1.015479, Accuracy: 78.01%\n",
      "Training - Epoch 40, Loss: 0.963217, Accuracy: 78.01%\n",
      "Validation Batch 1, Loss: 0.922002, Accuracy: 82.81%\n",
      "Validation Batch 2, Loss: 0.970465, Accuracy: 80.47%\n",
      "Validation Batch 3, Loss: 1.036646, Accuracy: 77.60%\n",
      "Validation Batch 4, Loss: 0.948739, Accuracy: 78.52%\n",
      "Validation Batch 5, Loss: 0.922660, Accuracy: 79.69%\n",
      "Validation Batch 6, Loss: 0.873121, Accuracy: 80.99%\n",
      "Validation Batch 7, Loss: 0.970163, Accuracy: 80.58%\n",
      "Validation Batch 8, Loss: 1.013570, Accuracy: 79.49%\n",
      "Validation Batch 9, Loss: 0.984583, Accuracy: 79.34%\n",
      "Validation Batch 10, Loss: 0.954385, Accuracy: 79.38%\n",
      "Validation Batch 11, Loss: 0.929285, Accuracy: 79.55%\n",
      "Validation Batch 12, Loss: 0.937347, Accuracy: 79.69%\n",
      "Validation Batch 13, Loss: 0.993071, Accuracy: 79.33%\n",
      "Validation Batch 14, Loss: 0.977462, Accuracy: 79.24%\n",
      "Validation Batch 15, Loss: 0.952289, Accuracy: 79.17%\n",
      "Validation Batch 16, Loss: 0.952626, Accuracy: 79.10%\n",
      "Validation Batch 17, Loss: 1.023564, Accuracy: 78.31%\n",
      "Validation Batch 18, Loss: 0.937332, Accuracy: 78.30%\n",
      "Validation Batch 19, Loss: 0.988425, Accuracy: 78.21%\n",
      "Validation Batch 20, Loss: 0.979644, Accuracy: 78.05%\n",
      "Validation Batch 21, Loss: 0.976210, Accuracy: 77.90%\n",
      "Validation Batch 22, Loss: 0.980198, Accuracy: 77.91%\n",
      "Validation Batch 23, Loss: 1.026482, Accuracy: 77.58%\n",
      "Validation Batch 24, Loss: 0.972227, Accuracy: 77.54%\n",
      "Validation Batch 25, Loss: 0.948185, Accuracy: 77.50%\n",
      "Validation Batch 26, Loss: 0.959725, Accuracy: 77.58%\n",
      "Validation Batch 27, Loss: 0.915839, Accuracy: 77.69%\n",
      "Validation - Epoch 40, Loss: 0.964676, Accuracy: 77.69%\n",
      "Patience—1\n",
      "Epoch 41\n",
      "Batch 1, Loss: 0.950727, Accuracy: 79.69%\n",
      "Batch 2, Loss: 0.965710, Accuracy: 77.34%\n",
      "Batch 3, Loss: 1.021888, Accuracy: 75.00%\n",
      "Batch 4, Loss: 0.949883, Accuracy: 75.78%\n",
      "Batch 5, Loss: 0.932697, Accuracy: 77.19%\n",
      "Batch 6, Loss: 0.981526, Accuracy: 77.08%\n",
      "Batch 7, Loss: 1.005535, Accuracy: 76.56%\n",
      "Batch 8, Loss: 0.988165, Accuracy: 76.37%\n",
      "Batch 9, Loss: 0.935095, Accuracy: 77.08%\n",
      "Batch 10, Loss: 0.983514, Accuracy: 77.19%\n",
      "Batch 11, Loss: 0.948180, Accuracy: 77.56%\n",
      "Batch 12, Loss: 0.982414, Accuracy: 77.60%\n",
      "Batch 13, Loss: 0.941496, Accuracy: 77.76%\n",
      "Batch 14, Loss: 1.097261, Accuracy: 76.79%\n",
      "Batch 15, Loss: 0.981718, Accuracy: 76.88%\n",
      "Batch 16, Loss: 0.911811, Accuracy: 77.34%\n",
      "Batch 17, Loss: 0.942611, Accuracy: 77.48%\n",
      "Batch 18, Loss: 0.969008, Accuracy: 77.43%\n",
      "Batch 19, Loss: 0.919427, Accuracy: 77.63%\n",
      "Batch 20, Loss: 0.993042, Accuracy: 77.50%\n",
      "Batch 21, Loss: 1.006217, Accuracy: 77.46%\n",
      "Batch 22, Loss: 0.921398, Accuracy: 77.70%\n",
      "Batch 23, Loss: 0.940717, Accuracy: 77.85%\n",
      "Batch 24, Loss: 0.835960, Accuracy: 78.52%\n",
      "Batch 25, Loss: 0.946194, Accuracy: 78.56%\n",
      "Batch 26, Loss: 0.890920, Accuracy: 78.85%\n",
      "Batch 27, Loss: 0.931894, Accuracy: 78.88%\n",
      "Batch 28, Loss: 1.036817, Accuracy: 78.57%\n",
      "Batch 29, Loss: 0.967059, Accuracy: 78.61%\n",
      "Batch 30, Loss: 1.030632, Accuracy: 78.33%\n",
      "Batch 31, Loss: 0.900483, Accuracy: 78.58%\n",
      "Batch 32, Loss: 1.054380, Accuracy: 78.27%\n",
      "Batch 33, Loss: 1.023143, Accuracy: 78.03%\n",
      "Batch 34, Loss: 0.996670, Accuracy: 77.90%\n",
      "Batch 35, Loss: 0.915333, Accuracy: 78.04%\n",
      "Batch 36, Loss: 0.889676, Accuracy: 78.21%\n",
      "Batch 37, Loss: 1.027145, Accuracy: 78.04%\n",
      "Batch 38, Loss: 1.017112, Accuracy: 77.84%\n",
      "Batch 39, Loss: 1.060978, Accuracy: 77.52%\n",
      "Batch 40, Loss: 0.910000, Accuracy: 77.70%\n",
      "Batch 41, Loss: 0.982670, Accuracy: 77.67%\n",
      "Batch 42, Loss: 1.014419, Accuracy: 77.49%\n",
      "Batch 43, Loss: 1.023141, Accuracy: 77.33%\n",
      "Batch 44, Loss: 0.856918, Accuracy: 77.63%\n",
      "Batch 45, Loss: 1.001524, Accuracy: 77.53%\n",
      "Batch 46, Loss: 0.927994, Accuracy: 77.62%\n",
      "Batch 47, Loss: 1.029278, Accuracy: 77.46%\n",
      "Batch 48, Loss: 0.952874, Accuracy: 77.51%\n",
      "Batch 49, Loss: 0.910186, Accuracy: 77.65%\n",
      "Batch 50, Loss: 0.962675, Accuracy: 77.66%\n",
      "Batch 51, Loss: 1.009189, Accuracy: 77.57%\n",
      "Batch 52, Loss: 1.035408, Accuracy: 77.40%\n",
      "Batch 53, Loss: 0.933755, Accuracy: 77.51%\n",
      "Batch 54, Loss: 0.950183, Accuracy: 77.52%\n",
      "Batch 55, Loss: 0.960249, Accuracy: 77.53%\n",
      "Batch 56, Loss: 0.957477, Accuracy: 77.57%\n",
      "Batch 57, Loss: 0.937399, Accuracy: 77.63%\n",
      "Batch 58, Loss: 0.948597, Accuracy: 77.67%\n",
      "Batch 59, Loss: 0.953747, Accuracy: 77.67%\n",
      "Batch 60, Loss: 1.003782, Accuracy: 77.55%\n",
      "Batch 61, Loss: 0.995093, Accuracy: 77.51%\n",
      "Batch 62, Loss: 1.016065, Accuracy: 77.37%\n",
      "Batch 63, Loss: 0.950870, Accuracy: 77.38%\n",
      "Batch 64, Loss: 0.866407, Accuracy: 77.61%\n",
      "Batch 65, Loss: 0.958812, Accuracy: 77.62%\n",
      "Batch 66, Loss: 0.888004, Accuracy: 77.75%\n",
      "Batch 67, Loss: 0.985232, Accuracy: 77.68%\n",
      "Batch 68, Loss: 0.956458, Accuracy: 77.69%\n",
      "Batch 69, Loss: 0.933721, Accuracy: 77.72%\n",
      "Batch 70, Loss: 0.960952, Accuracy: 77.70%\n",
      "Batch 71, Loss: 0.974436, Accuracy: 77.66%\n",
      "Batch 72, Loss: 0.976791, Accuracy: 77.63%\n",
      "Batch 73, Loss: 1.008758, Accuracy: 77.57%\n",
      "Batch 74, Loss: 0.948468, Accuracy: 77.58%\n",
      "Batch 75, Loss: 0.977615, Accuracy: 77.58%\n",
      "Batch 76, Loss: 1.023878, Accuracy: 77.51%\n",
      "Batch 77, Loss: 0.932331, Accuracy: 77.56%\n",
      "Batch 78, Loss: 0.957527, Accuracy: 77.54%\n",
      "Batch 79, Loss: 0.972087, Accuracy: 77.55%\n",
      "Batch 80, Loss: 0.990760, Accuracy: 77.54%\n",
      "Batch 81, Loss: 0.925131, Accuracy: 77.58%\n",
      "Batch 82, Loss: 0.961580, Accuracy: 77.59%\n",
      "Batch 83, Loss: 0.958466, Accuracy: 77.60%\n",
      "Batch 84, Loss: 0.916863, Accuracy: 77.66%\n",
      "Batch 85, Loss: 0.999085, Accuracy: 77.59%\n",
      "Batch 86, Loss: 1.018353, Accuracy: 77.53%\n",
      "Batch 87, Loss: 0.910517, Accuracy: 77.59%\n",
      "Batch 88, Loss: 0.998133, Accuracy: 77.57%\n",
      "Batch 89, Loss: 0.925301, Accuracy: 77.63%\n",
      "Batch 90, Loss: 1.029349, Accuracy: 77.55%\n",
      "Batch 91, Loss: 1.001607, Accuracy: 77.51%\n",
      "Batch 92, Loss: 0.876512, Accuracy: 77.60%\n",
      "Batch 93, Loss: 0.979987, Accuracy: 77.59%\n",
      "Batch 94, Loss: 0.938442, Accuracy: 77.61%\n",
      "Batch 95, Loss: 1.024798, Accuracy: 77.53%\n",
      "Batch 96, Loss: 0.876633, Accuracy: 77.65%\n",
      "Batch 97, Loss: 0.909315, Accuracy: 77.72%\n",
      "Batch 98, Loss: 0.993893, Accuracy: 77.71%\n",
      "Batch 99, Loss: 1.033461, Accuracy: 77.62%\n",
      "Batch 100, Loss: 0.949955, Accuracy: 77.62%\n",
      "Batch 101, Loss: 0.989062, Accuracy: 77.60%\n",
      "Batch 102, Loss: 0.906626, Accuracy: 77.65%\n",
      "Batch 103, Loss: 0.945902, Accuracy: 77.67%\n",
      "Batch 104, Loss: 0.984571, Accuracy: 77.64%\n",
      "Batch 105, Loss: 1.014141, Accuracy: 77.60%\n",
      "Batch 106, Loss: 0.976974, Accuracy: 77.61%\n",
      "Batch 107, Loss: 1.037195, Accuracy: 77.54%\n",
      "Batch 108, Loss: 1.000502, Accuracy: 77.52%\n",
      "Batch 109, Loss: 0.966355, Accuracy: 77.54%\n",
      "Batch 110, Loss: 0.948000, Accuracy: 77.53%\n",
      "Batch 111, Loss: 0.916205, Accuracy: 77.58%\n",
      "Batch 112, Loss: 0.969857, Accuracy: 77.57%\n",
      "Batch 113, Loss: 0.918058, Accuracy: 77.63%\n",
      "Batch 114, Loss: 0.942106, Accuracy: 77.66%\n",
      "Batch 115, Loss: 1.012484, Accuracy: 77.60%\n",
      "Batch 116, Loss: 0.869441, Accuracy: 77.68%\n",
      "Batch 117, Loss: 0.921538, Accuracy: 77.72%\n",
      "Batch 118, Loss: 0.917388, Accuracy: 77.77%\n",
      "Batch 119, Loss: 0.910786, Accuracy: 77.81%\n",
      "Batch 120, Loss: 0.958447, Accuracy: 77.80%\n",
      "Batch 121, Loss: 0.972469, Accuracy: 77.79%\n",
      "Batch 122, Loss: 0.988281, Accuracy: 77.75%\n",
      "Batch 123, Loss: 0.939808, Accuracy: 77.78%\n",
      "Batch 124, Loss: 0.935326, Accuracy: 77.81%\n",
      "Batch 125, Loss: 1.114322, Accuracy: 77.66%\n",
      "Batch 126, Loss: 1.002747, Accuracy: 77.64%\n",
      "Batch 127, Loss: 0.951922, Accuracy: 77.66%\n",
      "Batch 128, Loss: 0.919291, Accuracy: 77.72%\n",
      "Batch 129, Loss: 0.996076, Accuracy: 77.70%\n",
      "Batch 130, Loss: 0.934579, Accuracy: 77.73%\n",
      "Batch 131, Loss: 0.968867, Accuracy: 77.72%\n",
      "Batch 132, Loss: 0.921225, Accuracy: 77.75%\n",
      "Batch 133, Loss: 0.901887, Accuracy: 77.80%\n",
      "Batch 134, Loss: 0.908192, Accuracy: 77.85%\n",
      "Batch 135, Loss: 0.916950, Accuracy: 77.88%\n",
      "Batch 136, Loss: 1.011681, Accuracy: 77.85%\n",
      "Batch 137, Loss: 0.963800, Accuracy: 77.86%\n",
      "Batch 138, Loss: 0.903100, Accuracy: 77.90%\n",
      "Batch 139, Loss: 0.970725, Accuracy: 77.89%\n",
      "Batch 140, Loss: 0.894132, Accuracy: 77.92%\n",
      "Batch 141, Loss: 0.933888, Accuracy: 77.96%\n",
      "Batch 142, Loss: 0.988414, Accuracy: 77.93%\n",
      "Batch 143, Loss: 0.958429, Accuracy: 77.93%\n",
      "Batch 144, Loss: 0.894099, Accuracy: 77.98%\n",
      "Batch 145, Loss: 0.969710, Accuracy: 77.98%\n",
      "Batch 146, Loss: 0.922841, Accuracy: 78.03%\n",
      "Batch 147, Loss: 0.999380, Accuracy: 78.01%\n",
      "Batch 148, Loss: 1.089121, Accuracy: 77.92%\n",
      "Batch 149, Loss: 0.918333, Accuracy: 77.95%\n",
      "Batch 150, Loss: 0.937547, Accuracy: 77.95%\n",
      "Batch 151, Loss: 0.901202, Accuracy: 77.99%\n",
      "Batch 152, Loss: 0.901702, Accuracy: 78.03%\n",
      "Batch 153, Loss: 0.925016, Accuracy: 78.06%\n",
      "Batch 154, Loss: 0.941875, Accuracy: 78.08%\n",
      "Batch 155, Loss: 0.941460, Accuracy: 78.09%\n",
      "Batch 156, Loss: 1.012249, Accuracy: 78.06%\n",
      "Batch 157, Loss: 0.873858, Accuracy: 78.14%\n",
      "Batch 158, Loss: 0.990329, Accuracy: 78.13%\n",
      "Batch 159, Loss: 0.896357, Accuracy: 78.17%\n",
      "Batch 160, Loss: 0.931038, Accuracy: 78.19%\n",
      "Batch 161, Loss: 0.952415, Accuracy: 78.18%\n",
      "Batch 162, Loss: 1.007216, Accuracy: 78.15%\n",
      "Batch 163, Loss: 0.933952, Accuracy: 78.16%\n",
      "Batch 164, Loss: 0.972863, Accuracy: 78.15%\n",
      "Batch 165, Loss: 0.998459, Accuracy: 78.12%\n",
      "Batch 166, Loss: 0.969541, Accuracy: 78.11%\n",
      "Batch 167, Loss: 1.014737, Accuracy: 78.07%\n",
      "Batch 168, Loss: 0.962658, Accuracy: 78.07%\n",
      "Batch 169, Loss: 0.882452, Accuracy: 78.12%\n",
      "Batch 170, Loss: 0.925665, Accuracy: 78.14%\n",
      "Batch 171, Loss: 0.968101, Accuracy: 78.14%\n",
      "Batch 172, Loss: 0.956936, Accuracy: 78.15%\n",
      "Batch 173, Loss: 0.937036, Accuracy: 78.17%\n",
      "Batch 174, Loss: 0.842349, Accuracy: 78.25%\n",
      "Batch 175, Loss: 0.948979, Accuracy: 78.26%\n",
      "Batch 176, Loss: 0.984112, Accuracy: 78.24%\n",
      "Batch 177, Loss: 0.936131, Accuracy: 78.26%\n",
      "Batch 178, Loss: 0.882229, Accuracy: 78.31%\n",
      "Batch 179, Loss: 0.978259, Accuracy: 78.29%\n",
      "Batch 180, Loss: 0.894135, Accuracy: 78.33%\n",
      "Batch 181, Loss: 1.021491, Accuracy: 78.29%\n",
      "Batch 182, Loss: 0.971284, Accuracy: 78.29%\n",
      "Batch 183, Loss: 0.990091, Accuracy: 78.27%\n",
      "Batch 184, Loss: 0.957901, Accuracy: 78.27%\n",
      "Batch 185, Loss: 1.020597, Accuracy: 78.23%\n",
      "Batch 186, Loss: 1.030838, Accuracy: 78.18%\n",
      "Batch 187, Loss: 0.953378, Accuracy: 78.19%\n",
      "Batch 188, Loss: 0.954498, Accuracy: 78.21%\n",
      "Batch 189, Loss: 0.887522, Accuracy: 78.26%\n",
      "Batch 190, Loss: 1.000161, Accuracy: 78.24%\n",
      "Batch 191, Loss: 0.903103, Accuracy: 78.28%\n",
      "Batch 192, Loss: 1.066630, Accuracy: 78.22%\n",
      "Batch 193, Loss: 0.969908, Accuracy: 78.22%\n",
      "Batch 194, Loss: 1.023954, Accuracy: 78.20%\n",
      "Batch 195, Loss: 0.953187, Accuracy: 78.21%\n",
      "Batch 196, Loss: 1.024120, Accuracy: 78.16%\n",
      "Batch 197, Loss: 0.974073, Accuracy: 78.16%\n",
      "Batch 198, Loss: 0.976467, Accuracy: 78.16%\n",
      "Batch 199, Loss: 0.973273, Accuracy: 78.14%\n",
      "Batch 200, Loss: 0.903718, Accuracy: 78.17%\n",
      "Batch 201, Loss: 0.906413, Accuracy: 78.21%\n",
      "Batch 202, Loss: 0.957351, Accuracy: 78.22%\n",
      "Batch 203, Loss: 0.959363, Accuracy: 78.22%\n",
      "Batch 204, Loss: 0.981376, Accuracy: 78.21%\n",
      "Batch 205, Loss: 0.957115, Accuracy: 78.20%\n",
      "Batch 206, Loss: 1.019949, Accuracy: 78.18%\n",
      "Batch 207, Loss: 0.917154, Accuracy: 78.20%\n",
      "Batch 208, Loss: 1.039303, Accuracy: 78.16%\n",
      "Batch 209, Loss: 0.991193, Accuracy: 78.15%\n",
      "Batch 210, Loss: 0.987651, Accuracy: 78.15%\n",
      "Batch 211, Loss: 0.923264, Accuracy: 78.16%\n",
      "Batch 212, Loss: 0.933421, Accuracy: 78.18%\n",
      "Batch 213, Loss: 0.980630, Accuracy: 78.17%\n",
      "Training - Epoch 41, Loss: 0.961333, Accuracy: 78.17%\n",
      "Validation Batch 1, Loss: 0.922811, Accuracy: 82.81%\n",
      "Validation Batch 2, Loss: 0.973101, Accuracy: 82.03%\n",
      "Validation Batch 3, Loss: 1.052766, Accuracy: 77.60%\n",
      "Validation Batch 4, Loss: 0.953898, Accuracy: 78.12%\n",
      "Validation Batch 5, Loss: 0.936067, Accuracy: 79.06%\n",
      "Validation Batch 6, Loss: 0.879443, Accuracy: 80.73%\n",
      "Validation Batch 7, Loss: 0.977443, Accuracy: 80.36%\n",
      "Validation Batch 8, Loss: 1.017114, Accuracy: 79.49%\n",
      "Validation Batch 9, Loss: 0.990482, Accuracy: 79.34%\n",
      "Validation Batch 10, Loss: 0.962163, Accuracy: 79.22%\n",
      "Validation Batch 11, Loss: 0.947677, Accuracy: 79.26%\n",
      "Validation Batch 12, Loss: 0.946892, Accuracy: 79.04%\n",
      "Validation Batch 13, Loss: 1.000878, Accuracy: 78.61%\n",
      "Validation Batch 14, Loss: 0.986595, Accuracy: 78.35%\n",
      "Validation Batch 15, Loss: 0.950752, Accuracy: 78.33%\n",
      "Validation Batch 16, Loss: 0.957467, Accuracy: 78.32%\n",
      "Validation Batch 17, Loss: 1.038694, Accuracy: 77.67%\n",
      "Validation Batch 18, Loss: 0.943224, Accuracy: 77.86%\n",
      "Validation Batch 19, Loss: 1.011196, Accuracy: 77.63%\n",
      "Validation Batch 20, Loss: 0.999772, Accuracy: 77.42%\n",
      "Validation Batch 21, Loss: 0.992041, Accuracy: 77.16%\n",
      "Validation Batch 22, Loss: 0.987487, Accuracy: 77.20%\n",
      "Validation Batch 23, Loss: 1.030787, Accuracy: 76.90%\n",
      "Validation Batch 24, Loss: 0.974714, Accuracy: 76.89%\n",
      "Validation Batch 25, Loss: 0.956025, Accuracy: 76.88%\n",
      "Validation Batch 26, Loss: 0.968840, Accuracy: 76.92%\n",
      "Validation Batch 27, Loss: 0.929130, Accuracy: 77.10%\n",
      "Validation - Epoch 41, Loss: 0.973610, Accuracy: 77.10%\n",
      "Patience—2\n",
      "Epoch 42\n",
      "Batch 1, Loss: 1.008947, Accuracy: 73.44%\n",
      "Batch 2, Loss: 0.992711, Accuracy: 74.22%\n",
      "Batch 3, Loss: 0.939682, Accuracy: 76.04%\n",
      "Batch 4, Loss: 1.011598, Accuracy: 75.39%\n",
      "Batch 5, Loss: 0.888010, Accuracy: 77.50%\n",
      "Batch 6, Loss: 0.874688, Accuracy: 79.17%\n",
      "Batch 7, Loss: 1.078297, Accuracy: 77.68%\n",
      "Batch 8, Loss: 0.984915, Accuracy: 77.34%\n",
      "Batch 9, Loss: 0.969385, Accuracy: 77.08%\n",
      "Batch 10, Loss: 1.064748, Accuracy: 76.09%\n",
      "Batch 11, Loss: 0.897226, Accuracy: 76.85%\n",
      "Batch 12, Loss: 0.923123, Accuracy: 77.21%\n",
      "Batch 13, Loss: 1.085428, Accuracy: 76.32%\n",
      "Batch 14, Loss: 0.965740, Accuracy: 76.45%\n",
      "Batch 15, Loss: 0.997286, Accuracy: 76.25%\n",
      "Batch 16, Loss: 0.945418, Accuracy: 76.37%\n",
      "Batch 17, Loss: 1.040782, Accuracy: 75.92%\n",
      "Batch 18, Loss: 1.003272, Accuracy: 75.87%\n",
      "Batch 19, Loss: 1.021771, Accuracy: 75.49%\n",
      "Batch 20, Loss: 0.918913, Accuracy: 75.86%\n",
      "Batch 21, Loss: 0.952635, Accuracy: 76.04%\n",
      "Batch 22, Loss: 0.946511, Accuracy: 76.21%\n",
      "Batch 23, Loss: 0.912479, Accuracy: 76.56%\n",
      "Batch 24, Loss: 1.030614, Accuracy: 76.37%\n",
      "Batch 25, Loss: 0.860820, Accuracy: 76.88%\n",
      "Batch 26, Loss: 0.947287, Accuracy: 76.98%\n",
      "Batch 27, Loss: 0.947727, Accuracy: 77.14%\n",
      "Batch 28, Loss: 0.887859, Accuracy: 77.40%\n",
      "Batch 29, Loss: 1.026834, Accuracy: 77.16%\n",
      "Batch 30, Loss: 0.924184, Accuracy: 77.29%\n",
      "Batch 31, Loss: 1.001131, Accuracy: 77.07%\n",
      "Batch 32, Loss: 0.994111, Accuracy: 77.00%\n",
      "Batch 33, Loss: 0.906071, Accuracy: 77.23%\n",
      "Batch 34, Loss: 0.947283, Accuracy: 77.39%\n",
      "Batch 35, Loss: 1.019361, Accuracy: 77.32%\n",
      "Batch 36, Loss: 0.997262, Accuracy: 77.30%\n",
      "Batch 37, Loss: 0.915913, Accuracy: 77.49%\n",
      "Batch 38, Loss: 1.099238, Accuracy: 77.14%\n",
      "Batch 39, Loss: 0.958337, Accuracy: 77.20%\n",
      "Batch 40, Loss: 0.926336, Accuracy: 77.34%\n",
      "Batch 41, Loss: 1.068610, Accuracy: 77.13%\n",
      "Batch 42, Loss: 0.965101, Accuracy: 77.12%\n",
      "Batch 43, Loss: 0.984535, Accuracy: 77.11%\n",
      "Batch 44, Loss: 0.962225, Accuracy: 77.10%\n",
      "Batch 45, Loss: 0.879254, Accuracy: 77.29%\n",
      "Batch 46, Loss: 1.071015, Accuracy: 77.04%\n",
      "Batch 47, Loss: 0.986622, Accuracy: 76.99%\n",
      "Batch 48, Loss: 0.954521, Accuracy: 77.05%\n",
      "Batch 49, Loss: 0.945437, Accuracy: 77.10%\n",
      "Batch 50, Loss: 0.914942, Accuracy: 77.22%\n",
      "Batch 51, Loss: 1.025664, Accuracy: 77.14%\n",
      "Batch 52, Loss: 0.924375, Accuracy: 77.19%\n",
      "Batch 53, Loss: 0.994319, Accuracy: 77.15%\n",
      "Batch 54, Loss: 0.966563, Accuracy: 77.14%\n",
      "Batch 55, Loss: 0.956578, Accuracy: 77.10%\n",
      "Batch 56, Loss: 1.086953, Accuracy: 76.93%\n",
      "Batch 57, Loss: 1.013062, Accuracy: 76.86%\n",
      "Batch 58, Loss: 0.902953, Accuracy: 76.99%\n",
      "Batch 59, Loss: 1.008410, Accuracy: 76.91%\n",
      "Batch 60, Loss: 0.982564, Accuracy: 76.85%\n",
      "Batch 61, Loss: 0.974066, Accuracy: 76.82%\n",
      "Batch 62, Loss: 0.891342, Accuracy: 76.97%\n",
      "Batch 63, Loss: 0.974912, Accuracy: 76.98%\n",
      "Batch 64, Loss: 1.030747, Accuracy: 76.93%\n",
      "Batch 65, Loss: 0.962246, Accuracy: 76.97%\n",
      "Batch 66, Loss: 0.883694, Accuracy: 77.13%\n",
      "Batch 67, Loss: 0.948527, Accuracy: 77.17%\n",
      "Batch 68, Loss: 0.998917, Accuracy: 77.14%\n",
      "Batch 69, Loss: 0.977347, Accuracy: 77.13%\n",
      "Batch 70, Loss: 0.940061, Accuracy: 77.19%\n",
      "Batch 71, Loss: 0.856606, Accuracy: 77.35%\n",
      "Batch 72, Loss: 0.886980, Accuracy: 77.47%\n",
      "Batch 73, Loss: 1.050978, Accuracy: 77.35%\n",
      "Batch 74, Loss: 0.876548, Accuracy: 77.51%\n",
      "Batch 75, Loss: 0.940203, Accuracy: 77.56%\n",
      "Batch 76, Loss: 1.043056, Accuracy: 77.38%\n",
      "Batch 77, Loss: 0.921389, Accuracy: 77.46%\n",
      "Batch 78, Loss: 0.951146, Accuracy: 77.48%\n",
      "Batch 79, Loss: 0.983424, Accuracy: 77.45%\n",
      "Batch 80, Loss: 0.933443, Accuracy: 77.52%\n",
      "Batch 81, Loss: 0.933050, Accuracy: 77.58%\n",
      "Batch 82, Loss: 0.997053, Accuracy: 77.52%\n",
      "Batch 83, Loss: 0.976708, Accuracy: 77.48%\n",
      "Batch 84, Loss: 0.876996, Accuracy: 77.60%\n",
      "Batch 85, Loss: 1.012375, Accuracy: 77.54%\n",
      "Batch 86, Loss: 0.990669, Accuracy: 77.51%\n",
      "Batch 87, Loss: 0.995871, Accuracy: 77.50%\n",
      "Batch 88, Loss: 1.012078, Accuracy: 77.45%\n",
      "Batch 89, Loss: 0.954183, Accuracy: 77.49%\n",
      "Batch 90, Loss: 0.955277, Accuracy: 77.52%\n",
      "Batch 91, Loss: 1.008560, Accuracy: 77.46%\n",
      "Batch 92, Loss: 0.864100, Accuracy: 77.58%\n",
      "Batch 93, Loss: 0.950177, Accuracy: 77.60%\n",
      "Batch 94, Loss: 0.965416, Accuracy: 77.61%\n",
      "Batch 95, Loss: 0.961855, Accuracy: 77.63%\n",
      "Batch 96, Loss: 0.837958, Accuracy: 77.78%\n",
      "Batch 97, Loss: 0.964624, Accuracy: 77.79%\n",
      "Batch 98, Loss: 0.937187, Accuracy: 77.82%\n",
      "Batch 99, Loss: 1.030268, Accuracy: 77.75%\n",
      "Batch 100, Loss: 0.930538, Accuracy: 77.78%\n",
      "Batch 101, Loss: 0.990546, Accuracy: 77.74%\n",
      "Batch 102, Loss: 0.953948, Accuracy: 77.74%\n",
      "Batch 103, Loss: 0.948124, Accuracy: 77.78%\n",
      "Batch 104, Loss: 0.977269, Accuracy: 77.78%\n",
      "Batch 105, Loss: 1.063600, Accuracy: 77.68%\n",
      "Batch 106, Loss: 0.937099, Accuracy: 77.70%\n",
      "Batch 107, Loss: 0.939767, Accuracy: 77.72%\n",
      "Batch 108, Loss: 1.021503, Accuracy: 77.66%\n",
      "Batch 109, Loss: 1.013935, Accuracy: 77.61%\n",
      "Batch 110, Loss: 0.994992, Accuracy: 77.59%\n",
      "Batch 111, Loss: 0.961097, Accuracy: 77.59%\n",
      "Batch 112, Loss: 0.919836, Accuracy: 77.61%\n",
      "Batch 113, Loss: 0.922037, Accuracy: 77.65%\n",
      "Batch 114, Loss: 1.001504, Accuracy: 77.62%\n",
      "Batch 115, Loss: 0.958628, Accuracy: 77.64%\n",
      "Batch 116, Loss: 0.938239, Accuracy: 77.65%\n",
      "Batch 117, Loss: 1.058640, Accuracy: 77.58%\n",
      "Batch 118, Loss: 0.909995, Accuracy: 77.62%\n",
      "Batch 119, Loss: 0.902958, Accuracy: 77.67%\n",
      "Batch 120, Loss: 0.910354, Accuracy: 77.71%\n",
      "Batch 121, Loss: 0.939545, Accuracy: 77.71%\n",
      "Batch 122, Loss: 1.028894, Accuracy: 77.66%\n",
      "Batch 123, Loss: 0.920074, Accuracy: 77.71%\n",
      "Batch 124, Loss: 0.978944, Accuracy: 77.71%\n",
      "Batch 125, Loss: 0.836033, Accuracy: 77.84%\n",
      "Batch 126, Loss: 0.965712, Accuracy: 77.83%\n",
      "Batch 127, Loss: 0.905912, Accuracy: 77.88%\n",
      "Batch 128, Loss: 0.983579, Accuracy: 77.88%\n",
      "Batch 129, Loss: 0.989720, Accuracy: 77.86%\n",
      "Batch 130, Loss: 0.915731, Accuracy: 77.90%\n",
      "Batch 131, Loss: 1.004843, Accuracy: 77.85%\n",
      "Batch 132, Loss: 0.853050, Accuracy: 77.95%\n",
      "Batch 133, Loss: 0.895102, Accuracy: 78.01%\n",
      "Batch 134, Loss: 0.942268, Accuracy: 78.03%\n",
      "Batch 135, Loss: 1.037497, Accuracy: 77.96%\n",
      "Batch 136, Loss: 0.975681, Accuracy: 77.95%\n",
      "Batch 137, Loss: 1.000710, Accuracy: 77.93%\n",
      "Batch 138, Loss: 0.975953, Accuracy: 77.94%\n",
      "Batch 139, Loss: 1.041033, Accuracy: 77.89%\n",
      "Batch 140, Loss: 0.940356, Accuracy: 77.90%\n",
      "Batch 141, Loss: 0.988368, Accuracy: 77.87%\n",
      "Batch 142, Loss: 0.865962, Accuracy: 77.93%\n",
      "Batch 143, Loss: 0.955574, Accuracy: 77.94%\n",
      "Batch 144, Loss: 0.948651, Accuracy: 77.96%\n",
      "Batch 145, Loss: 0.991981, Accuracy: 77.94%\n",
      "Batch 146, Loss: 0.956680, Accuracy: 77.94%\n",
      "Batch 147, Loss: 0.991127, Accuracy: 77.93%\n",
      "Batch 148, Loss: 0.936388, Accuracy: 77.95%\n",
      "Batch 149, Loss: 0.914816, Accuracy: 77.98%\n",
      "Batch 150, Loss: 0.998145, Accuracy: 77.94%\n",
      "Batch 151, Loss: 1.023963, Accuracy: 77.91%\n",
      "Batch 152, Loss: 0.937693, Accuracy: 77.94%\n",
      "Batch 153, Loss: 0.899427, Accuracy: 77.97%\n",
      "Batch 154, Loss: 0.979417, Accuracy: 77.95%\n",
      "Batch 155, Loss: 0.980851, Accuracy: 77.94%\n",
      "Batch 156, Loss: 0.976236, Accuracy: 77.93%\n",
      "Batch 157, Loss: 0.904636, Accuracy: 77.99%\n",
      "Batch 158, Loss: 0.888573, Accuracy: 78.04%\n",
      "Batch 159, Loss: 0.971131, Accuracy: 78.04%\n",
      "Batch 160, Loss: 1.012548, Accuracy: 78.01%\n",
      "Batch 161, Loss: 1.021102, Accuracy: 77.97%\n",
      "Batch 162, Loss: 0.986060, Accuracy: 77.94%\n",
      "Batch 163, Loss: 0.943423, Accuracy: 77.95%\n",
      "Batch 164, Loss: 0.961818, Accuracy: 77.94%\n",
      "Batch 165, Loss: 1.064729, Accuracy: 77.90%\n",
      "Batch 166, Loss: 1.026915, Accuracy: 77.87%\n",
      "Batch 167, Loss: 0.995682, Accuracy: 77.85%\n",
      "Batch 168, Loss: 0.927194, Accuracy: 77.88%\n",
      "Batch 169, Loss: 0.982637, Accuracy: 77.86%\n",
      "Batch 170, Loss: 0.941380, Accuracy: 77.88%\n",
      "Batch 171, Loss: 0.943216, Accuracy: 77.90%\n",
      "Batch 172, Loss: 1.003616, Accuracy: 77.88%\n",
      "Batch 173, Loss: 0.987896, Accuracy: 77.88%\n",
      "Batch 174, Loss: 0.936446, Accuracy: 77.90%\n",
      "Batch 175, Loss: 0.937603, Accuracy: 77.93%\n",
      "Batch 176, Loss: 0.938618, Accuracy: 77.94%\n",
      "Batch 177, Loss: 0.949710, Accuracy: 77.95%\n",
      "Batch 178, Loss: 0.942545, Accuracy: 77.96%\n",
      "Batch 179, Loss: 1.064445, Accuracy: 77.91%\n",
      "Batch 180, Loss: 0.905365, Accuracy: 77.95%\n",
      "Batch 181, Loss: 0.870791, Accuracy: 78.01%\n",
      "Batch 182, Loss: 1.007192, Accuracy: 77.99%\n",
      "Batch 183, Loss: 0.916148, Accuracy: 78.01%\n",
      "Batch 184, Loss: 0.943193, Accuracy: 78.03%\n",
      "Batch 185, Loss: 0.963634, Accuracy: 78.03%\n",
      "Batch 186, Loss: 0.970411, Accuracy: 78.02%\n",
      "Batch 187, Loss: 1.014934, Accuracy: 78.00%\n",
      "Batch 188, Loss: 0.915242, Accuracy: 78.02%\n",
      "Batch 189, Loss: 1.010188, Accuracy: 78.00%\n",
      "Batch 190, Loss: 0.883197, Accuracy: 78.05%\n",
      "Batch 191, Loss: 0.977163, Accuracy: 78.04%\n",
      "Batch 192, Loss: 0.939607, Accuracy: 78.04%\n",
      "Batch 193, Loss: 0.968433, Accuracy: 78.04%\n",
      "Batch 194, Loss: 0.966407, Accuracy: 78.04%\n",
      "Batch 195, Loss: 0.984217, Accuracy: 78.04%\n",
      "Batch 196, Loss: 1.016075, Accuracy: 78.00%\n",
      "Batch 197, Loss: 0.930747, Accuracy: 78.01%\n",
      "Batch 198, Loss: 0.958457, Accuracy: 78.02%\n",
      "Batch 199, Loss: 0.920421, Accuracy: 78.04%\n",
      "Batch 200, Loss: 0.966971, Accuracy: 78.03%\n",
      "Batch 201, Loss: 0.954353, Accuracy: 78.02%\n",
      "Batch 202, Loss: 0.892563, Accuracy: 78.07%\n",
      "Batch 203, Loss: 0.950014, Accuracy: 78.07%\n",
      "Batch 204, Loss: 0.933415, Accuracy: 78.08%\n",
      "Batch 205, Loss: 0.897001, Accuracy: 78.12%\n",
      "Batch 206, Loss: 0.922699, Accuracy: 78.12%\n",
      "Batch 207, Loss: 0.968866, Accuracy: 78.12%\n",
      "Batch 208, Loss: 0.907164, Accuracy: 78.16%\n",
      "Batch 209, Loss: 0.939984, Accuracy: 78.18%\n",
      "Batch 210, Loss: 0.935662, Accuracy: 78.18%\n",
      "Batch 211, Loss: 1.023657, Accuracy: 78.15%\n",
      "Batch 212, Loss: 0.963004, Accuracy: 78.15%\n",
      "Batch 213, Loss: 0.975233, Accuracy: 78.15%\n",
      "Training - Epoch 42, Loss: 0.962500, Accuracy: 78.15%\n",
      "Validation Batch 1, Loss: 0.925996, Accuracy: 82.81%\n",
      "Validation Batch 2, Loss: 0.976064, Accuracy: 80.47%\n",
      "Validation Batch 3, Loss: 1.054719, Accuracy: 76.04%\n",
      "Validation Batch 4, Loss: 0.960227, Accuracy: 76.95%\n",
      "Validation Batch 5, Loss: 0.938191, Accuracy: 78.12%\n",
      "Validation Batch 6, Loss: 0.886755, Accuracy: 79.69%\n",
      "Validation Batch 7, Loss: 0.984271, Accuracy: 79.24%\n",
      "Validation Batch 8, Loss: 1.018847, Accuracy: 78.32%\n",
      "Validation Batch 9, Loss: 0.998442, Accuracy: 78.12%\n",
      "Validation Batch 10, Loss: 0.968439, Accuracy: 77.97%\n",
      "Validation Batch 11, Loss: 0.944799, Accuracy: 78.27%\n",
      "Validation Batch 12, Loss: 0.950278, Accuracy: 78.39%\n",
      "Validation Batch 13, Loss: 1.007148, Accuracy: 78.00%\n",
      "Validation Batch 14, Loss: 0.994886, Accuracy: 77.79%\n",
      "Validation Batch 15, Loss: 0.960625, Accuracy: 77.81%\n",
      "Validation Batch 16, Loss: 0.959709, Accuracy: 77.83%\n",
      "Validation Batch 17, Loss: 1.046455, Accuracy: 77.11%\n",
      "Validation Batch 18, Loss: 0.949835, Accuracy: 77.17%\n",
      "Validation Batch 19, Loss: 1.008544, Accuracy: 77.06%\n",
      "Validation Batch 20, Loss: 1.005085, Accuracy: 76.80%\n",
      "Validation Batch 21, Loss: 0.996896, Accuracy: 76.41%\n",
      "Validation Batch 22, Loss: 1.004530, Accuracy: 76.42%\n",
      "Validation Batch 23, Loss: 1.046495, Accuracy: 76.02%\n",
      "Validation Batch 24, Loss: 0.983484, Accuracy: 76.04%\n",
      "Validation Batch 25, Loss: 0.971244, Accuracy: 76.00%\n",
      "Validation Batch 26, Loss: 0.973505, Accuracy: 76.14%\n",
      "Validation Batch 27, Loss: 0.934349, Accuracy: 76.22%\n",
      "Validation - Epoch 42, Loss: 0.979623, Accuracy: 76.22%\n",
      "Patience—3\n",
      "Epoch 43\n",
      "Batch 1, Loss: 0.955735, Accuracy: 79.69%\n",
      "Batch 2, Loss: 0.969685, Accuracy: 78.91%\n",
      "Batch 3, Loss: 0.896801, Accuracy: 81.25%\n",
      "Batch 4, Loss: 0.922147, Accuracy: 81.64%\n",
      "Batch 5, Loss: 0.919140, Accuracy: 82.19%\n",
      "Batch 6, Loss: 0.913360, Accuracy: 82.29%\n",
      "Batch 7, Loss: 0.932346, Accuracy: 82.14%\n",
      "Batch 8, Loss: 1.052590, Accuracy: 80.47%\n",
      "Batch 9, Loss: 0.927653, Accuracy: 80.56%\n",
      "Batch 10, Loss: 0.971563, Accuracy: 80.00%\n",
      "Batch 11, Loss: 0.922321, Accuracy: 80.11%\n",
      "Batch 12, Loss: 1.023046, Accuracy: 79.56%\n",
      "Batch 13, Loss: 0.926696, Accuracy: 79.81%\n",
      "Batch 14, Loss: 0.970449, Accuracy: 79.58%\n",
      "Batch 15, Loss: 0.931537, Accuracy: 79.58%\n",
      "Batch 16, Loss: 1.020990, Accuracy: 79.00%\n",
      "Batch 17, Loss: 1.005733, Accuracy: 78.68%\n",
      "Batch 18, Loss: 0.993538, Accuracy: 78.56%\n",
      "Batch 19, Loss: 0.977239, Accuracy: 78.37%\n",
      "Batch 20, Loss: 0.954550, Accuracy: 78.44%\n",
      "Batch 21, Loss: 0.918050, Accuracy: 78.65%\n",
      "Batch 22, Loss: 0.963836, Accuracy: 78.62%\n",
      "Batch 23, Loss: 0.885959, Accuracy: 78.94%\n",
      "Batch 24, Loss: 0.987371, Accuracy: 78.84%\n",
      "Batch 25, Loss: 0.928065, Accuracy: 79.00%\n",
      "Batch 26, Loss: 1.014077, Accuracy: 78.79%\n",
      "Batch 27, Loss: 0.970854, Accuracy: 78.70%\n",
      "Batch 28, Loss: 0.912423, Accuracy: 78.85%\n",
      "Batch 29, Loss: 0.886016, Accuracy: 79.09%\n",
      "Batch 30, Loss: 0.896710, Accuracy: 79.38%\n",
      "Batch 31, Loss: 0.961919, Accuracy: 79.39%\n",
      "Batch 32, Loss: 0.986918, Accuracy: 79.30%\n",
      "Batch 33, Loss: 0.979001, Accuracy: 79.21%\n",
      "Batch 34, Loss: 0.899373, Accuracy: 79.37%\n",
      "Batch 35, Loss: 0.890269, Accuracy: 79.55%\n",
      "Batch 36, Loss: 0.940414, Accuracy: 79.60%\n",
      "Batch 37, Loss: 0.935134, Accuracy: 79.60%\n",
      "Batch 38, Loss: 0.882320, Accuracy: 79.73%\n",
      "Batch 39, Loss: 0.869401, Accuracy: 79.97%\n",
      "Batch 40, Loss: 0.878756, Accuracy: 80.12%\n",
      "Batch 41, Loss: 0.863851, Accuracy: 80.34%\n",
      "Batch 42, Loss: 0.975468, Accuracy: 80.25%\n",
      "Batch 43, Loss: 0.942626, Accuracy: 80.20%\n",
      "Batch 44, Loss: 0.957723, Accuracy: 80.18%\n",
      "Batch 45, Loss: 0.966962, Accuracy: 80.14%\n",
      "Batch 46, Loss: 0.957218, Accuracy: 80.13%\n",
      "Batch 47, Loss: 0.943994, Accuracy: 80.12%\n",
      "Batch 48, Loss: 0.937712, Accuracy: 80.11%\n",
      "Batch 49, Loss: 1.002924, Accuracy: 79.97%\n",
      "Batch 50, Loss: 0.967458, Accuracy: 79.94%\n",
      "Batch 51, Loss: 1.095470, Accuracy: 79.56%\n",
      "Batch 52, Loss: 1.048430, Accuracy: 79.36%\n",
      "Batch 53, Loss: 0.956825, Accuracy: 79.33%\n",
      "Batch 54, Loss: 0.985611, Accuracy: 79.28%\n",
      "Batch 55, Loss: 0.954161, Accuracy: 79.29%\n",
      "Batch 56, Loss: 0.841653, Accuracy: 79.49%\n",
      "Batch 57, Loss: 1.024591, Accuracy: 79.36%\n",
      "Batch 58, Loss: 0.964745, Accuracy: 79.34%\n",
      "Batch 59, Loss: 0.979825, Accuracy: 79.26%\n",
      "Batch 60, Loss: 0.950221, Accuracy: 79.27%\n",
      "Batch 61, Loss: 0.997573, Accuracy: 79.20%\n",
      "Batch 62, Loss: 0.977824, Accuracy: 79.13%\n",
      "Batch 63, Loss: 0.953839, Accuracy: 79.14%\n",
      "Batch 64, Loss: 0.943981, Accuracy: 79.17%\n",
      "Batch 65, Loss: 0.960477, Accuracy: 79.18%\n",
      "Batch 66, Loss: 0.946717, Accuracy: 79.19%\n",
      "Batch 67, Loss: 1.030310, Accuracy: 79.10%\n",
      "Batch 68, Loss: 0.965851, Accuracy: 79.09%\n",
      "Batch 69, Loss: 0.947179, Accuracy: 79.10%\n",
      "Batch 70, Loss: 1.031080, Accuracy: 78.97%\n",
      "Batch 71, Loss: 0.983520, Accuracy: 78.94%\n",
      "Batch 72, Loss: 0.947443, Accuracy: 78.95%\n",
      "Batch 73, Loss: 0.982185, Accuracy: 78.92%\n",
      "Batch 74, Loss: 0.932094, Accuracy: 78.97%\n",
      "Batch 75, Loss: 0.972215, Accuracy: 78.96%\n",
      "Batch 76, Loss: 0.978040, Accuracy: 78.95%\n",
      "Batch 77, Loss: 0.908998, Accuracy: 79.00%\n",
      "Batch 78, Loss: 0.932098, Accuracy: 79.03%\n",
      "Batch 79, Loss: 0.956909, Accuracy: 79.03%\n",
      "Batch 80, Loss: 1.014246, Accuracy: 78.96%\n",
      "Batch 81, Loss: 0.900199, Accuracy: 79.01%\n",
      "Batch 82, Loss: 1.011573, Accuracy: 78.94%\n",
      "Batch 83, Loss: 0.888489, Accuracy: 79.03%\n",
      "Batch 84, Loss: 0.962899, Accuracy: 79.02%\n",
      "Batch 85, Loss: 0.991123, Accuracy: 78.95%\n",
      "Batch 86, Loss: 0.886763, Accuracy: 79.03%\n",
      "Batch 87, Loss: 0.980873, Accuracy: 79.01%\n",
      "Batch 88, Loss: 1.022645, Accuracy: 78.91%\n",
      "Batch 89, Loss: 0.908038, Accuracy: 78.95%\n",
      "Batch 90, Loss: 0.962021, Accuracy: 78.92%\n",
      "Batch 91, Loss: 0.986205, Accuracy: 78.86%\n",
      "Batch 92, Loss: 0.947463, Accuracy: 78.89%\n",
      "Batch 93, Loss: 1.002987, Accuracy: 78.85%\n",
      "Batch 94, Loss: 0.933621, Accuracy: 78.89%\n",
      "Batch 95, Loss: 0.901049, Accuracy: 78.96%\n",
      "Batch 96, Loss: 0.891362, Accuracy: 79.05%\n",
      "Batch 97, Loss: 1.025530, Accuracy: 78.98%\n",
      "Batch 98, Loss: 1.022980, Accuracy: 78.92%\n",
      "Batch 99, Loss: 0.976854, Accuracy: 78.90%\n",
      "Batch 100, Loss: 0.969533, Accuracy: 78.88%\n",
      "Batch 101, Loss: 0.964649, Accuracy: 78.87%\n",
      "Batch 102, Loss: 0.930212, Accuracy: 78.91%\n",
      "Batch 103, Loss: 0.923758, Accuracy: 78.93%\n",
      "Batch 104, Loss: 0.929604, Accuracy: 78.94%\n",
      "Batch 105, Loss: 0.935506, Accuracy: 78.94%\n",
      "Batch 106, Loss: 1.054460, Accuracy: 78.83%\n",
      "Batch 107, Loss: 0.980023, Accuracy: 78.83%\n",
      "Batch 108, Loss: 0.997096, Accuracy: 78.79%\n",
      "Batch 109, Loss: 0.912432, Accuracy: 78.81%\n",
      "Batch 110, Loss: 0.912629, Accuracy: 78.86%\n",
      "Batch 111, Loss: 0.936174, Accuracy: 78.89%\n",
      "Batch 112, Loss: 0.986919, Accuracy: 78.88%\n",
      "Batch 113, Loss: 1.024081, Accuracy: 78.82%\n",
      "Batch 114, Loss: 0.949520, Accuracy: 78.81%\n",
      "Batch 115, Loss: 0.987828, Accuracy: 78.79%\n",
      "Batch 116, Loss: 1.024492, Accuracy: 78.74%\n",
      "Batch 117, Loss: 0.918241, Accuracy: 78.79%\n",
      "Batch 118, Loss: 0.986796, Accuracy: 78.79%\n",
      "Batch 119, Loss: 0.982653, Accuracy: 78.77%\n",
      "Batch 120, Loss: 0.936481, Accuracy: 78.80%\n",
      "Batch 121, Loss: 0.941556, Accuracy: 78.82%\n",
      "Batch 122, Loss: 0.943765, Accuracy: 78.84%\n",
      "Batch 123, Loss: 0.944253, Accuracy: 78.86%\n",
      "Batch 124, Loss: 0.962228, Accuracy: 78.87%\n",
      "Batch 125, Loss: 1.023949, Accuracy: 78.81%\n",
      "Batch 126, Loss: 1.009490, Accuracy: 78.77%\n",
      "Batch 127, Loss: 0.978484, Accuracy: 78.74%\n",
      "Batch 128, Loss: 0.925130, Accuracy: 78.77%\n",
      "Batch 129, Loss: 0.968979, Accuracy: 78.75%\n",
      "Batch 130, Loss: 0.950803, Accuracy: 78.76%\n",
      "Batch 131, Loss: 1.035688, Accuracy: 78.69%\n",
      "Batch 132, Loss: 0.939404, Accuracy: 78.71%\n",
      "Batch 133, Loss: 0.994488, Accuracy: 78.67%\n",
      "Batch 134, Loss: 0.991932, Accuracy: 78.64%\n",
      "Batch 135, Loss: 0.951087, Accuracy: 78.65%\n",
      "Batch 136, Loss: 1.031373, Accuracy: 78.57%\n",
      "Batch 137, Loss: 0.929463, Accuracy: 78.59%\n",
      "Batch 138, Loss: 0.963235, Accuracy: 78.59%\n",
      "Batch 139, Loss: 0.928312, Accuracy: 78.63%\n",
      "Batch 140, Loss: 1.016281, Accuracy: 78.57%\n",
      "Batch 141, Loss: 1.014354, Accuracy: 78.54%\n",
      "Batch 142, Loss: 0.930515, Accuracy: 78.57%\n",
      "Batch 143, Loss: 0.915767, Accuracy: 78.58%\n",
      "Batch 144, Loss: 0.931063, Accuracy: 78.61%\n",
      "Batch 145, Loss: 0.956783, Accuracy: 78.61%\n",
      "Batch 146, Loss: 0.938155, Accuracy: 78.63%\n",
      "Batch 147, Loss: 0.969024, Accuracy: 78.61%\n",
      "Batch 148, Loss: 1.024672, Accuracy: 78.57%\n",
      "Batch 149, Loss: 0.978092, Accuracy: 78.54%\n",
      "Batch 150, Loss: 1.008260, Accuracy: 78.50%\n",
      "Batch 151, Loss: 0.971622, Accuracy: 78.49%\n",
      "Batch 152, Loss: 0.940423, Accuracy: 78.50%\n",
      "Batch 153, Loss: 0.916557, Accuracy: 78.51%\n",
      "Batch 154, Loss: 0.859460, Accuracy: 78.57%\n",
      "Batch 155, Loss: 0.932207, Accuracy: 78.59%\n",
      "Batch 156, Loss: 1.037647, Accuracy: 78.54%\n",
      "Batch 157, Loss: 1.039396, Accuracy: 78.48%\n",
      "Batch 158, Loss: 0.969192, Accuracy: 78.48%\n",
      "Batch 159, Loss: 0.879992, Accuracy: 78.53%\n",
      "Batch 160, Loss: 1.003278, Accuracy: 78.49%\n",
      "Batch 161, Loss: 0.960132, Accuracy: 78.48%\n",
      "Batch 162, Loss: 0.941628, Accuracy: 78.49%\n",
      "Batch 163, Loss: 0.918726, Accuracy: 78.52%\n",
      "Batch 164, Loss: 0.932230, Accuracy: 78.53%\n",
      "Batch 165, Loss: 0.981721, Accuracy: 78.52%\n",
      "Batch 166, Loss: 0.966835, Accuracy: 78.52%\n",
      "Batch 167, Loss: 0.952892, Accuracy: 78.52%\n",
      "Batch 168, Loss: 0.912881, Accuracy: 78.55%\n",
      "Batch 169, Loss: 1.011856, Accuracy: 78.51%\n",
      "Batch 170, Loss: 0.969060, Accuracy: 78.51%\n",
      "Batch 171, Loss: 0.867598, Accuracy: 78.55%\n",
      "Batch 172, Loss: 1.005293, Accuracy: 78.50%\n",
      "Batch 173, Loss: 0.986168, Accuracy: 78.50%\n",
      "Batch 174, Loss: 1.045334, Accuracy: 78.44%\n",
      "Batch 175, Loss: 0.987342, Accuracy: 78.44%\n",
      "Batch 176, Loss: 1.010226, Accuracy: 78.41%\n",
      "Batch 177, Loss: 0.938833, Accuracy: 78.43%\n",
      "Batch 178, Loss: 0.989824, Accuracy: 78.41%\n",
      "Batch 179, Loss: 0.964709, Accuracy: 78.40%\n",
      "Batch 180, Loss: 0.888937, Accuracy: 78.45%\n",
      "Batch 181, Loss: 0.986407, Accuracy: 78.44%\n",
      "Batch 182, Loss: 0.920682, Accuracy: 78.46%\n",
      "Batch 183, Loss: 0.925118, Accuracy: 78.48%\n",
      "Batch 184, Loss: 0.923977, Accuracy: 78.49%\n",
      "Batch 185, Loss: 0.865735, Accuracy: 78.55%\n",
      "Batch 186, Loss: 1.005553, Accuracy: 78.52%\n",
      "Batch 187, Loss: 0.947320, Accuracy: 78.53%\n",
      "Batch 188, Loss: 0.981574, Accuracy: 78.52%\n",
      "Batch 189, Loss: 0.953478, Accuracy: 78.52%\n",
      "Batch 190, Loss: 1.016847, Accuracy: 78.50%\n",
      "Batch 191, Loss: 0.936905, Accuracy: 78.51%\n",
      "Batch 192, Loss: 0.958667, Accuracy: 78.52%\n",
      "Batch 193, Loss: 0.886181, Accuracy: 78.56%\n",
      "Batch 194, Loss: 0.948730, Accuracy: 78.57%\n",
      "Batch 195, Loss: 0.930941, Accuracy: 78.58%\n",
      "Batch 196, Loss: 1.042903, Accuracy: 78.53%\n",
      "Batch 197, Loss: 0.954567, Accuracy: 78.52%\n",
      "Batch 198, Loss: 1.040394, Accuracy: 78.47%\n",
      "Batch 199, Loss: 0.914098, Accuracy: 78.50%\n",
      "Batch 200, Loss: 0.940174, Accuracy: 78.51%\n",
      "Batch 201, Loss: 0.907738, Accuracy: 78.54%\n",
      "Batch 202, Loss: 1.034816, Accuracy: 78.50%\n",
      "Batch 203, Loss: 0.875205, Accuracy: 78.53%\n",
      "Batch 204, Loss: 0.864602, Accuracy: 78.58%\n",
      "Batch 205, Loss: 0.871623, Accuracy: 78.62%\n",
      "Batch 206, Loss: 0.972873, Accuracy: 78.62%\n",
      "Batch 207, Loss: 0.905796, Accuracy: 78.64%\n",
      "Batch 208, Loss: 0.979565, Accuracy: 78.64%\n",
      "Batch 209, Loss: 0.945529, Accuracy: 78.66%\n",
      "Batch 210, Loss: 0.953275, Accuracy: 78.67%\n",
      "Batch 211, Loss: 0.976093, Accuracy: 78.65%\n",
      "Batch 212, Loss: 0.980344, Accuracy: 78.65%\n",
      "Batch 213, Loss: 0.940708, Accuracy: 78.66%\n",
      "Training - Epoch 43, Loss: 0.958060, Accuracy: 78.66%\n",
      "Validation Batch 1, Loss: 0.922555, Accuracy: 82.81%\n",
      "Validation Batch 2, Loss: 0.968852, Accuracy: 81.25%\n",
      "Validation Batch 3, Loss: 1.044843, Accuracy: 77.08%\n",
      "Validation Batch 4, Loss: 0.957317, Accuracy: 77.34%\n",
      "Validation Batch 5, Loss: 0.927879, Accuracy: 78.75%\n",
      "Validation Batch 6, Loss: 0.878644, Accuracy: 80.21%\n",
      "Validation Batch 7, Loss: 0.975784, Accuracy: 79.91%\n",
      "Validation Batch 8, Loss: 1.013534, Accuracy: 79.10%\n",
      "Validation Batch 9, Loss: 0.991099, Accuracy: 78.82%\n",
      "Validation Batch 10, Loss: 0.960131, Accuracy: 78.75%\n",
      "Validation Batch 11, Loss: 0.940538, Accuracy: 78.84%\n",
      "Validation Batch 12, Loss: 0.949223, Accuracy: 78.91%\n",
      "Validation Batch 13, Loss: 0.992375, Accuracy: 78.61%\n",
      "Validation Batch 14, Loss: 0.989091, Accuracy: 78.46%\n",
      "Validation Batch 15, Loss: 0.953301, Accuracy: 78.44%\n",
      "Validation Batch 16, Loss: 0.953902, Accuracy: 78.42%\n",
      "Validation Batch 17, Loss: 1.027922, Accuracy: 78.12%\n",
      "Validation Batch 18, Loss: 0.942343, Accuracy: 78.12%\n",
      "Validation Batch 19, Loss: 0.998481, Accuracy: 78.04%\n",
      "Validation Batch 20, Loss: 0.993271, Accuracy: 77.89%\n",
      "Validation Batch 21, Loss: 0.986608, Accuracy: 77.53%\n",
      "Validation Batch 22, Loss: 0.986497, Accuracy: 77.56%\n",
      "Validation Batch 23, Loss: 1.029074, Accuracy: 77.38%\n",
      "Validation Batch 24, Loss: 0.973461, Accuracy: 77.34%\n",
      "Validation Batch 25, Loss: 0.953900, Accuracy: 77.31%\n",
      "Validation Batch 26, Loss: 0.962006, Accuracy: 77.46%\n",
      "Validation Batch 27, Loss: 0.918411, Accuracy: 77.57%\n",
      "Validation - Epoch 43, Loss: 0.970039, Accuracy: 77.57%\n",
      "Patience—4\n",
      "Epoch 44\n",
      "Batch 1, Loss: 1.028926, Accuracy: 70.31%\n",
      "Batch 2, Loss: 1.021917, Accuracy: 71.88%\n",
      "Batch 3, Loss: 0.901419, Accuracy: 76.04%\n",
      "Batch 4, Loss: 1.024576, Accuracy: 75.00%\n",
      "Batch 5, Loss: 1.052102, Accuracy: 74.06%\n",
      "Batch 6, Loss: 1.026340, Accuracy: 73.18%\n",
      "Batch 7, Loss: 1.003602, Accuracy: 73.21%\n",
      "Batch 8, Loss: 0.930181, Accuracy: 74.41%\n",
      "Batch 9, Loss: 1.017382, Accuracy: 74.13%\n",
      "Batch 10, Loss: 0.960206, Accuracy: 74.53%\n",
      "Batch 11, Loss: 0.939127, Accuracy: 75.00%\n",
      "Batch 12, Loss: 0.997378, Accuracy: 75.13%\n",
      "Batch 13, Loss: 0.974068, Accuracy: 75.24%\n",
      "Batch 14, Loss: 1.000929, Accuracy: 75.22%\n",
      "Batch 15, Loss: 0.920012, Accuracy: 75.73%\n",
      "Batch 16, Loss: 0.917669, Accuracy: 76.17%\n",
      "Batch 17, Loss: 0.966233, Accuracy: 76.19%\n",
      "Batch 18, Loss: 1.051015, Accuracy: 75.78%\n",
      "Batch 19, Loss: 0.949017, Accuracy: 75.99%\n",
      "Batch 20, Loss: 0.885407, Accuracy: 76.48%\n",
      "Batch 21, Loss: 0.987212, Accuracy: 76.41%\n",
      "Batch 22, Loss: 0.968418, Accuracy: 76.49%\n",
      "Batch 23, Loss: 0.949278, Accuracy: 76.63%\n",
      "Batch 24, Loss: 0.945724, Accuracy: 76.82%\n",
      "Batch 25, Loss: 0.996895, Accuracy: 76.75%\n",
      "Batch 26, Loss: 0.883466, Accuracy: 77.16%\n",
      "Batch 27, Loss: 0.916770, Accuracy: 77.37%\n",
      "Batch 28, Loss: 0.961032, Accuracy: 77.34%\n",
      "Batch 29, Loss: 0.917478, Accuracy: 77.53%\n",
      "Batch 30, Loss: 0.968899, Accuracy: 77.40%\n",
      "Batch 31, Loss: 0.906101, Accuracy: 77.62%\n",
      "Batch 32, Loss: 0.927866, Accuracy: 77.78%\n",
      "Batch 33, Loss: 0.908038, Accuracy: 77.94%\n",
      "Batch 34, Loss: 1.035844, Accuracy: 77.71%\n",
      "Batch 35, Loss: 0.931219, Accuracy: 77.86%\n",
      "Batch 36, Loss: 0.977323, Accuracy: 77.82%\n",
      "Batch 37, Loss: 1.035778, Accuracy: 77.58%\n",
      "Batch 38, Loss: 0.938162, Accuracy: 77.63%\n",
      "Batch 39, Loss: 0.996740, Accuracy: 77.60%\n",
      "Batch 40, Loss: 0.914677, Accuracy: 77.77%\n",
      "Batch 41, Loss: 0.953322, Accuracy: 77.86%\n",
      "Batch 42, Loss: 0.898484, Accuracy: 78.05%\n",
      "Batch 43, Loss: 0.945939, Accuracy: 78.09%\n",
      "Batch 44, Loss: 0.981299, Accuracy: 78.09%\n",
      "Batch 45, Loss: 0.960716, Accuracy: 78.09%\n",
      "Batch 46, Loss: 0.906419, Accuracy: 78.19%\n",
      "Batch 47, Loss: 0.892749, Accuracy: 78.39%\n",
      "Batch 48, Loss: 0.980921, Accuracy: 78.32%\n",
      "Batch 49, Loss: 1.002511, Accuracy: 78.25%\n",
      "Batch 50, Loss: 0.884126, Accuracy: 78.44%\n",
      "Batch 51, Loss: 0.920914, Accuracy: 78.52%\n",
      "Batch 52, Loss: 0.957166, Accuracy: 78.49%\n",
      "Batch 53, Loss: 0.969074, Accuracy: 78.42%\n",
      "Batch 54, Loss: 0.970788, Accuracy: 78.41%\n",
      "Batch 55, Loss: 0.938794, Accuracy: 78.44%\n",
      "Batch 56, Loss: 0.923999, Accuracy: 78.49%\n",
      "Batch 57, Loss: 0.987664, Accuracy: 78.43%\n",
      "Batch 58, Loss: 0.995396, Accuracy: 78.37%\n",
      "Batch 59, Loss: 0.942300, Accuracy: 78.39%\n",
      "Batch 60, Loss: 0.937604, Accuracy: 78.41%\n",
      "Batch 61, Loss: 1.002644, Accuracy: 78.33%\n",
      "Batch 62, Loss: 0.920874, Accuracy: 78.40%\n",
      "Batch 63, Loss: 0.974809, Accuracy: 78.40%\n",
      "Batch 64, Loss: 0.856474, Accuracy: 78.56%\n",
      "Batch 65, Loss: 1.000542, Accuracy: 78.49%\n",
      "Batch 66, Loss: 0.882382, Accuracy: 78.60%\n",
      "Batch 67, Loss: 0.983677, Accuracy: 78.52%\n",
      "Batch 68, Loss: 0.906611, Accuracy: 78.63%\n",
      "Batch 69, Loss: 0.938895, Accuracy: 78.65%\n",
      "Batch 70, Loss: 0.960655, Accuracy: 78.62%\n",
      "Batch 71, Loss: 0.905089, Accuracy: 78.70%\n",
      "Batch 72, Loss: 0.936250, Accuracy: 78.73%\n",
      "Batch 73, Loss: 0.950682, Accuracy: 78.72%\n",
      "Batch 74, Loss: 0.934089, Accuracy: 78.78%\n",
      "Batch 75, Loss: 0.869731, Accuracy: 78.92%\n",
      "Batch 76, Loss: 1.055223, Accuracy: 78.78%\n",
      "Batch 77, Loss: 1.086598, Accuracy: 78.63%\n",
      "Batch 78, Loss: 0.980000, Accuracy: 78.59%\n",
      "Batch 79, Loss: 0.967962, Accuracy: 78.60%\n",
      "Batch 80, Loss: 0.894687, Accuracy: 78.69%\n",
      "Batch 81, Loss: 0.913320, Accuracy: 78.74%\n",
      "Batch 82, Loss: 0.862527, Accuracy: 78.87%\n",
      "Batch 83, Loss: 0.928929, Accuracy: 78.90%\n",
      "Batch 84, Loss: 0.859353, Accuracy: 79.02%\n",
      "Batch 85, Loss: 1.029783, Accuracy: 78.92%\n",
      "Batch 86, Loss: 0.895490, Accuracy: 78.98%\n",
      "Batch 87, Loss: 0.907836, Accuracy: 79.02%\n",
      "Batch 88, Loss: 0.894387, Accuracy: 79.10%\n",
      "Batch 89, Loss: 0.936197, Accuracy: 79.11%\n",
      "Batch 90, Loss: 0.929520, Accuracy: 79.08%\n",
      "Batch 91, Loss: 1.010999, Accuracy: 79.00%\n",
      "Batch 92, Loss: 0.975230, Accuracy: 78.97%\n",
      "Batch 93, Loss: 0.864471, Accuracy: 79.08%\n",
      "Batch 94, Loss: 0.931478, Accuracy: 79.07%\n",
      "Batch 95, Loss: 0.917875, Accuracy: 79.10%\n",
      "Batch 96, Loss: 0.969995, Accuracy: 79.07%\n",
      "Batch 97, Loss: 0.947502, Accuracy: 79.09%\n",
      "Batch 98, Loss: 0.957348, Accuracy: 79.10%\n",
      "Batch 99, Loss: 1.014806, Accuracy: 79.01%\n",
      "Batch 100, Loss: 0.934713, Accuracy: 79.03%\n",
      "Batch 101, Loss: 0.990465, Accuracy: 78.99%\n",
      "Batch 102, Loss: 0.908079, Accuracy: 79.06%\n",
      "Batch 103, Loss: 0.984670, Accuracy: 79.02%\n",
      "Batch 104, Loss: 0.955826, Accuracy: 79.01%\n",
      "Batch 105, Loss: 0.885888, Accuracy: 79.08%\n",
      "Batch 106, Loss: 1.070330, Accuracy: 78.99%\n",
      "Batch 107, Loss: 0.936911, Accuracy: 78.99%\n",
      "Batch 108, Loss: 1.015872, Accuracy: 78.92%\n",
      "Batch 109, Loss: 0.894249, Accuracy: 78.97%\n",
      "Batch 110, Loss: 0.935994, Accuracy: 79.01%\n",
      "Batch 111, Loss: 0.959481, Accuracy: 79.00%\n",
      "Batch 112, Loss: 0.939696, Accuracy: 79.00%\n",
      "Batch 113, Loss: 0.931766, Accuracy: 79.01%\n",
      "Batch 114, Loss: 1.012293, Accuracy: 78.95%\n",
      "Batch 115, Loss: 0.905128, Accuracy: 78.99%\n",
      "Batch 116, Loss: 0.892620, Accuracy: 79.05%\n",
      "Batch 117, Loss: 0.933156, Accuracy: 79.09%\n",
      "Batch 118, Loss: 0.898182, Accuracy: 79.14%\n",
      "Batch 119, Loss: 0.979580, Accuracy: 79.12%\n",
      "Batch 120, Loss: 0.962662, Accuracy: 79.11%\n",
      "Batch 121, Loss: 0.934028, Accuracy: 79.12%\n",
      "Batch 122, Loss: 1.005598, Accuracy: 79.07%\n",
      "Batch 123, Loss: 0.888280, Accuracy: 79.13%\n",
      "Batch 124, Loss: 0.940584, Accuracy: 79.15%\n",
      "Batch 125, Loss: 0.893400, Accuracy: 79.17%\n",
      "Batch 126, Loss: 0.886462, Accuracy: 79.24%\n",
      "Batch 127, Loss: 0.906030, Accuracy: 79.28%\n",
      "Batch 128, Loss: 1.017212, Accuracy: 79.22%\n",
      "Batch 129, Loss: 0.940030, Accuracy: 79.26%\n",
      "Batch 130, Loss: 0.977708, Accuracy: 79.24%\n",
      "Batch 131, Loss: 0.917780, Accuracy: 79.28%\n",
      "Batch 132, Loss: 0.981300, Accuracy: 79.26%\n",
      "Batch 133, Loss: 0.845082, Accuracy: 79.34%\n",
      "Batch 134, Loss: 0.985150, Accuracy: 79.31%\n",
      "Batch 135, Loss: 0.881853, Accuracy: 79.39%\n",
      "Batch 136, Loss: 0.902767, Accuracy: 79.43%\n",
      "Batch 137, Loss: 1.013912, Accuracy: 79.38%\n",
      "Batch 138, Loss: 0.870141, Accuracy: 79.44%\n",
      "Batch 139, Loss: 0.933617, Accuracy: 79.45%\n",
      "Batch 140, Loss: 0.922686, Accuracy: 79.48%\n",
      "Batch 141, Loss: 0.920167, Accuracy: 79.50%\n",
      "Batch 142, Loss: 0.993220, Accuracy: 79.48%\n",
      "Batch 143, Loss: 0.921465, Accuracy: 79.50%\n",
      "Batch 144, Loss: 0.898675, Accuracy: 79.54%\n",
      "Batch 145, Loss: 0.887419, Accuracy: 79.60%\n",
      "Batch 146, Loss: 0.931784, Accuracy: 79.60%\n",
      "Batch 147, Loss: 0.953000, Accuracy: 79.60%\n",
      "Batch 148, Loss: 0.943186, Accuracy: 79.60%\n",
      "Batch 149, Loss: 0.907727, Accuracy: 79.64%\n",
      "Batch 150, Loss: 0.971215, Accuracy: 79.61%\n",
      "Batch 151, Loss: 0.992185, Accuracy: 79.58%\n",
      "Batch 152, Loss: 0.957631, Accuracy: 79.57%\n",
      "Batch 153, Loss: 0.963266, Accuracy: 79.54%\n",
      "Batch 154, Loss: 0.955687, Accuracy: 79.56%\n",
      "Batch 155, Loss: 0.881167, Accuracy: 79.61%\n",
      "Batch 156, Loss: 0.958960, Accuracy: 79.60%\n",
      "Batch 157, Loss: 0.973533, Accuracy: 79.60%\n",
      "Batch 158, Loss: 0.940170, Accuracy: 79.61%\n",
      "Batch 159, Loss: 1.038839, Accuracy: 79.55%\n",
      "Batch 160, Loss: 0.955658, Accuracy: 79.55%\n",
      "Batch 161, Loss: 0.929446, Accuracy: 79.56%\n",
      "Batch 162, Loss: 0.947355, Accuracy: 79.57%\n",
      "Batch 163, Loss: 0.975543, Accuracy: 79.55%\n",
      "Batch 164, Loss: 0.932744, Accuracy: 79.55%\n",
      "Batch 165, Loss: 0.975089, Accuracy: 79.54%\n",
      "Batch 166, Loss: 0.884412, Accuracy: 79.57%\n",
      "Batch 167, Loss: 0.962368, Accuracy: 79.58%\n",
      "Batch 168, Loss: 0.918069, Accuracy: 79.60%\n",
      "Batch 169, Loss: 0.910108, Accuracy: 79.62%\n",
      "Batch 170, Loss: 0.890704, Accuracy: 79.66%\n",
      "Batch 171, Loss: 1.001177, Accuracy: 79.62%\n",
      "Batch 172, Loss: 1.012298, Accuracy: 79.59%\n",
      "Batch 173, Loss: 0.885732, Accuracy: 79.62%\n",
      "Batch 174, Loss: 0.941266, Accuracy: 79.63%\n",
      "Batch 175, Loss: 0.846082, Accuracy: 79.69%\n",
      "Batch 176, Loss: 0.969054, Accuracy: 79.66%\n",
      "Batch 177, Loss: 0.943412, Accuracy: 79.64%\n",
      "Batch 178, Loss: 0.888141, Accuracy: 79.67%\n",
      "Batch 179, Loss: 0.886428, Accuracy: 79.71%\n",
      "Batch 180, Loss: 0.936630, Accuracy: 79.72%\n",
      "Batch 181, Loss: 0.942489, Accuracy: 79.73%\n",
      "Batch 182, Loss: 0.978387, Accuracy: 79.70%\n",
      "Batch 183, Loss: 0.923089, Accuracy: 79.72%\n",
      "Batch 184, Loss: 0.917946, Accuracy: 79.73%\n",
      "Batch 185, Loss: 0.984202, Accuracy: 79.71%\n",
      "Batch 186, Loss: 0.946344, Accuracy: 79.71%\n",
      "Batch 187, Loss: 0.850905, Accuracy: 79.76%\n",
      "Batch 188, Loss: 0.928799, Accuracy: 79.77%\n",
      "Batch 189, Loss: 0.961421, Accuracy: 79.75%\n",
      "Batch 190, Loss: 0.996885, Accuracy: 79.73%\n",
      "Batch 191, Loss: 0.903618, Accuracy: 79.76%\n",
      "Batch 192, Loss: 0.963755, Accuracy: 79.74%\n",
      "Batch 193, Loss: 0.911998, Accuracy: 79.78%\n",
      "Batch 194, Loss: 0.894666, Accuracy: 79.81%\n",
      "Batch 195, Loss: 0.878409, Accuracy: 79.84%\n",
      "Batch 196, Loss: 0.875761, Accuracy: 79.87%\n",
      "Batch 197, Loss: 0.951720, Accuracy: 79.85%\n",
      "Batch 198, Loss: 0.955708, Accuracy: 79.85%\n",
      "Batch 199, Loss: 0.935388, Accuracy: 79.85%\n",
      "Batch 200, Loss: 0.965905, Accuracy: 79.84%\n",
      "Batch 201, Loss: 0.926510, Accuracy: 79.85%\n",
      "Batch 202, Loss: 0.909607, Accuracy: 79.87%\n",
      "Batch 203, Loss: 1.004043, Accuracy: 79.85%\n",
      "Batch 204, Loss: 0.978054, Accuracy: 79.84%\n",
      "Batch 205, Loss: 0.925124, Accuracy: 79.86%\n",
      "Batch 206, Loss: 0.941417, Accuracy: 79.85%\n",
      "Batch 207, Loss: 0.883022, Accuracy: 79.88%\n",
      "Batch 208, Loss: 0.947225, Accuracy: 79.88%\n",
      "Batch 209, Loss: 0.889914, Accuracy: 79.91%\n",
      "Batch 210, Loss: 0.986121, Accuracy: 79.90%\n",
      "Batch 211, Loss: 0.862619, Accuracy: 79.93%\n",
      "Batch 212, Loss: 0.891882, Accuracy: 79.95%\n",
      "Batch 213, Loss: 1.016620, Accuracy: 79.92%\n",
      "Training - Epoch 44, Loss: 0.945170, Accuracy: 79.92%\n",
      "Validation Batch 1, Loss: 0.920599, Accuracy: 82.81%\n",
      "Validation Batch 2, Loss: 0.954041, Accuracy: 79.69%\n",
      "Validation Batch 3, Loss: 1.042324, Accuracy: 76.04%\n",
      "Validation Batch 4, Loss: 0.941225, Accuracy: 76.95%\n",
      "Validation Batch 5, Loss: 0.945641, Accuracy: 77.81%\n",
      "Validation Batch 6, Loss: 0.906772, Accuracy: 78.91%\n",
      "Validation Batch 7, Loss: 0.952998, Accuracy: 79.02%\n",
      "Validation Batch 8, Loss: 1.021516, Accuracy: 78.12%\n",
      "Validation Batch 9, Loss: 1.012721, Accuracy: 77.60%\n",
      "Validation Batch 10, Loss: 0.966389, Accuracy: 77.81%\n",
      "Validation Batch 11, Loss: 0.948026, Accuracy: 77.84%\n",
      "Validation Batch 12, Loss: 0.938205, Accuracy: 78.12%\n",
      "Validation Batch 13, Loss: 0.946032, Accuracy: 78.25%\n",
      "Validation Batch 14, Loss: 0.982765, Accuracy: 78.01%\n",
      "Validation Batch 15, Loss: 0.959179, Accuracy: 78.02%\n",
      "Validation Batch 16, Loss: 0.962773, Accuracy: 78.03%\n",
      "Validation Batch 17, Loss: 1.027005, Accuracy: 77.67%\n",
      "Validation Batch 18, Loss: 0.893918, Accuracy: 78.12%\n",
      "Validation Batch 19, Loss: 0.979903, Accuracy: 78.12%\n",
      "Validation Batch 20, Loss: 1.019147, Accuracy: 77.89%\n",
      "Validation Batch 21, Loss: 1.021347, Accuracy: 77.46%\n",
      "Validation Batch 22, Loss: 0.973654, Accuracy: 77.49%\n",
      "Validation Batch 23, Loss: 1.018886, Accuracy: 77.51%\n",
      "Validation Batch 24, Loss: 0.970841, Accuracy: 77.54%\n",
      "Validation Batch 25, Loss: 0.950289, Accuracy: 77.62%\n",
      "Validation Batch 26, Loss: 0.950518, Accuracy: 77.70%\n",
      "Validation Batch 27, Loss: 0.981035, Accuracy: 77.63%\n",
      "Validation - Epoch 44, Loss: 0.969917, Accuracy: 77.63%\n",
      "Patience—5\n",
      "Epoch 45\n",
      "Batch 1, Loss: 0.959927, Accuracy: 79.69%\n",
      "Batch 2, Loss: 0.982553, Accuracy: 77.34%\n",
      "Batch 3, Loss: 0.943068, Accuracy: 79.17%\n",
      "Batch 4, Loss: 0.931037, Accuracy: 80.08%\n",
      "Batch 5, Loss: 0.899010, Accuracy: 81.25%\n",
      "Batch 6, Loss: 0.948074, Accuracy: 80.99%\n",
      "Batch 7, Loss: 0.991778, Accuracy: 80.13%\n",
      "Batch 8, Loss: 0.972705, Accuracy: 79.69%\n",
      "Batch 9, Loss: 0.993605, Accuracy: 79.34%\n",
      "Batch 10, Loss: 0.944273, Accuracy: 79.53%\n",
      "Batch 11, Loss: 0.954332, Accuracy: 79.55%\n",
      "Batch 12, Loss: 0.894810, Accuracy: 80.08%\n",
      "Batch 13, Loss: 1.025834, Accuracy: 79.33%\n",
      "Batch 14, Loss: 0.895117, Accuracy: 79.69%\n",
      "Batch 15, Loss: 0.973575, Accuracy: 79.48%\n",
      "Batch 16, Loss: 0.953288, Accuracy: 79.49%\n",
      "Batch 17, Loss: 0.951674, Accuracy: 79.60%\n",
      "Batch 18, Loss: 0.844688, Accuracy: 80.12%\n",
      "Batch 19, Loss: 1.049253, Accuracy: 79.52%\n",
      "Batch 20, Loss: 0.907941, Accuracy: 79.84%\n",
      "Batch 21, Loss: 0.860095, Accuracy: 80.28%\n",
      "Batch 22, Loss: 0.955480, Accuracy: 80.26%\n",
      "Batch 23, Loss: 0.929764, Accuracy: 80.37%\n",
      "Batch 24, Loss: 0.926965, Accuracy: 80.40%\n",
      "Batch 25, Loss: 0.898253, Accuracy: 80.56%\n",
      "Batch 26, Loss: 0.908569, Accuracy: 80.71%\n",
      "Batch 27, Loss: 0.934851, Accuracy: 80.79%\n",
      "Batch 28, Loss: 1.040911, Accuracy: 80.36%\n",
      "Batch 29, Loss: 0.919292, Accuracy: 80.44%\n",
      "Batch 30, Loss: 0.930670, Accuracy: 80.52%\n",
      "Batch 31, Loss: 0.846272, Accuracy: 80.85%\n",
      "Batch 32, Loss: 0.931152, Accuracy: 80.81%\n",
      "Batch 33, Loss: 0.952945, Accuracy: 80.82%\n",
      "Batch 34, Loss: 0.981876, Accuracy: 80.74%\n",
      "Batch 35, Loss: 0.899661, Accuracy: 80.89%\n",
      "Batch 36, Loss: 1.018252, Accuracy: 80.64%\n",
      "Batch 37, Loss: 0.953417, Accuracy: 80.57%\n",
      "Batch 38, Loss: 0.951525, Accuracy: 80.59%\n",
      "Batch 39, Loss: 0.869712, Accuracy: 80.81%\n",
      "Batch 40, Loss: 0.912143, Accuracy: 80.86%\n",
      "Batch 41, Loss: 0.918262, Accuracy: 80.83%\n",
      "Batch 42, Loss: 0.904641, Accuracy: 80.92%\n",
      "Batch 43, Loss: 0.885649, Accuracy: 81.03%\n",
      "Batch 44, Loss: 0.928866, Accuracy: 81.00%\n",
      "Batch 45, Loss: 0.959614, Accuracy: 80.97%\n",
      "Batch 46, Loss: 0.878915, Accuracy: 81.11%\n",
      "Batch 47, Loss: 0.937351, Accuracy: 81.12%\n",
      "Batch 48, Loss: 0.899663, Accuracy: 81.18%\n",
      "Batch 49, Loss: 0.891867, Accuracy: 81.31%\n",
      "Batch 50, Loss: 0.867589, Accuracy: 81.44%\n",
      "Batch 51, Loss: 0.897833, Accuracy: 81.46%\n",
      "Batch 52, Loss: 0.901361, Accuracy: 81.58%\n",
      "Batch 53, Loss: 0.941803, Accuracy: 81.54%\n",
      "Batch 54, Loss: 0.939277, Accuracy: 81.51%\n",
      "Batch 55, Loss: 1.038142, Accuracy: 81.31%\n",
      "Batch 56, Loss: 0.868807, Accuracy: 81.47%\n",
      "Batch 57, Loss: 0.926411, Accuracy: 81.50%\n",
      "Batch 58, Loss: 0.971310, Accuracy: 81.44%\n",
      "Batch 59, Loss: 0.906979, Accuracy: 81.49%\n",
      "Batch 60, Loss: 0.988794, Accuracy: 81.43%\n",
      "Batch 61, Loss: 0.916156, Accuracy: 81.43%\n",
      "Batch 62, Loss: 0.877605, Accuracy: 81.53%\n",
      "Batch 63, Loss: 0.899845, Accuracy: 81.57%\n",
      "Batch 64, Loss: 0.935794, Accuracy: 81.57%\n",
      "Batch 65, Loss: 0.986605, Accuracy: 81.47%\n",
      "Batch 66, Loss: 0.921464, Accuracy: 81.49%\n",
      "Batch 67, Loss: 0.989207, Accuracy: 81.41%\n",
      "Batch 68, Loss: 0.953313, Accuracy: 81.36%\n",
      "Batch 69, Loss: 0.851018, Accuracy: 81.48%\n",
      "Batch 70, Loss: 1.000424, Accuracy: 81.36%\n",
      "Batch 71, Loss: 0.994156, Accuracy: 81.27%\n",
      "Batch 72, Loss: 0.910087, Accuracy: 81.32%\n",
      "Batch 73, Loss: 0.968046, Accuracy: 81.27%\n",
      "Batch 74, Loss: 0.893048, Accuracy: 81.33%\n",
      "Batch 75, Loss: 0.921943, Accuracy: 81.38%\n",
      "Batch 76, Loss: 0.924382, Accuracy: 81.37%\n",
      "Batch 77, Loss: 0.896493, Accuracy: 81.41%\n",
      "Batch 78, Loss: 0.868725, Accuracy: 81.49%\n",
      "Batch 79, Loss: 0.875410, Accuracy: 81.57%\n",
      "Batch 80, Loss: 0.890773, Accuracy: 81.62%\n",
      "Batch 81, Loss: 0.946704, Accuracy: 81.62%\n",
      "Batch 82, Loss: 0.907627, Accuracy: 81.65%\n",
      "Batch 83, Loss: 0.964009, Accuracy: 81.63%\n",
      "Batch 84, Loss: 0.925812, Accuracy: 81.64%\n",
      "Batch 85, Loss: 0.958805, Accuracy: 81.58%\n",
      "Batch 86, Loss: 0.900082, Accuracy: 81.61%\n",
      "Batch 87, Loss: 1.038469, Accuracy: 81.45%\n",
      "Batch 88, Loss: 0.935212, Accuracy: 81.45%\n",
      "Batch 89, Loss: 0.895108, Accuracy: 81.48%\n",
      "Batch 90, Loss: 0.922084, Accuracy: 81.49%\n",
      "Batch 91, Loss: 0.963610, Accuracy: 81.44%\n",
      "Batch 92, Loss: 1.028411, Accuracy: 81.33%\n",
      "Batch 93, Loss: 0.982705, Accuracy: 81.27%\n",
      "Batch 94, Loss: 0.965975, Accuracy: 81.23%\n",
      "Batch 95, Loss: 0.922610, Accuracy: 81.23%\n",
      "Batch 96, Loss: 0.961733, Accuracy: 81.22%\n",
      "Batch 97, Loss: 0.990133, Accuracy: 81.15%\n",
      "Batch 98, Loss: 0.887307, Accuracy: 81.20%\n",
      "Batch 99, Loss: 0.836593, Accuracy: 81.31%\n",
      "Batch 100, Loss: 0.939763, Accuracy: 81.31%\n",
      "Batch 101, Loss: 0.916943, Accuracy: 81.31%\n",
      "Batch 102, Loss: 0.935377, Accuracy: 81.31%\n",
      "Batch 103, Loss: 0.883443, Accuracy: 81.34%\n",
      "Batch 104, Loss: 0.860707, Accuracy: 81.42%\n",
      "Batch 105, Loss: 0.922900, Accuracy: 81.41%\n",
      "Batch 106, Loss: 0.895621, Accuracy: 81.46%\n",
      "Batch 107, Loss: 0.945943, Accuracy: 81.44%\n",
      "Batch 108, Loss: 0.969024, Accuracy: 81.39%\n",
      "Batch 109, Loss: 0.981547, Accuracy: 81.34%\n",
      "Batch 110, Loss: 0.951009, Accuracy: 81.31%\n",
      "Batch 111, Loss: 0.941571, Accuracy: 81.26%\n",
      "Batch 112, Loss: 0.978463, Accuracy: 81.21%\n",
      "Batch 113, Loss: 0.937692, Accuracy: 81.21%\n",
      "Batch 114, Loss: 0.887264, Accuracy: 81.25%\n",
      "Batch 115, Loss: 0.908859, Accuracy: 81.26%\n",
      "Batch 116, Loss: 0.929771, Accuracy: 81.28%\n",
      "Batch 117, Loss: 0.957917, Accuracy: 81.25%\n",
      "Batch 118, Loss: 0.981275, Accuracy: 81.20%\n",
      "Batch 119, Loss: 0.923345, Accuracy: 81.21%\n",
      "Batch 120, Loss: 0.933733, Accuracy: 81.21%\n",
      "Batch 121, Loss: 0.904319, Accuracy: 81.24%\n",
      "Batch 122, Loss: 0.932317, Accuracy: 81.24%\n",
      "Batch 123, Loss: 0.926492, Accuracy: 81.25%\n",
      "Batch 124, Loss: 0.905764, Accuracy: 81.26%\n",
      "Batch 125, Loss: 0.909848, Accuracy: 81.28%\n",
      "Batch 126, Loss: 0.873752, Accuracy: 81.34%\n",
      "Batch 127, Loss: 0.970434, Accuracy: 81.30%\n",
      "Batch 128, Loss: 0.991819, Accuracy: 81.24%\n",
      "Batch 129, Loss: 0.940357, Accuracy: 81.24%\n",
      "Batch 130, Loss: 0.859747, Accuracy: 81.30%\n",
      "Batch 131, Loss: 0.946242, Accuracy: 81.30%\n",
      "Batch 132, Loss: 0.957562, Accuracy: 81.27%\n",
      "Batch 133, Loss: 1.006926, Accuracy: 81.20%\n",
      "Batch 134, Loss: 0.997437, Accuracy: 81.13%\n",
      "Batch 135, Loss: 0.913429, Accuracy: 81.15%\n",
      "Batch 136, Loss: 0.895696, Accuracy: 81.17%\n",
      "Batch 137, Loss: 0.897812, Accuracy: 81.19%\n",
      "Batch 138, Loss: 0.890032, Accuracy: 81.24%\n",
      "Batch 139, Loss: 0.893264, Accuracy: 81.28%\n",
      "Batch 140, Loss: 0.939336, Accuracy: 81.27%\n",
      "Batch 141, Loss: 0.906318, Accuracy: 81.29%\n",
      "Batch 142, Loss: 0.932172, Accuracy: 81.28%\n",
      "Batch 143, Loss: 0.935582, Accuracy: 81.27%\n",
      "Batch 144, Loss: 0.876585, Accuracy: 81.32%\n",
      "Batch 145, Loss: 0.933386, Accuracy: 81.31%\n",
      "Batch 146, Loss: 0.914602, Accuracy: 81.34%\n",
      "Batch 147, Loss: 1.023596, Accuracy: 81.27%\n",
      "Batch 148, Loss: 0.933470, Accuracy: 81.28%\n",
      "Batch 149, Loss: 0.889489, Accuracy: 81.31%\n",
      "Batch 150, Loss: 0.935726, Accuracy: 81.31%\n",
      "Batch 151, Loss: 0.889536, Accuracy: 81.33%\n",
      "Batch 152, Loss: 0.944578, Accuracy: 81.32%\n",
      "Batch 153, Loss: 0.928922, Accuracy: 81.32%\n",
      "Batch 154, Loss: 0.943697, Accuracy: 81.30%\n",
      "Batch 155, Loss: 0.979813, Accuracy: 81.27%\n",
      "Batch 156, Loss: 0.915241, Accuracy: 81.27%\n",
      "Batch 157, Loss: 0.962119, Accuracy: 81.24%\n",
      "Batch 158, Loss: 1.043343, Accuracy: 81.16%\n",
      "Batch 159, Loss: 0.935194, Accuracy: 81.17%\n",
      "Batch 160, Loss: 0.960609, Accuracy: 81.16%\n",
      "Batch 161, Loss: 0.966224, Accuracy: 81.14%\n",
      "Batch 162, Loss: 0.865550, Accuracy: 81.19%\n",
      "Batch 163, Loss: 0.931480, Accuracy: 81.20%\n",
      "Batch 164, Loss: 0.981885, Accuracy: 81.15%\n",
      "Batch 165, Loss: 0.915356, Accuracy: 81.16%\n",
      "Batch 166, Loss: 0.981052, Accuracy: 81.14%\n",
      "Batch 167, Loss: 0.940112, Accuracy: 81.14%\n",
      "Batch 168, Loss: 0.902037, Accuracy: 81.16%\n",
      "Batch 169, Loss: 0.902047, Accuracy: 81.19%\n",
      "Batch 170, Loss: 0.963010, Accuracy: 81.17%\n",
      "Batch 171, Loss: 0.942121, Accuracy: 81.18%\n",
      "Batch 172, Loss: 0.938476, Accuracy: 81.17%\n",
      "Batch 173, Loss: 0.923025, Accuracy: 81.19%\n",
      "Batch 174, Loss: 0.923554, Accuracy: 81.20%\n",
      "Batch 175, Loss: 0.967221, Accuracy: 81.17%\n",
      "Batch 176, Loss: 0.966009, Accuracy: 81.16%\n",
      "Batch 177, Loss: 0.943818, Accuracy: 81.14%\n",
      "Batch 178, Loss: 0.945361, Accuracy: 81.14%\n",
      "Batch 179, Loss: 0.918662, Accuracy: 81.15%\n",
      "Batch 180, Loss: 0.888744, Accuracy: 81.17%\n",
      "Batch 181, Loss: 0.952003, Accuracy: 81.17%\n",
      "Batch 182, Loss: 0.961782, Accuracy: 81.16%\n",
      "Batch 183, Loss: 1.008380, Accuracy: 81.12%\n",
      "Batch 184, Loss: 0.981248, Accuracy: 81.09%\n",
      "Batch 185, Loss: 1.066194, Accuracy: 81.01%\n",
      "Batch 186, Loss: 0.956471, Accuracy: 81.00%\n",
      "Batch 187, Loss: 0.910503, Accuracy: 81.02%\n",
      "Batch 188, Loss: 0.896543, Accuracy: 81.03%\n",
      "Batch 189, Loss: 0.928121, Accuracy: 81.04%\n",
      "Batch 190, Loss: 0.931260, Accuracy: 81.03%\n",
      "Batch 191, Loss: 0.943796, Accuracy: 81.02%\n",
      "Batch 192, Loss: 0.982343, Accuracy: 81.00%\n",
      "Batch 193, Loss: 0.944764, Accuracy: 81.00%\n",
      "Batch 194, Loss: 0.930535, Accuracy: 81.02%\n",
      "Batch 195, Loss: 0.896538, Accuracy: 81.05%\n",
      "Batch 196, Loss: 0.911144, Accuracy: 81.07%\n",
      "Batch 197, Loss: 0.986431, Accuracy: 81.04%\n",
      "Batch 198, Loss: 0.901355, Accuracy: 81.06%\n",
      "Batch 199, Loss: 0.970666, Accuracy: 81.05%\n",
      "Batch 200, Loss: 0.898654, Accuracy: 81.06%\n",
      "Batch 201, Loss: 0.938069, Accuracy: 81.05%\n",
      "Batch 202, Loss: 0.984419, Accuracy: 81.02%\n",
      "Batch 203, Loss: 0.990116, Accuracy: 81.00%\n",
      "Batch 204, Loss: 0.929863, Accuracy: 81.00%\n",
      "Batch 205, Loss: 0.865742, Accuracy: 81.05%\n",
      "Batch 206, Loss: 0.888301, Accuracy: 81.08%\n",
      "Batch 207, Loss: 1.000738, Accuracy: 81.04%\n",
      "Batch 208, Loss: 0.965507, Accuracy: 81.02%\n",
      "Batch 209, Loss: 0.887769, Accuracy: 81.05%\n",
      "Batch 210, Loss: 0.948507, Accuracy: 81.05%\n",
      "Batch 211, Loss: 0.926583, Accuracy: 81.05%\n",
      "Batch 212, Loss: 0.949907, Accuracy: 81.04%\n",
      "Batch 213, Loss: 0.886889, Accuracy: 81.06%\n",
      "Training - Epoch 45, Loss: 0.935421, Accuracy: 81.06%\n",
      "Validation Batch 1, Loss: 0.907862, Accuracy: 84.38%\n",
      "Validation Batch 2, Loss: 0.932645, Accuracy: 82.81%\n",
      "Validation Batch 3, Loss: 1.031863, Accuracy: 78.12%\n",
      "Validation Batch 4, Loss: 0.934142, Accuracy: 78.52%\n",
      "Validation Batch 5, Loss: 0.917287, Accuracy: 79.69%\n",
      "Validation Batch 6, Loss: 0.889765, Accuracy: 80.73%\n",
      "Validation Batch 7, Loss: 0.936504, Accuracy: 81.25%\n",
      "Validation Batch 8, Loss: 1.009690, Accuracy: 80.27%\n",
      "Validation Batch 9, Loss: 0.998149, Accuracy: 79.86%\n",
      "Validation Batch 10, Loss: 0.950153, Accuracy: 79.84%\n",
      "Validation Batch 11, Loss: 0.935356, Accuracy: 79.83%\n",
      "Validation Batch 12, Loss: 0.925192, Accuracy: 80.08%\n",
      "Validation Batch 13, Loss: 0.929495, Accuracy: 80.17%\n",
      "Validation Batch 14, Loss: 0.965111, Accuracy: 80.13%\n",
      "Validation Batch 15, Loss: 0.940769, Accuracy: 80.10%\n",
      "Validation Batch 16, Loss: 0.945394, Accuracy: 80.08%\n",
      "Validation Batch 17, Loss: 0.999616, Accuracy: 79.87%\n",
      "Validation Batch 18, Loss: 0.883017, Accuracy: 80.30%\n",
      "Validation Batch 19, Loss: 0.962402, Accuracy: 80.26%\n",
      "Validation Batch 20, Loss: 1.006257, Accuracy: 80.00%\n",
      "Validation Batch 21, Loss: 1.001350, Accuracy: 79.54%\n",
      "Validation Batch 22, Loss: 0.947337, Accuracy: 79.62%\n",
      "Validation Batch 23, Loss: 0.988492, Accuracy: 79.55%\n",
      "Validation Batch 24, Loss: 0.950368, Accuracy: 79.56%\n",
      "Validation Batch 25, Loss: 0.921812, Accuracy: 79.62%\n",
      "Validation Batch 26, Loss: 0.936044, Accuracy: 79.69%\n",
      "Validation Batch 27, Loss: 0.953100, Accuracy: 79.62%\n",
      "Validation - Epoch 45, Loss: 0.951821, Accuracy: 79.62%\n",
      "Patience—6\n",
      "Epoch 46\n",
      "Batch 1, Loss: 0.938801, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.975927, Accuracy: 79.69%\n",
      "Batch 3, Loss: 0.980186, Accuracy: 79.17%\n",
      "Batch 4, Loss: 0.919280, Accuracy: 79.69%\n",
      "Batch 5, Loss: 0.940757, Accuracy: 79.69%\n",
      "Batch 6, Loss: 0.910378, Accuracy: 80.47%\n",
      "Batch 7, Loss: 0.884921, Accuracy: 81.47%\n",
      "Batch 8, Loss: 0.955610, Accuracy: 81.05%\n",
      "Batch 9, Loss: 0.936369, Accuracy: 81.25%\n",
      "Batch 10, Loss: 0.948403, Accuracy: 80.94%\n",
      "Batch 11, Loss: 0.970231, Accuracy: 80.68%\n",
      "Batch 12, Loss: 0.885416, Accuracy: 81.25%\n",
      "Batch 13, Loss: 0.919699, Accuracy: 81.25%\n",
      "Batch 14, Loss: 0.944899, Accuracy: 81.25%\n",
      "Batch 15, Loss: 0.907292, Accuracy: 81.35%\n",
      "Batch 16, Loss: 0.901938, Accuracy: 81.54%\n",
      "Batch 17, Loss: 0.945171, Accuracy: 81.34%\n",
      "Batch 18, Loss: 0.962528, Accuracy: 81.08%\n",
      "Batch 19, Loss: 0.933384, Accuracy: 81.09%\n",
      "Batch 20, Loss: 0.921059, Accuracy: 81.17%\n",
      "Batch 21, Loss: 0.931375, Accuracy: 81.25%\n",
      "Batch 22, Loss: 0.929120, Accuracy: 81.32%\n",
      "Batch 23, Loss: 0.933885, Accuracy: 81.25%\n",
      "Batch 24, Loss: 0.907423, Accuracy: 81.38%\n",
      "Batch 25, Loss: 0.909998, Accuracy: 81.44%\n",
      "Batch 26, Loss: 0.979993, Accuracy: 81.31%\n",
      "Batch 27, Loss: 0.955475, Accuracy: 81.25%\n",
      "Batch 28, Loss: 0.933450, Accuracy: 81.19%\n",
      "Batch 29, Loss: 0.854145, Accuracy: 81.47%\n",
      "Batch 30, Loss: 0.974626, Accuracy: 81.25%\n",
      "Batch 31, Loss: 0.940701, Accuracy: 81.20%\n",
      "Batch 32, Loss: 0.955348, Accuracy: 81.10%\n",
      "Batch 33, Loss: 0.885085, Accuracy: 81.25%\n",
      "Batch 34, Loss: 0.913791, Accuracy: 81.25%\n",
      "Batch 35, Loss: 0.948109, Accuracy: 81.25%\n",
      "Batch 36, Loss: 0.905726, Accuracy: 81.34%\n",
      "Batch 37, Loss: 0.967880, Accuracy: 81.25%\n",
      "Batch 38, Loss: 0.858419, Accuracy: 81.46%\n",
      "Batch 39, Loss: 0.929961, Accuracy: 81.41%\n",
      "Batch 40, Loss: 0.893892, Accuracy: 81.48%\n",
      "Batch 41, Loss: 0.932957, Accuracy: 81.44%\n",
      "Batch 42, Loss: 0.934662, Accuracy: 81.40%\n",
      "Batch 43, Loss: 1.026151, Accuracy: 81.21%\n",
      "Batch 44, Loss: 0.904437, Accuracy: 81.29%\n",
      "Batch 45, Loss: 0.880246, Accuracy: 81.42%\n",
      "Batch 46, Loss: 0.901832, Accuracy: 81.52%\n",
      "Batch 47, Loss: 0.847286, Accuracy: 81.68%\n",
      "Batch 48, Loss: 1.009616, Accuracy: 81.45%\n",
      "Batch 49, Loss: 0.907617, Accuracy: 81.47%\n",
      "Batch 50, Loss: 0.942019, Accuracy: 81.47%\n",
      "Batch 51, Loss: 0.936079, Accuracy: 81.43%\n",
      "Batch 52, Loss: 0.954686, Accuracy: 81.43%\n",
      "Batch 53, Loss: 0.884499, Accuracy: 81.52%\n",
      "Batch 54, Loss: 0.960904, Accuracy: 81.45%\n",
      "Batch 55, Loss: 0.916921, Accuracy: 81.51%\n",
      "Batch 56, Loss: 0.939452, Accuracy: 81.50%\n",
      "Batch 57, Loss: 0.983589, Accuracy: 81.41%\n",
      "Batch 58, Loss: 1.056364, Accuracy: 81.20%\n",
      "Batch 59, Loss: 0.954372, Accuracy: 81.14%\n",
      "Batch 60, Loss: 0.865815, Accuracy: 81.28%\n",
      "Batch 61, Loss: 0.852919, Accuracy: 81.43%\n",
      "Batch 62, Loss: 0.946133, Accuracy: 81.40%\n",
      "Batch 63, Loss: 0.973232, Accuracy: 81.32%\n",
      "Batch 64, Loss: 0.952772, Accuracy: 81.30%\n",
      "Batch 65, Loss: 0.928687, Accuracy: 81.32%\n",
      "Batch 66, Loss: 0.860041, Accuracy: 81.44%\n",
      "Batch 67, Loss: 0.941119, Accuracy: 81.44%\n",
      "Batch 68, Loss: 0.941056, Accuracy: 81.43%\n",
      "Batch 69, Loss: 1.032977, Accuracy: 81.25%\n",
      "Batch 70, Loss: 0.910879, Accuracy: 81.27%\n",
      "Batch 71, Loss: 1.002043, Accuracy: 81.16%\n",
      "Batch 72, Loss: 0.879542, Accuracy: 81.25%\n",
      "Batch 73, Loss: 0.857035, Accuracy: 81.36%\n",
      "Batch 74, Loss: 0.924545, Accuracy: 81.38%\n",
      "Batch 75, Loss: 0.984969, Accuracy: 81.29%\n",
      "Batch 76, Loss: 0.894818, Accuracy: 81.35%\n",
      "Batch 77, Loss: 0.888965, Accuracy: 81.39%\n",
      "Batch 78, Loss: 0.913808, Accuracy: 81.43%\n",
      "Batch 79, Loss: 0.939057, Accuracy: 81.39%\n",
      "Batch 80, Loss: 0.896812, Accuracy: 81.46%\n",
      "Batch 81, Loss: 0.962704, Accuracy: 81.42%\n",
      "Batch 82, Loss: 0.901293, Accuracy: 81.46%\n",
      "Batch 83, Loss: 0.812317, Accuracy: 81.61%\n",
      "Batch 84, Loss: 0.912080, Accuracy: 81.64%\n",
      "Batch 85, Loss: 0.934691, Accuracy: 81.62%\n",
      "Batch 86, Loss: 0.919730, Accuracy: 81.65%\n",
      "Batch 87, Loss: 0.899979, Accuracy: 81.68%\n",
      "Batch 88, Loss: 0.909613, Accuracy: 81.73%\n",
      "Batch 89, Loss: 0.907453, Accuracy: 81.74%\n",
      "Batch 90, Loss: 0.911679, Accuracy: 81.77%\n",
      "Batch 91, Loss: 0.880712, Accuracy: 81.83%\n",
      "Batch 92, Loss: 0.913408, Accuracy: 81.86%\n",
      "Batch 93, Loss: 0.960530, Accuracy: 81.80%\n",
      "Batch 94, Loss: 0.905772, Accuracy: 81.85%\n",
      "Batch 95, Loss: 0.841736, Accuracy: 81.96%\n",
      "Batch 96, Loss: 0.926603, Accuracy: 81.97%\n",
      "Batch 97, Loss: 0.970903, Accuracy: 81.91%\n",
      "Batch 98, Loss: 0.946422, Accuracy: 81.89%\n",
      "Batch 99, Loss: 0.885584, Accuracy: 81.96%\n",
      "Batch 100, Loss: 0.915898, Accuracy: 81.98%\n",
      "Batch 101, Loss: 0.875204, Accuracy: 82.04%\n",
      "Batch 102, Loss: 0.918905, Accuracy: 82.05%\n",
      "Batch 103, Loss: 0.999195, Accuracy: 81.98%\n",
      "Batch 104, Loss: 0.899628, Accuracy: 82.00%\n",
      "Batch 105, Loss: 0.946772, Accuracy: 81.98%\n",
      "Batch 106, Loss: 0.919139, Accuracy: 81.99%\n",
      "Batch 107, Loss: 0.876262, Accuracy: 82.05%\n",
      "Batch 108, Loss: 0.977786, Accuracy: 82.02%\n",
      "Batch 109, Loss: 0.939847, Accuracy: 82.00%\n",
      "Batch 110, Loss: 0.903297, Accuracy: 82.02%\n",
      "Batch 111, Loss: 0.972548, Accuracy: 81.97%\n",
      "Batch 112, Loss: 0.935832, Accuracy: 81.95%\n",
      "Batch 113, Loss: 0.900825, Accuracy: 81.97%\n",
      "Batch 114, Loss: 0.988609, Accuracy: 81.89%\n",
      "Batch 115, Loss: 0.946037, Accuracy: 81.89%\n",
      "Batch 116, Loss: 0.958521, Accuracy: 81.86%\n",
      "Batch 117, Loss: 0.888013, Accuracy: 81.89%\n",
      "Batch 118, Loss: 0.932161, Accuracy: 81.89%\n",
      "Batch 119, Loss: 0.948677, Accuracy: 81.84%\n",
      "Batch 120, Loss: 0.936762, Accuracy: 81.85%\n",
      "Batch 121, Loss: 0.901967, Accuracy: 81.87%\n",
      "Batch 122, Loss: 0.872178, Accuracy: 81.92%\n",
      "Batch 123, Loss: 0.922641, Accuracy: 81.92%\n",
      "Batch 124, Loss: 1.042881, Accuracy: 81.82%\n",
      "Batch 125, Loss: 0.949269, Accuracy: 81.78%\n",
      "Batch 126, Loss: 0.973485, Accuracy: 81.75%\n",
      "Batch 127, Loss: 0.988171, Accuracy: 81.71%\n",
      "Batch 128, Loss: 0.997179, Accuracy: 81.65%\n",
      "Batch 129, Loss: 0.904141, Accuracy: 81.67%\n",
      "Batch 130, Loss: 0.888132, Accuracy: 81.71%\n",
      "Batch 131, Loss: 0.905728, Accuracy: 81.74%\n",
      "Batch 132, Loss: 0.953590, Accuracy: 81.72%\n",
      "Batch 133, Loss: 0.877974, Accuracy: 81.76%\n",
      "Batch 134, Loss: 0.960745, Accuracy: 81.70%\n",
      "Batch 135, Loss: 0.926593, Accuracy: 81.69%\n",
      "Batch 136, Loss: 0.982569, Accuracy: 81.65%\n",
      "Batch 137, Loss: 0.904603, Accuracy: 81.66%\n",
      "Batch 138, Loss: 0.926748, Accuracy: 81.66%\n",
      "Batch 139, Loss: 0.889132, Accuracy: 81.69%\n",
      "Batch 140, Loss: 0.912034, Accuracy: 81.69%\n",
      "Batch 141, Loss: 0.886435, Accuracy: 81.72%\n",
      "Batch 142, Loss: 0.913313, Accuracy: 81.73%\n",
      "Batch 143, Loss: 0.892860, Accuracy: 81.75%\n",
      "Batch 144, Loss: 0.917975, Accuracy: 81.76%\n",
      "Batch 145, Loss: 0.948056, Accuracy: 81.75%\n",
      "Batch 146, Loss: 0.983021, Accuracy: 81.71%\n",
      "Batch 147, Loss: 0.944093, Accuracy: 81.70%\n",
      "Batch 148, Loss: 0.947857, Accuracy: 81.67%\n",
      "Batch 149, Loss: 0.943955, Accuracy: 81.66%\n",
      "Batch 150, Loss: 0.921451, Accuracy: 81.68%\n",
      "Batch 151, Loss: 0.993391, Accuracy: 81.63%\n",
      "Batch 152, Loss: 0.964736, Accuracy: 81.62%\n",
      "Batch 153, Loss: 0.931622, Accuracy: 81.61%\n",
      "Batch 154, Loss: 0.973917, Accuracy: 81.57%\n",
      "Batch 155, Loss: 0.882310, Accuracy: 81.61%\n",
      "Batch 156, Loss: 1.034310, Accuracy: 81.54%\n",
      "Batch 157, Loss: 0.968760, Accuracy: 81.51%\n",
      "Batch 158, Loss: 0.930397, Accuracy: 81.52%\n",
      "Batch 159, Loss: 1.021032, Accuracy: 81.47%\n",
      "Batch 160, Loss: 0.940338, Accuracy: 81.46%\n",
      "Batch 161, Loss: 0.898747, Accuracy: 81.48%\n",
      "Batch 162, Loss: 0.902064, Accuracy: 81.51%\n",
      "Batch 163, Loss: 1.007133, Accuracy: 81.47%\n",
      "Batch 164, Loss: 0.879612, Accuracy: 81.52%\n",
      "Batch 165, Loss: 0.922862, Accuracy: 81.53%\n",
      "Batch 166, Loss: 0.945988, Accuracy: 81.53%\n",
      "Batch 167, Loss: 0.880343, Accuracy: 81.56%\n",
      "Batch 168, Loss: 0.928448, Accuracy: 81.55%\n",
      "Batch 169, Loss: 0.924673, Accuracy: 81.56%\n",
      "Batch 170, Loss: 0.915045, Accuracy: 81.55%\n",
      "Batch 171, Loss: 0.939141, Accuracy: 81.54%\n",
      "Batch 172, Loss: 0.921477, Accuracy: 81.54%\n",
      "Batch 173, Loss: 0.923865, Accuracy: 81.56%\n",
      "Batch 174, Loss: 0.923433, Accuracy: 81.56%\n",
      "Batch 175, Loss: 0.941036, Accuracy: 81.54%\n",
      "Batch 176, Loss: 0.937253, Accuracy: 81.53%\n",
      "Batch 177, Loss: 0.896633, Accuracy: 81.54%\n",
      "Batch 178, Loss: 0.840204, Accuracy: 81.61%\n",
      "Batch 179, Loss: 0.929292, Accuracy: 81.60%\n",
      "Batch 180, Loss: 0.967987, Accuracy: 81.57%\n",
      "Batch 181, Loss: 0.968766, Accuracy: 81.54%\n",
      "Batch 182, Loss: 0.934991, Accuracy: 81.55%\n",
      "Batch 183, Loss: 0.915690, Accuracy: 81.56%\n",
      "Batch 184, Loss: 0.937689, Accuracy: 81.56%\n",
      "Batch 185, Loss: 0.965980, Accuracy: 81.54%\n",
      "Batch 186, Loss: 0.893135, Accuracy: 81.55%\n",
      "Batch 187, Loss: 0.901352, Accuracy: 81.56%\n",
      "Batch 188, Loss: 0.878554, Accuracy: 81.59%\n",
      "Batch 189, Loss: 0.951111, Accuracy: 81.58%\n",
      "Batch 190, Loss: 0.937495, Accuracy: 81.56%\n",
      "Batch 191, Loss: 0.962253, Accuracy: 81.54%\n",
      "Batch 192, Loss: 0.915170, Accuracy: 81.54%\n",
      "Batch 193, Loss: 0.923543, Accuracy: 81.55%\n",
      "Batch 194, Loss: 0.951404, Accuracy: 81.53%\n",
      "Batch 195, Loss: 0.954128, Accuracy: 81.52%\n",
      "Batch 196, Loss: 0.886609, Accuracy: 81.54%\n",
      "Batch 197, Loss: 0.934330, Accuracy: 81.54%\n",
      "Batch 198, Loss: 0.920420, Accuracy: 81.56%\n",
      "Batch 199, Loss: 0.898190, Accuracy: 81.56%\n",
      "Batch 200, Loss: 0.910048, Accuracy: 81.57%\n",
      "Batch 201, Loss: 0.964804, Accuracy: 81.55%\n",
      "Batch 202, Loss: 0.933663, Accuracy: 81.55%\n",
      "Batch 203, Loss: 0.878982, Accuracy: 81.58%\n",
      "Batch 204, Loss: 0.890184, Accuracy: 81.60%\n",
      "Batch 205, Loss: 0.989157, Accuracy: 81.56%\n",
      "Batch 206, Loss: 0.989539, Accuracy: 81.52%\n",
      "Batch 207, Loss: 0.872067, Accuracy: 81.57%\n",
      "Batch 208, Loss: 0.952346, Accuracy: 81.57%\n",
      "Batch 209, Loss: 0.904551, Accuracy: 81.59%\n",
      "Batch 210, Loss: 0.995320, Accuracy: 81.55%\n",
      "Batch 211, Loss: 0.893289, Accuracy: 81.56%\n",
      "Batch 212, Loss: 0.847546, Accuracy: 81.60%\n",
      "Batch 213, Loss: 0.914335, Accuracy: 81.61%\n",
      "Training - Epoch 46, Loss: 0.929325, Accuracy: 81.61%\n",
      "Validation Batch 1, Loss: 0.918908, Accuracy: 84.38%\n",
      "Validation Batch 2, Loss: 0.943404, Accuracy: 82.03%\n",
      "Validation Batch 3, Loss: 1.042063, Accuracy: 76.56%\n",
      "Validation Batch 4, Loss: 0.944058, Accuracy: 77.34%\n",
      "Validation Batch 5, Loss: 0.923222, Accuracy: 78.44%\n",
      "Validation Batch 6, Loss: 0.909225, Accuracy: 79.43%\n",
      "Validation Batch 7, Loss: 0.938123, Accuracy: 79.46%\n",
      "Validation Batch 8, Loss: 1.017458, Accuracy: 78.52%\n",
      "Validation Batch 9, Loss: 1.005550, Accuracy: 78.30%\n",
      "Validation Batch 10, Loss: 0.966856, Accuracy: 78.28%\n",
      "Validation Batch 11, Loss: 0.954305, Accuracy: 78.27%\n",
      "Validation Batch 12, Loss: 0.924667, Accuracy: 78.78%\n",
      "Validation Batch 13, Loss: 0.929987, Accuracy: 78.85%\n",
      "Validation Batch 14, Loss: 0.967934, Accuracy: 78.79%\n",
      "Validation Batch 15, Loss: 0.944609, Accuracy: 78.96%\n",
      "Validation Batch 16, Loss: 0.951272, Accuracy: 79.00%\n",
      "Validation Batch 17, Loss: 1.018373, Accuracy: 78.77%\n",
      "Validation Batch 18, Loss: 0.887112, Accuracy: 79.25%\n",
      "Validation Batch 19, Loss: 0.971726, Accuracy: 79.19%\n",
      "Validation Batch 20, Loss: 1.011384, Accuracy: 78.91%\n",
      "Validation Batch 21, Loss: 1.009314, Accuracy: 78.57%\n",
      "Validation Batch 22, Loss: 0.950662, Accuracy: 78.69%\n",
      "Validation Batch 23, Loss: 0.987701, Accuracy: 78.67%\n",
      "Validation Batch 24, Loss: 0.958318, Accuracy: 78.78%\n",
      "Validation Batch 25, Loss: 0.931902, Accuracy: 78.94%\n",
      "Validation Batch 26, Loss: 0.947718, Accuracy: 78.97%\n",
      "Validation Batch 27, Loss: 0.971192, Accuracy: 78.86%\n",
      "Validation - Epoch 46, Loss: 0.960261, Accuracy: 78.86%\n",
      "Patience—7\n",
      "Epoch 47\n",
      "Batch 1, Loss: 1.048588, Accuracy: 68.75%\n",
      "Batch 2, Loss: 0.917065, Accuracy: 77.34%\n",
      "Batch 3, Loss: 0.901112, Accuracy: 79.69%\n",
      "Batch 4, Loss: 0.934779, Accuracy: 80.08%\n",
      "Batch 5, Loss: 0.898541, Accuracy: 80.94%\n",
      "Batch 6, Loss: 0.912559, Accuracy: 81.51%\n",
      "Batch 7, Loss: 0.896837, Accuracy: 82.14%\n",
      "Batch 8, Loss: 0.979274, Accuracy: 81.64%\n",
      "Batch 9, Loss: 0.967277, Accuracy: 81.08%\n",
      "Batch 10, Loss: 0.966626, Accuracy: 80.94%\n",
      "Batch 11, Loss: 0.995650, Accuracy: 80.26%\n",
      "Batch 12, Loss: 0.922078, Accuracy: 80.47%\n",
      "Batch 13, Loss: 0.918503, Accuracy: 80.77%\n",
      "Batch 14, Loss: 0.979514, Accuracy: 80.58%\n",
      "Batch 15, Loss: 0.922106, Accuracy: 80.73%\n",
      "Batch 16, Loss: 0.840066, Accuracy: 81.45%\n",
      "Batch 17, Loss: 0.937722, Accuracy: 81.43%\n",
      "Batch 18, Loss: 1.040279, Accuracy: 80.82%\n",
      "Batch 19, Loss: 0.906601, Accuracy: 81.09%\n",
      "Batch 20, Loss: 0.915736, Accuracy: 81.25%\n",
      "Batch 21, Loss: 1.082959, Accuracy: 80.51%\n",
      "Batch 22, Loss: 0.838448, Accuracy: 81.04%\n",
      "Batch 23, Loss: 0.966462, Accuracy: 80.77%\n",
      "Batch 24, Loss: 0.920596, Accuracy: 80.79%\n",
      "Batch 25, Loss: 0.939886, Accuracy: 80.69%\n",
      "Batch 26, Loss: 0.898767, Accuracy: 80.83%\n",
      "Batch 27, Loss: 0.927391, Accuracy: 80.90%\n",
      "Batch 28, Loss: 0.955810, Accuracy: 80.86%\n",
      "Batch 29, Loss: 0.907372, Accuracy: 81.03%\n",
      "Batch 30, Loss: 0.875793, Accuracy: 81.25%\n",
      "Batch 31, Loss: 0.934399, Accuracy: 81.25%\n",
      "Batch 32, Loss: 0.962094, Accuracy: 81.10%\n",
      "Batch 33, Loss: 0.947477, Accuracy: 81.01%\n",
      "Batch 34, Loss: 0.871176, Accuracy: 81.25%\n",
      "Batch 35, Loss: 0.945579, Accuracy: 81.21%\n",
      "Batch 36, Loss: 0.906920, Accuracy: 81.25%\n",
      "Batch 37, Loss: 0.966997, Accuracy: 81.25%\n",
      "Batch 38, Loss: 0.896008, Accuracy: 81.33%\n",
      "Batch 39, Loss: 0.945159, Accuracy: 81.37%\n",
      "Batch 40, Loss: 0.997681, Accuracy: 81.17%\n",
      "Batch 41, Loss: 0.995702, Accuracy: 81.06%\n",
      "Batch 42, Loss: 0.921407, Accuracy: 81.10%\n",
      "Batch 43, Loss: 0.845106, Accuracy: 81.36%\n",
      "Batch 44, Loss: 0.945152, Accuracy: 81.32%\n",
      "Batch 45, Loss: 0.901004, Accuracy: 81.39%\n",
      "Batch 46, Loss: 0.858927, Accuracy: 81.59%\n",
      "Batch 47, Loss: 1.016924, Accuracy: 81.42%\n",
      "Batch 48, Loss: 0.957048, Accuracy: 81.32%\n",
      "Batch 49, Loss: 0.894070, Accuracy: 81.41%\n",
      "Batch 50, Loss: 0.870888, Accuracy: 81.53%\n",
      "Batch 51, Loss: 0.933124, Accuracy: 81.53%\n",
      "Batch 52, Loss: 0.930554, Accuracy: 81.55%\n",
      "Batch 53, Loss: 0.970753, Accuracy: 81.49%\n",
      "Batch 54, Loss: 0.910456, Accuracy: 81.48%\n",
      "Batch 55, Loss: 0.919117, Accuracy: 81.51%\n",
      "Batch 56, Loss: 0.902743, Accuracy: 81.53%\n",
      "Batch 57, Loss: 0.892090, Accuracy: 81.61%\n",
      "Batch 58, Loss: 0.938816, Accuracy: 81.60%\n",
      "Batch 59, Loss: 0.922985, Accuracy: 81.59%\n",
      "Batch 60, Loss: 0.891131, Accuracy: 81.64%\n",
      "Batch 61, Loss: 0.982154, Accuracy: 81.58%\n",
      "Batch 62, Loss: 0.880477, Accuracy: 81.68%\n",
      "Batch 63, Loss: 0.968863, Accuracy: 81.57%\n",
      "Batch 64, Loss: 0.913773, Accuracy: 81.62%\n",
      "Batch 65, Loss: 0.860848, Accuracy: 81.73%\n",
      "Batch 66, Loss: 0.896198, Accuracy: 81.77%\n",
      "Batch 67, Loss: 1.022016, Accuracy: 81.58%\n",
      "Batch 68, Loss: 0.974360, Accuracy: 81.50%\n",
      "Batch 69, Loss: 0.959070, Accuracy: 81.48%\n",
      "Batch 70, Loss: 0.935324, Accuracy: 81.47%\n",
      "Batch 71, Loss: 0.905350, Accuracy: 81.51%\n",
      "Batch 72, Loss: 0.885646, Accuracy: 81.60%\n",
      "Batch 73, Loss: 0.906209, Accuracy: 81.68%\n",
      "Batch 74, Loss: 0.984001, Accuracy: 81.63%\n",
      "Batch 75, Loss: 0.913830, Accuracy: 81.67%\n",
      "Batch 76, Loss: 0.856804, Accuracy: 81.76%\n",
      "Batch 77, Loss: 0.941001, Accuracy: 81.72%\n",
      "Batch 78, Loss: 0.976179, Accuracy: 81.63%\n",
      "Batch 79, Loss: 0.899536, Accuracy: 81.69%\n",
      "Batch 80, Loss: 0.962860, Accuracy: 81.64%\n",
      "Batch 81, Loss: 0.940495, Accuracy: 81.64%\n",
      "Batch 82, Loss: 0.959347, Accuracy: 81.57%\n",
      "Batch 83, Loss: 0.889577, Accuracy: 81.63%\n",
      "Batch 84, Loss: 0.864669, Accuracy: 81.72%\n",
      "Batch 85, Loss: 0.991303, Accuracy: 81.64%\n",
      "Batch 86, Loss: 0.882012, Accuracy: 81.70%\n",
      "Batch 87, Loss: 0.967475, Accuracy: 81.65%\n",
      "Batch 88, Loss: 0.960645, Accuracy: 81.61%\n",
      "Batch 89, Loss: 0.925061, Accuracy: 81.62%\n",
      "Batch 90, Loss: 0.891410, Accuracy: 81.67%\n",
      "Batch 91, Loss: 0.987002, Accuracy: 81.59%\n",
      "Batch 92, Loss: 0.968468, Accuracy: 81.56%\n",
      "Batch 93, Loss: 0.937730, Accuracy: 81.55%\n",
      "Batch 94, Loss: 0.949734, Accuracy: 81.52%\n",
      "Batch 95, Loss: 0.879635, Accuracy: 81.56%\n",
      "Batch 96, Loss: 0.950348, Accuracy: 81.54%\n",
      "Batch 97, Loss: 1.005501, Accuracy: 81.46%\n",
      "Batch 98, Loss: 0.977914, Accuracy: 81.41%\n",
      "Batch 99, Loss: 0.897286, Accuracy: 81.44%\n",
      "Batch 100, Loss: 0.945123, Accuracy: 81.42%\n",
      "Batch 101, Loss: 0.995139, Accuracy: 81.36%\n",
      "Batch 102, Loss: 0.862080, Accuracy: 81.45%\n",
      "Batch 103, Loss: 0.904534, Accuracy: 81.48%\n",
      "Batch 104, Loss: 0.938939, Accuracy: 81.49%\n",
      "Batch 105, Loss: 0.886438, Accuracy: 81.53%\n",
      "Batch 106, Loss: 0.967469, Accuracy: 81.53%\n",
      "Batch 107, Loss: 0.905533, Accuracy: 81.57%\n",
      "Batch 108, Loss: 0.881163, Accuracy: 81.63%\n",
      "Batch 109, Loss: 0.955896, Accuracy: 81.58%\n",
      "Batch 110, Loss: 0.952649, Accuracy: 81.55%\n",
      "Batch 111, Loss: 0.890810, Accuracy: 81.59%\n",
      "Batch 112, Loss: 0.962781, Accuracy: 81.56%\n",
      "Batch 113, Loss: 0.920382, Accuracy: 81.55%\n",
      "Batch 114, Loss: 0.963499, Accuracy: 81.54%\n",
      "Batch 115, Loss: 0.925882, Accuracy: 81.54%\n",
      "Batch 116, Loss: 0.936103, Accuracy: 81.53%\n",
      "Batch 117, Loss: 0.913050, Accuracy: 81.54%\n",
      "Batch 118, Loss: 0.940188, Accuracy: 81.53%\n",
      "Batch 119, Loss: 0.864338, Accuracy: 81.58%\n",
      "Batch 120, Loss: 0.903344, Accuracy: 81.59%\n",
      "Batch 121, Loss: 0.939988, Accuracy: 81.57%\n",
      "Batch 122, Loss: 0.852705, Accuracy: 81.65%\n",
      "Batch 123, Loss: 0.916155, Accuracy: 81.67%\n",
      "Batch 124, Loss: 1.000636, Accuracy: 81.62%\n",
      "Batch 125, Loss: 0.895971, Accuracy: 81.65%\n",
      "Batch 126, Loss: 0.916640, Accuracy: 81.67%\n",
      "Batch 127, Loss: 0.838134, Accuracy: 81.75%\n",
      "Batch 128, Loss: 0.848258, Accuracy: 81.82%\n",
      "Batch 129, Loss: 0.951991, Accuracy: 81.82%\n",
      "Batch 130, Loss: 0.815106, Accuracy: 81.90%\n",
      "Batch 131, Loss: 0.861172, Accuracy: 81.95%\n",
      "Batch 132, Loss: 0.958236, Accuracy: 81.94%\n",
      "Batch 133, Loss: 0.956534, Accuracy: 81.91%\n",
      "Batch 134, Loss: 0.977652, Accuracy: 81.87%\n",
      "Batch 135, Loss: 0.929973, Accuracy: 81.88%\n",
      "Batch 136, Loss: 0.951679, Accuracy: 81.86%\n",
      "Batch 137, Loss: 0.969771, Accuracy: 81.82%\n",
      "Batch 138, Loss: 0.835250, Accuracy: 81.88%\n",
      "Batch 139, Loss: 0.922445, Accuracy: 81.89%\n",
      "Batch 140, Loss: 0.922206, Accuracy: 81.90%\n",
      "Batch 141, Loss: 0.946842, Accuracy: 81.89%\n",
      "Batch 142, Loss: 0.944785, Accuracy: 81.89%\n",
      "Batch 143, Loss: 0.938904, Accuracy: 81.88%\n",
      "Batch 144, Loss: 0.903119, Accuracy: 81.91%\n",
      "Batch 145, Loss: 0.898413, Accuracy: 81.95%\n",
      "Batch 146, Loss: 0.985681, Accuracy: 81.91%\n",
      "Batch 147, Loss: 0.924439, Accuracy: 81.92%\n",
      "Batch 148, Loss: 0.919969, Accuracy: 81.94%\n",
      "Batch 149, Loss: 0.922467, Accuracy: 81.94%\n",
      "Batch 150, Loss: 0.893219, Accuracy: 81.97%\n",
      "Batch 151, Loss: 0.939800, Accuracy: 81.96%\n",
      "Batch 152, Loss: 0.905158, Accuracy: 81.97%\n",
      "Batch 153, Loss: 0.933151, Accuracy: 81.96%\n",
      "Batch 154, Loss: 0.928977, Accuracy: 81.96%\n",
      "Batch 155, Loss: 0.924134, Accuracy: 81.98%\n",
      "Batch 156, Loss: 0.894244, Accuracy: 81.99%\n",
      "Batch 157, Loss: 0.957666, Accuracy: 81.96%\n",
      "Batch 158, Loss: 0.873604, Accuracy: 81.99%\n",
      "Batch 159, Loss: 0.919036, Accuracy: 82.02%\n",
      "Batch 160, Loss: 0.919276, Accuracy: 82.02%\n",
      "Batch 161, Loss: 0.968650, Accuracy: 81.98%\n",
      "Batch 162, Loss: 0.975465, Accuracy: 81.96%\n",
      "Batch 163, Loss: 0.870104, Accuracy: 82.00%\n",
      "Batch 164, Loss: 0.927523, Accuracy: 82.00%\n",
      "Batch 165, Loss: 0.912067, Accuracy: 82.00%\n",
      "Batch 166, Loss: 0.928365, Accuracy: 81.98%\n",
      "Batch 167, Loss: 0.907339, Accuracy: 82.00%\n",
      "Batch 168, Loss: 0.883364, Accuracy: 82.02%\n",
      "Batch 169, Loss: 0.843077, Accuracy: 82.08%\n",
      "Batch 170, Loss: 0.893352, Accuracy: 82.11%\n",
      "Batch 171, Loss: 0.885338, Accuracy: 82.15%\n",
      "Batch 172, Loss: 0.924833, Accuracy: 82.14%\n",
      "Batch 173, Loss: 0.945081, Accuracy: 82.14%\n",
      "Batch 174, Loss: 0.922501, Accuracy: 82.15%\n",
      "Batch 175, Loss: 0.970862, Accuracy: 82.12%\n",
      "Batch 176, Loss: 0.990315, Accuracy: 82.07%\n",
      "Batch 177, Loss: 0.852713, Accuracy: 82.12%\n",
      "Batch 178, Loss: 0.909941, Accuracy: 82.14%\n",
      "Batch 179, Loss: 0.939207, Accuracy: 82.12%\n",
      "Batch 180, Loss: 0.991646, Accuracy: 82.07%\n",
      "Batch 181, Loss: 0.929442, Accuracy: 82.07%\n",
      "Batch 182, Loss: 0.857852, Accuracy: 82.11%\n",
      "Batch 183, Loss: 0.857235, Accuracy: 82.15%\n",
      "Batch 184, Loss: 0.953200, Accuracy: 82.13%\n",
      "Batch 185, Loss: 0.939833, Accuracy: 82.13%\n",
      "Batch 186, Loss: 0.936500, Accuracy: 82.12%\n",
      "Batch 187, Loss: 0.958228, Accuracy: 82.10%\n",
      "Batch 188, Loss: 0.892018, Accuracy: 82.11%\n",
      "Batch 189, Loss: 0.974650, Accuracy: 82.08%\n",
      "Batch 190, Loss: 0.915899, Accuracy: 82.09%\n",
      "Batch 191, Loss: 0.892974, Accuracy: 82.10%\n",
      "Batch 192, Loss: 0.941328, Accuracy: 82.10%\n",
      "Batch 193, Loss: 0.902156, Accuracy: 82.11%\n",
      "Batch 194, Loss: 0.908581, Accuracy: 82.11%\n",
      "Batch 195, Loss: 0.956565, Accuracy: 82.09%\n",
      "Batch 196, Loss: 0.940971, Accuracy: 82.09%\n",
      "Batch 197, Loss: 0.933161, Accuracy: 82.09%\n",
      "Batch 198, Loss: 0.929287, Accuracy: 82.09%\n",
      "Batch 199, Loss: 0.884961, Accuracy: 82.11%\n",
      "Batch 200, Loss: 0.916110, Accuracy: 82.12%\n",
      "Batch 201, Loss: 0.961377, Accuracy: 82.10%\n",
      "Batch 202, Loss: 0.941232, Accuracy: 82.09%\n",
      "Batch 203, Loss: 0.917660, Accuracy: 82.08%\n",
      "Batch 204, Loss: 0.941755, Accuracy: 82.07%\n",
      "Batch 205, Loss: 0.949569, Accuracy: 82.06%\n",
      "Batch 206, Loss: 0.872035, Accuracy: 82.09%\n",
      "Batch 207, Loss: 0.900299, Accuracy: 82.11%\n",
      "Batch 208, Loss: 0.943205, Accuracy: 82.10%\n",
      "Batch 209, Loss: 0.918219, Accuracy: 82.10%\n",
      "Batch 210, Loss: 0.952355, Accuracy: 82.08%\n",
      "Batch 211, Loss: 0.978596, Accuracy: 82.05%\n",
      "Batch 212, Loss: 0.927397, Accuracy: 82.05%\n",
      "Batch 213, Loss: 1.027838, Accuracy: 82.00%\n",
      "Training - Epoch 47, Loss: 0.927695, Accuracy: 82.00%\n",
      "Validation Batch 1, Loss: 0.905680, Accuracy: 84.38%\n",
      "Validation Batch 2, Loss: 0.919573, Accuracy: 82.81%\n",
      "Validation Batch 3, Loss: 1.006569, Accuracy: 80.21%\n",
      "Validation Batch 4, Loss: 0.923880, Accuracy: 80.08%\n",
      "Validation Batch 5, Loss: 0.892910, Accuracy: 81.56%\n",
      "Validation Batch 6, Loss: 0.881498, Accuracy: 82.29%\n",
      "Validation Batch 7, Loss: 0.918000, Accuracy: 82.81%\n",
      "Validation Batch 8, Loss: 1.004561, Accuracy: 81.45%\n",
      "Validation Batch 9, Loss: 0.988392, Accuracy: 80.90%\n",
      "Validation Batch 10, Loss: 0.949218, Accuracy: 80.62%\n",
      "Validation Batch 11, Loss: 0.929883, Accuracy: 80.82%\n",
      "Validation Batch 12, Loss: 0.911964, Accuracy: 81.12%\n",
      "Validation Batch 13, Loss: 0.920897, Accuracy: 81.25%\n",
      "Validation Batch 14, Loss: 0.952822, Accuracy: 81.03%\n",
      "Validation Batch 15, Loss: 0.924308, Accuracy: 81.04%\n",
      "Validation Batch 16, Loss: 0.929974, Accuracy: 81.05%\n",
      "Validation Batch 17, Loss: 0.985413, Accuracy: 80.79%\n",
      "Validation Batch 18, Loss: 0.868674, Accuracy: 81.16%\n",
      "Validation Batch 19, Loss: 0.950856, Accuracy: 81.09%\n",
      "Validation Batch 20, Loss: 0.986280, Accuracy: 80.78%\n",
      "Validation Batch 21, Loss: 0.973113, Accuracy: 80.58%\n",
      "Validation Batch 22, Loss: 0.927703, Accuracy: 80.61%\n",
      "Validation Batch 23, Loss: 0.960133, Accuracy: 80.64%\n",
      "Validation Batch 24, Loss: 0.939356, Accuracy: 80.66%\n",
      "Validation Batch 25, Loss: 0.900634, Accuracy: 80.88%\n",
      "Validation Batch 26, Loss: 0.922607, Accuracy: 81.01%\n",
      "Validation Batch 27, Loss: 0.928726, Accuracy: 81.03%\n",
      "Validation - Epoch 47, Loss: 0.937171, Accuracy: 81.03%\n",
      "Patience—0\n",
      "Epoch 48\n",
      "Batch 1, Loss: 0.925580, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.956856, Accuracy: 81.25%\n",
      "Batch 3, Loss: 0.875448, Accuracy: 83.33%\n",
      "Batch 4, Loss: 0.930720, Accuracy: 83.20%\n",
      "Batch 5, Loss: 0.914353, Accuracy: 83.12%\n",
      "Batch 6, Loss: 0.927961, Accuracy: 82.55%\n",
      "Batch 7, Loss: 0.964136, Accuracy: 81.92%\n",
      "Batch 8, Loss: 0.882229, Accuracy: 82.42%\n",
      "Batch 9, Loss: 0.970264, Accuracy: 81.77%\n",
      "Batch 10, Loss: 0.970507, Accuracy: 81.41%\n",
      "Batch 11, Loss: 0.920865, Accuracy: 81.68%\n",
      "Batch 12, Loss: 0.992068, Accuracy: 81.12%\n",
      "Batch 13, Loss: 0.974995, Accuracy: 80.77%\n",
      "Batch 14, Loss: 0.982650, Accuracy: 80.25%\n",
      "Batch 15, Loss: 0.885977, Accuracy: 80.73%\n",
      "Batch 16, Loss: 1.005949, Accuracy: 80.27%\n",
      "Batch 17, Loss: 0.984229, Accuracy: 80.06%\n",
      "Batch 18, Loss: 0.941138, Accuracy: 80.12%\n",
      "Batch 19, Loss: 0.932039, Accuracy: 80.26%\n",
      "Batch 20, Loss: 0.904943, Accuracy: 80.39%\n",
      "Batch 21, Loss: 0.873888, Accuracy: 80.65%\n",
      "Batch 22, Loss: 0.947802, Accuracy: 80.54%\n",
      "Batch 23, Loss: 0.950193, Accuracy: 80.50%\n",
      "Batch 24, Loss: 0.943581, Accuracy: 80.47%\n",
      "Batch 25, Loss: 0.943183, Accuracy: 80.56%\n",
      "Batch 26, Loss: 0.893882, Accuracy: 80.77%\n",
      "Batch 27, Loss: 0.893187, Accuracy: 80.96%\n",
      "Batch 28, Loss: 0.859432, Accuracy: 81.31%\n",
      "Batch 29, Loss: 0.882295, Accuracy: 81.47%\n",
      "Batch 30, Loss: 0.861494, Accuracy: 81.61%\n",
      "Batch 31, Loss: 0.881906, Accuracy: 81.70%\n",
      "Batch 32, Loss: 0.930192, Accuracy: 81.69%\n",
      "Batch 33, Loss: 0.910683, Accuracy: 81.82%\n",
      "Batch 34, Loss: 0.879287, Accuracy: 81.99%\n",
      "Batch 35, Loss: 0.906592, Accuracy: 82.05%\n",
      "Batch 36, Loss: 0.934941, Accuracy: 82.03%\n",
      "Batch 37, Loss: 0.895945, Accuracy: 82.09%\n",
      "Batch 38, Loss: 0.975127, Accuracy: 81.95%\n",
      "Batch 39, Loss: 0.966021, Accuracy: 81.81%\n",
      "Batch 40, Loss: 0.929780, Accuracy: 81.88%\n",
      "Batch 41, Loss: 0.975589, Accuracy: 81.75%\n",
      "Batch 42, Loss: 0.842944, Accuracy: 81.92%\n",
      "Batch 43, Loss: 0.892234, Accuracy: 82.01%\n",
      "Batch 44, Loss: 0.925945, Accuracy: 82.03%\n",
      "Batch 45, Loss: 0.948881, Accuracy: 81.98%\n",
      "Batch 46, Loss: 0.893647, Accuracy: 82.07%\n",
      "Batch 47, Loss: 0.888138, Accuracy: 82.08%\n",
      "Batch 48, Loss: 0.964431, Accuracy: 81.93%\n",
      "Batch 49, Loss: 0.901812, Accuracy: 81.98%\n",
      "Batch 50, Loss: 0.899633, Accuracy: 82.00%\n",
      "Batch 51, Loss: 0.978045, Accuracy: 81.89%\n",
      "Batch 52, Loss: 0.964420, Accuracy: 81.79%\n",
      "Batch 53, Loss: 0.873821, Accuracy: 81.93%\n",
      "Batch 54, Loss: 0.866731, Accuracy: 82.03%\n",
      "Batch 55, Loss: 0.904651, Accuracy: 82.10%\n",
      "Batch 56, Loss: 0.967996, Accuracy: 82.03%\n",
      "Batch 57, Loss: 0.923799, Accuracy: 82.04%\n",
      "Batch 58, Loss: 0.921134, Accuracy: 82.00%\n",
      "Batch 59, Loss: 0.914730, Accuracy: 81.97%\n",
      "Batch 60, Loss: 0.942764, Accuracy: 81.95%\n",
      "Batch 61, Loss: 0.926993, Accuracy: 81.97%\n",
      "Batch 62, Loss: 0.870717, Accuracy: 82.08%\n",
      "Batch 63, Loss: 0.943418, Accuracy: 82.04%\n",
      "Batch 64, Loss: 0.905722, Accuracy: 82.10%\n",
      "Batch 65, Loss: 0.945814, Accuracy: 82.07%\n",
      "Batch 66, Loss: 0.872761, Accuracy: 82.17%\n",
      "Batch 67, Loss: 0.834650, Accuracy: 82.32%\n",
      "Batch 68, Loss: 0.916934, Accuracy: 82.31%\n",
      "Batch 69, Loss: 0.904015, Accuracy: 82.34%\n",
      "Batch 70, Loss: 0.932310, Accuracy: 82.34%\n",
      "Batch 71, Loss: 0.907832, Accuracy: 82.35%\n",
      "Batch 72, Loss: 1.022263, Accuracy: 82.18%\n",
      "Batch 73, Loss: 0.909506, Accuracy: 82.21%\n",
      "Batch 74, Loss: 1.015523, Accuracy: 82.07%\n",
      "Batch 75, Loss: 0.930933, Accuracy: 82.06%\n",
      "Batch 76, Loss: 0.845961, Accuracy: 82.20%\n",
      "Batch 77, Loss: 0.975063, Accuracy: 82.12%\n",
      "Batch 78, Loss: 0.947338, Accuracy: 82.11%\n",
      "Batch 79, Loss: 0.940991, Accuracy: 82.08%\n",
      "Batch 80, Loss: 0.927928, Accuracy: 82.09%\n",
      "Batch 81, Loss: 0.939501, Accuracy: 82.08%\n",
      "Batch 82, Loss: 0.901020, Accuracy: 82.11%\n",
      "Batch 83, Loss: 0.937975, Accuracy: 82.10%\n",
      "Batch 84, Loss: 1.000993, Accuracy: 82.03%\n",
      "Batch 85, Loss: 0.885265, Accuracy: 82.08%\n",
      "Batch 86, Loss: 0.908639, Accuracy: 82.09%\n",
      "Batch 87, Loss: 0.970702, Accuracy: 82.02%\n",
      "Batch 88, Loss: 0.894559, Accuracy: 82.07%\n",
      "Batch 89, Loss: 0.939361, Accuracy: 82.08%\n",
      "Batch 90, Loss: 0.900524, Accuracy: 82.10%\n",
      "Batch 91, Loss: 0.884738, Accuracy: 82.16%\n",
      "Batch 92, Loss: 0.923188, Accuracy: 82.13%\n",
      "Batch 93, Loss: 0.913570, Accuracy: 82.16%\n",
      "Batch 94, Loss: 0.899266, Accuracy: 82.18%\n",
      "Batch 95, Loss: 0.961427, Accuracy: 82.14%\n",
      "Batch 96, Loss: 0.988740, Accuracy: 82.06%\n",
      "Batch 97, Loss: 0.918266, Accuracy: 82.07%\n",
      "Batch 98, Loss: 0.957175, Accuracy: 82.03%\n",
      "Batch 99, Loss: 0.950172, Accuracy: 82.01%\n",
      "Batch 100, Loss: 0.877520, Accuracy: 82.06%\n",
      "Batch 101, Loss: 0.986298, Accuracy: 82.01%\n",
      "Batch 102, Loss: 0.890048, Accuracy: 82.05%\n",
      "Batch 103, Loss: 0.963250, Accuracy: 82.01%\n",
      "Batch 104, Loss: 0.884607, Accuracy: 82.05%\n",
      "Batch 105, Loss: 0.937875, Accuracy: 82.04%\n",
      "Batch 106, Loss: 0.910565, Accuracy: 82.05%\n",
      "Batch 107, Loss: 0.920120, Accuracy: 82.07%\n",
      "Batch 108, Loss: 0.939122, Accuracy: 82.05%\n",
      "Batch 109, Loss: 0.955658, Accuracy: 82.01%\n",
      "Batch 110, Loss: 0.942580, Accuracy: 82.00%\n",
      "Batch 111, Loss: 0.944797, Accuracy: 82.01%\n",
      "Batch 112, Loss: 0.898169, Accuracy: 82.03%\n",
      "Batch 113, Loss: 0.929731, Accuracy: 82.04%\n",
      "Batch 114, Loss: 0.925152, Accuracy: 82.04%\n",
      "Batch 115, Loss: 0.970133, Accuracy: 82.00%\n",
      "Batch 116, Loss: 0.897909, Accuracy: 82.00%\n",
      "Batch 117, Loss: 0.882220, Accuracy: 82.05%\n",
      "Batch 118, Loss: 0.936720, Accuracy: 82.04%\n",
      "Batch 119, Loss: 0.859011, Accuracy: 82.12%\n",
      "Batch 120, Loss: 0.893969, Accuracy: 82.14%\n",
      "Batch 121, Loss: 0.976595, Accuracy: 82.09%\n",
      "Batch 122, Loss: 0.882237, Accuracy: 82.12%\n",
      "Batch 123, Loss: 0.820657, Accuracy: 82.22%\n",
      "Batch 124, Loss: 0.929179, Accuracy: 82.22%\n",
      "Batch 125, Loss: 0.933847, Accuracy: 82.20%\n",
      "Batch 126, Loss: 0.888958, Accuracy: 82.25%\n",
      "Batch 127, Loss: 0.993692, Accuracy: 82.20%\n",
      "Batch 128, Loss: 0.935664, Accuracy: 82.18%\n",
      "Batch 129, Loss: 0.963703, Accuracy: 82.15%\n",
      "Batch 130, Loss: 0.910971, Accuracy: 82.15%\n",
      "Batch 131, Loss: 0.951446, Accuracy: 82.13%\n",
      "Batch 132, Loss: 0.892755, Accuracy: 82.16%\n",
      "Batch 133, Loss: 0.940027, Accuracy: 82.15%\n",
      "Batch 134, Loss: 0.993538, Accuracy: 82.09%\n",
      "Batch 135, Loss: 1.039077, Accuracy: 81.98%\n",
      "Batch 136, Loss: 0.935094, Accuracy: 81.99%\n",
      "Batch 137, Loss: 0.935879, Accuracy: 81.98%\n",
      "Batch 138, Loss: 0.932791, Accuracy: 81.99%\n",
      "Batch 139, Loss: 0.863458, Accuracy: 82.03%\n",
      "Batch 140, Loss: 0.863838, Accuracy: 82.09%\n",
      "Batch 141, Loss: 0.916416, Accuracy: 82.09%\n",
      "Batch 142, Loss: 0.924066, Accuracy: 82.09%\n",
      "Batch 143, Loss: 0.936037, Accuracy: 82.08%\n",
      "Batch 144, Loss: 0.940003, Accuracy: 82.07%\n",
      "Batch 145, Loss: 0.929902, Accuracy: 82.06%\n",
      "Batch 146, Loss: 0.886864, Accuracy: 82.08%\n",
      "Batch 147, Loss: 0.859349, Accuracy: 82.13%\n",
      "Batch 148, Loss: 0.895076, Accuracy: 82.15%\n",
      "Batch 149, Loss: 0.901955, Accuracy: 82.16%\n",
      "Batch 150, Loss: 0.827972, Accuracy: 82.23%\n",
      "Batch 151, Loss: 1.003192, Accuracy: 82.17%\n",
      "Batch 152, Loss: 0.961329, Accuracy: 82.13%\n",
      "Batch 153, Loss: 0.863085, Accuracy: 82.17%\n",
      "Batch 154, Loss: 0.919235, Accuracy: 82.18%\n",
      "Batch 155, Loss: 0.917623, Accuracy: 82.20%\n",
      "Batch 156, Loss: 0.900712, Accuracy: 82.21%\n",
      "Batch 157, Loss: 0.906931, Accuracy: 82.22%\n",
      "Batch 158, Loss: 0.922156, Accuracy: 82.22%\n",
      "Batch 159, Loss: 0.901345, Accuracy: 82.23%\n",
      "Batch 160, Loss: 0.926298, Accuracy: 82.23%\n",
      "Batch 161, Loss: 0.976471, Accuracy: 82.18%\n",
      "Batch 162, Loss: 1.012840, Accuracy: 82.14%\n",
      "Batch 163, Loss: 0.891168, Accuracy: 82.16%\n",
      "Batch 164, Loss: 0.856222, Accuracy: 82.20%\n",
      "Batch 165, Loss: 0.924754, Accuracy: 82.20%\n",
      "Batch 166, Loss: 0.962286, Accuracy: 82.17%\n",
      "Batch 167, Loss: 0.964935, Accuracy: 82.12%\n",
      "Batch 168, Loss: 0.859448, Accuracy: 82.16%\n",
      "Batch 169, Loss: 0.962563, Accuracy: 82.14%\n",
      "Batch 170, Loss: 0.971347, Accuracy: 82.10%\n",
      "Batch 171, Loss: 0.924276, Accuracy: 82.09%\n",
      "Batch 172, Loss: 0.843300, Accuracy: 82.13%\n",
      "Batch 173, Loss: 0.897957, Accuracy: 82.15%\n",
      "Batch 174, Loss: 0.956613, Accuracy: 82.14%\n",
      "Batch 175, Loss: 0.933188, Accuracy: 82.13%\n",
      "Batch 176, Loss: 0.917724, Accuracy: 82.14%\n",
      "Batch 177, Loss: 1.004113, Accuracy: 82.10%\n",
      "Batch 178, Loss: 0.911317, Accuracy: 82.08%\n",
      "Batch 179, Loss: 0.950637, Accuracy: 82.06%\n",
      "Batch 180, Loss: 0.899152, Accuracy: 82.07%\n",
      "Batch 181, Loss: 0.910841, Accuracy: 82.09%\n",
      "Batch 182, Loss: 0.962580, Accuracy: 82.06%\n",
      "Batch 183, Loss: 0.998727, Accuracy: 82.02%\n",
      "Batch 184, Loss: 0.946384, Accuracy: 82.01%\n",
      "Batch 185, Loss: 0.967372, Accuracy: 81.98%\n",
      "Batch 186, Loss: 0.941419, Accuracy: 81.97%\n",
      "Batch 187, Loss: 0.921919, Accuracy: 81.97%\n",
      "Batch 188, Loss: 1.001560, Accuracy: 81.93%\n",
      "Batch 189, Loss: 0.943474, Accuracy: 81.93%\n",
      "Batch 190, Loss: 0.858282, Accuracy: 81.97%\n",
      "Batch 191, Loss: 0.892178, Accuracy: 81.99%\n",
      "Batch 192, Loss: 0.946128, Accuracy: 81.97%\n",
      "Batch 193, Loss: 0.957517, Accuracy: 81.96%\n",
      "Batch 194, Loss: 0.930894, Accuracy: 81.97%\n",
      "Batch 195, Loss: 0.890066, Accuracy: 81.98%\n",
      "Batch 196, Loss: 0.842568, Accuracy: 82.02%\n",
      "Batch 197, Loss: 0.925494, Accuracy: 82.03%\n",
      "Batch 198, Loss: 0.834435, Accuracy: 82.06%\n",
      "Batch 199, Loss: 0.941843, Accuracy: 82.05%\n",
      "Batch 200, Loss: 0.881952, Accuracy: 82.08%\n",
      "Batch 201, Loss: 0.851311, Accuracy: 82.11%\n",
      "Batch 202, Loss: 0.924753, Accuracy: 82.12%\n",
      "Batch 203, Loss: 0.852023, Accuracy: 82.17%\n",
      "Batch 204, Loss: 0.960009, Accuracy: 82.15%\n",
      "Batch 205, Loss: 0.948432, Accuracy: 82.14%\n",
      "Batch 206, Loss: 0.852450, Accuracy: 82.17%\n",
      "Batch 207, Loss: 0.880285, Accuracy: 82.19%\n",
      "Batch 208, Loss: 0.935617, Accuracy: 82.19%\n",
      "Batch 209, Loss: 0.968327, Accuracy: 82.17%\n",
      "Batch 210, Loss: 0.888701, Accuracy: 82.18%\n",
      "Batch 211, Loss: 0.860619, Accuracy: 82.22%\n",
      "Batch 212, Loss: 0.992051, Accuracy: 82.18%\n",
      "Batch 213, Loss: 0.999528, Accuracy: 82.13%\n",
      "Training - Epoch 48, Loss: 0.924150, Accuracy: 82.13%\n",
      "Validation Batch 1, Loss: 0.909711, Accuracy: 84.38%\n",
      "Validation Batch 2, Loss: 0.923806, Accuracy: 82.81%\n",
      "Validation Batch 3, Loss: 1.010377, Accuracy: 79.69%\n",
      "Validation Batch 4, Loss: 0.934246, Accuracy: 79.69%\n",
      "Validation Batch 5, Loss: 0.894205, Accuracy: 80.94%\n",
      "Validation Batch 6, Loss: 0.887939, Accuracy: 81.77%\n",
      "Validation Batch 7, Loss: 0.924748, Accuracy: 82.37%\n",
      "Validation Batch 8, Loss: 1.005413, Accuracy: 81.05%\n",
      "Validation Batch 9, Loss: 0.983941, Accuracy: 80.56%\n",
      "Validation Batch 10, Loss: 0.953650, Accuracy: 80.31%\n",
      "Validation Batch 11, Loss: 0.936003, Accuracy: 80.26%\n",
      "Validation Batch 12, Loss: 0.919968, Accuracy: 80.34%\n",
      "Validation Batch 13, Loss: 0.926939, Accuracy: 80.41%\n",
      "Validation Batch 14, Loss: 0.957161, Accuracy: 80.36%\n",
      "Validation Batch 15, Loss: 0.926341, Accuracy: 80.42%\n",
      "Validation Batch 16, Loss: 0.928453, Accuracy: 80.57%\n",
      "Validation Batch 17, Loss: 0.986178, Accuracy: 80.24%\n",
      "Validation Batch 18, Loss: 0.875422, Accuracy: 80.64%\n",
      "Validation Batch 19, Loss: 0.955805, Accuracy: 80.59%\n",
      "Validation Batch 20, Loss: 1.000878, Accuracy: 80.31%\n",
      "Validation Batch 21, Loss: 0.985168, Accuracy: 79.99%\n",
      "Validation Batch 22, Loss: 0.935837, Accuracy: 80.04%\n",
      "Validation Batch 23, Loss: 0.972227, Accuracy: 80.03%\n",
      "Validation Batch 24, Loss: 0.942869, Accuracy: 80.08%\n",
      "Validation Batch 25, Loss: 0.910437, Accuracy: 80.38%\n",
      "Validation Batch 26, Loss: 0.928534, Accuracy: 80.47%\n",
      "Validation Batch 27, Loss: 0.945114, Accuracy: 80.45%\n",
      "Validation - Epoch 48, Loss: 0.943014, Accuracy: 80.45%\n",
      "Patience—1\n",
      "Epoch 49\n",
      "Batch 1, Loss: 0.911493, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.981865, Accuracy: 79.69%\n",
      "Batch 3, Loss: 0.938424, Accuracy: 80.21%\n",
      "Batch 4, Loss: 0.928456, Accuracy: 80.47%\n",
      "Batch 5, Loss: 0.971576, Accuracy: 80.00%\n",
      "Batch 6, Loss: 0.914218, Accuracy: 80.21%\n",
      "Batch 7, Loss: 0.953115, Accuracy: 80.13%\n",
      "Batch 8, Loss: 0.943446, Accuracy: 80.08%\n",
      "Batch 9, Loss: 0.983463, Accuracy: 79.34%\n",
      "Batch 10, Loss: 0.919794, Accuracy: 79.53%\n",
      "Batch 11, Loss: 0.890273, Accuracy: 80.11%\n",
      "Batch 12, Loss: 0.860078, Accuracy: 80.86%\n",
      "Batch 13, Loss: 0.867577, Accuracy: 81.37%\n",
      "Batch 14, Loss: 0.876880, Accuracy: 81.70%\n",
      "Batch 15, Loss: 0.836079, Accuracy: 82.29%\n",
      "Batch 16, Loss: 0.867099, Accuracy: 82.52%\n",
      "Batch 17, Loss: 0.935957, Accuracy: 82.54%\n",
      "Batch 18, Loss: 0.879911, Accuracy: 82.81%\n",
      "Batch 19, Loss: 0.909991, Accuracy: 82.81%\n",
      "Batch 20, Loss: 0.913789, Accuracy: 82.89%\n",
      "Batch 21, Loss: 0.942581, Accuracy: 82.74%\n",
      "Batch 22, Loss: 0.888556, Accuracy: 82.88%\n",
      "Batch 23, Loss: 1.011064, Accuracy: 82.54%\n",
      "Batch 24, Loss: 0.898407, Accuracy: 82.62%\n",
      "Batch 25, Loss: 0.891687, Accuracy: 82.75%\n",
      "Batch 26, Loss: 0.943477, Accuracy: 82.63%\n",
      "Batch 27, Loss: 0.908593, Accuracy: 82.70%\n",
      "Batch 28, Loss: 0.883211, Accuracy: 82.81%\n",
      "Batch 29, Loss: 0.901430, Accuracy: 82.87%\n",
      "Batch 30, Loss: 0.975775, Accuracy: 82.66%\n",
      "Batch 31, Loss: 0.905729, Accuracy: 82.71%\n",
      "Batch 32, Loss: 0.940315, Accuracy: 82.62%\n",
      "Batch 33, Loss: 0.903033, Accuracy: 82.67%\n",
      "Batch 34, Loss: 0.939480, Accuracy: 82.58%\n",
      "Batch 35, Loss: 0.926905, Accuracy: 82.54%\n",
      "Batch 36, Loss: 0.899879, Accuracy: 82.60%\n",
      "Batch 37, Loss: 0.957424, Accuracy: 82.47%\n",
      "Batch 38, Loss: 0.933721, Accuracy: 82.40%\n",
      "Batch 39, Loss: 0.931147, Accuracy: 82.37%\n",
      "Batch 40, Loss: 0.873436, Accuracy: 82.50%\n",
      "Batch 41, Loss: 0.907263, Accuracy: 82.51%\n",
      "Batch 42, Loss: 0.948820, Accuracy: 82.44%\n",
      "Batch 43, Loss: 0.858773, Accuracy: 82.56%\n",
      "Batch 44, Loss: 0.895284, Accuracy: 82.67%\n",
      "Batch 45, Loss: 0.920208, Accuracy: 82.67%\n",
      "Batch 46, Loss: 0.911271, Accuracy: 82.71%\n",
      "Batch 47, Loss: 0.932045, Accuracy: 82.65%\n",
      "Batch 48, Loss: 0.917944, Accuracy: 82.62%\n",
      "Batch 49, Loss: 0.951675, Accuracy: 82.53%\n",
      "Batch 50, Loss: 0.978026, Accuracy: 82.41%\n",
      "Batch 51, Loss: 0.958363, Accuracy: 82.32%\n",
      "Batch 52, Loss: 0.894134, Accuracy: 82.39%\n",
      "Batch 53, Loss: 0.891097, Accuracy: 82.43%\n",
      "Batch 54, Loss: 0.956921, Accuracy: 82.35%\n",
      "Batch 55, Loss: 0.917851, Accuracy: 82.36%\n",
      "Batch 56, Loss: 0.928114, Accuracy: 82.34%\n",
      "Batch 57, Loss: 0.972140, Accuracy: 82.26%\n",
      "Batch 58, Loss: 0.882425, Accuracy: 82.35%\n",
      "Batch 59, Loss: 0.952933, Accuracy: 82.31%\n",
      "Batch 60, Loss: 0.888573, Accuracy: 82.40%\n",
      "Batch 61, Loss: 0.963372, Accuracy: 82.30%\n",
      "Batch 62, Loss: 0.821745, Accuracy: 82.46%\n",
      "Batch 63, Loss: 0.962732, Accuracy: 82.42%\n",
      "Batch 64, Loss: 0.950024, Accuracy: 82.32%\n",
      "Batch 65, Loss: 0.892370, Accuracy: 82.38%\n",
      "Batch 66, Loss: 0.963927, Accuracy: 82.32%\n",
      "Batch 67, Loss: 0.927101, Accuracy: 82.28%\n",
      "Batch 68, Loss: 0.897449, Accuracy: 82.33%\n",
      "Batch 69, Loss: 0.918471, Accuracy: 82.34%\n",
      "Batch 70, Loss: 1.025326, Accuracy: 82.14%\n",
      "Batch 71, Loss: 0.845732, Accuracy: 82.28%\n",
      "Batch 72, Loss: 0.834467, Accuracy: 82.42%\n",
      "Batch 73, Loss: 0.887745, Accuracy: 82.47%\n",
      "Batch 74, Loss: 0.848381, Accuracy: 82.54%\n",
      "Batch 75, Loss: 0.917214, Accuracy: 82.56%\n",
      "Batch 76, Loss: 0.920090, Accuracy: 82.57%\n",
      "Batch 77, Loss: 0.933636, Accuracy: 82.57%\n",
      "Batch 78, Loss: 1.036467, Accuracy: 82.41%\n",
      "Batch 79, Loss: 0.886481, Accuracy: 82.44%\n",
      "Batch 80, Loss: 0.878218, Accuracy: 82.50%\n",
      "Batch 81, Loss: 0.893785, Accuracy: 82.54%\n",
      "Batch 82, Loss: 0.933778, Accuracy: 82.51%\n",
      "Batch 83, Loss: 0.960761, Accuracy: 82.44%\n",
      "Batch 84, Loss: 0.968319, Accuracy: 82.37%\n",
      "Batch 85, Loss: 0.948699, Accuracy: 82.35%\n",
      "Batch 86, Loss: 0.972108, Accuracy: 82.29%\n",
      "Batch 87, Loss: 0.917648, Accuracy: 82.27%\n",
      "Batch 88, Loss: 0.879597, Accuracy: 82.33%\n",
      "Batch 89, Loss: 0.978563, Accuracy: 82.25%\n",
      "Batch 90, Loss: 0.877436, Accuracy: 82.31%\n",
      "Batch 91, Loss: 0.935484, Accuracy: 82.28%\n",
      "Batch 92, Loss: 0.862120, Accuracy: 82.35%\n",
      "Batch 93, Loss: 0.871738, Accuracy: 82.41%\n",
      "Batch 94, Loss: 1.006435, Accuracy: 82.30%\n",
      "Batch 95, Loss: 0.923107, Accuracy: 82.29%\n",
      "Batch 96, Loss: 0.953042, Accuracy: 82.28%\n",
      "Batch 97, Loss: 0.902990, Accuracy: 82.30%\n",
      "Batch 98, Loss: 0.836543, Accuracy: 82.38%\n",
      "Batch 99, Loss: 0.876724, Accuracy: 82.42%\n",
      "Batch 100, Loss: 0.886427, Accuracy: 82.45%\n",
      "Batch 101, Loss: 0.881983, Accuracy: 82.49%\n",
      "Batch 102, Loss: 0.939474, Accuracy: 82.48%\n",
      "Batch 103, Loss: 0.854290, Accuracy: 82.54%\n",
      "Batch 104, Loss: 0.913022, Accuracy: 82.56%\n",
      "Batch 105, Loss: 0.928513, Accuracy: 82.56%\n",
      "Batch 106, Loss: 0.883929, Accuracy: 82.61%\n",
      "Batch 107, Loss: 0.878244, Accuracy: 82.67%\n",
      "Batch 108, Loss: 0.902399, Accuracy: 82.67%\n",
      "Batch 109, Loss: 0.960614, Accuracy: 82.64%\n",
      "Batch 110, Loss: 0.880707, Accuracy: 82.67%\n",
      "Batch 111, Loss: 0.940134, Accuracy: 82.64%\n",
      "Batch 112, Loss: 0.952061, Accuracy: 82.60%\n",
      "Batch 113, Loss: 0.829537, Accuracy: 82.69%\n",
      "Batch 114, Loss: 0.937530, Accuracy: 82.66%\n",
      "Batch 115, Loss: 0.862858, Accuracy: 82.70%\n",
      "Batch 116, Loss: 0.925397, Accuracy: 82.72%\n",
      "Batch 117, Loss: 0.978752, Accuracy: 82.69%\n",
      "Batch 118, Loss: 0.923650, Accuracy: 82.69%\n",
      "Batch 119, Loss: 0.928244, Accuracy: 82.68%\n",
      "Batch 120, Loss: 0.988308, Accuracy: 82.62%\n",
      "Batch 121, Loss: 0.964058, Accuracy: 82.58%\n",
      "Batch 122, Loss: 0.940688, Accuracy: 82.56%\n",
      "Batch 123, Loss: 0.969236, Accuracy: 82.52%\n",
      "Batch 124, Loss: 0.874512, Accuracy: 82.59%\n",
      "Batch 125, Loss: 0.894512, Accuracy: 82.59%\n",
      "Batch 126, Loss: 0.864854, Accuracy: 82.63%\n",
      "Batch 127, Loss: 0.837805, Accuracy: 82.70%\n",
      "Batch 128, Loss: 0.910136, Accuracy: 82.70%\n",
      "Batch 129, Loss: 0.908047, Accuracy: 82.73%\n",
      "Batch 130, Loss: 0.913002, Accuracy: 82.74%\n",
      "Batch 131, Loss: 0.892006, Accuracy: 82.76%\n",
      "Batch 132, Loss: 0.940471, Accuracy: 82.75%\n",
      "Batch 133, Loss: 0.911395, Accuracy: 82.77%\n",
      "Batch 134, Loss: 0.907089, Accuracy: 82.78%\n",
      "Batch 135, Loss: 0.866111, Accuracy: 82.80%\n",
      "Batch 136, Loss: 0.936923, Accuracy: 82.77%\n",
      "Batch 137, Loss: 0.889503, Accuracy: 82.78%\n",
      "Batch 138, Loss: 0.857813, Accuracy: 82.82%\n",
      "Batch 139, Loss: 0.900780, Accuracy: 82.83%\n",
      "Batch 140, Loss: 0.948767, Accuracy: 82.80%\n",
      "Batch 141, Loss: 0.878958, Accuracy: 82.83%\n",
      "Batch 142, Loss: 1.036181, Accuracy: 82.74%\n",
      "Batch 143, Loss: 0.971717, Accuracy: 82.70%\n",
      "Batch 144, Loss: 0.866584, Accuracy: 82.74%\n",
      "Batch 145, Loss: 0.894771, Accuracy: 82.75%\n",
      "Batch 146, Loss: 0.892224, Accuracy: 82.78%\n",
      "Batch 147, Loss: 0.950051, Accuracy: 82.74%\n",
      "Batch 148, Loss: 0.935379, Accuracy: 82.73%\n",
      "Batch 149, Loss: 1.014960, Accuracy: 82.68%\n",
      "Batch 150, Loss: 0.901751, Accuracy: 82.69%\n",
      "Batch 151, Loss: 1.003336, Accuracy: 82.63%\n",
      "Batch 152, Loss: 0.914133, Accuracy: 82.63%\n",
      "Batch 153, Loss: 0.913733, Accuracy: 82.64%\n",
      "Batch 154, Loss: 0.888766, Accuracy: 82.66%\n",
      "Batch 155, Loss: 0.936154, Accuracy: 82.64%\n",
      "Batch 156, Loss: 0.836306, Accuracy: 82.69%\n",
      "Batch 157, Loss: 0.899560, Accuracy: 82.70%\n",
      "Batch 158, Loss: 0.900826, Accuracy: 82.71%\n",
      "Batch 159, Loss: 0.857754, Accuracy: 82.76%\n",
      "Batch 160, Loss: 0.880506, Accuracy: 82.78%\n",
      "Batch 161, Loss: 0.934540, Accuracy: 82.76%\n",
      "Batch 162, Loss: 0.899638, Accuracy: 82.78%\n",
      "Batch 163, Loss: 0.875957, Accuracy: 82.81%\n",
      "Batch 164, Loss: 0.921602, Accuracy: 82.81%\n",
      "Batch 165, Loss: 0.939621, Accuracy: 82.80%\n",
      "Batch 166, Loss: 0.936448, Accuracy: 82.79%\n",
      "Batch 167, Loss: 0.944665, Accuracy: 82.77%\n",
      "Batch 168, Loss: 0.858575, Accuracy: 82.79%\n",
      "Batch 169, Loss: 0.927261, Accuracy: 82.78%\n",
      "Batch 170, Loss: 0.959724, Accuracy: 82.74%\n",
      "Batch 171, Loss: 0.944578, Accuracy: 82.73%\n",
      "Batch 172, Loss: 0.936850, Accuracy: 82.70%\n",
      "Batch 173, Loss: 0.936993, Accuracy: 82.70%\n",
      "Batch 174, Loss: 0.925216, Accuracy: 82.70%\n",
      "Batch 175, Loss: 0.893792, Accuracy: 82.71%\n",
      "Batch 176, Loss: 0.938340, Accuracy: 82.71%\n",
      "Batch 177, Loss: 0.905808, Accuracy: 82.71%\n",
      "Batch 178, Loss: 0.929978, Accuracy: 82.71%\n",
      "Batch 179, Loss: 0.996563, Accuracy: 82.66%\n",
      "Batch 180, Loss: 0.922183, Accuracy: 82.66%\n",
      "Batch 181, Loss: 0.950973, Accuracy: 82.64%\n",
      "Batch 182, Loss: 0.962168, Accuracy: 82.61%\n",
      "Batch 183, Loss: 0.990007, Accuracy: 82.56%\n",
      "Batch 184, Loss: 1.012042, Accuracy: 82.51%\n",
      "Batch 185, Loss: 0.916530, Accuracy: 82.51%\n",
      "Batch 186, Loss: 0.889830, Accuracy: 82.54%\n",
      "Batch 187, Loss: 0.898577, Accuracy: 82.55%\n",
      "Batch 188, Loss: 0.916994, Accuracy: 82.55%\n",
      "Batch 189, Loss: 0.908552, Accuracy: 82.57%\n",
      "Batch 190, Loss: 0.992508, Accuracy: 82.54%\n",
      "Batch 191, Loss: 0.948990, Accuracy: 82.52%\n",
      "Batch 192, Loss: 0.947577, Accuracy: 82.50%\n",
      "Batch 193, Loss: 0.912775, Accuracy: 82.50%\n",
      "Batch 194, Loss: 0.880587, Accuracy: 82.52%\n",
      "Batch 195, Loss: 0.851335, Accuracy: 82.56%\n",
      "Batch 196, Loss: 0.921622, Accuracy: 82.57%\n",
      "Batch 197, Loss: 0.953281, Accuracy: 82.55%\n",
      "Batch 198, Loss: 1.014269, Accuracy: 82.50%\n",
      "Batch 199, Loss: 0.915667, Accuracy: 82.51%\n",
      "Batch 200, Loss: 0.887456, Accuracy: 82.54%\n",
      "Batch 201, Loss: 0.902048, Accuracy: 82.54%\n",
      "Batch 202, Loss: 0.883773, Accuracy: 82.56%\n",
      "Batch 203, Loss: 0.907737, Accuracy: 82.56%\n",
      "Batch 204, Loss: 0.905570, Accuracy: 82.58%\n",
      "Batch 205, Loss: 0.904730, Accuracy: 82.59%\n",
      "Batch 206, Loss: 0.957061, Accuracy: 82.57%\n",
      "Batch 207, Loss: 0.867594, Accuracy: 82.59%\n",
      "Batch 208, Loss: 0.899890, Accuracy: 82.61%\n",
      "Batch 209, Loss: 0.918599, Accuracy: 82.61%\n",
      "Batch 210, Loss: 0.943334, Accuracy: 82.60%\n",
      "Batch 211, Loss: 0.994608, Accuracy: 82.57%\n",
      "Batch 212, Loss: 0.871608, Accuracy: 82.60%\n",
      "Batch 213, Loss: 0.967590, Accuracy: 82.58%\n",
      "Training - Epoch 49, Loss: 0.919318, Accuracy: 82.58%\n",
      "Validation Batch 1, Loss: 0.901980, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.921147, Accuracy: 85.16%\n",
      "Validation Batch 3, Loss: 1.008799, Accuracy: 81.25%\n",
      "Validation Batch 4, Loss: 0.927930, Accuracy: 80.86%\n",
      "Validation Batch 5, Loss: 0.885468, Accuracy: 82.50%\n",
      "Validation Batch 6, Loss: 0.881461, Accuracy: 83.07%\n",
      "Validation Batch 7, Loss: 0.915298, Accuracy: 83.48%\n",
      "Validation Batch 8, Loss: 1.004398, Accuracy: 82.03%\n",
      "Validation Batch 9, Loss: 0.972059, Accuracy: 81.42%\n",
      "Validation Batch 10, Loss: 0.948261, Accuracy: 81.09%\n",
      "Validation Batch 11, Loss: 0.937350, Accuracy: 80.68%\n",
      "Validation Batch 12, Loss: 0.919606, Accuracy: 80.86%\n",
      "Validation Batch 13, Loss: 0.915173, Accuracy: 81.01%\n",
      "Validation Batch 14, Loss: 0.952870, Accuracy: 80.92%\n",
      "Validation Batch 15, Loss: 0.913385, Accuracy: 81.04%\n",
      "Validation Batch 16, Loss: 0.930340, Accuracy: 81.05%\n",
      "Validation Batch 17, Loss: 0.979139, Accuracy: 80.88%\n",
      "Validation Batch 18, Loss: 0.873629, Accuracy: 81.25%\n",
      "Validation Batch 19, Loss: 0.950768, Accuracy: 81.25%\n",
      "Validation Batch 20, Loss: 0.993105, Accuracy: 80.94%\n",
      "Validation Batch 21, Loss: 0.978725, Accuracy: 80.65%\n",
      "Validation Batch 22, Loss: 0.928398, Accuracy: 80.68%\n",
      "Validation Batch 23, Loss: 0.959341, Accuracy: 80.64%\n",
      "Validation Batch 24, Loss: 0.940481, Accuracy: 80.66%\n",
      "Validation Batch 25, Loss: 0.907572, Accuracy: 80.94%\n",
      "Validation Batch 26, Loss: 0.926548, Accuracy: 81.01%\n",
      "Validation Batch 27, Loss: 0.930751, Accuracy: 80.97%\n",
      "Validation - Epoch 49, Loss: 0.937184, Accuracy: 80.97%\n",
      "Patience—2\n",
      "Epoch 50\n",
      "Batch 1, Loss: 0.921823, Accuracy: 84.38%\n",
      "Batch 2, Loss: 0.915914, Accuracy: 83.59%\n",
      "Batch 3, Loss: 0.904239, Accuracy: 83.33%\n",
      "Batch 4, Loss: 0.985867, Accuracy: 82.03%\n",
      "Batch 5, Loss: 0.921847, Accuracy: 82.19%\n",
      "Batch 6, Loss: 0.934175, Accuracy: 82.29%\n",
      "Batch 7, Loss: 0.930508, Accuracy: 82.37%\n",
      "Batch 8, Loss: 0.910067, Accuracy: 82.42%\n",
      "Batch 9, Loss: 0.843788, Accuracy: 83.33%\n",
      "Batch 10, Loss: 0.958824, Accuracy: 82.81%\n",
      "Batch 11, Loss: 0.840383, Accuracy: 83.52%\n",
      "Batch 12, Loss: 0.996389, Accuracy: 82.68%\n",
      "Batch 13, Loss: 0.950440, Accuracy: 82.33%\n",
      "Batch 14, Loss: 0.955610, Accuracy: 82.25%\n",
      "Batch 15, Loss: 0.923457, Accuracy: 82.29%\n",
      "Batch 16, Loss: 0.832122, Accuracy: 83.01%\n",
      "Batch 17, Loss: 0.912791, Accuracy: 83.09%\n",
      "Batch 18, Loss: 0.899295, Accuracy: 83.16%\n",
      "Batch 19, Loss: 0.946614, Accuracy: 82.98%\n",
      "Batch 20, Loss: 0.917350, Accuracy: 82.97%\n",
      "Batch 21, Loss: 0.865910, Accuracy: 83.18%\n",
      "Batch 22, Loss: 0.848487, Accuracy: 83.59%\n",
      "Batch 23, Loss: 0.939825, Accuracy: 83.36%\n",
      "Batch 24, Loss: 0.932230, Accuracy: 83.20%\n",
      "Batch 25, Loss: 1.048754, Accuracy: 82.69%\n",
      "Batch 26, Loss: 0.899284, Accuracy: 82.81%\n",
      "Batch 27, Loss: 0.913193, Accuracy: 82.87%\n",
      "Batch 28, Loss: 0.847426, Accuracy: 83.15%\n",
      "Batch 29, Loss: 0.866280, Accuracy: 83.35%\n",
      "Batch 30, Loss: 0.913741, Accuracy: 83.33%\n",
      "Batch 31, Loss: 0.930653, Accuracy: 83.27%\n",
      "Batch 32, Loss: 0.871882, Accuracy: 83.40%\n",
      "Batch 33, Loss: 0.827226, Accuracy: 83.66%\n",
      "Batch 34, Loss: 0.917850, Accuracy: 83.59%\n",
      "Batch 35, Loss: 0.914335, Accuracy: 83.57%\n",
      "Batch 36, Loss: 0.936315, Accuracy: 83.46%\n",
      "Batch 37, Loss: 0.850395, Accuracy: 83.61%\n",
      "Batch 38, Loss: 0.895749, Accuracy: 83.68%\n",
      "Batch 39, Loss: 0.931719, Accuracy: 83.57%\n",
      "Batch 40, Loss: 0.891502, Accuracy: 83.67%\n",
      "Batch 41, Loss: 0.972677, Accuracy: 83.54%\n",
      "Batch 42, Loss: 0.969960, Accuracy: 83.41%\n",
      "Batch 43, Loss: 0.963389, Accuracy: 83.25%\n",
      "Batch 44, Loss: 0.952569, Accuracy: 83.13%\n",
      "Batch 45, Loss: 0.922133, Accuracy: 83.19%\n",
      "Batch 46, Loss: 0.955136, Accuracy: 83.08%\n",
      "Batch 47, Loss: 0.876812, Accuracy: 83.14%\n",
      "Batch 48, Loss: 0.949790, Accuracy: 83.07%\n",
      "Batch 49, Loss: 0.939092, Accuracy: 83.00%\n",
      "Batch 50, Loss: 0.875316, Accuracy: 83.06%\n",
      "Batch 51, Loss: 0.917327, Accuracy: 83.06%\n",
      "Batch 52, Loss: 0.902945, Accuracy: 83.05%\n",
      "Batch 53, Loss: 0.896051, Accuracy: 83.05%\n",
      "Batch 54, Loss: 0.954423, Accuracy: 83.02%\n",
      "Batch 55, Loss: 0.925702, Accuracy: 83.01%\n",
      "Batch 56, Loss: 0.877492, Accuracy: 83.09%\n",
      "Batch 57, Loss: 0.873446, Accuracy: 83.20%\n",
      "Batch 58, Loss: 0.832984, Accuracy: 83.35%\n",
      "Batch 59, Loss: 0.901905, Accuracy: 83.37%\n",
      "Batch 60, Loss: 0.905399, Accuracy: 83.36%\n",
      "Batch 61, Loss: 0.910635, Accuracy: 83.32%\n",
      "Batch 62, Loss: 0.930168, Accuracy: 83.29%\n",
      "Batch 63, Loss: 0.956289, Accuracy: 83.21%\n",
      "Batch 64, Loss: 0.898420, Accuracy: 83.23%\n",
      "Batch 65, Loss: 0.884227, Accuracy: 83.27%\n",
      "Batch 66, Loss: 0.947328, Accuracy: 83.21%\n",
      "Batch 67, Loss: 0.963696, Accuracy: 83.12%\n",
      "Batch 68, Loss: 0.927549, Accuracy: 83.11%\n",
      "Batch 69, Loss: 0.925738, Accuracy: 83.13%\n",
      "Batch 70, Loss: 0.914687, Accuracy: 83.08%\n",
      "Batch 71, Loss: 0.932721, Accuracy: 83.08%\n",
      "Batch 72, Loss: 0.880246, Accuracy: 83.12%\n",
      "Batch 73, Loss: 0.917167, Accuracy: 83.11%\n",
      "Batch 74, Loss: 0.890863, Accuracy: 83.15%\n",
      "Batch 75, Loss: 0.974198, Accuracy: 83.04%\n",
      "Batch 76, Loss: 0.915236, Accuracy: 83.04%\n",
      "Batch 77, Loss: 0.964095, Accuracy: 83.00%\n",
      "Batch 78, Loss: 0.975139, Accuracy: 82.91%\n",
      "Batch 79, Loss: 0.888539, Accuracy: 82.95%\n",
      "Batch 80, Loss: 0.964104, Accuracy: 82.89%\n",
      "Batch 81, Loss: 0.973041, Accuracy: 82.81%\n",
      "Batch 82, Loss: 0.927320, Accuracy: 82.79%\n",
      "Batch 83, Loss: 0.899657, Accuracy: 82.79%\n",
      "Batch 84, Loss: 0.951906, Accuracy: 82.74%\n",
      "Batch 85, Loss: 0.903134, Accuracy: 82.74%\n",
      "Batch 86, Loss: 0.914281, Accuracy: 82.74%\n",
      "Batch 87, Loss: 0.920739, Accuracy: 82.74%\n",
      "Batch 88, Loss: 0.943046, Accuracy: 82.71%\n",
      "Batch 89, Loss: 0.874750, Accuracy: 82.74%\n",
      "Batch 90, Loss: 0.933762, Accuracy: 82.73%\n",
      "Batch 91, Loss: 0.936386, Accuracy: 82.69%\n",
      "Batch 92, Loss: 0.881178, Accuracy: 82.74%\n",
      "Batch 93, Loss: 0.844746, Accuracy: 82.83%\n",
      "Batch 94, Loss: 0.968057, Accuracy: 82.75%\n",
      "Batch 95, Loss: 0.936182, Accuracy: 82.73%\n",
      "Batch 96, Loss: 0.923203, Accuracy: 82.70%\n",
      "Batch 97, Loss: 0.936207, Accuracy: 82.67%\n",
      "Batch 98, Loss: 0.888545, Accuracy: 82.68%\n",
      "Batch 99, Loss: 0.852849, Accuracy: 82.77%\n",
      "Batch 100, Loss: 0.874378, Accuracy: 82.81%\n",
      "Batch 101, Loss: 0.965225, Accuracy: 82.77%\n",
      "Batch 102, Loss: 0.919137, Accuracy: 82.77%\n",
      "Batch 103, Loss: 0.912638, Accuracy: 82.78%\n",
      "Batch 104, Loss: 0.970331, Accuracy: 82.74%\n",
      "Batch 105, Loss: 0.884973, Accuracy: 82.75%\n",
      "Batch 106, Loss: 0.819182, Accuracy: 82.84%\n",
      "Batch 107, Loss: 0.885217, Accuracy: 82.89%\n",
      "Batch 108, Loss: 0.934836, Accuracy: 82.87%\n",
      "Batch 109, Loss: 0.970908, Accuracy: 82.81%\n",
      "Batch 110, Loss: 0.973984, Accuracy: 82.76%\n",
      "Batch 111, Loss: 1.016967, Accuracy: 82.64%\n",
      "Batch 112, Loss: 0.876840, Accuracy: 82.69%\n",
      "Batch 113, Loss: 0.869618, Accuracy: 82.73%\n",
      "Batch 114, Loss: 0.879789, Accuracy: 82.74%\n",
      "Batch 115, Loss: 0.970890, Accuracy: 82.69%\n",
      "Batch 116, Loss: 0.852103, Accuracy: 82.76%\n",
      "Batch 117, Loss: 0.950011, Accuracy: 82.73%\n",
      "Batch 118, Loss: 0.977606, Accuracy: 82.68%\n",
      "Batch 119, Loss: 0.995937, Accuracy: 82.60%\n",
      "Batch 120, Loss: 0.883945, Accuracy: 82.63%\n",
      "Batch 121, Loss: 0.993549, Accuracy: 82.55%\n",
      "Batch 122, Loss: 0.961529, Accuracy: 82.52%\n",
      "Batch 123, Loss: 0.948950, Accuracy: 82.49%\n",
      "Batch 124, Loss: 0.924701, Accuracy: 82.50%\n",
      "Batch 125, Loss: 0.938353, Accuracy: 82.47%\n",
      "Batch 126, Loss: 0.972345, Accuracy: 82.44%\n",
      "Batch 127, Loss: 0.883594, Accuracy: 82.46%\n",
      "Batch 128, Loss: 0.921961, Accuracy: 82.43%\n",
      "Batch 129, Loss: 1.004158, Accuracy: 82.36%\n",
      "Batch 130, Loss: 0.837837, Accuracy: 82.43%\n",
      "Batch 131, Loss: 1.000367, Accuracy: 82.37%\n",
      "Batch 132, Loss: 0.930224, Accuracy: 82.36%\n",
      "Batch 133, Loss: 0.899933, Accuracy: 82.39%\n",
      "Batch 134, Loss: 0.890418, Accuracy: 82.40%\n",
      "Batch 135, Loss: 0.863376, Accuracy: 82.44%\n",
      "Batch 136, Loss: 0.933405, Accuracy: 82.44%\n",
      "Batch 137, Loss: 0.873740, Accuracy: 82.46%\n",
      "Batch 138, Loss: 0.950980, Accuracy: 82.45%\n",
      "Batch 139, Loss: 0.903268, Accuracy: 82.45%\n",
      "Batch 140, Loss: 0.916126, Accuracy: 82.46%\n",
      "Batch 141, Loss: 0.904918, Accuracy: 82.48%\n",
      "Batch 142, Loss: 0.869279, Accuracy: 82.50%\n",
      "Batch 143, Loss: 0.883093, Accuracy: 82.54%\n",
      "Batch 144, Loss: 0.985230, Accuracy: 82.51%\n",
      "Batch 145, Loss: 0.913289, Accuracy: 82.51%\n",
      "Batch 146, Loss: 0.968191, Accuracy: 82.48%\n",
      "Batch 147, Loss: 0.876151, Accuracy: 82.51%\n",
      "Batch 148, Loss: 0.949596, Accuracy: 82.50%\n",
      "Batch 149, Loss: 0.870396, Accuracy: 82.53%\n",
      "Batch 150, Loss: 1.046681, Accuracy: 82.44%\n",
      "Batch 151, Loss: 0.921724, Accuracy: 82.43%\n",
      "Batch 152, Loss: 0.891744, Accuracy: 82.45%\n",
      "Batch 153, Loss: 0.949298, Accuracy: 82.43%\n",
      "Batch 154, Loss: 0.900822, Accuracy: 82.46%\n",
      "Batch 155, Loss: 0.985341, Accuracy: 82.41%\n",
      "Batch 156, Loss: 0.913898, Accuracy: 82.42%\n",
      "Batch 157, Loss: 0.953844, Accuracy: 82.39%\n",
      "Batch 158, Loss: 0.959769, Accuracy: 82.37%\n",
      "Batch 159, Loss: 0.899404, Accuracy: 82.37%\n",
      "Batch 160, Loss: 0.817373, Accuracy: 82.43%\n",
      "Batch 161, Loss: 0.867734, Accuracy: 82.45%\n",
      "Batch 162, Loss: 0.963635, Accuracy: 82.43%\n",
      "Batch 163, Loss: 0.942453, Accuracy: 82.41%\n",
      "Batch 164, Loss: 0.885207, Accuracy: 82.43%\n",
      "Batch 165, Loss: 0.907319, Accuracy: 82.45%\n",
      "Batch 166, Loss: 0.934706, Accuracy: 82.45%\n",
      "Batch 167, Loss: 0.950000, Accuracy: 82.43%\n",
      "Batch 168, Loss: 0.906776, Accuracy: 82.43%\n",
      "Batch 169, Loss: 0.884796, Accuracy: 82.45%\n",
      "Batch 170, Loss: 0.883442, Accuracy: 82.47%\n",
      "Batch 171, Loss: 0.903081, Accuracy: 82.48%\n",
      "Batch 172, Loss: 0.893343, Accuracy: 82.50%\n",
      "Batch 173, Loss: 0.842052, Accuracy: 82.56%\n",
      "Batch 174, Loss: 0.949769, Accuracy: 82.55%\n",
      "Batch 175, Loss: 0.922182, Accuracy: 82.55%\n",
      "Batch 176, Loss: 0.846977, Accuracy: 82.60%\n",
      "Batch 177, Loss: 0.885383, Accuracy: 82.62%\n",
      "Batch 178, Loss: 0.949428, Accuracy: 82.60%\n",
      "Batch 179, Loss: 0.994860, Accuracy: 82.56%\n",
      "Batch 180, Loss: 0.881533, Accuracy: 82.59%\n",
      "Batch 181, Loss: 0.894971, Accuracy: 82.61%\n",
      "Batch 182, Loss: 0.920558, Accuracy: 82.62%\n",
      "Batch 183, Loss: 1.015202, Accuracy: 82.56%\n",
      "Batch 184, Loss: 0.972114, Accuracy: 82.53%\n",
      "Batch 185, Loss: 0.861264, Accuracy: 82.57%\n",
      "Batch 186, Loss: 0.946371, Accuracy: 82.54%\n",
      "Batch 187, Loss: 0.859257, Accuracy: 82.57%\n",
      "Batch 188, Loss: 0.908326, Accuracy: 82.58%\n",
      "Batch 189, Loss: 0.925730, Accuracy: 82.57%\n",
      "Batch 190, Loss: 0.889696, Accuracy: 82.59%\n",
      "Batch 191, Loss: 0.871106, Accuracy: 82.61%\n",
      "Batch 192, Loss: 0.903294, Accuracy: 82.61%\n",
      "Batch 193, Loss: 0.913079, Accuracy: 82.62%\n",
      "Batch 194, Loss: 0.890986, Accuracy: 82.64%\n",
      "Batch 195, Loss: 0.927805, Accuracy: 82.63%\n",
      "Batch 196, Loss: 0.871912, Accuracy: 82.65%\n",
      "Batch 197, Loss: 0.918990, Accuracy: 82.65%\n",
      "Batch 198, Loss: 0.870054, Accuracy: 82.67%\n",
      "Batch 199, Loss: 0.910500, Accuracy: 82.67%\n",
      "Batch 200, Loss: 0.984300, Accuracy: 82.63%\n",
      "Batch 201, Loss: 0.912434, Accuracy: 82.63%\n",
      "Batch 202, Loss: 0.953757, Accuracy: 82.61%\n",
      "Batch 203, Loss: 0.936429, Accuracy: 82.60%\n",
      "Batch 204, Loss: 0.936537, Accuracy: 82.60%\n",
      "Batch 205, Loss: 0.930377, Accuracy: 82.60%\n",
      "Batch 206, Loss: 0.989651, Accuracy: 82.55%\n",
      "Batch 207, Loss: 0.925931, Accuracy: 82.56%\n",
      "Batch 208, Loss: 0.895639, Accuracy: 82.57%\n",
      "Batch 209, Loss: 0.952952, Accuracy: 82.54%\n",
      "Batch 210, Loss: 0.876171, Accuracy: 82.55%\n",
      "Batch 211, Loss: 0.967959, Accuracy: 82.53%\n",
      "Batch 212, Loss: 0.916297, Accuracy: 82.53%\n",
      "Batch 213, Loss: 0.894950, Accuracy: 82.55%\n",
      "Training - Epoch 50, Loss: 0.918833, Accuracy: 82.55%\n",
      "Validation Batch 1, Loss: 0.903788, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.916100, Accuracy: 85.94%\n",
      "Validation Batch 3, Loss: 1.006355, Accuracy: 82.29%\n",
      "Validation Batch 4, Loss: 0.930485, Accuracy: 81.64%\n",
      "Validation Batch 5, Loss: 0.890476, Accuracy: 82.81%\n",
      "Validation Batch 6, Loss: 0.890029, Accuracy: 83.07%\n",
      "Validation Batch 7, Loss: 0.907990, Accuracy: 83.26%\n",
      "Validation Batch 8, Loss: 1.003429, Accuracy: 81.84%\n",
      "Validation Batch 9, Loss: 0.974103, Accuracy: 81.25%\n",
      "Validation Batch 10, Loss: 0.951862, Accuracy: 80.94%\n",
      "Validation Batch 11, Loss: 0.932854, Accuracy: 80.82%\n",
      "Validation Batch 12, Loss: 0.921695, Accuracy: 80.99%\n",
      "Validation Batch 13, Loss: 0.919995, Accuracy: 81.13%\n",
      "Validation Batch 14, Loss: 0.955505, Accuracy: 81.03%\n",
      "Validation Batch 15, Loss: 0.916619, Accuracy: 81.15%\n",
      "Validation Batch 16, Loss: 0.940857, Accuracy: 80.96%\n",
      "Validation Batch 17, Loss: 0.999786, Accuracy: 80.61%\n",
      "Validation Batch 18, Loss: 0.875372, Accuracy: 80.99%\n",
      "Validation Batch 19, Loss: 0.959213, Accuracy: 80.92%\n",
      "Validation Batch 20, Loss: 0.996108, Accuracy: 80.62%\n",
      "Validation Batch 21, Loss: 0.978680, Accuracy: 80.36%\n",
      "Validation Batch 22, Loss: 0.934144, Accuracy: 80.40%\n",
      "Validation Batch 23, Loss: 0.971563, Accuracy: 80.30%\n",
      "Validation Batch 24, Loss: 0.942279, Accuracy: 80.34%\n",
      "Validation Batch 25, Loss: 0.906300, Accuracy: 80.56%\n",
      "Validation Batch 26, Loss: 0.925982, Accuracy: 80.65%\n",
      "Validation Batch 27, Loss: 0.947041, Accuracy: 80.56%\n",
      "Validation - Epoch 50, Loss: 0.940689, Accuracy: 80.56%\n",
      "Patience—3\n",
      "Epoch 51\n",
      "Batch 1, Loss: 0.956218, Accuracy: 78.12%\n",
      "Batch 2, Loss: 0.880281, Accuracy: 82.03%\n",
      "Batch 3, Loss: 0.967257, Accuracy: 81.25%\n",
      "Batch 4, Loss: 0.959023, Accuracy: 80.86%\n",
      "Batch 5, Loss: 0.894121, Accuracy: 81.56%\n",
      "Batch 6, Loss: 0.829800, Accuracy: 83.33%\n",
      "Batch 7, Loss: 0.902943, Accuracy: 83.48%\n",
      "Batch 8, Loss: 0.852349, Accuracy: 83.98%\n",
      "Batch 9, Loss: 0.951431, Accuracy: 83.51%\n",
      "Batch 10, Loss: 0.958707, Accuracy: 82.97%\n",
      "Batch 11, Loss: 0.940670, Accuracy: 82.67%\n",
      "Batch 12, Loss: 0.850100, Accuracy: 83.33%\n",
      "Batch 13, Loss: 0.954776, Accuracy: 82.93%\n",
      "Batch 14, Loss: 0.850385, Accuracy: 83.48%\n",
      "Batch 15, Loss: 0.937295, Accuracy: 83.33%\n",
      "Batch 16, Loss: 0.821999, Accuracy: 83.98%\n",
      "Batch 17, Loss: 0.908251, Accuracy: 83.92%\n",
      "Batch 18, Loss: 0.897047, Accuracy: 84.03%\n",
      "Batch 19, Loss: 0.952369, Accuracy: 83.88%\n",
      "Batch 20, Loss: 0.917444, Accuracy: 83.75%\n",
      "Batch 21, Loss: 0.972701, Accuracy: 83.56%\n",
      "Batch 22, Loss: 0.948397, Accuracy: 83.31%\n",
      "Batch 23, Loss: 0.940722, Accuracy: 83.15%\n",
      "Batch 24, Loss: 0.923860, Accuracy: 83.07%\n",
      "Batch 25, Loss: 0.843773, Accuracy: 83.38%\n",
      "Batch 26, Loss: 0.932539, Accuracy: 83.29%\n",
      "Batch 27, Loss: 0.852363, Accuracy: 83.51%\n",
      "Batch 28, Loss: 0.872771, Accuracy: 83.65%\n",
      "Batch 29, Loss: 0.847739, Accuracy: 83.89%\n",
      "Batch 30, Loss: 0.891462, Accuracy: 83.96%\n",
      "Batch 31, Loss: 0.871717, Accuracy: 84.07%\n",
      "Batch 32, Loss: 0.871168, Accuracy: 84.23%\n",
      "Batch 33, Loss: 0.940327, Accuracy: 84.14%\n",
      "Batch 34, Loss: 0.923623, Accuracy: 84.10%\n",
      "Batch 35, Loss: 1.031180, Accuracy: 83.75%\n",
      "Batch 36, Loss: 0.880148, Accuracy: 83.85%\n",
      "Batch 37, Loss: 0.926328, Accuracy: 83.78%\n",
      "Batch 38, Loss: 0.967797, Accuracy: 83.59%\n",
      "Batch 39, Loss: 0.888890, Accuracy: 83.69%\n",
      "Batch 40, Loss: 0.927780, Accuracy: 83.63%\n",
      "Batch 41, Loss: 0.945282, Accuracy: 83.57%\n",
      "Batch 42, Loss: 0.893692, Accuracy: 83.59%\n",
      "Batch 43, Loss: 0.947314, Accuracy: 83.47%\n",
      "Batch 44, Loss: 0.870448, Accuracy: 83.56%\n",
      "Batch 45, Loss: 0.921259, Accuracy: 83.58%\n",
      "Batch 46, Loss: 0.963858, Accuracy: 83.39%\n",
      "Batch 47, Loss: 0.898494, Accuracy: 83.41%\n",
      "Batch 48, Loss: 0.876155, Accuracy: 83.50%\n",
      "Batch 49, Loss: 0.896388, Accuracy: 83.55%\n",
      "Batch 50, Loss: 0.956911, Accuracy: 83.47%\n",
      "Batch 51, Loss: 0.881394, Accuracy: 83.52%\n",
      "Batch 52, Loss: 0.896724, Accuracy: 83.50%\n",
      "Batch 53, Loss: 0.889653, Accuracy: 83.55%\n",
      "Batch 54, Loss: 0.935505, Accuracy: 83.54%\n",
      "Batch 55, Loss: 0.960187, Accuracy: 83.41%\n",
      "Batch 56, Loss: 0.900747, Accuracy: 83.43%\n",
      "Batch 57, Loss: 0.945850, Accuracy: 83.33%\n",
      "Batch 58, Loss: 0.984759, Accuracy: 83.22%\n",
      "Batch 59, Loss: 0.930043, Accuracy: 83.16%\n",
      "Batch 60, Loss: 0.933447, Accuracy: 83.10%\n",
      "Batch 61, Loss: 0.881195, Accuracy: 83.15%\n",
      "Batch 62, Loss: 0.904884, Accuracy: 83.17%\n",
      "Batch 63, Loss: 0.867340, Accuracy: 83.23%\n",
      "Batch 64, Loss: 0.943187, Accuracy: 83.18%\n",
      "Batch 65, Loss: 0.949613, Accuracy: 83.15%\n",
      "Batch 66, Loss: 0.959003, Accuracy: 83.07%\n",
      "Batch 67, Loss: 0.913779, Accuracy: 83.09%\n",
      "Batch 68, Loss: 0.853435, Accuracy: 83.20%\n",
      "Batch 69, Loss: 0.866046, Accuracy: 83.27%\n",
      "Batch 70, Loss: 0.887107, Accuracy: 83.26%\n",
      "Batch 71, Loss: 0.798162, Accuracy: 83.43%\n",
      "Batch 72, Loss: 0.932171, Accuracy: 83.38%\n",
      "Batch 73, Loss: 0.865966, Accuracy: 83.43%\n",
      "Batch 74, Loss: 0.919405, Accuracy: 83.40%\n",
      "Batch 75, Loss: 0.918075, Accuracy: 83.38%\n",
      "Batch 76, Loss: 0.970608, Accuracy: 83.26%\n",
      "Batch 77, Loss: 0.905647, Accuracy: 83.28%\n",
      "Batch 78, Loss: 0.959775, Accuracy: 83.21%\n",
      "Batch 79, Loss: 0.913935, Accuracy: 83.21%\n",
      "Batch 80, Loss: 0.932458, Accuracy: 83.20%\n",
      "Batch 81, Loss: 0.830076, Accuracy: 83.31%\n",
      "Batch 82, Loss: 0.879066, Accuracy: 83.37%\n",
      "Batch 83, Loss: 0.989240, Accuracy: 83.28%\n",
      "Batch 84, Loss: 0.938306, Accuracy: 83.24%\n",
      "Batch 85, Loss: 0.871229, Accuracy: 83.27%\n",
      "Batch 86, Loss: 0.949718, Accuracy: 83.21%\n",
      "Batch 87, Loss: 0.944604, Accuracy: 83.17%\n",
      "Batch 88, Loss: 0.901694, Accuracy: 83.20%\n",
      "Batch 89, Loss: 0.961765, Accuracy: 83.15%\n",
      "Batch 90, Loss: 0.895896, Accuracy: 83.16%\n",
      "Batch 91, Loss: 0.945178, Accuracy: 83.14%\n",
      "Batch 92, Loss: 0.967713, Accuracy: 83.08%\n",
      "Batch 93, Loss: 0.897654, Accuracy: 83.10%\n",
      "Batch 94, Loss: 0.873727, Accuracy: 83.16%\n",
      "Batch 95, Loss: 0.873648, Accuracy: 83.21%\n",
      "Batch 96, Loss: 0.905932, Accuracy: 83.24%\n",
      "Batch 97, Loss: 0.977747, Accuracy: 83.13%\n",
      "Batch 98, Loss: 0.930165, Accuracy: 83.13%\n",
      "Batch 99, Loss: 0.984438, Accuracy: 83.05%\n",
      "Batch 100, Loss: 0.909710, Accuracy: 83.03%\n",
      "Batch 101, Loss: 0.939237, Accuracy: 83.00%\n",
      "Batch 102, Loss: 0.916714, Accuracy: 83.00%\n",
      "Batch 103, Loss: 0.949355, Accuracy: 82.96%\n",
      "Batch 104, Loss: 0.897424, Accuracy: 82.98%\n",
      "Batch 105, Loss: 0.947411, Accuracy: 82.95%\n",
      "Batch 106, Loss: 0.825386, Accuracy: 83.05%\n",
      "Batch 107, Loss: 0.923377, Accuracy: 83.03%\n",
      "Batch 108, Loss: 0.882614, Accuracy: 83.06%\n",
      "Batch 109, Loss: 0.911456, Accuracy: 83.06%\n",
      "Batch 110, Loss: 0.902531, Accuracy: 83.07%\n",
      "Batch 111, Loss: 0.958993, Accuracy: 83.04%\n",
      "Batch 112, Loss: 0.863955, Accuracy: 83.09%\n",
      "Batch 113, Loss: 0.902696, Accuracy: 83.10%\n",
      "Batch 114, Loss: 0.949328, Accuracy: 83.07%\n",
      "Batch 115, Loss: 0.852665, Accuracy: 83.12%\n",
      "Batch 116, Loss: 0.944407, Accuracy: 83.11%\n",
      "Batch 117, Loss: 0.858564, Accuracy: 83.16%\n",
      "Batch 118, Loss: 0.940205, Accuracy: 83.12%\n",
      "Batch 119, Loss: 0.998450, Accuracy: 83.02%\n",
      "Batch 120, Loss: 0.882009, Accuracy: 83.05%\n",
      "Batch 121, Loss: 0.891002, Accuracy: 83.06%\n",
      "Batch 122, Loss: 1.006171, Accuracy: 82.99%\n",
      "Batch 123, Loss: 0.878231, Accuracy: 83.02%\n",
      "Batch 124, Loss: 0.904848, Accuracy: 83.01%\n",
      "Batch 125, Loss: 0.875428, Accuracy: 83.06%\n",
      "Batch 126, Loss: 1.011706, Accuracy: 83.00%\n",
      "Batch 127, Loss: 0.913194, Accuracy: 83.00%\n",
      "Batch 128, Loss: 0.892483, Accuracy: 83.02%\n",
      "Batch 129, Loss: 0.902360, Accuracy: 83.03%\n",
      "Batch 130, Loss: 0.992139, Accuracy: 82.96%\n",
      "Batch 131, Loss: 0.967753, Accuracy: 82.92%\n",
      "Batch 132, Loss: 0.954643, Accuracy: 82.88%\n",
      "Batch 133, Loss: 0.907506, Accuracy: 82.88%\n",
      "Batch 134, Loss: 0.848311, Accuracy: 82.95%\n",
      "Batch 135, Loss: 0.955793, Accuracy: 82.93%\n",
      "Batch 136, Loss: 0.884584, Accuracy: 82.94%\n",
      "Batch 137, Loss: 0.874824, Accuracy: 82.97%\n",
      "Batch 138, Loss: 0.851412, Accuracy: 83.02%\n",
      "Batch 139, Loss: 0.895183, Accuracy: 83.03%\n",
      "Batch 140, Loss: 0.971407, Accuracy: 82.99%\n",
      "Batch 141, Loss: 0.877318, Accuracy: 83.02%\n",
      "Batch 142, Loss: 0.949333, Accuracy: 83.00%\n",
      "Batch 143, Loss: 0.942779, Accuracy: 82.98%\n",
      "Batch 144, Loss: 0.885293, Accuracy: 83.00%\n",
      "Batch 145, Loss: 0.893670, Accuracy: 83.02%\n",
      "Batch 146, Loss: 0.922449, Accuracy: 83.02%\n",
      "Batch 147, Loss: 0.886156, Accuracy: 83.05%\n",
      "Batch 148, Loss: 0.904938, Accuracy: 83.04%\n",
      "Batch 149, Loss: 0.900396, Accuracy: 83.06%\n",
      "Batch 150, Loss: 0.915017, Accuracy: 83.07%\n",
      "Batch 151, Loss: 0.911645, Accuracy: 83.07%\n",
      "Batch 152, Loss: 0.938293, Accuracy: 83.06%\n",
      "Batch 153, Loss: 0.945389, Accuracy: 83.05%\n",
      "Batch 154, Loss: 0.947899, Accuracy: 83.02%\n",
      "Batch 155, Loss: 0.878002, Accuracy: 83.02%\n",
      "Batch 156, Loss: 0.854309, Accuracy: 83.09%\n",
      "Batch 157, Loss: 0.874308, Accuracy: 83.11%\n",
      "Batch 158, Loss: 0.886847, Accuracy: 83.13%\n",
      "Batch 159, Loss: 0.967439, Accuracy: 83.11%\n",
      "Batch 160, Loss: 0.870860, Accuracy: 83.12%\n",
      "Batch 161, Loss: 0.907575, Accuracy: 83.13%\n",
      "Batch 162, Loss: 0.906184, Accuracy: 83.14%\n",
      "Batch 163, Loss: 0.966139, Accuracy: 83.11%\n",
      "Batch 164, Loss: 0.903140, Accuracy: 83.13%\n",
      "Batch 165, Loss: 0.962085, Accuracy: 83.10%\n",
      "Batch 166, Loss: 0.892599, Accuracy: 83.11%\n",
      "Batch 167, Loss: 0.939673, Accuracy: 83.10%\n",
      "Batch 168, Loss: 0.917688, Accuracy: 83.10%\n",
      "Batch 169, Loss: 0.928128, Accuracy: 83.10%\n",
      "Batch 170, Loss: 0.877221, Accuracy: 83.12%\n",
      "Batch 171, Loss: 0.970343, Accuracy: 83.09%\n",
      "Batch 172, Loss: 0.928542, Accuracy: 83.08%\n",
      "Batch 173, Loss: 0.956318, Accuracy: 83.05%\n",
      "Batch 174, Loss: 0.946459, Accuracy: 83.03%\n",
      "Batch 175, Loss: 0.922742, Accuracy: 83.02%\n",
      "Batch 176, Loss: 0.948553, Accuracy: 82.99%\n",
      "Batch 177, Loss: 0.913419, Accuracy: 82.99%\n",
      "Batch 178, Loss: 0.874475, Accuracy: 83.01%\n",
      "Batch 179, Loss: 0.867659, Accuracy: 83.05%\n",
      "Batch 180, Loss: 0.901941, Accuracy: 83.06%\n",
      "Batch 181, Loss: 0.968691, Accuracy: 83.03%\n",
      "Batch 182, Loss: 0.998417, Accuracy: 82.97%\n",
      "Batch 183, Loss: 0.963199, Accuracy: 82.93%\n",
      "Batch 184, Loss: 0.869579, Accuracy: 82.97%\n",
      "Batch 185, Loss: 0.994273, Accuracy: 82.92%\n",
      "Batch 186, Loss: 0.922875, Accuracy: 82.92%\n",
      "Batch 187, Loss: 0.974429, Accuracy: 82.89%\n",
      "Batch 188, Loss: 0.926533, Accuracy: 82.89%\n",
      "Batch 189, Loss: 0.868890, Accuracy: 82.90%\n",
      "Batch 190, Loss: 0.962779, Accuracy: 82.87%\n",
      "Batch 191, Loss: 0.975833, Accuracy: 82.83%\n",
      "Batch 192, Loss: 0.859886, Accuracy: 82.87%\n",
      "Batch 193, Loss: 0.941425, Accuracy: 82.85%\n",
      "Batch 194, Loss: 0.987630, Accuracy: 82.82%\n",
      "Batch 195, Loss: 0.953728, Accuracy: 82.80%\n",
      "Batch 196, Loss: 0.842944, Accuracy: 82.84%\n",
      "Batch 197, Loss: 0.892885, Accuracy: 82.85%\n",
      "Batch 198, Loss: 0.884064, Accuracy: 82.86%\n",
      "Batch 199, Loss: 0.913278, Accuracy: 82.86%\n",
      "Batch 200, Loss: 0.936864, Accuracy: 82.85%\n",
      "Batch 201, Loss: 0.916681, Accuracy: 82.84%\n",
      "Batch 202, Loss: 0.922741, Accuracy: 82.84%\n",
      "Batch 203, Loss: 0.907699, Accuracy: 82.85%\n",
      "Batch 204, Loss: 0.987010, Accuracy: 82.83%\n",
      "Batch 205, Loss: 0.946591, Accuracy: 82.82%\n",
      "Batch 206, Loss: 0.942489, Accuracy: 82.80%\n",
      "Batch 207, Loss: 0.904743, Accuracy: 82.81%\n",
      "Batch 208, Loss: 0.895176, Accuracy: 82.83%\n",
      "Batch 209, Loss: 0.902668, Accuracy: 82.83%\n",
      "Batch 210, Loss: 0.851008, Accuracy: 82.87%\n",
      "Batch 211, Loss: 0.910098, Accuracy: 82.88%\n",
      "Batch 212, Loss: 0.845976, Accuracy: 82.92%\n",
      "Batch 213, Loss: 0.922572, Accuracy: 82.92%\n",
      "Training - Epoch 51, Loss: 0.916140, Accuracy: 82.92%\n",
      "Validation Batch 1, Loss: 0.909277, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.916664, Accuracy: 86.72%\n",
      "Validation Batch 3, Loss: 1.005969, Accuracy: 82.81%\n",
      "Validation Batch 4, Loss: 0.931040, Accuracy: 82.42%\n",
      "Validation Batch 5, Loss: 0.891705, Accuracy: 83.12%\n",
      "Validation Batch 6, Loss: 0.884950, Accuracy: 83.59%\n",
      "Validation Batch 7, Loss: 0.907223, Accuracy: 83.93%\n",
      "Validation Batch 8, Loss: 1.001864, Accuracy: 82.62%\n",
      "Validation Batch 9, Loss: 0.970764, Accuracy: 82.12%\n",
      "Validation Batch 10, Loss: 0.953831, Accuracy: 81.72%\n",
      "Validation Batch 11, Loss: 0.935358, Accuracy: 81.39%\n",
      "Validation Batch 12, Loss: 0.921480, Accuracy: 81.38%\n",
      "Validation Batch 13, Loss: 0.918312, Accuracy: 81.49%\n",
      "Validation Batch 14, Loss: 0.958417, Accuracy: 81.36%\n",
      "Validation Batch 15, Loss: 0.918460, Accuracy: 81.46%\n",
      "Validation Batch 16, Loss: 0.938049, Accuracy: 81.35%\n",
      "Validation Batch 17, Loss: 0.993254, Accuracy: 81.07%\n",
      "Validation Batch 18, Loss: 0.879770, Accuracy: 81.42%\n",
      "Validation Batch 19, Loss: 0.957158, Accuracy: 81.41%\n",
      "Validation Batch 20, Loss: 1.002071, Accuracy: 81.09%\n",
      "Validation Batch 21, Loss: 0.983378, Accuracy: 80.80%\n",
      "Validation Batch 22, Loss: 0.935971, Accuracy: 80.82%\n",
      "Validation Batch 23, Loss: 0.971588, Accuracy: 80.71%\n",
      "Validation Batch 24, Loss: 0.943398, Accuracy: 80.73%\n",
      "Validation Batch 25, Loss: 0.911585, Accuracy: 80.88%\n",
      "Validation Batch 26, Loss: 0.931671, Accuracy: 80.83%\n",
      "Validation Batch 27, Loss: 0.950823, Accuracy: 80.74%\n",
      "Validation - Epoch 51, Loss: 0.941631, Accuracy: 80.74%\n",
      "Patience—4\n",
      "Epoch 52\n",
      "Batch 1, Loss: 0.919116, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.905241, Accuracy: 83.59%\n",
      "Batch 3, Loss: 0.895853, Accuracy: 83.85%\n",
      "Batch 4, Loss: 0.900447, Accuracy: 84.38%\n",
      "Batch 5, Loss: 0.876490, Accuracy: 85.00%\n",
      "Batch 6, Loss: 0.923069, Accuracy: 84.38%\n",
      "Batch 7, Loss: 0.905894, Accuracy: 84.38%\n",
      "Batch 8, Loss: 0.951271, Accuracy: 83.79%\n",
      "Batch 9, Loss: 0.917148, Accuracy: 83.85%\n",
      "Batch 10, Loss: 0.881762, Accuracy: 84.22%\n",
      "Batch 11, Loss: 0.907926, Accuracy: 84.23%\n",
      "Batch 12, Loss: 0.929642, Accuracy: 84.24%\n",
      "Batch 13, Loss: 0.955379, Accuracy: 83.65%\n",
      "Batch 14, Loss: 0.910729, Accuracy: 83.59%\n",
      "Batch 15, Loss: 0.935472, Accuracy: 83.33%\n",
      "Batch 16, Loss: 0.829715, Accuracy: 83.89%\n",
      "Batch 17, Loss: 0.963830, Accuracy: 83.55%\n",
      "Batch 18, Loss: 0.918378, Accuracy: 83.42%\n",
      "Batch 19, Loss: 0.903559, Accuracy: 83.47%\n",
      "Batch 20, Loss: 0.839926, Accuracy: 83.91%\n",
      "Batch 21, Loss: 0.948953, Accuracy: 83.71%\n",
      "Batch 22, Loss: 0.905095, Accuracy: 83.74%\n",
      "Batch 23, Loss: 0.945851, Accuracy: 83.63%\n",
      "Batch 24, Loss: 0.890789, Accuracy: 83.72%\n",
      "Batch 25, Loss: 0.928285, Accuracy: 83.69%\n",
      "Batch 26, Loss: 0.960158, Accuracy: 83.47%\n",
      "Batch 27, Loss: 0.975109, Accuracy: 83.28%\n",
      "Batch 28, Loss: 0.931357, Accuracy: 83.26%\n",
      "Batch 29, Loss: 0.835898, Accuracy: 83.51%\n",
      "Batch 30, Loss: 0.952305, Accuracy: 83.49%\n",
      "Batch 31, Loss: 0.952769, Accuracy: 83.37%\n",
      "Batch 32, Loss: 0.965911, Accuracy: 83.15%\n",
      "Batch 33, Loss: 0.891874, Accuracy: 83.19%\n",
      "Batch 34, Loss: 0.919536, Accuracy: 83.18%\n",
      "Batch 35, Loss: 0.925484, Accuracy: 83.17%\n",
      "Batch 36, Loss: 0.841031, Accuracy: 83.38%\n",
      "Batch 37, Loss: 0.893705, Accuracy: 83.49%\n",
      "Batch 38, Loss: 0.924403, Accuracy: 83.47%\n",
      "Batch 39, Loss: 0.967050, Accuracy: 83.29%\n",
      "Batch 40, Loss: 0.922926, Accuracy: 83.28%\n",
      "Batch 41, Loss: 0.877303, Accuracy: 83.35%\n",
      "Batch 42, Loss: 0.844651, Accuracy: 83.52%\n",
      "Batch 43, Loss: 0.887060, Accuracy: 83.54%\n",
      "Batch 44, Loss: 0.919337, Accuracy: 83.49%\n",
      "Batch 45, Loss: 0.867842, Accuracy: 83.61%\n",
      "Batch 46, Loss: 0.893735, Accuracy: 83.70%\n",
      "Batch 47, Loss: 0.894703, Accuracy: 83.74%\n",
      "Batch 48, Loss: 0.900448, Accuracy: 83.79%\n",
      "Batch 49, Loss: 0.940448, Accuracy: 83.74%\n",
      "Batch 50, Loss: 0.921601, Accuracy: 83.72%\n",
      "Batch 51, Loss: 0.926761, Accuracy: 83.67%\n",
      "Batch 52, Loss: 0.947133, Accuracy: 83.56%\n",
      "Batch 53, Loss: 0.903364, Accuracy: 83.58%\n",
      "Batch 54, Loss: 0.881498, Accuracy: 83.62%\n",
      "Batch 55, Loss: 0.845942, Accuracy: 83.75%\n",
      "Batch 56, Loss: 0.830315, Accuracy: 83.90%\n",
      "Batch 57, Loss: 0.938978, Accuracy: 83.88%\n",
      "Batch 58, Loss: 0.905466, Accuracy: 83.89%\n",
      "Batch 59, Loss: 0.873687, Accuracy: 83.92%\n",
      "Batch 60, Loss: 0.878153, Accuracy: 84.01%\n",
      "Batch 61, Loss: 0.958218, Accuracy: 83.94%\n",
      "Batch 62, Loss: 0.916170, Accuracy: 83.92%\n",
      "Batch 63, Loss: 0.951294, Accuracy: 83.85%\n",
      "Batch 64, Loss: 0.904247, Accuracy: 83.84%\n",
      "Batch 65, Loss: 0.902095, Accuracy: 83.85%\n",
      "Batch 66, Loss: 0.933310, Accuracy: 83.81%\n",
      "Batch 67, Loss: 0.837936, Accuracy: 83.91%\n",
      "Batch 68, Loss: 0.963942, Accuracy: 83.80%\n",
      "Batch 69, Loss: 0.986734, Accuracy: 83.70%\n",
      "Batch 70, Loss: 0.906763, Accuracy: 83.71%\n",
      "Batch 71, Loss: 0.903093, Accuracy: 83.74%\n",
      "Batch 72, Loss: 0.873757, Accuracy: 83.79%\n",
      "Batch 73, Loss: 0.907612, Accuracy: 83.75%\n",
      "Batch 74, Loss: 0.948144, Accuracy: 83.70%\n",
      "Batch 75, Loss: 0.889363, Accuracy: 83.73%\n",
      "Batch 76, Loss: 0.901752, Accuracy: 83.72%\n",
      "Batch 77, Loss: 0.840289, Accuracy: 83.81%\n",
      "Batch 78, Loss: 0.837457, Accuracy: 83.91%\n",
      "Batch 79, Loss: 0.834318, Accuracy: 84.00%\n",
      "Batch 80, Loss: 0.863015, Accuracy: 84.06%\n",
      "Batch 81, Loss: 0.917582, Accuracy: 84.05%\n",
      "Batch 82, Loss: 0.896587, Accuracy: 84.05%\n",
      "Batch 83, Loss: 0.856837, Accuracy: 84.09%\n",
      "Batch 84, Loss: 0.884582, Accuracy: 84.13%\n",
      "Batch 85, Loss: 0.921878, Accuracy: 84.12%\n",
      "Batch 86, Loss: 0.949767, Accuracy: 84.05%\n",
      "Batch 87, Loss: 0.967707, Accuracy: 83.98%\n",
      "Batch 88, Loss: 0.942049, Accuracy: 83.95%\n",
      "Batch 89, Loss: 0.945382, Accuracy: 83.92%\n",
      "Batch 90, Loss: 0.907041, Accuracy: 83.92%\n",
      "Batch 91, Loss: 0.875428, Accuracy: 83.96%\n",
      "Batch 92, Loss: 0.980864, Accuracy: 83.87%\n",
      "Batch 93, Loss: 0.871233, Accuracy: 83.90%\n",
      "Batch 94, Loss: 0.937112, Accuracy: 83.88%\n",
      "Batch 95, Loss: 0.908353, Accuracy: 83.87%\n",
      "Batch 96, Loss: 0.891488, Accuracy: 83.89%\n",
      "Batch 97, Loss: 0.930094, Accuracy: 83.86%\n",
      "Batch 98, Loss: 0.875859, Accuracy: 83.90%\n",
      "Batch 99, Loss: 0.900573, Accuracy: 83.92%\n",
      "Batch 100, Loss: 0.855986, Accuracy: 83.97%\n",
      "Batch 101, Loss: 0.908746, Accuracy: 83.97%\n",
      "Batch 102, Loss: 0.873862, Accuracy: 84.01%\n",
      "Batch 103, Loss: 0.910761, Accuracy: 84.00%\n",
      "Batch 104, Loss: 0.980913, Accuracy: 83.92%\n",
      "Batch 105, Loss: 0.984047, Accuracy: 83.81%\n",
      "Batch 106, Loss: 0.859079, Accuracy: 83.87%\n",
      "Batch 107, Loss: 0.880604, Accuracy: 83.91%\n",
      "Batch 108, Loss: 0.865563, Accuracy: 83.94%\n",
      "Batch 109, Loss: 0.855016, Accuracy: 83.99%\n",
      "Batch 110, Loss: 0.958353, Accuracy: 83.95%\n",
      "Batch 111, Loss: 0.906187, Accuracy: 83.95%\n",
      "Batch 112, Loss: 0.877276, Accuracy: 83.98%\n",
      "Batch 113, Loss: 0.942457, Accuracy: 83.95%\n",
      "Batch 114, Loss: 0.895780, Accuracy: 83.95%\n",
      "Batch 115, Loss: 0.895073, Accuracy: 83.95%\n",
      "Batch 116, Loss: 0.956690, Accuracy: 83.92%\n",
      "Batch 117, Loss: 0.886312, Accuracy: 83.92%\n",
      "Batch 118, Loss: 0.948130, Accuracy: 83.87%\n",
      "Batch 119, Loss: 0.938990, Accuracy: 83.85%\n",
      "Batch 120, Loss: 0.820986, Accuracy: 83.93%\n",
      "Batch 121, Loss: 0.972877, Accuracy: 83.88%\n",
      "Batch 122, Loss: 0.879115, Accuracy: 83.91%\n",
      "Batch 123, Loss: 0.865983, Accuracy: 83.94%\n",
      "Batch 124, Loss: 0.894907, Accuracy: 83.97%\n",
      "Batch 125, Loss: 0.921543, Accuracy: 83.96%\n",
      "Batch 126, Loss: 0.932535, Accuracy: 83.93%\n",
      "Batch 127, Loss: 0.934740, Accuracy: 83.88%\n",
      "Batch 128, Loss: 0.972128, Accuracy: 83.84%\n",
      "Batch 129, Loss: 0.889381, Accuracy: 83.84%\n",
      "Batch 130, Loss: 0.995620, Accuracy: 83.76%\n",
      "Batch 131, Loss: 0.941990, Accuracy: 83.73%\n",
      "Batch 132, Loss: 0.892134, Accuracy: 83.75%\n",
      "Batch 133, Loss: 0.904632, Accuracy: 83.74%\n",
      "Batch 134, Loss: 0.981980, Accuracy: 83.69%\n",
      "Batch 135, Loss: 0.971391, Accuracy: 83.63%\n",
      "Batch 136, Loss: 0.974043, Accuracy: 83.57%\n",
      "Batch 137, Loss: 0.912314, Accuracy: 83.59%\n",
      "Batch 138, Loss: 0.848724, Accuracy: 83.64%\n",
      "Batch 139, Loss: 0.882302, Accuracy: 83.64%\n",
      "Batch 140, Loss: 0.930852, Accuracy: 83.60%\n",
      "Batch 141, Loss: 0.861843, Accuracy: 83.64%\n",
      "Batch 142, Loss: 0.969360, Accuracy: 83.60%\n",
      "Batch 143, Loss: 0.956196, Accuracy: 83.58%\n",
      "Batch 144, Loss: 0.943319, Accuracy: 83.55%\n",
      "Batch 145, Loss: 0.936899, Accuracy: 83.55%\n",
      "Batch 146, Loss: 0.859099, Accuracy: 83.58%\n",
      "Batch 147, Loss: 0.931570, Accuracy: 83.56%\n",
      "Batch 148, Loss: 0.929910, Accuracy: 83.54%\n",
      "Batch 149, Loss: 0.845399, Accuracy: 83.59%\n",
      "Batch 150, Loss: 0.863555, Accuracy: 83.64%\n",
      "Batch 151, Loss: 1.022612, Accuracy: 83.57%\n",
      "Batch 152, Loss: 0.922503, Accuracy: 83.56%\n",
      "Batch 153, Loss: 0.932684, Accuracy: 83.55%\n",
      "Batch 154, Loss: 0.900895, Accuracy: 83.55%\n",
      "Batch 155, Loss: 0.907369, Accuracy: 83.55%\n",
      "Batch 156, Loss: 0.892734, Accuracy: 83.55%\n",
      "Batch 157, Loss: 0.885027, Accuracy: 83.57%\n",
      "Batch 158, Loss: 0.871148, Accuracy: 83.59%\n",
      "Batch 159, Loss: 0.943568, Accuracy: 83.58%\n",
      "Batch 160, Loss: 0.887387, Accuracy: 83.58%\n",
      "Batch 161, Loss: 0.916808, Accuracy: 83.58%\n",
      "Batch 162, Loss: 0.949271, Accuracy: 83.55%\n",
      "Batch 163, Loss: 0.939895, Accuracy: 83.52%\n",
      "Batch 164, Loss: 0.889559, Accuracy: 83.54%\n",
      "Batch 165, Loss: 0.849773, Accuracy: 83.59%\n",
      "Batch 166, Loss: 0.940393, Accuracy: 83.58%\n",
      "Batch 167, Loss: 0.893427, Accuracy: 83.59%\n",
      "Batch 168, Loss: 0.959590, Accuracy: 83.56%\n",
      "Batch 169, Loss: 1.077379, Accuracy: 83.45%\n",
      "Batch 170, Loss: 0.874051, Accuracy: 83.47%\n",
      "Batch 171, Loss: 0.840306, Accuracy: 83.52%\n",
      "Batch 172, Loss: 0.937437, Accuracy: 83.49%\n",
      "Batch 173, Loss: 0.930076, Accuracy: 83.48%\n",
      "Batch 174, Loss: 0.886126, Accuracy: 83.49%\n",
      "Batch 175, Loss: 0.887298, Accuracy: 83.51%\n",
      "Batch 176, Loss: 0.935544, Accuracy: 83.50%\n",
      "Batch 177, Loss: 0.938735, Accuracy: 83.48%\n",
      "Batch 178, Loss: 0.911521, Accuracy: 83.47%\n",
      "Batch 179, Loss: 0.885203, Accuracy: 83.48%\n",
      "Batch 180, Loss: 0.897686, Accuracy: 83.48%\n",
      "Batch 181, Loss: 0.967427, Accuracy: 83.45%\n",
      "Batch 182, Loss: 0.807314, Accuracy: 83.53%\n",
      "Batch 183, Loss: 0.958851, Accuracy: 83.50%\n",
      "Batch 184, Loss: 0.950316, Accuracy: 83.47%\n",
      "Batch 185, Loss: 0.958575, Accuracy: 83.45%\n",
      "Batch 186, Loss: 0.921944, Accuracy: 83.44%\n",
      "Batch 187, Loss: 0.826807, Accuracy: 83.49%\n",
      "Batch 188, Loss: 0.893427, Accuracy: 83.49%\n",
      "Batch 189, Loss: 0.916197, Accuracy: 83.49%\n",
      "Batch 190, Loss: 0.873515, Accuracy: 83.52%\n",
      "Batch 191, Loss: 0.893812, Accuracy: 83.52%\n",
      "Batch 192, Loss: 0.933659, Accuracy: 83.51%\n",
      "Batch 193, Loss: 0.963433, Accuracy: 83.48%\n",
      "Batch 194, Loss: 0.929693, Accuracy: 83.48%\n",
      "Batch 195, Loss: 0.862905, Accuracy: 83.51%\n",
      "Batch 196, Loss: 0.943758, Accuracy: 83.50%\n",
      "Batch 197, Loss: 0.827567, Accuracy: 83.54%\n",
      "Batch 198, Loss: 0.887461, Accuracy: 83.55%\n",
      "Batch 199, Loss: 1.005191, Accuracy: 83.51%\n",
      "Batch 200, Loss: 0.897131, Accuracy: 83.52%\n",
      "Batch 201, Loss: 0.832438, Accuracy: 83.56%\n",
      "Batch 202, Loss: 0.872136, Accuracy: 83.58%\n",
      "Batch 203, Loss: 0.906405, Accuracy: 83.59%\n",
      "Batch 204, Loss: 0.849297, Accuracy: 83.62%\n",
      "Batch 205, Loss: 0.942537, Accuracy: 83.60%\n",
      "Batch 206, Loss: 0.881123, Accuracy: 83.61%\n",
      "Batch 207, Loss: 0.875790, Accuracy: 83.64%\n",
      "Batch 208, Loss: 0.965085, Accuracy: 83.60%\n",
      "Batch 209, Loss: 0.966697, Accuracy: 83.58%\n",
      "Batch 210, Loss: 0.867726, Accuracy: 83.61%\n",
      "Batch 211, Loss: 0.958217, Accuracy: 83.58%\n",
      "Batch 212, Loss: 0.859974, Accuracy: 83.61%\n",
      "Batch 213, Loss: 0.873209, Accuracy: 83.62%\n",
      "Training - Epoch 52, Loss: 0.910425, Accuracy: 83.62%\n",
      "Validation Batch 1, Loss: 0.904824, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.918637, Accuracy: 85.94%\n",
      "Validation Batch 3, Loss: 0.999113, Accuracy: 82.29%\n",
      "Validation Batch 4, Loss: 0.927135, Accuracy: 81.64%\n",
      "Validation Batch 5, Loss: 0.890067, Accuracy: 82.81%\n",
      "Validation Batch 6, Loss: 0.880492, Accuracy: 83.33%\n",
      "Validation Batch 7, Loss: 0.905225, Accuracy: 83.71%\n",
      "Validation Batch 8, Loss: 0.998769, Accuracy: 82.42%\n",
      "Validation Batch 9, Loss: 0.963363, Accuracy: 82.12%\n",
      "Validation Batch 10, Loss: 0.952093, Accuracy: 81.72%\n",
      "Validation Batch 11, Loss: 0.934780, Accuracy: 81.68%\n",
      "Validation Batch 12, Loss: 0.922719, Accuracy: 81.64%\n",
      "Validation Batch 13, Loss: 0.916230, Accuracy: 81.61%\n",
      "Validation Batch 14, Loss: 0.949457, Accuracy: 81.58%\n",
      "Validation Batch 15, Loss: 0.913676, Accuracy: 81.67%\n",
      "Validation Batch 16, Loss: 0.928488, Accuracy: 81.64%\n",
      "Validation Batch 17, Loss: 0.973197, Accuracy: 81.34%\n",
      "Validation Batch 18, Loss: 0.883417, Accuracy: 81.68%\n",
      "Validation Batch 19, Loss: 0.949616, Accuracy: 81.66%\n",
      "Validation Batch 20, Loss: 0.999233, Accuracy: 81.33%\n",
      "Validation Batch 21, Loss: 0.987182, Accuracy: 80.88%\n",
      "Validation Batch 22, Loss: 0.927387, Accuracy: 80.97%\n",
      "Validation Batch 23, Loss: 0.956035, Accuracy: 80.91%\n",
      "Validation Batch 24, Loss: 0.930065, Accuracy: 80.92%\n",
      "Validation Batch 25, Loss: 0.900873, Accuracy: 81.12%\n",
      "Validation Batch 26, Loss: 0.928720, Accuracy: 81.19%\n",
      "Validation Batch 27, Loss: 0.942053, Accuracy: 81.15%\n",
      "Validation - Epoch 52, Loss: 0.936402, Accuracy: 81.15%\n",
      "Patience—0\n",
      "Epoch 53\n",
      "Batch 1, Loss: 0.892095, Accuracy: 84.38%\n",
      "Batch 2, Loss: 0.973267, Accuracy: 80.47%\n",
      "Batch 3, Loss: 0.953651, Accuracy: 79.17%\n",
      "Batch 4, Loss: 0.851525, Accuracy: 82.03%\n",
      "Batch 5, Loss: 0.892287, Accuracy: 82.50%\n",
      "Batch 6, Loss: 0.919928, Accuracy: 82.29%\n",
      "Batch 7, Loss: 0.929214, Accuracy: 82.14%\n",
      "Batch 8, Loss: 0.948136, Accuracy: 81.84%\n",
      "Batch 9, Loss: 0.944091, Accuracy: 81.60%\n",
      "Batch 10, Loss: 0.885747, Accuracy: 82.03%\n",
      "Batch 11, Loss: 0.929118, Accuracy: 81.82%\n",
      "Batch 12, Loss: 0.942245, Accuracy: 81.64%\n",
      "Batch 13, Loss: 0.900418, Accuracy: 81.85%\n",
      "Batch 14, Loss: 0.957399, Accuracy: 81.58%\n",
      "Batch 15, Loss: 0.949692, Accuracy: 81.56%\n",
      "Batch 16, Loss: 0.979216, Accuracy: 81.25%\n",
      "Batch 17, Loss: 0.960822, Accuracy: 81.16%\n",
      "Batch 18, Loss: 0.928200, Accuracy: 81.16%\n",
      "Batch 19, Loss: 0.861095, Accuracy: 81.58%\n",
      "Batch 20, Loss: 0.863149, Accuracy: 82.11%\n",
      "Batch 21, Loss: 0.888413, Accuracy: 82.29%\n",
      "Batch 22, Loss: 0.882834, Accuracy: 82.53%\n",
      "Batch 23, Loss: 0.850150, Accuracy: 82.88%\n",
      "Batch 24, Loss: 0.940374, Accuracy: 82.81%\n",
      "Batch 25, Loss: 0.904756, Accuracy: 82.81%\n",
      "Batch 26, Loss: 0.942375, Accuracy: 82.69%\n",
      "Batch 27, Loss: 0.938676, Accuracy: 82.52%\n",
      "Batch 28, Loss: 0.893278, Accuracy: 82.65%\n",
      "Batch 29, Loss: 0.884660, Accuracy: 82.76%\n",
      "Batch 30, Loss: 0.917187, Accuracy: 82.76%\n",
      "Batch 31, Loss: 0.965929, Accuracy: 82.61%\n",
      "Batch 32, Loss: 0.873224, Accuracy: 82.76%\n",
      "Batch 33, Loss: 0.917803, Accuracy: 82.77%\n",
      "Batch 34, Loss: 0.908683, Accuracy: 82.77%\n",
      "Batch 35, Loss: 0.901807, Accuracy: 82.77%\n",
      "Batch 36, Loss: 0.956749, Accuracy: 82.68%\n",
      "Batch 37, Loss: 0.846676, Accuracy: 82.90%\n",
      "Batch 38, Loss: 0.853415, Accuracy: 83.10%\n",
      "Batch 39, Loss: 0.915913, Accuracy: 83.09%\n",
      "Batch 40, Loss: 0.901271, Accuracy: 83.09%\n",
      "Batch 41, Loss: 0.906633, Accuracy: 83.12%\n",
      "Batch 42, Loss: 0.901271, Accuracy: 83.15%\n",
      "Batch 43, Loss: 0.886686, Accuracy: 83.21%\n",
      "Batch 44, Loss: 0.893360, Accuracy: 83.24%\n",
      "Batch 45, Loss: 0.938121, Accuracy: 83.23%\n",
      "Batch 46, Loss: 0.955402, Accuracy: 83.15%\n",
      "Batch 47, Loss: 0.878035, Accuracy: 83.24%\n",
      "Batch 48, Loss: 0.880301, Accuracy: 83.30%\n",
      "Batch 49, Loss: 0.934837, Accuracy: 83.29%\n",
      "Batch 50, Loss: 1.014920, Accuracy: 83.06%\n",
      "Batch 51, Loss: 0.888938, Accuracy: 83.15%\n",
      "Batch 52, Loss: 0.915986, Accuracy: 83.17%\n",
      "Batch 53, Loss: 0.911914, Accuracy: 83.20%\n",
      "Batch 54, Loss: 0.865796, Accuracy: 83.30%\n",
      "Batch 55, Loss: 0.883857, Accuracy: 83.38%\n",
      "Batch 56, Loss: 0.882365, Accuracy: 83.48%\n",
      "Batch 57, Loss: 0.932967, Accuracy: 83.47%\n",
      "Batch 58, Loss: 0.938638, Accuracy: 83.49%\n",
      "Batch 59, Loss: 0.907489, Accuracy: 83.50%\n",
      "Batch 60, Loss: 0.977928, Accuracy: 83.41%\n",
      "Batch 61, Loss: 0.933330, Accuracy: 83.38%\n",
      "Batch 62, Loss: 0.930144, Accuracy: 83.32%\n",
      "Batch 63, Loss: 0.874379, Accuracy: 83.38%\n",
      "Batch 64, Loss: 0.959950, Accuracy: 83.28%\n",
      "Batch 65, Loss: 0.824507, Accuracy: 83.41%\n",
      "Batch 66, Loss: 0.948927, Accuracy: 83.36%\n",
      "Batch 67, Loss: 0.940181, Accuracy: 83.30%\n",
      "Batch 68, Loss: 0.893237, Accuracy: 83.34%\n",
      "Batch 69, Loss: 0.879541, Accuracy: 83.42%\n",
      "Batch 70, Loss: 0.876163, Accuracy: 83.48%\n",
      "Batch 71, Loss: 0.864853, Accuracy: 83.54%\n",
      "Batch 72, Loss: 0.867766, Accuracy: 83.59%\n",
      "Batch 73, Loss: 0.948673, Accuracy: 83.52%\n",
      "Batch 74, Loss: 0.936961, Accuracy: 83.47%\n",
      "Batch 75, Loss: 0.904519, Accuracy: 83.46%\n",
      "Batch 76, Loss: 0.876386, Accuracy: 83.49%\n",
      "Batch 77, Loss: 0.849066, Accuracy: 83.60%\n",
      "Batch 78, Loss: 0.952211, Accuracy: 83.51%\n",
      "Batch 79, Loss: 0.907528, Accuracy: 83.48%\n",
      "Batch 80, Loss: 0.966502, Accuracy: 83.42%\n",
      "Batch 81, Loss: 0.958691, Accuracy: 83.37%\n",
      "Batch 82, Loss: 0.933906, Accuracy: 83.35%\n",
      "Batch 83, Loss: 0.858093, Accuracy: 83.41%\n",
      "Batch 84, Loss: 0.873440, Accuracy: 83.44%\n",
      "Batch 85, Loss: 0.901341, Accuracy: 83.46%\n",
      "Batch 86, Loss: 0.905771, Accuracy: 83.45%\n",
      "Batch 87, Loss: 0.932861, Accuracy: 83.42%\n",
      "Batch 88, Loss: 0.887739, Accuracy: 83.45%\n",
      "Batch 89, Loss: 0.918500, Accuracy: 83.44%\n",
      "Batch 90, Loss: 0.890240, Accuracy: 83.45%\n",
      "Batch 91, Loss: 0.857846, Accuracy: 83.50%\n",
      "Batch 92, Loss: 0.863378, Accuracy: 83.54%\n",
      "Batch 93, Loss: 0.910481, Accuracy: 83.55%\n",
      "Batch 94, Loss: 0.944580, Accuracy: 83.53%\n",
      "Batch 95, Loss: 0.920719, Accuracy: 83.49%\n",
      "Batch 96, Loss: 0.904924, Accuracy: 83.50%\n",
      "Batch 97, Loss: 0.981653, Accuracy: 83.42%\n",
      "Batch 98, Loss: 0.922710, Accuracy: 83.42%\n",
      "Batch 99, Loss: 0.967053, Accuracy: 83.36%\n",
      "Batch 100, Loss: 0.940607, Accuracy: 83.33%\n",
      "Batch 101, Loss: 0.912050, Accuracy: 83.32%\n",
      "Batch 102, Loss: 0.848296, Accuracy: 83.41%\n",
      "Batch 103, Loss: 0.950545, Accuracy: 83.37%\n",
      "Batch 104, Loss: 0.977718, Accuracy: 83.29%\n",
      "Batch 105, Loss: 0.839691, Accuracy: 83.36%\n",
      "Batch 106, Loss: 0.935662, Accuracy: 83.34%\n",
      "Batch 107, Loss: 0.911450, Accuracy: 83.34%\n",
      "Batch 108, Loss: 0.924208, Accuracy: 83.32%\n",
      "Batch 109, Loss: 0.887984, Accuracy: 83.31%\n",
      "Batch 110, Loss: 0.895272, Accuracy: 83.32%\n",
      "Batch 111, Loss: 1.001035, Accuracy: 83.25%\n",
      "Batch 112, Loss: 0.919822, Accuracy: 83.26%\n",
      "Batch 113, Loss: 0.904205, Accuracy: 83.27%\n",
      "Batch 114, Loss: 0.910883, Accuracy: 83.26%\n",
      "Batch 115, Loss: 0.884649, Accuracy: 83.29%\n",
      "Batch 116, Loss: 0.922419, Accuracy: 83.27%\n",
      "Batch 117, Loss: 0.968096, Accuracy: 83.23%\n",
      "Batch 118, Loss: 0.934009, Accuracy: 83.21%\n",
      "Batch 119, Loss: 0.954456, Accuracy: 83.17%\n",
      "Batch 120, Loss: 0.923405, Accuracy: 83.16%\n",
      "Batch 121, Loss: 0.951887, Accuracy: 83.14%\n",
      "Batch 122, Loss: 0.880695, Accuracy: 83.17%\n",
      "Batch 123, Loss: 0.963848, Accuracy: 83.13%\n",
      "Batch 124, Loss: 0.952457, Accuracy: 83.11%\n",
      "Batch 125, Loss: 0.939346, Accuracy: 83.10%\n",
      "Batch 126, Loss: 0.980945, Accuracy: 83.05%\n",
      "Batch 127, Loss: 0.940133, Accuracy: 83.03%\n",
      "Batch 128, Loss: 0.880411, Accuracy: 83.06%\n",
      "Batch 129, Loss: 0.965553, Accuracy: 83.02%\n",
      "Batch 130, Loss: 0.859140, Accuracy: 83.08%\n",
      "Batch 131, Loss: 0.907979, Accuracy: 83.09%\n",
      "Batch 132, Loss: 0.924201, Accuracy: 83.07%\n",
      "Batch 133, Loss: 0.891540, Accuracy: 83.08%\n",
      "Batch 134, Loss: 0.969763, Accuracy: 83.02%\n",
      "Batch 135, Loss: 0.877388, Accuracy: 83.04%\n",
      "Batch 136, Loss: 0.961590, Accuracy: 83.01%\n",
      "Batch 137, Loss: 0.892102, Accuracy: 83.02%\n",
      "Batch 138, Loss: 0.879926, Accuracy: 83.05%\n",
      "Batch 139, Loss: 0.886176, Accuracy: 83.08%\n",
      "Batch 140, Loss: 0.910630, Accuracy: 83.08%\n",
      "Batch 141, Loss: 0.916608, Accuracy: 83.07%\n",
      "Batch 142, Loss: 0.883296, Accuracy: 83.09%\n",
      "Batch 143, Loss: 0.945609, Accuracy: 83.07%\n",
      "Batch 144, Loss: 0.855366, Accuracy: 83.13%\n",
      "Batch 145, Loss: 0.900479, Accuracy: 83.14%\n",
      "Batch 146, Loss: 0.883098, Accuracy: 83.14%\n",
      "Batch 147, Loss: 0.924903, Accuracy: 83.14%\n",
      "Batch 148, Loss: 0.876179, Accuracy: 83.18%\n",
      "Batch 149, Loss: 0.925927, Accuracy: 83.18%\n",
      "Batch 150, Loss: 0.918397, Accuracy: 83.19%\n",
      "Batch 151, Loss: 0.992601, Accuracy: 83.13%\n",
      "Batch 152, Loss: 1.001159, Accuracy: 83.06%\n",
      "Batch 153, Loss: 0.874495, Accuracy: 83.09%\n",
      "Batch 154, Loss: 0.939104, Accuracy: 83.08%\n",
      "Batch 155, Loss: 0.826050, Accuracy: 83.12%\n",
      "Batch 156, Loss: 0.884590, Accuracy: 83.14%\n",
      "Batch 157, Loss: 0.845957, Accuracy: 83.20%\n",
      "Batch 158, Loss: 0.848325, Accuracy: 83.26%\n",
      "Batch 159, Loss: 0.993490, Accuracy: 83.20%\n",
      "Batch 160, Loss: 0.907192, Accuracy: 83.20%\n",
      "Batch 161, Loss: 0.897968, Accuracy: 83.20%\n",
      "Batch 162, Loss: 0.918074, Accuracy: 83.19%\n",
      "Batch 163, Loss: 0.969114, Accuracy: 83.13%\n",
      "Batch 164, Loss: 0.895452, Accuracy: 83.14%\n",
      "Batch 165, Loss: 0.939821, Accuracy: 83.12%\n",
      "Batch 166, Loss: 0.867775, Accuracy: 83.16%\n",
      "Batch 167, Loss: 0.925185, Accuracy: 83.15%\n",
      "Batch 168, Loss: 0.898293, Accuracy: 83.16%\n",
      "Batch 169, Loss: 0.959758, Accuracy: 83.13%\n",
      "Batch 170, Loss: 0.918684, Accuracy: 83.12%\n",
      "Batch 171, Loss: 0.972498, Accuracy: 83.09%\n",
      "Batch 172, Loss: 0.875208, Accuracy: 83.11%\n",
      "Batch 173, Loss: 0.947676, Accuracy: 83.11%\n",
      "Batch 174, Loss: 0.874143, Accuracy: 83.14%\n",
      "Batch 175, Loss: 0.919426, Accuracy: 83.14%\n",
      "Batch 176, Loss: 0.841692, Accuracy: 83.18%\n",
      "Batch 177, Loss: 0.902999, Accuracy: 83.19%\n",
      "Batch 178, Loss: 0.945619, Accuracy: 83.17%\n",
      "Batch 179, Loss: 0.955404, Accuracy: 83.14%\n",
      "Batch 180, Loss: 0.921223, Accuracy: 83.15%\n",
      "Batch 181, Loss: 0.918286, Accuracy: 83.16%\n",
      "Batch 182, Loss: 0.869023, Accuracy: 83.19%\n",
      "Batch 183, Loss: 0.949721, Accuracy: 83.16%\n",
      "Batch 184, Loss: 0.917071, Accuracy: 83.17%\n",
      "Batch 185, Loss: 0.912567, Accuracy: 83.17%\n",
      "Batch 186, Loss: 0.809341, Accuracy: 83.22%\n",
      "Batch 187, Loss: 0.849083, Accuracy: 83.26%\n",
      "Batch 188, Loss: 0.874593, Accuracy: 83.29%\n",
      "Batch 189, Loss: 0.845908, Accuracy: 83.33%\n",
      "Batch 190, Loss: 0.990471, Accuracy: 83.29%\n",
      "Batch 191, Loss: 0.940900, Accuracy: 83.26%\n",
      "Batch 192, Loss: 0.895486, Accuracy: 83.26%\n",
      "Batch 193, Loss: 0.904500, Accuracy: 83.27%\n",
      "Batch 194, Loss: 0.876066, Accuracy: 83.30%\n",
      "Batch 195, Loss: 0.944221, Accuracy: 83.29%\n",
      "Batch 196, Loss: 0.864114, Accuracy: 83.32%\n",
      "Batch 197, Loss: 0.869290, Accuracy: 83.34%\n",
      "Batch 198, Loss: 1.005708, Accuracy: 83.29%\n",
      "Batch 199, Loss: 0.930062, Accuracy: 83.29%\n",
      "Batch 200, Loss: 0.858925, Accuracy: 83.33%\n",
      "Batch 201, Loss: 0.894102, Accuracy: 83.34%\n",
      "Batch 202, Loss: 0.915446, Accuracy: 83.34%\n",
      "Batch 203, Loss: 0.976579, Accuracy: 83.31%\n",
      "Batch 204, Loss: 0.930039, Accuracy: 83.30%\n",
      "Batch 205, Loss: 0.891560, Accuracy: 83.32%\n",
      "Batch 206, Loss: 0.889401, Accuracy: 83.33%\n",
      "Batch 207, Loss: 0.931492, Accuracy: 83.33%\n",
      "Batch 208, Loss: 0.864342, Accuracy: 83.36%\n",
      "Batch 209, Loss: 0.940896, Accuracy: 83.34%\n",
      "Batch 210, Loss: 0.887283, Accuracy: 83.35%\n",
      "Batch 211, Loss: 0.954562, Accuracy: 83.32%\n",
      "Batch 212, Loss: 0.969970, Accuracy: 83.31%\n",
      "Batch 213, Loss: 0.899510, Accuracy: 83.31%\n",
      "Training - Epoch 53, Loss: 0.913292, Accuracy: 83.31%\n",
      "Validation Batch 1, Loss: 0.909808, Accuracy: 85.94%\n",
      "Validation Batch 2, Loss: 0.920085, Accuracy: 83.59%\n",
      "Validation Batch 3, Loss: 0.990385, Accuracy: 80.73%\n",
      "Validation Batch 4, Loss: 0.922639, Accuracy: 80.86%\n",
      "Validation Batch 5, Loss: 0.893808, Accuracy: 82.50%\n",
      "Validation Batch 6, Loss: 0.881910, Accuracy: 83.07%\n",
      "Validation Batch 7, Loss: 0.898913, Accuracy: 83.26%\n",
      "Validation Batch 8, Loss: 0.994536, Accuracy: 82.03%\n",
      "Validation Batch 9, Loss: 0.958982, Accuracy: 81.77%\n",
      "Validation Batch 10, Loss: 0.949830, Accuracy: 81.41%\n",
      "Validation Batch 11, Loss: 0.938516, Accuracy: 81.39%\n",
      "Validation Batch 12, Loss: 0.925014, Accuracy: 81.38%\n",
      "Validation Batch 13, Loss: 0.917709, Accuracy: 81.25%\n",
      "Validation Batch 14, Loss: 0.952552, Accuracy: 81.25%\n",
      "Validation Batch 15, Loss: 0.913289, Accuracy: 81.35%\n",
      "Validation Batch 16, Loss: 0.936109, Accuracy: 81.15%\n",
      "Validation Batch 17, Loss: 0.976714, Accuracy: 80.97%\n",
      "Validation Batch 18, Loss: 0.893028, Accuracy: 81.25%\n",
      "Validation Batch 19, Loss: 0.949245, Accuracy: 81.17%\n",
      "Validation Batch 20, Loss: 0.997868, Accuracy: 80.86%\n",
      "Validation Batch 21, Loss: 0.981808, Accuracy: 80.65%\n",
      "Validation Batch 22, Loss: 0.931295, Accuracy: 80.75%\n",
      "Validation Batch 23, Loss: 0.963117, Accuracy: 80.64%\n",
      "Validation Batch 24, Loss: 0.941231, Accuracy: 80.66%\n",
      "Validation Batch 25, Loss: 0.905070, Accuracy: 80.75%\n",
      "Validation Batch 26, Loss: 0.922682, Accuracy: 80.83%\n",
      "Validation Batch 27, Loss: 0.941291, Accuracy: 80.74%\n",
      "Validation - Epoch 53, Loss: 0.937312, Accuracy: 80.74%\n",
      "Patience—1\n",
      "Epoch 54\n",
      "Batch 1, Loss: 0.922306, Accuracy: 81.25%\n",
      "Batch 2, Loss: 0.918425, Accuracy: 81.25%\n",
      "Batch 3, Loss: 0.868099, Accuracy: 83.85%\n",
      "Batch 4, Loss: 0.892255, Accuracy: 83.98%\n",
      "Batch 5, Loss: 0.901410, Accuracy: 83.75%\n",
      "Batch 6, Loss: 0.862727, Accuracy: 84.64%\n",
      "Batch 7, Loss: 0.974315, Accuracy: 83.48%\n",
      "Batch 8, Loss: 0.831634, Accuracy: 84.57%\n",
      "Batch 9, Loss: 0.892198, Accuracy: 84.72%\n",
      "Batch 10, Loss: 0.890180, Accuracy: 84.69%\n",
      "Batch 11, Loss: 0.926379, Accuracy: 84.23%\n",
      "Batch 12, Loss: 0.900593, Accuracy: 84.24%\n",
      "Batch 13, Loss: 0.925180, Accuracy: 84.01%\n",
      "Batch 14, Loss: 0.894691, Accuracy: 84.15%\n",
      "Batch 15, Loss: 0.938180, Accuracy: 83.96%\n",
      "Batch 16, Loss: 0.939843, Accuracy: 83.79%\n",
      "Batch 17, Loss: 0.933082, Accuracy: 83.55%\n",
      "Batch 18, Loss: 0.907197, Accuracy: 83.51%\n",
      "Batch 19, Loss: 0.917366, Accuracy: 83.39%\n",
      "Batch 20, Loss: 0.911062, Accuracy: 83.44%\n",
      "Batch 21, Loss: 0.852727, Accuracy: 83.78%\n",
      "Batch 22, Loss: 0.843307, Accuracy: 84.09%\n",
      "Batch 23, Loss: 0.922671, Accuracy: 84.10%\n",
      "Batch 24, Loss: 0.841263, Accuracy: 84.44%\n",
      "Batch 25, Loss: 0.887314, Accuracy: 84.44%\n",
      "Batch 26, Loss: 0.929170, Accuracy: 84.25%\n",
      "Batch 27, Loss: 0.871196, Accuracy: 84.38%\n",
      "Batch 28, Loss: 0.925533, Accuracy: 84.21%\n",
      "Batch 29, Loss: 0.853430, Accuracy: 84.43%\n",
      "Batch 30, Loss: 0.933700, Accuracy: 84.27%\n",
      "Batch 31, Loss: 0.887711, Accuracy: 84.32%\n",
      "Batch 32, Loss: 0.919619, Accuracy: 84.23%\n",
      "Batch 33, Loss: 0.939115, Accuracy: 84.14%\n",
      "Batch 34, Loss: 0.923823, Accuracy: 84.10%\n",
      "Batch 35, Loss: 0.891989, Accuracy: 84.15%\n",
      "Batch 36, Loss: 0.897407, Accuracy: 84.24%\n",
      "Batch 37, Loss: 0.847796, Accuracy: 84.42%\n",
      "Batch 38, Loss: 0.962539, Accuracy: 84.21%\n",
      "Batch 39, Loss: 0.863992, Accuracy: 84.29%\n",
      "Batch 40, Loss: 0.943885, Accuracy: 84.22%\n",
      "Batch 41, Loss: 0.989185, Accuracy: 84.03%\n",
      "Batch 42, Loss: 0.949971, Accuracy: 83.97%\n",
      "Batch 43, Loss: 0.911674, Accuracy: 83.98%\n",
      "Batch 44, Loss: 0.876135, Accuracy: 84.02%\n",
      "Batch 45, Loss: 0.884098, Accuracy: 84.06%\n",
      "Batch 46, Loss: 0.879197, Accuracy: 84.14%\n",
      "Batch 47, Loss: 0.950358, Accuracy: 84.04%\n",
      "Batch 48, Loss: 0.849755, Accuracy: 84.15%\n",
      "Batch 49, Loss: 0.950541, Accuracy: 84.06%\n",
      "Batch 50, Loss: 0.852313, Accuracy: 84.19%\n",
      "Batch 51, Loss: 0.891853, Accuracy: 84.19%\n",
      "Batch 52, Loss: 0.924368, Accuracy: 84.16%\n",
      "Batch 53, Loss: 0.860495, Accuracy: 84.26%\n",
      "Batch 54, Loss: 0.870892, Accuracy: 84.29%\n",
      "Batch 55, Loss: 0.908825, Accuracy: 84.29%\n",
      "Batch 56, Loss: 0.855415, Accuracy: 84.38%\n",
      "Batch 57, Loss: 0.844695, Accuracy: 84.48%\n",
      "Batch 58, Loss: 0.891681, Accuracy: 84.48%\n",
      "Batch 59, Loss: 0.827307, Accuracy: 84.64%\n",
      "Batch 60, Loss: 0.931942, Accuracy: 84.58%\n",
      "Batch 61, Loss: 0.883458, Accuracy: 84.61%\n",
      "Batch 62, Loss: 0.933640, Accuracy: 84.55%\n",
      "Batch 63, Loss: 0.935256, Accuracy: 84.47%\n",
      "Batch 64, Loss: 0.912795, Accuracy: 84.45%\n",
      "Batch 65, Loss: 0.927535, Accuracy: 84.40%\n",
      "Batch 66, Loss: 0.888631, Accuracy: 84.42%\n",
      "Batch 67, Loss: 0.979191, Accuracy: 84.31%\n",
      "Batch 68, Loss: 0.912485, Accuracy: 84.26%\n",
      "Batch 69, Loss: 0.875308, Accuracy: 84.28%\n",
      "Batch 70, Loss: 0.912019, Accuracy: 84.29%\n",
      "Batch 71, Loss: 0.955844, Accuracy: 84.22%\n",
      "Batch 72, Loss: 0.898694, Accuracy: 84.16%\n",
      "Batch 73, Loss: 0.916774, Accuracy: 84.14%\n",
      "Batch 74, Loss: 0.939540, Accuracy: 84.12%\n",
      "Batch 75, Loss: 0.860013, Accuracy: 84.19%\n",
      "Batch 76, Loss: 0.837424, Accuracy: 84.27%\n",
      "Batch 77, Loss: 0.993585, Accuracy: 84.11%\n",
      "Batch 78, Loss: 0.941510, Accuracy: 84.07%\n",
      "Batch 79, Loss: 0.905582, Accuracy: 84.06%\n",
      "Batch 80, Loss: 0.876080, Accuracy: 84.08%\n",
      "Batch 81, Loss: 0.920131, Accuracy: 84.05%\n",
      "Batch 82, Loss: 0.885650, Accuracy: 84.05%\n",
      "Batch 83, Loss: 0.866506, Accuracy: 84.07%\n",
      "Batch 84, Loss: 0.919177, Accuracy: 84.06%\n",
      "Batch 85, Loss: 0.874801, Accuracy: 84.10%\n",
      "Batch 86, Loss: 0.914965, Accuracy: 84.12%\n",
      "Batch 87, Loss: 0.985387, Accuracy: 84.00%\n",
      "Batch 88, Loss: 0.880089, Accuracy: 84.02%\n",
      "Batch 89, Loss: 0.974492, Accuracy: 83.92%\n",
      "Batch 90, Loss: 0.861870, Accuracy: 83.96%\n",
      "Batch 91, Loss: 0.880885, Accuracy: 83.98%\n",
      "Batch 92, Loss: 0.830676, Accuracy: 84.05%\n",
      "Batch 93, Loss: 0.895201, Accuracy: 84.07%\n",
      "Batch 94, Loss: 0.931640, Accuracy: 84.03%\n",
      "Batch 95, Loss: 0.902111, Accuracy: 84.05%\n",
      "Batch 96, Loss: 0.940570, Accuracy: 84.00%\n",
      "Batch 97, Loss: 0.935736, Accuracy: 83.96%\n",
      "Batch 98, Loss: 0.943123, Accuracy: 83.90%\n",
      "Batch 99, Loss: 0.890218, Accuracy: 83.92%\n",
      "Batch 100, Loss: 0.869792, Accuracy: 83.94%\n",
      "Batch 101, Loss: 0.877517, Accuracy: 83.97%\n",
      "Batch 102, Loss: 0.894459, Accuracy: 83.98%\n",
      "Batch 103, Loss: 0.861902, Accuracy: 84.04%\n",
      "Batch 104, Loss: 0.899290, Accuracy: 84.06%\n",
      "Batch 105, Loss: 0.867751, Accuracy: 84.11%\n",
      "Batch 106, Loss: 0.890915, Accuracy: 84.12%\n",
      "Batch 107, Loss: 0.910423, Accuracy: 84.11%\n",
      "Batch 108, Loss: 0.855018, Accuracy: 84.16%\n",
      "Batch 109, Loss: 0.981569, Accuracy: 84.09%\n",
      "Batch 110, Loss: 0.953166, Accuracy: 84.05%\n",
      "Batch 111, Loss: 0.900342, Accuracy: 84.04%\n",
      "Batch 112, Loss: 0.900856, Accuracy: 84.04%\n",
      "Batch 113, Loss: 0.838821, Accuracy: 84.10%\n",
      "Batch 114, Loss: 0.884123, Accuracy: 84.11%\n",
      "Batch 115, Loss: 0.927948, Accuracy: 84.08%\n",
      "Batch 116, Loss: 0.990142, Accuracy: 83.98%\n",
      "Batch 117, Loss: 0.932016, Accuracy: 83.95%\n",
      "Batch 118, Loss: 0.923684, Accuracy: 83.94%\n",
      "Batch 119, Loss: 0.906156, Accuracy: 83.95%\n",
      "Batch 120, Loss: 0.923801, Accuracy: 83.95%\n",
      "Batch 121, Loss: 0.943450, Accuracy: 83.91%\n",
      "Batch 122, Loss: 0.875034, Accuracy: 83.94%\n",
      "Batch 123, Loss: 0.880821, Accuracy: 83.96%\n",
      "Batch 124, Loss: 0.921538, Accuracy: 83.95%\n",
      "Batch 125, Loss: 0.914422, Accuracy: 83.92%\n",
      "Batch 126, Loss: 0.961125, Accuracy: 83.87%\n",
      "Batch 127, Loss: 0.988448, Accuracy: 83.78%\n",
      "Batch 128, Loss: 0.842680, Accuracy: 83.84%\n",
      "Batch 129, Loss: 0.967061, Accuracy: 83.79%\n",
      "Batch 130, Loss: 0.844086, Accuracy: 83.86%\n",
      "Batch 131, Loss: 0.897389, Accuracy: 83.85%\n",
      "Batch 132, Loss: 1.010448, Accuracy: 83.77%\n",
      "Batch 133, Loss: 0.924028, Accuracy: 83.76%\n",
      "Batch 134, Loss: 0.954070, Accuracy: 83.73%\n",
      "Batch 135, Loss: 0.886528, Accuracy: 83.76%\n",
      "Batch 136, Loss: 0.868792, Accuracy: 83.80%\n",
      "Batch 137, Loss: 0.919070, Accuracy: 83.78%\n",
      "Batch 138, Loss: 1.015323, Accuracy: 83.70%\n",
      "Batch 139, Loss: 0.965892, Accuracy: 83.66%\n",
      "Batch 140, Loss: 1.028885, Accuracy: 83.57%\n",
      "Batch 141, Loss: 0.820510, Accuracy: 83.63%\n",
      "Batch 142, Loss: 0.956886, Accuracy: 83.60%\n",
      "Batch 143, Loss: 0.873590, Accuracy: 83.63%\n",
      "Batch 144, Loss: 0.940701, Accuracy: 83.60%\n",
      "Batch 145, Loss: 0.894181, Accuracy: 83.62%\n",
      "Batch 146, Loss: 0.936928, Accuracy: 83.60%\n",
      "Batch 147, Loss: 0.925563, Accuracy: 83.60%\n",
      "Batch 148, Loss: 0.925007, Accuracy: 83.58%\n",
      "Batch 149, Loss: 0.886143, Accuracy: 83.60%\n",
      "Batch 150, Loss: 0.921057, Accuracy: 83.59%\n",
      "Batch 151, Loss: 0.983976, Accuracy: 83.53%\n",
      "Batch 152, Loss: 0.992928, Accuracy: 83.47%\n",
      "Batch 153, Loss: 0.957379, Accuracy: 83.44%\n",
      "Batch 154, Loss: 0.906062, Accuracy: 83.42%\n",
      "Batch 155, Loss: 0.897716, Accuracy: 83.43%\n",
      "Batch 156, Loss: 0.917508, Accuracy: 83.41%\n",
      "Batch 157, Loss: 0.841715, Accuracy: 83.46%\n",
      "Batch 158, Loss: 0.894292, Accuracy: 83.48%\n",
      "Batch 159, Loss: 0.963272, Accuracy: 83.44%\n",
      "Batch 160, Loss: 0.944688, Accuracy: 83.41%\n",
      "Batch 161, Loss: 0.881108, Accuracy: 83.42%\n",
      "Batch 162, Loss: 0.970924, Accuracy: 83.38%\n",
      "Batch 163, Loss: 0.914381, Accuracy: 83.37%\n",
      "Batch 164, Loss: 0.885289, Accuracy: 83.39%\n",
      "Batch 165, Loss: 0.866056, Accuracy: 83.41%\n",
      "Batch 166, Loss: 0.876864, Accuracy: 83.42%\n",
      "Batch 167, Loss: 0.909389, Accuracy: 83.43%\n",
      "Batch 168, Loss: 0.990672, Accuracy: 83.38%\n",
      "Batch 169, Loss: 0.930525, Accuracy: 83.37%\n",
      "Batch 170, Loss: 0.901068, Accuracy: 83.37%\n",
      "Batch 171, Loss: 0.925694, Accuracy: 83.36%\n",
      "Batch 172, Loss: 0.978279, Accuracy: 83.31%\n",
      "Batch 173, Loss: 0.955780, Accuracy: 83.27%\n",
      "Batch 174, Loss: 0.968736, Accuracy: 83.23%\n",
      "Batch 175, Loss: 0.895213, Accuracy: 83.25%\n",
      "Batch 176, Loss: 0.887804, Accuracy: 83.27%\n",
      "Batch 177, Loss: 0.876258, Accuracy: 83.30%\n",
      "Batch 178, Loss: 1.040332, Accuracy: 83.22%\n",
      "Batch 179, Loss: 0.930511, Accuracy: 83.20%\n",
      "Batch 180, Loss: 0.935676, Accuracy: 83.19%\n",
      "Batch 181, Loss: 0.897473, Accuracy: 83.20%\n",
      "Batch 182, Loss: 0.892722, Accuracy: 83.21%\n",
      "Batch 183, Loss: 0.909707, Accuracy: 83.21%\n",
      "Batch 184, Loss: 0.960391, Accuracy: 83.19%\n",
      "Batch 185, Loss: 0.891427, Accuracy: 83.21%\n",
      "Batch 186, Loss: 0.902531, Accuracy: 83.21%\n",
      "Batch 187, Loss: 0.913132, Accuracy: 83.21%\n",
      "Batch 188, Loss: 0.881544, Accuracy: 83.22%\n",
      "Batch 189, Loss: 0.914576, Accuracy: 83.22%\n",
      "Batch 190, Loss: 0.905908, Accuracy: 83.22%\n",
      "Batch 191, Loss: 0.877800, Accuracy: 83.25%\n",
      "Batch 192, Loss: 0.893025, Accuracy: 83.25%\n",
      "Batch 193, Loss: 0.858537, Accuracy: 83.28%\n",
      "Batch 194, Loss: 0.943437, Accuracy: 83.27%\n",
      "Batch 195, Loss: 0.919130, Accuracy: 83.27%\n",
      "Batch 196, Loss: 0.872971, Accuracy: 83.30%\n",
      "Batch 197, Loss: 0.840968, Accuracy: 83.34%\n",
      "Batch 198, Loss: 0.953900, Accuracy: 83.31%\n",
      "Batch 199, Loss: 0.900346, Accuracy: 83.32%\n",
      "Batch 200, Loss: 0.979443, Accuracy: 83.28%\n",
      "Batch 201, Loss: 0.914430, Accuracy: 83.28%\n",
      "Batch 202, Loss: 0.822735, Accuracy: 83.32%\n",
      "Batch 203, Loss: 0.844432, Accuracy: 83.36%\n",
      "Batch 204, Loss: 0.898700, Accuracy: 83.36%\n",
      "Batch 205, Loss: 0.905391, Accuracy: 83.37%\n",
      "Batch 206, Loss: 0.860645, Accuracy: 83.40%\n",
      "Batch 207, Loss: 0.947437, Accuracy: 83.37%\n",
      "Batch 208, Loss: 0.911933, Accuracy: 83.38%\n",
      "Batch 209, Loss: 0.916835, Accuracy: 83.38%\n",
      "Batch 210, Loss: 0.944410, Accuracy: 83.36%\n",
      "Batch 211, Loss: 0.948011, Accuracy: 83.33%\n",
      "Batch 212, Loss: 0.913699, Accuracy: 83.34%\n",
      "Batch 213, Loss: 0.895074, Accuracy: 83.35%\n",
      "Training - Epoch 54, Loss: 0.910032, Accuracy: 83.35%\n",
      "Validation Batch 1, Loss: 0.904274, Accuracy: 85.94%\n",
      "Validation Batch 2, Loss: 0.911298, Accuracy: 84.38%\n",
      "Validation Batch 3, Loss: 0.988827, Accuracy: 82.29%\n",
      "Validation Batch 4, Loss: 0.914707, Accuracy: 82.03%\n",
      "Validation Batch 5, Loss: 0.896677, Accuracy: 83.12%\n",
      "Validation Batch 6, Loss: 0.880779, Accuracy: 83.59%\n",
      "Validation Batch 7, Loss: 0.902494, Accuracy: 83.93%\n",
      "Validation Batch 8, Loss: 0.992999, Accuracy: 82.81%\n",
      "Validation Batch 9, Loss: 0.949810, Accuracy: 82.99%\n",
      "Validation Batch 10, Loss: 0.948804, Accuracy: 82.66%\n",
      "Validation Batch 11, Loss: 0.928469, Accuracy: 82.39%\n",
      "Validation Batch 12, Loss: 0.922603, Accuracy: 82.16%\n",
      "Validation Batch 13, Loss: 0.920877, Accuracy: 82.09%\n",
      "Validation Batch 14, Loss: 0.954937, Accuracy: 82.03%\n",
      "Validation Batch 15, Loss: 0.909339, Accuracy: 82.19%\n",
      "Validation Batch 16, Loss: 0.933719, Accuracy: 82.13%\n",
      "Validation Batch 17, Loss: 0.985365, Accuracy: 81.71%\n",
      "Validation Batch 18, Loss: 0.884376, Accuracy: 82.12%\n",
      "Validation Batch 19, Loss: 0.949813, Accuracy: 82.07%\n",
      "Validation Batch 20, Loss: 0.998444, Accuracy: 81.56%\n",
      "Validation Batch 21, Loss: 0.981587, Accuracy: 81.25%\n",
      "Validation Batch 22, Loss: 0.932695, Accuracy: 81.32%\n",
      "Validation Batch 23, Loss: 0.972465, Accuracy: 81.11%\n",
      "Validation Batch 24, Loss: 0.940639, Accuracy: 81.12%\n",
      "Validation Batch 25, Loss: 0.909400, Accuracy: 81.25%\n",
      "Validation Batch 26, Loss: 0.921404, Accuracy: 81.31%\n",
      "Validation Batch 27, Loss: 0.951519, Accuracy: 81.21%\n",
      "Validation - Epoch 54, Loss: 0.936605, Accuracy: 81.21%\n",
      "Patience—2\n",
      "Epoch 55\n",
      "Batch 1, Loss: 0.880920, Accuracy: 87.50%\n",
      "Batch 2, Loss: 0.924755, Accuracy: 83.59%\n",
      "Batch 3, Loss: 0.844252, Accuracy: 85.94%\n",
      "Batch 4, Loss: 0.891484, Accuracy: 85.94%\n",
      "Batch 5, Loss: 0.894696, Accuracy: 85.94%\n",
      "Batch 6, Loss: 0.868927, Accuracy: 86.46%\n",
      "Batch 7, Loss: 0.898269, Accuracy: 85.94%\n",
      "Batch 8, Loss: 0.914548, Accuracy: 85.55%\n",
      "Batch 9, Loss: 0.903972, Accuracy: 85.42%\n",
      "Batch 10, Loss: 0.935777, Accuracy: 84.69%\n",
      "Batch 11, Loss: 0.877097, Accuracy: 84.94%\n",
      "Batch 12, Loss: 0.938643, Accuracy: 84.64%\n",
      "Batch 13, Loss: 0.891525, Accuracy: 84.62%\n",
      "Batch 14, Loss: 0.926035, Accuracy: 84.60%\n",
      "Batch 15, Loss: 0.950264, Accuracy: 84.17%\n",
      "Batch 16, Loss: 0.854316, Accuracy: 84.57%\n",
      "Batch 17, Loss: 0.930779, Accuracy: 84.38%\n",
      "Batch 18, Loss: 0.913987, Accuracy: 84.29%\n",
      "Batch 19, Loss: 0.850482, Accuracy: 84.54%\n",
      "Batch 20, Loss: 0.938723, Accuracy: 84.38%\n",
      "Batch 21, Loss: 0.863892, Accuracy: 84.67%\n",
      "Batch 22, Loss: 0.915350, Accuracy: 84.66%\n",
      "Batch 23, Loss: 0.930848, Accuracy: 84.58%\n",
      "Batch 24, Loss: 0.935681, Accuracy: 84.38%\n",
      "Batch 25, Loss: 0.895354, Accuracy: 84.44%\n",
      "Batch 26, Loss: 0.967788, Accuracy: 84.13%\n",
      "Batch 27, Loss: 0.820491, Accuracy: 84.55%\n",
      "Batch 28, Loss: 0.849995, Accuracy: 84.71%\n",
      "Batch 29, Loss: 0.829650, Accuracy: 85.02%\n",
      "Batch 30, Loss: 0.900132, Accuracy: 84.95%\n",
      "Batch 31, Loss: 0.992962, Accuracy: 84.58%\n",
      "Batch 32, Loss: 0.891581, Accuracy: 84.57%\n",
      "Batch 33, Loss: 0.930701, Accuracy: 84.47%\n",
      "Batch 34, Loss: 0.884158, Accuracy: 84.51%\n",
      "Batch 35, Loss: 0.954062, Accuracy: 84.38%\n",
      "Batch 36, Loss: 0.928716, Accuracy: 84.38%\n",
      "Batch 37, Loss: 0.846050, Accuracy: 84.54%\n",
      "Batch 38, Loss: 0.886699, Accuracy: 84.58%\n",
      "Batch 39, Loss: 0.869603, Accuracy: 84.70%\n",
      "Batch 40, Loss: 0.916455, Accuracy: 84.65%\n",
      "Batch 41, Loss: 0.955844, Accuracy: 84.53%\n",
      "Batch 42, Loss: 0.955465, Accuracy: 84.45%\n",
      "Batch 43, Loss: 0.900087, Accuracy: 84.45%\n",
      "Batch 44, Loss: 0.898221, Accuracy: 84.52%\n",
      "Batch 45, Loss: 0.849545, Accuracy: 84.58%\n",
      "Batch 46, Loss: 0.899595, Accuracy: 84.54%\n",
      "Batch 47, Loss: 0.907372, Accuracy: 84.54%\n",
      "Batch 48, Loss: 0.889165, Accuracy: 84.57%\n",
      "Batch 49, Loss: 0.956469, Accuracy: 84.44%\n",
      "Batch 50, Loss: 0.935369, Accuracy: 84.38%\n",
      "Batch 51, Loss: 0.886951, Accuracy: 84.44%\n",
      "Batch 52, Loss: 0.913727, Accuracy: 84.44%\n",
      "Batch 53, Loss: 0.951967, Accuracy: 84.32%\n",
      "Batch 54, Loss: 0.844301, Accuracy: 84.38%\n",
      "Batch 55, Loss: 0.987065, Accuracy: 84.18%\n",
      "Batch 56, Loss: 0.908576, Accuracy: 84.12%\n",
      "Batch 57, Loss: 0.954582, Accuracy: 84.02%\n",
      "Batch 58, Loss: 0.812173, Accuracy: 84.16%\n",
      "Batch 59, Loss: 0.912511, Accuracy: 84.14%\n",
      "Batch 60, Loss: 0.886939, Accuracy: 84.19%\n",
      "Batch 61, Loss: 0.901094, Accuracy: 84.20%\n",
      "Batch 62, Loss: 0.917149, Accuracy: 84.15%\n",
      "Batch 63, Loss: 0.898850, Accuracy: 84.20%\n",
      "Batch 64, Loss: 0.921777, Accuracy: 84.18%\n",
      "Batch 65, Loss: 0.930060, Accuracy: 84.16%\n",
      "Batch 66, Loss: 0.934176, Accuracy: 84.11%\n",
      "Batch 67, Loss: 0.873349, Accuracy: 84.21%\n",
      "Batch 68, Loss: 1.013423, Accuracy: 84.03%\n",
      "Batch 69, Loss: 0.835893, Accuracy: 84.15%\n",
      "Batch 70, Loss: 0.913987, Accuracy: 84.13%\n",
      "Batch 71, Loss: 0.918874, Accuracy: 84.11%\n",
      "Batch 72, Loss: 0.932641, Accuracy: 84.05%\n",
      "Batch 73, Loss: 0.868365, Accuracy: 84.10%\n",
      "Batch 74, Loss: 0.884508, Accuracy: 84.12%\n",
      "Batch 75, Loss: 0.863946, Accuracy: 84.17%\n",
      "Batch 76, Loss: 0.914362, Accuracy: 84.15%\n",
      "Batch 77, Loss: 0.840733, Accuracy: 84.25%\n",
      "Batch 78, Loss: 0.927265, Accuracy: 84.23%\n",
      "Batch 79, Loss: 0.885849, Accuracy: 84.26%\n",
      "Batch 80, Loss: 0.920794, Accuracy: 84.24%\n",
      "Batch 81, Loss: 0.896121, Accuracy: 84.24%\n",
      "Batch 82, Loss: 0.870372, Accuracy: 84.30%\n",
      "Batch 83, Loss: 0.881337, Accuracy: 84.32%\n",
      "Batch 84, Loss: 0.905319, Accuracy: 84.30%\n",
      "Batch 85, Loss: 0.854766, Accuracy: 84.36%\n",
      "Batch 86, Loss: 0.932890, Accuracy: 84.30%\n",
      "Batch 87, Loss: 0.943302, Accuracy: 84.23%\n",
      "Batch 88, Loss: 0.845744, Accuracy: 84.30%\n",
      "Batch 89, Loss: 0.921959, Accuracy: 84.29%\n",
      "Batch 90, Loss: 0.949635, Accuracy: 84.25%\n",
      "Batch 91, Loss: 0.933930, Accuracy: 84.22%\n",
      "Batch 92, Loss: 0.981892, Accuracy: 84.14%\n",
      "Batch 93, Loss: 0.923006, Accuracy: 84.11%\n",
      "Batch 94, Loss: 0.868005, Accuracy: 84.16%\n",
      "Batch 95, Loss: 0.955778, Accuracy: 84.08%\n",
      "Batch 96, Loss: 0.918300, Accuracy: 84.08%\n",
      "Batch 97, Loss: 0.900479, Accuracy: 84.10%\n",
      "Batch 98, Loss: 0.931081, Accuracy: 84.07%\n",
      "Batch 99, Loss: 0.912077, Accuracy: 84.08%\n",
      "Batch 100, Loss: 0.932334, Accuracy: 84.06%\n",
      "Batch 101, Loss: 0.903635, Accuracy: 84.07%\n",
      "Batch 102, Loss: 0.945660, Accuracy: 84.02%\n",
      "Batch 103, Loss: 0.831182, Accuracy: 84.10%\n",
      "Batch 104, Loss: 0.978992, Accuracy: 84.03%\n",
      "Batch 105, Loss: 0.847497, Accuracy: 84.09%\n",
      "Batch 106, Loss: 0.900163, Accuracy: 84.09%\n",
      "Batch 107, Loss: 0.898120, Accuracy: 84.10%\n",
      "Batch 108, Loss: 0.892962, Accuracy: 84.11%\n",
      "Batch 109, Loss: 0.892835, Accuracy: 84.10%\n",
      "Batch 110, Loss: 0.910062, Accuracy: 84.09%\n",
      "Batch 111, Loss: 0.960859, Accuracy: 84.02%\n",
      "Batch 112, Loss: 0.874481, Accuracy: 84.05%\n",
      "Batch 113, Loss: 0.870556, Accuracy: 84.10%\n",
      "Batch 114, Loss: 0.947490, Accuracy: 84.06%\n",
      "Batch 115, Loss: 0.985349, Accuracy: 83.98%\n",
      "Batch 116, Loss: 0.903890, Accuracy: 84.00%\n",
      "Batch 117, Loss: 0.910038, Accuracy: 83.99%\n",
      "Batch 118, Loss: 0.887378, Accuracy: 83.99%\n",
      "Batch 119, Loss: 0.985646, Accuracy: 83.94%\n",
      "Batch 120, Loss: 0.959990, Accuracy: 83.91%\n",
      "Batch 121, Loss: 0.947815, Accuracy: 83.87%\n",
      "Batch 122, Loss: 0.899936, Accuracy: 83.88%\n",
      "Batch 123, Loss: 0.919733, Accuracy: 83.87%\n",
      "Batch 124, Loss: 0.885500, Accuracy: 83.87%\n",
      "Batch 125, Loss: 0.852037, Accuracy: 83.92%\n",
      "Batch 126, Loss: 0.962279, Accuracy: 83.88%\n",
      "Batch 127, Loss: 0.902125, Accuracy: 83.88%\n",
      "Batch 128, Loss: 0.878138, Accuracy: 83.91%\n",
      "Batch 129, Loss: 0.883774, Accuracy: 83.93%\n",
      "Batch 130, Loss: 0.851896, Accuracy: 83.98%\n",
      "Batch 131, Loss: 0.855242, Accuracy: 84.03%\n",
      "Batch 132, Loss: 0.969161, Accuracy: 83.97%\n",
      "Batch 133, Loss: 0.855838, Accuracy: 84.01%\n",
      "Batch 134, Loss: 0.901305, Accuracy: 84.03%\n",
      "Batch 135, Loss: 0.865537, Accuracy: 84.06%\n",
      "Batch 136, Loss: 0.951696, Accuracy: 84.03%\n",
      "Batch 137, Loss: 0.998375, Accuracy: 83.96%\n",
      "Batch 138, Loss: 0.881133, Accuracy: 83.99%\n",
      "Batch 139, Loss: 0.885361, Accuracy: 84.00%\n",
      "Batch 140, Loss: 0.917485, Accuracy: 84.00%\n",
      "Batch 141, Loss: 0.938493, Accuracy: 83.96%\n",
      "Batch 142, Loss: 0.966279, Accuracy: 83.92%\n",
      "Batch 143, Loss: 0.892690, Accuracy: 83.94%\n",
      "Batch 144, Loss: 0.897659, Accuracy: 83.94%\n",
      "Batch 145, Loss: 0.950140, Accuracy: 83.92%\n",
      "Batch 146, Loss: 0.935042, Accuracy: 83.91%\n",
      "Batch 147, Loss: 0.945077, Accuracy: 83.89%\n",
      "Batch 148, Loss: 0.936666, Accuracy: 83.87%\n",
      "Batch 149, Loss: 0.877394, Accuracy: 83.88%\n",
      "Batch 150, Loss: 0.880432, Accuracy: 83.90%\n",
      "Batch 151, Loss: 0.860301, Accuracy: 83.93%\n",
      "Batch 152, Loss: 0.914625, Accuracy: 83.92%\n",
      "Batch 153, Loss: 0.882034, Accuracy: 83.95%\n",
      "Batch 154, Loss: 0.974701, Accuracy: 83.90%\n",
      "Batch 155, Loss: 0.830811, Accuracy: 83.96%\n",
      "Batch 156, Loss: 0.918551, Accuracy: 83.95%\n",
      "Batch 157, Loss: 0.904246, Accuracy: 83.95%\n",
      "Batch 158, Loss: 0.878108, Accuracy: 83.97%\n",
      "Batch 159, Loss: 0.914588, Accuracy: 83.96%\n",
      "Batch 160, Loss: 0.888978, Accuracy: 83.97%\n",
      "Batch 161, Loss: 0.962828, Accuracy: 83.95%\n",
      "Batch 162, Loss: 0.959363, Accuracy: 83.91%\n",
      "Batch 163, Loss: 0.925378, Accuracy: 83.90%\n",
      "Batch 164, Loss: 0.923492, Accuracy: 83.87%\n",
      "Batch 165, Loss: 0.896227, Accuracy: 83.88%\n",
      "Batch 166, Loss: 0.925683, Accuracy: 83.88%\n",
      "Batch 167, Loss: 0.854637, Accuracy: 83.92%\n",
      "Batch 168, Loss: 0.926792, Accuracy: 83.91%\n",
      "Batch 169, Loss: 0.921186, Accuracy: 83.90%\n",
      "Batch 170, Loss: 0.888092, Accuracy: 83.91%\n",
      "Batch 171, Loss: 0.879044, Accuracy: 83.93%\n",
      "Batch 172, Loss: 0.848393, Accuracy: 83.96%\n",
      "Batch 173, Loss: 0.916949, Accuracy: 83.96%\n",
      "Batch 174, Loss: 0.910028, Accuracy: 83.96%\n",
      "Batch 175, Loss: 0.884370, Accuracy: 83.97%\n",
      "Batch 176, Loss: 0.918798, Accuracy: 83.97%\n",
      "Batch 177, Loss: 0.895801, Accuracy: 83.98%\n",
      "Batch 178, Loss: 0.982827, Accuracy: 83.94%\n",
      "Batch 179, Loss: 0.857013, Accuracy: 83.96%\n",
      "Batch 180, Loss: 0.916252, Accuracy: 83.96%\n",
      "Batch 181, Loss: 0.861946, Accuracy: 83.99%\n",
      "Batch 182, Loss: 0.845880, Accuracy: 84.01%\n",
      "Batch 183, Loss: 0.866884, Accuracy: 84.04%\n",
      "Batch 184, Loss: 0.896331, Accuracy: 84.04%\n",
      "Batch 185, Loss: 0.902196, Accuracy: 84.05%\n",
      "Batch 186, Loss: 0.865674, Accuracy: 84.07%\n",
      "Batch 187, Loss: 0.974712, Accuracy: 84.03%\n",
      "Batch 188, Loss: 0.926704, Accuracy: 84.02%\n",
      "Batch 189, Loss: 0.877786, Accuracy: 84.04%\n",
      "Batch 190, Loss: 0.897205, Accuracy: 84.04%\n",
      "Batch 191, Loss: 0.918513, Accuracy: 84.03%\n",
      "Batch 192, Loss: 0.948587, Accuracy: 84.01%\n",
      "Batch 193, Loss: 0.872045, Accuracy: 84.03%\n",
      "Batch 194, Loss: 0.853427, Accuracy: 84.07%\n",
      "Batch 195, Loss: 0.842549, Accuracy: 84.10%\n",
      "Batch 196, Loss: 0.868717, Accuracy: 84.12%\n",
      "Batch 197, Loss: 0.905006, Accuracy: 84.13%\n",
      "Batch 198, Loss: 0.863943, Accuracy: 84.15%\n",
      "Batch 199, Loss: 0.938169, Accuracy: 84.12%\n",
      "Batch 200, Loss: 0.922551, Accuracy: 84.12%\n",
      "Batch 201, Loss: 0.974604, Accuracy: 84.09%\n",
      "Batch 202, Loss: 0.935651, Accuracy: 84.07%\n",
      "Batch 203, Loss: 0.851026, Accuracy: 84.10%\n",
      "Batch 204, Loss: 0.922297, Accuracy: 84.09%\n",
      "Batch 205, Loss: 0.943906, Accuracy: 84.07%\n",
      "Batch 206, Loss: 0.885704, Accuracy: 84.08%\n",
      "Batch 207, Loss: 0.845777, Accuracy: 84.11%\n",
      "Batch 208, Loss: 0.931758, Accuracy: 84.10%\n",
      "Batch 209, Loss: 0.900682, Accuracy: 84.11%\n",
      "Batch 210, Loss: 0.952598, Accuracy: 84.08%\n",
      "Batch 211, Loss: 1.003086, Accuracy: 84.03%\n",
      "Batch 212, Loss: 0.840583, Accuracy: 84.07%\n",
      "Batch 213, Loss: 0.960031, Accuracy: 84.04%\n",
      "Training - Epoch 55, Loss: 0.906707, Accuracy: 84.04%\n",
      "Validation Batch 1, Loss: 0.899770, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.913177, Accuracy: 86.72%\n",
      "Validation Batch 3, Loss: 0.997312, Accuracy: 82.81%\n",
      "Validation Batch 4, Loss: 0.915501, Accuracy: 82.42%\n",
      "Validation Batch 5, Loss: 0.895286, Accuracy: 83.44%\n",
      "Validation Batch 6, Loss: 0.874081, Accuracy: 84.38%\n",
      "Validation Batch 7, Loss: 0.899048, Accuracy: 84.60%\n",
      "Validation Batch 8, Loss: 0.989511, Accuracy: 83.40%\n",
      "Validation Batch 9, Loss: 0.944728, Accuracy: 83.51%\n",
      "Validation Batch 10, Loss: 0.942887, Accuracy: 82.97%\n",
      "Validation Batch 11, Loss: 0.928652, Accuracy: 82.53%\n",
      "Validation Batch 12, Loss: 0.923815, Accuracy: 82.16%\n",
      "Validation Batch 13, Loss: 0.916526, Accuracy: 82.21%\n",
      "Validation Batch 14, Loss: 0.954340, Accuracy: 82.14%\n",
      "Validation Batch 15, Loss: 0.902283, Accuracy: 82.29%\n",
      "Validation Batch 16, Loss: 0.924590, Accuracy: 82.32%\n",
      "Validation Batch 17, Loss: 0.972519, Accuracy: 81.89%\n",
      "Validation Batch 18, Loss: 0.882325, Accuracy: 82.12%\n",
      "Validation Batch 19, Loss: 0.957679, Accuracy: 81.99%\n",
      "Validation Batch 20, Loss: 1.000881, Accuracy: 81.48%\n",
      "Validation Batch 21, Loss: 0.977229, Accuracy: 81.18%\n",
      "Validation Batch 22, Loss: 0.921275, Accuracy: 81.25%\n",
      "Validation Batch 23, Loss: 0.971278, Accuracy: 81.11%\n",
      "Validation Batch 24, Loss: 0.934107, Accuracy: 81.05%\n",
      "Validation Batch 25, Loss: 0.906235, Accuracy: 81.12%\n",
      "Validation Batch 26, Loss: 0.924075, Accuracy: 81.19%\n",
      "Validation Batch 27, Loss: 0.940493, Accuracy: 81.15%\n",
      "Validation - Epoch 55, Loss: 0.933689, Accuracy: 81.15%\n",
      "Patience—0\n",
      "Epoch 56\n",
      "Batch 1, Loss: 0.934564, Accuracy: 81.25%\n",
      "Batch 2, Loss: 0.908206, Accuracy: 82.81%\n",
      "Batch 3, Loss: 0.915838, Accuracy: 83.33%\n",
      "Batch 4, Loss: 0.848695, Accuracy: 84.77%\n",
      "Batch 5, Loss: 0.859421, Accuracy: 85.31%\n",
      "Batch 6, Loss: 0.924563, Accuracy: 84.64%\n",
      "Batch 7, Loss: 0.944459, Accuracy: 83.93%\n",
      "Batch 8, Loss: 0.925389, Accuracy: 83.59%\n",
      "Batch 9, Loss: 0.928765, Accuracy: 83.16%\n",
      "Batch 10, Loss: 0.946566, Accuracy: 82.81%\n",
      "Batch 11, Loss: 0.833734, Accuracy: 83.66%\n",
      "Batch 12, Loss: 0.901055, Accuracy: 83.72%\n",
      "Batch 13, Loss: 0.873197, Accuracy: 83.89%\n",
      "Batch 14, Loss: 0.851809, Accuracy: 84.26%\n",
      "Batch 15, Loss: 0.851732, Accuracy: 84.58%\n",
      "Batch 16, Loss: 0.947168, Accuracy: 84.28%\n",
      "Batch 17, Loss: 0.958491, Accuracy: 84.01%\n",
      "Batch 18, Loss: 0.996509, Accuracy: 83.51%\n",
      "Batch 19, Loss: 0.883147, Accuracy: 83.63%\n",
      "Batch 20, Loss: 0.926659, Accuracy: 83.44%\n",
      "Batch 21, Loss: 0.932730, Accuracy: 83.41%\n",
      "Batch 22, Loss: 0.933019, Accuracy: 83.24%\n",
      "Batch 23, Loss: 0.941171, Accuracy: 83.08%\n",
      "Batch 24, Loss: 0.906675, Accuracy: 83.20%\n",
      "Batch 25, Loss: 0.860907, Accuracy: 83.50%\n",
      "Batch 26, Loss: 0.990490, Accuracy: 83.23%\n",
      "Batch 27, Loss: 0.890472, Accuracy: 83.33%\n",
      "Batch 28, Loss: 0.876568, Accuracy: 83.48%\n",
      "Batch 29, Loss: 0.913260, Accuracy: 83.46%\n",
      "Batch 30, Loss: 0.883518, Accuracy: 83.59%\n",
      "Batch 31, Loss: 0.888088, Accuracy: 83.67%\n",
      "Batch 32, Loss: 0.855514, Accuracy: 83.79%\n",
      "Batch 33, Loss: 0.791269, Accuracy: 84.14%\n",
      "Batch 34, Loss: 1.004448, Accuracy: 83.92%\n",
      "Batch 35, Loss: 0.881520, Accuracy: 84.06%\n",
      "Batch 36, Loss: 0.902387, Accuracy: 84.03%\n",
      "Batch 37, Loss: 0.877322, Accuracy: 84.12%\n",
      "Batch 38, Loss: 0.867619, Accuracy: 84.21%\n",
      "Batch 39, Loss: 0.885562, Accuracy: 84.25%\n",
      "Batch 40, Loss: 0.880152, Accuracy: 84.38%\n",
      "Batch 41, Loss: 0.966447, Accuracy: 84.22%\n",
      "Batch 42, Loss: 0.894370, Accuracy: 84.26%\n",
      "Batch 43, Loss: 0.939851, Accuracy: 84.12%\n",
      "Batch 44, Loss: 0.913784, Accuracy: 84.09%\n",
      "Batch 45, Loss: 0.921386, Accuracy: 84.03%\n",
      "Batch 46, Loss: 0.923847, Accuracy: 83.87%\n",
      "Batch 47, Loss: 0.859622, Accuracy: 83.94%\n",
      "Batch 48, Loss: 0.916739, Accuracy: 83.92%\n",
      "Batch 49, Loss: 0.897487, Accuracy: 83.99%\n",
      "Batch 50, Loss: 0.921559, Accuracy: 84.00%\n",
      "Batch 51, Loss: 0.900614, Accuracy: 84.04%\n",
      "Batch 52, Loss: 0.884210, Accuracy: 84.10%\n",
      "Batch 53, Loss: 0.865037, Accuracy: 84.14%\n",
      "Batch 54, Loss: 0.866916, Accuracy: 84.23%\n",
      "Batch 55, Loss: 0.880134, Accuracy: 84.26%\n",
      "Batch 56, Loss: 0.974872, Accuracy: 84.15%\n",
      "Batch 57, Loss: 0.838838, Accuracy: 84.29%\n",
      "Batch 58, Loss: 0.933248, Accuracy: 84.21%\n",
      "Batch 59, Loss: 0.867781, Accuracy: 84.27%\n",
      "Batch 60, Loss: 0.891249, Accuracy: 84.30%\n",
      "Batch 61, Loss: 1.012428, Accuracy: 84.09%\n",
      "Batch 62, Loss: 0.874425, Accuracy: 84.15%\n",
      "Batch 63, Loss: 0.878875, Accuracy: 84.18%\n",
      "Batch 64, Loss: 0.884820, Accuracy: 84.16%\n",
      "Batch 65, Loss: 0.909141, Accuracy: 84.13%\n",
      "Batch 66, Loss: 0.889761, Accuracy: 84.16%\n",
      "Batch 67, Loss: 0.850836, Accuracy: 84.26%\n",
      "Batch 68, Loss: 0.985985, Accuracy: 84.15%\n",
      "Batch 69, Loss: 0.899075, Accuracy: 84.15%\n",
      "Batch 70, Loss: 0.908164, Accuracy: 84.15%\n",
      "Batch 71, Loss: 0.881925, Accuracy: 84.18%\n",
      "Batch 72, Loss: 0.979960, Accuracy: 84.07%\n",
      "Batch 73, Loss: 0.874784, Accuracy: 84.12%\n",
      "Batch 74, Loss: 0.884717, Accuracy: 84.12%\n",
      "Batch 75, Loss: 0.867966, Accuracy: 84.19%\n",
      "Batch 76, Loss: 0.850566, Accuracy: 84.27%\n",
      "Batch 77, Loss: 0.928989, Accuracy: 84.19%\n",
      "Batch 78, Loss: 0.877394, Accuracy: 84.21%\n",
      "Batch 79, Loss: 0.870661, Accuracy: 84.26%\n",
      "Batch 80, Loss: 0.921845, Accuracy: 84.22%\n",
      "Batch 81, Loss: 0.877103, Accuracy: 84.24%\n",
      "Batch 82, Loss: 0.873593, Accuracy: 84.30%\n",
      "Batch 83, Loss: 0.869995, Accuracy: 84.34%\n",
      "Batch 84, Loss: 0.930069, Accuracy: 84.28%\n",
      "Batch 85, Loss: 0.860868, Accuracy: 84.32%\n",
      "Batch 86, Loss: 0.954703, Accuracy: 84.27%\n",
      "Batch 87, Loss: 0.853027, Accuracy: 84.34%\n",
      "Batch 88, Loss: 0.883860, Accuracy: 84.38%\n",
      "Batch 89, Loss: 0.891637, Accuracy: 84.39%\n",
      "Batch 90, Loss: 0.938573, Accuracy: 84.34%\n",
      "Batch 91, Loss: 0.867879, Accuracy: 84.38%\n",
      "Batch 92, Loss: 0.956891, Accuracy: 84.32%\n",
      "Batch 93, Loss: 0.951340, Accuracy: 84.26%\n",
      "Batch 94, Loss: 0.913668, Accuracy: 84.23%\n",
      "Batch 95, Loss: 0.947835, Accuracy: 84.13%\n",
      "Batch 96, Loss: 0.913296, Accuracy: 84.13%\n",
      "Batch 97, Loss: 0.899903, Accuracy: 84.13%\n",
      "Batch 98, Loss: 0.859654, Accuracy: 84.17%\n",
      "Batch 99, Loss: 0.866088, Accuracy: 84.19%\n",
      "Batch 100, Loss: 0.887459, Accuracy: 84.20%\n",
      "Batch 101, Loss: 0.909464, Accuracy: 84.19%\n",
      "Batch 102, Loss: 0.885959, Accuracy: 84.21%\n",
      "Batch 103, Loss: 0.907173, Accuracy: 84.22%\n",
      "Batch 104, Loss: 0.902356, Accuracy: 84.22%\n",
      "Batch 105, Loss: 0.907228, Accuracy: 84.23%\n",
      "Batch 106, Loss: 1.019814, Accuracy: 84.11%\n",
      "Batch 107, Loss: 0.844127, Accuracy: 84.17%\n",
      "Batch 108, Loss: 0.888632, Accuracy: 84.17%\n",
      "Batch 109, Loss: 0.948544, Accuracy: 84.13%\n",
      "Batch 110, Loss: 0.871278, Accuracy: 84.18%\n",
      "Batch 111, Loss: 0.896586, Accuracy: 84.21%\n",
      "Batch 112, Loss: 0.961488, Accuracy: 84.15%\n",
      "Batch 113, Loss: 0.875319, Accuracy: 84.15%\n",
      "Batch 114, Loss: 1.027527, Accuracy: 84.02%\n",
      "Batch 115, Loss: 0.926555, Accuracy: 83.99%\n",
      "Batch 116, Loss: 0.849106, Accuracy: 84.05%\n",
      "Batch 117, Loss: 0.867629, Accuracy: 84.09%\n",
      "Batch 118, Loss: 0.936651, Accuracy: 84.07%\n",
      "Batch 119, Loss: 0.930006, Accuracy: 84.05%\n",
      "Batch 120, Loss: 0.928525, Accuracy: 84.04%\n",
      "Batch 121, Loss: 0.881779, Accuracy: 84.05%\n",
      "Batch 122, Loss: 0.847615, Accuracy: 84.09%\n",
      "Batch 123, Loss: 0.897147, Accuracy: 84.08%\n",
      "Batch 124, Loss: 0.914484, Accuracy: 84.07%\n",
      "Batch 125, Loss: 0.926237, Accuracy: 84.08%\n",
      "Batch 126, Loss: 0.909927, Accuracy: 84.08%\n",
      "Batch 127, Loss: 0.931838, Accuracy: 84.04%\n",
      "Batch 128, Loss: 0.899854, Accuracy: 84.06%\n",
      "Batch 129, Loss: 0.963500, Accuracy: 84.01%\n",
      "Batch 130, Loss: 0.960427, Accuracy: 83.98%\n",
      "Batch 131, Loss: 0.912675, Accuracy: 83.97%\n",
      "Batch 132, Loss: 0.889423, Accuracy: 83.98%\n",
      "Batch 133, Loss: 0.903454, Accuracy: 83.98%\n",
      "Batch 134, Loss: 0.883489, Accuracy: 84.01%\n",
      "Batch 135, Loss: 0.863060, Accuracy: 84.06%\n",
      "Batch 136, Loss: 0.969439, Accuracy: 84.02%\n",
      "Batch 137, Loss: 0.913352, Accuracy: 84.01%\n",
      "Batch 138, Loss: 0.877608, Accuracy: 84.04%\n",
      "Batch 139, Loss: 0.884921, Accuracy: 84.05%\n",
      "Batch 140, Loss: 0.867810, Accuracy: 84.08%\n",
      "Batch 141, Loss: 0.939386, Accuracy: 84.06%\n",
      "Batch 142, Loss: 0.867441, Accuracy: 84.10%\n",
      "Batch 143, Loss: 0.970154, Accuracy: 84.06%\n",
      "Batch 144, Loss: 0.844664, Accuracy: 84.10%\n",
      "Batch 145, Loss: 0.886930, Accuracy: 84.12%\n",
      "Batch 146, Loss: 0.918473, Accuracy: 84.11%\n",
      "Batch 147, Loss: 0.909643, Accuracy: 84.08%\n",
      "Batch 148, Loss: 0.903632, Accuracy: 84.07%\n",
      "Batch 149, Loss: 0.914703, Accuracy: 84.07%\n",
      "Batch 150, Loss: 0.934063, Accuracy: 84.05%\n",
      "Batch 151, Loss: 0.920296, Accuracy: 84.04%\n",
      "Batch 152, Loss: 0.843431, Accuracy: 84.09%\n",
      "Batch 153, Loss: 0.953131, Accuracy: 84.06%\n",
      "Batch 154, Loss: 0.958126, Accuracy: 84.02%\n",
      "Batch 155, Loss: 0.877523, Accuracy: 84.04%\n",
      "Batch 156, Loss: 0.953899, Accuracy: 84.02%\n",
      "Batch 157, Loss: 0.898827, Accuracy: 84.04%\n",
      "Batch 158, Loss: 0.921231, Accuracy: 84.03%\n",
      "Batch 159, Loss: 0.912506, Accuracy: 84.03%\n",
      "Batch 160, Loss: 0.972493, Accuracy: 83.97%\n",
      "Batch 161, Loss: 0.942754, Accuracy: 83.95%\n",
      "Batch 162, Loss: 0.846455, Accuracy: 83.98%\n",
      "Batch 163, Loss: 0.847780, Accuracy: 84.01%\n",
      "Batch 164, Loss: 0.830884, Accuracy: 84.06%\n",
      "Batch 165, Loss: 0.921863, Accuracy: 84.04%\n",
      "Batch 166, Loss: 0.905430, Accuracy: 84.04%\n",
      "Batch 167, Loss: 0.925206, Accuracy: 84.03%\n",
      "Batch 168, Loss: 0.868428, Accuracy: 84.06%\n",
      "Batch 169, Loss: 0.911967, Accuracy: 84.04%\n",
      "Batch 170, Loss: 0.925219, Accuracy: 84.03%\n",
      "Batch 171, Loss: 1.017532, Accuracy: 83.95%\n",
      "Batch 172, Loss: 0.881385, Accuracy: 83.98%\n",
      "Batch 173, Loss: 0.998132, Accuracy: 83.91%\n",
      "Batch 174, Loss: 0.942551, Accuracy: 83.89%\n",
      "Batch 175, Loss: 0.890628, Accuracy: 83.89%\n",
      "Batch 176, Loss: 0.890782, Accuracy: 83.90%\n",
      "Batch 177, Loss: 0.868550, Accuracy: 83.92%\n",
      "Batch 178, Loss: 0.826709, Accuracy: 83.95%\n",
      "Batch 179, Loss: 0.893716, Accuracy: 83.97%\n",
      "Batch 180, Loss: 0.887043, Accuracy: 83.98%\n",
      "Batch 181, Loss: 0.930041, Accuracy: 83.96%\n",
      "Batch 182, Loss: 0.931044, Accuracy: 83.96%\n",
      "Batch 183, Loss: 0.977642, Accuracy: 83.91%\n",
      "Batch 184, Loss: 0.914948, Accuracy: 83.92%\n",
      "Batch 185, Loss: 0.938693, Accuracy: 83.89%\n",
      "Batch 186, Loss: 0.863454, Accuracy: 83.90%\n",
      "Batch 187, Loss: 0.877647, Accuracy: 83.92%\n",
      "Batch 188, Loss: 0.975134, Accuracy: 83.88%\n",
      "Batch 189, Loss: 0.938152, Accuracy: 83.86%\n",
      "Batch 190, Loss: 0.987880, Accuracy: 83.82%\n",
      "Batch 191, Loss: 0.886879, Accuracy: 83.82%\n",
      "Batch 192, Loss: 0.964309, Accuracy: 83.79%\n",
      "Batch 193, Loss: 0.887962, Accuracy: 83.81%\n",
      "Batch 194, Loss: 0.821171, Accuracy: 83.86%\n",
      "Batch 195, Loss: 0.957113, Accuracy: 83.81%\n",
      "Batch 196, Loss: 0.868990, Accuracy: 83.84%\n",
      "Batch 197, Loss: 0.887356, Accuracy: 83.84%\n",
      "Batch 198, Loss: 0.906612, Accuracy: 83.83%\n",
      "Batch 199, Loss: 0.943725, Accuracy: 83.82%\n",
      "Batch 200, Loss: 0.939751, Accuracy: 83.81%\n",
      "Batch 201, Loss: 0.916880, Accuracy: 83.81%\n",
      "Batch 202, Loss: 0.951529, Accuracy: 83.79%\n",
      "Batch 203, Loss: 0.886544, Accuracy: 83.82%\n",
      "Batch 204, Loss: 0.853248, Accuracy: 83.85%\n",
      "Batch 205, Loss: 0.945501, Accuracy: 83.83%\n",
      "Batch 206, Loss: 0.887031, Accuracy: 83.84%\n",
      "Batch 207, Loss: 0.952661, Accuracy: 83.82%\n",
      "Batch 208, Loss: 0.851771, Accuracy: 83.84%\n",
      "Batch 209, Loss: 0.885702, Accuracy: 83.85%\n",
      "Batch 210, Loss: 0.886658, Accuracy: 83.86%\n",
      "Batch 211, Loss: 0.939396, Accuracy: 83.85%\n",
      "Batch 212, Loss: 0.873313, Accuracy: 83.87%\n",
      "Batch 213, Loss: 0.829805, Accuracy: 83.91%\n",
      "Training - Epoch 56, Loss: 0.905984, Accuracy: 83.91%\n",
      "Validation Batch 1, Loss: 0.896827, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.898260, Accuracy: 85.94%\n",
      "Validation Batch 3, Loss: 0.968236, Accuracy: 84.38%\n",
      "Validation Batch 4, Loss: 0.902601, Accuracy: 84.38%\n",
      "Validation Batch 5, Loss: 0.891526, Accuracy: 85.00%\n",
      "Validation Batch 6, Loss: 0.866646, Accuracy: 85.68%\n",
      "Validation Batch 7, Loss: 0.883720, Accuracy: 85.94%\n",
      "Validation Batch 8, Loss: 0.981023, Accuracy: 84.57%\n",
      "Validation Batch 9, Loss: 0.940499, Accuracy: 84.38%\n",
      "Validation Batch 10, Loss: 0.940767, Accuracy: 83.91%\n",
      "Validation Batch 11, Loss: 0.923511, Accuracy: 83.52%\n",
      "Validation Batch 12, Loss: 0.909574, Accuracy: 83.33%\n",
      "Validation Batch 13, Loss: 0.911266, Accuracy: 83.41%\n",
      "Validation Batch 14, Loss: 0.943351, Accuracy: 83.26%\n",
      "Validation Batch 15, Loss: 0.904470, Accuracy: 83.23%\n",
      "Validation Batch 16, Loss: 0.918356, Accuracy: 83.30%\n",
      "Validation Batch 17, Loss: 0.970985, Accuracy: 83.00%\n",
      "Validation Batch 18, Loss: 0.881066, Accuracy: 83.16%\n",
      "Validation Batch 19, Loss: 0.944743, Accuracy: 83.06%\n",
      "Validation Batch 20, Loss: 0.988396, Accuracy: 82.66%\n",
      "Validation Batch 21, Loss: 0.961687, Accuracy: 82.44%\n",
      "Validation Batch 22, Loss: 0.924217, Accuracy: 82.39%\n",
      "Validation Batch 23, Loss: 0.953662, Accuracy: 82.27%\n",
      "Validation Batch 24, Loss: 0.936089, Accuracy: 82.23%\n",
      "Validation Batch 25, Loss: 0.896878, Accuracy: 82.31%\n",
      "Validation Batch 26, Loss: 0.909482, Accuracy: 82.45%\n",
      "Validation Batch 27, Loss: 0.929655, Accuracy: 82.33%\n",
      "Validation - Epoch 56, Loss: 0.925092, Accuracy: 82.33%\n",
      "Patience—0\n",
      "Epoch 57\n",
      "Batch 1, Loss: 0.999830, Accuracy: 71.88%\n",
      "Batch 2, Loss: 0.939228, Accuracy: 76.56%\n",
      "Batch 3, Loss: 0.894076, Accuracy: 79.69%\n",
      "Batch 4, Loss: 0.889073, Accuracy: 81.25%\n",
      "Batch 5, Loss: 1.035101, Accuracy: 79.06%\n",
      "Batch 6, Loss: 0.895621, Accuracy: 80.21%\n",
      "Batch 7, Loss: 0.900218, Accuracy: 81.25%\n",
      "Batch 8, Loss: 1.031624, Accuracy: 80.08%\n",
      "Batch 9, Loss: 0.876451, Accuracy: 80.73%\n",
      "Batch 10, Loss: 0.912375, Accuracy: 80.94%\n",
      "Batch 11, Loss: 0.954300, Accuracy: 80.82%\n",
      "Batch 12, Loss: 0.927555, Accuracy: 80.86%\n",
      "Batch 13, Loss: 0.970311, Accuracy: 80.53%\n",
      "Batch 14, Loss: 0.890827, Accuracy: 80.80%\n",
      "Batch 15, Loss: 0.877424, Accuracy: 81.25%\n",
      "Batch 16, Loss: 0.864120, Accuracy: 81.64%\n",
      "Batch 17, Loss: 0.947208, Accuracy: 81.62%\n",
      "Batch 18, Loss: 1.005037, Accuracy: 81.25%\n",
      "Batch 19, Loss: 0.916781, Accuracy: 81.33%\n",
      "Batch 20, Loss: 0.969937, Accuracy: 81.09%\n",
      "Batch 21, Loss: 0.894595, Accuracy: 81.32%\n",
      "Batch 22, Loss: 0.884103, Accuracy: 81.61%\n",
      "Batch 23, Loss: 0.972253, Accuracy: 81.39%\n",
      "Batch 24, Loss: 0.881530, Accuracy: 81.58%\n",
      "Batch 25, Loss: 0.895961, Accuracy: 81.69%\n",
      "Batch 26, Loss: 0.867301, Accuracy: 81.91%\n",
      "Batch 27, Loss: 0.875083, Accuracy: 82.06%\n",
      "Batch 28, Loss: 0.927619, Accuracy: 82.03%\n",
      "Batch 29, Loss: 0.875116, Accuracy: 82.22%\n",
      "Batch 30, Loss: 0.875626, Accuracy: 82.45%\n",
      "Batch 31, Loss: 0.927346, Accuracy: 82.36%\n",
      "Batch 32, Loss: 0.883258, Accuracy: 82.42%\n",
      "Batch 33, Loss: 0.916910, Accuracy: 82.43%\n",
      "Batch 34, Loss: 0.902607, Accuracy: 82.49%\n",
      "Batch 35, Loss: 0.871886, Accuracy: 82.63%\n",
      "Batch 36, Loss: 0.975372, Accuracy: 82.47%\n",
      "Batch 37, Loss: 0.832546, Accuracy: 82.73%\n",
      "Batch 38, Loss: 0.985789, Accuracy: 82.57%\n",
      "Batch 39, Loss: 0.938207, Accuracy: 82.49%\n",
      "Batch 40, Loss: 0.916620, Accuracy: 82.50%\n",
      "Batch 41, Loss: 0.915156, Accuracy: 82.51%\n",
      "Batch 42, Loss: 0.878391, Accuracy: 82.59%\n",
      "Batch 43, Loss: 0.841151, Accuracy: 82.81%\n",
      "Batch 44, Loss: 0.823305, Accuracy: 83.10%\n",
      "Batch 45, Loss: 0.929237, Accuracy: 83.09%\n",
      "Batch 46, Loss: 0.912619, Accuracy: 83.12%\n",
      "Batch 47, Loss: 0.927109, Accuracy: 83.08%\n",
      "Batch 48, Loss: 0.979739, Accuracy: 82.91%\n",
      "Batch 49, Loss: 0.950176, Accuracy: 82.84%\n",
      "Batch 50, Loss: 0.878393, Accuracy: 82.94%\n",
      "Batch 51, Loss: 0.870847, Accuracy: 82.97%\n",
      "Batch 52, Loss: 0.882409, Accuracy: 82.99%\n",
      "Batch 53, Loss: 0.817294, Accuracy: 83.17%\n",
      "Batch 54, Loss: 0.906152, Accuracy: 83.16%\n",
      "Batch 55, Loss: 0.886554, Accuracy: 83.21%\n",
      "Batch 56, Loss: 0.917070, Accuracy: 83.23%\n",
      "Batch 57, Loss: 0.875177, Accuracy: 83.28%\n",
      "Batch 58, Loss: 0.911343, Accuracy: 83.27%\n",
      "Batch 59, Loss: 0.945039, Accuracy: 83.21%\n",
      "Batch 60, Loss: 0.882024, Accuracy: 83.26%\n",
      "Batch 61, Loss: 0.893511, Accuracy: 83.27%\n",
      "Batch 62, Loss: 0.897547, Accuracy: 83.32%\n",
      "Batch 63, Loss: 0.860295, Accuracy: 83.41%\n",
      "Batch 64, Loss: 0.888835, Accuracy: 83.45%\n",
      "Batch 65, Loss: 0.895620, Accuracy: 83.51%\n",
      "Batch 66, Loss: 0.916637, Accuracy: 83.50%\n",
      "Batch 67, Loss: 0.848843, Accuracy: 83.56%\n",
      "Batch 68, Loss: 0.912410, Accuracy: 83.57%\n",
      "Batch 69, Loss: 0.864562, Accuracy: 83.65%\n",
      "Batch 70, Loss: 0.930877, Accuracy: 83.66%\n",
      "Batch 71, Loss: 0.842363, Accuracy: 83.76%\n",
      "Batch 72, Loss: 0.898350, Accuracy: 83.79%\n",
      "Batch 73, Loss: 0.886974, Accuracy: 83.84%\n",
      "Batch 74, Loss: 0.958927, Accuracy: 83.76%\n",
      "Batch 75, Loss: 0.923587, Accuracy: 83.73%\n",
      "Batch 76, Loss: 0.903176, Accuracy: 83.78%\n",
      "Batch 77, Loss: 0.908527, Accuracy: 83.79%\n",
      "Batch 78, Loss: 0.906329, Accuracy: 83.75%\n",
      "Batch 79, Loss: 0.916565, Accuracy: 83.72%\n",
      "Batch 80, Loss: 0.880764, Accuracy: 83.75%\n",
      "Batch 81, Loss: 0.891658, Accuracy: 83.78%\n",
      "Batch 82, Loss: 0.910110, Accuracy: 83.78%\n",
      "Batch 83, Loss: 0.908638, Accuracy: 83.77%\n",
      "Batch 84, Loss: 0.842943, Accuracy: 83.85%\n",
      "Batch 85, Loss: 0.922071, Accuracy: 83.82%\n",
      "Batch 86, Loss: 0.936313, Accuracy: 83.81%\n",
      "Batch 87, Loss: 0.834319, Accuracy: 83.91%\n",
      "Batch 88, Loss: 0.857421, Accuracy: 83.95%\n",
      "Batch 89, Loss: 0.847361, Accuracy: 83.99%\n",
      "Batch 90, Loss: 0.904692, Accuracy: 83.98%\n",
      "Batch 91, Loss: 0.921166, Accuracy: 83.96%\n",
      "Batch 92, Loss: 0.902655, Accuracy: 83.95%\n",
      "Batch 93, Loss: 0.793088, Accuracy: 84.09%\n",
      "Batch 94, Loss: 0.899526, Accuracy: 84.09%\n",
      "Batch 95, Loss: 0.823903, Accuracy: 84.18%\n",
      "Batch 96, Loss: 0.891643, Accuracy: 84.20%\n",
      "Batch 97, Loss: 0.938196, Accuracy: 84.17%\n",
      "Batch 98, Loss: 0.880889, Accuracy: 84.18%\n",
      "Batch 99, Loss: 0.896266, Accuracy: 84.20%\n",
      "Batch 100, Loss: 0.915831, Accuracy: 84.17%\n",
      "Batch 101, Loss: 0.899195, Accuracy: 84.19%\n",
      "Batch 102, Loss: 0.951702, Accuracy: 84.15%\n",
      "Batch 103, Loss: 0.926089, Accuracy: 84.12%\n",
      "Batch 104, Loss: 0.971501, Accuracy: 84.04%\n",
      "Batch 105, Loss: 0.837150, Accuracy: 84.11%\n",
      "Batch 106, Loss: 0.875502, Accuracy: 84.11%\n",
      "Batch 107, Loss: 0.929589, Accuracy: 84.07%\n",
      "Batch 108, Loss: 0.855068, Accuracy: 84.13%\n",
      "Batch 109, Loss: 0.895782, Accuracy: 84.15%\n",
      "Batch 110, Loss: 0.931454, Accuracy: 84.12%\n",
      "Batch 111, Loss: 0.892604, Accuracy: 84.12%\n",
      "Batch 112, Loss: 0.926983, Accuracy: 84.10%\n",
      "Batch 113, Loss: 0.948594, Accuracy: 84.06%\n",
      "Batch 114, Loss: 0.902853, Accuracy: 84.06%\n",
      "Batch 115, Loss: 0.957284, Accuracy: 84.01%\n",
      "Batch 116, Loss: 0.898965, Accuracy: 84.01%\n",
      "Batch 117, Loss: 0.866230, Accuracy: 84.04%\n",
      "Batch 118, Loss: 0.927880, Accuracy: 84.02%\n",
      "Batch 119, Loss: 0.973975, Accuracy: 83.97%\n",
      "Batch 120, Loss: 0.895192, Accuracy: 83.97%\n",
      "Batch 121, Loss: 0.921559, Accuracy: 83.96%\n",
      "Batch 122, Loss: 0.922596, Accuracy: 83.95%\n",
      "Batch 123, Loss: 0.867825, Accuracy: 83.98%\n",
      "Batch 124, Loss: 0.887872, Accuracy: 84.00%\n",
      "Batch 125, Loss: 0.868438, Accuracy: 84.03%\n",
      "Batch 126, Loss: 0.934447, Accuracy: 83.99%\n",
      "Batch 127, Loss: 0.971268, Accuracy: 83.93%\n",
      "Batch 128, Loss: 0.981478, Accuracy: 83.86%\n",
      "Batch 129, Loss: 0.886497, Accuracy: 83.88%\n",
      "Batch 130, Loss: 0.867567, Accuracy: 83.92%\n",
      "Batch 131, Loss: 0.927551, Accuracy: 83.90%\n",
      "Batch 132, Loss: 0.903389, Accuracy: 83.89%\n",
      "Batch 133, Loss: 0.920730, Accuracy: 83.86%\n",
      "Batch 134, Loss: 0.949233, Accuracy: 83.82%\n",
      "Batch 135, Loss: 0.922384, Accuracy: 83.80%\n",
      "Batch 136, Loss: 0.855533, Accuracy: 83.85%\n",
      "Batch 137, Loss: 0.838054, Accuracy: 83.91%\n",
      "Batch 138, Loss: 0.880652, Accuracy: 83.92%\n",
      "Batch 139, Loss: 0.867667, Accuracy: 83.95%\n",
      "Batch 140, Loss: 0.886830, Accuracy: 83.96%\n",
      "Batch 141, Loss: 0.859203, Accuracy: 84.00%\n",
      "Batch 142, Loss: 0.882799, Accuracy: 84.01%\n",
      "Batch 143, Loss: 0.879140, Accuracy: 84.04%\n",
      "Batch 144, Loss: 0.917105, Accuracy: 84.03%\n",
      "Batch 145, Loss: 0.900338, Accuracy: 84.04%\n",
      "Batch 146, Loss: 0.910410, Accuracy: 84.02%\n",
      "Batch 147, Loss: 0.896505, Accuracy: 84.03%\n",
      "Batch 148, Loss: 0.868920, Accuracy: 84.06%\n",
      "Batch 149, Loss: 0.875993, Accuracy: 84.07%\n",
      "Batch 150, Loss: 0.885462, Accuracy: 84.08%\n",
      "Batch 151, Loss: 0.889358, Accuracy: 84.10%\n",
      "Batch 152, Loss: 0.972430, Accuracy: 84.06%\n",
      "Batch 153, Loss: 0.957017, Accuracy: 84.03%\n",
      "Batch 154, Loss: 0.935179, Accuracy: 84.02%\n",
      "Batch 155, Loss: 0.872134, Accuracy: 84.04%\n",
      "Batch 156, Loss: 0.923627, Accuracy: 84.02%\n",
      "Batch 157, Loss: 0.955492, Accuracy: 83.99%\n",
      "Batch 158, Loss: 0.969212, Accuracy: 83.96%\n",
      "Batch 159, Loss: 0.983798, Accuracy: 83.90%\n",
      "Batch 160, Loss: 0.836091, Accuracy: 83.96%\n",
      "Batch 161, Loss: 0.968097, Accuracy: 83.92%\n",
      "Batch 162, Loss: 0.918411, Accuracy: 83.91%\n",
      "Batch 163, Loss: 0.905546, Accuracy: 83.92%\n",
      "Batch 164, Loss: 0.867902, Accuracy: 83.95%\n",
      "Batch 165, Loss: 0.930194, Accuracy: 83.92%\n",
      "Batch 166, Loss: 0.868195, Accuracy: 83.94%\n",
      "Batch 167, Loss: 0.966123, Accuracy: 83.90%\n",
      "Batch 168, Loss: 0.903609, Accuracy: 83.90%\n",
      "Batch 169, Loss: 0.903815, Accuracy: 83.90%\n",
      "Batch 170, Loss: 0.928564, Accuracy: 83.91%\n",
      "Batch 171, Loss: 0.920066, Accuracy: 83.90%\n",
      "Batch 172, Loss: 0.939416, Accuracy: 83.86%\n",
      "Batch 173, Loss: 0.920027, Accuracy: 83.85%\n",
      "Batch 174, Loss: 0.829292, Accuracy: 83.90%\n",
      "Batch 175, Loss: 0.910020, Accuracy: 83.90%\n",
      "Batch 176, Loss: 0.934721, Accuracy: 83.89%\n",
      "Batch 177, Loss: 0.885913, Accuracy: 83.89%\n",
      "Batch 178, Loss: 0.902178, Accuracy: 83.89%\n",
      "Batch 179, Loss: 0.883543, Accuracy: 83.91%\n",
      "Batch 180, Loss: 0.845263, Accuracy: 83.95%\n",
      "Batch 181, Loss: 0.928994, Accuracy: 83.93%\n",
      "Batch 182, Loss: 0.886988, Accuracy: 83.93%\n",
      "Batch 183, Loss: 0.881370, Accuracy: 83.94%\n",
      "Batch 184, Loss: 0.928737, Accuracy: 83.92%\n",
      "Batch 185, Loss: 0.918007, Accuracy: 83.89%\n",
      "Batch 186, Loss: 0.908429, Accuracy: 83.90%\n",
      "Batch 187, Loss: 0.856272, Accuracy: 83.92%\n",
      "Batch 188, Loss: 0.878267, Accuracy: 83.93%\n",
      "Batch 189, Loss: 0.859799, Accuracy: 83.95%\n",
      "Batch 190, Loss: 0.894376, Accuracy: 83.96%\n",
      "Batch 191, Loss: 0.993650, Accuracy: 83.91%\n",
      "Batch 192, Loss: 0.878858, Accuracy: 83.93%\n",
      "Batch 193, Loss: 0.986923, Accuracy: 83.88%\n",
      "Batch 194, Loss: 0.890562, Accuracy: 83.88%\n",
      "Batch 195, Loss: 0.862481, Accuracy: 83.91%\n",
      "Batch 196, Loss: 0.886698, Accuracy: 83.92%\n",
      "Batch 197, Loss: 0.873653, Accuracy: 83.93%\n",
      "Batch 198, Loss: 0.865816, Accuracy: 83.95%\n",
      "Batch 199, Loss: 0.875299, Accuracy: 83.97%\n",
      "Batch 200, Loss: 0.893266, Accuracy: 83.98%\n",
      "Batch 201, Loss: 0.900341, Accuracy: 83.99%\n",
      "Batch 202, Loss: 0.882225, Accuracy: 84.00%\n",
      "Batch 203, Loss: 0.893207, Accuracy: 84.01%\n",
      "Batch 204, Loss: 0.894056, Accuracy: 84.00%\n",
      "Batch 205, Loss: 0.903112, Accuracy: 84.01%\n",
      "Batch 206, Loss: 0.879455, Accuracy: 84.02%\n",
      "Batch 207, Loss: 0.918297, Accuracy: 84.01%\n",
      "Batch 208, Loss: 0.942230, Accuracy: 83.98%\n",
      "Batch 209, Loss: 0.859064, Accuracy: 84.01%\n",
      "Batch 210, Loss: 0.919697, Accuracy: 84.00%\n",
      "Batch 211, Loss: 0.891721, Accuracy: 84.01%\n",
      "Batch 212, Loss: 0.883290, Accuracy: 84.02%\n",
      "Batch 213, Loss: 0.838159, Accuracy: 84.05%\n",
      "Training - Epoch 57, Loss: 0.904033, Accuracy: 84.05%\n",
      "Validation Batch 1, Loss: 0.897108, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.908893, Accuracy: 85.16%\n",
      "Validation Batch 3, Loss: 0.984938, Accuracy: 82.81%\n",
      "Validation Batch 4, Loss: 0.917235, Accuracy: 82.42%\n",
      "Validation Batch 5, Loss: 0.904036, Accuracy: 83.44%\n",
      "Validation Batch 6, Loss: 0.871396, Accuracy: 84.11%\n",
      "Validation Batch 7, Loss: 0.890474, Accuracy: 84.82%\n",
      "Validation Batch 8, Loss: 0.983374, Accuracy: 83.59%\n",
      "Validation Batch 9, Loss: 0.944080, Accuracy: 83.68%\n",
      "Validation Batch 10, Loss: 0.944793, Accuracy: 83.28%\n",
      "Validation Batch 11, Loss: 0.933015, Accuracy: 82.95%\n",
      "Validation Batch 12, Loss: 0.916051, Accuracy: 82.81%\n",
      "Validation Batch 13, Loss: 0.910186, Accuracy: 82.81%\n",
      "Validation Batch 14, Loss: 0.943918, Accuracy: 82.70%\n",
      "Validation Batch 15, Loss: 0.900165, Accuracy: 82.81%\n",
      "Validation Batch 16, Loss: 0.925779, Accuracy: 82.62%\n",
      "Validation Batch 17, Loss: 0.969048, Accuracy: 82.35%\n",
      "Validation Batch 18, Loss: 0.892222, Accuracy: 82.55%\n",
      "Validation Batch 19, Loss: 0.949794, Accuracy: 82.48%\n",
      "Validation Batch 20, Loss: 1.002899, Accuracy: 82.03%\n",
      "Validation Batch 21, Loss: 0.968444, Accuracy: 81.85%\n",
      "Validation Batch 22, Loss: 0.924458, Accuracy: 81.82%\n",
      "Validation Batch 23, Loss: 0.957878, Accuracy: 81.73%\n",
      "Validation Batch 24, Loss: 0.940902, Accuracy: 81.71%\n",
      "Validation Batch 25, Loss: 0.895921, Accuracy: 81.94%\n",
      "Validation Batch 26, Loss: 0.917301, Accuracy: 82.03%\n",
      "Validation Batch 27, Loss: 0.947762, Accuracy: 81.91%\n",
      "Validation - Epoch 57, Loss: 0.931188, Accuracy: 81.91%\n",
      "Patience—1\n",
      "Epoch 58\n",
      "Batch 1, Loss: 0.928119, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.919716, Accuracy: 82.03%\n",
      "Batch 3, Loss: 0.848875, Accuracy: 84.90%\n",
      "Batch 4, Loss: 0.871589, Accuracy: 85.16%\n",
      "Batch 5, Loss: 0.900774, Accuracy: 85.62%\n",
      "Batch 6, Loss: 0.945298, Accuracy: 84.64%\n",
      "Batch 7, Loss: 0.971689, Accuracy: 83.71%\n",
      "Batch 8, Loss: 0.824787, Accuracy: 84.96%\n",
      "Batch 9, Loss: 0.909587, Accuracy: 84.90%\n",
      "Batch 10, Loss: 0.853528, Accuracy: 85.31%\n",
      "Batch 11, Loss: 0.911873, Accuracy: 85.09%\n",
      "Batch 12, Loss: 0.939649, Accuracy: 84.77%\n",
      "Batch 13, Loss: 0.867255, Accuracy: 84.98%\n",
      "Batch 14, Loss: 0.942162, Accuracy: 84.60%\n",
      "Batch 15, Loss: 0.829572, Accuracy: 85.10%\n",
      "Batch 16, Loss: 0.865768, Accuracy: 85.35%\n",
      "Batch 17, Loss: 0.846610, Accuracy: 85.66%\n",
      "Batch 18, Loss: 0.912707, Accuracy: 85.42%\n",
      "Batch 19, Loss: 0.887625, Accuracy: 85.44%\n",
      "Batch 20, Loss: 1.004119, Accuracy: 84.84%\n",
      "Batch 21, Loss: 0.973275, Accuracy: 84.52%\n",
      "Batch 22, Loss: 0.895216, Accuracy: 84.52%\n",
      "Batch 23, Loss: 0.946585, Accuracy: 84.31%\n",
      "Batch 24, Loss: 0.859074, Accuracy: 84.57%\n",
      "Batch 25, Loss: 0.962944, Accuracy: 84.31%\n",
      "Batch 26, Loss: 0.927451, Accuracy: 84.13%\n",
      "Batch 27, Loss: 0.937016, Accuracy: 84.03%\n",
      "Batch 28, Loss: 0.880560, Accuracy: 84.10%\n",
      "Batch 29, Loss: 0.872036, Accuracy: 84.21%\n",
      "Batch 30, Loss: 0.889964, Accuracy: 84.22%\n",
      "Batch 31, Loss: 0.894833, Accuracy: 84.27%\n",
      "Batch 32, Loss: 0.896771, Accuracy: 84.28%\n",
      "Batch 33, Loss: 0.861920, Accuracy: 84.42%\n",
      "Batch 34, Loss: 0.904947, Accuracy: 84.47%\n",
      "Batch 35, Loss: 0.865944, Accuracy: 84.55%\n",
      "Batch 36, Loss: 0.920178, Accuracy: 84.46%\n",
      "Batch 37, Loss: 0.919975, Accuracy: 84.38%\n",
      "Batch 38, Loss: 0.956316, Accuracy: 84.17%\n",
      "Batch 39, Loss: 0.907737, Accuracy: 84.13%\n",
      "Batch 40, Loss: 0.855630, Accuracy: 84.22%\n",
      "Batch 41, Loss: 0.913872, Accuracy: 84.22%\n",
      "Batch 42, Loss: 0.919043, Accuracy: 84.15%\n",
      "Batch 43, Loss: 0.892516, Accuracy: 84.19%\n",
      "Batch 44, Loss: 0.832602, Accuracy: 84.34%\n",
      "Batch 45, Loss: 0.881551, Accuracy: 84.38%\n",
      "Batch 46, Loss: 0.949991, Accuracy: 84.24%\n",
      "Batch 47, Loss: 0.814505, Accuracy: 84.44%\n",
      "Batch 48, Loss: 0.900074, Accuracy: 84.44%\n",
      "Batch 49, Loss: 0.963043, Accuracy: 84.34%\n",
      "Batch 50, Loss: 0.897025, Accuracy: 84.38%\n",
      "Batch 51, Loss: 0.850213, Accuracy: 84.44%\n",
      "Batch 52, Loss: 0.896257, Accuracy: 84.44%\n",
      "Batch 53, Loss: 0.935018, Accuracy: 84.35%\n",
      "Batch 54, Loss: 0.891394, Accuracy: 84.35%\n",
      "Batch 55, Loss: 0.862952, Accuracy: 84.43%\n",
      "Batch 56, Loss: 0.979305, Accuracy: 84.29%\n",
      "Batch 57, Loss: 0.866870, Accuracy: 84.32%\n",
      "Batch 58, Loss: 0.943475, Accuracy: 84.19%\n",
      "Batch 59, Loss: 0.882612, Accuracy: 84.24%\n",
      "Batch 60, Loss: 0.895835, Accuracy: 84.24%\n",
      "Batch 61, Loss: 0.905794, Accuracy: 84.22%\n",
      "Batch 62, Loss: 0.904996, Accuracy: 84.22%\n",
      "Batch 63, Loss: 0.951990, Accuracy: 84.10%\n",
      "Batch 64, Loss: 0.900253, Accuracy: 84.16%\n",
      "Batch 65, Loss: 0.849848, Accuracy: 84.23%\n",
      "Batch 66, Loss: 0.901356, Accuracy: 84.26%\n",
      "Batch 67, Loss: 0.936516, Accuracy: 84.19%\n",
      "Batch 68, Loss: 0.950380, Accuracy: 84.10%\n",
      "Batch 69, Loss: 0.927339, Accuracy: 84.08%\n",
      "Batch 70, Loss: 0.959390, Accuracy: 84.00%\n",
      "Batch 71, Loss: 0.919313, Accuracy: 84.00%\n",
      "Batch 72, Loss: 0.950137, Accuracy: 83.94%\n",
      "Batch 73, Loss: 0.942660, Accuracy: 83.86%\n",
      "Batch 74, Loss: 0.882599, Accuracy: 83.89%\n",
      "Batch 75, Loss: 0.970184, Accuracy: 83.81%\n",
      "Batch 76, Loss: 0.876570, Accuracy: 83.86%\n",
      "Batch 77, Loss: 0.881087, Accuracy: 83.91%\n",
      "Batch 78, Loss: 0.974132, Accuracy: 83.85%\n",
      "Batch 79, Loss: 0.974811, Accuracy: 83.78%\n",
      "Batch 80, Loss: 0.939661, Accuracy: 83.73%\n",
      "Batch 81, Loss: 0.925984, Accuracy: 83.70%\n",
      "Batch 82, Loss: 0.857739, Accuracy: 83.78%\n",
      "Batch 83, Loss: 0.837295, Accuracy: 83.87%\n",
      "Batch 84, Loss: 0.967405, Accuracy: 83.82%\n",
      "Batch 85, Loss: 0.927853, Accuracy: 83.82%\n",
      "Batch 86, Loss: 0.946039, Accuracy: 83.79%\n",
      "Batch 87, Loss: 0.913552, Accuracy: 83.78%\n",
      "Batch 88, Loss: 0.909334, Accuracy: 83.77%\n",
      "Batch 89, Loss: 0.880372, Accuracy: 83.83%\n",
      "Batch 90, Loss: 0.953004, Accuracy: 83.75%\n",
      "Batch 91, Loss: 0.928685, Accuracy: 83.72%\n",
      "Batch 92, Loss: 0.897834, Accuracy: 83.73%\n",
      "Batch 93, Loss: 0.917659, Accuracy: 83.70%\n",
      "Batch 94, Loss: 0.887910, Accuracy: 83.71%\n",
      "Batch 95, Loss: 0.955099, Accuracy: 83.63%\n",
      "Batch 96, Loss: 0.977723, Accuracy: 83.58%\n",
      "Batch 97, Loss: 0.943835, Accuracy: 83.52%\n",
      "Batch 98, Loss: 0.928250, Accuracy: 83.51%\n",
      "Batch 99, Loss: 0.838217, Accuracy: 83.60%\n",
      "Batch 100, Loss: 0.931295, Accuracy: 83.58%\n",
      "Batch 101, Loss: 0.922991, Accuracy: 83.56%\n",
      "Batch 102, Loss: 0.887917, Accuracy: 83.58%\n",
      "Batch 103, Loss: 0.901120, Accuracy: 83.59%\n",
      "Batch 104, Loss: 0.851489, Accuracy: 83.67%\n",
      "Batch 105, Loss: 0.858421, Accuracy: 83.72%\n",
      "Batch 106, Loss: 0.826855, Accuracy: 83.81%\n",
      "Batch 107, Loss: 0.886963, Accuracy: 83.85%\n",
      "Batch 108, Loss: 0.943883, Accuracy: 83.81%\n",
      "Batch 109, Loss: 0.874906, Accuracy: 83.83%\n",
      "Batch 110, Loss: 0.924294, Accuracy: 83.81%\n",
      "Batch 111, Loss: 0.824866, Accuracy: 83.90%\n",
      "Batch 112, Loss: 0.917170, Accuracy: 83.90%\n",
      "Batch 113, Loss: 1.027537, Accuracy: 83.79%\n",
      "Batch 114, Loss: 0.916185, Accuracy: 83.79%\n",
      "Batch 115, Loss: 0.879357, Accuracy: 83.79%\n",
      "Batch 116, Loss: 0.802584, Accuracy: 83.89%\n",
      "Batch 117, Loss: 0.937838, Accuracy: 83.85%\n",
      "Batch 118, Loss: 0.913868, Accuracy: 83.86%\n",
      "Batch 119, Loss: 0.949411, Accuracy: 83.82%\n",
      "Batch 120, Loss: 0.910711, Accuracy: 83.83%\n",
      "Batch 121, Loss: 0.868886, Accuracy: 83.86%\n",
      "Batch 122, Loss: 0.920583, Accuracy: 83.86%\n",
      "Batch 123, Loss: 0.852037, Accuracy: 83.90%\n",
      "Batch 124, Loss: 0.920813, Accuracy: 83.90%\n",
      "Batch 125, Loss: 0.902808, Accuracy: 83.90%\n",
      "Batch 126, Loss: 0.876857, Accuracy: 83.93%\n",
      "Batch 127, Loss: 0.930727, Accuracy: 83.91%\n",
      "Batch 128, Loss: 0.904483, Accuracy: 83.91%\n",
      "Batch 129, Loss: 0.879964, Accuracy: 83.94%\n",
      "Batch 130, Loss: 0.879835, Accuracy: 83.95%\n",
      "Batch 131, Loss: 0.905017, Accuracy: 83.96%\n",
      "Batch 132, Loss: 0.893980, Accuracy: 83.97%\n",
      "Batch 133, Loss: 0.899785, Accuracy: 83.96%\n",
      "Batch 134, Loss: 0.951457, Accuracy: 83.92%\n",
      "Batch 135, Loss: 0.910734, Accuracy: 83.91%\n",
      "Batch 136, Loss: 1.002592, Accuracy: 83.84%\n",
      "Batch 137, Loss: 0.917457, Accuracy: 83.83%\n",
      "Batch 138, Loss: 0.939800, Accuracy: 83.80%\n",
      "Batch 139, Loss: 0.890296, Accuracy: 83.79%\n",
      "Batch 140, Loss: 0.921005, Accuracy: 83.78%\n",
      "Batch 141, Loss: 0.886002, Accuracy: 83.81%\n",
      "Batch 142, Loss: 0.918676, Accuracy: 83.79%\n",
      "Batch 143, Loss: 0.951285, Accuracy: 83.75%\n",
      "Batch 144, Loss: 0.967394, Accuracy: 83.70%\n",
      "Batch 145, Loss: 0.909998, Accuracy: 83.70%\n",
      "Batch 146, Loss: 0.866071, Accuracy: 83.72%\n",
      "Batch 147, Loss: 0.914031, Accuracy: 83.72%\n",
      "Batch 148, Loss: 0.871632, Accuracy: 83.75%\n",
      "Batch 149, Loss: 0.876962, Accuracy: 83.79%\n",
      "Batch 150, Loss: 0.939906, Accuracy: 83.77%\n",
      "Batch 151, Loss: 0.997167, Accuracy: 83.70%\n",
      "Batch 152, Loss: 0.883836, Accuracy: 83.72%\n",
      "Batch 153, Loss: 0.841248, Accuracy: 83.76%\n",
      "Batch 154, Loss: 0.837066, Accuracy: 83.82%\n",
      "Batch 155, Loss: 0.890105, Accuracy: 83.84%\n",
      "Batch 156, Loss: 0.836045, Accuracy: 83.89%\n",
      "Batch 157, Loss: 0.917026, Accuracy: 83.89%\n",
      "Batch 158, Loss: 0.870596, Accuracy: 83.92%\n",
      "Batch 159, Loss: 0.912999, Accuracy: 83.92%\n",
      "Batch 160, Loss: 0.909047, Accuracy: 83.93%\n",
      "Batch 161, Loss: 0.978154, Accuracy: 83.89%\n",
      "Batch 162, Loss: 0.875127, Accuracy: 83.91%\n",
      "Batch 163, Loss: 0.863094, Accuracy: 83.94%\n",
      "Batch 164, Loss: 0.845341, Accuracy: 83.97%\n",
      "Batch 165, Loss: 0.932673, Accuracy: 83.96%\n",
      "Batch 166, Loss: 1.050298, Accuracy: 83.86%\n",
      "Batch 167, Loss: 0.919352, Accuracy: 83.86%\n",
      "Batch 168, Loss: 0.959087, Accuracy: 83.82%\n",
      "Batch 169, Loss: 0.943026, Accuracy: 83.80%\n",
      "Batch 170, Loss: 0.866980, Accuracy: 83.81%\n",
      "Batch 171, Loss: 0.896979, Accuracy: 83.82%\n",
      "Batch 172, Loss: 0.896018, Accuracy: 83.81%\n",
      "Batch 173, Loss: 0.898963, Accuracy: 83.82%\n",
      "Batch 174, Loss: 0.849157, Accuracy: 83.84%\n",
      "Batch 175, Loss: 0.927879, Accuracy: 83.83%\n",
      "Batch 176, Loss: 0.876720, Accuracy: 83.85%\n",
      "Batch 177, Loss: 0.883843, Accuracy: 83.87%\n",
      "Batch 178, Loss: 0.909947, Accuracy: 83.86%\n",
      "Batch 179, Loss: 0.915929, Accuracy: 83.86%\n",
      "Batch 180, Loss: 0.889130, Accuracy: 83.86%\n",
      "Batch 181, Loss: 0.942786, Accuracy: 83.84%\n",
      "Batch 182, Loss: 0.918896, Accuracy: 83.83%\n",
      "Batch 183, Loss: 0.976995, Accuracy: 83.79%\n",
      "Batch 184, Loss: 0.893985, Accuracy: 83.80%\n",
      "Batch 185, Loss: 0.942143, Accuracy: 83.77%\n",
      "Batch 186, Loss: 0.977273, Accuracy: 83.73%\n",
      "Batch 187, Loss: 0.876674, Accuracy: 83.75%\n",
      "Batch 188, Loss: 0.972499, Accuracy: 83.71%\n",
      "Batch 189, Loss: 0.863072, Accuracy: 83.73%\n",
      "Batch 190, Loss: 0.907091, Accuracy: 83.73%\n",
      "Batch 191, Loss: 0.872942, Accuracy: 83.75%\n",
      "Batch 192, Loss: 0.948321, Accuracy: 83.71%\n",
      "Batch 193, Loss: 0.840485, Accuracy: 83.74%\n",
      "Batch 194, Loss: 0.963086, Accuracy: 83.71%\n",
      "Batch 195, Loss: 0.910383, Accuracy: 83.70%\n",
      "Batch 196, Loss: 0.944901, Accuracy: 83.68%\n",
      "Batch 197, Loss: 0.889524, Accuracy: 83.68%\n",
      "Batch 198, Loss: 0.867566, Accuracy: 83.70%\n",
      "Batch 199, Loss: 0.929737, Accuracy: 83.68%\n",
      "Batch 200, Loss: 0.882120, Accuracy: 83.69%\n",
      "Batch 201, Loss: 0.938310, Accuracy: 83.67%\n",
      "Batch 202, Loss: 0.882311, Accuracy: 83.68%\n",
      "Batch 203, Loss: 0.946397, Accuracy: 83.67%\n",
      "Batch 204, Loss: 0.953114, Accuracy: 83.65%\n",
      "Batch 205, Loss: 0.925637, Accuracy: 83.64%\n",
      "Batch 206, Loss: 0.895591, Accuracy: 83.65%\n",
      "Batch 207, Loss: 0.878738, Accuracy: 83.67%\n",
      "Batch 208, Loss: 0.881152, Accuracy: 83.68%\n",
      "Batch 209, Loss: 0.900080, Accuracy: 83.69%\n",
      "Batch 210, Loss: 0.883913, Accuracy: 83.71%\n",
      "Batch 211, Loss: 0.853732, Accuracy: 83.74%\n",
      "Batch 212, Loss: 0.862125, Accuracy: 83.76%\n",
      "Batch 213, Loss: 0.884328, Accuracy: 83.78%\n",
      "Training - Epoch 58, Loss: 0.907090, Accuracy: 83.78%\n",
      "Validation Batch 1, Loss: 0.896245, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.915161, Accuracy: 85.16%\n",
      "Validation Batch 3, Loss: 1.000022, Accuracy: 81.25%\n",
      "Validation Batch 4, Loss: 0.921992, Accuracy: 81.25%\n",
      "Validation Batch 5, Loss: 0.902498, Accuracy: 82.19%\n",
      "Validation Batch 6, Loss: 0.870549, Accuracy: 83.33%\n",
      "Validation Batch 7, Loss: 0.899871, Accuracy: 83.26%\n",
      "Validation Batch 8, Loss: 0.984843, Accuracy: 82.23%\n",
      "Validation Batch 9, Loss: 0.946652, Accuracy: 82.29%\n",
      "Validation Batch 10, Loss: 0.943185, Accuracy: 81.88%\n",
      "Validation Batch 11, Loss: 0.922600, Accuracy: 81.68%\n",
      "Validation Batch 12, Loss: 0.921969, Accuracy: 81.64%\n",
      "Validation Batch 13, Loss: 0.912857, Accuracy: 81.61%\n",
      "Validation Batch 14, Loss: 0.951122, Accuracy: 81.47%\n",
      "Validation Batch 15, Loss: 0.901827, Accuracy: 81.67%\n",
      "Validation Batch 16, Loss: 0.920657, Accuracy: 81.64%\n",
      "Validation Batch 17, Loss: 0.968382, Accuracy: 81.43%\n",
      "Validation Batch 18, Loss: 0.887699, Accuracy: 81.68%\n",
      "Validation Batch 19, Loss: 0.951280, Accuracy: 81.66%\n",
      "Validation Batch 20, Loss: 1.011584, Accuracy: 81.17%\n",
      "Validation Batch 21, Loss: 0.974317, Accuracy: 80.95%\n",
      "Validation Batch 22, Loss: 0.927876, Accuracy: 81.11%\n",
      "Validation Batch 23, Loss: 0.957824, Accuracy: 80.98%\n",
      "Validation Batch 24, Loss: 0.933933, Accuracy: 80.99%\n",
      "Validation Batch 25, Loss: 0.899714, Accuracy: 81.25%\n",
      "Validation Batch 26, Loss: 0.921730, Accuracy: 81.31%\n",
      "Validation Batch 27, Loss: 0.960123, Accuracy: 81.09%\n",
      "Validation - Epoch 58, Loss: 0.933575, Accuracy: 81.09%\n",
      "Patience—2\n",
      "Epoch 59\n",
      "Batch 1, Loss: 0.933913, Accuracy: 79.69%\n",
      "Batch 2, Loss: 0.859783, Accuracy: 84.38%\n",
      "Batch 3, Loss: 0.863672, Accuracy: 85.42%\n",
      "Batch 4, Loss: 0.820289, Accuracy: 87.50%\n",
      "Batch 5, Loss: 0.884060, Accuracy: 87.50%\n",
      "Batch 6, Loss: 0.828554, Accuracy: 88.28%\n",
      "Batch 7, Loss: 0.937721, Accuracy: 87.28%\n",
      "Batch 8, Loss: 0.858773, Accuracy: 87.50%\n",
      "Batch 9, Loss: 0.891877, Accuracy: 87.15%\n",
      "Batch 10, Loss: 0.903113, Accuracy: 86.88%\n",
      "Batch 11, Loss: 0.882330, Accuracy: 86.93%\n",
      "Batch 12, Loss: 0.926777, Accuracy: 86.33%\n",
      "Batch 13, Loss: 0.838800, Accuracy: 86.78%\n",
      "Batch 14, Loss: 0.907336, Accuracy: 86.61%\n",
      "Batch 15, Loss: 0.905683, Accuracy: 86.46%\n",
      "Batch 16, Loss: 0.930957, Accuracy: 86.23%\n",
      "Batch 17, Loss: 0.872633, Accuracy: 86.31%\n",
      "Batch 18, Loss: 0.900403, Accuracy: 86.20%\n",
      "Batch 19, Loss: 0.888580, Accuracy: 86.18%\n",
      "Batch 20, Loss: 0.826612, Accuracy: 86.48%\n",
      "Batch 21, Loss: 0.973065, Accuracy: 86.09%\n",
      "Batch 22, Loss: 0.904419, Accuracy: 85.94%\n",
      "Batch 23, Loss: 0.918709, Accuracy: 85.73%\n",
      "Batch 24, Loss: 0.974845, Accuracy: 85.29%\n",
      "Batch 25, Loss: 0.833347, Accuracy: 85.50%\n",
      "Batch 26, Loss: 0.972533, Accuracy: 85.16%\n",
      "Batch 27, Loss: 0.898097, Accuracy: 85.13%\n",
      "Batch 28, Loss: 0.922715, Accuracy: 84.93%\n",
      "Batch 29, Loss: 0.937360, Accuracy: 84.81%\n",
      "Batch 30, Loss: 0.956092, Accuracy: 84.64%\n",
      "Batch 31, Loss: 0.952556, Accuracy: 84.43%\n",
      "Batch 32, Loss: 0.964240, Accuracy: 84.23%\n",
      "Batch 33, Loss: 0.939360, Accuracy: 84.14%\n",
      "Batch 34, Loss: 0.992279, Accuracy: 83.82%\n",
      "Batch 35, Loss: 0.911027, Accuracy: 83.84%\n",
      "Batch 36, Loss: 0.852842, Accuracy: 83.98%\n",
      "Batch 37, Loss: 0.921141, Accuracy: 83.95%\n",
      "Batch 38, Loss: 0.848238, Accuracy: 84.09%\n",
      "Batch 39, Loss: 0.820875, Accuracy: 84.33%\n",
      "Batch 40, Loss: 0.844872, Accuracy: 84.45%\n",
      "Batch 41, Loss: 0.918073, Accuracy: 84.38%\n",
      "Batch 42, Loss: 0.905592, Accuracy: 84.41%\n",
      "Batch 43, Loss: 0.874694, Accuracy: 84.48%\n",
      "Batch 44, Loss: 0.864354, Accuracy: 84.62%\n",
      "Batch 45, Loss: 0.982172, Accuracy: 84.44%\n",
      "Batch 46, Loss: 0.919474, Accuracy: 84.38%\n",
      "Batch 47, Loss: 0.816240, Accuracy: 84.57%\n",
      "Batch 48, Loss: 0.884627, Accuracy: 84.60%\n",
      "Batch 49, Loss: 0.873258, Accuracy: 84.66%\n",
      "Batch 50, Loss: 0.927629, Accuracy: 84.59%\n",
      "Batch 51, Loss: 0.860151, Accuracy: 84.68%\n",
      "Batch 52, Loss: 0.835083, Accuracy: 84.83%\n",
      "Batch 53, Loss: 0.800615, Accuracy: 85.05%\n",
      "Batch 54, Loss: 0.895206, Accuracy: 85.07%\n",
      "Batch 55, Loss: 0.930097, Accuracy: 85.00%\n",
      "Batch 56, Loss: 0.970178, Accuracy: 84.88%\n",
      "Batch 57, Loss: 0.915587, Accuracy: 84.84%\n",
      "Batch 58, Loss: 0.789569, Accuracy: 85.05%\n",
      "Batch 59, Loss: 0.901381, Accuracy: 85.04%\n",
      "Batch 60, Loss: 0.885219, Accuracy: 85.00%\n",
      "Batch 61, Loss: 0.913589, Accuracy: 84.99%\n",
      "Batch 62, Loss: 0.903630, Accuracy: 84.98%\n",
      "Batch 63, Loss: 0.987351, Accuracy: 84.82%\n",
      "Batch 64, Loss: 0.957577, Accuracy: 84.72%\n",
      "Batch 65, Loss: 0.832284, Accuracy: 84.81%\n",
      "Batch 66, Loss: 0.879573, Accuracy: 84.82%\n",
      "Batch 67, Loss: 0.846556, Accuracy: 84.91%\n",
      "Batch 68, Loss: 0.885237, Accuracy: 84.88%\n",
      "Batch 69, Loss: 0.827285, Accuracy: 84.96%\n",
      "Batch 70, Loss: 0.819556, Accuracy: 85.07%\n",
      "Batch 71, Loss: 0.935337, Accuracy: 85.01%\n",
      "Batch 72, Loss: 0.886240, Accuracy: 85.03%\n",
      "Batch 73, Loss: 0.960828, Accuracy: 84.93%\n",
      "Batch 74, Loss: 0.834432, Accuracy: 85.03%\n",
      "Batch 75, Loss: 0.895483, Accuracy: 85.06%\n",
      "Batch 76, Loss: 0.924611, Accuracy: 85.05%\n",
      "Batch 77, Loss: 0.867578, Accuracy: 85.09%\n",
      "Batch 78, Loss: 0.884178, Accuracy: 85.14%\n",
      "Batch 79, Loss: 0.863689, Accuracy: 85.19%\n",
      "Batch 80, Loss: 0.853572, Accuracy: 85.23%\n",
      "Batch 81, Loss: 0.867661, Accuracy: 85.30%\n",
      "Batch 82, Loss: 0.878176, Accuracy: 85.33%\n",
      "Batch 83, Loss: 0.940887, Accuracy: 85.28%\n",
      "Batch 84, Loss: 0.967462, Accuracy: 85.19%\n",
      "Batch 85, Loss: 0.939337, Accuracy: 85.13%\n",
      "Batch 86, Loss: 0.887735, Accuracy: 85.12%\n",
      "Batch 87, Loss: 0.891511, Accuracy: 85.13%\n",
      "Batch 88, Loss: 0.881779, Accuracy: 85.14%\n",
      "Batch 89, Loss: 0.882082, Accuracy: 85.15%\n",
      "Batch 90, Loss: 0.902755, Accuracy: 85.14%\n",
      "Batch 91, Loss: 0.907731, Accuracy: 85.11%\n",
      "Batch 92, Loss: 0.914042, Accuracy: 85.11%\n",
      "Batch 93, Loss: 0.860762, Accuracy: 85.13%\n",
      "Batch 94, Loss: 0.964374, Accuracy: 85.04%\n",
      "Batch 95, Loss: 0.962464, Accuracy: 84.95%\n",
      "Batch 96, Loss: 0.873930, Accuracy: 84.98%\n",
      "Batch 97, Loss: 0.970767, Accuracy: 84.87%\n",
      "Batch 98, Loss: 0.968451, Accuracy: 84.79%\n",
      "Batch 99, Loss: 0.968905, Accuracy: 84.69%\n",
      "Batch 100, Loss: 0.869363, Accuracy: 84.73%\n",
      "Batch 101, Loss: 0.899320, Accuracy: 84.72%\n",
      "Batch 102, Loss: 0.950149, Accuracy: 84.65%\n",
      "Batch 103, Loss: 0.857773, Accuracy: 84.71%\n",
      "Batch 104, Loss: 0.870150, Accuracy: 84.74%\n",
      "Batch 105, Loss: 0.834424, Accuracy: 84.81%\n",
      "Batch 106, Loss: 1.007554, Accuracy: 84.70%\n",
      "Batch 107, Loss: 0.890953, Accuracy: 84.71%\n",
      "Batch 108, Loss: 0.823215, Accuracy: 84.79%\n",
      "Batch 109, Loss: 0.914459, Accuracy: 84.76%\n",
      "Batch 110, Loss: 0.950887, Accuracy: 84.70%\n",
      "Batch 111, Loss: 0.908913, Accuracy: 84.67%\n",
      "Batch 112, Loss: 0.903322, Accuracy: 84.67%\n",
      "Batch 113, Loss: 0.956789, Accuracy: 84.60%\n",
      "Batch 114, Loss: 0.859928, Accuracy: 84.62%\n",
      "Batch 115, Loss: 0.882255, Accuracy: 84.62%\n",
      "Batch 116, Loss: 0.909933, Accuracy: 84.60%\n",
      "Batch 117, Loss: 0.926992, Accuracy: 84.58%\n",
      "Batch 118, Loss: 0.911533, Accuracy: 84.57%\n",
      "Batch 119, Loss: 0.948762, Accuracy: 84.52%\n",
      "Batch 120, Loss: 0.872782, Accuracy: 84.53%\n",
      "Batch 121, Loss: 0.837559, Accuracy: 84.59%\n",
      "Batch 122, Loss: 0.913526, Accuracy: 84.58%\n",
      "Batch 123, Loss: 1.006421, Accuracy: 84.49%\n",
      "Batch 124, Loss: 0.835717, Accuracy: 84.55%\n",
      "Batch 125, Loss: 0.962816, Accuracy: 84.50%\n",
      "Batch 126, Loss: 0.943501, Accuracy: 84.47%\n",
      "Batch 127, Loss: 0.924366, Accuracy: 84.46%\n",
      "Batch 128, Loss: 0.908957, Accuracy: 84.46%\n",
      "Batch 129, Loss: 0.915403, Accuracy: 84.46%\n",
      "Batch 130, Loss: 0.899958, Accuracy: 84.45%\n",
      "Batch 131, Loss: 0.902996, Accuracy: 84.46%\n",
      "Batch 132, Loss: 0.867753, Accuracy: 84.46%\n",
      "Batch 133, Loss: 0.881313, Accuracy: 84.48%\n",
      "Batch 134, Loss: 0.843047, Accuracy: 84.51%\n",
      "Batch 135, Loss: 1.025369, Accuracy: 84.41%\n",
      "Batch 136, Loss: 0.931711, Accuracy: 84.39%\n",
      "Batch 137, Loss: 0.885214, Accuracy: 84.40%\n",
      "Batch 138, Loss: 0.951282, Accuracy: 84.34%\n",
      "Batch 139, Loss: 0.936066, Accuracy: 84.32%\n",
      "Batch 140, Loss: 0.833674, Accuracy: 84.38%\n",
      "Batch 141, Loss: 0.944646, Accuracy: 84.34%\n",
      "Batch 142, Loss: 0.897090, Accuracy: 84.35%\n",
      "Batch 143, Loss: 0.812412, Accuracy: 84.42%\n",
      "Batch 144, Loss: 0.851573, Accuracy: 84.46%\n",
      "Batch 145, Loss: 0.928889, Accuracy: 84.44%\n",
      "Batch 146, Loss: 0.907359, Accuracy: 84.43%\n",
      "Batch 147, Loss: 0.957003, Accuracy: 84.39%\n",
      "Batch 148, Loss: 0.934747, Accuracy: 84.36%\n",
      "Batch 149, Loss: 0.942851, Accuracy: 84.32%\n",
      "Batch 150, Loss: 0.850287, Accuracy: 84.35%\n",
      "Batch 151, Loss: 0.895535, Accuracy: 84.35%\n",
      "Batch 152, Loss: 0.951237, Accuracy: 84.32%\n",
      "Batch 153, Loss: 0.948868, Accuracy: 84.28%\n",
      "Batch 154, Loss: 0.907071, Accuracy: 84.28%\n",
      "Batch 155, Loss: 0.937003, Accuracy: 84.25%\n",
      "Batch 156, Loss: 0.932741, Accuracy: 84.23%\n",
      "Batch 157, Loss: 0.874515, Accuracy: 84.25%\n",
      "Batch 158, Loss: 0.903722, Accuracy: 84.25%\n",
      "Batch 159, Loss: 0.876760, Accuracy: 84.27%\n",
      "Batch 160, Loss: 0.912218, Accuracy: 84.26%\n",
      "Batch 161, Loss: 0.938364, Accuracy: 84.23%\n",
      "Batch 162, Loss: 0.847367, Accuracy: 84.27%\n",
      "Batch 163, Loss: 0.812626, Accuracy: 84.33%\n",
      "Batch 164, Loss: 0.868878, Accuracy: 84.36%\n",
      "Batch 165, Loss: 0.921168, Accuracy: 84.35%\n",
      "Batch 166, Loss: 0.894470, Accuracy: 84.37%\n",
      "Batch 167, Loss: 0.945084, Accuracy: 84.35%\n",
      "Batch 168, Loss: 0.885666, Accuracy: 84.35%\n",
      "Batch 169, Loss: 0.899388, Accuracy: 84.35%\n",
      "Batch 170, Loss: 0.832359, Accuracy: 84.39%\n",
      "Batch 171, Loss: 0.991703, Accuracy: 84.34%\n",
      "Batch 172, Loss: 0.896105, Accuracy: 84.34%\n",
      "Batch 173, Loss: 0.912229, Accuracy: 84.34%\n",
      "Batch 174, Loss: 0.878003, Accuracy: 84.35%\n",
      "Batch 175, Loss: 0.857467, Accuracy: 84.38%\n",
      "Batch 176, Loss: 0.881707, Accuracy: 84.39%\n",
      "Batch 177, Loss: 0.832032, Accuracy: 84.44%\n",
      "Batch 178, Loss: 0.938072, Accuracy: 84.42%\n",
      "Batch 179, Loss: 0.907025, Accuracy: 84.41%\n",
      "Batch 180, Loss: 0.843214, Accuracy: 84.44%\n",
      "Batch 181, Loss: 0.885603, Accuracy: 84.44%\n",
      "Batch 182, Loss: 0.925648, Accuracy: 84.43%\n",
      "Batch 183, Loss: 0.845083, Accuracy: 84.46%\n",
      "Batch 184, Loss: 0.853240, Accuracy: 84.49%\n",
      "Batch 185, Loss: 0.851050, Accuracy: 84.51%\n",
      "Batch 186, Loss: 0.912226, Accuracy: 84.51%\n",
      "Batch 187, Loss: 0.974916, Accuracy: 84.46%\n",
      "Batch 188, Loss: 0.903528, Accuracy: 84.45%\n",
      "Batch 189, Loss: 0.895777, Accuracy: 84.46%\n",
      "Batch 190, Loss: 0.893719, Accuracy: 84.47%\n",
      "Batch 191, Loss: 0.900387, Accuracy: 84.46%\n",
      "Batch 192, Loss: 0.893149, Accuracy: 84.46%\n",
      "Batch 193, Loss: 0.867206, Accuracy: 84.48%\n",
      "Batch 194, Loss: 0.913633, Accuracy: 84.47%\n",
      "Batch 195, Loss: 0.915116, Accuracy: 84.46%\n",
      "Batch 196, Loss: 0.909211, Accuracy: 84.45%\n",
      "Batch 197, Loss: 0.906922, Accuracy: 84.44%\n",
      "Batch 198, Loss: 0.925519, Accuracy: 84.41%\n",
      "Batch 199, Loss: 0.792412, Accuracy: 84.48%\n",
      "Batch 200, Loss: 0.896850, Accuracy: 84.47%\n",
      "Batch 201, Loss: 0.931000, Accuracy: 84.46%\n",
      "Batch 202, Loss: 0.892989, Accuracy: 84.47%\n",
      "Batch 203, Loss: 0.909089, Accuracy: 84.47%\n",
      "Batch 204, Loss: 0.910046, Accuracy: 84.47%\n",
      "Batch 205, Loss: 0.901778, Accuracy: 84.46%\n",
      "Batch 206, Loss: 0.845547, Accuracy: 84.49%\n",
      "Batch 207, Loss: 0.844475, Accuracy: 84.52%\n",
      "Batch 208, Loss: 0.914512, Accuracy: 84.51%\n",
      "Batch 209, Loss: 0.992239, Accuracy: 84.45%\n",
      "Batch 210, Loss: 0.858865, Accuracy: 84.48%\n",
      "Batch 211, Loss: 0.908441, Accuracy: 84.47%\n",
      "Batch 212, Loss: 0.896339, Accuracy: 84.48%\n",
      "Batch 213, Loss: 0.907889, Accuracy: 84.48%\n",
      "Training - Epoch 59, Loss: 0.899620, Accuracy: 84.48%\n",
      "Validation Batch 1, Loss: 0.885590, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.887010, Accuracy: 86.72%\n",
      "Validation Batch 3, Loss: 0.961535, Accuracy: 84.90%\n",
      "Validation Batch 4, Loss: 0.906131, Accuracy: 84.38%\n",
      "Validation Batch 5, Loss: 0.889111, Accuracy: 85.00%\n",
      "Validation Batch 6, Loss: 0.852355, Accuracy: 85.94%\n",
      "Validation Batch 7, Loss: 0.872347, Accuracy: 86.61%\n",
      "Validation Batch 8, Loss: 0.972711, Accuracy: 85.16%\n",
      "Validation Batch 9, Loss: 0.933207, Accuracy: 85.07%\n",
      "Validation Batch 10, Loss: 0.927361, Accuracy: 84.53%\n",
      "Validation Batch 11, Loss: 0.904674, Accuracy: 84.52%\n",
      "Validation Batch 12, Loss: 0.895621, Accuracy: 84.51%\n",
      "Validation Batch 13, Loss: 0.897681, Accuracy: 84.38%\n",
      "Validation Batch 14, Loss: 0.931055, Accuracy: 84.15%\n",
      "Validation Batch 15, Loss: 0.892912, Accuracy: 84.17%\n",
      "Validation Batch 16, Loss: 0.905632, Accuracy: 84.08%\n",
      "Validation Batch 17, Loss: 0.938556, Accuracy: 83.92%\n",
      "Validation Batch 18, Loss: 0.872224, Accuracy: 84.03%\n",
      "Validation Batch 19, Loss: 0.929022, Accuracy: 83.88%\n",
      "Validation Batch 20, Loss: 0.984495, Accuracy: 83.36%\n",
      "Validation Batch 21, Loss: 0.938997, Accuracy: 83.26%\n",
      "Validation Batch 22, Loss: 0.910678, Accuracy: 83.17%\n",
      "Validation Batch 23, Loss: 0.932884, Accuracy: 83.02%\n",
      "Validation Batch 24, Loss: 0.925253, Accuracy: 82.94%\n",
      "Validation Batch 25, Loss: 0.881416, Accuracy: 83.12%\n",
      "Validation Batch 26, Loss: 0.897712, Accuracy: 83.23%\n",
      "Validation Batch 27, Loss: 0.915922, Accuracy: 83.26%\n",
      "Validation - Epoch 59, Loss: 0.912670, Accuracy: 83.26%\n",
      "Patience—0\n",
      "Epoch 60\n",
      "Batch 1, Loss: 0.909036, Accuracy: 84.38%\n",
      "Batch 2, Loss: 0.845536, Accuracy: 87.50%\n",
      "Batch 3, Loss: 0.901003, Accuracy: 85.94%\n",
      "Batch 4, Loss: 0.919968, Accuracy: 85.55%\n",
      "Batch 5, Loss: 0.990399, Accuracy: 83.12%\n",
      "Batch 6, Loss: 0.843368, Accuracy: 84.64%\n",
      "Batch 7, Loss: 0.843986, Accuracy: 85.71%\n",
      "Batch 8, Loss: 0.862530, Accuracy: 85.94%\n",
      "Batch 9, Loss: 0.852824, Accuracy: 86.28%\n",
      "Batch 10, Loss: 0.883487, Accuracy: 86.25%\n",
      "Batch 11, Loss: 0.969757, Accuracy: 85.51%\n",
      "Batch 12, Loss: 0.865070, Accuracy: 85.68%\n",
      "Batch 13, Loss: 0.950107, Accuracy: 85.34%\n",
      "Batch 14, Loss: 0.901881, Accuracy: 85.38%\n",
      "Batch 15, Loss: 0.880090, Accuracy: 85.31%\n",
      "Batch 16, Loss: 0.925879, Accuracy: 85.06%\n",
      "Batch 17, Loss: 0.897207, Accuracy: 84.93%\n",
      "Batch 18, Loss: 0.893057, Accuracy: 84.98%\n",
      "Batch 19, Loss: 0.938713, Accuracy: 84.79%\n",
      "Batch 20, Loss: 0.949081, Accuracy: 84.53%\n",
      "Batch 21, Loss: 0.883048, Accuracy: 84.60%\n",
      "Batch 22, Loss: 0.925499, Accuracy: 84.45%\n",
      "Batch 23, Loss: 0.891325, Accuracy: 84.44%\n",
      "Batch 24, Loss: 0.922460, Accuracy: 84.38%\n",
      "Batch 25, Loss: 0.900936, Accuracy: 84.38%\n",
      "Batch 26, Loss: 0.922348, Accuracy: 84.31%\n",
      "Batch 27, Loss: 0.997667, Accuracy: 83.97%\n",
      "Batch 28, Loss: 0.998944, Accuracy: 83.59%\n",
      "Batch 29, Loss: 0.913109, Accuracy: 83.57%\n",
      "Batch 30, Loss: 0.841244, Accuracy: 83.80%\n",
      "Batch 31, Loss: 0.931189, Accuracy: 83.67%\n",
      "Batch 32, Loss: 0.913632, Accuracy: 83.69%\n",
      "Batch 33, Loss: 0.897318, Accuracy: 83.76%\n",
      "Batch 34, Loss: 0.835346, Accuracy: 83.96%\n",
      "Batch 35, Loss: 0.953534, Accuracy: 83.79%\n",
      "Batch 36, Loss: 0.894795, Accuracy: 83.81%\n",
      "Batch 37, Loss: 0.915954, Accuracy: 83.83%\n",
      "Batch 38, Loss: 0.950343, Accuracy: 83.72%\n",
      "Batch 39, Loss: 0.920162, Accuracy: 83.73%\n",
      "Batch 40, Loss: 0.907100, Accuracy: 83.75%\n",
      "Batch 41, Loss: 0.898134, Accuracy: 83.80%\n",
      "Batch 42, Loss: 0.888516, Accuracy: 83.85%\n",
      "Batch 43, Loss: 0.829548, Accuracy: 84.08%\n",
      "Batch 44, Loss: 0.854462, Accuracy: 84.20%\n",
      "Batch 45, Loss: 0.959523, Accuracy: 84.06%\n",
      "Batch 46, Loss: 0.869360, Accuracy: 84.14%\n",
      "Batch 47, Loss: 0.949690, Accuracy: 84.08%\n",
      "Batch 48, Loss: 0.865978, Accuracy: 84.15%\n",
      "Batch 49, Loss: 0.864079, Accuracy: 84.22%\n",
      "Batch 50, Loss: 0.871781, Accuracy: 84.28%\n",
      "Batch 51, Loss: 0.904860, Accuracy: 84.25%\n",
      "Batch 52, Loss: 0.874907, Accuracy: 84.25%\n",
      "Batch 53, Loss: 0.929218, Accuracy: 84.20%\n",
      "Batch 54, Loss: 0.906762, Accuracy: 84.17%\n",
      "Batch 55, Loss: 0.780332, Accuracy: 84.40%\n",
      "Batch 56, Loss: 0.901828, Accuracy: 84.38%\n",
      "Batch 57, Loss: 0.879136, Accuracy: 84.40%\n",
      "Batch 58, Loss: 0.960522, Accuracy: 84.27%\n",
      "Batch 59, Loss: 0.860202, Accuracy: 84.35%\n",
      "Batch 60, Loss: 0.907640, Accuracy: 84.32%\n",
      "Batch 61, Loss: 0.931148, Accuracy: 84.30%\n",
      "Batch 62, Loss: 0.920796, Accuracy: 84.25%\n",
      "Batch 63, Loss: 0.951792, Accuracy: 84.20%\n",
      "Batch 64, Loss: 0.857186, Accuracy: 84.28%\n",
      "Batch 65, Loss: 0.915401, Accuracy: 84.23%\n",
      "Batch 66, Loss: 0.865378, Accuracy: 84.28%\n",
      "Batch 67, Loss: 0.858448, Accuracy: 84.38%\n",
      "Batch 68, Loss: 0.836225, Accuracy: 84.49%\n",
      "Batch 69, Loss: 0.911508, Accuracy: 84.49%\n",
      "Batch 70, Loss: 0.882381, Accuracy: 84.51%\n",
      "Batch 71, Loss: 0.912420, Accuracy: 84.49%\n",
      "Batch 72, Loss: 0.857665, Accuracy: 84.55%\n",
      "Batch 73, Loss: 0.950237, Accuracy: 84.46%\n",
      "Batch 74, Loss: 0.912328, Accuracy: 84.44%\n",
      "Batch 75, Loss: 0.920211, Accuracy: 84.42%\n",
      "Batch 76, Loss: 0.894316, Accuracy: 84.44%\n",
      "Batch 77, Loss: 0.840906, Accuracy: 84.52%\n",
      "Batch 78, Loss: 0.899801, Accuracy: 84.54%\n",
      "Batch 79, Loss: 0.957107, Accuracy: 84.47%\n",
      "Batch 80, Loss: 0.920287, Accuracy: 84.45%\n",
      "Batch 81, Loss: 0.948325, Accuracy: 84.38%\n",
      "Batch 82, Loss: 0.875269, Accuracy: 84.41%\n",
      "Batch 83, Loss: 0.874209, Accuracy: 84.41%\n",
      "Batch 84, Loss: 0.907052, Accuracy: 84.38%\n",
      "Batch 85, Loss: 0.914156, Accuracy: 84.36%\n",
      "Batch 86, Loss: 0.896131, Accuracy: 84.36%\n",
      "Batch 87, Loss: 0.874531, Accuracy: 84.38%\n",
      "Batch 88, Loss: 0.880874, Accuracy: 84.39%\n",
      "Batch 89, Loss: 0.913212, Accuracy: 84.39%\n",
      "Batch 90, Loss: 0.930698, Accuracy: 84.34%\n",
      "Batch 91, Loss: 0.868393, Accuracy: 84.38%\n",
      "Batch 92, Loss: 0.923218, Accuracy: 84.36%\n",
      "Batch 93, Loss: 0.923865, Accuracy: 84.31%\n",
      "Batch 94, Loss: 0.863223, Accuracy: 84.36%\n",
      "Batch 95, Loss: 0.868278, Accuracy: 84.39%\n",
      "Batch 96, Loss: 0.852678, Accuracy: 84.46%\n",
      "Batch 97, Loss: 0.884944, Accuracy: 84.47%\n",
      "Batch 98, Loss: 0.888547, Accuracy: 84.50%\n",
      "Batch 99, Loss: 0.955929, Accuracy: 84.44%\n",
      "Batch 100, Loss: 0.897797, Accuracy: 84.44%\n",
      "Batch 101, Loss: 0.866684, Accuracy: 84.48%\n",
      "Batch 102, Loss: 0.894362, Accuracy: 84.48%\n",
      "Batch 103, Loss: 0.922445, Accuracy: 84.45%\n",
      "Batch 104, Loss: 0.875363, Accuracy: 84.48%\n",
      "Batch 105, Loss: 0.956751, Accuracy: 84.43%\n",
      "Batch 106, Loss: 0.944531, Accuracy: 84.39%\n",
      "Batch 107, Loss: 0.959895, Accuracy: 84.33%\n",
      "Batch 108, Loss: 0.899556, Accuracy: 84.35%\n",
      "Batch 109, Loss: 0.921960, Accuracy: 84.35%\n",
      "Batch 110, Loss: 0.948516, Accuracy: 84.30%\n",
      "Batch 111, Loss: 0.942179, Accuracy: 84.26%\n",
      "Batch 112, Loss: 0.920375, Accuracy: 84.25%\n",
      "Batch 113, Loss: 0.977317, Accuracy: 84.18%\n",
      "Batch 114, Loss: 0.928118, Accuracy: 84.16%\n",
      "Batch 115, Loss: 0.877819, Accuracy: 84.18%\n",
      "Batch 116, Loss: 0.876645, Accuracy: 84.23%\n",
      "Batch 117, Loss: 0.878790, Accuracy: 84.25%\n",
      "Batch 118, Loss: 0.823386, Accuracy: 84.32%\n",
      "Batch 119, Loss: 0.924676, Accuracy: 84.31%\n",
      "Batch 120, Loss: 0.927734, Accuracy: 84.28%\n",
      "Batch 121, Loss: 0.881973, Accuracy: 84.30%\n",
      "Batch 122, Loss: 0.973054, Accuracy: 84.25%\n",
      "Batch 123, Loss: 0.854758, Accuracy: 84.30%\n",
      "Batch 124, Loss: 0.845770, Accuracy: 84.35%\n",
      "Batch 125, Loss: 0.863299, Accuracy: 84.39%\n",
      "Batch 126, Loss: 0.903176, Accuracy: 84.39%\n",
      "Batch 127, Loss: 0.950166, Accuracy: 84.35%\n",
      "Batch 128, Loss: 0.941102, Accuracy: 84.31%\n",
      "Batch 129, Loss: 0.865602, Accuracy: 84.34%\n",
      "Batch 130, Loss: 0.917519, Accuracy: 84.33%\n",
      "Batch 131, Loss: 0.938321, Accuracy: 84.29%\n",
      "Batch 132, Loss: 0.863762, Accuracy: 84.32%\n",
      "Batch 133, Loss: 0.842222, Accuracy: 84.36%\n",
      "Batch 134, Loss: 0.864204, Accuracy: 84.40%\n",
      "Batch 135, Loss: 0.896780, Accuracy: 84.39%\n",
      "Batch 136, Loss: 0.839494, Accuracy: 84.43%\n",
      "Batch 137, Loss: 0.890095, Accuracy: 84.45%\n",
      "Batch 138, Loss: 0.873886, Accuracy: 84.47%\n",
      "Batch 139, Loss: 0.853481, Accuracy: 84.51%\n",
      "Batch 140, Loss: 0.960835, Accuracy: 84.46%\n",
      "Batch 141, Loss: 0.869939, Accuracy: 84.49%\n",
      "Batch 142, Loss: 0.826433, Accuracy: 84.54%\n",
      "Batch 143, Loss: 0.893609, Accuracy: 84.54%\n",
      "Batch 144, Loss: 0.912023, Accuracy: 84.54%\n",
      "Batch 145, Loss: 0.829147, Accuracy: 84.60%\n",
      "Batch 146, Loss: 0.879514, Accuracy: 84.61%\n",
      "Batch 147, Loss: 0.894105, Accuracy: 84.61%\n",
      "Batch 148, Loss: 0.916139, Accuracy: 84.60%\n",
      "Batch 149, Loss: 0.911903, Accuracy: 84.60%\n",
      "Batch 150, Loss: 0.917080, Accuracy: 84.56%\n",
      "Batch 151, Loss: 0.964664, Accuracy: 84.52%\n",
      "Batch 152, Loss: 0.866711, Accuracy: 84.54%\n",
      "Batch 153, Loss: 0.842641, Accuracy: 84.58%\n",
      "Batch 154, Loss: 0.943864, Accuracy: 84.56%\n",
      "Batch 155, Loss: 0.953202, Accuracy: 84.52%\n",
      "Batch 156, Loss: 0.897729, Accuracy: 84.54%\n",
      "Batch 157, Loss: 0.892978, Accuracy: 84.54%\n",
      "Batch 158, Loss: 0.891675, Accuracy: 84.54%\n",
      "Batch 159, Loss: 0.905903, Accuracy: 84.53%\n",
      "Batch 160, Loss: 0.951137, Accuracy: 84.49%\n",
      "Batch 161, Loss: 0.842720, Accuracy: 84.52%\n",
      "Batch 162, Loss: 0.820381, Accuracy: 84.57%\n",
      "Batch 163, Loss: 0.834004, Accuracy: 84.61%\n",
      "Batch 164, Loss: 0.868629, Accuracy: 84.61%\n",
      "Batch 165, Loss: 0.880013, Accuracy: 84.63%\n",
      "Batch 166, Loss: 0.916310, Accuracy: 84.61%\n",
      "Batch 167, Loss: 0.910948, Accuracy: 84.60%\n",
      "Batch 168, Loss: 0.935854, Accuracy: 84.58%\n",
      "Batch 169, Loss: 0.902721, Accuracy: 84.58%\n",
      "Batch 170, Loss: 0.863891, Accuracy: 84.60%\n",
      "Batch 171, Loss: 0.901149, Accuracy: 84.59%\n",
      "Batch 172, Loss: 0.985467, Accuracy: 84.54%\n",
      "Batch 173, Loss: 0.858110, Accuracy: 84.56%\n",
      "Batch 174, Loss: 0.955890, Accuracy: 84.52%\n",
      "Batch 175, Loss: 0.844565, Accuracy: 84.54%\n",
      "Batch 176, Loss: 0.868433, Accuracy: 84.56%\n",
      "Batch 177, Loss: 0.887278, Accuracy: 84.57%\n",
      "Batch 178, Loss: 0.894514, Accuracy: 84.57%\n",
      "Batch 179, Loss: 0.922422, Accuracy: 84.56%\n",
      "Batch 180, Loss: 0.856222, Accuracy: 84.58%\n",
      "Batch 181, Loss: 0.908705, Accuracy: 84.58%\n",
      "Batch 182, Loss: 0.867291, Accuracy: 84.59%\n",
      "Batch 183, Loss: 0.867022, Accuracy: 84.61%\n",
      "Batch 184, Loss: 0.887090, Accuracy: 84.62%\n",
      "Batch 185, Loss: 0.935869, Accuracy: 84.60%\n",
      "Batch 186, Loss: 0.945499, Accuracy: 84.59%\n",
      "Batch 187, Loss: 0.853705, Accuracy: 84.61%\n",
      "Batch 188, Loss: 0.939803, Accuracy: 84.57%\n",
      "Batch 189, Loss: 0.920278, Accuracy: 84.57%\n",
      "Batch 190, Loss: 0.852816, Accuracy: 84.60%\n",
      "Batch 191, Loss: 0.894035, Accuracy: 84.60%\n",
      "Batch 192, Loss: 0.842436, Accuracy: 84.63%\n",
      "Batch 193, Loss: 0.912122, Accuracy: 84.63%\n",
      "Batch 194, Loss: 0.925689, Accuracy: 84.60%\n",
      "Batch 195, Loss: 0.928225, Accuracy: 84.58%\n",
      "Batch 196, Loss: 0.963900, Accuracy: 84.54%\n",
      "Batch 197, Loss: 0.931416, Accuracy: 84.53%\n",
      "Batch 198, Loss: 0.943132, Accuracy: 84.50%\n",
      "Batch 199, Loss: 0.894751, Accuracy: 84.51%\n",
      "Batch 200, Loss: 0.938519, Accuracy: 84.50%\n",
      "Batch 201, Loss: 0.862881, Accuracy: 84.52%\n",
      "Batch 202, Loss: 0.944263, Accuracy: 84.50%\n",
      "Batch 203, Loss: 0.965773, Accuracy: 84.47%\n",
      "Batch 204, Loss: 0.878425, Accuracy: 84.48%\n",
      "Batch 205, Loss: 0.902918, Accuracy: 84.48%\n",
      "Batch 206, Loss: 0.874619, Accuracy: 84.50%\n",
      "Batch 207, Loss: 0.875334, Accuracy: 84.52%\n",
      "Batch 208, Loss: 0.824934, Accuracy: 84.57%\n",
      "Batch 209, Loss: 0.992599, Accuracy: 84.52%\n",
      "Batch 210, Loss: 0.968812, Accuracy: 84.49%\n",
      "Batch 211, Loss: 0.909069, Accuracy: 84.49%\n",
      "Batch 212, Loss: 0.905515, Accuracy: 84.49%\n",
      "Batch 213, Loss: 0.962818, Accuracy: 84.45%\n",
      "Training - Epoch 60, Loss: 0.900700, Accuracy: 84.45%\n",
      "Validation Batch 1, Loss: 0.887931, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.892047, Accuracy: 86.72%\n",
      "Validation Batch 3, Loss: 0.973213, Accuracy: 83.33%\n",
      "Validation Batch 4, Loss: 0.906918, Accuracy: 82.81%\n",
      "Validation Batch 5, Loss: 0.892316, Accuracy: 83.75%\n",
      "Validation Batch 6, Loss: 0.854122, Accuracy: 85.16%\n",
      "Validation Batch 7, Loss: 0.872956, Accuracy: 85.94%\n",
      "Validation Batch 8, Loss: 0.970025, Accuracy: 84.57%\n",
      "Validation Batch 9, Loss: 0.936093, Accuracy: 84.55%\n",
      "Validation Batch 10, Loss: 0.928648, Accuracy: 83.91%\n",
      "Validation Batch 11, Loss: 0.904468, Accuracy: 83.95%\n",
      "Validation Batch 12, Loss: 0.902236, Accuracy: 83.85%\n",
      "Validation Batch 13, Loss: 0.899076, Accuracy: 84.01%\n",
      "Validation Batch 14, Loss: 0.934329, Accuracy: 83.82%\n",
      "Validation Batch 15, Loss: 0.887571, Accuracy: 83.96%\n",
      "Validation Batch 16, Loss: 0.908302, Accuracy: 84.08%\n",
      "Validation Batch 17, Loss: 0.941989, Accuracy: 83.92%\n",
      "Validation Batch 18, Loss: 0.871514, Accuracy: 84.11%\n",
      "Validation Batch 19, Loss: 0.938935, Accuracy: 83.96%\n",
      "Validation Batch 20, Loss: 0.992305, Accuracy: 83.44%\n",
      "Validation Batch 21, Loss: 0.946266, Accuracy: 83.33%\n",
      "Validation Batch 22, Loss: 0.901405, Accuracy: 83.38%\n",
      "Validation Batch 23, Loss: 0.940277, Accuracy: 83.36%\n",
      "Validation Batch 24, Loss: 0.923346, Accuracy: 83.27%\n",
      "Validation Batch 25, Loss: 0.882787, Accuracy: 83.44%\n",
      "Validation Batch 26, Loss: 0.903466, Accuracy: 83.53%\n",
      "Validation Batch 27, Loss: 0.924741, Accuracy: 83.38%\n",
      "Validation - Epoch 60, Loss: 0.915455, Accuracy: 83.38%\n",
      "Patience—1\n",
      "Epoch 61\n",
      "Batch 1, Loss: 0.857523, Accuracy: 89.06%\n",
      "Batch 2, Loss: 0.815968, Accuracy: 91.41%\n",
      "Batch 3, Loss: 0.842409, Accuracy: 91.15%\n",
      "Batch 4, Loss: 0.943307, Accuracy: 88.28%\n",
      "Batch 5, Loss: 0.880352, Accuracy: 88.12%\n",
      "Batch 6, Loss: 0.890076, Accuracy: 87.76%\n",
      "Batch 7, Loss: 0.977984, Accuracy: 86.16%\n",
      "Batch 8, Loss: 0.931636, Accuracy: 85.74%\n",
      "Batch 9, Loss: 0.937073, Accuracy: 85.24%\n",
      "Batch 10, Loss: 0.848859, Accuracy: 85.78%\n",
      "Batch 11, Loss: 0.937170, Accuracy: 85.09%\n",
      "Batch 12, Loss: 0.890825, Accuracy: 85.16%\n",
      "Batch 13, Loss: 0.870543, Accuracy: 85.34%\n",
      "Batch 14, Loss: 0.859886, Accuracy: 85.60%\n",
      "Batch 15, Loss: 0.946768, Accuracy: 85.21%\n",
      "Batch 16, Loss: 0.950975, Accuracy: 84.86%\n",
      "Batch 17, Loss: 0.914409, Accuracy: 84.83%\n",
      "Batch 18, Loss: 0.885588, Accuracy: 84.90%\n",
      "Batch 19, Loss: 0.869744, Accuracy: 85.12%\n",
      "Batch 20, Loss: 0.887849, Accuracy: 85.08%\n",
      "Batch 21, Loss: 0.858106, Accuracy: 85.34%\n",
      "Batch 22, Loss: 0.857398, Accuracy: 85.51%\n",
      "Batch 23, Loss: 0.869727, Accuracy: 85.60%\n",
      "Batch 24, Loss: 0.881182, Accuracy: 85.61%\n",
      "Batch 25, Loss: 0.900973, Accuracy: 85.62%\n",
      "Batch 26, Loss: 0.918035, Accuracy: 85.52%\n",
      "Batch 27, Loss: 0.989846, Accuracy: 85.19%\n",
      "Batch 28, Loss: 0.826096, Accuracy: 85.38%\n",
      "Batch 29, Loss: 0.907124, Accuracy: 85.29%\n",
      "Batch 30, Loss: 0.943649, Accuracy: 85.10%\n",
      "Batch 31, Loss: 0.873865, Accuracy: 85.13%\n",
      "Batch 32, Loss: 0.860408, Accuracy: 85.25%\n",
      "Batch 33, Loss: 0.921486, Accuracy: 85.13%\n",
      "Batch 34, Loss: 0.873149, Accuracy: 85.20%\n",
      "Batch 35, Loss: 0.967619, Accuracy: 85.00%\n",
      "Batch 36, Loss: 0.896800, Accuracy: 85.03%\n",
      "Batch 37, Loss: 0.912589, Accuracy: 84.97%\n",
      "Batch 38, Loss: 0.875973, Accuracy: 84.99%\n",
      "Batch 39, Loss: 0.903005, Accuracy: 84.98%\n",
      "Batch 40, Loss: 0.882213, Accuracy: 85.00%\n",
      "Batch 41, Loss: 0.870079, Accuracy: 85.06%\n",
      "Batch 42, Loss: 0.900981, Accuracy: 85.04%\n",
      "Batch 43, Loss: 0.821691, Accuracy: 85.25%\n",
      "Batch 44, Loss: 0.924611, Accuracy: 85.16%\n",
      "Batch 45, Loss: 0.855152, Accuracy: 85.24%\n",
      "Batch 46, Loss: 0.836490, Accuracy: 85.39%\n",
      "Batch 47, Loss: 0.952845, Accuracy: 85.24%\n",
      "Batch 48, Loss: 0.923560, Accuracy: 85.16%\n",
      "Batch 49, Loss: 0.915201, Accuracy: 85.08%\n",
      "Batch 50, Loss: 0.888003, Accuracy: 85.09%\n",
      "Batch 51, Loss: 0.846356, Accuracy: 85.20%\n",
      "Batch 52, Loss: 0.796715, Accuracy: 85.40%\n",
      "Batch 53, Loss: 0.880477, Accuracy: 85.41%\n",
      "Batch 54, Loss: 0.854596, Accuracy: 85.45%\n",
      "Batch 55, Loss: 0.852449, Accuracy: 85.54%\n",
      "Batch 56, Loss: 0.885325, Accuracy: 85.57%\n",
      "Batch 57, Loss: 0.882312, Accuracy: 85.58%\n",
      "Batch 58, Loss: 0.924158, Accuracy: 85.51%\n",
      "Batch 59, Loss: 0.844161, Accuracy: 85.59%\n",
      "Batch 60, Loss: 0.870971, Accuracy: 85.60%\n",
      "Batch 61, Loss: 0.844713, Accuracy: 85.68%\n",
      "Batch 62, Loss: 0.942306, Accuracy: 85.61%\n",
      "Batch 63, Loss: 0.857310, Accuracy: 85.66%\n",
      "Batch 64, Loss: 0.889739, Accuracy: 85.67%\n",
      "Batch 65, Loss: 0.954458, Accuracy: 85.55%\n",
      "Batch 66, Loss: 0.973320, Accuracy: 85.39%\n",
      "Batch 67, Loss: 0.866302, Accuracy: 85.47%\n",
      "Batch 68, Loss: 0.856455, Accuracy: 85.55%\n",
      "Batch 69, Loss: 0.919221, Accuracy: 85.48%\n",
      "Batch 70, Loss: 0.835154, Accuracy: 85.58%\n",
      "Batch 71, Loss: 0.867449, Accuracy: 85.63%\n",
      "Batch 72, Loss: 0.893957, Accuracy: 85.63%\n",
      "Batch 73, Loss: 0.938707, Accuracy: 85.57%\n",
      "Batch 74, Loss: 0.935025, Accuracy: 85.52%\n",
      "Batch 75, Loss: 0.872750, Accuracy: 85.54%\n",
      "Batch 76, Loss: 0.898691, Accuracy: 85.55%\n",
      "Batch 77, Loss: 0.966392, Accuracy: 85.47%\n",
      "Batch 78, Loss: 0.882200, Accuracy: 85.50%\n",
      "Batch 79, Loss: 0.873975, Accuracy: 85.52%\n",
      "Batch 80, Loss: 0.886256, Accuracy: 85.55%\n",
      "Batch 81, Loss: 0.891683, Accuracy: 85.55%\n",
      "Batch 82, Loss: 0.925187, Accuracy: 85.52%\n",
      "Batch 83, Loss: 0.904440, Accuracy: 85.50%\n",
      "Batch 84, Loss: 0.925039, Accuracy: 85.45%\n",
      "Batch 85, Loss: 0.917365, Accuracy: 85.42%\n",
      "Batch 86, Loss: 0.927616, Accuracy: 85.39%\n",
      "Batch 87, Loss: 0.830163, Accuracy: 85.45%\n",
      "Batch 88, Loss: 0.906511, Accuracy: 85.44%\n",
      "Batch 89, Loss: 1.005714, Accuracy: 85.29%\n",
      "Batch 90, Loss: 0.892243, Accuracy: 85.28%\n",
      "Batch 91, Loss: 0.893517, Accuracy: 85.27%\n",
      "Batch 92, Loss: 0.824830, Accuracy: 85.34%\n",
      "Batch 93, Loss: 0.848138, Accuracy: 85.40%\n",
      "Batch 94, Loss: 0.904262, Accuracy: 85.37%\n",
      "Batch 95, Loss: 0.854681, Accuracy: 85.41%\n",
      "Batch 96, Loss: 0.975269, Accuracy: 85.29%\n",
      "Batch 97, Loss: 0.930536, Accuracy: 85.23%\n",
      "Batch 98, Loss: 0.879565, Accuracy: 85.24%\n",
      "Batch 99, Loss: 0.855737, Accuracy: 85.27%\n",
      "Batch 100, Loss: 0.887186, Accuracy: 85.27%\n",
      "Batch 101, Loss: 0.921313, Accuracy: 85.26%\n",
      "Batch 102, Loss: 0.863968, Accuracy: 85.28%\n",
      "Batch 103, Loss: 0.924008, Accuracy: 85.24%\n",
      "Batch 104, Loss: 0.950778, Accuracy: 85.17%\n",
      "Batch 105, Loss: 0.918780, Accuracy: 85.15%\n",
      "Batch 106, Loss: 0.874826, Accuracy: 85.19%\n",
      "Batch 107, Loss: 0.832918, Accuracy: 85.25%\n",
      "Batch 108, Loss: 0.850555, Accuracy: 85.30%\n",
      "Batch 109, Loss: 0.940467, Accuracy: 85.25%\n",
      "Batch 110, Loss: 0.979405, Accuracy: 85.18%\n",
      "Batch 111, Loss: 0.903385, Accuracy: 85.15%\n",
      "Batch 112, Loss: 0.946735, Accuracy: 85.10%\n",
      "Batch 113, Loss: 0.875289, Accuracy: 85.11%\n",
      "Batch 114, Loss: 0.901026, Accuracy: 85.10%\n",
      "Batch 115, Loss: 0.896704, Accuracy: 85.10%\n",
      "Batch 116, Loss: 0.868411, Accuracy: 85.12%\n",
      "Batch 117, Loss: 0.968959, Accuracy: 85.06%\n",
      "Batch 118, Loss: 0.886185, Accuracy: 85.06%\n",
      "Batch 119, Loss: 0.992543, Accuracy: 84.98%\n",
      "Batch 120, Loss: 0.894521, Accuracy: 84.99%\n",
      "Batch 121, Loss: 0.870412, Accuracy: 84.99%\n",
      "Batch 122, Loss: 0.866010, Accuracy: 85.00%\n",
      "Batch 123, Loss: 0.851422, Accuracy: 85.04%\n",
      "Batch 124, Loss: 0.926110, Accuracy: 84.99%\n",
      "Batch 125, Loss: 1.046928, Accuracy: 84.84%\n",
      "Batch 126, Loss: 0.862026, Accuracy: 84.87%\n",
      "Batch 127, Loss: 0.939479, Accuracy: 84.82%\n",
      "Batch 128, Loss: 0.878914, Accuracy: 84.84%\n",
      "Batch 129, Loss: 0.843075, Accuracy: 84.90%\n",
      "Batch 130, Loss: 0.984794, Accuracy: 84.83%\n",
      "Batch 131, Loss: 0.866440, Accuracy: 84.88%\n",
      "Batch 132, Loss: 0.909886, Accuracy: 84.86%\n",
      "Batch 133, Loss: 0.931376, Accuracy: 84.86%\n",
      "Batch 134, Loss: 0.958717, Accuracy: 84.81%\n",
      "Batch 135, Loss: 0.947486, Accuracy: 84.77%\n",
      "Batch 136, Loss: 0.920415, Accuracy: 84.73%\n",
      "Batch 137, Loss: 0.911936, Accuracy: 84.72%\n",
      "Batch 138, Loss: 0.815680, Accuracy: 84.78%\n",
      "Batch 139, Loss: 0.924240, Accuracy: 84.77%\n",
      "Batch 140, Loss: 1.050988, Accuracy: 84.65%\n",
      "Batch 141, Loss: 0.938671, Accuracy: 84.64%\n",
      "Batch 142, Loss: 0.880998, Accuracy: 84.66%\n",
      "Batch 143, Loss: 0.877035, Accuracy: 84.68%\n",
      "Batch 144, Loss: 0.901052, Accuracy: 84.69%\n",
      "Batch 145, Loss: 0.905955, Accuracy: 84.69%\n",
      "Batch 146, Loss: 0.844635, Accuracy: 84.73%\n",
      "Batch 147, Loss: 0.870087, Accuracy: 84.76%\n",
      "Batch 148, Loss: 0.849095, Accuracy: 84.81%\n",
      "Batch 149, Loss: 0.864731, Accuracy: 84.83%\n",
      "Batch 150, Loss: 0.858397, Accuracy: 84.85%\n",
      "Batch 151, Loss: 0.866540, Accuracy: 84.88%\n",
      "Batch 152, Loss: 0.983174, Accuracy: 84.83%\n",
      "Batch 153, Loss: 0.904418, Accuracy: 84.82%\n",
      "Batch 154, Loss: 0.988543, Accuracy: 84.77%\n",
      "Batch 155, Loss: 0.921606, Accuracy: 84.75%\n",
      "Batch 156, Loss: 0.894058, Accuracy: 84.75%\n",
      "Batch 157, Loss: 0.874583, Accuracy: 84.75%\n",
      "Batch 158, Loss: 0.901208, Accuracy: 84.74%\n",
      "Batch 159, Loss: 0.890181, Accuracy: 84.75%\n",
      "Batch 160, Loss: 0.891297, Accuracy: 84.76%\n",
      "Batch 161, Loss: 0.880247, Accuracy: 84.77%\n",
      "Batch 162, Loss: 0.881238, Accuracy: 84.79%\n",
      "Batch 163, Loss: 0.910989, Accuracy: 84.77%\n",
      "Batch 164, Loss: 1.074268, Accuracy: 84.67%\n",
      "Batch 165, Loss: 0.865208, Accuracy: 84.70%\n",
      "Batch 166, Loss: 0.951686, Accuracy: 84.65%\n",
      "Batch 167, Loss: 0.875557, Accuracy: 84.66%\n",
      "Batch 168, Loss: 0.884657, Accuracy: 84.66%\n",
      "Batch 169, Loss: 0.890663, Accuracy: 84.67%\n",
      "Batch 170, Loss: 0.814271, Accuracy: 84.72%\n",
      "Batch 171, Loss: 0.868069, Accuracy: 84.75%\n",
      "Batch 172, Loss: 0.862772, Accuracy: 84.77%\n",
      "Batch 173, Loss: 0.959484, Accuracy: 84.74%\n",
      "Batch 174, Loss: 0.864527, Accuracy: 84.75%\n",
      "Batch 175, Loss: 0.890836, Accuracy: 84.75%\n",
      "Batch 176, Loss: 0.920683, Accuracy: 84.73%\n",
      "Batch 177, Loss: 0.850993, Accuracy: 84.75%\n",
      "Batch 178, Loss: 0.884447, Accuracy: 84.76%\n",
      "Batch 179, Loss: 0.960384, Accuracy: 84.72%\n",
      "Batch 180, Loss: 0.882426, Accuracy: 84.74%\n",
      "Batch 181, Loss: 0.871177, Accuracy: 84.75%\n",
      "Batch 182, Loss: 0.902350, Accuracy: 84.74%\n",
      "Batch 183, Loss: 0.881222, Accuracy: 84.76%\n",
      "Batch 184, Loss: 0.896060, Accuracy: 84.76%\n",
      "Batch 185, Loss: 0.884685, Accuracy: 84.76%\n",
      "Batch 186, Loss: 0.937323, Accuracy: 84.72%\n",
      "Batch 187, Loss: 0.891187, Accuracy: 84.73%\n",
      "Batch 188, Loss: 0.939945, Accuracy: 84.69%\n",
      "Batch 189, Loss: 0.931111, Accuracy: 84.67%\n",
      "Batch 190, Loss: 0.916426, Accuracy: 84.67%\n",
      "Batch 191, Loss: 0.895201, Accuracy: 84.68%\n",
      "Batch 192, Loss: 0.921435, Accuracy: 84.66%\n",
      "Batch 193, Loss: 0.882105, Accuracy: 84.67%\n",
      "Batch 194, Loss: 0.823288, Accuracy: 84.71%\n",
      "Batch 195, Loss: 0.841247, Accuracy: 84.74%\n",
      "Batch 196, Loss: 0.929021, Accuracy: 84.73%\n",
      "Batch 197, Loss: 0.880561, Accuracy: 84.74%\n",
      "Batch 198, Loss: 0.929116, Accuracy: 84.73%\n",
      "Batch 199, Loss: 0.859968, Accuracy: 84.75%\n",
      "Batch 200, Loss: 0.966639, Accuracy: 84.70%\n",
      "Batch 201, Loss: 0.841672, Accuracy: 84.72%\n",
      "Batch 202, Loss: 0.909126, Accuracy: 84.71%\n",
      "Batch 203, Loss: 0.865941, Accuracy: 84.72%\n",
      "Batch 204, Loss: 0.907902, Accuracy: 84.72%\n",
      "Batch 205, Loss: 0.877777, Accuracy: 84.74%\n",
      "Batch 206, Loss: 0.942377, Accuracy: 84.70%\n",
      "Batch 207, Loss: 0.958366, Accuracy: 84.66%\n",
      "Batch 208, Loss: 0.965495, Accuracy: 84.63%\n",
      "Batch 209, Loss: 0.888445, Accuracy: 84.64%\n",
      "Batch 210, Loss: 0.863085, Accuracy: 84.65%\n",
      "Batch 211, Loss: 0.931178, Accuracy: 84.64%\n",
      "Batch 212, Loss: 0.922335, Accuracy: 84.63%\n",
      "Batch 213, Loss: 0.910341, Accuracy: 84.61%\n",
      "Training - Epoch 61, Loss: 0.898870, Accuracy: 84.61%\n",
      "Validation Batch 1, Loss: 0.880346, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.874865, Accuracy: 89.06%\n",
      "Validation Batch 3, Loss: 0.950598, Accuracy: 86.98%\n",
      "Validation Batch 4, Loss: 0.897367, Accuracy: 85.94%\n",
      "Validation Batch 5, Loss: 0.881335, Accuracy: 86.25%\n",
      "Validation Batch 6, Loss: 0.840537, Accuracy: 87.24%\n",
      "Validation Batch 7, Loss: 0.856042, Accuracy: 87.72%\n",
      "Validation Batch 8, Loss: 0.965126, Accuracy: 86.33%\n",
      "Validation Batch 9, Loss: 0.933439, Accuracy: 85.94%\n",
      "Validation Batch 10, Loss: 0.916534, Accuracy: 85.31%\n",
      "Validation Batch 11, Loss: 0.901610, Accuracy: 85.23%\n",
      "Validation Batch 12, Loss: 0.892471, Accuracy: 85.29%\n",
      "Validation Batch 13, Loss: 0.890403, Accuracy: 85.46%\n",
      "Validation Batch 14, Loss: 0.923273, Accuracy: 85.16%\n",
      "Validation Batch 15, Loss: 0.879319, Accuracy: 85.21%\n",
      "Validation Batch 16, Loss: 0.889505, Accuracy: 85.45%\n",
      "Validation Batch 17, Loss: 0.922281, Accuracy: 85.39%\n",
      "Validation Batch 18, Loss: 0.870057, Accuracy: 85.42%\n",
      "Validation Batch 19, Loss: 0.927886, Accuracy: 85.28%\n",
      "Validation Batch 20, Loss: 0.979227, Accuracy: 84.69%\n",
      "Validation Batch 21, Loss: 0.925975, Accuracy: 84.60%\n",
      "Validation Batch 22, Loss: 0.894309, Accuracy: 84.66%\n",
      "Validation Batch 23, Loss: 0.926227, Accuracy: 84.44%\n",
      "Validation Batch 24, Loss: 0.912039, Accuracy: 84.44%\n",
      "Validation Batch 25, Loss: 0.866829, Accuracy: 84.62%\n",
      "Validation Batch 26, Loss: 0.890131, Accuracy: 84.68%\n",
      "Validation Batch 27, Loss: 0.895203, Accuracy: 84.73%\n",
      "Validation - Epoch 61, Loss: 0.903072, Accuracy: 84.73%\n",
      "Patience—0\n",
      "Epoch 62\n",
      "Batch 1, Loss: 0.902727, Accuracy: 84.38%\n",
      "Batch 2, Loss: 0.864209, Accuracy: 86.72%\n",
      "Batch 3, Loss: 0.880908, Accuracy: 86.98%\n",
      "Batch 4, Loss: 0.935039, Accuracy: 85.55%\n",
      "Batch 5, Loss: 0.866658, Accuracy: 85.62%\n",
      "Batch 6, Loss: 0.907666, Accuracy: 85.16%\n",
      "Batch 7, Loss: 0.932426, Accuracy: 84.60%\n",
      "Batch 8, Loss: 0.810991, Accuracy: 85.94%\n",
      "Batch 9, Loss: 0.871871, Accuracy: 86.11%\n",
      "Batch 10, Loss: 0.886695, Accuracy: 86.25%\n",
      "Batch 11, Loss: 0.820036, Accuracy: 86.65%\n",
      "Batch 12, Loss: 0.921952, Accuracy: 86.20%\n",
      "Batch 13, Loss: 0.905596, Accuracy: 86.18%\n",
      "Batch 14, Loss: 0.919936, Accuracy: 85.94%\n",
      "Batch 15, Loss: 0.966244, Accuracy: 85.52%\n",
      "Batch 16, Loss: 0.884390, Accuracy: 85.64%\n",
      "Batch 17, Loss: 0.870178, Accuracy: 85.75%\n",
      "Batch 18, Loss: 0.983211, Accuracy: 85.24%\n",
      "Batch 19, Loss: 0.853762, Accuracy: 85.44%\n",
      "Batch 20, Loss: 0.876607, Accuracy: 85.55%\n",
      "Batch 21, Loss: 0.869344, Accuracy: 85.64%\n",
      "Batch 22, Loss: 0.895447, Accuracy: 85.65%\n",
      "Batch 23, Loss: 0.971763, Accuracy: 85.33%\n",
      "Batch 24, Loss: 0.909542, Accuracy: 85.29%\n",
      "Batch 25, Loss: 0.853248, Accuracy: 85.50%\n",
      "Batch 26, Loss: 0.862236, Accuracy: 85.64%\n",
      "Batch 27, Loss: 0.915902, Accuracy: 85.59%\n",
      "Batch 28, Loss: 0.912349, Accuracy: 85.49%\n",
      "Batch 29, Loss: 0.954182, Accuracy: 85.34%\n",
      "Batch 30, Loss: 0.888325, Accuracy: 85.31%\n",
      "Batch 31, Loss: 0.960228, Accuracy: 85.18%\n",
      "Batch 32, Loss: 0.936993, Accuracy: 85.11%\n",
      "Batch 33, Loss: 0.960314, Accuracy: 84.85%\n",
      "Batch 34, Loss: 0.850176, Accuracy: 85.06%\n",
      "Batch 35, Loss: 0.907314, Accuracy: 85.00%\n",
      "Batch 36, Loss: 0.875945, Accuracy: 85.03%\n",
      "Batch 37, Loss: 0.838865, Accuracy: 85.26%\n",
      "Batch 38, Loss: 0.930910, Accuracy: 85.12%\n",
      "Batch 39, Loss: 0.912154, Accuracy: 85.10%\n",
      "Batch 40, Loss: 0.959279, Accuracy: 84.96%\n",
      "Batch 41, Loss: 0.860397, Accuracy: 85.06%\n",
      "Batch 42, Loss: 0.820709, Accuracy: 85.27%\n",
      "Batch 43, Loss: 0.830037, Accuracy: 85.43%\n",
      "Batch 44, Loss: 0.868802, Accuracy: 85.48%\n",
      "Batch 45, Loss: 0.884611, Accuracy: 85.45%\n",
      "Batch 46, Loss: 0.951442, Accuracy: 85.33%\n",
      "Batch 47, Loss: 0.890868, Accuracy: 85.31%\n",
      "Batch 48, Loss: 0.970231, Accuracy: 85.16%\n",
      "Batch 49, Loss: 0.952879, Accuracy: 84.95%\n",
      "Batch 50, Loss: 0.923667, Accuracy: 84.88%\n",
      "Batch 51, Loss: 0.916600, Accuracy: 84.80%\n",
      "Batch 52, Loss: 0.846005, Accuracy: 84.92%\n",
      "Batch 53, Loss: 0.894765, Accuracy: 84.91%\n",
      "Batch 54, Loss: 0.780225, Accuracy: 85.10%\n",
      "Batch 55, Loss: 0.964424, Accuracy: 84.97%\n",
      "Batch 56, Loss: 0.880554, Accuracy: 84.96%\n",
      "Batch 57, Loss: 0.893591, Accuracy: 84.98%\n",
      "Batch 58, Loss: 0.868419, Accuracy: 85.05%\n",
      "Batch 59, Loss: 0.937446, Accuracy: 84.96%\n",
      "Batch 60, Loss: 0.949672, Accuracy: 84.90%\n",
      "Batch 61, Loss: 0.902108, Accuracy: 84.89%\n",
      "Batch 62, Loss: 0.911568, Accuracy: 84.88%\n",
      "Batch 63, Loss: 0.902264, Accuracy: 84.82%\n",
      "Batch 64, Loss: 0.836380, Accuracy: 84.91%\n",
      "Batch 65, Loss: 0.838118, Accuracy: 85.02%\n",
      "Batch 66, Loss: 0.867817, Accuracy: 85.09%\n",
      "Batch 67, Loss: 0.961759, Accuracy: 84.98%\n",
      "Batch 68, Loss: 0.950918, Accuracy: 84.90%\n",
      "Batch 69, Loss: 0.962629, Accuracy: 84.81%\n",
      "Batch 70, Loss: 0.927294, Accuracy: 84.73%\n",
      "Batch 71, Loss: 0.863861, Accuracy: 84.77%\n",
      "Batch 72, Loss: 0.831225, Accuracy: 84.87%\n",
      "Batch 73, Loss: 0.896768, Accuracy: 84.85%\n",
      "Batch 74, Loss: 0.918665, Accuracy: 84.80%\n",
      "Batch 75, Loss: 0.933270, Accuracy: 84.75%\n",
      "Batch 76, Loss: 0.858562, Accuracy: 84.81%\n",
      "Batch 77, Loss: 0.950724, Accuracy: 84.74%\n",
      "Batch 78, Loss: 0.861147, Accuracy: 84.78%\n",
      "Batch 79, Loss: 0.928522, Accuracy: 84.71%\n",
      "Batch 80, Loss: 0.913294, Accuracy: 84.67%\n",
      "Batch 81, Loss: 0.924537, Accuracy: 84.63%\n",
      "Batch 82, Loss: 0.930193, Accuracy: 84.58%\n",
      "Batch 83, Loss: 0.946591, Accuracy: 84.51%\n",
      "Batch 84, Loss: 0.876392, Accuracy: 84.54%\n",
      "Batch 85, Loss: 0.929739, Accuracy: 84.49%\n",
      "Batch 86, Loss: 0.863785, Accuracy: 84.54%\n",
      "Batch 87, Loss: 0.893321, Accuracy: 84.52%\n",
      "Batch 88, Loss: 0.874732, Accuracy: 84.53%\n",
      "Batch 89, Loss: 0.904740, Accuracy: 84.55%\n",
      "Batch 90, Loss: 0.933969, Accuracy: 84.50%\n",
      "Batch 91, Loss: 0.916580, Accuracy: 84.48%\n",
      "Batch 92, Loss: 0.939112, Accuracy: 84.41%\n",
      "Batch 93, Loss: 0.875488, Accuracy: 84.43%\n",
      "Batch 94, Loss: 0.879409, Accuracy: 84.44%\n",
      "Batch 95, Loss: 0.909009, Accuracy: 84.44%\n",
      "Batch 96, Loss: 0.905382, Accuracy: 84.44%\n",
      "Batch 97, Loss: 0.898450, Accuracy: 84.44%\n",
      "Batch 98, Loss: 0.922343, Accuracy: 84.42%\n",
      "Batch 99, Loss: 0.873113, Accuracy: 84.44%\n",
      "Batch 100, Loss: 0.858819, Accuracy: 84.47%\n",
      "Batch 101, Loss: 0.869378, Accuracy: 84.48%\n",
      "Batch 102, Loss: 0.893252, Accuracy: 84.47%\n",
      "Batch 103, Loss: 0.917354, Accuracy: 84.45%\n",
      "Batch 104, Loss: 0.877824, Accuracy: 84.47%\n",
      "Batch 105, Loss: 0.876531, Accuracy: 84.48%\n",
      "Batch 106, Loss: 0.865380, Accuracy: 84.51%\n",
      "Batch 107, Loss: 0.889649, Accuracy: 84.52%\n",
      "Batch 108, Loss: 0.876835, Accuracy: 84.55%\n",
      "Batch 109, Loss: 0.916481, Accuracy: 84.55%\n",
      "Batch 110, Loss: 0.961105, Accuracy: 84.47%\n",
      "Batch 111, Loss: 0.954898, Accuracy: 84.40%\n",
      "Batch 112, Loss: 0.876633, Accuracy: 84.42%\n",
      "Batch 113, Loss: 0.873636, Accuracy: 84.44%\n",
      "Batch 114, Loss: 0.949738, Accuracy: 84.39%\n",
      "Batch 115, Loss: 0.847373, Accuracy: 84.44%\n",
      "Batch 116, Loss: 0.967137, Accuracy: 84.38%\n",
      "Batch 117, Loss: 0.909757, Accuracy: 84.38%\n",
      "Batch 118, Loss: 0.879107, Accuracy: 84.39%\n",
      "Batch 119, Loss: 0.932600, Accuracy: 84.36%\n",
      "Batch 120, Loss: 0.909655, Accuracy: 84.36%\n",
      "Batch 121, Loss: 0.851833, Accuracy: 84.40%\n",
      "Batch 122, Loss: 0.945883, Accuracy: 84.38%\n",
      "Batch 123, Loss: 0.900445, Accuracy: 84.39%\n",
      "Batch 124, Loss: 0.915331, Accuracy: 84.38%\n",
      "Batch 125, Loss: 0.884457, Accuracy: 84.36%\n",
      "Batch 126, Loss: 0.839518, Accuracy: 84.42%\n",
      "Batch 127, Loss: 0.856182, Accuracy: 84.46%\n",
      "Batch 128, Loss: 0.854136, Accuracy: 84.50%\n",
      "Batch 129, Loss: 0.906069, Accuracy: 84.50%\n",
      "Batch 130, Loss: 0.906214, Accuracy: 84.50%\n",
      "Batch 131, Loss: 0.857281, Accuracy: 84.53%\n",
      "Batch 132, Loss: 0.865519, Accuracy: 84.55%\n",
      "Batch 133, Loss: 0.834689, Accuracy: 84.61%\n",
      "Batch 134, Loss: 0.885695, Accuracy: 84.62%\n",
      "Batch 135, Loss: 0.888433, Accuracy: 84.63%\n",
      "Batch 136, Loss: 0.914446, Accuracy: 84.62%\n",
      "Batch 137, Loss: 0.893887, Accuracy: 84.61%\n",
      "Batch 138, Loss: 0.899104, Accuracy: 84.62%\n",
      "Batch 139, Loss: 0.944248, Accuracy: 84.59%\n",
      "Batch 140, Loss: 0.868095, Accuracy: 84.60%\n",
      "Batch 141, Loss: 0.876039, Accuracy: 84.61%\n",
      "Batch 142, Loss: 0.929938, Accuracy: 84.60%\n",
      "Batch 143, Loss: 0.889498, Accuracy: 84.62%\n",
      "Batch 144, Loss: 0.928696, Accuracy: 84.59%\n",
      "Batch 145, Loss: 0.912027, Accuracy: 84.58%\n",
      "Batch 146, Loss: 0.872267, Accuracy: 84.59%\n",
      "Batch 147, Loss: 0.881646, Accuracy: 84.61%\n",
      "Batch 148, Loss: 0.848488, Accuracy: 84.66%\n",
      "Batch 149, Loss: 0.888945, Accuracy: 84.66%\n",
      "Batch 150, Loss: 0.916144, Accuracy: 84.64%\n",
      "Batch 151, Loss: 0.923123, Accuracy: 84.60%\n",
      "Batch 152, Loss: 0.934025, Accuracy: 84.58%\n",
      "Batch 153, Loss: 0.913304, Accuracy: 84.57%\n",
      "Batch 154, Loss: 0.872716, Accuracy: 84.58%\n",
      "Batch 155, Loss: 0.898870, Accuracy: 84.58%\n",
      "Batch 156, Loss: 0.951228, Accuracy: 84.54%\n",
      "Batch 157, Loss: 0.883830, Accuracy: 84.53%\n",
      "Batch 158, Loss: 0.937152, Accuracy: 84.51%\n",
      "Batch 159, Loss: 0.857328, Accuracy: 84.53%\n",
      "Batch 160, Loss: 0.912768, Accuracy: 84.53%\n",
      "Batch 161, Loss: 0.914026, Accuracy: 84.51%\n",
      "Batch 162, Loss: 0.980699, Accuracy: 84.45%\n",
      "Batch 163, Loss: 0.875786, Accuracy: 84.48%\n",
      "Batch 164, Loss: 0.868034, Accuracy: 84.51%\n",
      "Batch 165, Loss: 0.884011, Accuracy: 84.52%\n",
      "Batch 166, Loss: 0.833232, Accuracy: 84.56%\n",
      "Batch 167, Loss: 0.920324, Accuracy: 84.55%\n",
      "Batch 168, Loss: 0.925309, Accuracy: 84.54%\n",
      "Batch 169, Loss: 0.903485, Accuracy: 84.54%\n",
      "Batch 170, Loss: 0.887023, Accuracy: 84.55%\n",
      "Batch 171, Loss: 0.937059, Accuracy: 84.53%\n",
      "Batch 172, Loss: 0.964872, Accuracy: 84.49%\n",
      "Batch 173, Loss: 0.842256, Accuracy: 84.54%\n",
      "Batch 174, Loss: 1.002980, Accuracy: 84.47%\n",
      "Batch 175, Loss: 0.880194, Accuracy: 84.48%\n",
      "Batch 176, Loss: 0.945263, Accuracy: 84.46%\n",
      "Batch 177, Loss: 0.958133, Accuracy: 84.44%\n",
      "Batch 178, Loss: 0.948055, Accuracy: 84.40%\n",
      "Batch 179, Loss: 0.829343, Accuracy: 84.44%\n",
      "Batch 180, Loss: 0.907791, Accuracy: 84.44%\n",
      "Batch 181, Loss: 0.874807, Accuracy: 84.46%\n",
      "Batch 182, Loss: 0.971641, Accuracy: 84.42%\n",
      "Batch 183, Loss: 0.912695, Accuracy: 84.42%\n",
      "Batch 184, Loss: 0.868962, Accuracy: 84.43%\n",
      "Batch 185, Loss: 0.874616, Accuracy: 84.45%\n",
      "Batch 186, Loss: 0.946028, Accuracy: 84.43%\n",
      "Batch 187, Loss: 0.902658, Accuracy: 84.42%\n",
      "Batch 188, Loss: 0.824561, Accuracy: 84.46%\n",
      "Batch 189, Loss: 0.867180, Accuracy: 84.47%\n",
      "Batch 190, Loss: 0.895009, Accuracy: 84.48%\n",
      "Batch 191, Loss: 0.866289, Accuracy: 84.50%\n",
      "Batch 192, Loss: 0.863729, Accuracy: 84.51%\n",
      "Batch 193, Loss: 0.941521, Accuracy: 84.48%\n",
      "Batch 194, Loss: 0.856952, Accuracy: 84.50%\n",
      "Batch 195, Loss: 0.906012, Accuracy: 84.50%\n",
      "Batch 196, Loss: 0.897581, Accuracy: 84.49%\n",
      "Batch 197, Loss: 0.842556, Accuracy: 84.53%\n",
      "Batch 198, Loss: 0.864585, Accuracy: 84.55%\n",
      "Batch 199, Loss: 0.943293, Accuracy: 84.52%\n",
      "Batch 200, Loss: 0.884990, Accuracy: 84.54%\n",
      "Batch 201, Loss: 0.881475, Accuracy: 84.55%\n",
      "Batch 202, Loss: 0.864139, Accuracy: 84.58%\n",
      "Batch 203, Loss: 0.915861, Accuracy: 84.58%\n",
      "Batch 204, Loss: 1.011267, Accuracy: 84.53%\n",
      "Batch 205, Loss: 0.891775, Accuracy: 84.53%\n",
      "Batch 206, Loss: 0.833060, Accuracy: 84.56%\n",
      "Batch 207, Loss: 0.890766, Accuracy: 84.56%\n",
      "Batch 208, Loss: 0.824332, Accuracy: 84.60%\n",
      "Batch 209, Loss: 0.893904, Accuracy: 84.61%\n",
      "Batch 210, Loss: 0.929214, Accuracy: 84.60%\n",
      "Batch 211, Loss: 0.935252, Accuracy: 84.59%\n",
      "Batch 212, Loss: 0.896767, Accuracy: 84.58%\n",
      "Batch 213, Loss: 0.835040, Accuracy: 84.61%\n",
      "Training - Epoch 62, Loss: 0.898515, Accuracy: 84.61%\n",
      "Validation Batch 1, Loss: 0.879735, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.875897, Accuracy: 89.06%\n",
      "Validation Batch 3, Loss: 0.963244, Accuracy: 85.94%\n",
      "Validation Batch 4, Loss: 0.900077, Accuracy: 85.16%\n",
      "Validation Batch 5, Loss: 0.881503, Accuracy: 85.62%\n",
      "Validation Batch 6, Loss: 0.846503, Accuracy: 86.72%\n",
      "Validation Batch 7, Loss: 0.866287, Accuracy: 87.28%\n",
      "Validation Batch 8, Loss: 0.965717, Accuracy: 85.74%\n",
      "Validation Batch 9, Loss: 0.929376, Accuracy: 85.59%\n",
      "Validation Batch 10, Loss: 0.924720, Accuracy: 85.00%\n",
      "Validation Batch 11, Loss: 0.905009, Accuracy: 84.94%\n",
      "Validation Batch 12, Loss: 0.899228, Accuracy: 84.64%\n",
      "Validation Batch 13, Loss: 0.895795, Accuracy: 84.74%\n",
      "Validation Batch 14, Loss: 0.929868, Accuracy: 84.49%\n",
      "Validation Batch 15, Loss: 0.875957, Accuracy: 84.58%\n",
      "Validation Batch 16, Loss: 0.893962, Accuracy: 84.77%\n",
      "Validation Batch 17, Loss: 0.930193, Accuracy: 84.74%\n",
      "Validation Batch 18, Loss: 0.868884, Accuracy: 84.90%\n",
      "Validation Batch 19, Loss: 0.935048, Accuracy: 84.62%\n",
      "Validation Batch 20, Loss: 0.987108, Accuracy: 84.06%\n",
      "Validation Batch 21, Loss: 0.933251, Accuracy: 84.00%\n",
      "Validation Batch 22, Loss: 0.896066, Accuracy: 84.02%\n",
      "Validation Batch 23, Loss: 0.935450, Accuracy: 83.90%\n",
      "Validation Batch 24, Loss: 0.916273, Accuracy: 83.85%\n",
      "Validation Batch 25, Loss: 0.873718, Accuracy: 84.00%\n",
      "Validation Batch 26, Loss: 0.895096, Accuracy: 84.07%\n",
      "Validation Batch 27, Loss: 0.906631, Accuracy: 84.20%\n",
      "Validation - Epoch 62, Loss: 0.907800, Accuracy: 84.20%\n",
      "Patience—1\n",
      "Epoch 63\n",
      "Batch 1, Loss: 0.902712, Accuracy: 84.38%\n",
      "Batch 2, Loss: 0.927148, Accuracy: 83.59%\n",
      "Batch 3, Loss: 0.865036, Accuracy: 85.42%\n",
      "Batch 4, Loss: 0.947936, Accuracy: 83.98%\n",
      "Batch 5, Loss: 0.946472, Accuracy: 83.44%\n",
      "Batch 6, Loss: 0.856143, Accuracy: 84.38%\n",
      "Batch 7, Loss: 0.872916, Accuracy: 84.82%\n",
      "Batch 8, Loss: 0.982011, Accuracy: 83.79%\n",
      "Batch 9, Loss: 0.867268, Accuracy: 84.38%\n",
      "Batch 10, Loss: 0.881650, Accuracy: 84.53%\n",
      "Batch 11, Loss: 0.889655, Accuracy: 84.66%\n",
      "Batch 12, Loss: 0.837087, Accuracy: 85.42%\n",
      "Batch 13, Loss: 0.839459, Accuracy: 85.94%\n",
      "Batch 14, Loss: 0.847198, Accuracy: 86.16%\n",
      "Batch 15, Loss: 0.862396, Accuracy: 86.15%\n",
      "Batch 16, Loss: 0.919810, Accuracy: 85.94%\n",
      "Batch 17, Loss: 0.883956, Accuracy: 86.03%\n",
      "Batch 18, Loss: 0.887581, Accuracy: 86.11%\n",
      "Batch 19, Loss: 0.894970, Accuracy: 86.02%\n",
      "Batch 20, Loss: 0.908026, Accuracy: 85.94%\n",
      "Batch 21, Loss: 0.945345, Accuracy: 85.42%\n",
      "Batch 22, Loss: 0.905746, Accuracy: 85.30%\n",
      "Batch 23, Loss: 0.934636, Accuracy: 85.19%\n",
      "Batch 24, Loss: 0.894198, Accuracy: 85.22%\n",
      "Batch 25, Loss: 0.915170, Accuracy: 85.12%\n",
      "Batch 26, Loss: 0.899131, Accuracy: 85.04%\n",
      "Batch 27, Loss: 0.844383, Accuracy: 85.30%\n",
      "Batch 28, Loss: 0.838152, Accuracy: 85.49%\n",
      "Batch 29, Loss: 0.870221, Accuracy: 85.51%\n",
      "Batch 30, Loss: 0.948499, Accuracy: 85.31%\n",
      "Batch 31, Loss: 0.895358, Accuracy: 85.28%\n",
      "Batch 32, Loss: 0.891263, Accuracy: 85.30%\n",
      "Batch 33, Loss: 0.919327, Accuracy: 85.23%\n",
      "Batch 34, Loss: 0.918981, Accuracy: 85.06%\n",
      "Batch 35, Loss: 0.838269, Accuracy: 85.18%\n",
      "Batch 36, Loss: 0.826359, Accuracy: 85.37%\n",
      "Batch 37, Loss: 0.859234, Accuracy: 85.47%\n",
      "Batch 38, Loss: 0.882747, Accuracy: 85.53%\n",
      "Batch 39, Loss: 0.827684, Accuracy: 85.70%\n",
      "Batch 40, Loss: 0.977981, Accuracy: 85.47%\n",
      "Batch 41, Loss: 0.934252, Accuracy: 85.37%\n",
      "Batch 42, Loss: 0.950929, Accuracy: 85.19%\n",
      "Batch 43, Loss: 0.961318, Accuracy: 84.99%\n",
      "Batch 44, Loss: 0.855218, Accuracy: 85.09%\n",
      "Batch 45, Loss: 0.870919, Accuracy: 85.17%\n",
      "Batch 46, Loss: 0.971868, Accuracy: 84.99%\n",
      "Batch 47, Loss: 0.833281, Accuracy: 85.14%\n",
      "Batch 48, Loss: 0.896184, Accuracy: 85.12%\n",
      "Batch 49, Loss: 0.878501, Accuracy: 85.17%\n",
      "Batch 50, Loss: 0.890329, Accuracy: 85.19%\n",
      "Batch 51, Loss: 0.859129, Accuracy: 85.26%\n",
      "Batch 52, Loss: 0.826137, Accuracy: 85.40%\n",
      "Batch 53, Loss: 0.963859, Accuracy: 85.26%\n",
      "Batch 54, Loss: 0.909620, Accuracy: 85.21%\n",
      "Batch 55, Loss: 0.951850, Accuracy: 85.09%\n",
      "Batch 56, Loss: 0.883327, Accuracy: 85.10%\n",
      "Batch 57, Loss: 0.907012, Accuracy: 85.12%\n",
      "Batch 58, Loss: 0.907850, Accuracy: 85.08%\n",
      "Batch 59, Loss: 0.850577, Accuracy: 85.17%\n",
      "Batch 60, Loss: 0.831106, Accuracy: 85.29%\n",
      "Batch 61, Loss: 0.900847, Accuracy: 85.25%\n",
      "Batch 62, Loss: 0.903083, Accuracy: 85.23%\n",
      "Batch 63, Loss: 0.878802, Accuracy: 85.27%\n",
      "Batch 64, Loss: 0.861066, Accuracy: 85.30%\n",
      "Batch 65, Loss: 0.957831, Accuracy: 85.19%\n",
      "Batch 66, Loss: 0.937503, Accuracy: 85.11%\n",
      "Batch 67, Loss: 0.834123, Accuracy: 85.19%\n",
      "Batch 68, Loss: 0.910648, Accuracy: 85.20%\n",
      "Batch 69, Loss: 0.839964, Accuracy: 85.28%\n",
      "Batch 70, Loss: 0.971111, Accuracy: 85.13%\n",
      "Batch 71, Loss: 0.864963, Accuracy: 85.19%\n",
      "Batch 72, Loss: 0.889164, Accuracy: 85.22%\n",
      "Batch 73, Loss: 0.960235, Accuracy: 85.12%\n",
      "Batch 74, Loss: 0.823458, Accuracy: 85.24%\n",
      "Batch 75, Loss: 0.903930, Accuracy: 85.23%\n",
      "Batch 76, Loss: 0.831949, Accuracy: 85.30%\n",
      "Batch 77, Loss: 0.907537, Accuracy: 85.29%\n",
      "Batch 78, Loss: 0.932994, Accuracy: 85.26%\n",
      "Batch 79, Loss: 0.889058, Accuracy: 85.25%\n",
      "Batch 80, Loss: 0.910625, Accuracy: 85.23%\n",
      "Batch 81, Loss: 0.849834, Accuracy: 85.30%\n",
      "Batch 82, Loss: 0.873398, Accuracy: 85.35%\n",
      "Batch 83, Loss: 0.922092, Accuracy: 85.30%\n",
      "Batch 84, Loss: 0.914128, Accuracy: 85.27%\n",
      "Batch 85, Loss: 0.983884, Accuracy: 85.15%\n",
      "Batch 86, Loss: 0.884104, Accuracy: 85.14%\n",
      "Batch 87, Loss: 0.962578, Accuracy: 85.06%\n",
      "Batch 88, Loss: 0.857866, Accuracy: 85.09%\n",
      "Batch 89, Loss: 0.907664, Accuracy: 85.08%\n",
      "Batch 90, Loss: 0.907552, Accuracy: 85.07%\n",
      "Batch 91, Loss: 0.969962, Accuracy: 84.98%\n",
      "Batch 92, Loss: 0.929720, Accuracy: 84.95%\n",
      "Batch 93, Loss: 0.886803, Accuracy: 84.95%\n",
      "Batch 94, Loss: 0.915208, Accuracy: 84.92%\n",
      "Batch 95, Loss: 0.913733, Accuracy: 84.90%\n",
      "Batch 96, Loss: 0.943609, Accuracy: 84.85%\n",
      "Batch 97, Loss: 0.910238, Accuracy: 84.84%\n",
      "Batch 98, Loss: 0.900054, Accuracy: 84.82%\n",
      "Batch 99, Loss: 0.914489, Accuracy: 84.82%\n",
      "Batch 100, Loss: 0.844853, Accuracy: 84.88%\n",
      "Batch 101, Loss: 0.890137, Accuracy: 84.89%\n",
      "Batch 102, Loss: 0.918183, Accuracy: 84.87%\n",
      "Batch 103, Loss: 0.952200, Accuracy: 84.81%\n",
      "Batch 104, Loss: 0.891939, Accuracy: 84.83%\n",
      "Batch 105, Loss: 0.924047, Accuracy: 84.81%\n",
      "Batch 106, Loss: 0.918330, Accuracy: 84.77%\n",
      "Batch 107, Loss: 0.920950, Accuracy: 84.77%\n",
      "Batch 108, Loss: 0.961468, Accuracy: 84.71%\n",
      "Batch 109, Loss: 0.836131, Accuracy: 84.76%\n",
      "Batch 110, Loss: 0.960392, Accuracy: 84.72%\n",
      "Batch 111, Loss: 0.830615, Accuracy: 84.77%\n",
      "Batch 112, Loss: 0.808206, Accuracy: 84.85%\n",
      "Batch 113, Loss: 0.984222, Accuracy: 84.78%\n",
      "Batch 114, Loss: 0.909312, Accuracy: 84.77%\n",
      "Batch 115, Loss: 0.877762, Accuracy: 84.80%\n",
      "Batch 116, Loss: 0.836213, Accuracy: 84.85%\n",
      "Batch 117, Loss: 0.915444, Accuracy: 84.83%\n",
      "Batch 118, Loss: 0.892800, Accuracy: 84.83%\n",
      "Batch 119, Loss: 0.950405, Accuracy: 84.77%\n",
      "Batch 120, Loss: 0.971604, Accuracy: 84.69%\n",
      "Batch 121, Loss: 0.899585, Accuracy: 84.70%\n",
      "Batch 122, Loss: 0.886483, Accuracy: 84.70%\n",
      "Batch 123, Loss: 0.899215, Accuracy: 84.69%\n",
      "Batch 124, Loss: 0.899196, Accuracy: 84.69%\n",
      "Batch 125, Loss: 0.916377, Accuracy: 84.69%\n",
      "Batch 126, Loss: 0.901428, Accuracy: 84.69%\n",
      "Batch 127, Loss: 0.878729, Accuracy: 84.71%\n",
      "Batch 128, Loss: 0.924104, Accuracy: 84.69%\n",
      "Batch 129, Loss: 0.862025, Accuracy: 84.73%\n",
      "Batch 130, Loss: 0.777935, Accuracy: 84.82%\n",
      "Batch 131, Loss: 0.887443, Accuracy: 84.84%\n",
      "Batch 132, Loss: 0.857123, Accuracy: 84.88%\n",
      "Batch 133, Loss: 0.882348, Accuracy: 84.88%\n",
      "Batch 134, Loss: 0.920053, Accuracy: 84.86%\n",
      "Batch 135, Loss: 0.960372, Accuracy: 84.80%\n",
      "Batch 136, Loss: 0.949356, Accuracy: 84.77%\n",
      "Batch 137, Loss: 0.841843, Accuracy: 84.80%\n",
      "Batch 138, Loss: 0.870305, Accuracy: 84.82%\n",
      "Batch 139, Loss: 0.871224, Accuracy: 84.84%\n",
      "Batch 140, Loss: 0.950189, Accuracy: 84.80%\n",
      "Batch 141, Loss: 0.847982, Accuracy: 84.83%\n",
      "Batch 142, Loss: 0.881144, Accuracy: 84.84%\n",
      "Batch 143, Loss: 0.885375, Accuracy: 84.84%\n",
      "Batch 144, Loss: 0.886330, Accuracy: 84.85%\n",
      "Batch 145, Loss: 0.828698, Accuracy: 84.89%\n",
      "Batch 146, Loss: 0.949359, Accuracy: 84.86%\n",
      "Batch 147, Loss: 0.930864, Accuracy: 84.83%\n",
      "Batch 148, Loss: 0.984003, Accuracy: 84.78%\n",
      "Batch 149, Loss: 0.923764, Accuracy: 84.75%\n",
      "Batch 150, Loss: 0.955891, Accuracy: 84.71%\n",
      "Batch 151, Loss: 0.956816, Accuracy: 84.65%\n",
      "Batch 152, Loss: 0.871555, Accuracy: 84.68%\n",
      "Batch 153, Loss: 0.905934, Accuracy: 84.68%\n",
      "Batch 154, Loss: 0.907244, Accuracy: 84.67%\n",
      "Batch 155, Loss: 0.936294, Accuracy: 84.65%\n",
      "Batch 156, Loss: 0.882343, Accuracy: 84.67%\n",
      "Batch 157, Loss: 0.913835, Accuracy: 84.66%\n",
      "Batch 158, Loss: 0.862633, Accuracy: 84.70%\n",
      "Batch 159, Loss: 0.902760, Accuracy: 84.71%\n",
      "Batch 160, Loss: 0.828495, Accuracy: 84.76%\n",
      "Batch 161, Loss: 0.976508, Accuracy: 84.70%\n",
      "Batch 162, Loss: 0.895443, Accuracy: 84.70%\n",
      "Batch 163, Loss: 0.841891, Accuracy: 84.75%\n",
      "Batch 164, Loss: 0.823357, Accuracy: 84.79%\n",
      "Batch 165, Loss: 0.862545, Accuracy: 84.82%\n",
      "Batch 166, Loss: 0.868924, Accuracy: 84.84%\n",
      "Batch 167, Loss: 0.901360, Accuracy: 84.83%\n",
      "Batch 168, Loss: 0.917873, Accuracy: 84.81%\n",
      "Batch 169, Loss: 0.877668, Accuracy: 84.84%\n",
      "Batch 170, Loss: 0.886711, Accuracy: 84.84%\n",
      "Batch 171, Loss: 0.894633, Accuracy: 84.85%\n",
      "Batch 172, Loss: 0.885602, Accuracy: 84.85%\n",
      "Batch 173, Loss: 0.916768, Accuracy: 84.84%\n",
      "Batch 174, Loss: 0.946079, Accuracy: 84.80%\n",
      "Batch 175, Loss: 0.896283, Accuracy: 84.81%\n",
      "Batch 176, Loss: 0.893875, Accuracy: 84.83%\n",
      "Batch 177, Loss: 0.878928, Accuracy: 84.84%\n",
      "Batch 178, Loss: 0.867973, Accuracy: 84.85%\n",
      "Batch 179, Loss: 0.958952, Accuracy: 84.81%\n",
      "Batch 180, Loss: 0.906592, Accuracy: 84.80%\n",
      "Batch 181, Loss: 0.874788, Accuracy: 84.82%\n",
      "Batch 182, Loss: 0.868096, Accuracy: 84.82%\n",
      "Batch 183, Loss: 0.882990, Accuracy: 84.83%\n",
      "Batch 184, Loss: 0.876706, Accuracy: 84.86%\n",
      "Batch 185, Loss: 0.936764, Accuracy: 84.85%\n",
      "Batch 186, Loss: 0.852406, Accuracy: 84.87%\n",
      "Batch 187, Loss: 0.972326, Accuracy: 84.83%\n",
      "Batch 188, Loss: 0.963932, Accuracy: 84.78%\n",
      "Batch 189, Loss: 0.902671, Accuracy: 84.77%\n",
      "Batch 190, Loss: 0.919199, Accuracy: 84.76%\n",
      "Batch 191, Loss: 0.881634, Accuracy: 84.78%\n",
      "Batch 192, Loss: 0.968407, Accuracy: 84.72%\n",
      "Batch 193, Loss: 0.910717, Accuracy: 84.72%\n",
      "Batch 194, Loss: 0.901216, Accuracy: 84.72%\n",
      "Batch 195, Loss: 0.928009, Accuracy: 84.71%\n",
      "Batch 196, Loss: 0.914002, Accuracy: 84.69%\n",
      "Batch 197, Loss: 0.890614, Accuracy: 84.70%\n",
      "Batch 198, Loss: 0.818477, Accuracy: 84.74%\n",
      "Batch 199, Loss: 0.887365, Accuracy: 84.74%\n",
      "Batch 200, Loss: 0.968719, Accuracy: 84.70%\n",
      "Batch 201, Loss: 0.877780, Accuracy: 84.71%\n",
      "Batch 202, Loss: 0.876964, Accuracy: 84.72%\n",
      "Batch 203, Loss: 0.860157, Accuracy: 84.74%\n",
      "Batch 204, Loss: 0.868128, Accuracy: 84.76%\n",
      "Batch 205, Loss: 0.925726, Accuracy: 84.74%\n",
      "Batch 206, Loss: 0.893140, Accuracy: 84.75%\n",
      "Batch 207, Loss: 0.862719, Accuracy: 84.77%\n",
      "Batch 208, Loss: 0.803236, Accuracy: 84.81%\n",
      "Batch 209, Loss: 0.978025, Accuracy: 84.76%\n",
      "Batch 210, Loss: 0.882246, Accuracy: 84.78%\n",
      "Batch 211, Loss: 0.877427, Accuracy: 84.79%\n",
      "Batch 212, Loss: 0.905371, Accuracy: 84.79%\n",
      "Batch 213, Loss: 0.885138, Accuracy: 84.81%\n",
      "Training - Epoch 63, Loss: 0.897775, Accuracy: 84.81%\n",
      "Validation Batch 1, Loss: 0.874484, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.863193, Accuracy: 89.06%\n",
      "Validation Batch 3, Loss: 0.946874, Accuracy: 86.46%\n",
      "Validation Batch 4, Loss: 0.888154, Accuracy: 85.94%\n",
      "Validation Batch 5, Loss: 0.877870, Accuracy: 85.94%\n",
      "Validation Batch 6, Loss: 0.832320, Accuracy: 86.98%\n",
      "Validation Batch 7, Loss: 0.857304, Accuracy: 87.50%\n",
      "Validation Batch 8, Loss: 0.960709, Accuracy: 86.33%\n",
      "Validation Batch 9, Loss: 0.927181, Accuracy: 85.94%\n",
      "Validation Batch 10, Loss: 0.911959, Accuracy: 85.47%\n",
      "Validation Batch 11, Loss: 0.893556, Accuracy: 85.37%\n",
      "Validation Batch 12, Loss: 0.883705, Accuracy: 85.55%\n",
      "Validation Batch 13, Loss: 0.884748, Accuracy: 85.70%\n",
      "Validation Batch 14, Loss: 0.918599, Accuracy: 85.38%\n",
      "Validation Batch 15, Loss: 0.873528, Accuracy: 85.31%\n",
      "Validation Batch 16, Loss: 0.878502, Accuracy: 85.55%\n",
      "Validation Batch 17, Loss: 0.917082, Accuracy: 85.57%\n",
      "Validation Batch 18, Loss: 0.863373, Accuracy: 85.76%\n",
      "Validation Batch 19, Loss: 0.926842, Accuracy: 85.61%\n",
      "Validation Batch 20, Loss: 0.972496, Accuracy: 85.08%\n",
      "Validation Batch 21, Loss: 0.919441, Accuracy: 85.04%\n",
      "Validation Batch 22, Loss: 0.890027, Accuracy: 85.09%\n",
      "Validation Batch 23, Loss: 0.925508, Accuracy: 84.92%\n",
      "Validation Batch 24, Loss: 0.905140, Accuracy: 84.90%\n",
      "Validation Batch 25, Loss: 0.860818, Accuracy: 85.06%\n",
      "Validation Batch 26, Loss: 0.885356, Accuracy: 85.16%\n",
      "Validation Batch 27, Loss: 0.877370, Accuracy: 85.32%\n",
      "Validation - Epoch 63, Loss: 0.896894, Accuracy: 85.32%\n",
      "Patience—0\n",
      "Epoch 64\n",
      "Batch 1, Loss: 0.910082, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.898842, Accuracy: 83.59%\n",
      "Batch 3, Loss: 0.949526, Accuracy: 82.29%\n",
      "Batch 4, Loss: 0.931364, Accuracy: 82.03%\n",
      "Batch 5, Loss: 0.889919, Accuracy: 82.81%\n",
      "Batch 6, Loss: 0.953428, Accuracy: 82.03%\n",
      "Batch 7, Loss: 0.876461, Accuracy: 82.81%\n",
      "Batch 8, Loss: 0.901369, Accuracy: 83.01%\n",
      "Batch 9, Loss: 0.933222, Accuracy: 82.64%\n",
      "Batch 10, Loss: 0.928137, Accuracy: 82.50%\n",
      "Batch 11, Loss: 0.913645, Accuracy: 82.53%\n",
      "Batch 12, Loss: 0.840549, Accuracy: 83.20%\n",
      "Batch 13, Loss: 0.888898, Accuracy: 83.53%\n",
      "Batch 14, Loss: 0.840870, Accuracy: 84.04%\n",
      "Batch 15, Loss: 0.879795, Accuracy: 84.17%\n",
      "Batch 16, Loss: 0.959269, Accuracy: 83.89%\n",
      "Batch 17, Loss: 0.915511, Accuracy: 83.73%\n",
      "Batch 18, Loss: 1.005874, Accuracy: 83.16%\n",
      "Batch 19, Loss: 0.841561, Accuracy: 83.55%\n",
      "Batch 20, Loss: 0.872892, Accuracy: 83.75%\n",
      "Batch 21, Loss: 0.883399, Accuracy: 83.78%\n",
      "Batch 22, Loss: 0.906159, Accuracy: 83.74%\n",
      "Batch 23, Loss: 0.913405, Accuracy: 83.70%\n",
      "Batch 24, Loss: 0.902761, Accuracy: 83.72%\n",
      "Batch 25, Loss: 0.899244, Accuracy: 83.75%\n",
      "Batch 26, Loss: 0.904862, Accuracy: 83.83%\n",
      "Batch 27, Loss: 0.914560, Accuracy: 83.85%\n",
      "Batch 28, Loss: 0.873607, Accuracy: 83.98%\n",
      "Batch 29, Loss: 0.906458, Accuracy: 84.05%\n",
      "Batch 30, Loss: 0.897851, Accuracy: 84.06%\n",
      "Batch 31, Loss: 0.880930, Accuracy: 84.17%\n",
      "Batch 32, Loss: 0.909521, Accuracy: 84.13%\n",
      "Batch 33, Loss: 0.891776, Accuracy: 84.19%\n",
      "Batch 34, Loss: 0.863638, Accuracy: 84.33%\n",
      "Batch 35, Loss: 0.880106, Accuracy: 84.38%\n",
      "Batch 36, Loss: 0.846158, Accuracy: 84.55%\n",
      "Batch 37, Loss: 0.859930, Accuracy: 84.63%\n",
      "Batch 38, Loss: 0.933354, Accuracy: 84.54%\n",
      "Batch 39, Loss: 0.885365, Accuracy: 84.62%\n",
      "Batch 40, Loss: 0.909751, Accuracy: 84.61%\n",
      "Batch 41, Loss: 0.864564, Accuracy: 84.72%\n",
      "Batch 42, Loss: 0.924407, Accuracy: 84.67%\n",
      "Batch 43, Loss: 0.887867, Accuracy: 84.70%\n",
      "Batch 44, Loss: 0.879488, Accuracy: 84.73%\n",
      "Batch 45, Loss: 0.874104, Accuracy: 84.83%\n",
      "Batch 46, Loss: 0.900054, Accuracy: 84.75%\n",
      "Batch 47, Loss: 0.864161, Accuracy: 84.81%\n",
      "Batch 48, Loss: 0.898565, Accuracy: 84.80%\n",
      "Batch 49, Loss: 0.846430, Accuracy: 84.89%\n",
      "Batch 50, Loss: 0.863166, Accuracy: 84.94%\n",
      "Batch 51, Loss: 0.893513, Accuracy: 84.93%\n",
      "Batch 52, Loss: 0.891507, Accuracy: 84.92%\n",
      "Batch 53, Loss: 0.912959, Accuracy: 84.88%\n",
      "Batch 54, Loss: 0.822239, Accuracy: 85.04%\n",
      "Batch 55, Loss: 0.921277, Accuracy: 84.97%\n",
      "Batch 56, Loss: 0.864287, Accuracy: 85.04%\n",
      "Batch 57, Loss: 0.860171, Accuracy: 85.12%\n",
      "Batch 58, Loss: 0.888529, Accuracy: 85.13%\n",
      "Batch 59, Loss: 0.857060, Accuracy: 85.20%\n",
      "Batch 60, Loss: 0.890757, Accuracy: 85.18%\n",
      "Batch 61, Loss: 0.890096, Accuracy: 85.19%\n",
      "Batch 62, Loss: 0.884813, Accuracy: 85.21%\n",
      "Batch 63, Loss: 0.907114, Accuracy: 85.19%\n",
      "Batch 64, Loss: 0.832854, Accuracy: 85.30%\n",
      "Batch 65, Loss: 0.938739, Accuracy: 85.24%\n",
      "Batch 66, Loss: 0.957816, Accuracy: 85.13%\n",
      "Batch 67, Loss: 0.927664, Accuracy: 85.05%\n",
      "Batch 68, Loss: 0.904258, Accuracy: 85.02%\n",
      "Batch 69, Loss: 0.859753, Accuracy: 85.05%\n",
      "Batch 70, Loss: 0.897685, Accuracy: 85.04%\n",
      "Batch 71, Loss: 0.917171, Accuracy: 84.99%\n",
      "Batch 72, Loss: 0.850681, Accuracy: 85.05%\n",
      "Batch 73, Loss: 0.893176, Accuracy: 85.04%\n",
      "Batch 74, Loss: 0.920703, Accuracy: 85.01%\n",
      "Batch 75, Loss: 0.922686, Accuracy: 84.96%\n",
      "Batch 76, Loss: 0.876012, Accuracy: 84.99%\n",
      "Batch 77, Loss: 0.984760, Accuracy: 84.84%\n",
      "Batch 78, Loss: 0.812384, Accuracy: 84.96%\n",
      "Batch 79, Loss: 0.890053, Accuracy: 84.95%\n",
      "Batch 80, Loss: 0.872676, Accuracy: 85.00%\n",
      "Batch 81, Loss: 0.932676, Accuracy: 84.97%\n",
      "Batch 82, Loss: 0.843494, Accuracy: 85.02%\n",
      "Batch 83, Loss: 0.941432, Accuracy: 84.96%\n",
      "Batch 84, Loss: 0.905954, Accuracy: 84.93%\n",
      "Batch 85, Loss: 0.906361, Accuracy: 84.91%\n",
      "Batch 86, Loss: 0.941187, Accuracy: 84.87%\n",
      "Batch 87, Loss: 0.840903, Accuracy: 84.97%\n",
      "Batch 88, Loss: 0.852333, Accuracy: 85.03%\n",
      "Batch 89, Loss: 0.890049, Accuracy: 85.04%\n",
      "Batch 90, Loss: 0.902795, Accuracy: 85.03%\n",
      "Batch 91, Loss: 0.860385, Accuracy: 85.08%\n",
      "Batch 92, Loss: 0.931787, Accuracy: 85.05%\n",
      "Batch 93, Loss: 0.859475, Accuracy: 85.11%\n",
      "Batch 94, Loss: 0.943994, Accuracy: 85.07%\n",
      "Batch 95, Loss: 0.930583, Accuracy: 85.03%\n",
      "Batch 96, Loss: 0.902063, Accuracy: 85.03%\n",
      "Batch 97, Loss: 1.045093, Accuracy: 84.86%\n",
      "Batch 98, Loss: 0.869856, Accuracy: 84.90%\n",
      "Batch 99, Loss: 0.876890, Accuracy: 84.93%\n",
      "Batch 100, Loss: 0.868620, Accuracy: 84.97%\n",
      "Batch 101, Loss: 0.888211, Accuracy: 84.95%\n",
      "Batch 102, Loss: 0.893371, Accuracy: 84.97%\n",
      "Batch 103, Loss: 0.919616, Accuracy: 84.94%\n",
      "Batch 104, Loss: 0.898889, Accuracy: 84.92%\n",
      "Batch 105, Loss: 0.904921, Accuracy: 84.91%\n",
      "Batch 106, Loss: 0.935683, Accuracy: 84.88%\n",
      "Batch 107, Loss: 0.923274, Accuracy: 84.84%\n",
      "Batch 108, Loss: 0.947139, Accuracy: 84.79%\n",
      "Batch 109, Loss: 0.860150, Accuracy: 84.83%\n",
      "Batch 110, Loss: 0.907753, Accuracy: 84.82%\n",
      "Batch 111, Loss: 0.868071, Accuracy: 84.83%\n",
      "Batch 112, Loss: 0.916004, Accuracy: 84.81%\n",
      "Batch 113, Loss: 0.876744, Accuracy: 84.82%\n",
      "Batch 114, Loss: 0.857813, Accuracy: 84.85%\n",
      "Batch 115, Loss: 0.911336, Accuracy: 84.84%\n",
      "Batch 116, Loss: 0.878149, Accuracy: 84.86%\n",
      "Batch 117, Loss: 0.861022, Accuracy: 84.88%\n",
      "Batch 118, Loss: 0.928745, Accuracy: 84.85%\n",
      "Batch 119, Loss: 0.962962, Accuracy: 84.81%\n",
      "Batch 120, Loss: 0.907912, Accuracy: 84.79%\n",
      "Batch 121, Loss: 0.942688, Accuracy: 84.76%\n",
      "Batch 122, Loss: 0.899475, Accuracy: 84.75%\n",
      "Batch 123, Loss: 0.903913, Accuracy: 84.73%\n",
      "Batch 124, Loss: 0.896307, Accuracy: 84.73%\n",
      "Batch 125, Loss: 0.941373, Accuracy: 84.67%\n",
      "Batch 126, Loss: 0.870752, Accuracy: 84.69%\n",
      "Batch 127, Loss: 0.952503, Accuracy: 84.63%\n",
      "Batch 128, Loss: 0.892147, Accuracy: 84.63%\n",
      "Batch 129, Loss: 0.903042, Accuracy: 84.62%\n",
      "Batch 130, Loss: 0.923070, Accuracy: 84.60%\n",
      "Batch 131, Loss: 0.887472, Accuracy: 84.60%\n",
      "Batch 132, Loss: 0.891724, Accuracy: 84.61%\n",
      "Batch 133, Loss: 0.926122, Accuracy: 84.59%\n",
      "Batch 134, Loss: 0.901867, Accuracy: 84.58%\n",
      "Batch 135, Loss: 0.852320, Accuracy: 84.63%\n",
      "Batch 136, Loss: 0.794330, Accuracy: 84.73%\n",
      "Batch 137, Loss: 0.939237, Accuracy: 84.71%\n",
      "Batch 138, Loss: 0.925380, Accuracy: 84.69%\n",
      "Batch 139, Loss: 0.872684, Accuracy: 84.71%\n",
      "Batch 140, Loss: 0.875614, Accuracy: 84.72%\n",
      "Batch 141, Loss: 0.889154, Accuracy: 84.74%\n",
      "Batch 142, Loss: 0.953585, Accuracy: 84.68%\n",
      "Batch 143, Loss: 0.953309, Accuracy: 84.65%\n",
      "Batch 144, Loss: 0.885475, Accuracy: 84.64%\n",
      "Batch 145, Loss: 0.853664, Accuracy: 84.68%\n",
      "Batch 146, Loss: 0.887669, Accuracy: 84.69%\n",
      "Batch 147, Loss: 0.871386, Accuracy: 84.72%\n",
      "Batch 148, Loss: 0.888069, Accuracy: 84.71%\n",
      "Batch 149, Loss: 0.837655, Accuracy: 84.76%\n",
      "Batch 150, Loss: 0.859801, Accuracy: 84.79%\n",
      "Batch 151, Loss: 0.906693, Accuracy: 84.78%\n",
      "Batch 152, Loss: 0.892318, Accuracy: 84.78%\n",
      "Batch 153, Loss: 0.920388, Accuracy: 84.75%\n",
      "Batch 154, Loss: 0.821521, Accuracy: 84.81%\n",
      "Batch 155, Loss: 0.962786, Accuracy: 84.77%\n",
      "Batch 156, Loss: 0.866659, Accuracy: 84.79%\n",
      "Batch 157, Loss: 0.877290, Accuracy: 84.80%\n",
      "Batch 158, Loss: 0.869846, Accuracy: 84.82%\n",
      "Batch 159, Loss: 0.859381, Accuracy: 84.84%\n",
      "Batch 160, Loss: 0.875187, Accuracy: 84.85%\n",
      "Batch 161, Loss: 1.044611, Accuracy: 84.74%\n",
      "Batch 162, Loss: 0.816820, Accuracy: 84.81%\n",
      "Batch 163, Loss: 0.845928, Accuracy: 84.84%\n",
      "Batch 164, Loss: 0.888144, Accuracy: 84.86%\n",
      "Batch 165, Loss: 0.909698, Accuracy: 84.85%\n",
      "Batch 166, Loss: 0.913532, Accuracy: 84.84%\n",
      "Batch 167, Loss: 0.875017, Accuracy: 84.85%\n",
      "Batch 168, Loss: 0.891477, Accuracy: 84.85%\n",
      "Batch 169, Loss: 0.897705, Accuracy: 84.86%\n",
      "Batch 170, Loss: 0.905545, Accuracy: 84.84%\n",
      "Batch 171, Loss: 0.836539, Accuracy: 84.89%\n",
      "Batch 172, Loss: 0.890258, Accuracy: 84.89%\n",
      "Batch 173, Loss: 0.920278, Accuracy: 84.87%\n",
      "Batch 174, Loss: 0.878954, Accuracy: 84.88%\n",
      "Batch 175, Loss: 0.862822, Accuracy: 84.90%\n",
      "Batch 176, Loss: 0.890397, Accuracy: 84.90%\n",
      "Batch 177, Loss: 0.949194, Accuracy: 84.87%\n",
      "Batch 178, Loss: 0.821709, Accuracy: 84.92%\n",
      "Batch 179, Loss: 0.926729, Accuracy: 84.90%\n",
      "Batch 180, Loss: 0.851617, Accuracy: 84.92%\n",
      "Batch 181, Loss: 0.855614, Accuracy: 84.94%\n",
      "Batch 182, Loss: 0.872475, Accuracy: 84.95%\n",
      "Batch 183, Loss: 0.903245, Accuracy: 84.96%\n",
      "Batch 184, Loss: 0.930626, Accuracy: 84.95%\n",
      "Batch 185, Loss: 0.953023, Accuracy: 84.92%\n",
      "Batch 186, Loss: 0.971349, Accuracy: 84.89%\n",
      "Batch 187, Loss: 0.953728, Accuracy: 84.87%\n",
      "Batch 188, Loss: 0.895988, Accuracy: 84.87%\n",
      "Batch 189, Loss: 0.864651, Accuracy: 84.89%\n",
      "Batch 190, Loss: 0.876219, Accuracy: 84.90%\n",
      "Batch 191, Loss: 0.896360, Accuracy: 84.90%\n",
      "Batch 192, Loss: 0.886867, Accuracy: 84.89%\n",
      "Batch 193, Loss: 0.913755, Accuracy: 84.89%\n",
      "Batch 194, Loss: 0.853182, Accuracy: 84.91%\n",
      "Batch 195, Loss: 0.849588, Accuracy: 84.94%\n",
      "Batch 196, Loss: 0.927097, Accuracy: 84.93%\n",
      "Batch 197, Loss: 0.816358, Accuracy: 84.97%\n",
      "Batch 198, Loss: 0.964550, Accuracy: 84.94%\n",
      "Batch 199, Loss: 0.886080, Accuracy: 84.95%\n",
      "Batch 200, Loss: 0.935100, Accuracy: 84.93%\n",
      "Batch 201, Loss: 0.889228, Accuracy: 84.93%\n",
      "Batch 202, Loss: 0.899705, Accuracy: 84.92%\n",
      "Batch 203, Loss: 0.803318, Accuracy: 84.95%\n",
      "Batch 204, Loss: 0.943108, Accuracy: 84.92%\n",
      "Batch 205, Loss: 0.845474, Accuracy: 84.95%\n",
      "Batch 206, Loss: 0.939464, Accuracy: 84.92%\n",
      "Batch 207, Loss: 0.908980, Accuracy: 84.92%\n",
      "Batch 208, Loss: 0.888642, Accuracy: 84.92%\n",
      "Batch 209, Loss: 0.902901, Accuracy: 84.91%\n",
      "Batch 210, Loss: 0.868545, Accuracy: 84.93%\n",
      "Batch 211, Loss: 0.868075, Accuracy: 84.95%\n",
      "Batch 212, Loss: 0.922545, Accuracy: 84.93%\n",
      "Batch 213, Loss: 0.872012, Accuracy: 84.94%\n",
      "Training - Epoch 64, Loss: 0.895458, Accuracy: 84.94%\n",
      "Validation Batch 1, Loss: 0.884573, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.887998, Accuracy: 88.28%\n",
      "Validation Batch 3, Loss: 0.961392, Accuracy: 85.94%\n",
      "Validation Batch 4, Loss: 0.906598, Accuracy: 84.77%\n",
      "Validation Batch 5, Loss: 0.894230, Accuracy: 85.00%\n",
      "Validation Batch 6, Loss: 0.848948, Accuracy: 86.20%\n",
      "Validation Batch 7, Loss: 0.862624, Accuracy: 86.83%\n",
      "Validation Batch 8, Loss: 0.968842, Accuracy: 85.55%\n",
      "Validation Batch 9, Loss: 0.940096, Accuracy: 85.42%\n",
      "Validation Batch 10, Loss: 0.914534, Accuracy: 85.00%\n",
      "Validation Batch 11, Loss: 0.907695, Accuracy: 84.94%\n",
      "Validation Batch 12, Loss: 0.905759, Accuracy: 84.64%\n",
      "Validation Batch 13, Loss: 0.891081, Accuracy: 84.86%\n",
      "Validation Batch 14, Loss: 0.927055, Accuracy: 84.60%\n",
      "Validation Batch 15, Loss: 0.872031, Accuracy: 84.69%\n",
      "Validation Batch 16, Loss: 0.887055, Accuracy: 85.06%\n",
      "Validation Batch 17, Loss: 0.925772, Accuracy: 84.93%\n",
      "Validation Batch 18, Loss: 0.875717, Accuracy: 84.98%\n",
      "Validation Batch 19, Loss: 0.938896, Accuracy: 84.70%\n",
      "Validation Batch 20, Loss: 0.990529, Accuracy: 84.14%\n",
      "Validation Batch 21, Loss: 0.933745, Accuracy: 84.08%\n",
      "Validation Batch 22, Loss: 0.896558, Accuracy: 84.09%\n",
      "Validation Batch 23, Loss: 0.934977, Accuracy: 83.97%\n",
      "Validation Batch 24, Loss: 0.913627, Accuracy: 83.92%\n",
      "Validation Batch 25, Loss: 0.870098, Accuracy: 84.12%\n",
      "Validation Batch 26, Loss: 0.900216, Accuracy: 84.19%\n",
      "Validation Batch 27, Loss: 0.909796, Accuracy: 84.26%\n",
      "Validation - Epoch 64, Loss: 0.909276, Accuracy: 84.26%\n",
      "Patience—1\n",
      "Epoch 65\n",
      "Batch 1, Loss: 0.903363, Accuracy: 84.38%\n",
      "Batch 2, Loss: 0.953285, Accuracy: 82.03%\n",
      "Batch 3, Loss: 0.880853, Accuracy: 83.85%\n",
      "Batch 4, Loss: 0.895171, Accuracy: 83.98%\n",
      "Batch 5, Loss: 0.937746, Accuracy: 82.81%\n",
      "Batch 6, Loss: 0.868835, Accuracy: 83.85%\n",
      "Batch 7, Loss: 0.944838, Accuracy: 83.26%\n",
      "Batch 8, Loss: 0.848962, Accuracy: 84.18%\n",
      "Batch 9, Loss: 0.928759, Accuracy: 83.85%\n",
      "Batch 10, Loss: 0.886940, Accuracy: 84.22%\n",
      "Batch 11, Loss: 0.941181, Accuracy: 83.81%\n",
      "Batch 12, Loss: 0.917352, Accuracy: 83.72%\n",
      "Batch 13, Loss: 0.811143, Accuracy: 84.50%\n",
      "Batch 14, Loss: 0.954950, Accuracy: 84.04%\n",
      "Batch 15, Loss: 0.911188, Accuracy: 83.96%\n",
      "Batch 16, Loss: 0.863743, Accuracy: 84.18%\n",
      "Batch 17, Loss: 0.864523, Accuracy: 84.38%\n",
      "Batch 18, Loss: 0.882187, Accuracy: 84.55%\n",
      "Batch 19, Loss: 0.868764, Accuracy: 84.70%\n",
      "Batch 20, Loss: 0.911418, Accuracy: 84.69%\n",
      "Batch 21, Loss: 0.890715, Accuracy: 84.75%\n",
      "Batch 22, Loss: 0.871097, Accuracy: 84.87%\n",
      "Batch 23, Loss: 0.924637, Accuracy: 84.71%\n",
      "Batch 24, Loss: 0.825743, Accuracy: 85.09%\n",
      "Batch 25, Loss: 0.896283, Accuracy: 85.00%\n",
      "Batch 26, Loss: 0.912906, Accuracy: 84.92%\n",
      "Batch 27, Loss: 0.935412, Accuracy: 84.72%\n",
      "Batch 28, Loss: 0.899719, Accuracy: 84.65%\n",
      "Batch 29, Loss: 0.822398, Accuracy: 84.91%\n",
      "Batch 30, Loss: 0.894153, Accuracy: 85.00%\n",
      "Batch 31, Loss: 0.991112, Accuracy: 84.68%\n",
      "Batch 32, Loss: 0.892645, Accuracy: 84.72%\n",
      "Batch 33, Loss: 0.864437, Accuracy: 84.85%\n",
      "Batch 34, Loss: 0.882636, Accuracy: 84.93%\n",
      "Batch 35, Loss: 0.877064, Accuracy: 85.00%\n",
      "Batch 36, Loss: 0.902430, Accuracy: 84.98%\n",
      "Batch 37, Loss: 0.847709, Accuracy: 85.09%\n",
      "Batch 38, Loss: 0.896353, Accuracy: 85.07%\n",
      "Batch 39, Loss: 0.852775, Accuracy: 85.22%\n",
      "Batch 40, Loss: 0.974562, Accuracy: 84.96%\n",
      "Batch 41, Loss: 0.929748, Accuracy: 84.87%\n",
      "Batch 42, Loss: 0.908206, Accuracy: 84.90%\n",
      "Batch 43, Loss: 0.885104, Accuracy: 84.96%\n",
      "Batch 44, Loss: 0.936922, Accuracy: 84.84%\n",
      "Batch 45, Loss: 0.888023, Accuracy: 84.86%\n",
      "Batch 46, Loss: 0.899009, Accuracy: 84.88%\n",
      "Batch 47, Loss: 0.912259, Accuracy: 84.84%\n",
      "Batch 48, Loss: 0.848796, Accuracy: 84.96%\n",
      "Batch 49, Loss: 0.912058, Accuracy: 84.89%\n",
      "Batch 50, Loss: 0.921576, Accuracy: 84.88%\n",
      "Batch 51, Loss: 0.929187, Accuracy: 84.80%\n",
      "Batch 52, Loss: 0.895037, Accuracy: 84.80%\n",
      "Batch 53, Loss: 0.943963, Accuracy: 84.70%\n",
      "Batch 54, Loss: 0.854279, Accuracy: 84.78%\n",
      "Batch 55, Loss: 0.884827, Accuracy: 84.77%\n",
      "Batch 56, Loss: 0.816062, Accuracy: 84.93%\n",
      "Batch 57, Loss: 1.010063, Accuracy: 84.73%\n",
      "Batch 58, Loss: 0.921180, Accuracy: 84.62%\n",
      "Batch 59, Loss: 0.854294, Accuracy: 84.67%\n",
      "Batch 60, Loss: 0.862946, Accuracy: 84.74%\n",
      "Batch 61, Loss: 0.808240, Accuracy: 84.89%\n",
      "Batch 62, Loss: 0.892656, Accuracy: 84.88%\n",
      "Batch 63, Loss: 0.910967, Accuracy: 84.85%\n",
      "Batch 64, Loss: 0.907233, Accuracy: 84.81%\n",
      "Batch 65, Loss: 0.907200, Accuracy: 84.78%\n",
      "Batch 66, Loss: 0.871125, Accuracy: 84.80%\n",
      "Batch 67, Loss: 0.902272, Accuracy: 84.77%\n",
      "Batch 68, Loss: 0.872440, Accuracy: 84.81%\n",
      "Batch 69, Loss: 0.898322, Accuracy: 84.81%\n",
      "Batch 70, Loss: 0.973641, Accuracy: 84.69%\n",
      "Batch 71, Loss: 0.924162, Accuracy: 84.68%\n",
      "Batch 72, Loss: 0.870915, Accuracy: 84.72%\n",
      "Batch 73, Loss: 0.896912, Accuracy: 84.76%\n",
      "Batch 74, Loss: 0.886374, Accuracy: 84.78%\n",
      "Batch 75, Loss: 0.947497, Accuracy: 84.73%\n",
      "Batch 76, Loss: 0.820452, Accuracy: 84.85%\n",
      "Batch 77, Loss: 0.938221, Accuracy: 84.78%\n",
      "Batch 78, Loss: 0.902580, Accuracy: 84.78%\n",
      "Batch 79, Loss: 0.921329, Accuracy: 84.77%\n",
      "Batch 80, Loss: 0.908184, Accuracy: 84.77%\n",
      "Batch 81, Loss: 0.832894, Accuracy: 84.86%\n",
      "Batch 82, Loss: 0.879096, Accuracy: 84.87%\n",
      "Batch 83, Loss: 0.919832, Accuracy: 84.85%\n",
      "Batch 84, Loss: 0.937715, Accuracy: 84.78%\n",
      "Batch 85, Loss: 0.855064, Accuracy: 84.83%\n",
      "Batch 86, Loss: 0.910619, Accuracy: 84.81%\n",
      "Batch 87, Loss: 0.846187, Accuracy: 84.88%\n",
      "Batch 88, Loss: 0.893156, Accuracy: 84.89%\n",
      "Batch 89, Loss: 0.875983, Accuracy: 84.90%\n",
      "Batch 90, Loss: 0.860179, Accuracy: 84.95%\n",
      "Batch 91, Loss: 0.841092, Accuracy: 85.01%\n",
      "Batch 92, Loss: 0.949835, Accuracy: 84.97%\n",
      "Batch 93, Loss: 0.918943, Accuracy: 84.95%\n",
      "Batch 94, Loss: 0.895168, Accuracy: 84.92%\n",
      "Batch 95, Loss: 0.944807, Accuracy: 84.88%\n",
      "Batch 96, Loss: 0.872048, Accuracy: 84.91%\n",
      "Batch 97, Loss: 0.833713, Accuracy: 84.99%\n",
      "Batch 98, Loss: 0.912964, Accuracy: 84.96%\n",
      "Batch 99, Loss: 0.864350, Accuracy: 84.99%\n",
      "Batch 100, Loss: 0.923790, Accuracy: 84.97%\n",
      "Batch 101, Loss: 0.951471, Accuracy: 84.93%\n",
      "Batch 102, Loss: 0.897000, Accuracy: 84.91%\n",
      "Batch 103, Loss: 0.862702, Accuracy: 84.95%\n",
      "Batch 104, Loss: 0.992923, Accuracy: 84.84%\n",
      "Batch 105, Loss: 0.899561, Accuracy: 84.84%\n",
      "Batch 106, Loss: 0.828225, Accuracy: 84.89%\n",
      "Batch 107, Loss: 0.865429, Accuracy: 84.92%\n",
      "Batch 108, Loss: 0.973150, Accuracy: 84.84%\n",
      "Batch 109, Loss: 0.953001, Accuracy: 84.76%\n",
      "Batch 110, Loss: 0.898434, Accuracy: 84.79%\n",
      "Batch 111, Loss: 0.893741, Accuracy: 84.80%\n",
      "Batch 112, Loss: 0.876723, Accuracy: 84.82%\n",
      "Batch 113, Loss: 0.902362, Accuracy: 84.82%\n",
      "Batch 114, Loss: 0.874974, Accuracy: 84.85%\n",
      "Batch 115, Loss: 0.892055, Accuracy: 84.86%\n",
      "Batch 116, Loss: 0.897544, Accuracy: 84.87%\n",
      "Batch 117, Loss: 0.919890, Accuracy: 84.84%\n",
      "Batch 118, Loss: 0.929661, Accuracy: 84.81%\n",
      "Batch 119, Loss: 0.904309, Accuracy: 84.81%\n",
      "Batch 120, Loss: 0.865063, Accuracy: 84.83%\n",
      "Batch 121, Loss: 0.882509, Accuracy: 84.84%\n",
      "Batch 122, Loss: 0.940518, Accuracy: 84.78%\n",
      "Batch 123, Loss: 0.890057, Accuracy: 84.78%\n",
      "Batch 124, Loss: 0.858380, Accuracy: 84.82%\n",
      "Batch 125, Loss: 0.960834, Accuracy: 84.76%\n",
      "Batch 126, Loss: 0.877939, Accuracy: 84.77%\n",
      "Batch 127, Loss: 0.889618, Accuracy: 84.77%\n",
      "Batch 128, Loss: 0.910926, Accuracy: 84.77%\n",
      "Batch 129, Loss: 0.944406, Accuracy: 84.74%\n",
      "Batch 130, Loss: 0.890095, Accuracy: 84.75%\n",
      "Batch 131, Loss: 0.908957, Accuracy: 84.73%\n",
      "Batch 132, Loss: 0.846000, Accuracy: 84.78%\n",
      "Batch 133, Loss: 0.902409, Accuracy: 84.77%\n",
      "Batch 134, Loss: 0.819885, Accuracy: 84.84%\n",
      "Batch 135, Loss: 0.932062, Accuracy: 84.83%\n",
      "Batch 136, Loss: 0.866089, Accuracy: 84.85%\n",
      "Batch 137, Loss: 0.904718, Accuracy: 84.85%\n",
      "Batch 138, Loss: 0.846013, Accuracy: 84.88%\n",
      "Batch 139, Loss: 0.933683, Accuracy: 84.86%\n",
      "Batch 140, Loss: 0.878030, Accuracy: 84.87%\n",
      "Batch 141, Loss: 0.927610, Accuracy: 84.84%\n",
      "Batch 142, Loss: 0.897682, Accuracy: 84.84%\n",
      "Batch 143, Loss: 0.877171, Accuracy: 84.86%\n",
      "Batch 144, Loss: 0.816988, Accuracy: 84.91%\n",
      "Batch 145, Loss: 0.906495, Accuracy: 84.90%\n",
      "Batch 146, Loss: 0.961648, Accuracy: 84.82%\n",
      "Batch 147, Loss: 0.902103, Accuracy: 84.82%\n",
      "Batch 148, Loss: 0.885238, Accuracy: 84.84%\n",
      "Batch 149, Loss: 0.854992, Accuracy: 84.87%\n",
      "Batch 150, Loss: 0.886073, Accuracy: 84.88%\n",
      "Batch 151, Loss: 0.831910, Accuracy: 84.92%\n",
      "Batch 152, Loss: 0.928722, Accuracy: 84.91%\n",
      "Batch 153, Loss: 0.923453, Accuracy: 84.89%\n",
      "Batch 154, Loss: 0.853695, Accuracy: 84.91%\n",
      "Batch 155, Loss: 0.878610, Accuracy: 84.93%\n",
      "Batch 156, Loss: 0.885690, Accuracy: 84.94%\n",
      "Batch 157, Loss: 0.882134, Accuracy: 84.95%\n",
      "Batch 158, Loss: 0.873390, Accuracy: 84.96%\n",
      "Batch 159, Loss: 0.913518, Accuracy: 84.94%\n",
      "Batch 160, Loss: 0.885847, Accuracy: 84.94%\n",
      "Batch 161, Loss: 0.903080, Accuracy: 84.94%\n",
      "Batch 162, Loss: 0.899087, Accuracy: 84.92%\n",
      "Batch 163, Loss: 0.839512, Accuracy: 84.96%\n",
      "Batch 164, Loss: 0.899386, Accuracy: 84.96%\n",
      "Batch 165, Loss: 0.889448, Accuracy: 84.96%\n",
      "Batch 166, Loss: 0.950067, Accuracy: 84.91%\n",
      "Batch 167, Loss: 0.915551, Accuracy: 84.90%\n",
      "Batch 168, Loss: 0.917186, Accuracy: 84.90%\n",
      "Batch 169, Loss: 0.923826, Accuracy: 84.87%\n",
      "Batch 170, Loss: 0.933184, Accuracy: 84.85%\n",
      "Batch 171, Loss: 0.880934, Accuracy: 84.86%\n",
      "Batch 172, Loss: 0.871840, Accuracy: 84.87%\n",
      "Batch 173, Loss: 0.861919, Accuracy: 84.89%\n",
      "Batch 174, Loss: 0.875437, Accuracy: 84.90%\n",
      "Batch 175, Loss: 0.837081, Accuracy: 84.92%\n",
      "Batch 176, Loss: 0.895583, Accuracy: 84.92%\n",
      "Batch 177, Loss: 0.890102, Accuracy: 84.90%\n",
      "Batch 178, Loss: 0.913565, Accuracy: 84.89%\n",
      "Batch 179, Loss: 0.825908, Accuracy: 84.94%\n",
      "Batch 180, Loss: 0.905886, Accuracy: 84.94%\n",
      "Batch 181, Loss: 0.894664, Accuracy: 84.94%\n",
      "Batch 182, Loss: 0.899259, Accuracy: 84.93%\n",
      "Batch 183, Loss: 0.903507, Accuracy: 84.93%\n",
      "Batch 184, Loss: 0.935998, Accuracy: 84.89%\n",
      "Batch 185, Loss: 0.981873, Accuracy: 84.84%\n",
      "Batch 186, Loss: 0.915948, Accuracy: 84.84%\n",
      "Batch 187, Loss: 1.004683, Accuracy: 84.79%\n",
      "Batch 188, Loss: 0.861402, Accuracy: 84.81%\n",
      "Batch 189, Loss: 0.956839, Accuracy: 84.77%\n",
      "Batch 190, Loss: 0.967230, Accuracy: 84.75%\n",
      "Batch 191, Loss: 0.862638, Accuracy: 84.76%\n",
      "Batch 192, Loss: 0.935815, Accuracy: 84.74%\n",
      "Batch 193, Loss: 0.882515, Accuracy: 84.75%\n",
      "Batch 194, Loss: 0.895797, Accuracy: 84.75%\n",
      "Batch 195, Loss: 0.864569, Accuracy: 84.78%\n",
      "Batch 196, Loss: 0.906672, Accuracy: 84.77%\n",
      "Batch 197, Loss: 0.899513, Accuracy: 84.76%\n",
      "Batch 198, Loss: 0.885143, Accuracy: 84.76%\n",
      "Batch 199, Loss: 0.853541, Accuracy: 84.78%\n",
      "Batch 200, Loss: 0.869591, Accuracy: 84.80%\n",
      "Batch 201, Loss: 0.807679, Accuracy: 84.85%\n",
      "Batch 202, Loss: 0.896132, Accuracy: 84.85%\n",
      "Batch 203, Loss: 0.870680, Accuracy: 84.85%\n",
      "Batch 204, Loss: 1.022339, Accuracy: 84.78%\n",
      "Batch 205, Loss: 0.899578, Accuracy: 84.78%\n",
      "Batch 206, Loss: 0.922028, Accuracy: 84.77%\n",
      "Batch 207, Loss: 0.961504, Accuracy: 84.74%\n",
      "Batch 208, Loss: 0.872412, Accuracy: 84.76%\n",
      "Batch 209, Loss: 0.919647, Accuracy: 84.74%\n",
      "Batch 210, Loss: 0.875194, Accuracy: 84.75%\n",
      "Batch 211, Loss: 0.957687, Accuracy: 84.72%\n",
      "Batch 212, Loss: 0.907081, Accuracy: 84.71%\n",
      "Batch 213, Loss: 0.884635, Accuracy: 84.71%\n",
      "Training - Epoch 65, Loss: 0.897222, Accuracy: 84.71%\n",
      "Validation Batch 1, Loss: 0.888122, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.878086, Accuracy: 89.06%\n",
      "Validation Batch 3, Loss: 0.956552, Accuracy: 86.46%\n",
      "Validation Batch 4, Loss: 0.910865, Accuracy: 85.16%\n",
      "Validation Batch 5, Loss: 0.889436, Accuracy: 85.00%\n",
      "Validation Batch 6, Loss: 0.848230, Accuracy: 85.94%\n",
      "Validation Batch 7, Loss: 0.864312, Accuracy: 86.38%\n",
      "Validation Batch 8, Loss: 0.967835, Accuracy: 85.35%\n",
      "Validation Batch 9, Loss: 0.941766, Accuracy: 84.90%\n",
      "Validation Batch 10, Loss: 0.909194, Accuracy: 84.84%\n",
      "Validation Batch 11, Loss: 0.907348, Accuracy: 84.80%\n",
      "Validation Batch 12, Loss: 0.910818, Accuracy: 84.51%\n",
      "Validation Batch 13, Loss: 0.887482, Accuracy: 84.74%\n",
      "Validation Batch 14, Loss: 0.924586, Accuracy: 84.60%\n",
      "Validation Batch 15, Loss: 0.870578, Accuracy: 84.79%\n",
      "Validation Batch 16, Loss: 0.883459, Accuracy: 85.16%\n",
      "Validation Batch 17, Loss: 0.919340, Accuracy: 85.11%\n",
      "Validation Batch 18, Loss: 0.870718, Accuracy: 85.24%\n",
      "Validation Batch 19, Loss: 0.930839, Accuracy: 85.03%\n",
      "Validation Batch 20, Loss: 0.987402, Accuracy: 84.38%\n",
      "Validation Batch 21, Loss: 0.932408, Accuracy: 84.38%\n",
      "Validation Batch 22, Loss: 0.894186, Accuracy: 84.38%\n",
      "Validation Batch 23, Loss: 0.934974, Accuracy: 84.17%\n",
      "Validation Batch 24, Loss: 0.909957, Accuracy: 84.11%\n",
      "Validation Batch 25, Loss: 0.868018, Accuracy: 84.25%\n",
      "Validation Batch 26, Loss: 0.898311, Accuracy: 84.31%\n",
      "Validation Batch 27, Loss: 0.906698, Accuracy: 84.38%\n",
      "Validation - Epoch 65, Loss: 0.907093, Accuracy: 84.38%\n",
      "Patience—2\n",
      "Epoch 66\n",
      "Batch 1, Loss: 0.848425, Accuracy: 89.06%\n",
      "Batch 2, Loss: 0.937737, Accuracy: 85.16%\n",
      "Batch 3, Loss: 0.873689, Accuracy: 85.94%\n",
      "Batch 4, Loss: 0.893055, Accuracy: 86.33%\n",
      "Batch 5, Loss: 0.899408, Accuracy: 86.25%\n",
      "Batch 6, Loss: 0.935256, Accuracy: 85.16%\n",
      "Batch 7, Loss: 0.851977, Accuracy: 85.94%\n",
      "Batch 8, Loss: 0.876961, Accuracy: 86.13%\n",
      "Batch 9, Loss: 0.903950, Accuracy: 85.94%\n",
      "Batch 10, Loss: 0.850232, Accuracy: 86.41%\n",
      "Batch 11, Loss: 0.888451, Accuracy: 86.36%\n",
      "Batch 12, Loss: 1.002497, Accuracy: 85.29%\n",
      "Batch 13, Loss: 0.943840, Accuracy: 84.86%\n",
      "Batch 14, Loss: 0.875416, Accuracy: 85.04%\n",
      "Batch 15, Loss: 0.916583, Accuracy: 84.90%\n",
      "Batch 16, Loss: 0.977496, Accuracy: 84.47%\n",
      "Batch 17, Loss: 0.913441, Accuracy: 84.38%\n",
      "Batch 18, Loss: 0.899785, Accuracy: 84.29%\n",
      "Batch 19, Loss: 0.856637, Accuracy: 84.54%\n",
      "Batch 20, Loss: 0.932043, Accuracy: 84.38%\n",
      "Batch 21, Loss: 0.860415, Accuracy: 84.52%\n",
      "Batch 22, Loss: 0.936346, Accuracy: 84.30%\n",
      "Batch 23, Loss: 0.854922, Accuracy: 84.58%\n",
      "Batch 24, Loss: 0.901135, Accuracy: 84.51%\n",
      "Batch 25, Loss: 0.882027, Accuracy: 84.56%\n",
      "Batch 26, Loss: 0.915704, Accuracy: 84.50%\n",
      "Batch 27, Loss: 0.878484, Accuracy: 84.61%\n",
      "Batch 28, Loss: 0.883832, Accuracy: 84.71%\n",
      "Batch 29, Loss: 0.888961, Accuracy: 84.64%\n",
      "Batch 30, Loss: 0.908385, Accuracy: 84.64%\n",
      "Batch 31, Loss: 0.918724, Accuracy: 84.58%\n",
      "Batch 32, Loss: 0.855466, Accuracy: 84.72%\n",
      "Batch 33, Loss: 0.905935, Accuracy: 84.71%\n",
      "Batch 34, Loss: 0.883349, Accuracy: 84.74%\n",
      "Batch 35, Loss: 0.851836, Accuracy: 84.87%\n",
      "Batch 36, Loss: 0.955650, Accuracy: 84.68%\n",
      "Batch 37, Loss: 0.928444, Accuracy: 84.50%\n",
      "Batch 38, Loss: 0.921096, Accuracy: 84.46%\n",
      "Batch 39, Loss: 0.881101, Accuracy: 84.50%\n",
      "Batch 40, Loss: 0.931740, Accuracy: 84.41%\n",
      "Batch 41, Loss: 0.968265, Accuracy: 84.26%\n",
      "Batch 42, Loss: 0.946272, Accuracy: 84.15%\n",
      "Batch 43, Loss: 0.869264, Accuracy: 84.27%\n",
      "Batch 44, Loss: 0.865717, Accuracy: 84.38%\n",
      "Batch 45, Loss: 0.950479, Accuracy: 84.27%\n",
      "Batch 46, Loss: 0.926246, Accuracy: 84.21%\n",
      "Batch 47, Loss: 0.829692, Accuracy: 84.31%\n",
      "Batch 48, Loss: 0.926620, Accuracy: 84.28%\n",
      "Batch 49, Loss: 0.862141, Accuracy: 84.34%\n",
      "Batch 50, Loss: 0.891488, Accuracy: 84.34%\n",
      "Batch 51, Loss: 0.875066, Accuracy: 84.41%\n",
      "Batch 52, Loss: 0.923205, Accuracy: 84.38%\n",
      "Batch 53, Loss: 0.850284, Accuracy: 84.49%\n",
      "Batch 54, Loss: 0.949953, Accuracy: 84.40%\n",
      "Batch 55, Loss: 0.865189, Accuracy: 84.49%\n",
      "Batch 56, Loss: 0.861765, Accuracy: 84.51%\n",
      "Batch 57, Loss: 0.855092, Accuracy: 84.54%\n",
      "Batch 58, Loss: 0.875807, Accuracy: 84.59%\n",
      "Batch 59, Loss: 0.922608, Accuracy: 84.53%\n",
      "Batch 60, Loss: 0.914768, Accuracy: 84.51%\n",
      "Batch 61, Loss: 0.916301, Accuracy: 84.50%\n",
      "Batch 62, Loss: 0.902294, Accuracy: 84.50%\n",
      "Batch 63, Loss: 0.873516, Accuracy: 84.52%\n",
      "Batch 64, Loss: 0.862135, Accuracy: 84.62%\n",
      "Batch 65, Loss: 0.914199, Accuracy: 84.59%\n",
      "Batch 66, Loss: 0.960727, Accuracy: 84.49%\n",
      "Batch 67, Loss: 0.884616, Accuracy: 84.49%\n",
      "Batch 68, Loss: 0.820215, Accuracy: 84.60%\n",
      "Batch 69, Loss: 0.946300, Accuracy: 84.51%\n",
      "Batch 70, Loss: 0.933507, Accuracy: 84.44%\n",
      "Batch 71, Loss: 0.888562, Accuracy: 84.44%\n",
      "Batch 72, Loss: 0.857414, Accuracy: 84.48%\n",
      "Batch 73, Loss: 0.846451, Accuracy: 84.57%\n",
      "Batch 74, Loss: 0.851507, Accuracy: 84.65%\n",
      "Batch 75, Loss: 1.020882, Accuracy: 84.46%\n",
      "Batch 76, Loss: 0.930816, Accuracy: 84.42%\n",
      "Batch 77, Loss: 0.861316, Accuracy: 84.48%\n",
      "Batch 78, Loss: 0.833641, Accuracy: 84.58%\n",
      "Batch 79, Loss: 0.919232, Accuracy: 84.55%\n",
      "Batch 80, Loss: 0.860527, Accuracy: 84.61%\n",
      "Batch 81, Loss: 0.910835, Accuracy: 84.61%\n",
      "Batch 82, Loss: 0.841042, Accuracy: 84.70%\n",
      "Batch 83, Loss: 0.888298, Accuracy: 84.73%\n",
      "Batch 84, Loss: 0.866630, Accuracy: 84.77%\n",
      "Batch 85, Loss: 0.887209, Accuracy: 84.76%\n",
      "Batch 86, Loss: 0.868413, Accuracy: 84.79%\n",
      "Batch 87, Loss: 0.921685, Accuracy: 84.75%\n",
      "Batch 88, Loss: 0.944874, Accuracy: 84.69%\n",
      "Batch 89, Loss: 0.877861, Accuracy: 84.73%\n",
      "Batch 90, Loss: 0.889137, Accuracy: 84.74%\n",
      "Batch 91, Loss: 0.873481, Accuracy: 84.77%\n",
      "Batch 92, Loss: 0.836661, Accuracy: 84.83%\n",
      "Batch 93, Loss: 0.861325, Accuracy: 84.88%\n",
      "Batch 94, Loss: 0.915978, Accuracy: 84.86%\n",
      "Batch 95, Loss: 0.890939, Accuracy: 84.84%\n",
      "Batch 96, Loss: 0.909518, Accuracy: 84.83%\n",
      "Batch 97, Loss: 0.868801, Accuracy: 84.84%\n",
      "Batch 98, Loss: 0.858358, Accuracy: 84.90%\n",
      "Batch 99, Loss: 0.899408, Accuracy: 84.90%\n",
      "Batch 100, Loss: 0.867877, Accuracy: 84.89%\n",
      "Batch 101, Loss: 0.890824, Accuracy: 84.89%\n",
      "Batch 102, Loss: 0.855967, Accuracy: 84.91%\n",
      "Batch 103, Loss: 0.873484, Accuracy: 84.94%\n",
      "Batch 104, Loss: 0.950890, Accuracy: 84.86%\n",
      "Batch 105, Loss: 0.951365, Accuracy: 84.82%\n",
      "Batch 106, Loss: 0.891029, Accuracy: 84.82%\n",
      "Batch 107, Loss: 0.880253, Accuracy: 84.83%\n",
      "Batch 108, Loss: 0.885684, Accuracy: 84.82%\n",
      "Batch 109, Loss: 0.889585, Accuracy: 84.82%\n",
      "Batch 110, Loss: 0.948723, Accuracy: 84.76%\n",
      "Batch 111, Loss: 0.919194, Accuracy: 84.71%\n",
      "Batch 112, Loss: 0.917655, Accuracy: 84.71%\n",
      "Batch 113, Loss: 0.887368, Accuracy: 84.69%\n",
      "Batch 114, Loss: 0.839675, Accuracy: 84.76%\n",
      "Batch 115, Loss: 0.822496, Accuracy: 84.82%\n",
      "Batch 116, Loss: 0.842598, Accuracy: 84.87%\n",
      "Batch 117, Loss: 0.844365, Accuracy: 84.91%\n",
      "Batch 118, Loss: 0.916844, Accuracy: 84.90%\n",
      "Batch 119, Loss: 0.922617, Accuracy: 84.87%\n",
      "Batch 120, Loss: 0.882283, Accuracy: 84.88%\n",
      "Batch 121, Loss: 0.897653, Accuracy: 84.89%\n",
      "Batch 122, Loss: 0.834155, Accuracy: 84.95%\n",
      "Batch 123, Loss: 0.889100, Accuracy: 84.93%\n",
      "Batch 124, Loss: 0.991470, Accuracy: 84.85%\n",
      "Batch 125, Loss: 0.932953, Accuracy: 84.83%\n",
      "Batch 126, Loss: 0.877982, Accuracy: 84.85%\n",
      "Batch 127, Loss: 0.844117, Accuracy: 84.89%\n",
      "Batch 128, Loss: 0.879385, Accuracy: 84.90%\n",
      "Batch 129, Loss: 0.901470, Accuracy: 84.90%\n",
      "Batch 130, Loss: 0.899281, Accuracy: 84.90%\n",
      "Batch 131, Loss: 0.888071, Accuracy: 84.90%\n",
      "Batch 132, Loss: 0.876555, Accuracy: 84.92%\n",
      "Batch 133, Loss: 0.877953, Accuracy: 84.95%\n",
      "Batch 134, Loss: 0.832582, Accuracy: 85.00%\n",
      "Batch 135, Loss: 0.895629, Accuracy: 85.00%\n",
      "Batch 136, Loss: 0.912599, Accuracy: 84.98%\n",
      "Batch 137, Loss: 0.858447, Accuracy: 85.00%\n",
      "Batch 138, Loss: 0.869866, Accuracy: 85.02%\n",
      "Batch 139, Loss: 0.861130, Accuracy: 85.05%\n",
      "Batch 140, Loss: 0.847608, Accuracy: 85.09%\n",
      "Batch 141, Loss: 0.882576, Accuracy: 85.10%\n",
      "Batch 142, Loss: 0.928883, Accuracy: 85.07%\n",
      "Batch 143, Loss: 0.886164, Accuracy: 85.07%\n",
      "Batch 144, Loss: 0.847035, Accuracy: 85.12%\n",
      "Batch 145, Loss: 0.891314, Accuracy: 85.11%\n",
      "Batch 146, Loss: 0.863539, Accuracy: 85.12%\n",
      "Batch 147, Loss: 0.918222, Accuracy: 85.11%\n",
      "Batch 148, Loss: 0.911648, Accuracy: 85.09%\n",
      "Batch 149, Loss: 0.894537, Accuracy: 85.11%\n",
      "Batch 150, Loss: 0.869396, Accuracy: 85.14%\n",
      "Batch 151, Loss: 0.969405, Accuracy: 85.08%\n",
      "Batch 152, Loss: 0.907869, Accuracy: 85.06%\n",
      "Batch 153, Loss: 0.876252, Accuracy: 85.07%\n",
      "Batch 154, Loss: 0.873249, Accuracy: 85.09%\n",
      "Batch 155, Loss: 0.947115, Accuracy: 85.05%\n",
      "Batch 156, Loss: 0.890328, Accuracy: 85.05%\n",
      "Batch 157, Loss: 0.857497, Accuracy: 85.07%\n",
      "Batch 158, Loss: 0.983627, Accuracy: 85.01%\n",
      "Batch 159, Loss: 0.946547, Accuracy: 84.96%\n",
      "Batch 160, Loss: 0.903802, Accuracy: 84.96%\n",
      "Batch 161, Loss: 0.965097, Accuracy: 84.91%\n",
      "Batch 162, Loss: 0.897058, Accuracy: 84.92%\n",
      "Batch 163, Loss: 0.924901, Accuracy: 84.89%\n",
      "Batch 164, Loss: 0.905479, Accuracy: 84.89%\n",
      "Batch 165, Loss: 0.939716, Accuracy: 84.87%\n",
      "Batch 166, Loss: 0.970691, Accuracy: 84.83%\n",
      "Batch 167, Loss: 0.931889, Accuracy: 84.81%\n",
      "Batch 168, Loss: 0.876462, Accuracy: 84.82%\n",
      "Batch 169, Loss: 0.867264, Accuracy: 84.84%\n",
      "Batch 170, Loss: 0.940885, Accuracy: 84.82%\n",
      "Batch 171, Loss: 0.904593, Accuracy: 84.81%\n",
      "Batch 172, Loss: 0.907361, Accuracy: 84.79%\n",
      "Batch 173, Loss: 0.949875, Accuracy: 84.76%\n",
      "Batch 174, Loss: 0.880914, Accuracy: 84.78%\n",
      "Batch 175, Loss: 0.889845, Accuracy: 84.79%\n",
      "Batch 176, Loss: 0.869943, Accuracy: 84.81%\n",
      "Batch 177, Loss: 0.913654, Accuracy: 84.79%\n",
      "Batch 178, Loss: 0.866065, Accuracy: 84.82%\n",
      "Batch 179, Loss: 0.916817, Accuracy: 84.81%\n",
      "Batch 180, Loss: 0.862963, Accuracy: 84.84%\n",
      "Batch 181, Loss: 0.877256, Accuracy: 84.84%\n",
      "Batch 182, Loss: 0.889031, Accuracy: 84.84%\n",
      "Batch 183, Loss: 0.842438, Accuracy: 84.86%\n",
      "Batch 184, Loss: 0.876709, Accuracy: 84.88%\n",
      "Batch 185, Loss: 0.865742, Accuracy: 84.88%\n",
      "Batch 186, Loss: 0.915660, Accuracy: 84.86%\n",
      "Batch 187, Loss: 0.937522, Accuracy: 84.83%\n",
      "Batch 188, Loss: 0.898395, Accuracy: 84.83%\n",
      "Batch 189, Loss: 0.894997, Accuracy: 84.83%\n",
      "Batch 190, Loss: 0.933107, Accuracy: 84.80%\n",
      "Batch 191, Loss: 0.877275, Accuracy: 84.82%\n",
      "Batch 192, Loss: 0.910489, Accuracy: 84.79%\n",
      "Batch 193, Loss: 0.905437, Accuracy: 84.79%\n",
      "Batch 194, Loss: 0.848711, Accuracy: 84.80%\n",
      "Batch 195, Loss: 0.941445, Accuracy: 84.78%\n",
      "Batch 196, Loss: 0.895955, Accuracy: 84.78%\n",
      "Batch 197, Loss: 0.850373, Accuracy: 84.80%\n",
      "Batch 198, Loss: 0.884741, Accuracy: 84.81%\n",
      "Batch 199, Loss: 0.903360, Accuracy: 84.81%\n",
      "Batch 200, Loss: 0.952614, Accuracy: 84.78%\n",
      "Batch 201, Loss: 0.961120, Accuracy: 84.75%\n",
      "Batch 202, Loss: 0.868229, Accuracy: 84.75%\n",
      "Batch 203, Loss: 0.842231, Accuracy: 84.78%\n",
      "Batch 204, Loss: 0.849092, Accuracy: 84.81%\n",
      "Batch 205, Loss: 0.894367, Accuracy: 84.82%\n",
      "Batch 206, Loss: 0.857431, Accuracy: 84.85%\n",
      "Batch 207, Loss: 0.838803, Accuracy: 84.88%\n",
      "Batch 208, Loss: 0.865224, Accuracy: 84.89%\n",
      "Batch 209, Loss: 0.891981, Accuracy: 84.88%\n",
      "Batch 210, Loss: 0.898817, Accuracy: 84.88%\n",
      "Batch 211, Loss: 0.851789, Accuracy: 84.90%\n",
      "Batch 212, Loss: 0.882917, Accuracy: 84.91%\n",
      "Batch 213, Loss: 0.871708, Accuracy: 84.92%\n",
      "Training - Epoch 66, Loss: 0.894362, Accuracy: 84.92%\n",
      "Validation Batch 1, Loss: 0.876779, Accuracy: 89.06%\n",
      "Validation Batch 2, Loss: 0.858199, Accuracy: 90.62%\n",
      "Validation Batch 3, Loss: 0.943189, Accuracy: 88.02%\n",
      "Validation Batch 4, Loss: 0.890460, Accuracy: 87.50%\n",
      "Validation Batch 5, Loss: 0.877355, Accuracy: 87.50%\n",
      "Validation Batch 6, Loss: 0.836179, Accuracy: 88.28%\n",
      "Validation Batch 7, Loss: 0.854391, Accuracy: 88.62%\n",
      "Validation Batch 8, Loss: 0.963462, Accuracy: 87.11%\n",
      "Validation Batch 9, Loss: 0.931028, Accuracy: 86.63%\n",
      "Validation Batch 10, Loss: 0.904014, Accuracy: 86.41%\n",
      "Validation Batch 11, Loss: 0.898890, Accuracy: 86.22%\n",
      "Validation Batch 12, Loss: 0.890459, Accuracy: 86.07%\n",
      "Validation Batch 13, Loss: 0.884456, Accuracy: 86.18%\n",
      "Validation Batch 14, Loss: 0.914848, Accuracy: 85.94%\n",
      "Validation Batch 15, Loss: 0.865899, Accuracy: 86.04%\n",
      "Validation Batch 16, Loss: 0.877629, Accuracy: 86.33%\n",
      "Validation Batch 17, Loss: 0.921279, Accuracy: 86.31%\n",
      "Validation Batch 18, Loss: 0.866512, Accuracy: 86.37%\n",
      "Validation Batch 19, Loss: 0.928192, Accuracy: 86.10%\n",
      "Validation Batch 20, Loss: 0.966034, Accuracy: 85.62%\n",
      "Validation Batch 21, Loss: 0.918444, Accuracy: 85.57%\n",
      "Validation Batch 22, Loss: 0.892247, Accuracy: 85.58%\n",
      "Validation Batch 23, Loss: 0.929668, Accuracy: 85.39%\n",
      "Validation Batch 24, Loss: 0.910783, Accuracy: 85.29%\n",
      "Validation Batch 25, Loss: 0.864166, Accuracy: 85.38%\n",
      "Validation Batch 26, Loss: 0.893344, Accuracy: 85.40%\n",
      "Validation Batch 27, Loss: 0.878030, Accuracy: 85.44%\n",
      "Validation - Epoch 66, Loss: 0.897627, Accuracy: 85.44%\n",
      "Patience—3\n",
      "Epoch 67\n",
      "Batch 1, Loss: 0.829572, Accuracy: 92.19%\n",
      "Batch 2, Loss: 0.870543, Accuracy: 89.84%\n",
      "Batch 3, Loss: 0.913887, Accuracy: 87.50%\n",
      "Batch 4, Loss: 0.910757, Accuracy: 86.33%\n",
      "Batch 5, Loss: 0.875375, Accuracy: 86.88%\n",
      "Batch 6, Loss: 0.886868, Accuracy: 86.46%\n",
      "Batch 7, Loss: 0.864594, Accuracy: 86.61%\n",
      "Batch 8, Loss: 0.916044, Accuracy: 86.13%\n",
      "Batch 9, Loss: 0.950597, Accuracy: 85.59%\n",
      "Batch 10, Loss: 0.876491, Accuracy: 85.78%\n",
      "Batch 11, Loss: 0.892722, Accuracy: 85.80%\n",
      "Batch 12, Loss: 0.894138, Accuracy: 85.55%\n",
      "Batch 13, Loss: 0.896470, Accuracy: 85.46%\n",
      "Batch 14, Loss: 0.795583, Accuracy: 86.16%\n",
      "Batch 15, Loss: 0.961600, Accuracy: 85.62%\n",
      "Batch 16, Loss: 0.869653, Accuracy: 85.74%\n",
      "Batch 17, Loss: 0.859172, Accuracy: 85.94%\n",
      "Batch 18, Loss: 0.866408, Accuracy: 86.11%\n",
      "Batch 19, Loss: 0.859900, Accuracy: 86.27%\n",
      "Batch 20, Loss: 0.909489, Accuracy: 86.09%\n",
      "Batch 21, Loss: 0.862884, Accuracy: 86.24%\n",
      "Batch 22, Loss: 0.879294, Accuracy: 86.22%\n",
      "Batch 23, Loss: 0.950260, Accuracy: 86.01%\n",
      "Batch 24, Loss: 0.905266, Accuracy: 85.94%\n",
      "Batch 25, Loss: 0.939922, Accuracy: 85.69%\n",
      "Batch 26, Loss: 0.852987, Accuracy: 85.82%\n",
      "Batch 27, Loss: 0.822316, Accuracy: 86.11%\n",
      "Batch 28, Loss: 0.947119, Accuracy: 85.88%\n",
      "Batch 29, Loss: 0.909591, Accuracy: 85.83%\n",
      "Batch 30, Loss: 0.859230, Accuracy: 85.94%\n",
      "Batch 31, Loss: 0.876537, Accuracy: 85.99%\n",
      "Batch 32, Loss: 0.842198, Accuracy: 86.13%\n",
      "Batch 33, Loss: 0.924234, Accuracy: 85.98%\n",
      "Batch 34, Loss: 0.924227, Accuracy: 85.89%\n",
      "Batch 35, Loss: 0.889969, Accuracy: 85.89%\n",
      "Batch 36, Loss: 0.894435, Accuracy: 85.89%\n",
      "Batch 37, Loss: 0.914038, Accuracy: 85.81%\n",
      "Batch 38, Loss: 0.882765, Accuracy: 85.81%\n",
      "Batch 39, Loss: 0.868875, Accuracy: 85.82%\n",
      "Batch 40, Loss: 0.956729, Accuracy: 85.66%\n",
      "Batch 41, Loss: 0.849673, Accuracy: 85.79%\n",
      "Batch 42, Loss: 0.857444, Accuracy: 85.90%\n",
      "Batch 43, Loss: 0.841815, Accuracy: 86.01%\n",
      "Batch 44, Loss: 0.873519, Accuracy: 86.04%\n",
      "Batch 45, Loss: 0.922613, Accuracy: 85.97%\n",
      "Batch 46, Loss: 0.863315, Accuracy: 86.04%\n",
      "Batch 47, Loss: 0.854973, Accuracy: 86.10%\n",
      "Batch 48, Loss: 0.960328, Accuracy: 85.87%\n",
      "Batch 49, Loss: 0.941670, Accuracy: 85.75%\n",
      "Batch 50, Loss: 0.937680, Accuracy: 85.62%\n",
      "Batch 51, Loss: 0.827814, Accuracy: 85.72%\n",
      "Batch 52, Loss: 0.858648, Accuracy: 85.79%\n",
      "Batch 53, Loss: 0.857286, Accuracy: 85.85%\n",
      "Batch 54, Loss: 0.865299, Accuracy: 85.85%\n",
      "Batch 55, Loss: 0.877767, Accuracy: 85.85%\n",
      "Batch 56, Loss: 0.951384, Accuracy: 85.74%\n",
      "Batch 57, Loss: 0.886475, Accuracy: 85.75%\n",
      "Batch 58, Loss: 0.914882, Accuracy: 85.67%\n",
      "Batch 59, Loss: 0.858157, Accuracy: 85.73%\n",
      "Batch 60, Loss: 0.896144, Accuracy: 85.70%\n",
      "Batch 61, Loss: 0.882052, Accuracy: 85.71%\n",
      "Batch 62, Loss: 0.870876, Accuracy: 85.74%\n",
      "Batch 63, Loss: 0.907390, Accuracy: 85.74%\n",
      "Batch 64, Loss: 0.888738, Accuracy: 85.72%\n",
      "Batch 65, Loss: 0.843289, Accuracy: 85.79%\n",
      "Batch 66, Loss: 0.857942, Accuracy: 85.84%\n",
      "Batch 67, Loss: 0.861200, Accuracy: 85.89%\n",
      "Batch 68, Loss: 0.876280, Accuracy: 85.91%\n",
      "Batch 69, Loss: 0.881241, Accuracy: 85.96%\n",
      "Batch 70, Loss: 0.927738, Accuracy: 85.85%\n",
      "Batch 71, Loss: 0.868058, Accuracy: 85.87%\n",
      "Batch 72, Loss: 0.889835, Accuracy: 85.89%\n",
      "Batch 73, Loss: 0.982966, Accuracy: 85.74%\n",
      "Batch 74, Loss: 0.819500, Accuracy: 85.85%\n",
      "Batch 75, Loss: 0.854060, Accuracy: 85.90%\n",
      "Batch 76, Loss: 0.939581, Accuracy: 85.79%\n",
      "Batch 77, Loss: 0.912369, Accuracy: 85.75%\n",
      "Batch 78, Loss: 0.855488, Accuracy: 85.80%\n",
      "Batch 79, Loss: 0.800263, Accuracy: 85.92%\n",
      "Batch 80, Loss: 0.929061, Accuracy: 85.84%\n",
      "Batch 81, Loss: 0.876798, Accuracy: 85.82%\n",
      "Batch 82, Loss: 0.874355, Accuracy: 85.84%\n",
      "Batch 83, Loss: 0.898343, Accuracy: 85.79%\n",
      "Batch 84, Loss: 0.917570, Accuracy: 85.73%\n",
      "Batch 85, Loss: 0.861420, Accuracy: 85.75%\n",
      "Batch 86, Loss: 0.890903, Accuracy: 85.76%\n",
      "Batch 87, Loss: 0.855120, Accuracy: 85.81%\n",
      "Batch 88, Loss: 0.967529, Accuracy: 85.72%\n",
      "Batch 89, Loss: 0.932592, Accuracy: 85.69%\n",
      "Batch 90, Loss: 0.914527, Accuracy: 85.66%\n",
      "Batch 91, Loss: 0.844335, Accuracy: 85.73%\n",
      "Batch 92, Loss: 0.923154, Accuracy: 85.68%\n",
      "Batch 93, Loss: 0.870910, Accuracy: 85.72%\n",
      "Batch 94, Loss: 0.920599, Accuracy: 85.65%\n",
      "Batch 95, Loss: 0.941942, Accuracy: 85.58%\n",
      "Batch 96, Loss: 0.852863, Accuracy: 85.61%\n",
      "Batch 97, Loss: 0.920311, Accuracy: 85.58%\n",
      "Batch 98, Loss: 0.900213, Accuracy: 85.57%\n",
      "Batch 99, Loss: 0.893360, Accuracy: 85.57%\n",
      "Batch 100, Loss: 0.906112, Accuracy: 85.56%\n",
      "Batch 101, Loss: 0.850310, Accuracy: 85.63%\n",
      "Batch 102, Loss: 0.863494, Accuracy: 85.66%\n",
      "Batch 103, Loss: 0.908336, Accuracy: 85.65%\n",
      "Batch 104, Loss: 0.966379, Accuracy: 85.56%\n",
      "Batch 105, Loss: 0.878722, Accuracy: 85.58%\n",
      "Batch 106, Loss: 0.927215, Accuracy: 85.54%\n",
      "Batch 107, Loss: 0.954495, Accuracy: 85.47%\n",
      "Batch 108, Loss: 0.830662, Accuracy: 85.55%\n",
      "Batch 109, Loss: 0.926059, Accuracy: 85.52%\n",
      "Batch 110, Loss: 0.833575, Accuracy: 85.58%\n",
      "Batch 111, Loss: 0.868629, Accuracy: 85.61%\n",
      "Batch 112, Loss: 0.848515, Accuracy: 85.66%\n",
      "Batch 113, Loss: 0.998155, Accuracy: 85.55%\n",
      "Batch 114, Loss: 0.927917, Accuracy: 85.50%\n",
      "Batch 115, Loss: 0.864463, Accuracy: 85.52%\n",
      "Batch 116, Loss: 0.877227, Accuracy: 85.53%\n",
      "Batch 117, Loss: 0.870082, Accuracy: 85.55%\n",
      "Batch 118, Loss: 0.860314, Accuracy: 85.57%\n",
      "Batch 119, Loss: 0.899226, Accuracy: 85.56%\n",
      "Batch 120, Loss: 0.852724, Accuracy: 85.60%\n",
      "Batch 121, Loss: 0.891694, Accuracy: 85.60%\n",
      "Batch 122, Loss: 0.873195, Accuracy: 85.63%\n",
      "Batch 123, Loss: 0.895863, Accuracy: 85.62%\n",
      "Batch 124, Loss: 0.945444, Accuracy: 85.58%\n",
      "Batch 125, Loss: 0.948904, Accuracy: 85.54%\n",
      "Batch 126, Loss: 0.870924, Accuracy: 85.55%\n",
      "Batch 127, Loss: 0.892930, Accuracy: 85.54%\n",
      "Batch 128, Loss: 0.882467, Accuracy: 85.55%\n",
      "Batch 129, Loss: 0.935283, Accuracy: 85.50%\n",
      "Batch 130, Loss: 0.938653, Accuracy: 85.47%\n",
      "Batch 131, Loss: 0.925412, Accuracy: 85.44%\n",
      "Batch 132, Loss: 0.856935, Accuracy: 85.46%\n",
      "Batch 133, Loss: 0.882743, Accuracy: 85.47%\n",
      "Batch 134, Loss: 0.957056, Accuracy: 85.41%\n",
      "Batch 135, Loss: 0.892428, Accuracy: 85.41%\n",
      "Batch 136, Loss: 0.900250, Accuracy: 85.40%\n",
      "Batch 137, Loss: 0.900183, Accuracy: 85.40%\n",
      "Batch 138, Loss: 0.855449, Accuracy: 85.43%\n",
      "Batch 139, Loss: 0.886297, Accuracy: 85.44%\n",
      "Batch 140, Loss: 0.867392, Accuracy: 85.46%\n",
      "Batch 141, Loss: 0.966293, Accuracy: 85.39%\n",
      "Batch 142, Loss: 0.900078, Accuracy: 85.39%\n",
      "Batch 143, Loss: 0.808411, Accuracy: 85.46%\n",
      "Batch 144, Loss: 0.899510, Accuracy: 85.45%\n",
      "Batch 145, Loss: 0.870777, Accuracy: 85.45%\n",
      "Batch 146, Loss: 0.895086, Accuracy: 85.46%\n",
      "Batch 147, Loss: 0.906347, Accuracy: 85.45%\n",
      "Batch 148, Loss: 0.969839, Accuracy: 85.39%\n",
      "Batch 149, Loss: 0.872047, Accuracy: 85.40%\n",
      "Batch 150, Loss: 0.884272, Accuracy: 85.42%\n",
      "Batch 151, Loss: 1.038037, Accuracy: 85.33%\n",
      "Batch 152, Loss: 0.952363, Accuracy: 85.29%\n",
      "Batch 153, Loss: 0.879801, Accuracy: 85.29%\n",
      "Batch 154, Loss: 0.910860, Accuracy: 85.29%\n",
      "Batch 155, Loss: 0.924406, Accuracy: 85.26%\n",
      "Batch 156, Loss: 0.947876, Accuracy: 85.22%\n",
      "Batch 157, Loss: 0.983280, Accuracy: 85.14%\n",
      "Batch 158, Loss: 0.853607, Accuracy: 85.18%\n",
      "Batch 159, Loss: 0.947471, Accuracy: 85.14%\n",
      "Batch 160, Loss: 0.848256, Accuracy: 85.17%\n",
      "Batch 161, Loss: 0.924205, Accuracy: 85.15%\n",
      "Batch 162, Loss: 0.909436, Accuracy: 85.14%\n",
      "Batch 163, Loss: 0.832083, Accuracy: 85.19%\n",
      "Batch 164, Loss: 0.938263, Accuracy: 85.17%\n",
      "Batch 165, Loss: 0.935881, Accuracy: 85.14%\n",
      "Batch 166, Loss: 0.957071, Accuracy: 85.08%\n",
      "Batch 167, Loss: 0.911936, Accuracy: 85.07%\n",
      "Batch 168, Loss: 0.880083, Accuracy: 85.07%\n",
      "Batch 169, Loss: 0.895672, Accuracy: 85.07%\n",
      "Batch 170, Loss: 0.853267, Accuracy: 85.09%\n",
      "Batch 171, Loss: 0.852839, Accuracy: 85.12%\n",
      "Batch 172, Loss: 0.876078, Accuracy: 85.12%\n",
      "Batch 173, Loss: 0.942014, Accuracy: 85.10%\n",
      "Batch 174, Loss: 0.898833, Accuracy: 85.10%\n",
      "Batch 175, Loss: 0.913298, Accuracy: 85.10%\n",
      "Batch 176, Loss: 0.908479, Accuracy: 85.09%\n",
      "Batch 177, Loss: 0.890583, Accuracy: 85.11%\n",
      "Batch 178, Loss: 0.830013, Accuracy: 85.15%\n",
      "Batch 179, Loss: 0.846185, Accuracy: 85.18%\n",
      "Batch 180, Loss: 0.907308, Accuracy: 85.17%\n",
      "Batch 181, Loss: 0.896949, Accuracy: 85.17%\n",
      "Batch 182, Loss: 0.848147, Accuracy: 85.20%\n",
      "Batch 183, Loss: 0.879710, Accuracy: 85.21%\n",
      "Batch 184, Loss: 0.834519, Accuracy: 85.24%\n",
      "Batch 185, Loss: 0.923320, Accuracy: 85.22%\n",
      "Batch 186, Loss: 0.889597, Accuracy: 85.24%\n",
      "Batch 187, Loss: 0.894399, Accuracy: 85.25%\n",
      "Batch 188, Loss: 0.881355, Accuracy: 85.26%\n",
      "Batch 189, Loss: 0.850791, Accuracy: 85.28%\n",
      "Batch 190, Loss: 0.888706, Accuracy: 85.29%\n",
      "Batch 191, Loss: 0.867485, Accuracy: 85.31%\n",
      "Batch 192, Loss: 0.869161, Accuracy: 85.32%\n",
      "Batch 193, Loss: 0.858610, Accuracy: 85.33%\n",
      "Batch 194, Loss: 0.900876, Accuracy: 85.33%\n",
      "Batch 195, Loss: 0.845448, Accuracy: 85.34%\n",
      "Batch 196, Loss: 0.893015, Accuracy: 85.34%\n",
      "Batch 197, Loss: 0.917390, Accuracy: 85.33%\n",
      "Batch 198, Loss: 0.864220, Accuracy: 85.34%\n",
      "Batch 199, Loss: 0.903890, Accuracy: 85.34%\n",
      "Batch 200, Loss: 0.905293, Accuracy: 85.33%\n",
      "Batch 201, Loss: 0.868221, Accuracy: 85.34%\n",
      "Batch 202, Loss: 0.930837, Accuracy: 85.31%\n",
      "Batch 203, Loss: 0.858236, Accuracy: 85.32%\n",
      "Batch 204, Loss: 0.868957, Accuracy: 85.33%\n",
      "Batch 205, Loss: 0.887040, Accuracy: 85.33%\n",
      "Batch 206, Loss: 0.901906, Accuracy: 85.31%\n",
      "Batch 207, Loss: 0.889030, Accuracy: 85.31%\n",
      "Batch 208, Loss: 0.812500, Accuracy: 85.36%\n",
      "Batch 209, Loss: 0.881264, Accuracy: 85.37%\n",
      "Batch 210, Loss: 0.875138, Accuracy: 85.38%\n",
      "Batch 211, Loss: 0.845063, Accuracy: 85.40%\n",
      "Batch 212, Loss: 0.891095, Accuracy: 85.40%\n",
      "Batch 213, Loss: 0.887996, Accuracy: 85.40%\n",
      "Training - Epoch 67, Loss: 0.891497, Accuracy: 85.40%\n",
      "Validation Batch 1, Loss: 0.875789, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.859634, Accuracy: 89.06%\n",
      "Validation Batch 3, Loss: 0.947305, Accuracy: 86.46%\n",
      "Validation Batch 4, Loss: 0.889114, Accuracy: 86.33%\n",
      "Validation Batch 5, Loss: 0.875950, Accuracy: 86.56%\n",
      "Validation Batch 6, Loss: 0.839566, Accuracy: 87.50%\n",
      "Validation Batch 7, Loss: 0.858101, Accuracy: 87.95%\n",
      "Validation Batch 8, Loss: 0.964392, Accuracy: 86.33%\n",
      "Validation Batch 9, Loss: 0.923462, Accuracy: 86.11%\n",
      "Validation Batch 10, Loss: 0.909411, Accuracy: 85.78%\n",
      "Validation Batch 11, Loss: 0.892517, Accuracy: 85.65%\n",
      "Validation Batch 12, Loss: 0.888610, Accuracy: 85.55%\n",
      "Validation Batch 13, Loss: 0.894890, Accuracy: 85.58%\n",
      "Validation Batch 14, Loss: 0.920521, Accuracy: 85.38%\n",
      "Validation Batch 15, Loss: 0.867907, Accuracy: 85.52%\n",
      "Validation Batch 16, Loss: 0.890753, Accuracy: 85.55%\n",
      "Validation Batch 17, Loss: 0.932801, Accuracy: 85.29%\n",
      "Validation Batch 18, Loss: 0.868097, Accuracy: 85.42%\n",
      "Validation Batch 19, Loss: 0.933939, Accuracy: 85.20%\n",
      "Validation Batch 20, Loss: 0.967935, Accuracy: 84.84%\n",
      "Validation Batch 21, Loss: 0.920648, Accuracy: 84.82%\n",
      "Validation Batch 22, Loss: 0.898270, Accuracy: 84.87%\n",
      "Validation Batch 23, Loss: 0.939807, Accuracy: 84.65%\n",
      "Validation Batch 24, Loss: 0.915967, Accuracy: 84.57%\n",
      "Validation Batch 25, Loss: 0.868849, Accuracy: 84.75%\n",
      "Validation Batch 26, Loss: 0.896834, Accuracy: 84.80%\n",
      "Validation Batch 27, Loss: 0.888282, Accuracy: 84.85%\n",
      "Validation - Epoch 67, Loss: 0.901087, Accuracy: 84.85%\n",
      "Patience—4\n",
      "Epoch 68\n",
      "Batch 1, Loss: 0.855232, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.927770, Accuracy: 85.16%\n",
      "Batch 3, Loss: 0.906720, Accuracy: 84.38%\n",
      "Batch 4, Loss: 0.865311, Accuracy: 85.55%\n",
      "Batch 5, Loss: 0.882135, Accuracy: 85.62%\n",
      "Batch 6, Loss: 0.878371, Accuracy: 85.94%\n",
      "Batch 7, Loss: 0.955401, Accuracy: 84.82%\n",
      "Batch 8, Loss: 0.931210, Accuracy: 84.57%\n",
      "Batch 9, Loss: 0.813814, Accuracy: 85.59%\n",
      "Batch 10, Loss: 0.884911, Accuracy: 85.62%\n",
      "Batch 11, Loss: 0.918559, Accuracy: 85.37%\n",
      "Batch 12, Loss: 0.930524, Accuracy: 85.03%\n",
      "Batch 13, Loss: 0.957478, Accuracy: 84.50%\n",
      "Batch 14, Loss: 0.845791, Accuracy: 84.93%\n",
      "Batch 15, Loss: 0.933472, Accuracy: 84.69%\n",
      "Batch 16, Loss: 0.925863, Accuracy: 84.47%\n",
      "Batch 17, Loss: 0.863205, Accuracy: 84.65%\n",
      "Batch 18, Loss: 0.857994, Accuracy: 84.90%\n",
      "Batch 19, Loss: 0.883880, Accuracy: 84.87%\n",
      "Batch 20, Loss: 0.873522, Accuracy: 84.92%\n",
      "Batch 21, Loss: 0.865824, Accuracy: 85.12%\n",
      "Batch 22, Loss: 0.921805, Accuracy: 84.94%\n",
      "Batch 23, Loss: 0.811416, Accuracy: 85.39%\n",
      "Batch 24, Loss: 0.863840, Accuracy: 85.48%\n",
      "Batch 25, Loss: 0.899831, Accuracy: 85.44%\n",
      "Batch 26, Loss: 0.940083, Accuracy: 85.34%\n",
      "Batch 27, Loss: 0.906538, Accuracy: 85.36%\n",
      "Batch 28, Loss: 0.912837, Accuracy: 85.27%\n",
      "Batch 29, Loss: 0.918838, Accuracy: 85.24%\n",
      "Batch 30, Loss: 0.897848, Accuracy: 85.26%\n",
      "Batch 31, Loss: 0.916655, Accuracy: 85.13%\n",
      "Batch 32, Loss: 0.897509, Accuracy: 85.16%\n",
      "Batch 33, Loss: 0.908469, Accuracy: 85.09%\n",
      "Batch 34, Loss: 0.900299, Accuracy: 85.11%\n",
      "Batch 35, Loss: 0.812319, Accuracy: 85.36%\n",
      "Batch 36, Loss: 0.890276, Accuracy: 85.33%\n",
      "Batch 37, Loss: 0.866852, Accuracy: 85.43%\n",
      "Batch 38, Loss: 0.931061, Accuracy: 85.32%\n",
      "Batch 39, Loss: 0.911441, Accuracy: 85.30%\n",
      "Batch 40, Loss: 0.911215, Accuracy: 85.20%\n",
      "Batch 41, Loss: 0.941653, Accuracy: 85.02%\n",
      "Batch 42, Loss: 0.916355, Accuracy: 84.97%\n",
      "Batch 43, Loss: 0.861743, Accuracy: 85.07%\n",
      "Batch 44, Loss: 0.881800, Accuracy: 85.09%\n",
      "Batch 45, Loss: 0.825901, Accuracy: 85.24%\n",
      "Batch 46, Loss: 0.905543, Accuracy: 85.22%\n",
      "Batch 47, Loss: 0.872483, Accuracy: 85.24%\n",
      "Batch 48, Loss: 0.913786, Accuracy: 85.19%\n",
      "Batch 49, Loss: 0.966870, Accuracy: 85.04%\n",
      "Batch 50, Loss: 0.818440, Accuracy: 85.19%\n",
      "Batch 51, Loss: 0.909854, Accuracy: 85.11%\n",
      "Batch 52, Loss: 0.855223, Accuracy: 85.19%\n",
      "Batch 53, Loss: 0.886299, Accuracy: 85.14%\n",
      "Batch 54, Loss: 0.879343, Accuracy: 85.19%\n",
      "Batch 55, Loss: 0.833782, Accuracy: 85.31%\n",
      "Batch 56, Loss: 0.882777, Accuracy: 85.35%\n",
      "Batch 57, Loss: 0.832591, Accuracy: 85.47%\n",
      "Batch 58, Loss: 0.934900, Accuracy: 85.40%\n",
      "Batch 59, Loss: 0.898199, Accuracy: 85.38%\n",
      "Batch 60, Loss: 0.858939, Accuracy: 85.42%\n",
      "Batch 61, Loss: 0.917268, Accuracy: 85.37%\n",
      "Batch 62, Loss: 0.888904, Accuracy: 85.38%\n",
      "Batch 63, Loss: 0.861651, Accuracy: 85.44%\n",
      "Batch 64, Loss: 0.915734, Accuracy: 85.40%\n",
      "Batch 65, Loss: 0.857695, Accuracy: 85.43%\n",
      "Batch 66, Loss: 0.868529, Accuracy: 85.49%\n",
      "Batch 67, Loss: 0.853274, Accuracy: 85.49%\n",
      "Batch 68, Loss: 0.886235, Accuracy: 85.48%\n",
      "Batch 69, Loss: 0.926279, Accuracy: 85.42%\n",
      "Batch 70, Loss: 0.815218, Accuracy: 85.51%\n",
      "Batch 71, Loss: 0.910667, Accuracy: 85.45%\n",
      "Batch 72, Loss: 0.899975, Accuracy: 85.46%\n",
      "Batch 73, Loss: 0.824547, Accuracy: 85.57%\n",
      "Batch 74, Loss: 0.858792, Accuracy: 85.62%\n",
      "Batch 75, Loss: 0.854057, Accuracy: 85.69%\n",
      "Batch 76, Loss: 0.857779, Accuracy: 85.71%\n",
      "Batch 77, Loss: 0.837154, Accuracy: 85.80%\n",
      "Batch 78, Loss: 0.948874, Accuracy: 85.70%\n",
      "Batch 79, Loss: 0.840633, Accuracy: 85.76%\n",
      "Batch 80, Loss: 0.858948, Accuracy: 85.80%\n",
      "Batch 81, Loss: 0.863526, Accuracy: 85.84%\n",
      "Batch 82, Loss: 0.983994, Accuracy: 85.71%\n",
      "Batch 83, Loss: 0.818574, Accuracy: 85.81%\n",
      "Batch 84, Loss: 0.934280, Accuracy: 85.77%\n",
      "Batch 85, Loss: 0.862661, Accuracy: 85.81%\n",
      "Batch 86, Loss: 0.901523, Accuracy: 85.81%\n",
      "Batch 87, Loss: 0.871618, Accuracy: 85.81%\n",
      "Batch 88, Loss: 0.893546, Accuracy: 85.81%\n",
      "Batch 89, Loss: 0.869649, Accuracy: 85.83%\n",
      "Batch 90, Loss: 0.887871, Accuracy: 85.80%\n",
      "Batch 91, Loss: 0.869209, Accuracy: 85.80%\n",
      "Batch 92, Loss: 0.891193, Accuracy: 85.78%\n",
      "Batch 93, Loss: 0.927777, Accuracy: 85.75%\n",
      "Batch 94, Loss: 0.897779, Accuracy: 85.74%\n",
      "Batch 95, Loss: 0.923020, Accuracy: 85.69%\n",
      "Batch 96, Loss: 0.888246, Accuracy: 85.69%\n",
      "Batch 97, Loss: 0.915469, Accuracy: 85.66%\n",
      "Batch 98, Loss: 0.921177, Accuracy: 85.63%\n",
      "Batch 99, Loss: 0.913182, Accuracy: 85.62%\n",
      "Batch 100, Loss: 0.866154, Accuracy: 85.64%\n",
      "Batch 101, Loss: 0.882411, Accuracy: 85.66%\n",
      "Batch 102, Loss: 0.883091, Accuracy: 85.68%\n",
      "Batch 103, Loss: 0.939684, Accuracy: 85.62%\n",
      "Batch 104, Loss: 0.945539, Accuracy: 85.56%\n",
      "Batch 105, Loss: 0.889210, Accuracy: 85.55%\n",
      "Batch 106, Loss: 0.957680, Accuracy: 85.50%\n",
      "Batch 107, Loss: 0.839202, Accuracy: 85.56%\n",
      "Batch 108, Loss: 0.895131, Accuracy: 85.55%\n",
      "Batch 109, Loss: 0.867748, Accuracy: 85.58%\n",
      "Batch 110, Loss: 0.913635, Accuracy: 85.53%\n",
      "Batch 111, Loss: 0.870127, Accuracy: 85.54%\n",
      "Batch 112, Loss: 0.886871, Accuracy: 85.55%\n",
      "Batch 113, Loss: 0.920538, Accuracy: 85.50%\n",
      "Batch 114, Loss: 0.924398, Accuracy: 85.46%\n",
      "Batch 115, Loss: 0.947566, Accuracy: 85.37%\n",
      "Batch 116, Loss: 0.951350, Accuracy: 85.30%\n",
      "Batch 117, Loss: 0.964701, Accuracy: 85.23%\n",
      "Batch 118, Loss: 0.877346, Accuracy: 85.24%\n",
      "Batch 119, Loss: 0.898404, Accuracy: 85.23%\n",
      "Batch 120, Loss: 0.828526, Accuracy: 85.27%\n",
      "Batch 121, Loss: 0.833717, Accuracy: 85.33%\n",
      "Batch 122, Loss: 0.852054, Accuracy: 85.35%\n",
      "Batch 123, Loss: 0.949454, Accuracy: 85.30%\n",
      "Batch 124, Loss: 0.891011, Accuracy: 85.31%\n",
      "Batch 125, Loss: 0.859443, Accuracy: 85.34%\n",
      "Batch 126, Loss: 0.993955, Accuracy: 85.23%\n",
      "Batch 127, Loss: 0.890863, Accuracy: 85.24%\n",
      "Batch 128, Loss: 0.885911, Accuracy: 85.24%\n",
      "Batch 129, Loss: 0.901065, Accuracy: 85.23%\n",
      "Batch 130, Loss: 0.898513, Accuracy: 85.23%\n",
      "Batch 131, Loss: 0.913189, Accuracy: 85.21%\n",
      "Batch 132, Loss: 0.908740, Accuracy: 85.20%\n",
      "Batch 133, Loss: 0.967605, Accuracy: 85.16%\n",
      "Batch 134, Loss: 0.914486, Accuracy: 85.14%\n",
      "Batch 135, Loss: 0.874727, Accuracy: 85.17%\n",
      "Batch 136, Loss: 0.915055, Accuracy: 85.17%\n",
      "Batch 137, Loss: 0.809108, Accuracy: 85.23%\n",
      "Batch 138, Loss: 0.908792, Accuracy: 85.24%\n",
      "Batch 139, Loss: 0.917231, Accuracy: 85.22%\n",
      "Batch 140, Loss: 0.810261, Accuracy: 85.29%\n",
      "Batch 141, Loss: 0.907076, Accuracy: 85.27%\n",
      "Batch 142, Loss: 0.847301, Accuracy: 85.31%\n",
      "Batch 143, Loss: 0.903485, Accuracy: 85.30%\n",
      "Batch 144, Loss: 0.864925, Accuracy: 85.32%\n",
      "Batch 145, Loss: 0.879908, Accuracy: 85.34%\n",
      "Batch 146, Loss: 0.871968, Accuracy: 85.36%\n",
      "Batch 147, Loss: 0.871435, Accuracy: 85.36%\n",
      "Batch 148, Loss: 0.871180, Accuracy: 85.38%\n",
      "Batch 149, Loss: 0.908529, Accuracy: 85.36%\n",
      "Batch 150, Loss: 0.859250, Accuracy: 85.39%\n",
      "Batch 151, Loss: 0.889553, Accuracy: 85.39%\n",
      "Batch 152, Loss: 0.883491, Accuracy: 85.39%\n",
      "Batch 153, Loss: 0.865771, Accuracy: 85.40%\n",
      "Batch 154, Loss: 0.913854, Accuracy: 85.38%\n",
      "Batch 155, Loss: 0.861234, Accuracy: 85.40%\n",
      "Batch 156, Loss: 0.850285, Accuracy: 85.44%\n",
      "Batch 157, Loss: 0.882334, Accuracy: 85.45%\n",
      "Batch 158, Loss: 0.858639, Accuracy: 85.46%\n",
      "Batch 159, Loss: 0.824154, Accuracy: 85.51%\n",
      "Batch 160, Loss: 0.878015, Accuracy: 85.51%\n",
      "Batch 161, Loss: 0.872473, Accuracy: 85.52%\n",
      "Batch 162, Loss: 0.888837, Accuracy: 85.52%\n",
      "Batch 163, Loss: 0.887120, Accuracy: 85.52%\n",
      "Batch 164, Loss: 0.909008, Accuracy: 85.50%\n",
      "Batch 165, Loss: 0.870618, Accuracy: 85.51%\n",
      "Batch 166, Loss: 0.865109, Accuracy: 85.53%\n",
      "Batch 167, Loss: 0.876560, Accuracy: 85.54%\n",
      "Batch 168, Loss: 0.948140, Accuracy: 85.50%\n",
      "Batch 169, Loss: 0.848288, Accuracy: 85.52%\n",
      "Batch 170, Loss: 0.933740, Accuracy: 85.49%\n",
      "Batch 171, Loss: 0.969383, Accuracy: 85.43%\n",
      "Batch 172, Loss: 0.880221, Accuracy: 85.44%\n",
      "Batch 173, Loss: 0.879379, Accuracy: 85.45%\n",
      "Batch 174, Loss: 0.876379, Accuracy: 85.46%\n",
      "Batch 175, Loss: 0.927910, Accuracy: 85.45%\n",
      "Batch 176, Loss: 0.946017, Accuracy: 85.42%\n",
      "Batch 177, Loss: 0.885254, Accuracy: 85.43%\n",
      "Batch 178, Loss: 0.885197, Accuracy: 85.43%\n",
      "Batch 179, Loss: 0.909989, Accuracy: 85.42%\n",
      "Batch 180, Loss: 0.881629, Accuracy: 85.43%\n",
      "Batch 181, Loss: 0.841983, Accuracy: 85.46%\n",
      "Batch 182, Loss: 0.873681, Accuracy: 85.47%\n",
      "Batch 183, Loss: 0.930771, Accuracy: 85.46%\n",
      "Batch 184, Loss: 0.899867, Accuracy: 85.44%\n",
      "Batch 185, Loss: 0.943057, Accuracy: 85.41%\n",
      "Batch 186, Loss: 0.938510, Accuracy: 85.37%\n",
      "Batch 187, Loss: 0.874077, Accuracy: 85.39%\n",
      "Batch 188, Loss: 0.888685, Accuracy: 85.39%\n",
      "Batch 189, Loss: 0.883330, Accuracy: 85.39%\n",
      "Batch 190, Loss: 0.903312, Accuracy: 85.38%\n",
      "Batch 191, Loss: 0.886967, Accuracy: 85.38%\n",
      "Batch 192, Loss: 0.857509, Accuracy: 85.39%\n",
      "Batch 193, Loss: 0.865733, Accuracy: 85.40%\n",
      "Batch 194, Loss: 0.942817, Accuracy: 85.37%\n",
      "Batch 195, Loss: 0.953230, Accuracy: 85.34%\n",
      "Batch 196, Loss: 0.879098, Accuracy: 85.33%\n",
      "Batch 197, Loss: 0.880965, Accuracy: 85.34%\n",
      "Batch 198, Loss: 0.841580, Accuracy: 85.38%\n",
      "Batch 199, Loss: 0.838682, Accuracy: 85.40%\n",
      "Batch 200, Loss: 0.888193, Accuracy: 85.41%\n",
      "Batch 201, Loss: 0.874680, Accuracy: 85.42%\n",
      "Batch 202, Loss: 0.798743, Accuracy: 85.47%\n",
      "Batch 203, Loss: 0.857103, Accuracy: 85.49%\n",
      "Batch 204, Loss: 0.854759, Accuracy: 85.50%\n",
      "Batch 205, Loss: 0.893956, Accuracy: 85.50%\n",
      "Batch 206, Loss: 0.899789, Accuracy: 85.51%\n",
      "Batch 207, Loss: 0.860922, Accuracy: 85.52%\n",
      "Batch 208, Loss: 0.886883, Accuracy: 85.52%\n",
      "Batch 209, Loss: 0.912496, Accuracy: 85.51%\n",
      "Batch 210, Loss: 0.892513, Accuracy: 85.51%\n",
      "Batch 211, Loss: 0.924264, Accuracy: 85.48%\n",
      "Batch 212, Loss: 0.839261, Accuracy: 85.51%\n",
      "Batch 213, Loss: 0.924984, Accuracy: 85.50%\n",
      "Training - Epoch 68, Loss: 0.889576, Accuracy: 85.50%\n",
      "Validation Batch 1, Loss: 0.865094, Accuracy: 89.06%\n",
      "Validation Batch 2, Loss: 0.844657, Accuracy: 90.62%\n",
      "Validation Batch 3, Loss: 0.923897, Accuracy: 88.54%\n",
      "Validation Batch 4, Loss: 0.876096, Accuracy: 88.28%\n",
      "Validation Batch 5, Loss: 0.859854, Accuracy: 88.44%\n",
      "Validation Batch 6, Loss: 0.826827, Accuracy: 89.32%\n",
      "Validation Batch 7, Loss: 0.845605, Accuracy: 89.51%\n",
      "Validation Batch 8, Loss: 0.949000, Accuracy: 88.09%\n",
      "Validation Batch 9, Loss: 0.919728, Accuracy: 87.50%\n",
      "Validation Batch 10, Loss: 0.882841, Accuracy: 87.50%\n",
      "Validation Batch 11, Loss: 0.874466, Accuracy: 87.50%\n",
      "Validation Batch 12, Loss: 0.867489, Accuracy: 87.76%\n",
      "Validation Batch 13, Loss: 0.876124, Accuracy: 87.74%\n",
      "Validation Batch 14, Loss: 0.899511, Accuracy: 87.50%\n",
      "Validation Batch 15, Loss: 0.858522, Accuracy: 87.50%\n",
      "Validation Batch 16, Loss: 0.873914, Accuracy: 87.60%\n",
      "Validation Batch 17, Loss: 0.915239, Accuracy: 87.41%\n",
      "Validation Batch 18, Loss: 0.862995, Accuracy: 87.41%\n",
      "Validation Batch 19, Loss: 0.919644, Accuracy: 87.17%\n",
      "Validation Batch 20, Loss: 0.941003, Accuracy: 86.80%\n",
      "Validation Batch 21, Loss: 0.901974, Accuracy: 86.76%\n",
      "Validation Batch 22, Loss: 0.884219, Accuracy: 86.72%\n",
      "Validation Batch 23, Loss: 0.916523, Accuracy: 86.55%\n",
      "Validation Batch 24, Loss: 0.902275, Accuracy: 86.39%\n",
      "Validation Batch 25, Loss: 0.846694, Accuracy: 86.56%\n",
      "Validation Batch 26, Loss: 0.879418, Accuracy: 86.60%\n",
      "Validation Batch 27, Loss: 0.844458, Accuracy: 86.79%\n",
      "Validation - Epoch 68, Loss: 0.883632, Accuracy: 86.79%\n",
      "Patience—0\n",
      "Epoch 69\n",
      "Batch 1, Loss: 0.865865, Accuracy: 87.50%\n",
      "Batch 2, Loss: 0.864813, Accuracy: 88.28%\n",
      "Batch 3, Loss: 0.823870, Accuracy: 90.10%\n",
      "Batch 4, Loss: 0.839242, Accuracy: 90.23%\n",
      "Batch 5, Loss: 0.835311, Accuracy: 90.31%\n",
      "Batch 6, Loss: 0.834278, Accuracy: 90.36%\n",
      "Batch 7, Loss: 0.858426, Accuracy: 90.18%\n",
      "Batch 8, Loss: 0.952810, Accuracy: 88.67%\n",
      "Batch 9, Loss: 0.952471, Accuracy: 87.50%\n",
      "Batch 10, Loss: 0.894389, Accuracy: 87.50%\n",
      "Batch 11, Loss: 0.952677, Accuracy: 86.51%\n",
      "Batch 12, Loss: 0.927268, Accuracy: 86.07%\n",
      "Batch 13, Loss: 0.854721, Accuracy: 86.30%\n",
      "Batch 14, Loss: 0.880824, Accuracy: 86.27%\n",
      "Batch 15, Loss: 0.843100, Accuracy: 86.46%\n",
      "Batch 16, Loss: 0.867908, Accuracy: 86.62%\n",
      "Batch 17, Loss: 0.890382, Accuracy: 86.49%\n",
      "Batch 18, Loss: 0.852506, Accuracy: 86.63%\n",
      "Batch 19, Loss: 0.854241, Accuracy: 86.84%\n",
      "Batch 20, Loss: 0.845151, Accuracy: 87.03%\n",
      "Batch 21, Loss: 0.887451, Accuracy: 86.90%\n",
      "Batch 22, Loss: 0.859710, Accuracy: 86.79%\n",
      "Batch 23, Loss: 0.877672, Accuracy: 86.75%\n",
      "Batch 24, Loss: 0.932441, Accuracy: 86.59%\n",
      "Batch 25, Loss: 0.971695, Accuracy: 86.19%\n",
      "Batch 26, Loss: 0.875307, Accuracy: 86.18%\n",
      "Batch 27, Loss: 0.939369, Accuracy: 85.94%\n",
      "Batch 28, Loss: 0.889102, Accuracy: 85.88%\n",
      "Batch 29, Loss: 0.892025, Accuracy: 85.88%\n",
      "Batch 30, Loss: 0.897621, Accuracy: 85.83%\n",
      "Batch 31, Loss: 0.919239, Accuracy: 85.69%\n",
      "Batch 32, Loss: 0.902073, Accuracy: 85.64%\n",
      "Batch 33, Loss: 0.891923, Accuracy: 85.65%\n",
      "Batch 34, Loss: 0.937253, Accuracy: 85.52%\n",
      "Batch 35, Loss: 0.820855, Accuracy: 85.76%\n",
      "Batch 36, Loss: 0.943397, Accuracy: 85.59%\n",
      "Batch 37, Loss: 0.958993, Accuracy: 85.43%\n",
      "Batch 38, Loss: 0.850885, Accuracy: 85.57%\n",
      "Batch 39, Loss: 0.856960, Accuracy: 85.66%\n",
      "Batch 40, Loss: 0.882771, Accuracy: 85.62%\n",
      "Batch 41, Loss: 0.883764, Accuracy: 85.63%\n",
      "Batch 42, Loss: 0.862276, Accuracy: 85.71%\n",
      "Batch 43, Loss: 0.899989, Accuracy: 85.68%\n",
      "Batch 44, Loss: 0.910811, Accuracy: 85.58%\n",
      "Batch 45, Loss: 0.939879, Accuracy: 85.52%\n",
      "Batch 46, Loss: 0.881597, Accuracy: 85.53%\n",
      "Batch 47, Loss: 0.851271, Accuracy: 85.61%\n",
      "Batch 48, Loss: 0.895584, Accuracy: 85.58%\n",
      "Batch 49, Loss: 0.911031, Accuracy: 85.55%\n",
      "Batch 50, Loss: 0.853453, Accuracy: 85.62%\n",
      "Batch 51, Loss: 0.887167, Accuracy: 85.66%\n",
      "Batch 52, Loss: 0.909743, Accuracy: 85.55%\n",
      "Batch 53, Loss: 0.855009, Accuracy: 85.61%\n",
      "Batch 54, Loss: 0.890278, Accuracy: 85.62%\n",
      "Batch 55, Loss: 0.903716, Accuracy: 85.62%\n",
      "Batch 56, Loss: 0.900652, Accuracy: 85.60%\n",
      "Batch 57, Loss: 0.914155, Accuracy: 85.55%\n",
      "Batch 58, Loss: 0.896924, Accuracy: 85.53%\n",
      "Batch 59, Loss: 0.934898, Accuracy: 85.46%\n",
      "Batch 60, Loss: 0.920212, Accuracy: 85.42%\n",
      "Batch 61, Loss: 0.882868, Accuracy: 85.45%\n",
      "Batch 62, Loss: 0.918418, Accuracy: 85.38%\n",
      "Batch 63, Loss: 0.954330, Accuracy: 85.27%\n",
      "Batch 64, Loss: 0.903031, Accuracy: 85.25%\n",
      "Batch 65, Loss: 0.919397, Accuracy: 85.24%\n",
      "Batch 66, Loss: 0.893518, Accuracy: 85.23%\n",
      "Batch 67, Loss: 1.020229, Accuracy: 85.03%\n",
      "Batch 68, Loss: 0.881903, Accuracy: 85.02%\n",
      "Batch 69, Loss: 0.867142, Accuracy: 85.08%\n",
      "Batch 70, Loss: 0.840598, Accuracy: 85.16%\n",
      "Batch 71, Loss: 0.842733, Accuracy: 85.21%\n",
      "Batch 72, Loss: 0.889224, Accuracy: 85.24%\n",
      "Batch 73, Loss: 0.850800, Accuracy: 85.30%\n",
      "Batch 74, Loss: 0.858834, Accuracy: 85.35%\n",
      "Batch 75, Loss: 0.918738, Accuracy: 85.33%\n",
      "Batch 76, Loss: 0.887085, Accuracy: 85.34%\n",
      "Batch 77, Loss: 0.873883, Accuracy: 85.39%\n",
      "Batch 78, Loss: 0.923860, Accuracy: 85.34%\n",
      "Batch 79, Loss: 0.906305, Accuracy: 85.30%\n",
      "Batch 80, Loss: 0.926450, Accuracy: 85.23%\n",
      "Batch 81, Loss: 0.847922, Accuracy: 85.28%\n",
      "Batch 82, Loss: 0.897032, Accuracy: 85.29%\n",
      "Batch 83, Loss: 0.947642, Accuracy: 85.24%\n",
      "Batch 84, Loss: 0.880652, Accuracy: 85.25%\n",
      "Batch 85, Loss: 0.839453, Accuracy: 85.31%\n",
      "Batch 86, Loss: 0.899448, Accuracy: 85.30%\n",
      "Batch 87, Loss: 0.958356, Accuracy: 85.22%\n",
      "Batch 88, Loss: 0.932670, Accuracy: 85.14%\n",
      "Batch 89, Loss: 0.873595, Accuracy: 85.18%\n",
      "Batch 90, Loss: 0.902717, Accuracy: 85.19%\n",
      "Batch 91, Loss: 0.943697, Accuracy: 85.13%\n",
      "Batch 92, Loss: 0.877149, Accuracy: 85.16%\n",
      "Batch 93, Loss: 0.855541, Accuracy: 85.22%\n",
      "Batch 94, Loss: 0.870594, Accuracy: 85.24%\n",
      "Batch 95, Loss: 0.917989, Accuracy: 85.21%\n",
      "Batch 96, Loss: 0.897789, Accuracy: 85.21%\n",
      "Batch 97, Loss: 0.936721, Accuracy: 85.15%\n",
      "Batch 98, Loss: 0.957300, Accuracy: 85.08%\n",
      "Batch 99, Loss: 0.892349, Accuracy: 85.09%\n",
      "Batch 100, Loss: 0.859488, Accuracy: 85.11%\n",
      "Batch 101, Loss: 0.841061, Accuracy: 85.15%\n",
      "Batch 102, Loss: 0.871601, Accuracy: 85.16%\n",
      "Batch 103, Loss: 0.897603, Accuracy: 85.15%\n",
      "Batch 104, Loss: 0.914939, Accuracy: 85.11%\n",
      "Batch 105, Loss: 0.902173, Accuracy: 85.09%\n",
      "Batch 106, Loss: 0.941337, Accuracy: 85.02%\n",
      "Batch 107, Loss: 0.915152, Accuracy: 85.02%\n",
      "Batch 108, Loss: 0.858178, Accuracy: 85.05%\n",
      "Batch 109, Loss: 0.933943, Accuracy: 85.01%\n",
      "Batch 110, Loss: 0.885418, Accuracy: 85.01%\n",
      "Batch 111, Loss: 0.901214, Accuracy: 84.99%\n",
      "Batch 112, Loss: 0.860586, Accuracy: 85.02%\n",
      "Batch 113, Loss: 0.887783, Accuracy: 85.02%\n",
      "Batch 114, Loss: 0.927141, Accuracy: 85.01%\n",
      "Batch 115, Loss: 0.905171, Accuracy: 85.00%\n",
      "Batch 116, Loss: 0.879942, Accuracy: 85.01%\n",
      "Batch 117, Loss: 0.863979, Accuracy: 85.04%\n",
      "Batch 118, Loss: 0.903004, Accuracy: 85.04%\n",
      "Batch 119, Loss: 0.859425, Accuracy: 85.07%\n",
      "Batch 120, Loss: 0.944923, Accuracy: 85.01%\n",
      "Batch 121, Loss: 0.930948, Accuracy: 84.98%\n",
      "Batch 122, Loss: 0.928369, Accuracy: 84.95%\n",
      "Batch 123, Loss: 0.872909, Accuracy: 84.97%\n",
      "Batch 124, Loss: 0.896416, Accuracy: 84.97%\n",
      "Batch 125, Loss: 0.857342, Accuracy: 84.99%\n",
      "Batch 126, Loss: 0.862417, Accuracy: 85.01%\n",
      "Batch 127, Loss: 0.891231, Accuracy: 85.00%\n",
      "Batch 128, Loss: 0.904853, Accuracy: 85.02%\n",
      "Batch 129, Loss: 0.883558, Accuracy: 85.03%\n",
      "Batch 130, Loss: 0.856149, Accuracy: 85.07%\n",
      "Batch 131, Loss: 0.880346, Accuracy: 85.09%\n",
      "Batch 132, Loss: 0.882458, Accuracy: 85.11%\n",
      "Batch 133, Loss: 0.845688, Accuracy: 85.15%\n",
      "Batch 134, Loss: 0.915904, Accuracy: 85.12%\n",
      "Batch 135, Loss: 0.943170, Accuracy: 85.09%\n",
      "Batch 136, Loss: 0.861867, Accuracy: 85.12%\n",
      "Batch 137, Loss: 0.870754, Accuracy: 85.15%\n",
      "Batch 138, Loss: 0.856066, Accuracy: 85.19%\n",
      "Batch 139, Loss: 0.897665, Accuracy: 85.17%\n",
      "Batch 140, Loss: 0.971337, Accuracy: 85.11%\n",
      "Batch 141, Loss: 0.869627, Accuracy: 85.14%\n",
      "Batch 142, Loss: 0.851519, Accuracy: 85.18%\n",
      "Batch 143, Loss: 0.901644, Accuracy: 85.17%\n",
      "Batch 144, Loss: 0.890767, Accuracy: 85.17%\n",
      "Batch 145, Loss: 0.892694, Accuracy: 85.16%\n",
      "Batch 146, Loss: 0.886220, Accuracy: 85.17%\n",
      "Batch 147, Loss: 0.899735, Accuracy: 85.16%\n",
      "Batch 148, Loss: 0.910541, Accuracy: 85.15%\n",
      "Batch 149, Loss: 0.970321, Accuracy: 85.10%\n",
      "Batch 150, Loss: 0.874681, Accuracy: 85.12%\n",
      "Batch 151, Loss: 0.817797, Accuracy: 85.19%\n",
      "Batch 152, Loss: 0.873538, Accuracy: 85.20%\n",
      "Batch 153, Loss: 0.935345, Accuracy: 85.15%\n",
      "Batch 154, Loss: 0.897522, Accuracy: 85.15%\n",
      "Batch 155, Loss: 0.890990, Accuracy: 85.14%\n",
      "Batch 156, Loss: 0.845962, Accuracy: 85.18%\n",
      "Batch 157, Loss: 0.956600, Accuracy: 85.14%\n",
      "Batch 158, Loss: 0.876446, Accuracy: 85.15%\n",
      "Batch 159, Loss: 0.855938, Accuracy: 85.17%\n",
      "Batch 160, Loss: 0.909209, Accuracy: 85.16%\n",
      "Batch 161, Loss: 0.931813, Accuracy: 85.14%\n",
      "Batch 162, Loss: 0.908289, Accuracy: 85.14%\n",
      "Batch 163, Loss: 0.863686, Accuracy: 85.16%\n",
      "Batch 164, Loss: 0.912620, Accuracy: 85.15%\n",
      "Batch 165, Loss: 0.835535, Accuracy: 85.19%\n",
      "Batch 166, Loss: 0.861782, Accuracy: 85.21%\n",
      "Batch 167, Loss: 0.890602, Accuracy: 85.22%\n",
      "Batch 168, Loss: 0.875487, Accuracy: 85.24%\n",
      "Batch 169, Loss: 0.824854, Accuracy: 85.28%\n",
      "Batch 170, Loss: 0.845805, Accuracy: 85.31%\n",
      "Batch 171, Loss: 0.880326, Accuracy: 85.33%\n",
      "Batch 172, Loss: 0.900752, Accuracy: 85.32%\n",
      "Batch 173, Loss: 0.908038, Accuracy: 85.31%\n",
      "Batch 174, Loss: 0.848497, Accuracy: 85.35%\n",
      "Batch 175, Loss: 0.931991, Accuracy: 85.34%\n",
      "Batch 176, Loss: 0.859891, Accuracy: 85.35%\n",
      "Batch 177, Loss: 0.840343, Accuracy: 85.39%\n",
      "Batch 178, Loss: 0.879118, Accuracy: 85.38%\n",
      "Batch 179, Loss: 0.907020, Accuracy: 85.38%\n",
      "Batch 180, Loss: 0.882560, Accuracy: 85.38%\n",
      "Batch 181, Loss: 0.952812, Accuracy: 85.34%\n",
      "Batch 182, Loss: 0.890629, Accuracy: 85.35%\n",
      "Batch 183, Loss: 0.861276, Accuracy: 85.37%\n",
      "Batch 184, Loss: 0.935971, Accuracy: 85.34%\n",
      "Batch 185, Loss: 0.877798, Accuracy: 85.35%\n",
      "Batch 186, Loss: 0.834707, Accuracy: 85.39%\n",
      "Batch 187, Loss: 0.865766, Accuracy: 85.41%\n",
      "Batch 188, Loss: 0.865528, Accuracy: 85.42%\n",
      "Batch 189, Loss: 0.903344, Accuracy: 85.41%\n",
      "Batch 190, Loss: 0.885586, Accuracy: 85.40%\n",
      "Batch 191, Loss: 0.861122, Accuracy: 85.42%\n",
      "Batch 192, Loss: 0.970489, Accuracy: 85.39%\n",
      "Batch 193, Loss: 0.845263, Accuracy: 85.41%\n",
      "Batch 194, Loss: 0.933605, Accuracy: 85.39%\n",
      "Batch 195, Loss: 0.932530, Accuracy: 85.35%\n",
      "Batch 196, Loss: 0.900126, Accuracy: 85.35%\n",
      "Batch 197, Loss: 0.870163, Accuracy: 85.35%\n",
      "Batch 198, Loss: 0.864186, Accuracy: 85.35%\n",
      "Batch 199, Loss: 0.866187, Accuracy: 85.37%\n",
      "Batch 200, Loss: 0.815448, Accuracy: 85.41%\n",
      "Batch 201, Loss: 0.926921, Accuracy: 85.39%\n",
      "Batch 202, Loss: 0.898104, Accuracy: 85.38%\n",
      "Batch 203, Loss: 0.901946, Accuracy: 85.37%\n",
      "Batch 204, Loss: 0.906981, Accuracy: 85.35%\n",
      "Batch 205, Loss: 0.914448, Accuracy: 85.34%\n",
      "Batch 206, Loss: 0.828580, Accuracy: 85.37%\n",
      "Batch 207, Loss: 0.886513, Accuracy: 85.36%\n",
      "Batch 208, Loss: 0.914574, Accuracy: 85.36%\n",
      "Batch 209, Loss: 0.846283, Accuracy: 85.38%\n",
      "Batch 210, Loss: 0.951926, Accuracy: 85.34%\n",
      "Batch 211, Loss: 0.846241, Accuracy: 85.36%\n",
      "Batch 212, Loss: 0.932396, Accuracy: 85.33%\n",
      "Batch 213, Loss: 0.936930, Accuracy: 85.31%\n",
      "Training - Epoch 69, Loss: 0.891321, Accuracy: 85.31%\n",
      "Validation Batch 1, Loss: 0.866983, Accuracy: 89.06%\n",
      "Validation Batch 2, Loss: 0.847019, Accuracy: 90.62%\n",
      "Validation Batch 3, Loss: 0.921718, Accuracy: 88.54%\n",
      "Validation Batch 4, Loss: 0.880624, Accuracy: 88.28%\n",
      "Validation Batch 5, Loss: 0.872759, Accuracy: 88.44%\n",
      "Validation Batch 6, Loss: 0.834482, Accuracy: 89.32%\n",
      "Validation Batch 7, Loss: 0.849541, Accuracy: 89.51%\n",
      "Validation Batch 8, Loss: 0.957837, Accuracy: 87.89%\n",
      "Validation Batch 9, Loss: 0.927358, Accuracy: 87.33%\n",
      "Validation Batch 10, Loss: 0.888180, Accuracy: 87.19%\n",
      "Validation Batch 11, Loss: 0.881047, Accuracy: 87.22%\n",
      "Validation Batch 12, Loss: 0.868952, Accuracy: 87.50%\n",
      "Validation Batch 13, Loss: 0.883700, Accuracy: 87.38%\n",
      "Validation Batch 14, Loss: 0.903772, Accuracy: 87.17%\n",
      "Validation Batch 15, Loss: 0.866642, Accuracy: 87.19%\n",
      "Validation Batch 16, Loss: 0.880976, Accuracy: 87.21%\n",
      "Validation Batch 17, Loss: 0.921982, Accuracy: 86.95%\n",
      "Validation Batch 18, Loss: 0.865143, Accuracy: 86.89%\n",
      "Validation Batch 19, Loss: 0.927133, Accuracy: 86.68%\n",
      "Validation Batch 20, Loss: 0.948224, Accuracy: 86.25%\n",
      "Validation Batch 21, Loss: 0.905513, Accuracy: 86.24%\n",
      "Validation Batch 22, Loss: 0.898536, Accuracy: 86.15%\n",
      "Validation Batch 23, Loss: 0.918205, Accuracy: 86.01%\n",
      "Validation Batch 24, Loss: 0.908763, Accuracy: 85.94%\n",
      "Validation Batch 25, Loss: 0.848307, Accuracy: 86.12%\n",
      "Validation Batch 26, Loss: 0.886701, Accuracy: 86.12%\n",
      "Validation Batch 27, Loss: 0.862343, Accuracy: 86.14%\n",
      "Validation - Epoch 69, Loss: 0.889720, Accuracy: 86.14%\n",
      "Patience—1\n",
      "Epoch 70\n",
      "Batch 1, Loss: 0.847975, Accuracy: 89.06%\n",
      "Batch 2, Loss: 0.910504, Accuracy: 85.94%\n",
      "Batch 3, Loss: 0.956916, Accuracy: 82.81%\n",
      "Batch 4, Loss: 0.935806, Accuracy: 82.42%\n",
      "Batch 5, Loss: 0.919763, Accuracy: 82.50%\n",
      "Batch 6, Loss: 0.891814, Accuracy: 83.07%\n",
      "Batch 7, Loss: 0.835797, Accuracy: 84.38%\n",
      "Batch 8, Loss: 0.902004, Accuracy: 84.18%\n",
      "Batch 9, Loss: 0.871959, Accuracy: 84.55%\n",
      "Batch 10, Loss: 0.913165, Accuracy: 84.22%\n",
      "Batch 11, Loss: 0.885550, Accuracy: 84.66%\n",
      "Batch 12, Loss: 0.868611, Accuracy: 85.03%\n",
      "Batch 13, Loss: 0.942748, Accuracy: 84.74%\n",
      "Batch 14, Loss: 0.864368, Accuracy: 84.93%\n",
      "Batch 15, Loss: 0.889066, Accuracy: 85.00%\n",
      "Batch 16, Loss: 0.845280, Accuracy: 85.45%\n",
      "Batch 17, Loss: 0.895408, Accuracy: 85.39%\n",
      "Batch 18, Loss: 0.924823, Accuracy: 85.16%\n",
      "Batch 19, Loss: 0.842984, Accuracy: 85.44%\n",
      "Batch 20, Loss: 0.890238, Accuracy: 85.39%\n",
      "Batch 21, Loss: 0.836155, Accuracy: 85.64%\n",
      "Batch 22, Loss: 0.818101, Accuracy: 86.01%\n",
      "Batch 23, Loss: 0.849699, Accuracy: 86.14%\n",
      "Batch 24, Loss: 0.877145, Accuracy: 86.20%\n",
      "Batch 25, Loss: 0.882010, Accuracy: 86.19%\n",
      "Batch 26, Loss: 0.887816, Accuracy: 86.18%\n",
      "Batch 27, Loss: 0.871796, Accuracy: 86.23%\n",
      "Batch 28, Loss: 0.920259, Accuracy: 86.16%\n",
      "Batch 29, Loss: 0.925161, Accuracy: 85.99%\n",
      "Batch 30, Loss: 0.865099, Accuracy: 86.04%\n",
      "Batch 31, Loss: 0.925643, Accuracy: 85.94%\n",
      "Batch 32, Loss: 1.000744, Accuracy: 85.55%\n",
      "Batch 33, Loss: 0.818396, Accuracy: 85.80%\n",
      "Batch 34, Loss: 0.836970, Accuracy: 85.94%\n",
      "Batch 35, Loss: 0.924467, Accuracy: 85.85%\n",
      "Batch 36, Loss: 0.854238, Accuracy: 85.94%\n",
      "Batch 37, Loss: 0.882845, Accuracy: 85.94%\n",
      "Batch 38, Loss: 0.868688, Accuracy: 85.94%\n",
      "Batch 39, Loss: 0.908000, Accuracy: 85.90%\n",
      "Batch 40, Loss: 0.821328, Accuracy: 86.05%\n",
      "Batch 41, Loss: 0.815968, Accuracy: 86.24%\n",
      "Batch 42, Loss: 0.887810, Accuracy: 86.24%\n",
      "Batch 43, Loss: 0.946217, Accuracy: 86.12%\n",
      "Batch 44, Loss: 0.875134, Accuracy: 86.15%\n",
      "Batch 45, Loss: 0.858973, Accuracy: 86.22%\n",
      "Batch 46, Loss: 0.943940, Accuracy: 86.07%\n",
      "Batch 47, Loss: 0.889110, Accuracy: 86.04%\n",
      "Batch 48, Loss: 0.927752, Accuracy: 85.94%\n",
      "Batch 49, Loss: 0.870597, Accuracy: 85.94%\n",
      "Batch 50, Loss: 0.884309, Accuracy: 85.94%\n",
      "Batch 51, Loss: 0.897273, Accuracy: 85.91%\n",
      "Batch 52, Loss: 1.035450, Accuracy: 85.64%\n",
      "Batch 53, Loss: 0.879662, Accuracy: 85.64%\n",
      "Batch 54, Loss: 0.856783, Accuracy: 85.68%\n",
      "Batch 55, Loss: 0.876982, Accuracy: 85.68%\n",
      "Batch 56, Loss: 0.883288, Accuracy: 85.69%\n",
      "Batch 57, Loss: 0.849917, Accuracy: 85.75%\n",
      "Batch 58, Loss: 0.856647, Accuracy: 85.80%\n",
      "Batch 59, Loss: 0.885804, Accuracy: 85.78%\n",
      "Batch 60, Loss: 0.908752, Accuracy: 85.76%\n",
      "Batch 61, Loss: 0.884589, Accuracy: 85.73%\n",
      "Batch 62, Loss: 0.911165, Accuracy: 85.69%\n",
      "Batch 63, Loss: 0.902730, Accuracy: 85.66%\n",
      "Batch 64, Loss: 0.892432, Accuracy: 85.62%\n",
      "Batch 65, Loss: 0.889307, Accuracy: 85.62%\n",
      "Batch 66, Loss: 0.910792, Accuracy: 85.61%\n",
      "Batch 67, Loss: 0.917973, Accuracy: 85.56%\n",
      "Batch 68, Loss: 0.859852, Accuracy: 85.59%\n",
      "Batch 69, Loss: 0.919250, Accuracy: 85.51%\n",
      "Batch 70, Loss: 0.819624, Accuracy: 85.62%\n",
      "Batch 71, Loss: 0.889681, Accuracy: 85.63%\n",
      "Batch 72, Loss: 0.924681, Accuracy: 85.59%\n",
      "Batch 73, Loss: 1.035972, Accuracy: 85.34%\n",
      "Batch 74, Loss: 0.845449, Accuracy: 85.41%\n",
      "Batch 75, Loss: 0.929075, Accuracy: 85.35%\n",
      "Batch 76, Loss: 0.901033, Accuracy: 85.32%\n",
      "Batch 77, Loss: 0.896611, Accuracy: 85.31%\n",
      "Batch 78, Loss: 0.897799, Accuracy: 85.30%\n",
      "Batch 79, Loss: 0.860920, Accuracy: 85.36%\n",
      "Batch 80, Loss: 0.851506, Accuracy: 85.41%\n",
      "Batch 81, Loss: 0.848805, Accuracy: 85.44%\n",
      "Batch 82, Loss: 0.918829, Accuracy: 85.40%\n",
      "Batch 83, Loss: 0.830557, Accuracy: 85.49%\n",
      "Batch 84, Loss: 0.917394, Accuracy: 85.45%\n",
      "Batch 85, Loss: 0.867741, Accuracy: 85.50%\n",
      "Batch 86, Loss: 0.897901, Accuracy: 85.47%\n",
      "Batch 87, Loss: 0.853794, Accuracy: 85.52%\n",
      "Batch 88, Loss: 0.822623, Accuracy: 85.60%\n",
      "Batch 89, Loss: 0.879748, Accuracy: 85.60%\n",
      "Batch 90, Loss: 0.911774, Accuracy: 85.59%\n",
      "Batch 91, Loss: 0.832229, Accuracy: 85.65%\n",
      "Batch 92, Loss: 0.887217, Accuracy: 85.67%\n",
      "Batch 93, Loss: 0.871492, Accuracy: 85.70%\n",
      "Batch 94, Loss: 0.861489, Accuracy: 85.74%\n",
      "Batch 95, Loss: 0.874079, Accuracy: 85.76%\n",
      "Batch 96, Loss: 0.868724, Accuracy: 85.77%\n",
      "Batch 97, Loss: 0.926789, Accuracy: 85.73%\n",
      "Batch 98, Loss: 0.855043, Accuracy: 85.79%\n",
      "Batch 99, Loss: 0.923007, Accuracy: 85.76%\n",
      "Batch 100, Loss: 0.896536, Accuracy: 85.78%\n",
      "Batch 101, Loss: 0.812129, Accuracy: 85.86%\n",
      "Batch 102, Loss: 0.837919, Accuracy: 85.91%\n",
      "Batch 103, Loss: 0.860293, Accuracy: 85.92%\n",
      "Batch 104, Loss: 0.932295, Accuracy: 85.86%\n",
      "Batch 105, Loss: 0.941270, Accuracy: 85.82%\n",
      "Batch 106, Loss: 0.907327, Accuracy: 85.82%\n",
      "Batch 107, Loss: 0.848942, Accuracy: 85.88%\n",
      "Batch 108, Loss: 0.967703, Accuracy: 85.81%\n",
      "Batch 109, Loss: 0.869551, Accuracy: 85.82%\n",
      "Batch 110, Loss: 0.914153, Accuracy: 85.80%\n",
      "Batch 111, Loss: 0.889037, Accuracy: 85.78%\n",
      "Batch 112, Loss: 0.809299, Accuracy: 85.87%\n",
      "Batch 113, Loss: 0.863057, Accuracy: 85.88%\n",
      "Batch 114, Loss: 0.849866, Accuracy: 85.92%\n",
      "Batch 115, Loss: 0.876034, Accuracy: 85.92%\n",
      "Batch 116, Loss: 0.803725, Accuracy: 86.00%\n",
      "Batch 117, Loss: 0.872867, Accuracy: 86.03%\n",
      "Batch 118, Loss: 0.870840, Accuracy: 86.04%\n",
      "Batch 119, Loss: 0.903006, Accuracy: 86.04%\n",
      "Batch 120, Loss: 0.922844, Accuracy: 86.02%\n",
      "Batch 121, Loss: 0.821782, Accuracy: 86.07%\n",
      "Batch 122, Loss: 0.913180, Accuracy: 86.01%\n",
      "Batch 123, Loss: 0.852010, Accuracy: 86.04%\n",
      "Batch 124, Loss: 0.891233, Accuracy: 86.04%\n",
      "Batch 125, Loss: 0.874169, Accuracy: 86.05%\n",
      "Batch 126, Loss: 0.843915, Accuracy: 86.07%\n",
      "Batch 127, Loss: 0.829826, Accuracy: 86.12%\n",
      "Batch 128, Loss: 0.948084, Accuracy: 86.08%\n",
      "Batch 129, Loss: 0.872388, Accuracy: 86.09%\n",
      "Batch 130, Loss: 0.971173, Accuracy: 86.01%\n",
      "Batch 131, Loss: 0.866301, Accuracy: 86.03%\n",
      "Batch 132, Loss: 0.831607, Accuracy: 86.08%\n",
      "Batch 133, Loss: 0.980184, Accuracy: 86.01%\n",
      "Batch 134, Loss: 0.879622, Accuracy: 86.02%\n",
      "Batch 135, Loss: 0.971333, Accuracy: 85.95%\n",
      "Batch 136, Loss: 0.864251, Accuracy: 85.97%\n",
      "Batch 137, Loss: 0.850012, Accuracy: 86.01%\n",
      "Batch 138, Loss: 0.897193, Accuracy: 85.99%\n",
      "Batch 139, Loss: 0.859209, Accuracy: 86.00%\n",
      "Batch 140, Loss: 0.902283, Accuracy: 85.99%\n",
      "Batch 141, Loss: 0.928701, Accuracy: 85.95%\n",
      "Batch 142, Loss: 0.881720, Accuracy: 85.96%\n",
      "Batch 143, Loss: 0.835743, Accuracy: 85.99%\n",
      "Batch 144, Loss: 0.915613, Accuracy: 85.98%\n",
      "Batch 145, Loss: 0.879962, Accuracy: 85.99%\n",
      "Batch 146, Loss: 0.854206, Accuracy: 86.02%\n",
      "Batch 147, Loss: 0.894353, Accuracy: 86.02%\n",
      "Batch 148, Loss: 0.890076, Accuracy: 86.02%\n",
      "Batch 149, Loss: 0.887882, Accuracy: 86.03%\n",
      "Batch 150, Loss: 0.930253, Accuracy: 85.99%\n",
      "Batch 151, Loss: 0.870628, Accuracy: 86.00%\n",
      "Batch 152, Loss: 0.854582, Accuracy: 86.01%\n",
      "Batch 153, Loss: 0.838126, Accuracy: 86.05%\n",
      "Batch 154, Loss: 0.831702, Accuracy: 86.08%\n",
      "Batch 155, Loss: 0.987947, Accuracy: 86.00%\n",
      "Batch 156, Loss: 0.895169, Accuracy: 85.98%\n",
      "Batch 157, Loss: 0.852909, Accuracy: 86.01%\n",
      "Batch 158, Loss: 0.886423, Accuracy: 86.01%\n",
      "Batch 159, Loss: 0.872835, Accuracy: 86.03%\n",
      "Batch 160, Loss: 0.875360, Accuracy: 86.04%\n",
      "Batch 161, Loss: 0.884573, Accuracy: 86.02%\n",
      "Batch 162, Loss: 0.909081, Accuracy: 86.01%\n",
      "Batch 163, Loss: 0.920553, Accuracy: 85.98%\n",
      "Batch 164, Loss: 0.925259, Accuracy: 85.95%\n",
      "Batch 165, Loss: 0.861312, Accuracy: 85.97%\n",
      "Batch 166, Loss: 0.900685, Accuracy: 85.95%\n",
      "Batch 167, Loss: 0.921476, Accuracy: 85.93%\n",
      "Batch 168, Loss: 0.936322, Accuracy: 85.90%\n",
      "Batch 169, Loss: 0.915780, Accuracy: 85.86%\n",
      "Batch 170, Loss: 0.875104, Accuracy: 85.87%\n",
      "Batch 171, Loss: 0.896705, Accuracy: 85.86%\n",
      "Batch 172, Loss: 0.833578, Accuracy: 85.89%\n",
      "Batch 173, Loss: 0.870906, Accuracy: 85.90%\n",
      "Batch 174, Loss: 0.850797, Accuracy: 85.93%\n",
      "Batch 175, Loss: 0.894769, Accuracy: 85.92%\n",
      "Batch 176, Loss: 0.843520, Accuracy: 85.96%\n",
      "Batch 177, Loss: 0.872919, Accuracy: 85.96%\n",
      "Batch 178, Loss: 0.847813, Accuracy: 85.98%\n",
      "Batch 179, Loss: 0.881158, Accuracy: 85.98%\n",
      "Batch 180, Loss: 0.836443, Accuracy: 86.01%\n",
      "Batch 181, Loss: 0.895106, Accuracy: 86.00%\n",
      "Batch 182, Loss: 0.848317, Accuracy: 86.03%\n",
      "Batch 183, Loss: 0.811637, Accuracy: 86.08%\n",
      "Batch 184, Loss: 0.858623, Accuracy: 86.10%\n",
      "Batch 185, Loss: 0.885731, Accuracy: 86.10%\n",
      "Batch 186, Loss: 0.965484, Accuracy: 86.05%\n",
      "Batch 187, Loss: 0.881347, Accuracy: 86.05%\n",
      "Batch 188, Loss: 0.952631, Accuracy: 86.00%\n",
      "Batch 189, Loss: 0.895766, Accuracy: 86.00%\n",
      "Batch 190, Loss: 0.886933, Accuracy: 86.00%\n",
      "Batch 191, Loss: 0.867240, Accuracy: 86.01%\n",
      "Batch 192, Loss: 0.832157, Accuracy: 86.04%\n",
      "Batch 193, Loss: 0.872159, Accuracy: 86.05%\n",
      "Batch 194, Loss: 0.871214, Accuracy: 86.06%\n",
      "Batch 195, Loss: 0.849698, Accuracy: 86.09%\n",
      "Batch 196, Loss: 0.867802, Accuracy: 86.10%\n",
      "Batch 197, Loss: 0.897559, Accuracy: 86.08%\n",
      "Batch 198, Loss: 0.834988, Accuracy: 86.11%\n",
      "Batch 199, Loss: 0.920235, Accuracy: 86.09%\n",
      "Batch 200, Loss: 0.945501, Accuracy: 86.06%\n",
      "Batch 201, Loss: 0.859123, Accuracy: 86.08%\n",
      "Batch 202, Loss: 0.932659, Accuracy: 86.05%\n",
      "Batch 203, Loss: 0.878273, Accuracy: 86.05%\n",
      "Batch 204, Loss: 0.930751, Accuracy: 86.04%\n",
      "Batch 205, Loss: 0.829902, Accuracy: 86.07%\n",
      "Batch 206, Loss: 0.904076, Accuracy: 86.05%\n",
      "Batch 207, Loss: 0.922759, Accuracy: 86.03%\n",
      "Batch 208, Loss: 0.849842, Accuracy: 86.04%\n",
      "Batch 209, Loss: 0.919947, Accuracy: 86.02%\n",
      "Batch 210, Loss: 0.918310, Accuracy: 86.00%\n",
      "Batch 211, Loss: 0.934103, Accuracy: 85.98%\n",
      "Batch 212, Loss: 0.921196, Accuracy: 85.96%\n",
      "Batch 213, Loss: 0.871190, Accuracy: 85.96%\n",
      "Training - Epoch 70, Loss: 0.885695, Accuracy: 85.96%\n",
      "Validation Batch 1, Loss: 0.878546, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.875920, Accuracy: 87.50%\n",
      "Validation Batch 3, Loss: 0.950677, Accuracy: 84.90%\n",
      "Validation Batch 4, Loss: 0.897452, Accuracy: 85.16%\n",
      "Validation Batch 5, Loss: 0.889369, Accuracy: 85.62%\n",
      "Validation Batch 6, Loss: 0.848329, Accuracy: 86.72%\n",
      "Validation Batch 7, Loss: 0.858959, Accuracy: 87.28%\n",
      "Validation Batch 8, Loss: 0.961597, Accuracy: 85.94%\n",
      "Validation Batch 9, Loss: 0.934047, Accuracy: 85.76%\n",
      "Validation Batch 10, Loss: 0.905774, Accuracy: 85.31%\n",
      "Validation Batch 11, Loss: 0.894265, Accuracy: 85.37%\n",
      "Validation Batch 12, Loss: 0.884656, Accuracy: 85.42%\n",
      "Validation Batch 13, Loss: 0.898750, Accuracy: 85.34%\n",
      "Validation Batch 14, Loss: 0.923564, Accuracy: 85.16%\n",
      "Validation Batch 15, Loss: 0.870455, Accuracy: 85.31%\n",
      "Validation Batch 16, Loss: 0.899175, Accuracy: 85.25%\n",
      "Validation Batch 17, Loss: 0.935971, Accuracy: 85.02%\n",
      "Validation Batch 18, Loss: 0.876040, Accuracy: 85.16%\n",
      "Validation Batch 19, Loss: 0.939260, Accuracy: 84.95%\n",
      "Validation Batch 20, Loss: 0.980761, Accuracy: 84.45%\n",
      "Validation Batch 21, Loss: 0.928082, Accuracy: 84.38%\n",
      "Validation Batch 22, Loss: 0.907043, Accuracy: 84.45%\n",
      "Validation Batch 23, Loss: 0.947564, Accuracy: 84.10%\n",
      "Validation Batch 24, Loss: 0.927524, Accuracy: 84.05%\n",
      "Validation Batch 25, Loss: 0.870180, Accuracy: 84.25%\n",
      "Validation Batch 26, Loss: 0.907520, Accuracy: 84.25%\n",
      "Validation Batch 27, Loss: 0.899825, Accuracy: 84.26%\n",
      "Validation - Epoch 70, Loss: 0.907085, Accuracy: 84.26%\n",
      "Patience—2\n",
      "Epoch 71\n",
      "Batch 1, Loss: 0.881305, Accuracy: 87.50%\n",
      "Batch 2, Loss: 0.906569, Accuracy: 86.72%\n",
      "Batch 3, Loss: 0.828080, Accuracy: 89.06%\n",
      "Batch 4, Loss: 0.899798, Accuracy: 88.28%\n",
      "Batch 5, Loss: 0.850729, Accuracy: 88.75%\n",
      "Batch 6, Loss: 0.863540, Accuracy: 88.54%\n",
      "Batch 7, Loss: 0.858464, Accuracy: 88.39%\n",
      "Batch 8, Loss: 0.859897, Accuracy: 88.67%\n",
      "Batch 9, Loss: 0.872594, Accuracy: 88.54%\n",
      "Batch 10, Loss: 0.919615, Accuracy: 87.97%\n",
      "Batch 11, Loss: 0.851171, Accuracy: 88.21%\n",
      "Batch 12, Loss: 0.881084, Accuracy: 88.15%\n",
      "Batch 13, Loss: 0.916589, Accuracy: 87.74%\n",
      "Batch 14, Loss: 0.838302, Accuracy: 88.06%\n",
      "Batch 15, Loss: 0.846529, Accuracy: 88.12%\n",
      "Batch 16, Loss: 0.853681, Accuracy: 88.28%\n",
      "Batch 17, Loss: 0.939709, Accuracy: 87.78%\n",
      "Batch 18, Loss: 0.868630, Accuracy: 87.76%\n",
      "Batch 19, Loss: 0.953698, Accuracy: 87.34%\n",
      "Batch 20, Loss: 0.859896, Accuracy: 87.42%\n",
      "Batch 21, Loss: 0.909052, Accuracy: 87.20%\n",
      "Batch 22, Loss: 0.911683, Accuracy: 87.07%\n",
      "Batch 23, Loss: 0.929525, Accuracy: 86.82%\n",
      "Batch 24, Loss: 0.896776, Accuracy: 86.72%\n",
      "Batch 25, Loss: 0.937822, Accuracy: 86.38%\n",
      "Batch 26, Loss: 0.900716, Accuracy: 86.30%\n",
      "Batch 27, Loss: 0.839509, Accuracy: 86.46%\n",
      "Batch 28, Loss: 0.948320, Accuracy: 86.27%\n",
      "Batch 29, Loss: 0.873010, Accuracy: 86.31%\n",
      "Batch 30, Loss: 0.916217, Accuracy: 86.15%\n",
      "Batch 31, Loss: 0.827450, Accuracy: 86.34%\n",
      "Batch 32, Loss: 0.879734, Accuracy: 86.38%\n",
      "Batch 33, Loss: 0.975142, Accuracy: 86.08%\n",
      "Batch 34, Loss: 0.906413, Accuracy: 85.98%\n",
      "Batch 35, Loss: 0.886325, Accuracy: 85.98%\n",
      "Batch 36, Loss: 0.894918, Accuracy: 85.98%\n",
      "Batch 37, Loss: 0.847843, Accuracy: 86.06%\n",
      "Batch 38, Loss: 0.895025, Accuracy: 86.06%\n",
      "Batch 39, Loss: 0.912417, Accuracy: 85.98%\n",
      "Batch 40, Loss: 0.923943, Accuracy: 85.86%\n",
      "Batch 41, Loss: 0.850989, Accuracy: 85.98%\n",
      "Batch 42, Loss: 0.875104, Accuracy: 86.01%\n",
      "Batch 43, Loss: 0.895002, Accuracy: 86.01%\n",
      "Batch 44, Loss: 0.919362, Accuracy: 85.94%\n",
      "Batch 45, Loss: 0.837613, Accuracy: 86.08%\n",
      "Batch 46, Loss: 1.005156, Accuracy: 85.84%\n",
      "Batch 47, Loss: 0.887168, Accuracy: 85.84%\n",
      "Batch 48, Loss: 0.841188, Accuracy: 85.94%\n",
      "Batch 49, Loss: 0.865907, Accuracy: 86.00%\n",
      "Batch 50, Loss: 0.838858, Accuracy: 86.12%\n",
      "Batch 51, Loss: 0.880280, Accuracy: 86.09%\n",
      "Batch 52, Loss: 0.895026, Accuracy: 86.09%\n",
      "Batch 53, Loss: 0.917107, Accuracy: 86.00%\n",
      "Batch 54, Loss: 0.810464, Accuracy: 86.14%\n",
      "Batch 55, Loss: 0.884135, Accuracy: 86.16%\n",
      "Batch 56, Loss: 0.903710, Accuracy: 86.10%\n",
      "Batch 57, Loss: 0.912183, Accuracy: 86.05%\n",
      "Batch 58, Loss: 0.901624, Accuracy: 86.02%\n",
      "Batch 59, Loss: 0.909130, Accuracy: 85.96%\n",
      "Batch 60, Loss: 0.882562, Accuracy: 85.99%\n",
      "Batch 61, Loss: 0.829886, Accuracy: 86.07%\n",
      "Batch 62, Loss: 0.925984, Accuracy: 85.99%\n",
      "Batch 63, Loss: 0.896332, Accuracy: 85.96%\n",
      "Batch 64, Loss: 0.893097, Accuracy: 85.96%\n",
      "Batch 65, Loss: 0.943031, Accuracy: 85.89%\n",
      "Batch 66, Loss: 0.931861, Accuracy: 85.80%\n",
      "Batch 67, Loss: 0.905895, Accuracy: 85.77%\n",
      "Batch 68, Loss: 0.984504, Accuracy: 85.62%\n",
      "Batch 69, Loss: 0.914346, Accuracy: 85.55%\n",
      "Batch 70, Loss: 0.860379, Accuracy: 85.60%\n",
      "Batch 71, Loss: 0.938376, Accuracy: 85.52%\n",
      "Batch 72, Loss: 0.891509, Accuracy: 85.53%\n",
      "Batch 73, Loss: 0.861751, Accuracy: 85.55%\n",
      "Batch 74, Loss: 0.920556, Accuracy: 85.49%\n",
      "Batch 75, Loss: 0.841814, Accuracy: 85.54%\n",
      "Batch 76, Loss: 0.813575, Accuracy: 85.65%\n",
      "Batch 77, Loss: 0.874608, Accuracy: 85.65%\n",
      "Batch 78, Loss: 0.857430, Accuracy: 85.68%\n",
      "Batch 79, Loss: 0.867406, Accuracy: 85.72%\n",
      "Batch 80, Loss: 0.851512, Accuracy: 85.76%\n",
      "Batch 81, Loss: 0.780374, Accuracy: 85.90%\n",
      "Batch 82, Loss: 0.966742, Accuracy: 85.79%\n",
      "Batch 83, Loss: 0.904909, Accuracy: 85.75%\n",
      "Batch 84, Loss: 0.938149, Accuracy: 85.70%\n",
      "Batch 85, Loss: 0.801072, Accuracy: 85.81%\n",
      "Batch 86, Loss: 0.916383, Accuracy: 85.79%\n",
      "Batch 87, Loss: 0.879810, Accuracy: 85.79%\n",
      "Batch 88, Loss: 0.837912, Accuracy: 85.85%\n",
      "Batch 89, Loss: 0.864903, Accuracy: 85.88%\n",
      "Batch 90, Loss: 0.895961, Accuracy: 85.85%\n",
      "Batch 91, Loss: 0.910470, Accuracy: 85.85%\n",
      "Batch 92, Loss: 0.900679, Accuracy: 85.84%\n",
      "Batch 93, Loss: 0.882657, Accuracy: 85.84%\n",
      "Batch 94, Loss: 0.827528, Accuracy: 85.90%\n",
      "Batch 95, Loss: 0.966317, Accuracy: 85.81%\n",
      "Batch 96, Loss: 0.808270, Accuracy: 85.89%\n",
      "Batch 97, Loss: 0.884356, Accuracy: 85.89%\n",
      "Batch 98, Loss: 0.913092, Accuracy: 85.86%\n",
      "Batch 99, Loss: 0.885137, Accuracy: 85.87%\n",
      "Batch 100, Loss: 0.852537, Accuracy: 85.89%\n",
      "Batch 101, Loss: 0.951897, Accuracy: 85.83%\n",
      "Batch 102, Loss: 0.847826, Accuracy: 85.88%\n",
      "Batch 103, Loss: 0.865554, Accuracy: 85.88%\n",
      "Batch 104, Loss: 0.937207, Accuracy: 85.80%\n",
      "Batch 105, Loss: 0.974313, Accuracy: 85.71%\n",
      "Batch 106, Loss: 0.856386, Accuracy: 85.76%\n",
      "Batch 107, Loss: 0.912092, Accuracy: 85.75%\n",
      "Batch 108, Loss: 0.906049, Accuracy: 85.72%\n",
      "Batch 109, Loss: 0.868765, Accuracy: 85.75%\n",
      "Batch 110, Loss: 0.897803, Accuracy: 85.72%\n",
      "Batch 111, Loss: 0.856883, Accuracy: 85.75%\n",
      "Batch 112, Loss: 0.849258, Accuracy: 85.77%\n",
      "Batch 113, Loss: 0.953734, Accuracy: 85.70%\n",
      "Batch 114, Loss: 0.865856, Accuracy: 85.72%\n",
      "Batch 115, Loss: 0.894269, Accuracy: 85.72%\n",
      "Batch 116, Loss: 0.958356, Accuracy: 85.67%\n",
      "Batch 117, Loss: 0.893634, Accuracy: 85.66%\n",
      "Batch 118, Loss: 0.864944, Accuracy: 85.69%\n",
      "Batch 119, Loss: 0.884164, Accuracy: 85.69%\n",
      "Batch 120, Loss: 0.905070, Accuracy: 85.68%\n",
      "Batch 121, Loss: 0.844349, Accuracy: 85.73%\n",
      "Batch 122, Loss: 0.899445, Accuracy: 85.73%\n",
      "Batch 123, Loss: 0.924217, Accuracy: 85.71%\n",
      "Batch 124, Loss: 0.852029, Accuracy: 85.75%\n",
      "Batch 125, Loss: 0.866061, Accuracy: 85.78%\n",
      "Batch 126, Loss: 0.936831, Accuracy: 85.73%\n",
      "Batch 127, Loss: 0.878307, Accuracy: 85.73%\n",
      "Batch 128, Loss: 0.821205, Accuracy: 85.79%\n",
      "Batch 129, Loss: 0.904728, Accuracy: 85.77%\n",
      "Batch 130, Loss: 0.829941, Accuracy: 85.82%\n",
      "Batch 131, Loss: 0.915231, Accuracy: 85.79%\n",
      "Batch 132, Loss: 0.916531, Accuracy: 85.78%\n",
      "Batch 133, Loss: 0.927062, Accuracy: 85.76%\n",
      "Batch 134, Loss: 0.852403, Accuracy: 85.80%\n",
      "Batch 135, Loss: 0.827236, Accuracy: 85.84%\n",
      "Batch 136, Loss: 0.898485, Accuracy: 85.82%\n",
      "Batch 137, Loss: 0.869808, Accuracy: 85.83%\n",
      "Batch 138, Loss: 0.919791, Accuracy: 85.81%\n",
      "Batch 139, Loss: 0.879972, Accuracy: 85.81%\n",
      "Batch 140, Loss: 0.902696, Accuracy: 85.79%\n",
      "Batch 141, Loss: 0.857872, Accuracy: 85.80%\n",
      "Batch 142, Loss: 0.858498, Accuracy: 85.82%\n",
      "Batch 143, Loss: 0.902798, Accuracy: 85.81%\n",
      "Batch 144, Loss: 0.906477, Accuracy: 85.79%\n",
      "Batch 145, Loss: 0.828856, Accuracy: 85.82%\n",
      "Batch 146, Loss: 0.883745, Accuracy: 85.82%\n",
      "Batch 147, Loss: 0.814577, Accuracy: 85.87%\n",
      "Batch 148, Loss: 0.921161, Accuracy: 85.84%\n",
      "Batch 149, Loss: 0.879152, Accuracy: 85.84%\n",
      "Batch 150, Loss: 0.851242, Accuracy: 85.88%\n",
      "Batch 151, Loss: 0.849163, Accuracy: 85.90%\n",
      "Batch 152, Loss: 0.855652, Accuracy: 85.91%\n",
      "Batch 153, Loss: 0.884535, Accuracy: 85.90%\n",
      "Batch 154, Loss: 0.814370, Accuracy: 85.97%\n",
      "Batch 155, Loss: 0.842438, Accuracy: 86.00%\n",
      "Batch 156, Loss: 0.866418, Accuracy: 86.02%\n",
      "Batch 157, Loss: 0.898018, Accuracy: 86.01%\n",
      "Batch 158, Loss: 0.912680, Accuracy: 85.98%\n",
      "Batch 159, Loss: 0.872905, Accuracy: 86.00%\n",
      "Batch 160, Loss: 0.838691, Accuracy: 86.03%\n",
      "Batch 161, Loss: 0.923122, Accuracy: 86.01%\n",
      "Batch 162, Loss: 0.885409, Accuracy: 86.01%\n",
      "Batch 163, Loss: 0.932351, Accuracy: 85.99%\n",
      "Batch 164, Loss: 0.868723, Accuracy: 85.99%\n",
      "Batch 165, Loss: 0.844522, Accuracy: 86.03%\n",
      "Batch 166, Loss: 0.903157, Accuracy: 86.02%\n",
      "Batch 167, Loss: 0.797267, Accuracy: 86.09%\n",
      "Batch 168, Loss: 0.905924, Accuracy: 86.06%\n",
      "Batch 169, Loss: 0.960940, Accuracy: 85.99%\n",
      "Batch 170, Loss: 0.850140, Accuracy: 86.02%\n",
      "Batch 171, Loss: 0.906496, Accuracy: 86.01%\n",
      "Batch 172, Loss: 0.889598, Accuracy: 86.01%\n",
      "Batch 173, Loss: 0.857656, Accuracy: 86.03%\n",
      "Batch 174, Loss: 0.820513, Accuracy: 86.06%\n",
      "Batch 175, Loss: 0.839742, Accuracy: 86.09%\n",
      "Batch 176, Loss: 0.956139, Accuracy: 86.05%\n",
      "Batch 177, Loss: 0.911713, Accuracy: 86.03%\n",
      "Batch 178, Loss: 0.945641, Accuracy: 85.98%\n",
      "Batch 179, Loss: 0.903923, Accuracy: 85.97%\n",
      "Batch 180, Loss: 0.889408, Accuracy: 85.97%\n",
      "Batch 181, Loss: 0.881006, Accuracy: 85.96%\n",
      "Batch 182, Loss: 0.914801, Accuracy: 85.95%\n",
      "Batch 183, Loss: 0.850622, Accuracy: 85.96%\n",
      "Batch 184, Loss: 0.933131, Accuracy: 85.94%\n",
      "Batch 185, Loss: 0.892400, Accuracy: 85.94%\n",
      "Batch 186, Loss: 0.929913, Accuracy: 85.91%\n",
      "Batch 187, Loss: 0.871272, Accuracy: 85.91%\n",
      "Batch 188, Loss: 0.876761, Accuracy: 85.91%\n",
      "Batch 189, Loss: 0.931678, Accuracy: 85.90%\n",
      "Batch 190, Loss: 0.862394, Accuracy: 85.91%\n",
      "Batch 191, Loss: 0.938149, Accuracy: 85.89%\n",
      "Batch 192, Loss: 0.880841, Accuracy: 85.90%\n",
      "Batch 193, Loss: 0.885907, Accuracy: 85.91%\n",
      "Batch 194, Loss: 0.907417, Accuracy: 85.90%\n",
      "Batch 195, Loss: 0.881199, Accuracy: 85.88%\n",
      "Batch 196, Loss: 0.918085, Accuracy: 85.87%\n",
      "Batch 197, Loss: 0.853444, Accuracy: 85.88%\n",
      "Batch 198, Loss: 0.874590, Accuracy: 85.90%\n",
      "Batch 199, Loss: 0.893679, Accuracy: 85.89%\n",
      "Batch 200, Loss: 0.840045, Accuracy: 85.91%\n",
      "Batch 201, Loss: 0.910886, Accuracy: 85.89%\n",
      "Batch 202, Loss: 0.870520, Accuracy: 85.91%\n",
      "Batch 203, Loss: 0.856834, Accuracy: 85.92%\n",
      "Batch 204, Loss: 0.889877, Accuracy: 85.91%\n",
      "Batch 205, Loss: 0.871773, Accuracy: 85.91%\n",
      "Batch 206, Loss: 0.896124, Accuracy: 85.91%\n",
      "Batch 207, Loss: 0.901333, Accuracy: 85.91%\n",
      "Batch 208, Loss: 0.915079, Accuracy: 85.88%\n",
      "Batch 209, Loss: 0.842705, Accuracy: 85.89%\n",
      "Batch 210, Loss: 0.888844, Accuracy: 85.89%\n",
      "Batch 211, Loss: 0.847967, Accuracy: 85.92%\n",
      "Batch 212, Loss: 0.978994, Accuracy: 85.87%\n",
      "Batch 213, Loss: 0.839045, Accuracy: 85.89%\n",
      "Training - Epoch 71, Loss: 0.886219, Accuracy: 85.89%\n",
      "Validation Batch 1, Loss: 0.860409, Accuracy: 90.62%\n",
      "Validation Batch 2, Loss: 0.853245, Accuracy: 91.41%\n",
      "Validation Batch 3, Loss: 0.931920, Accuracy: 88.02%\n",
      "Validation Batch 4, Loss: 0.877694, Accuracy: 88.28%\n",
      "Validation Batch 5, Loss: 0.866979, Accuracy: 88.44%\n",
      "Validation Batch 6, Loss: 0.832377, Accuracy: 89.32%\n",
      "Validation Batch 7, Loss: 0.844229, Accuracy: 89.51%\n",
      "Validation Batch 8, Loss: 0.948559, Accuracy: 87.89%\n",
      "Validation Batch 9, Loss: 0.918512, Accuracy: 87.50%\n",
      "Validation Batch 10, Loss: 0.889985, Accuracy: 87.34%\n",
      "Validation Batch 11, Loss: 0.873054, Accuracy: 87.36%\n",
      "Validation Batch 12, Loss: 0.874948, Accuracy: 87.50%\n",
      "Validation Batch 13, Loss: 0.885702, Accuracy: 87.38%\n",
      "Validation Batch 14, Loss: 0.909339, Accuracy: 87.05%\n",
      "Validation Batch 15, Loss: 0.861948, Accuracy: 87.08%\n",
      "Validation Batch 16, Loss: 0.883328, Accuracy: 87.21%\n",
      "Validation Batch 17, Loss: 0.921827, Accuracy: 86.95%\n",
      "Validation Batch 18, Loss: 0.864019, Accuracy: 86.98%\n",
      "Validation Batch 19, Loss: 0.923057, Accuracy: 86.84%\n",
      "Validation Batch 20, Loss: 0.952880, Accuracy: 86.41%\n",
      "Validation Batch 21, Loss: 0.909353, Accuracy: 86.24%\n",
      "Validation Batch 22, Loss: 0.889031, Accuracy: 86.22%\n",
      "Validation Batch 23, Loss: 0.923052, Accuracy: 86.14%\n",
      "Validation Batch 24, Loss: 0.910106, Accuracy: 86.13%\n",
      "Validation Batch 25, Loss: 0.853538, Accuracy: 86.31%\n",
      "Validation Batch 26, Loss: 0.886758, Accuracy: 86.36%\n",
      "Validation Batch 27, Loss: 0.866956, Accuracy: 86.38%\n",
      "Validation - Epoch 71, Loss: 0.889363, Accuracy: 86.38%\n",
      "Patience—3\n",
      "Epoch 72\n",
      "Batch 1, Loss: 0.906149, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.841245, Accuracy: 86.72%\n",
      "Batch 3, Loss: 0.853733, Accuracy: 87.50%\n",
      "Batch 4, Loss: 0.903554, Accuracy: 86.33%\n",
      "Batch 5, Loss: 0.958971, Accuracy: 84.69%\n",
      "Batch 6, Loss: 0.872411, Accuracy: 84.90%\n",
      "Batch 7, Loss: 0.871485, Accuracy: 85.04%\n",
      "Batch 8, Loss: 0.922789, Accuracy: 84.57%\n",
      "Batch 9, Loss: 0.925783, Accuracy: 84.03%\n",
      "Batch 10, Loss: 0.888984, Accuracy: 84.06%\n",
      "Batch 11, Loss: 0.958268, Accuracy: 83.38%\n",
      "Batch 12, Loss: 0.857713, Accuracy: 83.85%\n",
      "Batch 13, Loss: 0.884656, Accuracy: 84.13%\n",
      "Batch 14, Loss: 0.877395, Accuracy: 84.26%\n",
      "Batch 15, Loss: 0.830378, Accuracy: 84.90%\n",
      "Batch 16, Loss: 0.863179, Accuracy: 85.06%\n",
      "Batch 17, Loss: 0.810086, Accuracy: 85.57%\n",
      "Batch 18, Loss: 0.887450, Accuracy: 85.59%\n",
      "Batch 19, Loss: 0.925499, Accuracy: 85.44%\n",
      "Batch 20, Loss: 0.908709, Accuracy: 85.39%\n",
      "Batch 21, Loss: 0.852544, Accuracy: 85.57%\n",
      "Batch 22, Loss: 0.834887, Accuracy: 85.87%\n",
      "Batch 23, Loss: 0.870498, Accuracy: 85.94%\n",
      "Batch 24, Loss: 0.892804, Accuracy: 85.87%\n",
      "Batch 25, Loss: 0.973157, Accuracy: 85.56%\n",
      "Batch 26, Loss: 0.937932, Accuracy: 85.46%\n",
      "Batch 27, Loss: 0.843692, Accuracy: 85.65%\n",
      "Batch 28, Loss: 0.950553, Accuracy: 85.38%\n",
      "Batch 29, Loss: 0.835572, Accuracy: 85.61%\n",
      "Batch 30, Loss: 0.923440, Accuracy: 85.47%\n",
      "Batch 31, Loss: 0.874164, Accuracy: 85.58%\n",
      "Batch 32, Loss: 0.913382, Accuracy: 85.55%\n",
      "Batch 33, Loss: 0.962254, Accuracy: 85.32%\n",
      "Batch 34, Loss: 0.888050, Accuracy: 85.34%\n",
      "Batch 35, Loss: 0.880946, Accuracy: 85.36%\n",
      "Batch 36, Loss: 0.903929, Accuracy: 85.33%\n",
      "Batch 37, Loss: 0.902819, Accuracy: 85.30%\n",
      "Batch 38, Loss: 0.833278, Accuracy: 85.49%\n",
      "Batch 39, Loss: 0.941841, Accuracy: 85.34%\n",
      "Batch 40, Loss: 0.886092, Accuracy: 85.39%\n",
      "Batch 41, Loss: 0.843466, Accuracy: 85.52%\n",
      "Batch 42, Loss: 0.932551, Accuracy: 85.42%\n",
      "Batch 43, Loss: 0.898464, Accuracy: 85.43%\n",
      "Batch 44, Loss: 0.946090, Accuracy: 85.30%\n",
      "Batch 45, Loss: 0.823091, Accuracy: 85.49%\n",
      "Batch 46, Loss: 0.909925, Accuracy: 85.43%\n",
      "Batch 47, Loss: 0.851023, Accuracy: 85.51%\n",
      "Batch 48, Loss: 0.912616, Accuracy: 85.48%\n",
      "Batch 49, Loss: 0.865460, Accuracy: 85.52%\n",
      "Batch 50, Loss: 0.843880, Accuracy: 85.66%\n",
      "Batch 51, Loss: 0.877862, Accuracy: 85.69%\n",
      "Batch 52, Loss: 0.874255, Accuracy: 85.73%\n",
      "Batch 53, Loss: 0.878082, Accuracy: 85.76%\n",
      "Batch 54, Loss: 0.861684, Accuracy: 85.82%\n",
      "Batch 55, Loss: 0.931506, Accuracy: 85.74%\n",
      "Batch 56, Loss: 0.891961, Accuracy: 85.71%\n",
      "Batch 57, Loss: 0.941748, Accuracy: 85.61%\n",
      "Batch 58, Loss: 0.933285, Accuracy: 85.51%\n",
      "Batch 59, Loss: 0.951926, Accuracy: 85.41%\n",
      "Batch 60, Loss: 0.852488, Accuracy: 85.47%\n",
      "Batch 61, Loss: 0.794947, Accuracy: 85.63%\n",
      "Batch 62, Loss: 0.889655, Accuracy: 85.61%\n",
      "Batch 63, Loss: 0.902328, Accuracy: 85.59%\n",
      "Batch 64, Loss: 0.868350, Accuracy: 85.62%\n",
      "Batch 65, Loss: 0.884912, Accuracy: 85.65%\n",
      "Batch 66, Loss: 0.841640, Accuracy: 85.68%\n",
      "Batch 67, Loss: 0.899743, Accuracy: 85.66%\n",
      "Batch 68, Loss: 0.855450, Accuracy: 85.71%\n",
      "Batch 69, Loss: 0.850166, Accuracy: 85.78%\n",
      "Batch 70, Loss: 0.854081, Accuracy: 85.83%\n",
      "Batch 71, Loss: 0.927592, Accuracy: 85.74%\n",
      "Batch 72, Loss: 0.877879, Accuracy: 85.76%\n",
      "Batch 73, Loss: 1.005078, Accuracy: 85.62%\n",
      "Batch 74, Loss: 0.866077, Accuracy: 85.64%\n",
      "Batch 75, Loss: 0.886558, Accuracy: 85.62%\n",
      "Batch 76, Loss: 0.914583, Accuracy: 85.59%\n",
      "Batch 77, Loss: 0.899662, Accuracy: 85.57%\n",
      "Batch 78, Loss: 0.893797, Accuracy: 85.56%\n",
      "Batch 79, Loss: 0.934065, Accuracy: 85.52%\n",
      "Batch 80, Loss: 0.919432, Accuracy: 85.49%\n",
      "Batch 81, Loss: 0.866263, Accuracy: 85.53%\n",
      "Batch 82, Loss: 0.863089, Accuracy: 85.54%\n",
      "Batch 83, Loss: 0.789597, Accuracy: 85.66%\n",
      "Batch 84, Loss: 0.941528, Accuracy: 85.60%\n",
      "Batch 85, Loss: 0.836667, Accuracy: 85.66%\n",
      "Batch 86, Loss: 0.900908, Accuracy: 85.63%\n",
      "Batch 87, Loss: 0.961086, Accuracy: 85.54%\n",
      "Batch 88, Loss: 0.888432, Accuracy: 85.55%\n",
      "Batch 89, Loss: 0.865836, Accuracy: 85.60%\n",
      "Batch 90, Loss: 0.890240, Accuracy: 85.59%\n",
      "Batch 91, Loss: 0.894654, Accuracy: 85.56%\n",
      "Batch 92, Loss: 0.880805, Accuracy: 85.56%\n",
      "Batch 93, Loss: 0.880835, Accuracy: 85.57%\n",
      "Batch 94, Loss: 0.848991, Accuracy: 85.59%\n",
      "Batch 95, Loss: 0.903561, Accuracy: 85.56%\n",
      "Batch 96, Loss: 0.820318, Accuracy: 85.64%\n",
      "Batch 97, Loss: 0.977590, Accuracy: 85.55%\n",
      "Batch 98, Loss: 0.791525, Accuracy: 85.63%\n",
      "Batch 99, Loss: 0.925971, Accuracy: 85.59%\n",
      "Batch 100, Loss: 0.950446, Accuracy: 85.53%\n",
      "Batch 101, Loss: 0.988288, Accuracy: 85.41%\n",
      "Batch 102, Loss: 0.956593, Accuracy: 85.34%\n",
      "Batch 103, Loss: 0.879128, Accuracy: 85.36%\n",
      "Batch 104, Loss: 0.928129, Accuracy: 85.32%\n",
      "Batch 105, Loss: 0.863946, Accuracy: 85.34%\n",
      "Batch 106, Loss: 0.862947, Accuracy: 85.38%\n",
      "Batch 107, Loss: 0.860153, Accuracy: 85.40%\n",
      "Batch 108, Loss: 0.893258, Accuracy: 85.40%\n",
      "Batch 109, Loss: 0.940017, Accuracy: 85.34%\n",
      "Batch 110, Loss: 0.884196, Accuracy: 85.34%\n",
      "Batch 111, Loss: 0.861387, Accuracy: 85.37%\n",
      "Batch 112, Loss: 0.834495, Accuracy: 85.42%\n",
      "Batch 113, Loss: 0.861910, Accuracy: 85.43%\n",
      "Batch 114, Loss: 0.896895, Accuracy: 85.42%\n",
      "Batch 115, Loss: 0.896349, Accuracy: 85.41%\n",
      "Batch 116, Loss: 0.863804, Accuracy: 85.45%\n",
      "Batch 117, Loss: 0.982600, Accuracy: 85.36%\n",
      "Batch 118, Loss: 0.863515, Accuracy: 85.38%\n",
      "Batch 119, Loss: 0.896010, Accuracy: 85.36%\n",
      "Batch 120, Loss: 0.874018, Accuracy: 85.38%\n",
      "Batch 121, Loss: 0.925076, Accuracy: 85.34%\n",
      "Batch 122, Loss: 0.843178, Accuracy: 85.40%\n",
      "Batch 123, Loss: 0.888663, Accuracy: 85.39%\n",
      "Batch 124, Loss: 0.932674, Accuracy: 85.37%\n",
      "Batch 125, Loss: 0.928227, Accuracy: 85.35%\n",
      "Batch 126, Loss: 0.912437, Accuracy: 85.34%\n",
      "Batch 127, Loss: 0.837969, Accuracy: 85.38%\n",
      "Batch 128, Loss: 0.964097, Accuracy: 85.33%\n",
      "Batch 129, Loss: 0.961355, Accuracy: 85.27%\n",
      "Batch 130, Loss: 0.871000, Accuracy: 85.28%\n",
      "Batch 131, Loss: 0.890228, Accuracy: 85.29%\n",
      "Batch 132, Loss: 0.994288, Accuracy: 85.22%\n",
      "Batch 133, Loss: 0.919986, Accuracy: 85.22%\n",
      "Batch 134, Loss: 0.911803, Accuracy: 85.20%\n",
      "Batch 135, Loss: 0.827133, Accuracy: 85.25%\n",
      "Batch 136, Loss: 0.843771, Accuracy: 85.28%\n",
      "Batch 137, Loss: 0.825133, Accuracy: 85.33%\n",
      "Batch 138, Loss: 0.875920, Accuracy: 85.33%\n",
      "Batch 139, Loss: 0.796428, Accuracy: 85.41%\n",
      "Batch 140, Loss: 0.894015, Accuracy: 85.40%\n",
      "Batch 141, Loss: 0.949160, Accuracy: 85.35%\n",
      "Batch 142, Loss: 0.879194, Accuracy: 85.37%\n",
      "Batch 143, Loss: 0.852620, Accuracy: 85.39%\n",
      "Batch 144, Loss: 0.867092, Accuracy: 85.41%\n",
      "Batch 145, Loss: 0.941562, Accuracy: 85.38%\n",
      "Batch 146, Loss: 0.893474, Accuracy: 85.36%\n",
      "Batch 147, Loss: 0.852643, Accuracy: 85.40%\n",
      "Batch 148, Loss: 0.815837, Accuracy: 85.45%\n",
      "Batch 149, Loss: 0.821283, Accuracy: 85.50%\n",
      "Batch 150, Loss: 0.858744, Accuracy: 85.51%\n",
      "Batch 151, Loss: 0.892425, Accuracy: 85.51%\n",
      "Batch 152, Loss: 0.855376, Accuracy: 85.53%\n",
      "Batch 153, Loss: 0.859653, Accuracy: 85.55%\n",
      "Batch 154, Loss: 0.931090, Accuracy: 85.52%\n",
      "Batch 155, Loss: 0.908792, Accuracy: 85.49%\n",
      "Batch 156, Loss: 0.822066, Accuracy: 85.54%\n",
      "Batch 157, Loss: 0.835148, Accuracy: 85.58%\n",
      "Batch 158, Loss: 0.911184, Accuracy: 85.56%\n",
      "Batch 159, Loss: 0.873249, Accuracy: 85.56%\n",
      "Batch 160, Loss: 0.881709, Accuracy: 85.58%\n",
      "Batch 161, Loss: 0.920604, Accuracy: 85.55%\n",
      "Batch 162, Loss: 0.875132, Accuracy: 85.57%\n",
      "Batch 163, Loss: 0.946011, Accuracy: 85.53%\n",
      "Batch 164, Loss: 0.882780, Accuracy: 85.55%\n",
      "Batch 165, Loss: 0.816165, Accuracy: 85.59%\n",
      "Batch 166, Loss: 0.902007, Accuracy: 85.58%\n",
      "Batch 167, Loss: 0.916530, Accuracy: 85.55%\n",
      "Batch 168, Loss: 0.893521, Accuracy: 85.55%\n",
      "Batch 169, Loss: 0.879191, Accuracy: 85.56%\n",
      "Batch 170, Loss: 0.877242, Accuracy: 85.57%\n",
      "Batch 171, Loss: 0.892786, Accuracy: 85.57%\n",
      "Batch 172, Loss: 0.852986, Accuracy: 85.59%\n",
      "Batch 173, Loss: 0.896685, Accuracy: 85.59%\n",
      "Batch 174, Loss: 0.888389, Accuracy: 85.58%\n",
      "Batch 175, Loss: 0.855884, Accuracy: 85.60%\n",
      "Batch 176, Loss: 0.837548, Accuracy: 85.63%\n",
      "Batch 177, Loss: 0.871759, Accuracy: 85.64%\n",
      "Batch 178, Loss: 0.858726, Accuracy: 85.67%\n",
      "Batch 179, Loss: 0.937400, Accuracy: 85.64%\n",
      "Batch 180, Loss: 0.869437, Accuracy: 85.65%\n",
      "Batch 181, Loss: 0.904349, Accuracy: 85.64%\n",
      "Batch 182, Loss: 0.943276, Accuracy: 85.59%\n",
      "Batch 183, Loss: 0.865844, Accuracy: 85.60%\n",
      "Batch 184, Loss: 0.897105, Accuracy: 85.59%\n",
      "Batch 185, Loss: 0.885504, Accuracy: 85.59%\n",
      "Batch 186, Loss: 0.869154, Accuracy: 85.59%\n",
      "Batch 187, Loss: 0.898784, Accuracy: 85.59%\n",
      "Batch 188, Loss: 0.877123, Accuracy: 85.61%\n",
      "Batch 189, Loss: 0.927587, Accuracy: 85.59%\n",
      "Batch 190, Loss: 0.940456, Accuracy: 85.57%\n",
      "Batch 191, Loss: 0.869990, Accuracy: 85.58%\n",
      "Batch 192, Loss: 0.923497, Accuracy: 85.56%\n",
      "Batch 193, Loss: 0.889524, Accuracy: 85.57%\n",
      "Batch 194, Loss: 0.821536, Accuracy: 85.60%\n",
      "Batch 195, Loss: 0.861896, Accuracy: 85.60%\n",
      "Batch 196, Loss: 0.941904, Accuracy: 85.58%\n",
      "Batch 197, Loss: 0.856406, Accuracy: 85.59%\n",
      "Batch 198, Loss: 0.891333, Accuracy: 85.58%\n",
      "Batch 199, Loss: 1.001194, Accuracy: 85.53%\n",
      "Batch 200, Loss: 0.846247, Accuracy: 85.56%\n",
      "Batch 201, Loss: 0.962950, Accuracy: 85.52%\n",
      "Batch 202, Loss: 0.858154, Accuracy: 85.54%\n",
      "Batch 203, Loss: 0.853739, Accuracy: 85.55%\n",
      "Batch 204, Loss: 0.858516, Accuracy: 85.57%\n",
      "Batch 205, Loss: 0.856395, Accuracy: 85.58%\n",
      "Batch 206, Loss: 0.932412, Accuracy: 85.55%\n",
      "Batch 207, Loss: 0.851468, Accuracy: 85.57%\n",
      "Batch 208, Loss: 0.902764, Accuracy: 85.56%\n",
      "Batch 209, Loss: 0.926306, Accuracy: 85.54%\n",
      "Batch 210, Loss: 0.869068, Accuracy: 85.56%\n",
      "Batch 211, Loss: 0.870717, Accuracy: 85.57%\n",
      "Batch 212, Loss: 0.872346, Accuracy: 85.57%\n",
      "Batch 213, Loss: 0.889906, Accuracy: 85.57%\n",
      "Training - Epoch 72, Loss: 0.888439, Accuracy: 85.57%\n",
      "Validation Batch 1, Loss: 0.854022, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.842253, Accuracy: 91.41%\n",
      "Validation Batch 3, Loss: 0.921226, Accuracy: 88.54%\n",
      "Validation Batch 4, Loss: 0.866791, Accuracy: 88.67%\n",
      "Validation Batch 5, Loss: 0.854577, Accuracy: 89.06%\n",
      "Validation Batch 6, Loss: 0.822434, Accuracy: 89.84%\n",
      "Validation Batch 7, Loss: 0.837236, Accuracy: 90.18%\n",
      "Validation Batch 8, Loss: 0.945321, Accuracy: 88.67%\n",
      "Validation Batch 9, Loss: 0.917378, Accuracy: 88.02%\n",
      "Validation Batch 10, Loss: 0.886917, Accuracy: 87.81%\n",
      "Validation Batch 11, Loss: 0.862090, Accuracy: 87.78%\n",
      "Validation Batch 12, Loss: 0.854437, Accuracy: 88.15%\n",
      "Validation Batch 13, Loss: 0.872934, Accuracy: 87.98%\n",
      "Validation Batch 14, Loss: 0.895116, Accuracy: 87.72%\n",
      "Validation Batch 15, Loss: 0.862142, Accuracy: 87.81%\n",
      "Validation Batch 16, Loss: 0.871537, Accuracy: 87.79%\n",
      "Validation Batch 17, Loss: 0.915255, Accuracy: 87.50%\n",
      "Validation Batch 18, Loss: 0.853667, Accuracy: 87.59%\n",
      "Validation Batch 19, Loss: 0.911263, Accuracy: 87.34%\n",
      "Validation Batch 20, Loss: 0.933835, Accuracy: 86.95%\n",
      "Validation Batch 21, Loss: 0.892128, Accuracy: 86.90%\n",
      "Validation Batch 22, Loss: 0.883108, Accuracy: 86.79%\n",
      "Validation Batch 23, Loss: 0.905679, Accuracy: 86.75%\n",
      "Validation Batch 24, Loss: 0.903883, Accuracy: 86.65%\n",
      "Validation Batch 25, Loss: 0.840085, Accuracy: 86.81%\n",
      "Validation Batch 26, Loss: 0.871536, Accuracy: 86.84%\n",
      "Validation Batch 27, Loss: 0.844693, Accuracy: 86.96%\n",
      "Validation - Epoch 72, Loss: 0.878576, Accuracy: 86.96%\n",
      "Patience—0\n",
      "Epoch 73\n",
      "Batch 1, Loss: 0.839544, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.972847, Accuracy: 82.81%\n",
      "Batch 3, Loss: 0.892113, Accuracy: 83.33%\n",
      "Batch 4, Loss: 0.888981, Accuracy: 83.98%\n",
      "Batch 5, Loss: 0.898470, Accuracy: 84.06%\n",
      "Batch 6, Loss: 0.909235, Accuracy: 83.85%\n",
      "Batch 7, Loss: 0.921227, Accuracy: 83.48%\n",
      "Batch 8, Loss: 0.968837, Accuracy: 83.01%\n",
      "Batch 9, Loss: 0.825168, Accuracy: 84.03%\n",
      "Batch 10, Loss: 0.953463, Accuracy: 83.28%\n",
      "Batch 11, Loss: 0.879565, Accuracy: 83.66%\n",
      "Batch 12, Loss: 0.903394, Accuracy: 83.85%\n",
      "Batch 13, Loss: 0.951084, Accuracy: 83.53%\n",
      "Batch 14, Loss: 0.895726, Accuracy: 83.71%\n",
      "Batch 15, Loss: 0.856483, Accuracy: 84.17%\n",
      "Batch 16, Loss: 0.904081, Accuracy: 84.08%\n",
      "Batch 17, Loss: 0.844624, Accuracy: 84.56%\n",
      "Batch 18, Loss: 0.848673, Accuracy: 84.98%\n",
      "Batch 19, Loss: 0.903360, Accuracy: 84.95%\n",
      "Batch 20, Loss: 0.844521, Accuracy: 85.23%\n",
      "Batch 21, Loss: 0.884785, Accuracy: 85.34%\n",
      "Batch 22, Loss: 0.839702, Accuracy: 85.58%\n",
      "Batch 23, Loss: 0.864529, Accuracy: 85.73%\n",
      "Batch 24, Loss: 0.936621, Accuracy: 85.61%\n",
      "Batch 25, Loss: 0.821065, Accuracy: 85.94%\n",
      "Batch 26, Loss: 0.819432, Accuracy: 86.24%\n",
      "Batch 27, Loss: 0.901827, Accuracy: 86.23%\n",
      "Batch 28, Loss: 0.925745, Accuracy: 86.05%\n",
      "Batch 29, Loss: 0.905903, Accuracy: 85.99%\n",
      "Batch 30, Loss: 0.860194, Accuracy: 86.04%\n",
      "Batch 31, Loss: 0.922214, Accuracy: 85.84%\n",
      "Batch 32, Loss: 0.881437, Accuracy: 85.79%\n",
      "Batch 33, Loss: 0.930521, Accuracy: 85.65%\n",
      "Batch 34, Loss: 0.915307, Accuracy: 85.62%\n",
      "Batch 35, Loss: 0.898162, Accuracy: 85.58%\n",
      "Batch 36, Loss: 0.876601, Accuracy: 85.63%\n",
      "Batch 37, Loss: 0.879866, Accuracy: 85.73%\n",
      "Batch 38, Loss: 0.906211, Accuracy: 85.69%\n",
      "Batch 39, Loss: 0.840332, Accuracy: 85.82%\n",
      "Batch 40, Loss: 0.933622, Accuracy: 85.74%\n",
      "Batch 41, Loss: 0.894854, Accuracy: 85.71%\n",
      "Batch 42, Loss: 0.957317, Accuracy: 85.53%\n",
      "Batch 43, Loss: 0.897093, Accuracy: 85.50%\n",
      "Batch 44, Loss: 0.900790, Accuracy: 85.48%\n",
      "Batch 45, Loss: 0.860513, Accuracy: 85.56%\n",
      "Batch 46, Loss: 0.913738, Accuracy: 85.50%\n",
      "Batch 47, Loss: 0.883611, Accuracy: 85.51%\n",
      "Batch 48, Loss: 0.892911, Accuracy: 85.48%\n",
      "Batch 49, Loss: 0.932954, Accuracy: 85.40%\n",
      "Batch 50, Loss: 0.873211, Accuracy: 85.41%\n",
      "Batch 51, Loss: 0.919826, Accuracy: 85.32%\n",
      "Batch 52, Loss: 0.907708, Accuracy: 85.25%\n",
      "Batch 53, Loss: 0.885744, Accuracy: 85.26%\n",
      "Batch 54, Loss: 0.929941, Accuracy: 85.21%\n",
      "Batch 55, Loss: 0.826111, Accuracy: 85.34%\n",
      "Batch 56, Loss: 0.879133, Accuracy: 85.35%\n",
      "Batch 57, Loss: 0.938812, Accuracy: 85.25%\n",
      "Batch 58, Loss: 0.811704, Accuracy: 85.37%\n",
      "Batch 59, Loss: 0.843014, Accuracy: 85.43%\n",
      "Batch 60, Loss: 0.949455, Accuracy: 85.36%\n",
      "Batch 61, Loss: 0.899249, Accuracy: 85.37%\n",
      "Batch 62, Loss: 0.919291, Accuracy: 85.36%\n",
      "Batch 63, Loss: 0.890488, Accuracy: 85.39%\n",
      "Batch 64, Loss: 0.884274, Accuracy: 85.42%\n",
      "Batch 65, Loss: 0.879391, Accuracy: 85.43%\n",
      "Batch 66, Loss: 0.913874, Accuracy: 85.39%\n",
      "Batch 67, Loss: 0.880297, Accuracy: 85.42%\n",
      "Batch 68, Loss: 0.934516, Accuracy: 85.36%\n",
      "Batch 69, Loss: 0.899907, Accuracy: 85.37%\n",
      "Batch 70, Loss: 0.927242, Accuracy: 85.33%\n",
      "Batch 71, Loss: 0.866077, Accuracy: 85.37%\n",
      "Batch 72, Loss: 0.850004, Accuracy: 85.44%\n",
      "Batch 73, Loss: 0.904093, Accuracy: 85.42%\n",
      "Batch 74, Loss: 0.873147, Accuracy: 85.45%\n",
      "Batch 75, Loss: 0.814225, Accuracy: 85.56%\n",
      "Batch 76, Loss: 0.868875, Accuracy: 85.61%\n",
      "Batch 77, Loss: 0.941388, Accuracy: 85.55%\n",
      "Batch 78, Loss: 0.854041, Accuracy: 85.60%\n",
      "Batch 79, Loss: 0.861617, Accuracy: 85.62%\n",
      "Batch 80, Loss: 0.827788, Accuracy: 85.70%\n",
      "Batch 81, Loss: 0.862482, Accuracy: 85.74%\n",
      "Batch 82, Loss: 0.937131, Accuracy: 85.69%\n",
      "Batch 83, Loss: 0.866877, Accuracy: 85.73%\n",
      "Batch 84, Loss: 0.942374, Accuracy: 85.66%\n",
      "Batch 85, Loss: 0.866816, Accuracy: 85.68%\n",
      "Batch 86, Loss: 0.868651, Accuracy: 85.70%\n",
      "Batch 87, Loss: 0.839333, Accuracy: 85.78%\n",
      "Batch 88, Loss: 0.937129, Accuracy: 85.71%\n",
      "Batch 89, Loss: 0.870140, Accuracy: 85.73%\n",
      "Batch 90, Loss: 0.888692, Accuracy: 85.71%\n",
      "Batch 91, Loss: 0.823827, Accuracy: 85.77%\n",
      "Batch 92, Loss: 0.835803, Accuracy: 85.84%\n",
      "Batch 93, Loss: 0.893637, Accuracy: 85.84%\n",
      "Batch 94, Loss: 0.879033, Accuracy: 85.85%\n",
      "Batch 95, Loss: 0.805876, Accuracy: 85.95%\n",
      "Batch 96, Loss: 0.883828, Accuracy: 85.94%\n",
      "Batch 97, Loss: 0.832710, Accuracy: 85.99%\n",
      "Batch 98, Loss: 0.880698, Accuracy: 85.99%\n",
      "Batch 99, Loss: 0.912212, Accuracy: 85.97%\n",
      "Batch 100, Loss: 0.886532, Accuracy: 85.98%\n",
      "Batch 101, Loss: 0.854865, Accuracy: 86.01%\n",
      "Batch 102, Loss: 0.879987, Accuracy: 86.01%\n",
      "Batch 103, Loss: 0.893668, Accuracy: 86.00%\n",
      "Batch 104, Loss: 0.834839, Accuracy: 86.03%\n",
      "Batch 105, Loss: 0.823913, Accuracy: 86.07%\n",
      "Batch 106, Loss: 0.830412, Accuracy: 86.13%\n",
      "Batch 107, Loss: 0.834673, Accuracy: 86.17%\n",
      "Batch 108, Loss: 0.875544, Accuracy: 86.20%\n",
      "Batch 109, Loss: 0.911277, Accuracy: 86.15%\n",
      "Batch 110, Loss: 0.865349, Accuracy: 86.16%\n",
      "Batch 111, Loss: 0.852822, Accuracy: 86.19%\n",
      "Batch 112, Loss: 0.824408, Accuracy: 86.24%\n",
      "Batch 113, Loss: 0.829267, Accuracy: 86.28%\n",
      "Batch 114, Loss: 0.870779, Accuracy: 86.29%\n",
      "Batch 115, Loss: 0.855594, Accuracy: 86.32%\n",
      "Batch 116, Loss: 0.841520, Accuracy: 86.36%\n",
      "Batch 117, Loss: 0.911250, Accuracy: 86.32%\n",
      "Batch 118, Loss: 0.914711, Accuracy: 86.30%\n",
      "Batch 119, Loss: 0.913188, Accuracy: 86.28%\n",
      "Batch 120, Loss: 0.897348, Accuracy: 86.25%\n",
      "Batch 121, Loss: 0.908253, Accuracy: 86.22%\n",
      "Batch 122, Loss: 0.878749, Accuracy: 86.23%\n",
      "Batch 123, Loss: 0.885569, Accuracy: 86.22%\n",
      "Batch 124, Loss: 0.835890, Accuracy: 86.27%\n",
      "Batch 125, Loss: 0.833799, Accuracy: 86.31%\n",
      "Batch 126, Loss: 0.915225, Accuracy: 86.30%\n",
      "Batch 127, Loss: 0.872594, Accuracy: 86.31%\n",
      "Batch 128, Loss: 0.992181, Accuracy: 86.22%\n",
      "Batch 129, Loss: 0.897867, Accuracy: 86.20%\n",
      "Batch 130, Loss: 0.880284, Accuracy: 86.20%\n",
      "Batch 131, Loss: 0.855920, Accuracy: 86.22%\n",
      "Batch 132, Loss: 0.865814, Accuracy: 86.25%\n",
      "Batch 133, Loss: 0.926503, Accuracy: 86.22%\n",
      "Batch 134, Loss: 0.904919, Accuracy: 86.21%\n",
      "Batch 135, Loss: 0.874843, Accuracy: 86.23%\n",
      "Batch 136, Loss: 0.837864, Accuracy: 86.26%\n",
      "Batch 137, Loss: 0.845317, Accuracy: 86.29%\n",
      "Batch 138, Loss: 0.881152, Accuracy: 86.29%\n",
      "Batch 139, Loss: 0.899552, Accuracy: 86.27%\n",
      "Batch 140, Loss: 0.887101, Accuracy: 86.27%\n",
      "Batch 141, Loss: 0.891149, Accuracy: 86.27%\n",
      "Batch 142, Loss: 0.919217, Accuracy: 86.23%\n",
      "Batch 143, Loss: 0.912529, Accuracy: 86.20%\n",
      "Batch 144, Loss: 0.832025, Accuracy: 86.24%\n",
      "Batch 145, Loss: 0.955913, Accuracy: 86.17%\n",
      "Batch 146, Loss: 0.904958, Accuracy: 86.17%\n",
      "Batch 147, Loss: 0.864263, Accuracy: 86.18%\n",
      "Batch 148, Loss: 0.912279, Accuracy: 86.16%\n",
      "Batch 149, Loss: 0.885646, Accuracy: 86.16%\n",
      "Batch 150, Loss: 0.905311, Accuracy: 86.15%\n",
      "Batch 151, Loss: 0.886901, Accuracy: 86.15%\n",
      "Batch 152, Loss: 0.815913, Accuracy: 86.20%\n",
      "Batch 153, Loss: 0.894837, Accuracy: 86.19%\n",
      "Batch 154, Loss: 0.883605, Accuracy: 86.19%\n",
      "Batch 155, Loss: 0.873512, Accuracy: 86.19%\n",
      "Batch 156, Loss: 0.893060, Accuracy: 86.19%\n",
      "Batch 157, Loss: 0.850035, Accuracy: 86.22%\n",
      "Batch 158, Loss: 0.836135, Accuracy: 86.26%\n",
      "Batch 159, Loss: 0.883864, Accuracy: 86.26%\n",
      "Batch 160, Loss: 0.825845, Accuracy: 86.30%\n",
      "Batch 161, Loss: 0.885038, Accuracy: 86.30%\n",
      "Batch 162, Loss: 0.947727, Accuracy: 86.25%\n",
      "Batch 163, Loss: 0.839436, Accuracy: 86.27%\n",
      "Batch 164, Loss: 0.959898, Accuracy: 86.22%\n",
      "Batch 165, Loss: 0.864742, Accuracy: 86.24%\n",
      "Batch 166, Loss: 0.852987, Accuracy: 86.27%\n",
      "Batch 167, Loss: 0.895975, Accuracy: 86.26%\n",
      "Batch 168, Loss: 0.915968, Accuracy: 86.24%\n",
      "Batch 169, Loss: 0.844118, Accuracy: 86.25%\n",
      "Batch 170, Loss: 0.826130, Accuracy: 86.29%\n",
      "Batch 171, Loss: 0.905540, Accuracy: 86.27%\n",
      "Batch 172, Loss: 0.927391, Accuracy: 86.23%\n",
      "Batch 173, Loss: 0.834947, Accuracy: 86.25%\n",
      "Batch 174, Loss: 0.796771, Accuracy: 86.31%\n",
      "Batch 175, Loss: 0.894258, Accuracy: 86.30%\n",
      "Batch 176, Loss: 0.807112, Accuracy: 86.35%\n",
      "Batch 177, Loss: 0.874022, Accuracy: 86.35%\n",
      "Batch 178, Loss: 0.931328, Accuracy: 86.32%\n",
      "Batch 179, Loss: 0.913325, Accuracy: 86.30%\n",
      "Batch 180, Loss: 0.885494, Accuracy: 86.30%\n",
      "Batch 181, Loss: 0.841632, Accuracy: 86.33%\n",
      "Batch 182, Loss: 0.903124, Accuracy: 86.32%\n",
      "Batch 183, Loss: 0.895483, Accuracy: 86.32%\n",
      "Batch 184, Loss: 0.910824, Accuracy: 86.30%\n",
      "Batch 185, Loss: 0.919250, Accuracy: 86.28%\n",
      "Batch 186, Loss: 0.890446, Accuracy: 86.27%\n",
      "Batch 187, Loss: 0.856773, Accuracy: 86.29%\n",
      "Batch 188, Loss: 0.864711, Accuracy: 86.30%\n",
      "Batch 189, Loss: 0.826205, Accuracy: 86.33%\n",
      "Batch 190, Loss: 0.838342, Accuracy: 86.36%\n",
      "Batch 191, Loss: 0.947036, Accuracy: 86.31%\n",
      "Batch 192, Loss: 0.902064, Accuracy: 86.30%\n",
      "Batch 193, Loss: 0.863265, Accuracy: 86.32%\n",
      "Batch 194, Loss: 0.943056, Accuracy: 86.28%\n",
      "Batch 195, Loss: 0.886001, Accuracy: 86.27%\n",
      "Batch 196, Loss: 0.869280, Accuracy: 86.28%\n",
      "Batch 197, Loss: 0.955787, Accuracy: 86.25%\n",
      "Batch 198, Loss: 0.888760, Accuracy: 86.25%\n",
      "Batch 199, Loss: 0.870709, Accuracy: 86.27%\n",
      "Batch 200, Loss: 0.925860, Accuracy: 86.24%\n",
      "Batch 201, Loss: 0.876314, Accuracy: 86.24%\n",
      "Batch 202, Loss: 0.847425, Accuracy: 86.24%\n",
      "Batch 203, Loss: 0.897034, Accuracy: 86.23%\n",
      "Batch 204, Loss: 0.859735, Accuracy: 86.24%\n",
      "Batch 205, Loss: 0.918191, Accuracy: 86.22%\n",
      "Batch 206, Loss: 0.843250, Accuracy: 86.25%\n",
      "Batch 207, Loss: 0.912128, Accuracy: 86.22%\n",
      "Batch 208, Loss: 0.818537, Accuracy: 86.25%\n",
      "Batch 209, Loss: 0.876403, Accuracy: 86.26%\n",
      "Batch 210, Loss: 0.868068, Accuracy: 86.26%\n",
      "Batch 211, Loss: 0.917493, Accuracy: 86.25%\n",
      "Batch 212, Loss: 0.922974, Accuracy: 86.22%\n",
      "Batch 213, Loss: 0.947918, Accuracy: 86.19%\n",
      "Training - Epoch 73, Loss: 0.883923, Accuracy: 86.19%\n",
      "Validation Batch 1, Loss: 0.866571, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.859692, Accuracy: 89.06%\n",
      "Validation Batch 3, Loss: 0.926291, Accuracy: 86.46%\n",
      "Validation Batch 4, Loss: 0.885042, Accuracy: 86.33%\n",
      "Validation Batch 5, Loss: 0.869652, Accuracy: 87.19%\n",
      "Validation Batch 6, Loss: 0.837785, Accuracy: 88.28%\n",
      "Validation Batch 7, Loss: 0.849293, Accuracy: 88.62%\n",
      "Validation Batch 8, Loss: 0.959757, Accuracy: 87.11%\n",
      "Validation Batch 9, Loss: 0.923075, Accuracy: 86.81%\n",
      "Validation Batch 10, Loss: 0.890150, Accuracy: 86.72%\n",
      "Validation Batch 11, Loss: 0.877186, Accuracy: 86.79%\n",
      "Validation Batch 12, Loss: 0.872883, Accuracy: 86.85%\n",
      "Validation Batch 13, Loss: 0.887233, Accuracy: 86.78%\n",
      "Validation Batch 14, Loss: 0.907548, Accuracy: 86.61%\n",
      "Validation Batch 15, Loss: 0.858915, Accuracy: 86.67%\n",
      "Validation Batch 16, Loss: 0.883320, Accuracy: 86.72%\n",
      "Validation Batch 17, Loss: 0.932043, Accuracy: 86.49%\n",
      "Validation Batch 18, Loss: 0.867663, Accuracy: 86.55%\n",
      "Validation Batch 19, Loss: 0.932687, Accuracy: 86.35%\n",
      "Validation Batch 20, Loss: 0.962461, Accuracy: 85.94%\n",
      "Validation Batch 21, Loss: 0.904995, Accuracy: 85.94%\n",
      "Validation Batch 22, Loss: 0.898585, Accuracy: 85.87%\n",
      "Validation Batch 23, Loss: 0.947232, Accuracy: 85.53%\n",
      "Validation Batch 24, Loss: 0.914070, Accuracy: 85.48%\n",
      "Validation Batch 25, Loss: 0.851530, Accuracy: 85.69%\n",
      "Validation Batch 26, Loss: 0.890321, Accuracy: 85.70%\n",
      "Validation Batch 27, Loss: 0.872835, Accuracy: 85.79%\n",
      "Validation - Epoch 73, Loss: 0.893660, Accuracy: 85.79%\n",
      "Patience—1\n",
      "Epoch 74\n",
      "Batch 1, Loss: 0.932649, Accuracy: 81.25%\n",
      "Batch 2, Loss: 0.932243, Accuracy: 80.47%\n",
      "Batch 3, Loss: 0.861004, Accuracy: 82.81%\n",
      "Batch 4, Loss: 0.962420, Accuracy: 81.64%\n",
      "Batch 5, Loss: 0.875000, Accuracy: 82.81%\n",
      "Batch 6, Loss: 0.862344, Accuracy: 83.59%\n",
      "Batch 7, Loss: 0.921377, Accuracy: 83.71%\n",
      "Batch 8, Loss: 0.921197, Accuracy: 83.40%\n",
      "Batch 9, Loss: 0.852358, Accuracy: 84.20%\n",
      "Batch 10, Loss: 0.879621, Accuracy: 84.53%\n",
      "Batch 11, Loss: 0.888570, Accuracy: 84.66%\n",
      "Batch 12, Loss: 0.860544, Accuracy: 85.03%\n",
      "Batch 13, Loss: 0.952633, Accuracy: 84.62%\n",
      "Batch 14, Loss: 0.915672, Accuracy: 84.49%\n",
      "Batch 15, Loss: 0.865045, Accuracy: 84.58%\n",
      "Batch 16, Loss: 0.898203, Accuracy: 84.57%\n",
      "Batch 17, Loss: 0.924331, Accuracy: 84.47%\n",
      "Batch 18, Loss: 0.879449, Accuracy: 84.64%\n",
      "Batch 19, Loss: 0.911973, Accuracy: 84.54%\n",
      "Batch 20, Loss: 0.812586, Accuracy: 85.00%\n",
      "Batch 21, Loss: 0.855968, Accuracy: 85.19%\n",
      "Batch 22, Loss: 0.824966, Accuracy: 85.58%\n",
      "Batch 23, Loss: 0.867056, Accuracy: 85.67%\n",
      "Batch 24, Loss: 0.861404, Accuracy: 85.74%\n",
      "Batch 25, Loss: 0.861794, Accuracy: 85.88%\n",
      "Batch 26, Loss: 0.877166, Accuracy: 86.00%\n",
      "Batch 27, Loss: 0.892007, Accuracy: 86.05%\n",
      "Batch 28, Loss: 0.880376, Accuracy: 86.05%\n",
      "Batch 29, Loss: 0.882571, Accuracy: 86.05%\n",
      "Batch 30, Loss: 0.838014, Accuracy: 86.20%\n",
      "Batch 31, Loss: 0.919372, Accuracy: 86.04%\n",
      "Batch 32, Loss: 0.903362, Accuracy: 85.99%\n",
      "Batch 33, Loss: 0.921661, Accuracy: 85.84%\n",
      "Batch 34, Loss: 0.887751, Accuracy: 85.89%\n",
      "Batch 35, Loss: 0.986387, Accuracy: 85.58%\n",
      "Batch 36, Loss: 0.827297, Accuracy: 85.76%\n",
      "Batch 37, Loss: 0.891968, Accuracy: 85.73%\n",
      "Batch 38, Loss: 0.898813, Accuracy: 85.69%\n",
      "Batch 39, Loss: 0.900908, Accuracy: 85.66%\n",
      "Batch 40, Loss: 0.873597, Accuracy: 85.66%\n",
      "Batch 41, Loss: 0.905898, Accuracy: 85.63%\n",
      "Batch 42, Loss: 0.908089, Accuracy: 85.60%\n",
      "Batch 43, Loss: 0.859046, Accuracy: 85.65%\n",
      "Batch 44, Loss: 0.900309, Accuracy: 85.65%\n",
      "Batch 45, Loss: 0.944421, Accuracy: 85.49%\n",
      "Batch 46, Loss: 0.812545, Accuracy: 85.70%\n",
      "Batch 47, Loss: 0.803502, Accuracy: 85.90%\n",
      "Batch 48, Loss: 0.905201, Accuracy: 85.84%\n",
      "Batch 49, Loss: 0.950590, Accuracy: 85.65%\n",
      "Batch 50, Loss: 0.884681, Accuracy: 85.66%\n",
      "Batch 51, Loss: 0.882145, Accuracy: 85.66%\n",
      "Batch 52, Loss: 0.894635, Accuracy: 85.64%\n",
      "Batch 53, Loss: 0.896064, Accuracy: 85.67%\n",
      "Batch 54, Loss: 0.869458, Accuracy: 85.71%\n",
      "Batch 55, Loss: 0.890978, Accuracy: 85.68%\n",
      "Batch 56, Loss: 0.872772, Accuracy: 85.66%\n",
      "Batch 57, Loss: 0.974943, Accuracy: 85.50%\n",
      "Batch 58, Loss: 0.869288, Accuracy: 85.53%\n",
      "Batch 59, Loss: 0.924673, Accuracy: 85.43%\n",
      "Batch 60, Loss: 0.910797, Accuracy: 85.39%\n",
      "Batch 61, Loss: 0.889263, Accuracy: 85.37%\n",
      "Batch 62, Loss: 0.916505, Accuracy: 85.31%\n",
      "Batch 63, Loss: 0.898414, Accuracy: 85.24%\n",
      "Batch 64, Loss: 0.867788, Accuracy: 85.25%\n",
      "Batch 65, Loss: 0.864240, Accuracy: 85.29%\n",
      "Batch 66, Loss: 0.958454, Accuracy: 85.18%\n",
      "Batch 67, Loss: 0.786724, Accuracy: 85.35%\n",
      "Batch 68, Loss: 0.880950, Accuracy: 85.36%\n",
      "Batch 69, Loss: 0.847804, Accuracy: 85.44%\n",
      "Batch 70, Loss: 0.892336, Accuracy: 85.42%\n",
      "Batch 71, Loss: 0.846685, Accuracy: 85.52%\n",
      "Batch 72, Loss: 0.830778, Accuracy: 85.61%\n",
      "Batch 73, Loss: 0.935859, Accuracy: 85.55%\n",
      "Batch 74, Loss: 0.882672, Accuracy: 85.56%\n",
      "Batch 75, Loss: 0.866348, Accuracy: 85.58%\n",
      "Batch 76, Loss: 0.841687, Accuracy: 85.67%\n",
      "Batch 77, Loss: 0.911495, Accuracy: 85.63%\n",
      "Batch 78, Loss: 0.881026, Accuracy: 85.64%\n",
      "Batch 79, Loss: 0.884073, Accuracy: 85.60%\n",
      "Batch 80, Loss: 0.957187, Accuracy: 85.49%\n",
      "Batch 81, Loss: 0.848976, Accuracy: 85.55%\n",
      "Batch 82, Loss: 0.834419, Accuracy: 85.63%\n",
      "Batch 83, Loss: 0.854751, Accuracy: 85.69%\n",
      "Batch 84, Loss: 0.848524, Accuracy: 85.75%\n",
      "Batch 85, Loss: 0.876021, Accuracy: 85.77%\n",
      "Batch 86, Loss: 0.934453, Accuracy: 85.72%\n",
      "Batch 87, Loss: 0.885725, Accuracy: 85.72%\n",
      "Batch 88, Loss: 0.890238, Accuracy: 85.72%\n",
      "Batch 89, Loss: 0.947173, Accuracy: 85.66%\n",
      "Batch 90, Loss: 0.813725, Accuracy: 85.76%\n",
      "Batch 91, Loss: 0.912818, Accuracy: 85.73%\n",
      "Batch 92, Loss: 0.968856, Accuracy: 85.63%\n",
      "Batch 93, Loss: 0.849684, Accuracy: 85.67%\n",
      "Batch 94, Loss: 0.827195, Accuracy: 85.74%\n",
      "Batch 95, Loss: 0.852203, Accuracy: 85.76%\n",
      "Batch 96, Loss: 0.928576, Accuracy: 85.74%\n",
      "Batch 97, Loss: 0.924983, Accuracy: 85.70%\n",
      "Batch 98, Loss: 0.862286, Accuracy: 85.73%\n",
      "Batch 99, Loss: 0.903032, Accuracy: 85.72%\n",
      "Batch 100, Loss: 1.001940, Accuracy: 85.59%\n",
      "Batch 101, Loss: 0.932065, Accuracy: 85.54%\n",
      "Batch 102, Loss: 0.876411, Accuracy: 85.55%\n",
      "Batch 103, Loss: 0.930035, Accuracy: 85.50%\n",
      "Batch 104, Loss: 0.872423, Accuracy: 85.52%\n",
      "Batch 105, Loss: 0.875164, Accuracy: 85.54%\n",
      "Batch 106, Loss: 0.906413, Accuracy: 85.52%\n",
      "Batch 107, Loss: 0.848768, Accuracy: 85.56%\n",
      "Batch 108, Loss: 0.921388, Accuracy: 85.55%\n",
      "Batch 109, Loss: 0.869556, Accuracy: 85.56%\n",
      "Batch 110, Loss: 0.913387, Accuracy: 85.54%\n",
      "Batch 111, Loss: 0.880640, Accuracy: 85.54%\n",
      "Batch 112, Loss: 0.887229, Accuracy: 85.53%\n",
      "Batch 113, Loss: 0.899302, Accuracy: 85.50%\n",
      "Batch 114, Loss: 0.858122, Accuracy: 85.53%\n",
      "Batch 115, Loss: 0.909553, Accuracy: 85.50%\n",
      "Batch 116, Loss: 0.821502, Accuracy: 85.57%\n",
      "Batch 117, Loss: 0.850220, Accuracy: 85.62%\n",
      "Batch 118, Loss: 0.875853, Accuracy: 85.63%\n",
      "Batch 119, Loss: 0.852589, Accuracy: 85.66%\n",
      "Batch 120, Loss: 0.866801, Accuracy: 85.68%\n",
      "Batch 121, Loss: 0.899396, Accuracy: 85.69%\n",
      "Batch 122, Loss: 0.867634, Accuracy: 85.71%\n",
      "Batch 123, Loss: 0.886010, Accuracy: 85.71%\n",
      "Batch 124, Loss: 0.893488, Accuracy: 85.70%\n",
      "Batch 125, Loss: 0.932023, Accuracy: 85.66%\n",
      "Batch 126, Loss: 0.883936, Accuracy: 85.66%\n",
      "Batch 127, Loss: 0.951179, Accuracy: 85.61%\n",
      "Batch 128, Loss: 0.870071, Accuracy: 85.61%\n",
      "Batch 129, Loss: 0.833908, Accuracy: 85.65%\n",
      "Batch 130, Loss: 0.864663, Accuracy: 85.69%\n",
      "Batch 131, Loss: 0.895814, Accuracy: 85.69%\n",
      "Batch 132, Loss: 0.875573, Accuracy: 85.69%\n",
      "Batch 133, Loss: 0.912765, Accuracy: 85.67%\n",
      "Batch 134, Loss: 0.909497, Accuracy: 85.66%\n",
      "Batch 135, Loss: 0.836012, Accuracy: 85.69%\n",
      "Batch 136, Loss: 0.824431, Accuracy: 85.73%\n",
      "Batch 137, Loss: 0.844552, Accuracy: 85.76%\n",
      "Batch 138, Loss: 0.915683, Accuracy: 85.73%\n",
      "Batch 139, Loss: 0.888361, Accuracy: 85.74%\n",
      "Batch 140, Loss: 0.941121, Accuracy: 85.69%\n",
      "Batch 141, Loss: 0.878514, Accuracy: 85.68%\n",
      "Batch 142, Loss: 0.829560, Accuracy: 85.72%\n",
      "Batch 143, Loss: 0.876022, Accuracy: 85.74%\n",
      "Batch 144, Loss: 0.902168, Accuracy: 85.73%\n",
      "Batch 145, Loss: 0.785706, Accuracy: 85.80%\n",
      "Batch 146, Loss: 0.894416, Accuracy: 85.79%\n",
      "Batch 147, Loss: 0.929639, Accuracy: 85.76%\n",
      "Batch 148, Loss: 0.922222, Accuracy: 85.74%\n",
      "Batch 149, Loss: 0.915433, Accuracy: 85.71%\n",
      "Batch 150, Loss: 0.944903, Accuracy: 85.67%\n",
      "Batch 151, Loss: 0.933903, Accuracy: 85.63%\n",
      "Batch 152, Loss: 0.904924, Accuracy: 85.61%\n",
      "Batch 153, Loss: 0.835491, Accuracy: 85.64%\n",
      "Batch 154, Loss: 0.811547, Accuracy: 85.69%\n",
      "Batch 155, Loss: 0.941781, Accuracy: 85.67%\n",
      "Batch 156, Loss: 0.880872, Accuracy: 85.67%\n",
      "Batch 157, Loss: 0.810821, Accuracy: 85.72%\n",
      "Batch 158, Loss: 0.946747, Accuracy: 85.68%\n",
      "Batch 159, Loss: 0.841218, Accuracy: 85.70%\n",
      "Batch 160, Loss: 0.873872, Accuracy: 85.71%\n",
      "Batch 161, Loss: 0.894087, Accuracy: 85.70%\n",
      "Batch 162, Loss: 0.879264, Accuracy: 85.72%\n",
      "Batch 163, Loss: 0.892807, Accuracy: 85.71%\n",
      "Batch 164, Loss: 0.854028, Accuracy: 85.74%\n",
      "Batch 165, Loss: 0.870431, Accuracy: 85.74%\n",
      "Batch 166, Loss: 0.851097, Accuracy: 85.76%\n",
      "Batch 167, Loss: 0.905355, Accuracy: 85.73%\n",
      "Batch 168, Loss: 0.908712, Accuracy: 85.70%\n",
      "Batch 169, Loss: 0.876360, Accuracy: 85.72%\n",
      "Batch 170, Loss: 0.886055, Accuracy: 85.73%\n",
      "Batch 171, Loss: 0.830377, Accuracy: 85.76%\n",
      "Batch 172, Loss: 0.889737, Accuracy: 85.76%\n",
      "Batch 173, Loss: 0.848616, Accuracy: 85.79%\n",
      "Batch 174, Loss: 0.894224, Accuracy: 85.79%\n",
      "Batch 175, Loss: 0.919169, Accuracy: 85.78%\n",
      "Batch 176, Loss: 0.900260, Accuracy: 85.78%\n",
      "Batch 177, Loss: 0.887036, Accuracy: 85.78%\n",
      "Batch 178, Loss: 0.891876, Accuracy: 85.77%\n",
      "Batch 179, Loss: 0.866207, Accuracy: 85.78%\n",
      "Batch 180, Loss: 0.914245, Accuracy: 85.77%\n",
      "Batch 181, Loss: 0.865074, Accuracy: 85.79%\n",
      "Batch 182, Loss: 0.865648, Accuracy: 85.80%\n",
      "Batch 183, Loss: 0.897175, Accuracy: 85.79%\n",
      "Batch 184, Loss: 0.857988, Accuracy: 85.81%\n",
      "Batch 185, Loss: 0.907572, Accuracy: 85.80%\n",
      "Batch 186, Loss: 0.915137, Accuracy: 85.79%\n",
      "Batch 187, Loss: 0.949430, Accuracy: 85.75%\n",
      "Batch 188, Loss: 0.938896, Accuracy: 85.71%\n",
      "Batch 189, Loss: 0.929312, Accuracy: 85.70%\n",
      "Batch 190, Loss: 0.909473, Accuracy: 85.68%\n",
      "Batch 191, Loss: 0.865679, Accuracy: 85.68%\n",
      "Batch 192, Loss: 0.849906, Accuracy: 85.71%\n",
      "Batch 193, Loss: 0.903474, Accuracy: 85.69%\n",
      "Batch 194, Loss: 0.862232, Accuracy: 85.71%\n",
      "Batch 195, Loss: 0.873230, Accuracy: 85.72%\n",
      "Batch 196, Loss: 0.875161, Accuracy: 85.73%\n",
      "Batch 197, Loss: 0.891199, Accuracy: 85.72%\n",
      "Batch 198, Loss: 0.860800, Accuracy: 85.73%\n",
      "Batch 199, Loss: 0.950786, Accuracy: 85.69%\n",
      "Batch 200, Loss: 0.860911, Accuracy: 85.71%\n",
      "Batch 201, Loss: 0.935062, Accuracy: 85.68%\n",
      "Batch 202, Loss: 0.892628, Accuracy: 85.67%\n",
      "Batch 203, Loss: 0.837858, Accuracy: 85.68%\n",
      "Batch 204, Loss: 0.902485, Accuracy: 85.67%\n",
      "Batch 205, Loss: 0.861357, Accuracy: 85.67%\n",
      "Batch 206, Loss: 0.926078, Accuracy: 85.66%\n",
      "Batch 207, Loss: 0.870802, Accuracy: 85.67%\n",
      "Batch 208, Loss: 0.893783, Accuracy: 85.67%\n",
      "Batch 209, Loss: 0.859768, Accuracy: 85.68%\n",
      "Batch 210, Loss: 0.895118, Accuracy: 85.68%\n",
      "Batch 211, Loss: 0.947002, Accuracy: 85.64%\n",
      "Batch 212, Loss: 0.915300, Accuracy: 85.64%\n",
      "Batch 213, Loss: 0.892936, Accuracy: 85.63%\n",
      "Training - Epoch 74, Loss: 0.887230, Accuracy: 85.63%\n",
      "Validation Batch 1, Loss: 0.864274, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.850711, Accuracy: 89.84%\n",
      "Validation Batch 3, Loss: 0.923590, Accuracy: 88.02%\n",
      "Validation Batch 4, Loss: 0.881945, Accuracy: 87.89%\n",
      "Validation Batch 5, Loss: 0.863840, Accuracy: 88.44%\n",
      "Validation Batch 6, Loss: 0.835008, Accuracy: 89.32%\n",
      "Validation Batch 7, Loss: 0.843535, Accuracy: 89.73%\n",
      "Validation Batch 8, Loss: 0.953559, Accuracy: 88.09%\n",
      "Validation Batch 9, Loss: 0.922459, Accuracy: 87.67%\n",
      "Validation Batch 10, Loss: 0.889660, Accuracy: 87.50%\n",
      "Validation Batch 11, Loss: 0.873183, Accuracy: 87.50%\n",
      "Validation Batch 12, Loss: 0.871175, Accuracy: 87.50%\n",
      "Validation Batch 13, Loss: 0.883989, Accuracy: 87.38%\n",
      "Validation Batch 14, Loss: 0.901903, Accuracy: 87.17%\n",
      "Validation Batch 15, Loss: 0.863147, Accuracy: 87.29%\n",
      "Validation Batch 16, Loss: 0.880143, Accuracy: 87.30%\n",
      "Validation Batch 17, Loss: 0.923679, Accuracy: 87.04%\n",
      "Validation Batch 18, Loss: 0.863387, Accuracy: 87.07%\n",
      "Validation Batch 19, Loss: 0.926067, Accuracy: 86.84%\n",
      "Validation Batch 20, Loss: 0.954258, Accuracy: 86.41%\n",
      "Validation Batch 21, Loss: 0.904800, Accuracy: 86.38%\n",
      "Validation Batch 22, Loss: 0.893548, Accuracy: 86.36%\n",
      "Validation Batch 23, Loss: 0.923531, Accuracy: 86.28%\n",
      "Validation Batch 24, Loss: 0.911358, Accuracy: 86.13%\n",
      "Validation Batch 25, Loss: 0.852066, Accuracy: 86.31%\n",
      "Validation Batch 26, Loss: 0.889370, Accuracy: 86.30%\n",
      "Validation Batch 27, Loss: 0.872009, Accuracy: 86.38%\n",
      "Validation - Epoch 74, Loss: 0.889489, Accuracy: 86.38%\n",
      "Patience—2\n",
      "Epoch 75\n",
      "Batch 1, Loss: 0.940914, Accuracy: 78.12%\n",
      "Batch 2, Loss: 0.886396, Accuracy: 82.03%\n",
      "Batch 3, Loss: 0.879168, Accuracy: 83.33%\n",
      "Batch 4, Loss: 0.866417, Accuracy: 84.77%\n",
      "Batch 5, Loss: 0.830237, Accuracy: 85.94%\n",
      "Batch 6, Loss: 0.957876, Accuracy: 84.64%\n",
      "Batch 7, Loss: 0.889716, Accuracy: 84.82%\n",
      "Batch 8, Loss: 0.873331, Accuracy: 85.16%\n",
      "Batch 9, Loss: 0.924755, Accuracy: 84.55%\n",
      "Batch 10, Loss: 0.944331, Accuracy: 84.06%\n",
      "Batch 11, Loss: 0.888159, Accuracy: 84.23%\n",
      "Batch 12, Loss: 0.976168, Accuracy: 83.46%\n",
      "Batch 13, Loss: 0.823215, Accuracy: 84.13%\n",
      "Batch 14, Loss: 0.927511, Accuracy: 83.93%\n",
      "Batch 15, Loss: 0.817612, Accuracy: 84.58%\n",
      "Batch 16, Loss: 0.857628, Accuracy: 84.77%\n",
      "Batch 17, Loss: 0.813828, Accuracy: 85.20%\n",
      "Batch 18, Loss: 0.866395, Accuracy: 85.42%\n",
      "Batch 19, Loss: 0.962431, Accuracy: 85.03%\n",
      "Batch 20, Loss: 0.918677, Accuracy: 84.92%\n",
      "Batch 21, Loss: 0.860560, Accuracy: 85.04%\n",
      "Batch 22, Loss: 0.839990, Accuracy: 85.30%\n",
      "Batch 23, Loss: 0.945128, Accuracy: 84.99%\n",
      "Batch 24, Loss: 0.932064, Accuracy: 84.83%\n",
      "Batch 25, Loss: 0.849607, Accuracy: 85.00%\n",
      "Batch 26, Loss: 0.921033, Accuracy: 84.92%\n",
      "Batch 27, Loss: 0.942667, Accuracy: 84.72%\n",
      "Batch 28, Loss: 0.864053, Accuracy: 84.82%\n",
      "Batch 29, Loss: 0.827562, Accuracy: 85.08%\n",
      "Batch 30, Loss: 0.942214, Accuracy: 84.90%\n",
      "Batch 31, Loss: 0.854782, Accuracy: 85.03%\n",
      "Batch 32, Loss: 0.918689, Accuracy: 84.86%\n",
      "Batch 33, Loss: 0.850606, Accuracy: 85.04%\n",
      "Batch 34, Loss: 0.860281, Accuracy: 85.11%\n",
      "Batch 35, Loss: 0.890519, Accuracy: 85.13%\n",
      "Batch 36, Loss: 0.979895, Accuracy: 84.85%\n",
      "Batch 37, Loss: 0.891885, Accuracy: 84.88%\n",
      "Batch 38, Loss: 0.874413, Accuracy: 84.95%\n",
      "Batch 39, Loss: 0.970409, Accuracy: 84.78%\n",
      "Batch 40, Loss: 0.836614, Accuracy: 84.92%\n",
      "Batch 41, Loss: 0.840738, Accuracy: 85.10%\n",
      "Batch 42, Loss: 0.818191, Accuracy: 85.31%\n",
      "Batch 43, Loss: 0.846295, Accuracy: 85.43%\n",
      "Batch 44, Loss: 0.915969, Accuracy: 85.33%\n",
      "Batch 45, Loss: 0.888581, Accuracy: 85.35%\n",
      "Batch 46, Loss: 0.829255, Accuracy: 85.50%\n",
      "Batch 47, Loss: 0.870340, Accuracy: 85.54%\n",
      "Batch 48, Loss: 0.899344, Accuracy: 85.51%\n",
      "Batch 49, Loss: 0.886458, Accuracy: 85.52%\n",
      "Batch 50, Loss: 0.902730, Accuracy: 85.53%\n",
      "Batch 51, Loss: 0.877993, Accuracy: 85.57%\n",
      "Batch 52, Loss: 0.892448, Accuracy: 85.58%\n",
      "Batch 53, Loss: 0.853700, Accuracy: 85.64%\n",
      "Batch 54, Loss: 0.917399, Accuracy: 85.59%\n",
      "Batch 55, Loss: 0.871282, Accuracy: 85.65%\n",
      "Batch 56, Loss: 0.839334, Accuracy: 85.74%\n",
      "Batch 57, Loss: 0.909027, Accuracy: 85.72%\n",
      "Batch 58, Loss: 0.916060, Accuracy: 85.67%\n",
      "Batch 59, Loss: 0.806125, Accuracy: 85.86%\n",
      "Batch 60, Loss: 0.915734, Accuracy: 85.83%\n",
      "Batch 61, Loss: 0.894367, Accuracy: 85.84%\n",
      "Batch 62, Loss: 0.898236, Accuracy: 85.79%\n",
      "Batch 63, Loss: 0.903283, Accuracy: 85.79%\n",
      "Batch 64, Loss: 0.824087, Accuracy: 85.89%\n",
      "Batch 65, Loss: 0.874378, Accuracy: 85.94%\n",
      "Batch 66, Loss: 0.971306, Accuracy: 85.80%\n",
      "Batch 67, Loss: 0.905197, Accuracy: 85.80%\n",
      "Batch 68, Loss: 0.908841, Accuracy: 85.78%\n",
      "Batch 69, Loss: 0.894789, Accuracy: 85.76%\n",
      "Batch 70, Loss: 0.920777, Accuracy: 85.71%\n",
      "Batch 71, Loss: 0.920345, Accuracy: 85.65%\n",
      "Batch 72, Loss: 0.856511, Accuracy: 85.72%\n",
      "Batch 73, Loss: 0.957668, Accuracy: 85.62%\n",
      "Batch 74, Loss: 0.903534, Accuracy: 85.60%\n",
      "Batch 75, Loss: 0.893718, Accuracy: 85.60%\n",
      "Batch 76, Loss: 0.866318, Accuracy: 85.65%\n",
      "Batch 77, Loss: 0.879450, Accuracy: 85.69%\n",
      "Batch 78, Loss: 0.863905, Accuracy: 85.72%\n",
      "Batch 79, Loss: 0.910175, Accuracy: 85.68%\n",
      "Batch 80, Loss: 0.796581, Accuracy: 85.80%\n",
      "Batch 81, Loss: 0.882433, Accuracy: 85.78%\n",
      "Batch 82, Loss: 0.903913, Accuracy: 85.75%\n",
      "Batch 83, Loss: 0.864505, Accuracy: 85.77%\n",
      "Batch 84, Loss: 0.893517, Accuracy: 85.75%\n",
      "Batch 85, Loss: 0.842901, Accuracy: 85.81%\n",
      "Batch 86, Loss: 0.898418, Accuracy: 85.79%\n",
      "Batch 87, Loss: 0.788892, Accuracy: 85.90%\n",
      "Batch 88, Loss: 0.950913, Accuracy: 85.81%\n",
      "Batch 89, Loss: 0.844240, Accuracy: 85.85%\n",
      "Batch 90, Loss: 0.899338, Accuracy: 85.83%\n",
      "Batch 91, Loss: 0.896299, Accuracy: 85.82%\n",
      "Batch 92, Loss: 0.899137, Accuracy: 85.78%\n",
      "Batch 93, Loss: 0.883164, Accuracy: 85.79%\n",
      "Batch 94, Loss: 0.961944, Accuracy: 85.69%\n",
      "Batch 95, Loss: 0.836664, Accuracy: 85.74%\n",
      "Batch 96, Loss: 0.834085, Accuracy: 85.79%\n",
      "Batch 97, Loss: 0.933024, Accuracy: 85.73%\n",
      "Batch 98, Loss: 0.904897, Accuracy: 85.70%\n",
      "Batch 99, Loss: 0.861228, Accuracy: 85.72%\n",
      "Batch 100, Loss: 0.843121, Accuracy: 85.77%\n",
      "Batch 101, Loss: 0.906068, Accuracy: 85.74%\n",
      "Batch 102, Loss: 0.890446, Accuracy: 85.74%\n",
      "Batch 103, Loss: 0.853027, Accuracy: 85.77%\n",
      "Batch 104, Loss: 0.905094, Accuracy: 85.76%\n",
      "Batch 105, Loss: 0.871950, Accuracy: 85.76%\n",
      "Batch 106, Loss: 0.808314, Accuracy: 85.83%\n",
      "Batch 107, Loss: 0.907575, Accuracy: 85.82%\n",
      "Batch 108, Loss: 0.892495, Accuracy: 85.82%\n",
      "Batch 109, Loss: 0.911324, Accuracy: 85.81%\n",
      "Batch 110, Loss: 0.833007, Accuracy: 85.87%\n",
      "Batch 111, Loss: 0.872472, Accuracy: 85.90%\n",
      "Batch 112, Loss: 0.897017, Accuracy: 85.87%\n",
      "Batch 113, Loss: 0.873891, Accuracy: 85.87%\n",
      "Batch 114, Loss: 0.849667, Accuracy: 85.90%\n",
      "Batch 115, Loss: 0.826286, Accuracy: 85.95%\n",
      "Batch 116, Loss: 0.832333, Accuracy: 85.99%\n",
      "Batch 117, Loss: 0.883198, Accuracy: 86.00%\n",
      "Batch 118, Loss: 0.835435, Accuracy: 86.06%\n",
      "Batch 119, Loss: 0.832873, Accuracy: 86.10%\n",
      "Batch 120, Loss: 0.950197, Accuracy: 86.03%\n",
      "Batch 121, Loss: 0.942554, Accuracy: 85.99%\n",
      "Batch 122, Loss: 0.867133, Accuracy: 86.01%\n",
      "Batch 123, Loss: 0.921667, Accuracy: 85.99%\n",
      "Batch 124, Loss: 0.862880, Accuracy: 86.01%\n",
      "Batch 125, Loss: 0.890294, Accuracy: 85.99%\n",
      "Batch 126, Loss: 0.854043, Accuracy: 86.02%\n",
      "Batch 127, Loss: 0.862965, Accuracy: 86.05%\n",
      "Batch 128, Loss: 0.803917, Accuracy: 86.12%\n",
      "Batch 129, Loss: 0.899962, Accuracy: 86.11%\n",
      "Batch 130, Loss: 0.846083, Accuracy: 86.13%\n",
      "Batch 131, Loss: 0.855872, Accuracy: 86.16%\n",
      "Batch 132, Loss: 0.904782, Accuracy: 86.15%\n",
      "Batch 133, Loss: 0.840564, Accuracy: 86.18%\n",
      "Batch 134, Loss: 0.939688, Accuracy: 86.16%\n",
      "Batch 135, Loss: 0.842890, Accuracy: 86.19%\n",
      "Batch 136, Loss: 0.858035, Accuracy: 86.21%\n",
      "Batch 137, Loss: 0.863239, Accuracy: 86.23%\n",
      "Batch 138, Loss: 0.832176, Accuracy: 86.27%\n",
      "Batch 139, Loss: 0.839259, Accuracy: 86.29%\n",
      "Batch 140, Loss: 0.876145, Accuracy: 86.28%\n",
      "Batch 141, Loss: 0.875425, Accuracy: 86.28%\n",
      "Batch 142, Loss: 0.892786, Accuracy: 86.28%\n",
      "Batch 143, Loss: 0.808682, Accuracy: 86.33%\n",
      "Batch 144, Loss: 0.878407, Accuracy: 86.33%\n",
      "Batch 145, Loss: 0.874409, Accuracy: 86.34%\n",
      "Batch 146, Loss: 0.886209, Accuracy: 86.33%\n",
      "Batch 147, Loss: 0.872984, Accuracy: 86.33%\n",
      "Batch 148, Loss: 0.873786, Accuracy: 86.35%\n",
      "Batch 149, Loss: 0.955686, Accuracy: 86.29%\n",
      "Batch 150, Loss: 0.947313, Accuracy: 86.24%\n",
      "Batch 151, Loss: 0.826988, Accuracy: 86.27%\n",
      "Batch 152, Loss: 0.892786, Accuracy: 86.27%\n",
      "Batch 153, Loss: 0.924602, Accuracy: 86.23%\n",
      "Batch 154, Loss: 0.911238, Accuracy: 86.21%\n",
      "Batch 155, Loss: 0.879909, Accuracy: 86.21%\n",
      "Batch 156, Loss: 0.880512, Accuracy: 86.22%\n",
      "Batch 157, Loss: 0.915968, Accuracy: 86.20%\n",
      "Batch 158, Loss: 0.946344, Accuracy: 86.15%\n",
      "Batch 159, Loss: 0.891373, Accuracy: 86.13%\n",
      "Batch 160, Loss: 0.871940, Accuracy: 86.14%\n",
      "Batch 161, Loss: 0.906219, Accuracy: 86.11%\n",
      "Batch 162, Loss: 0.867713, Accuracy: 86.11%\n",
      "Batch 163, Loss: 0.921638, Accuracy: 86.09%\n",
      "Batch 164, Loss: 0.954185, Accuracy: 86.04%\n",
      "Batch 165, Loss: 0.913616, Accuracy: 86.02%\n",
      "Batch 166, Loss: 0.881470, Accuracy: 86.01%\n",
      "Batch 167, Loss: 0.844592, Accuracy: 86.04%\n",
      "Batch 168, Loss: 0.894072, Accuracy: 86.04%\n",
      "Batch 169, Loss: 0.808485, Accuracy: 86.09%\n",
      "Batch 170, Loss: 0.936726, Accuracy: 86.06%\n",
      "Batch 171, Loss: 0.864826, Accuracy: 86.07%\n",
      "Batch 172, Loss: 0.887219, Accuracy: 86.07%\n",
      "Batch 173, Loss: 0.911818, Accuracy: 86.05%\n",
      "Batch 174, Loss: 0.886909, Accuracy: 86.05%\n",
      "Batch 175, Loss: 0.921251, Accuracy: 86.04%\n",
      "Batch 176, Loss: 0.868441, Accuracy: 86.05%\n",
      "Batch 177, Loss: 0.887251, Accuracy: 86.05%\n",
      "Batch 178, Loss: 0.858057, Accuracy: 86.07%\n",
      "Batch 179, Loss: 0.916307, Accuracy: 86.04%\n",
      "Batch 180, Loss: 0.919257, Accuracy: 86.02%\n",
      "Batch 181, Loss: 0.902825, Accuracy: 86.00%\n",
      "Batch 182, Loss: 0.845181, Accuracy: 86.03%\n",
      "Batch 183, Loss: 0.860945, Accuracy: 86.05%\n",
      "Batch 184, Loss: 0.827401, Accuracy: 86.08%\n",
      "Batch 185, Loss: 0.853098, Accuracy: 86.10%\n",
      "Batch 186, Loss: 0.927313, Accuracy: 86.08%\n",
      "Batch 187, Loss: 0.841932, Accuracy: 86.11%\n",
      "Batch 188, Loss: 0.844683, Accuracy: 86.15%\n",
      "Batch 189, Loss: 0.884751, Accuracy: 86.14%\n",
      "Batch 190, Loss: 0.968441, Accuracy: 86.10%\n",
      "Batch 191, Loss: 0.998921, Accuracy: 86.04%\n",
      "Batch 192, Loss: 0.886719, Accuracy: 86.06%\n",
      "Batch 193, Loss: 0.828772, Accuracy: 86.09%\n",
      "Batch 194, Loss: 0.883095, Accuracy: 86.09%\n",
      "Batch 195, Loss: 0.904060, Accuracy: 86.08%\n",
      "Batch 196, Loss: 0.889032, Accuracy: 86.08%\n",
      "Batch 197, Loss: 0.908522, Accuracy: 86.06%\n",
      "Batch 198, Loss: 0.834061, Accuracy: 86.09%\n",
      "Batch 199, Loss: 0.812883, Accuracy: 86.13%\n",
      "Batch 200, Loss: 0.853171, Accuracy: 86.15%\n",
      "Batch 201, Loss: 0.869647, Accuracy: 86.16%\n",
      "Batch 202, Loss: 0.920878, Accuracy: 86.13%\n",
      "Batch 203, Loss: 0.892020, Accuracy: 86.13%\n",
      "Batch 204, Loss: 0.899759, Accuracy: 86.13%\n",
      "Batch 205, Loss: 0.966618, Accuracy: 86.09%\n",
      "Batch 206, Loss: 0.916442, Accuracy: 86.07%\n",
      "Batch 207, Loss: 0.856214, Accuracy: 86.10%\n",
      "Batch 208, Loss: 0.875849, Accuracy: 86.11%\n",
      "Batch 209, Loss: 0.834447, Accuracy: 86.12%\n",
      "Batch 210, Loss: 0.861474, Accuracy: 86.13%\n",
      "Batch 211, Loss: 0.953733, Accuracy: 86.10%\n",
      "Batch 212, Loss: 0.882714, Accuracy: 86.10%\n",
      "Batch 213, Loss: 0.897944, Accuracy: 86.10%\n",
      "Training - Epoch 75, Loss: 0.884219, Accuracy: 86.10%\n",
      "Validation Batch 1, Loss: 0.854880, Accuracy: 89.06%\n",
      "Validation Batch 2, Loss: 0.844702, Accuracy: 90.62%\n",
      "Validation Batch 3, Loss: 0.923430, Accuracy: 87.50%\n",
      "Validation Batch 4, Loss: 0.871926, Accuracy: 87.89%\n",
      "Validation Batch 5, Loss: 0.858630, Accuracy: 88.44%\n",
      "Validation Batch 6, Loss: 0.829929, Accuracy: 89.32%\n",
      "Validation Batch 7, Loss: 0.841843, Accuracy: 89.51%\n",
      "Validation Batch 8, Loss: 0.941905, Accuracy: 88.28%\n",
      "Validation Batch 9, Loss: 0.914362, Accuracy: 87.85%\n",
      "Validation Batch 10, Loss: 0.877974, Accuracy: 87.66%\n",
      "Validation Batch 11, Loss: 0.865645, Accuracy: 87.50%\n",
      "Validation Batch 12, Loss: 0.860945, Accuracy: 87.63%\n",
      "Validation Batch 13, Loss: 0.879626, Accuracy: 87.62%\n",
      "Validation Batch 14, Loss: 0.899016, Accuracy: 87.39%\n",
      "Validation Batch 15, Loss: 0.857974, Accuracy: 87.40%\n",
      "Validation Batch 16, Loss: 0.873843, Accuracy: 87.50%\n",
      "Validation Batch 17, Loss: 0.918604, Accuracy: 87.22%\n",
      "Validation Batch 18, Loss: 0.857759, Accuracy: 87.33%\n",
      "Validation Batch 19, Loss: 0.923557, Accuracy: 87.09%\n",
      "Validation Batch 20, Loss: 0.943594, Accuracy: 86.72%\n",
      "Validation Batch 21, Loss: 0.899486, Accuracy: 86.68%\n",
      "Validation Batch 22, Loss: 0.888636, Accuracy: 86.65%\n",
      "Validation Batch 23, Loss: 0.917172, Accuracy: 86.62%\n",
      "Validation Batch 24, Loss: 0.902338, Accuracy: 86.59%\n",
      "Validation Batch 25, Loss: 0.851484, Accuracy: 86.75%\n",
      "Validation Batch 26, Loss: 0.885187, Accuracy: 86.78%\n",
      "Validation Batch 27, Loss: 0.844844, Accuracy: 86.85%\n",
      "Validation - Epoch 75, Loss: 0.882566, Accuracy: 86.85%\n",
      "Patience—3\n",
      "Epoch 76\n",
      "Batch 1, Loss: 0.892345, Accuracy: 85.94%\n",
      "Batch 2, Loss: 0.889278, Accuracy: 85.94%\n",
      "Batch 3, Loss: 0.900313, Accuracy: 84.38%\n",
      "Batch 4, Loss: 0.825449, Accuracy: 86.72%\n",
      "Batch 5, Loss: 0.889284, Accuracy: 85.94%\n",
      "Batch 6, Loss: 0.875103, Accuracy: 85.94%\n",
      "Batch 7, Loss: 0.909864, Accuracy: 85.27%\n",
      "Batch 8, Loss: 0.834576, Accuracy: 86.13%\n",
      "Batch 9, Loss: 0.845810, Accuracy: 86.46%\n",
      "Batch 10, Loss: 0.908757, Accuracy: 86.09%\n",
      "Batch 11, Loss: 0.894428, Accuracy: 85.94%\n",
      "Batch 12, Loss: 0.876080, Accuracy: 86.07%\n",
      "Batch 13, Loss: 0.918042, Accuracy: 85.94%\n",
      "Batch 14, Loss: 0.908911, Accuracy: 85.71%\n",
      "Batch 15, Loss: 0.922737, Accuracy: 85.62%\n",
      "Batch 16, Loss: 0.895100, Accuracy: 85.35%\n",
      "Batch 17, Loss: 0.840010, Accuracy: 85.66%\n",
      "Batch 18, Loss: 0.867438, Accuracy: 85.76%\n",
      "Batch 19, Loss: 0.875510, Accuracy: 85.86%\n",
      "Batch 20, Loss: 0.878906, Accuracy: 85.86%\n",
      "Batch 21, Loss: 0.856923, Accuracy: 85.94%\n",
      "Batch 22, Loss: 0.861871, Accuracy: 86.01%\n",
      "Batch 23, Loss: 0.901943, Accuracy: 85.87%\n",
      "Batch 24, Loss: 0.817833, Accuracy: 86.20%\n",
      "Batch 25, Loss: 0.928615, Accuracy: 85.94%\n",
      "Batch 26, Loss: 0.879880, Accuracy: 85.94%\n",
      "Batch 27, Loss: 0.811296, Accuracy: 86.23%\n",
      "Batch 28, Loss: 0.814333, Accuracy: 86.50%\n",
      "Batch 29, Loss: 0.898435, Accuracy: 86.42%\n",
      "Batch 30, Loss: 0.873048, Accuracy: 86.51%\n",
      "Batch 31, Loss: 0.914915, Accuracy: 86.34%\n",
      "Batch 32, Loss: 0.842208, Accuracy: 86.47%\n",
      "Batch 33, Loss: 0.809047, Accuracy: 86.70%\n",
      "Batch 34, Loss: 0.893221, Accuracy: 86.63%\n",
      "Batch 35, Loss: 0.909966, Accuracy: 86.52%\n",
      "Batch 36, Loss: 0.867787, Accuracy: 86.55%\n",
      "Batch 37, Loss: 0.874516, Accuracy: 86.57%\n",
      "Batch 38, Loss: 0.945826, Accuracy: 86.35%\n",
      "Batch 39, Loss: 0.887211, Accuracy: 86.34%\n",
      "Batch 40, Loss: 0.841890, Accuracy: 86.48%\n",
      "Batch 41, Loss: 0.926911, Accuracy: 86.36%\n",
      "Batch 42, Loss: 0.909955, Accuracy: 86.27%\n",
      "Batch 43, Loss: 0.935723, Accuracy: 86.16%\n",
      "Batch 44, Loss: 0.912085, Accuracy: 86.08%\n",
      "Batch 45, Loss: 0.934494, Accuracy: 85.90%\n",
      "Batch 46, Loss: 0.922770, Accuracy: 85.80%\n",
      "Batch 47, Loss: 0.822273, Accuracy: 85.97%\n",
      "Batch 48, Loss: 0.861528, Accuracy: 86.00%\n",
      "Batch 49, Loss: 0.928210, Accuracy: 85.91%\n",
      "Batch 50, Loss: 0.919798, Accuracy: 85.78%\n",
      "Batch 51, Loss: 0.895200, Accuracy: 85.75%\n",
      "Batch 52, Loss: 0.881200, Accuracy: 85.76%\n",
      "Batch 53, Loss: 0.838453, Accuracy: 85.85%\n",
      "Batch 54, Loss: 0.896287, Accuracy: 85.82%\n",
      "Batch 55, Loss: 0.911080, Accuracy: 85.77%\n",
      "Batch 56, Loss: 0.854640, Accuracy: 85.83%\n",
      "Batch 57, Loss: 0.893433, Accuracy: 85.77%\n",
      "Batch 58, Loss: 0.876518, Accuracy: 85.80%\n",
      "Batch 59, Loss: 0.901073, Accuracy: 85.81%\n",
      "Batch 60, Loss: 0.841903, Accuracy: 85.89%\n",
      "Batch 61, Loss: 0.902686, Accuracy: 85.86%\n",
      "Batch 62, Loss: 0.911039, Accuracy: 85.81%\n",
      "Batch 63, Loss: 0.858125, Accuracy: 85.86%\n",
      "Batch 64, Loss: 0.822420, Accuracy: 85.94%\n",
      "Batch 65, Loss: 0.907188, Accuracy: 85.89%\n",
      "Batch 66, Loss: 0.870079, Accuracy: 85.94%\n",
      "Batch 67, Loss: 0.942949, Accuracy: 85.82%\n",
      "Batch 68, Loss: 0.943154, Accuracy: 85.73%\n",
      "Batch 69, Loss: 0.923202, Accuracy: 85.69%\n",
      "Batch 70, Loss: 0.835494, Accuracy: 85.78%\n",
      "Batch 71, Loss: 0.833130, Accuracy: 85.87%\n",
      "Batch 72, Loss: 0.914907, Accuracy: 85.83%\n",
      "Batch 73, Loss: 0.930305, Accuracy: 85.77%\n",
      "Batch 74, Loss: 0.844719, Accuracy: 85.83%\n",
      "Batch 75, Loss: 0.911503, Accuracy: 85.81%\n",
      "Batch 76, Loss: 0.853741, Accuracy: 85.88%\n",
      "Batch 77, Loss: 0.926670, Accuracy: 85.84%\n",
      "Batch 78, Loss: 0.908549, Accuracy: 85.80%\n",
      "Batch 79, Loss: 0.914335, Accuracy: 85.78%\n",
      "Batch 80, Loss: 0.952040, Accuracy: 85.68%\n",
      "Batch 81, Loss: 0.862789, Accuracy: 85.73%\n",
      "Batch 82, Loss: 0.950685, Accuracy: 85.63%\n",
      "Batch 83, Loss: 0.876933, Accuracy: 85.64%\n",
      "Batch 84, Loss: 0.860207, Accuracy: 85.68%\n",
      "Batch 85, Loss: 0.918109, Accuracy: 85.61%\n",
      "Batch 86, Loss: 0.823868, Accuracy: 85.70%\n",
      "Batch 87, Loss: 0.892887, Accuracy: 85.69%\n",
      "Batch 88, Loss: 0.865844, Accuracy: 85.69%\n",
      "Batch 89, Loss: 0.933123, Accuracy: 85.64%\n",
      "Batch 90, Loss: 0.848792, Accuracy: 85.68%\n",
      "Batch 91, Loss: 0.839732, Accuracy: 85.73%\n",
      "Batch 92, Loss: 0.916571, Accuracy: 85.72%\n",
      "Batch 93, Loss: 0.873564, Accuracy: 85.72%\n",
      "Batch 94, Loss: 0.843717, Accuracy: 85.77%\n",
      "Batch 95, Loss: 0.896834, Accuracy: 85.77%\n",
      "Batch 96, Loss: 0.824813, Accuracy: 85.86%\n",
      "Batch 97, Loss: 0.900990, Accuracy: 85.86%\n",
      "Batch 98, Loss: 0.931885, Accuracy: 85.81%\n",
      "Batch 99, Loss: 0.920631, Accuracy: 85.78%\n",
      "Batch 100, Loss: 0.868226, Accuracy: 85.80%\n",
      "Batch 101, Loss: 0.866977, Accuracy: 85.81%\n",
      "Batch 102, Loss: 0.950802, Accuracy: 85.75%\n",
      "Batch 103, Loss: 0.869405, Accuracy: 85.77%\n",
      "Batch 104, Loss: 0.878471, Accuracy: 85.77%\n",
      "Batch 105, Loss: 0.875209, Accuracy: 85.79%\n",
      "Batch 106, Loss: 0.898337, Accuracy: 85.78%\n",
      "Batch 107, Loss: 0.924056, Accuracy: 85.73%\n",
      "Batch 108, Loss: 0.827244, Accuracy: 85.79%\n",
      "Batch 109, Loss: 0.877531, Accuracy: 85.78%\n",
      "Batch 110, Loss: 0.860273, Accuracy: 85.81%\n",
      "Batch 111, Loss: 0.855736, Accuracy: 85.84%\n",
      "Batch 112, Loss: 0.871546, Accuracy: 85.85%\n",
      "Batch 113, Loss: 0.803012, Accuracy: 85.94%\n",
      "Batch 114, Loss: 0.980934, Accuracy: 85.86%\n",
      "Batch 115, Loss: 0.924490, Accuracy: 85.80%\n",
      "Batch 116, Loss: 0.972530, Accuracy: 85.74%\n",
      "Batch 117, Loss: 0.915110, Accuracy: 85.72%\n",
      "Batch 118, Loss: 0.860176, Accuracy: 85.74%\n",
      "Batch 119, Loss: 0.917402, Accuracy: 85.70%\n",
      "Batch 120, Loss: 0.801993, Accuracy: 85.77%\n",
      "Batch 121, Loss: 0.898471, Accuracy: 85.76%\n",
      "Batch 122, Loss: 0.914166, Accuracy: 85.76%\n",
      "Batch 123, Loss: 0.932743, Accuracy: 85.72%\n",
      "Batch 124, Loss: 0.871207, Accuracy: 85.74%\n",
      "Batch 125, Loss: 0.917431, Accuracy: 85.70%\n",
      "Batch 126, Loss: 0.877539, Accuracy: 85.71%\n",
      "Batch 127, Loss: 0.898202, Accuracy: 85.70%\n",
      "Batch 128, Loss: 0.882320, Accuracy: 85.69%\n",
      "Batch 129, Loss: 0.877901, Accuracy: 85.71%\n",
      "Batch 130, Loss: 0.854346, Accuracy: 85.76%\n",
      "Batch 131, Loss: 0.853454, Accuracy: 85.79%\n",
      "Batch 132, Loss: 0.874374, Accuracy: 85.81%\n",
      "Batch 133, Loss: 0.914115, Accuracy: 85.80%\n",
      "Batch 134, Loss: 0.864860, Accuracy: 85.81%\n",
      "Batch 135, Loss: 0.870122, Accuracy: 85.81%\n",
      "Batch 136, Loss: 0.914769, Accuracy: 85.79%\n",
      "Batch 137, Loss: 0.875521, Accuracy: 85.78%\n",
      "Batch 138, Loss: 0.903899, Accuracy: 85.76%\n",
      "Batch 139, Loss: 0.928485, Accuracy: 85.72%\n",
      "Batch 140, Loss: 0.921989, Accuracy: 85.69%\n",
      "Batch 141, Loss: 0.905798, Accuracy: 85.67%\n",
      "Batch 142, Loss: 0.840393, Accuracy: 85.72%\n",
      "Batch 143, Loss: 0.859018, Accuracy: 85.74%\n",
      "Batch 144, Loss: 0.889800, Accuracy: 85.73%\n",
      "Batch 145, Loss: 0.961518, Accuracy: 85.68%\n",
      "Batch 146, Loss: 0.836941, Accuracy: 85.71%\n",
      "Batch 147, Loss: 0.907607, Accuracy: 85.69%\n",
      "Batch 148, Loss: 0.860425, Accuracy: 85.71%\n",
      "Batch 149, Loss: 0.879159, Accuracy: 85.71%\n",
      "Batch 150, Loss: 0.855005, Accuracy: 85.72%\n",
      "Batch 151, Loss: 0.841436, Accuracy: 85.74%\n",
      "Batch 152, Loss: 0.848870, Accuracy: 85.77%\n",
      "Batch 153, Loss: 0.830169, Accuracy: 85.80%\n",
      "Batch 154, Loss: 0.846951, Accuracy: 85.84%\n",
      "Batch 155, Loss: 0.841700, Accuracy: 85.88%\n",
      "Batch 156, Loss: 0.860585, Accuracy: 85.90%\n",
      "Batch 157, Loss: 0.885212, Accuracy: 85.90%\n",
      "Batch 158, Loss: 0.815952, Accuracy: 85.95%\n",
      "Batch 159, Loss: 0.908070, Accuracy: 85.94%\n",
      "Batch 160, Loss: 0.859391, Accuracy: 85.96%\n",
      "Batch 161, Loss: 0.886208, Accuracy: 85.97%\n",
      "Batch 162, Loss: 0.894202, Accuracy: 85.96%\n",
      "Batch 163, Loss: 0.861064, Accuracy: 85.98%\n",
      "Batch 164, Loss: 0.854812, Accuracy: 85.99%\n",
      "Batch 165, Loss: 0.902466, Accuracy: 85.97%\n",
      "Batch 166, Loss: 0.921096, Accuracy: 85.94%\n",
      "Batch 167, Loss: 0.862785, Accuracy: 85.97%\n",
      "Batch 168, Loss: 0.869802, Accuracy: 85.97%\n",
      "Batch 169, Loss: 0.903840, Accuracy: 85.97%\n",
      "Batch 170, Loss: 0.865661, Accuracy: 85.99%\n",
      "Batch 171, Loss: 0.904001, Accuracy: 85.97%\n",
      "Batch 172, Loss: 0.876484, Accuracy: 85.97%\n",
      "Batch 173, Loss: 0.966083, Accuracy: 85.92%\n",
      "Batch 174, Loss: 0.900908, Accuracy: 85.92%\n",
      "Batch 175, Loss: 0.909129, Accuracy: 85.89%\n",
      "Batch 176, Loss: 0.844614, Accuracy: 85.90%\n",
      "Batch 177, Loss: 0.893891, Accuracy: 85.89%\n",
      "Batch 178, Loss: 0.887605, Accuracy: 85.89%\n",
      "Batch 179, Loss: 0.923628, Accuracy: 85.88%\n",
      "Batch 180, Loss: 0.886303, Accuracy: 85.87%\n",
      "Batch 181, Loss: 0.857204, Accuracy: 85.89%\n",
      "Batch 182, Loss: 0.963038, Accuracy: 85.83%\n",
      "Batch 183, Loss: 0.903173, Accuracy: 85.83%\n",
      "Batch 184, Loss: 0.937891, Accuracy: 85.79%\n",
      "Batch 185, Loss: 0.864460, Accuracy: 85.81%\n",
      "Batch 186, Loss: 0.896743, Accuracy: 85.80%\n",
      "Batch 187, Loss: 0.848734, Accuracy: 85.83%\n",
      "Batch 188, Loss: 0.899576, Accuracy: 85.81%\n",
      "Batch 189, Loss: 0.956286, Accuracy: 85.77%\n",
      "Batch 190, Loss: 0.914686, Accuracy: 85.76%\n",
      "Batch 191, Loss: 0.907612, Accuracy: 85.75%\n",
      "Batch 192, Loss: 0.882298, Accuracy: 85.76%\n",
      "Batch 193, Loss: 0.883439, Accuracy: 85.77%\n",
      "Batch 194, Loss: 0.882213, Accuracy: 85.77%\n",
      "Batch 195, Loss: 0.896793, Accuracy: 85.77%\n",
      "Batch 196, Loss: 0.905158, Accuracy: 85.76%\n",
      "Batch 197, Loss: 0.863179, Accuracy: 85.79%\n",
      "Batch 198, Loss: 0.846519, Accuracy: 85.80%\n",
      "Batch 199, Loss: 0.874755, Accuracy: 85.81%\n",
      "Batch 200, Loss: 0.848123, Accuracy: 85.84%\n",
      "Batch 201, Loss: 0.827670, Accuracy: 85.88%\n",
      "Batch 202, Loss: 0.910555, Accuracy: 85.87%\n",
      "Batch 203, Loss: 0.940316, Accuracy: 85.84%\n",
      "Batch 204, Loss: 0.845488, Accuracy: 85.85%\n",
      "Batch 205, Loss: 0.823881, Accuracy: 85.88%\n",
      "Batch 206, Loss: 0.833530, Accuracy: 85.91%\n",
      "Batch 207, Loss: 0.874454, Accuracy: 85.91%\n",
      "Batch 208, Loss: 0.937900, Accuracy: 85.88%\n",
      "Batch 209, Loss: 0.824537, Accuracy: 85.92%\n",
      "Batch 210, Loss: 0.876423, Accuracy: 85.92%\n",
      "Batch 211, Loss: 0.865532, Accuracy: 85.93%\n",
      "Batch 212, Loss: 0.912325, Accuracy: 85.92%\n",
      "Batch 213, Loss: 0.959666, Accuracy: 85.88%\n",
      "Training - Epoch 76, Loss: 0.884923, Accuracy: 85.88%\n",
      "Validation Batch 1, Loss: 0.857144, Accuracy: 90.62%\n",
      "Validation Batch 2, Loss: 0.828923, Accuracy: 92.19%\n",
      "Validation Batch 3, Loss: 0.917809, Accuracy: 89.06%\n",
      "Validation Batch 4, Loss: 0.874132, Accuracy: 88.28%\n",
      "Validation Batch 5, Loss: 0.855284, Accuracy: 88.75%\n",
      "Validation Batch 6, Loss: 0.827351, Accuracy: 89.58%\n",
      "Validation Batch 7, Loss: 0.838294, Accuracy: 89.96%\n",
      "Validation Batch 8, Loss: 0.943679, Accuracy: 88.67%\n",
      "Validation Batch 9, Loss: 0.913012, Accuracy: 88.19%\n",
      "Validation Batch 10, Loss: 0.876265, Accuracy: 87.97%\n",
      "Validation Batch 11, Loss: 0.868899, Accuracy: 87.93%\n",
      "Validation Batch 12, Loss: 0.862795, Accuracy: 88.15%\n",
      "Validation Batch 13, Loss: 0.868354, Accuracy: 88.10%\n",
      "Validation Batch 14, Loss: 0.892851, Accuracy: 87.83%\n",
      "Validation Batch 15, Loss: 0.857283, Accuracy: 87.92%\n",
      "Validation Batch 16, Loss: 0.864154, Accuracy: 88.18%\n",
      "Validation Batch 17, Loss: 0.902948, Accuracy: 87.96%\n",
      "Validation Batch 18, Loss: 0.860340, Accuracy: 88.02%\n",
      "Validation Batch 19, Loss: 0.918769, Accuracy: 87.66%\n",
      "Validation Batch 20, Loss: 0.937344, Accuracy: 87.34%\n",
      "Validation Batch 21, Loss: 0.897654, Accuracy: 87.28%\n",
      "Validation Batch 22, Loss: 0.885495, Accuracy: 87.14%\n",
      "Validation Batch 23, Loss: 0.912526, Accuracy: 86.89%\n",
      "Validation Batch 24, Loss: 0.893563, Accuracy: 86.91%\n",
      "Validation Batch 25, Loss: 0.848860, Accuracy: 87.06%\n",
      "Validation Batch 26, Loss: 0.878513, Accuracy: 87.08%\n",
      "Validation Batch 27, Loss: 0.838931, Accuracy: 87.20%\n",
      "Validation - Epoch 76, Loss: 0.878562, Accuracy: 87.20%\n",
      "Patience—0\n",
      "Epoch 77\n",
      "Batch 1, Loss: 0.932486, Accuracy: 81.25%\n",
      "Batch 2, Loss: 0.888657, Accuracy: 82.81%\n",
      "Batch 3, Loss: 0.877405, Accuracy: 84.90%\n",
      "Batch 4, Loss: 0.898725, Accuracy: 85.16%\n",
      "Batch 5, Loss: 0.869269, Accuracy: 85.31%\n",
      "Batch 6, Loss: 0.930663, Accuracy: 84.64%\n",
      "Batch 7, Loss: 0.875202, Accuracy: 85.27%\n",
      "Batch 8, Loss: 0.862481, Accuracy: 85.74%\n",
      "Batch 9, Loss: 0.893880, Accuracy: 85.59%\n",
      "Batch 10, Loss: 0.879629, Accuracy: 85.62%\n",
      "Batch 11, Loss: 0.868119, Accuracy: 85.80%\n",
      "Batch 12, Loss: 0.846532, Accuracy: 86.33%\n",
      "Batch 13, Loss: 0.943344, Accuracy: 85.94%\n",
      "Batch 14, Loss: 0.850739, Accuracy: 86.27%\n",
      "Batch 15, Loss: 0.863560, Accuracy: 86.56%\n",
      "Batch 16, Loss: 0.871458, Accuracy: 86.52%\n",
      "Batch 17, Loss: 0.873680, Accuracy: 86.58%\n",
      "Batch 18, Loss: 0.884977, Accuracy: 86.63%\n",
      "Batch 19, Loss: 0.845120, Accuracy: 86.84%\n",
      "Batch 20, Loss: 0.882235, Accuracy: 86.80%\n",
      "Batch 21, Loss: 0.844506, Accuracy: 86.90%\n",
      "Batch 22, Loss: 0.842749, Accuracy: 87.07%\n",
      "Batch 23, Loss: 0.908886, Accuracy: 86.89%\n",
      "Batch 24, Loss: 0.897089, Accuracy: 86.78%\n",
      "Batch 25, Loss: 0.854183, Accuracy: 86.88%\n",
      "Batch 26, Loss: 0.955794, Accuracy: 86.60%\n",
      "Batch 27, Loss: 0.861698, Accuracy: 86.69%\n",
      "Batch 28, Loss: 0.905360, Accuracy: 86.61%\n",
      "Batch 29, Loss: 0.859093, Accuracy: 86.69%\n",
      "Batch 30, Loss: 0.876279, Accuracy: 86.67%\n",
      "Batch 31, Loss: 0.832165, Accuracy: 86.84%\n",
      "Batch 32, Loss: 0.901877, Accuracy: 86.82%\n",
      "Batch 33, Loss: 0.821720, Accuracy: 86.93%\n",
      "Batch 34, Loss: 0.963813, Accuracy: 86.72%\n",
      "Batch 35, Loss: 0.945862, Accuracy: 86.52%\n",
      "Batch 36, Loss: 0.898750, Accuracy: 86.46%\n",
      "Batch 37, Loss: 0.861888, Accuracy: 86.49%\n",
      "Batch 38, Loss: 0.864283, Accuracy: 86.51%\n",
      "Batch 39, Loss: 0.892691, Accuracy: 86.50%\n",
      "Batch 40, Loss: 0.907805, Accuracy: 86.41%\n",
      "Batch 41, Loss: 0.832254, Accuracy: 86.55%\n",
      "Batch 42, Loss: 0.869288, Accuracy: 86.57%\n",
      "Batch 43, Loss: 0.904400, Accuracy: 86.48%\n",
      "Batch 44, Loss: 0.863718, Accuracy: 86.51%\n",
      "Batch 45, Loss: 0.939936, Accuracy: 86.39%\n",
      "Batch 46, Loss: 0.875791, Accuracy: 86.41%\n",
      "Batch 47, Loss: 0.819551, Accuracy: 86.54%\n",
      "Batch 48, Loss: 0.927013, Accuracy: 86.43%\n",
      "Batch 49, Loss: 0.915474, Accuracy: 86.32%\n",
      "Batch 50, Loss: 0.894194, Accuracy: 86.28%\n",
      "Batch 51, Loss: 0.899304, Accuracy: 86.27%\n",
      "Batch 52, Loss: 0.840711, Accuracy: 86.36%\n",
      "Batch 53, Loss: 0.902646, Accuracy: 86.32%\n",
      "Batch 54, Loss: 0.867588, Accuracy: 86.34%\n",
      "Batch 55, Loss: 0.930908, Accuracy: 86.22%\n",
      "Batch 56, Loss: 0.869288, Accuracy: 86.22%\n",
      "Batch 57, Loss: 0.829416, Accuracy: 86.29%\n",
      "Batch 58, Loss: 0.843151, Accuracy: 86.37%\n",
      "Batch 59, Loss: 0.821578, Accuracy: 86.49%\n",
      "Batch 60, Loss: 0.885101, Accuracy: 86.51%\n",
      "Batch 61, Loss: 0.879328, Accuracy: 86.53%\n",
      "Batch 62, Loss: 0.935458, Accuracy: 86.42%\n",
      "Batch 63, Loss: 0.934858, Accuracy: 86.31%\n",
      "Batch 64, Loss: 0.882503, Accuracy: 86.33%\n",
      "Batch 65, Loss: 0.872103, Accuracy: 86.32%\n",
      "Batch 66, Loss: 0.899420, Accuracy: 86.29%\n",
      "Batch 67, Loss: 0.942022, Accuracy: 86.17%\n",
      "Batch 68, Loss: 0.947044, Accuracy: 86.08%\n",
      "Batch 69, Loss: 0.860375, Accuracy: 86.12%\n",
      "Batch 70, Loss: 0.925966, Accuracy: 86.07%\n",
      "Batch 71, Loss: 0.901343, Accuracy: 86.03%\n",
      "Batch 72, Loss: 0.832293, Accuracy: 86.09%\n",
      "Batch 73, Loss: 0.831007, Accuracy: 86.15%\n",
      "Batch 74, Loss: 0.831154, Accuracy: 86.23%\n",
      "Batch 75, Loss: 0.810678, Accuracy: 86.33%\n",
      "Batch 76, Loss: 0.916630, Accuracy: 86.25%\n",
      "Batch 77, Loss: 0.907573, Accuracy: 86.22%\n",
      "Batch 78, Loss: 0.891380, Accuracy: 86.18%\n",
      "Batch 79, Loss: 0.927316, Accuracy: 86.12%\n",
      "Batch 80, Loss: 0.849621, Accuracy: 86.17%\n",
      "Batch 81, Loss: 0.841342, Accuracy: 86.25%\n",
      "Batch 82, Loss: 0.831520, Accuracy: 86.32%\n",
      "Batch 83, Loss: 0.896389, Accuracy: 86.30%\n",
      "Batch 84, Loss: 0.851252, Accuracy: 86.31%\n",
      "Batch 85, Loss: 0.888167, Accuracy: 86.31%\n",
      "Batch 86, Loss: 0.899375, Accuracy: 86.30%\n",
      "Batch 87, Loss: 0.830886, Accuracy: 86.37%\n",
      "Batch 88, Loss: 0.860650, Accuracy: 86.40%\n",
      "Batch 89, Loss: 0.943408, Accuracy: 86.31%\n",
      "Batch 90, Loss: 0.853702, Accuracy: 86.35%\n",
      "Batch 91, Loss: 0.837752, Accuracy: 86.42%\n",
      "Batch 92, Loss: 0.913747, Accuracy: 86.40%\n",
      "Batch 93, Loss: 0.882372, Accuracy: 86.39%\n",
      "Batch 94, Loss: 0.948264, Accuracy: 86.32%\n",
      "Batch 95, Loss: 0.864674, Accuracy: 86.33%\n",
      "Batch 96, Loss: 0.855830, Accuracy: 86.34%\n",
      "Batch 97, Loss: 0.894532, Accuracy: 86.34%\n",
      "Batch 98, Loss: 0.855659, Accuracy: 86.37%\n",
      "Batch 99, Loss: 0.942078, Accuracy: 86.28%\n",
      "Batch 100, Loss: 0.854398, Accuracy: 86.30%\n",
      "Batch 101, Loss: 0.934040, Accuracy: 86.26%\n",
      "Batch 102, Loss: 0.892289, Accuracy: 86.24%\n",
      "Batch 103, Loss: 0.813498, Accuracy: 86.32%\n",
      "Batch 104, Loss: 0.828931, Accuracy: 86.36%\n",
      "Batch 105, Loss: 0.853891, Accuracy: 86.41%\n",
      "Batch 106, Loss: 0.872762, Accuracy: 86.42%\n",
      "Batch 107, Loss: 0.893641, Accuracy: 86.40%\n",
      "Batch 108, Loss: 0.855624, Accuracy: 86.44%\n",
      "Batch 109, Loss: 0.848169, Accuracy: 86.48%\n",
      "Batch 110, Loss: 0.868841, Accuracy: 86.49%\n",
      "Batch 111, Loss: 0.889481, Accuracy: 86.50%\n",
      "Batch 112, Loss: 0.964646, Accuracy: 86.41%\n",
      "Batch 113, Loss: 0.870637, Accuracy: 86.42%\n",
      "Batch 114, Loss: 0.824894, Accuracy: 86.47%\n",
      "Batch 115, Loss: 0.836711, Accuracy: 86.51%\n",
      "Batch 116, Loss: 0.849584, Accuracy: 86.53%\n",
      "Batch 117, Loss: 0.777138, Accuracy: 86.63%\n",
      "Batch 118, Loss: 0.859188, Accuracy: 86.64%\n",
      "Batch 119, Loss: 0.954045, Accuracy: 86.57%\n",
      "Batch 120, Loss: 0.864416, Accuracy: 86.56%\n",
      "Batch 121, Loss: 0.862719, Accuracy: 86.58%\n",
      "Batch 122, Loss: 0.872414, Accuracy: 86.60%\n",
      "Batch 123, Loss: 0.924968, Accuracy: 86.57%\n",
      "Batch 124, Loss: 0.917318, Accuracy: 86.54%\n",
      "Batch 125, Loss: 0.889867, Accuracy: 86.54%\n",
      "Batch 126, Loss: 0.847363, Accuracy: 86.56%\n",
      "Batch 127, Loss: 0.850405, Accuracy: 86.58%\n",
      "Batch 128, Loss: 0.844078, Accuracy: 86.61%\n",
      "Batch 129, Loss: 0.909445, Accuracy: 86.58%\n",
      "Batch 130, Loss: 0.843516, Accuracy: 86.60%\n",
      "Batch 131, Loss: 0.845148, Accuracy: 86.63%\n",
      "Batch 132, Loss: 0.866199, Accuracy: 86.64%\n",
      "Batch 133, Loss: 0.853832, Accuracy: 86.65%\n",
      "Batch 134, Loss: 0.916242, Accuracy: 86.63%\n",
      "Batch 135, Loss: 0.856777, Accuracy: 86.64%\n",
      "Batch 136, Loss: 0.862505, Accuracy: 86.66%\n",
      "Batch 137, Loss: 0.937579, Accuracy: 86.61%\n",
      "Batch 138, Loss: 0.876828, Accuracy: 86.59%\n",
      "Batch 139, Loss: 0.918589, Accuracy: 86.56%\n",
      "Batch 140, Loss: 0.916715, Accuracy: 86.53%\n",
      "Batch 141, Loss: 0.868787, Accuracy: 86.56%\n",
      "Batch 142, Loss: 0.963898, Accuracy: 86.50%\n",
      "Batch 143, Loss: 0.941610, Accuracy: 86.45%\n",
      "Batch 144, Loss: 0.925067, Accuracy: 86.41%\n",
      "Batch 145, Loss: 0.926293, Accuracy: 86.39%\n",
      "Batch 146, Loss: 0.957371, Accuracy: 86.33%\n",
      "Batch 147, Loss: 0.866760, Accuracy: 86.34%\n",
      "Batch 148, Loss: 0.912297, Accuracy: 86.32%\n",
      "Batch 149, Loss: 0.904873, Accuracy: 86.30%\n",
      "Batch 150, Loss: 0.956997, Accuracy: 86.25%\n",
      "Batch 151, Loss: 0.875836, Accuracy: 86.26%\n",
      "Batch 152, Loss: 0.852429, Accuracy: 86.29%\n",
      "Batch 153, Loss: 0.871468, Accuracy: 86.29%\n",
      "Batch 154, Loss: 0.867269, Accuracy: 86.30%\n",
      "Batch 155, Loss: 0.900010, Accuracy: 86.29%\n",
      "Batch 156, Loss: 0.862352, Accuracy: 86.31%\n",
      "Batch 157, Loss: 0.962595, Accuracy: 86.27%\n",
      "Batch 158, Loss: 0.905163, Accuracy: 86.24%\n",
      "Batch 159, Loss: 0.910404, Accuracy: 86.22%\n",
      "Batch 160, Loss: 0.956196, Accuracy: 86.16%\n",
      "Batch 161, Loss: 0.911578, Accuracy: 86.15%\n",
      "Batch 162, Loss: 0.879614, Accuracy: 86.16%\n",
      "Batch 163, Loss: 0.975046, Accuracy: 86.09%\n",
      "Batch 164, Loss: 0.942873, Accuracy: 86.04%\n",
      "Batch 165, Loss: 0.863885, Accuracy: 86.05%\n",
      "Batch 166, Loss: 0.850680, Accuracy: 86.07%\n",
      "Batch 167, Loss: 0.883606, Accuracy: 86.07%\n",
      "Batch 168, Loss: 0.888218, Accuracy: 86.07%\n",
      "Batch 169, Loss: 0.842228, Accuracy: 86.10%\n",
      "Batch 170, Loss: 0.810517, Accuracy: 86.16%\n",
      "Batch 171, Loss: 0.901551, Accuracy: 86.15%\n",
      "Batch 172, Loss: 0.875249, Accuracy: 86.15%\n",
      "Batch 173, Loss: 0.917446, Accuracy: 86.12%\n",
      "Batch 174, Loss: 0.954110, Accuracy: 86.07%\n",
      "Batch 175, Loss: 0.893881, Accuracy: 86.07%\n",
      "Batch 176, Loss: 0.898153, Accuracy: 86.06%\n",
      "Batch 177, Loss: 0.854794, Accuracy: 86.08%\n",
      "Batch 178, Loss: 0.908519, Accuracy: 86.06%\n",
      "Batch 179, Loss: 0.843887, Accuracy: 86.09%\n",
      "Batch 180, Loss: 0.849454, Accuracy: 86.10%\n",
      "Batch 181, Loss: 0.844697, Accuracy: 86.13%\n",
      "Batch 182, Loss: 0.846665, Accuracy: 86.15%\n",
      "Batch 183, Loss: 0.895081, Accuracy: 86.14%\n",
      "Batch 184, Loss: 0.929177, Accuracy: 86.12%\n",
      "Batch 185, Loss: 0.929039, Accuracy: 86.09%\n",
      "Batch 186, Loss: 0.865398, Accuracy: 86.10%\n",
      "Batch 187, Loss: 0.855129, Accuracy: 86.10%\n",
      "Batch 188, Loss: 0.865159, Accuracy: 86.11%\n",
      "Batch 189, Loss: 0.917002, Accuracy: 86.10%\n",
      "Batch 190, Loss: 0.875075, Accuracy: 86.11%\n",
      "Batch 191, Loss: 0.875952, Accuracy: 86.11%\n",
      "Batch 192, Loss: 0.837035, Accuracy: 86.14%\n",
      "Batch 193, Loss: 0.860090, Accuracy: 86.15%\n",
      "Batch 194, Loss: 0.824316, Accuracy: 86.18%\n",
      "Batch 195, Loss: 0.840714, Accuracy: 86.19%\n",
      "Batch 196, Loss: 0.868637, Accuracy: 86.20%\n",
      "Batch 197, Loss: 0.920170, Accuracy: 86.18%\n",
      "Batch 198, Loss: 0.932063, Accuracy: 86.15%\n",
      "Batch 199, Loss: 0.847276, Accuracy: 86.17%\n",
      "Batch 200, Loss: 0.943326, Accuracy: 86.13%\n",
      "Batch 201, Loss: 0.864709, Accuracy: 86.15%\n",
      "Batch 202, Loss: 0.856214, Accuracy: 86.15%\n",
      "Batch 203, Loss: 0.868352, Accuracy: 86.16%\n",
      "Batch 204, Loss: 0.921589, Accuracy: 86.14%\n",
      "Batch 205, Loss: 0.895570, Accuracy: 86.12%\n",
      "Batch 206, Loss: 0.871042, Accuracy: 86.13%\n",
      "Batch 207, Loss: 0.858086, Accuracy: 86.13%\n",
      "Batch 208, Loss: 0.832551, Accuracy: 86.16%\n",
      "Batch 209, Loss: 0.856177, Accuracy: 86.18%\n",
      "Batch 210, Loss: 0.889019, Accuracy: 86.17%\n",
      "Batch 211, Loss: 0.892298, Accuracy: 86.16%\n",
      "Batch 212, Loss: 0.911013, Accuracy: 86.14%\n",
      "Batch 213, Loss: 0.906984, Accuracy: 86.13%\n",
      "Training - Epoch 77, Loss: 0.883021, Accuracy: 86.13%\n",
      "Validation Batch 1, Loss: 0.849524, Accuracy: 90.62%\n",
      "Validation Batch 2, Loss: 0.827852, Accuracy: 91.41%\n",
      "Validation Batch 3, Loss: 0.909234, Accuracy: 89.06%\n",
      "Validation Batch 4, Loss: 0.861183, Accuracy: 89.06%\n",
      "Validation Batch 5, Loss: 0.850361, Accuracy: 89.38%\n",
      "Validation Batch 6, Loss: 0.821320, Accuracy: 90.10%\n",
      "Validation Batch 7, Loss: 0.828512, Accuracy: 90.40%\n",
      "Validation Batch 8, Loss: 0.934674, Accuracy: 89.06%\n",
      "Validation Batch 9, Loss: 0.910963, Accuracy: 88.54%\n",
      "Validation Batch 10, Loss: 0.865519, Accuracy: 88.44%\n",
      "Validation Batch 11, Loss: 0.853411, Accuracy: 88.49%\n",
      "Validation Batch 12, Loss: 0.851120, Accuracy: 88.67%\n",
      "Validation Batch 13, Loss: 0.867081, Accuracy: 88.58%\n",
      "Validation Batch 14, Loss: 0.887773, Accuracy: 88.28%\n",
      "Validation Batch 15, Loss: 0.853610, Accuracy: 88.33%\n",
      "Validation Batch 16, Loss: 0.859939, Accuracy: 88.57%\n",
      "Validation Batch 17, Loss: 0.907532, Accuracy: 88.24%\n",
      "Validation Batch 18, Loss: 0.852578, Accuracy: 88.28%\n",
      "Validation Batch 19, Loss: 0.918765, Accuracy: 87.91%\n",
      "Validation Batch 20, Loss: 0.912310, Accuracy: 87.58%\n",
      "Validation Batch 21, Loss: 0.888797, Accuracy: 87.50%\n",
      "Validation Batch 22, Loss: 0.882914, Accuracy: 87.43%\n",
      "Validation Batch 23, Loss: 0.899830, Accuracy: 87.30%\n",
      "Validation Batch 24, Loss: 0.890243, Accuracy: 87.30%\n",
      "Validation Batch 25, Loss: 0.838250, Accuracy: 87.44%\n",
      "Validation Batch 26, Loss: 0.871568, Accuracy: 87.44%\n",
      "Validation Batch 27, Loss: 0.835692, Accuracy: 87.55%\n",
      "Validation - Epoch 77, Loss: 0.871502, Accuracy: 87.55%\n",
      "Patience—0\n",
      "Epoch 78\n",
      "Batch 1, Loss: 0.856307, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.814391, Accuracy: 92.19%\n",
      "Batch 3, Loss: 0.857034, Accuracy: 90.62%\n",
      "Batch 4, Loss: 0.888080, Accuracy: 89.06%\n",
      "Batch 5, Loss: 0.851583, Accuracy: 89.06%\n",
      "Batch 6, Loss: 0.889267, Accuracy: 88.54%\n",
      "Batch 7, Loss: 0.883066, Accuracy: 88.39%\n",
      "Batch 8, Loss: 0.939533, Accuracy: 87.50%\n",
      "Batch 9, Loss: 0.963462, Accuracy: 86.46%\n",
      "Batch 10, Loss: 0.849948, Accuracy: 86.88%\n",
      "Batch 11, Loss: 0.825250, Accuracy: 87.50%\n",
      "Batch 12, Loss: 0.825638, Accuracy: 88.02%\n",
      "Batch 13, Loss: 0.882151, Accuracy: 87.86%\n",
      "Batch 14, Loss: 0.880296, Accuracy: 87.83%\n",
      "Batch 15, Loss: 0.956399, Accuracy: 87.08%\n",
      "Batch 16, Loss: 0.850743, Accuracy: 87.30%\n",
      "Batch 17, Loss: 0.822258, Accuracy: 87.68%\n",
      "Batch 18, Loss: 0.965231, Accuracy: 87.15%\n",
      "Batch 19, Loss: 0.863122, Accuracy: 87.17%\n",
      "Batch 20, Loss: 0.875652, Accuracy: 87.19%\n",
      "Batch 21, Loss: 0.863468, Accuracy: 87.20%\n",
      "Batch 22, Loss: 0.870373, Accuracy: 87.29%\n",
      "Batch 23, Loss: 0.883713, Accuracy: 87.23%\n",
      "Batch 24, Loss: 0.835712, Accuracy: 87.37%\n",
      "Batch 25, Loss: 0.889056, Accuracy: 87.25%\n",
      "Batch 26, Loss: 0.896390, Accuracy: 87.20%\n",
      "Batch 27, Loss: 0.931355, Accuracy: 86.98%\n",
      "Batch 28, Loss: 0.955343, Accuracy: 86.66%\n",
      "Batch 29, Loss: 0.882868, Accuracy: 86.64%\n",
      "Batch 30, Loss: 0.839207, Accuracy: 86.77%\n",
      "Batch 31, Loss: 0.822580, Accuracy: 86.95%\n",
      "Batch 32, Loss: 0.950004, Accuracy: 86.72%\n",
      "Batch 33, Loss: 0.923471, Accuracy: 86.65%\n",
      "Batch 34, Loss: 0.829469, Accuracy: 86.81%\n",
      "Batch 35, Loss: 0.899538, Accuracy: 86.74%\n",
      "Batch 36, Loss: 0.920568, Accuracy: 86.63%\n",
      "Batch 37, Loss: 0.870439, Accuracy: 86.66%\n",
      "Batch 38, Loss: 0.907324, Accuracy: 86.60%\n",
      "Batch 39, Loss: 0.931122, Accuracy: 86.46%\n",
      "Batch 40, Loss: 0.891043, Accuracy: 86.45%\n",
      "Batch 41, Loss: 0.921894, Accuracy: 86.36%\n",
      "Batch 42, Loss: 0.862252, Accuracy: 86.42%\n",
      "Batch 43, Loss: 0.874758, Accuracy: 86.48%\n",
      "Batch 44, Loss: 0.886250, Accuracy: 86.54%\n",
      "Batch 45, Loss: 0.981282, Accuracy: 86.28%\n",
      "Batch 46, Loss: 0.839030, Accuracy: 86.38%\n",
      "Batch 47, Loss: 0.902121, Accuracy: 86.34%\n",
      "Batch 48, Loss: 0.847059, Accuracy: 86.43%\n",
      "Batch 49, Loss: 0.871459, Accuracy: 86.45%\n",
      "Batch 50, Loss: 0.919250, Accuracy: 86.34%\n",
      "Batch 51, Loss: 0.884833, Accuracy: 86.34%\n",
      "Batch 52, Loss: 0.920836, Accuracy: 86.27%\n",
      "Batch 53, Loss: 0.899720, Accuracy: 86.23%\n",
      "Batch 54, Loss: 0.887430, Accuracy: 86.23%\n",
      "Batch 55, Loss: 0.852686, Accuracy: 86.31%\n",
      "Batch 56, Loss: 0.944218, Accuracy: 86.19%\n",
      "Batch 57, Loss: 0.974816, Accuracy: 85.96%\n",
      "Batch 58, Loss: 0.896259, Accuracy: 85.94%\n",
      "Batch 59, Loss: 0.896256, Accuracy: 85.91%\n",
      "Batch 60, Loss: 0.906323, Accuracy: 85.86%\n",
      "Batch 61, Loss: 0.934127, Accuracy: 85.78%\n",
      "Batch 62, Loss: 0.899085, Accuracy: 85.76%\n",
      "Batch 63, Loss: 0.923529, Accuracy: 85.66%\n",
      "Batch 64, Loss: 0.880985, Accuracy: 85.69%\n",
      "Batch 65, Loss: 0.839504, Accuracy: 85.79%\n",
      "Batch 66, Loss: 0.877611, Accuracy: 85.77%\n",
      "Batch 67, Loss: 0.797841, Accuracy: 85.94%\n",
      "Batch 68, Loss: 0.811000, Accuracy: 86.03%\n",
      "Batch 69, Loss: 0.961302, Accuracy: 85.91%\n",
      "Batch 70, Loss: 0.839294, Accuracy: 85.98%\n",
      "Batch 71, Loss: 0.797226, Accuracy: 86.14%\n",
      "Batch 72, Loss: 0.846369, Accuracy: 86.20%\n",
      "Batch 73, Loss: 0.901190, Accuracy: 86.17%\n",
      "Batch 74, Loss: 0.902235, Accuracy: 86.13%\n",
      "Batch 75, Loss: 0.917995, Accuracy: 86.08%\n",
      "Batch 76, Loss: 0.960205, Accuracy: 85.96%\n",
      "Batch 77, Loss: 0.904112, Accuracy: 85.94%\n",
      "Batch 78, Loss: 0.916873, Accuracy: 85.92%\n",
      "Batch 79, Loss: 0.962897, Accuracy: 85.82%\n",
      "Batch 80, Loss: 0.903450, Accuracy: 85.78%\n",
      "Batch 81, Loss: 0.893792, Accuracy: 85.76%\n",
      "Batch 82, Loss: 0.897981, Accuracy: 85.75%\n",
      "Batch 83, Loss: 0.894330, Accuracy: 85.75%\n",
      "Batch 84, Loss: 0.889366, Accuracy: 85.77%\n",
      "Batch 85, Loss: 0.840416, Accuracy: 85.83%\n",
      "Batch 86, Loss: 0.846486, Accuracy: 85.86%\n",
      "Batch 87, Loss: 0.856297, Accuracy: 85.90%\n",
      "Batch 88, Loss: 0.814412, Accuracy: 85.97%\n",
      "Batch 89, Loss: 0.940616, Accuracy: 85.90%\n",
      "Batch 90, Loss: 0.880005, Accuracy: 85.90%\n",
      "Batch 91, Loss: 0.881796, Accuracy: 85.90%\n",
      "Batch 92, Loss: 0.849963, Accuracy: 85.95%\n",
      "Batch 93, Loss: 0.931000, Accuracy: 85.89%\n",
      "Batch 94, Loss: 0.907012, Accuracy: 85.84%\n",
      "Batch 95, Loss: 0.886793, Accuracy: 85.86%\n",
      "Batch 96, Loss: 0.914742, Accuracy: 85.81%\n",
      "Batch 97, Loss: 0.847898, Accuracy: 85.86%\n",
      "Batch 98, Loss: 0.838027, Accuracy: 85.89%\n",
      "Batch 99, Loss: 0.860584, Accuracy: 85.91%\n",
      "Batch 100, Loss: 0.808151, Accuracy: 86.00%\n",
      "Batch 101, Loss: 0.875874, Accuracy: 86.01%\n",
      "Batch 102, Loss: 0.838081, Accuracy: 86.06%\n",
      "Batch 103, Loss: 0.883166, Accuracy: 86.07%\n",
      "Batch 104, Loss: 0.859712, Accuracy: 86.10%\n",
      "Batch 105, Loss: 0.865312, Accuracy: 86.12%\n",
      "Batch 106, Loss: 0.836349, Accuracy: 86.17%\n",
      "Batch 107, Loss: 0.893949, Accuracy: 86.17%\n",
      "Batch 108, Loss: 0.878257, Accuracy: 86.17%\n",
      "Batch 109, Loss: 0.871754, Accuracy: 86.20%\n",
      "Batch 110, Loss: 0.857132, Accuracy: 86.22%\n",
      "Batch 111, Loss: 0.864179, Accuracy: 86.23%\n",
      "Batch 112, Loss: 0.935098, Accuracy: 86.19%\n",
      "Batch 113, Loss: 0.917724, Accuracy: 86.14%\n",
      "Batch 114, Loss: 0.835606, Accuracy: 86.18%\n",
      "Batch 115, Loss: 0.854264, Accuracy: 86.21%\n",
      "Batch 116, Loss: 0.909954, Accuracy: 86.19%\n",
      "Batch 117, Loss: 0.889402, Accuracy: 86.19%\n",
      "Batch 118, Loss: 0.880980, Accuracy: 86.19%\n",
      "Batch 119, Loss: 0.857300, Accuracy: 86.21%\n",
      "Batch 120, Loss: 0.822360, Accuracy: 86.26%\n",
      "Batch 121, Loss: 0.885860, Accuracy: 86.26%\n",
      "Batch 122, Loss: 0.935076, Accuracy: 86.22%\n",
      "Batch 123, Loss: 0.917794, Accuracy: 86.22%\n",
      "Batch 124, Loss: 0.874231, Accuracy: 86.21%\n",
      "Batch 125, Loss: 0.868901, Accuracy: 86.21%\n",
      "Batch 126, Loss: 0.821822, Accuracy: 86.27%\n",
      "Batch 127, Loss: 0.841499, Accuracy: 86.32%\n",
      "Batch 128, Loss: 0.837784, Accuracy: 86.35%\n",
      "Batch 129, Loss: 0.886320, Accuracy: 86.34%\n",
      "Batch 130, Loss: 0.983990, Accuracy: 86.25%\n",
      "Batch 131, Loss: 0.876059, Accuracy: 86.25%\n",
      "Batch 132, Loss: 0.943813, Accuracy: 86.19%\n",
      "Batch 133, Loss: 0.919296, Accuracy: 86.15%\n",
      "Batch 134, Loss: 0.964785, Accuracy: 86.08%\n",
      "Batch 135, Loss: 0.812281, Accuracy: 86.13%\n",
      "Batch 136, Loss: 0.918586, Accuracy: 86.11%\n",
      "Batch 137, Loss: 0.854450, Accuracy: 86.14%\n",
      "Batch 138, Loss: 0.893137, Accuracy: 86.14%\n",
      "Batch 139, Loss: 0.875790, Accuracy: 86.15%\n",
      "Batch 140, Loss: 0.863821, Accuracy: 86.16%\n",
      "Batch 141, Loss: 0.838751, Accuracy: 86.20%\n",
      "Batch 142, Loss: 0.868450, Accuracy: 86.20%\n",
      "Batch 143, Loss: 0.865236, Accuracy: 86.21%\n",
      "Batch 144, Loss: 0.941676, Accuracy: 86.18%\n",
      "Batch 145, Loss: 0.905554, Accuracy: 86.16%\n",
      "Batch 146, Loss: 0.832007, Accuracy: 86.21%\n",
      "Batch 147, Loss: 0.840250, Accuracy: 86.24%\n",
      "Batch 148, Loss: 0.901105, Accuracy: 86.22%\n",
      "Batch 149, Loss: 0.865161, Accuracy: 86.24%\n",
      "Batch 150, Loss: 0.831727, Accuracy: 86.28%\n",
      "Batch 151, Loss: 0.816361, Accuracy: 86.34%\n",
      "Batch 152, Loss: 0.836510, Accuracy: 86.37%\n",
      "Batch 153, Loss: 0.905318, Accuracy: 86.35%\n",
      "Batch 154, Loss: 0.853632, Accuracy: 86.36%\n",
      "Batch 155, Loss: 0.868850, Accuracy: 86.38%\n",
      "Batch 156, Loss: 0.907361, Accuracy: 86.36%\n",
      "Batch 157, Loss: 0.875037, Accuracy: 86.37%\n",
      "Batch 158, Loss: 0.905404, Accuracy: 86.34%\n",
      "Batch 159, Loss: 0.843128, Accuracy: 86.37%\n",
      "Batch 160, Loss: 0.879536, Accuracy: 86.38%\n",
      "Batch 161, Loss: 0.902787, Accuracy: 86.36%\n",
      "Batch 162, Loss: 0.907687, Accuracy: 86.34%\n",
      "Batch 163, Loss: 0.849192, Accuracy: 86.37%\n",
      "Batch 164, Loss: 0.855062, Accuracy: 86.38%\n",
      "Batch 165, Loss: 0.915090, Accuracy: 86.34%\n",
      "Batch 166, Loss: 0.898134, Accuracy: 86.32%\n",
      "Batch 167, Loss: 0.914381, Accuracy: 86.29%\n",
      "Batch 168, Loss: 0.879803, Accuracy: 86.29%\n",
      "Batch 169, Loss: 0.864025, Accuracy: 86.30%\n",
      "Batch 170, Loss: 0.905061, Accuracy: 86.29%\n",
      "Batch 171, Loss: 0.855822, Accuracy: 86.29%\n",
      "Batch 172, Loss: 0.893805, Accuracy: 86.29%\n",
      "Batch 173, Loss: 0.822051, Accuracy: 86.33%\n",
      "Batch 174, Loss: 0.838585, Accuracy: 86.34%\n",
      "Batch 175, Loss: 0.901839, Accuracy: 86.34%\n",
      "Batch 176, Loss: 0.845216, Accuracy: 86.35%\n",
      "Batch 177, Loss: 0.918692, Accuracy: 86.33%\n",
      "Batch 178, Loss: 0.912264, Accuracy: 86.31%\n",
      "Batch 179, Loss: 0.895441, Accuracy: 86.30%\n",
      "Batch 180, Loss: 0.915659, Accuracy: 86.30%\n",
      "Batch 181, Loss: 0.839245, Accuracy: 86.33%\n",
      "Batch 182, Loss: 0.895019, Accuracy: 86.32%\n",
      "Batch 183, Loss: 0.923942, Accuracy: 86.30%\n",
      "Batch 184, Loss: 0.902712, Accuracy: 86.29%\n",
      "Batch 185, Loss: 0.860252, Accuracy: 86.31%\n",
      "Batch 186, Loss: 0.950379, Accuracy: 86.27%\n",
      "Batch 187, Loss: 0.901085, Accuracy: 86.24%\n",
      "Batch 188, Loss: 0.905517, Accuracy: 86.22%\n",
      "Batch 189, Loss: 0.909630, Accuracy: 86.21%\n",
      "Batch 190, Loss: 0.889719, Accuracy: 86.20%\n",
      "Batch 191, Loss: 0.908844, Accuracy: 86.19%\n",
      "Batch 192, Loss: 0.831930, Accuracy: 86.21%\n",
      "Batch 193, Loss: 0.925107, Accuracy: 86.18%\n",
      "Batch 194, Loss: 0.894440, Accuracy: 86.17%\n",
      "Batch 195, Loss: 0.908388, Accuracy: 86.16%\n",
      "Batch 196, Loss: 0.887064, Accuracy: 86.16%\n",
      "Batch 197, Loss: 0.904623, Accuracy: 86.15%\n",
      "Batch 198, Loss: 0.802994, Accuracy: 86.19%\n",
      "Batch 199, Loss: 0.910718, Accuracy: 86.17%\n",
      "Batch 200, Loss: 0.939316, Accuracy: 86.15%\n",
      "Batch 201, Loss: 0.988274, Accuracy: 86.09%\n",
      "Batch 202, Loss: 0.932650, Accuracy: 86.06%\n",
      "Batch 203, Loss: 0.872042, Accuracy: 86.07%\n",
      "Batch 204, Loss: 0.931598, Accuracy: 86.04%\n",
      "Batch 205, Loss: 0.932904, Accuracy: 86.01%\n",
      "Batch 206, Loss: 0.924886, Accuracy: 85.99%\n",
      "Batch 207, Loss: 0.817962, Accuracy: 86.03%\n",
      "Batch 208, Loss: 0.913032, Accuracy: 86.02%\n",
      "Batch 209, Loss: 0.837172, Accuracy: 86.04%\n",
      "Batch 210, Loss: 0.863551, Accuracy: 86.03%\n",
      "Batch 211, Loss: 0.845011, Accuracy: 86.06%\n",
      "Batch 212, Loss: 0.809644, Accuracy: 86.09%\n",
      "Batch 213, Loss: 0.940422, Accuracy: 86.07%\n",
      "Training - Epoch 78, Loss: 0.884184, Accuracy: 86.07%\n",
      "Validation Batch 1, Loss: 0.845880, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.826829, Accuracy: 92.19%\n",
      "Validation Batch 3, Loss: 0.911922, Accuracy: 89.58%\n",
      "Validation Batch 4, Loss: 0.860576, Accuracy: 89.45%\n",
      "Validation Batch 5, Loss: 0.841481, Accuracy: 89.69%\n",
      "Validation Batch 6, Loss: 0.813729, Accuracy: 90.36%\n",
      "Validation Batch 7, Loss: 0.829136, Accuracy: 90.62%\n",
      "Validation Batch 8, Loss: 0.926502, Accuracy: 89.45%\n",
      "Validation Batch 9, Loss: 0.913208, Accuracy: 88.89%\n",
      "Validation Batch 10, Loss: 0.855547, Accuracy: 89.06%\n",
      "Validation Batch 11, Loss: 0.852414, Accuracy: 89.06%\n",
      "Validation Batch 12, Loss: 0.848720, Accuracy: 89.32%\n",
      "Validation Batch 13, Loss: 0.862606, Accuracy: 89.18%\n",
      "Validation Batch 14, Loss: 0.885778, Accuracy: 88.84%\n",
      "Validation Batch 15, Loss: 0.843515, Accuracy: 89.06%\n",
      "Validation Batch 16, Loss: 0.848465, Accuracy: 89.26%\n",
      "Validation Batch 17, Loss: 0.897171, Accuracy: 89.06%\n",
      "Validation Batch 18, Loss: 0.846718, Accuracy: 89.06%\n",
      "Validation Batch 19, Loss: 0.908630, Accuracy: 88.65%\n",
      "Validation Batch 20, Loss: 0.915457, Accuracy: 88.28%\n",
      "Validation Batch 21, Loss: 0.885779, Accuracy: 88.17%\n",
      "Validation Batch 22, Loss: 0.872802, Accuracy: 88.07%\n",
      "Validation Batch 23, Loss: 0.891969, Accuracy: 88.04%\n",
      "Validation Batch 24, Loss: 0.881282, Accuracy: 88.02%\n",
      "Validation Batch 25, Loss: 0.839261, Accuracy: 88.12%\n",
      "Validation Batch 26, Loss: 0.871091, Accuracy: 88.10%\n",
      "Validation Batch 27, Loss: 0.822321, Accuracy: 88.26%\n",
      "Validation - Epoch 78, Loss: 0.866622, Accuracy: 88.26%\n",
      "Patience—0\n",
      "Epoch 79\n",
      "Batch 1, Loss: 0.890660, Accuracy: 84.38%\n",
      "Batch 2, Loss: 0.864788, Accuracy: 86.72%\n",
      "Batch 3, Loss: 0.896352, Accuracy: 86.46%\n",
      "Batch 4, Loss: 0.878795, Accuracy: 86.33%\n",
      "Batch 5, Loss: 0.909006, Accuracy: 85.62%\n",
      "Batch 6, Loss: 0.867796, Accuracy: 85.94%\n",
      "Batch 7, Loss: 0.846378, Accuracy: 86.61%\n",
      "Batch 8, Loss: 0.877121, Accuracy: 86.72%\n",
      "Batch 9, Loss: 0.961592, Accuracy: 85.76%\n",
      "Batch 10, Loss: 0.866141, Accuracy: 86.09%\n",
      "Batch 11, Loss: 0.836450, Accuracy: 86.51%\n",
      "Batch 12, Loss: 0.862560, Accuracy: 86.72%\n",
      "Batch 13, Loss: 0.836906, Accuracy: 86.90%\n",
      "Batch 14, Loss: 0.937763, Accuracy: 86.27%\n",
      "Batch 15, Loss: 0.781542, Accuracy: 87.08%\n",
      "Batch 16, Loss: 0.879153, Accuracy: 87.01%\n",
      "Batch 17, Loss: 0.840757, Accuracy: 87.22%\n",
      "Batch 18, Loss: 0.861430, Accuracy: 87.15%\n",
      "Batch 19, Loss: 0.869661, Accuracy: 87.17%\n",
      "Batch 20, Loss: 0.853339, Accuracy: 87.27%\n",
      "Batch 21, Loss: 0.903508, Accuracy: 87.20%\n",
      "Batch 22, Loss: 0.907052, Accuracy: 87.07%\n",
      "Batch 23, Loss: 0.850479, Accuracy: 87.23%\n",
      "Batch 24, Loss: 0.983872, Accuracy: 86.65%\n",
      "Batch 25, Loss: 0.918133, Accuracy: 86.50%\n",
      "Batch 26, Loss: 0.865985, Accuracy: 86.60%\n",
      "Batch 27, Loss: 0.890456, Accuracy: 86.57%\n",
      "Batch 28, Loss: 0.873945, Accuracy: 86.66%\n",
      "Batch 29, Loss: 0.847936, Accuracy: 86.80%\n",
      "Batch 30, Loss: 0.920730, Accuracy: 86.67%\n",
      "Batch 31, Loss: 0.868620, Accuracy: 86.69%\n",
      "Batch 32, Loss: 0.870737, Accuracy: 86.72%\n",
      "Batch 33, Loss: 0.910856, Accuracy: 86.60%\n",
      "Batch 34, Loss: 0.834936, Accuracy: 86.72%\n",
      "Batch 35, Loss: 0.863629, Accuracy: 86.74%\n",
      "Batch 36, Loss: 0.924159, Accuracy: 86.63%\n",
      "Batch 37, Loss: 0.859401, Accuracy: 86.74%\n",
      "Batch 38, Loss: 0.886682, Accuracy: 86.68%\n",
      "Batch 39, Loss: 0.800998, Accuracy: 86.94%\n",
      "Batch 40, Loss: 0.880622, Accuracy: 86.88%\n",
      "Batch 41, Loss: 0.873393, Accuracy: 86.85%\n",
      "Batch 42, Loss: 0.863637, Accuracy: 86.87%\n",
      "Batch 43, Loss: 0.860240, Accuracy: 86.88%\n",
      "Batch 44, Loss: 0.879479, Accuracy: 86.86%\n",
      "Batch 45, Loss: 0.981398, Accuracy: 86.60%\n",
      "Batch 46, Loss: 0.814392, Accuracy: 86.79%\n",
      "Batch 47, Loss: 0.867063, Accuracy: 86.84%\n",
      "Batch 48, Loss: 0.842214, Accuracy: 86.91%\n",
      "Batch 49, Loss: 0.897815, Accuracy: 86.86%\n",
      "Batch 50, Loss: 0.870839, Accuracy: 86.91%\n",
      "Batch 51, Loss: 0.941981, Accuracy: 86.76%\n",
      "Batch 52, Loss: 0.905187, Accuracy: 86.72%\n",
      "Batch 53, Loss: 0.840911, Accuracy: 86.79%\n",
      "Batch 54, Loss: 0.972085, Accuracy: 86.57%\n",
      "Batch 55, Loss: 0.895471, Accuracy: 86.53%\n",
      "Batch 56, Loss: 0.896322, Accuracy: 86.50%\n",
      "Batch 57, Loss: 0.886656, Accuracy: 86.49%\n",
      "Batch 58, Loss: 0.830044, Accuracy: 86.53%\n",
      "Batch 59, Loss: 0.926006, Accuracy: 86.49%\n",
      "Batch 60, Loss: 0.807023, Accuracy: 86.64%\n",
      "Batch 61, Loss: 0.850837, Accuracy: 86.71%\n",
      "Batch 62, Loss: 0.854523, Accuracy: 86.74%\n",
      "Batch 63, Loss: 0.859740, Accuracy: 86.78%\n",
      "Batch 64, Loss: 0.902741, Accuracy: 86.72%\n",
      "Batch 65, Loss: 0.857630, Accuracy: 86.73%\n",
      "Batch 66, Loss: 0.914315, Accuracy: 86.67%\n",
      "Batch 67, Loss: 0.866876, Accuracy: 86.68%\n",
      "Batch 68, Loss: 0.904814, Accuracy: 86.65%\n",
      "Batch 69, Loss: 0.897302, Accuracy: 86.64%\n",
      "Batch 70, Loss: 0.851313, Accuracy: 86.72%\n",
      "Batch 71, Loss: 0.916201, Accuracy: 86.64%\n",
      "Batch 72, Loss: 0.872955, Accuracy: 86.63%\n",
      "Batch 73, Loss: 0.891536, Accuracy: 86.62%\n",
      "Batch 74, Loss: 0.891273, Accuracy: 86.59%\n",
      "Batch 75, Loss: 0.875536, Accuracy: 86.60%\n",
      "Batch 76, Loss: 0.942971, Accuracy: 86.51%\n",
      "Batch 77, Loss: 0.883124, Accuracy: 86.51%\n",
      "Batch 78, Loss: 0.873508, Accuracy: 86.50%\n",
      "Batch 79, Loss: 0.866770, Accuracy: 86.51%\n",
      "Batch 80, Loss: 0.913589, Accuracy: 86.48%\n",
      "Batch 81, Loss: 0.891329, Accuracy: 86.48%\n",
      "Batch 82, Loss: 0.864565, Accuracy: 86.51%\n",
      "Batch 83, Loss: 0.846393, Accuracy: 86.56%\n",
      "Batch 84, Loss: 0.850923, Accuracy: 86.61%\n",
      "Batch 85, Loss: 0.857634, Accuracy: 86.62%\n",
      "Batch 86, Loss: 0.868238, Accuracy: 86.63%\n",
      "Batch 87, Loss: 0.920904, Accuracy: 86.58%\n",
      "Batch 88, Loss: 0.854520, Accuracy: 86.63%\n",
      "Batch 89, Loss: 0.775051, Accuracy: 86.76%\n",
      "Batch 90, Loss: 0.919739, Accuracy: 86.72%\n",
      "Batch 91, Loss: 0.861995, Accuracy: 86.71%\n",
      "Batch 92, Loss: 0.893797, Accuracy: 86.70%\n",
      "Batch 93, Loss: 0.980410, Accuracy: 86.56%\n",
      "Batch 94, Loss: 0.877734, Accuracy: 86.55%\n",
      "Batch 95, Loss: 0.836326, Accuracy: 86.60%\n",
      "Batch 96, Loss: 0.997454, Accuracy: 86.47%\n",
      "Batch 97, Loss: 0.845460, Accuracy: 86.52%\n",
      "Batch 98, Loss: 0.821147, Accuracy: 86.59%\n",
      "Batch 99, Loss: 0.969699, Accuracy: 86.49%\n",
      "Batch 100, Loss: 0.866229, Accuracy: 86.50%\n",
      "Batch 101, Loss: 0.839433, Accuracy: 86.56%\n",
      "Batch 102, Loss: 0.861408, Accuracy: 86.58%\n",
      "Batch 103, Loss: 0.878526, Accuracy: 86.59%\n",
      "Batch 104, Loss: 0.846391, Accuracy: 86.61%\n",
      "Batch 105, Loss: 0.872009, Accuracy: 86.61%\n",
      "Batch 106, Loss: 0.873861, Accuracy: 86.60%\n",
      "Batch 107, Loss: 0.835168, Accuracy: 86.64%\n",
      "Batch 108, Loss: 0.787407, Accuracy: 86.72%\n",
      "Batch 109, Loss: 0.850883, Accuracy: 86.75%\n",
      "Batch 110, Loss: 0.814941, Accuracy: 86.82%\n",
      "Batch 111, Loss: 0.880736, Accuracy: 86.82%\n",
      "Batch 112, Loss: 0.864714, Accuracy: 86.82%\n",
      "Batch 113, Loss: 0.876199, Accuracy: 86.84%\n",
      "Batch 114, Loss: 0.893413, Accuracy: 86.83%\n",
      "Batch 115, Loss: 0.884797, Accuracy: 86.82%\n",
      "Batch 116, Loss: 0.863992, Accuracy: 86.83%\n",
      "Batch 117, Loss: 0.843254, Accuracy: 86.87%\n",
      "Batch 118, Loss: 0.903630, Accuracy: 86.84%\n",
      "Batch 119, Loss: 0.835146, Accuracy: 86.87%\n",
      "Batch 120, Loss: 0.893518, Accuracy: 86.85%\n",
      "Batch 121, Loss: 0.951771, Accuracy: 86.78%\n",
      "Batch 122, Loss: 0.880164, Accuracy: 86.77%\n",
      "Batch 123, Loss: 0.851095, Accuracy: 86.80%\n",
      "Batch 124, Loss: 0.924839, Accuracy: 86.76%\n",
      "Batch 125, Loss: 0.871764, Accuracy: 86.76%\n",
      "Batch 126, Loss: 0.960938, Accuracy: 86.68%\n",
      "Batch 127, Loss: 0.941838, Accuracy: 86.63%\n",
      "Batch 128, Loss: 0.906474, Accuracy: 86.60%\n",
      "Batch 129, Loss: 0.956297, Accuracy: 86.54%\n",
      "Batch 130, Loss: 0.876440, Accuracy: 86.54%\n",
      "Batch 131, Loss: 0.911400, Accuracy: 86.51%\n",
      "Batch 132, Loss: 0.894102, Accuracy: 86.51%\n",
      "Batch 133, Loss: 0.869058, Accuracy: 86.51%\n",
      "Batch 134, Loss: 0.859614, Accuracy: 86.53%\n",
      "Batch 135, Loss: 0.849281, Accuracy: 86.55%\n",
      "Batch 136, Loss: 0.940174, Accuracy: 86.50%\n",
      "Batch 137, Loss: 0.890890, Accuracy: 86.48%\n",
      "Batch 138, Loss: 0.868641, Accuracy: 86.50%\n",
      "Batch 139, Loss: 0.985758, Accuracy: 86.43%\n",
      "Batch 140, Loss: 0.949720, Accuracy: 86.38%\n",
      "Batch 141, Loss: 0.904382, Accuracy: 86.36%\n",
      "Batch 142, Loss: 0.881130, Accuracy: 86.36%\n",
      "Batch 143, Loss: 0.918799, Accuracy: 86.32%\n",
      "Batch 144, Loss: 0.909572, Accuracy: 86.31%\n",
      "Batch 145, Loss: 0.851310, Accuracy: 86.34%\n",
      "Batch 146, Loss: 0.852865, Accuracy: 86.34%\n",
      "Batch 147, Loss: 0.893055, Accuracy: 86.33%\n",
      "Batch 148, Loss: 0.863852, Accuracy: 86.35%\n",
      "Batch 149, Loss: 0.863664, Accuracy: 86.37%\n",
      "Batch 150, Loss: 0.877245, Accuracy: 86.38%\n",
      "Batch 151, Loss: 0.880283, Accuracy: 86.36%\n",
      "Batch 152, Loss: 0.842552, Accuracy: 86.38%\n",
      "Batch 153, Loss: 0.956991, Accuracy: 86.33%\n",
      "Batch 154, Loss: 0.795315, Accuracy: 86.38%\n",
      "Batch 155, Loss: 0.894180, Accuracy: 86.37%\n",
      "Batch 156, Loss: 0.862026, Accuracy: 86.39%\n",
      "Batch 157, Loss: 0.878818, Accuracy: 86.39%\n",
      "Batch 158, Loss: 0.937833, Accuracy: 86.34%\n",
      "Batch 159, Loss: 0.841756, Accuracy: 86.36%\n",
      "Batch 160, Loss: 0.879642, Accuracy: 86.36%\n",
      "Batch 161, Loss: 0.884225, Accuracy: 86.35%\n",
      "Batch 162, Loss: 0.859217, Accuracy: 86.37%\n",
      "Batch 163, Loss: 0.912247, Accuracy: 86.36%\n",
      "Batch 164, Loss: 0.911955, Accuracy: 86.33%\n",
      "Batch 165, Loss: 0.870170, Accuracy: 86.34%\n",
      "Batch 166, Loss: 0.872489, Accuracy: 86.34%\n",
      "Batch 167, Loss: 0.895252, Accuracy: 86.33%\n",
      "Batch 168, Loss: 0.845591, Accuracy: 86.35%\n",
      "Batch 169, Loss: 0.958827, Accuracy: 86.30%\n",
      "Batch 170, Loss: 0.909363, Accuracy: 86.29%\n",
      "Batch 171, Loss: 0.836678, Accuracy: 86.31%\n",
      "Batch 172, Loss: 0.918050, Accuracy: 86.28%\n",
      "Batch 173, Loss: 0.881850, Accuracy: 86.28%\n",
      "Batch 174, Loss: 0.872139, Accuracy: 86.27%\n",
      "Batch 175, Loss: 0.917861, Accuracy: 86.25%\n",
      "Batch 176, Loss: 0.901086, Accuracy: 86.25%\n",
      "Batch 177, Loss: 0.892566, Accuracy: 86.23%\n",
      "Batch 178, Loss: 0.895689, Accuracy: 86.24%\n",
      "Batch 179, Loss: 0.860757, Accuracy: 86.24%\n",
      "Batch 180, Loss: 0.877120, Accuracy: 86.24%\n",
      "Batch 181, Loss: 0.848976, Accuracy: 86.26%\n",
      "Batch 182, Loss: 0.839986, Accuracy: 86.29%\n",
      "Batch 183, Loss: 0.964046, Accuracy: 86.24%\n",
      "Batch 184, Loss: 0.845314, Accuracy: 86.27%\n",
      "Batch 185, Loss: 0.910260, Accuracy: 86.25%\n",
      "Batch 186, Loss: 0.858050, Accuracy: 86.26%\n",
      "Batch 187, Loss: 0.848405, Accuracy: 86.27%\n",
      "Batch 188, Loss: 0.855825, Accuracy: 86.29%\n",
      "Batch 189, Loss: 0.877061, Accuracy: 86.30%\n",
      "Batch 190, Loss: 0.888570, Accuracy: 86.31%\n",
      "Batch 191, Loss: 0.865590, Accuracy: 86.32%\n",
      "Batch 192, Loss: 0.947958, Accuracy: 86.28%\n",
      "Batch 193, Loss: 0.896783, Accuracy: 86.27%\n",
      "Batch 194, Loss: 0.922954, Accuracy: 86.24%\n",
      "Batch 195, Loss: 0.909914, Accuracy: 86.23%\n",
      "Batch 196, Loss: 0.869526, Accuracy: 86.23%\n",
      "Batch 197, Loss: 0.844158, Accuracy: 86.25%\n",
      "Batch 198, Loss: 0.877443, Accuracy: 86.26%\n",
      "Batch 199, Loss: 0.860957, Accuracy: 86.27%\n",
      "Batch 200, Loss: 0.934173, Accuracy: 86.24%\n",
      "Batch 201, Loss: 0.815423, Accuracy: 86.28%\n",
      "Batch 202, Loss: 0.903831, Accuracy: 86.27%\n",
      "Batch 203, Loss: 0.851265, Accuracy: 86.28%\n",
      "Batch 204, Loss: 0.892301, Accuracy: 86.27%\n",
      "Batch 205, Loss: 0.865076, Accuracy: 86.28%\n",
      "Batch 206, Loss: 0.926142, Accuracy: 86.26%\n",
      "Batch 207, Loss: 0.824185, Accuracy: 86.30%\n",
      "Batch 208, Loss: 0.903036, Accuracy: 86.29%\n",
      "Batch 209, Loss: 0.866943, Accuracy: 86.30%\n",
      "Batch 210, Loss: 0.824763, Accuracy: 86.33%\n",
      "Batch 211, Loss: 0.957569, Accuracy: 86.29%\n",
      "Batch 212, Loss: 0.847152, Accuracy: 86.31%\n",
      "Batch 213, Loss: 0.852301, Accuracy: 86.32%\n",
      "Training - Epoch 79, Loss: 0.881287, Accuracy: 86.32%\n",
      "Validation Batch 1, Loss: 0.856117, Accuracy: 90.62%\n",
      "Validation Batch 2, Loss: 0.846446, Accuracy: 92.19%\n",
      "Validation Batch 3, Loss: 0.925492, Accuracy: 88.54%\n",
      "Validation Batch 4, Loss: 0.872136, Accuracy: 87.89%\n",
      "Validation Batch 5, Loss: 0.854385, Accuracy: 88.44%\n",
      "Validation Batch 6, Loss: 0.824433, Accuracy: 89.32%\n",
      "Validation Batch 7, Loss: 0.840068, Accuracy: 89.51%\n",
      "Validation Batch 8, Loss: 0.939905, Accuracy: 88.28%\n",
      "Validation Batch 9, Loss: 0.916490, Accuracy: 87.85%\n",
      "Validation Batch 10, Loss: 0.870408, Accuracy: 87.81%\n",
      "Validation Batch 11, Loss: 0.866499, Accuracy: 87.93%\n",
      "Validation Batch 12, Loss: 0.868036, Accuracy: 88.02%\n",
      "Validation Batch 13, Loss: 0.881982, Accuracy: 87.98%\n",
      "Validation Batch 14, Loss: 0.900836, Accuracy: 87.72%\n",
      "Validation Batch 15, Loss: 0.846366, Accuracy: 87.92%\n",
      "Validation Batch 16, Loss: 0.866446, Accuracy: 88.09%\n",
      "Validation Batch 17, Loss: 0.911490, Accuracy: 87.87%\n",
      "Validation Batch 18, Loss: 0.862366, Accuracy: 87.85%\n",
      "Validation Batch 19, Loss: 0.923202, Accuracy: 87.50%\n",
      "Validation Batch 20, Loss: 0.949818, Accuracy: 87.11%\n",
      "Validation Batch 21, Loss: 0.898735, Accuracy: 87.05%\n",
      "Validation Batch 22, Loss: 0.889904, Accuracy: 86.93%\n",
      "Validation Batch 23, Loss: 0.925972, Accuracy: 86.68%\n",
      "Validation Batch 24, Loss: 0.902597, Accuracy: 86.65%\n",
      "Validation Batch 25, Loss: 0.849498, Accuracy: 86.81%\n",
      "Validation Batch 26, Loss: 0.885648, Accuracy: 86.84%\n",
      "Validation Batch 27, Loss: 0.853742, Accuracy: 86.91%\n",
      "Validation - Epoch 79, Loss: 0.882556, Accuracy: 86.91%\n",
      "Patience—1\n",
      "Epoch 80\n",
      "Batch 1, Loss: 0.864032, Accuracy: 87.50%\n",
      "Batch 2, Loss: 1.000360, Accuracy: 82.03%\n",
      "Batch 3, Loss: 0.852329, Accuracy: 84.38%\n",
      "Batch 4, Loss: 0.833727, Accuracy: 86.33%\n",
      "Batch 5, Loss: 0.867060, Accuracy: 86.56%\n",
      "Batch 6, Loss: 0.871034, Accuracy: 86.46%\n",
      "Batch 7, Loss: 0.965628, Accuracy: 85.04%\n",
      "Batch 8, Loss: 0.909828, Accuracy: 85.16%\n",
      "Batch 9, Loss: 0.896867, Accuracy: 85.07%\n",
      "Batch 10, Loss: 0.840059, Accuracy: 85.62%\n",
      "Batch 11, Loss: 0.942237, Accuracy: 84.94%\n",
      "Batch 12, Loss: 0.866735, Accuracy: 85.16%\n",
      "Batch 13, Loss: 0.868774, Accuracy: 85.34%\n",
      "Batch 14, Loss: 0.816466, Accuracy: 85.83%\n",
      "Batch 15, Loss: 0.901063, Accuracy: 85.73%\n",
      "Batch 16, Loss: 0.859961, Accuracy: 85.84%\n",
      "Batch 17, Loss: 0.895235, Accuracy: 85.85%\n",
      "Batch 18, Loss: 0.846741, Accuracy: 86.11%\n",
      "Batch 19, Loss: 0.884927, Accuracy: 86.10%\n",
      "Batch 20, Loss: 0.821169, Accuracy: 86.41%\n",
      "Batch 21, Loss: 0.844636, Accuracy: 86.53%\n",
      "Batch 22, Loss: 0.896595, Accuracy: 86.51%\n",
      "Batch 23, Loss: 0.853965, Accuracy: 86.62%\n",
      "Batch 24, Loss: 0.858454, Accuracy: 86.72%\n",
      "Batch 25, Loss: 0.877371, Accuracy: 86.75%\n",
      "Batch 26, Loss: 0.873726, Accuracy: 86.72%\n",
      "Batch 27, Loss: 0.885778, Accuracy: 86.75%\n",
      "Batch 28, Loss: 0.907474, Accuracy: 86.61%\n",
      "Batch 29, Loss: 0.821834, Accuracy: 86.80%\n",
      "Batch 30, Loss: 0.911452, Accuracy: 86.72%\n",
      "Batch 31, Loss: 0.840951, Accuracy: 86.79%\n",
      "Batch 32, Loss: 0.862213, Accuracy: 86.91%\n",
      "Batch 33, Loss: 0.869730, Accuracy: 86.93%\n",
      "Batch 34, Loss: 0.856506, Accuracy: 86.95%\n",
      "Batch 35, Loss: 0.899988, Accuracy: 86.83%\n",
      "Batch 36, Loss: 0.838454, Accuracy: 86.94%\n",
      "Batch 37, Loss: 0.873618, Accuracy: 86.91%\n",
      "Batch 38, Loss: 0.921472, Accuracy: 86.76%\n",
      "Batch 39, Loss: 0.886117, Accuracy: 86.70%\n",
      "Batch 40, Loss: 0.844577, Accuracy: 86.80%\n",
      "Batch 41, Loss: 0.939706, Accuracy: 86.59%\n",
      "Batch 42, Loss: 0.858917, Accuracy: 86.64%\n",
      "Batch 43, Loss: 0.889516, Accuracy: 86.63%\n",
      "Batch 44, Loss: 0.826878, Accuracy: 86.72%\n",
      "Batch 45, Loss: 0.906508, Accuracy: 86.63%\n",
      "Batch 46, Loss: 0.912155, Accuracy: 86.55%\n",
      "Batch 47, Loss: 0.851866, Accuracy: 86.64%\n",
      "Batch 48, Loss: 0.892752, Accuracy: 86.59%\n",
      "Batch 49, Loss: 0.869347, Accuracy: 86.61%\n",
      "Batch 50, Loss: 0.810224, Accuracy: 86.75%\n",
      "Batch 51, Loss: 0.872721, Accuracy: 86.80%\n",
      "Batch 52, Loss: 0.887706, Accuracy: 86.81%\n",
      "Batch 53, Loss: 0.903416, Accuracy: 86.76%\n",
      "Batch 54, Loss: 0.918753, Accuracy: 86.69%\n",
      "Batch 55, Loss: 0.873312, Accuracy: 86.70%\n",
      "Batch 56, Loss: 0.907515, Accuracy: 86.64%\n",
      "Batch 57, Loss: 0.851200, Accuracy: 86.68%\n",
      "Batch 58, Loss: 0.878330, Accuracy: 86.72%\n",
      "Batch 59, Loss: 0.849210, Accuracy: 86.78%\n",
      "Batch 60, Loss: 0.840646, Accuracy: 86.85%\n",
      "Batch 61, Loss: 0.822622, Accuracy: 86.94%\n",
      "Batch 62, Loss: 0.888224, Accuracy: 86.92%\n",
      "Batch 63, Loss: 0.842803, Accuracy: 86.98%\n",
      "Batch 64, Loss: 0.883336, Accuracy: 86.96%\n",
      "Batch 65, Loss: 0.858382, Accuracy: 86.97%\n",
      "Batch 66, Loss: 0.825486, Accuracy: 87.05%\n",
      "Batch 67, Loss: 0.813722, Accuracy: 87.15%\n",
      "Batch 68, Loss: 0.790526, Accuracy: 87.29%\n",
      "Batch 69, Loss: 0.896804, Accuracy: 87.25%\n",
      "Batch 70, Loss: 0.855999, Accuracy: 87.28%\n",
      "Batch 71, Loss: 0.909625, Accuracy: 87.24%\n",
      "Batch 72, Loss: 0.886728, Accuracy: 87.20%\n",
      "Batch 73, Loss: 0.810962, Accuracy: 87.31%\n",
      "Batch 74, Loss: 0.896648, Accuracy: 87.29%\n",
      "Batch 75, Loss: 0.872336, Accuracy: 87.29%\n",
      "Batch 76, Loss: 0.909599, Accuracy: 87.25%\n",
      "Batch 77, Loss: 0.940650, Accuracy: 87.16%\n",
      "Batch 78, Loss: 0.831168, Accuracy: 87.22%\n",
      "Batch 79, Loss: 0.876599, Accuracy: 87.22%\n",
      "Batch 80, Loss: 0.811053, Accuracy: 87.30%\n",
      "Batch 81, Loss: 0.934620, Accuracy: 87.21%\n",
      "Batch 82, Loss: 0.883798, Accuracy: 87.18%\n",
      "Batch 83, Loss: 0.854783, Accuracy: 87.18%\n",
      "Batch 84, Loss: 0.869679, Accuracy: 87.20%\n",
      "Batch 85, Loss: 0.913091, Accuracy: 87.15%\n",
      "Batch 86, Loss: 0.842224, Accuracy: 87.19%\n",
      "Batch 87, Loss: 0.845979, Accuracy: 87.21%\n",
      "Batch 88, Loss: 0.829409, Accuracy: 87.27%\n",
      "Batch 89, Loss: 0.904704, Accuracy: 87.22%\n",
      "Batch 90, Loss: 0.915487, Accuracy: 87.17%\n",
      "Batch 91, Loss: 0.806549, Accuracy: 87.24%\n",
      "Batch 92, Loss: 0.928947, Accuracy: 87.18%\n",
      "Batch 93, Loss: 0.887573, Accuracy: 87.18%\n",
      "Batch 94, Loss: 0.839981, Accuracy: 87.22%\n",
      "Batch 95, Loss: 0.826278, Accuracy: 87.25%\n",
      "Batch 96, Loss: 0.887382, Accuracy: 87.24%\n",
      "Batch 97, Loss: 0.859031, Accuracy: 87.26%\n",
      "Batch 98, Loss: 0.923239, Accuracy: 87.18%\n",
      "Batch 99, Loss: 0.821596, Accuracy: 87.23%\n",
      "Batch 100, Loss: 0.902172, Accuracy: 87.19%\n",
      "Batch 101, Loss: 0.806691, Accuracy: 87.24%\n",
      "Batch 102, Loss: 0.894953, Accuracy: 87.24%\n",
      "Batch 103, Loss: 0.904635, Accuracy: 87.21%\n",
      "Batch 104, Loss: 0.858173, Accuracy: 87.21%\n",
      "Batch 105, Loss: 0.870070, Accuracy: 87.22%\n",
      "Batch 106, Loss: 0.892416, Accuracy: 87.21%\n",
      "Batch 107, Loss: 0.856687, Accuracy: 87.22%\n",
      "Batch 108, Loss: 0.904786, Accuracy: 87.21%\n",
      "Batch 109, Loss: 0.853660, Accuracy: 87.24%\n",
      "Batch 110, Loss: 0.884553, Accuracy: 87.24%\n",
      "Batch 111, Loss: 0.823599, Accuracy: 87.30%\n",
      "Batch 112, Loss: 0.906233, Accuracy: 87.26%\n",
      "Batch 113, Loss: 0.775977, Accuracy: 87.36%\n",
      "Batch 114, Loss: 0.901728, Accuracy: 87.34%\n",
      "Batch 115, Loss: 0.853412, Accuracy: 87.35%\n",
      "Batch 116, Loss: 0.853985, Accuracy: 87.37%\n",
      "Batch 117, Loss: 0.914446, Accuracy: 87.33%\n",
      "Batch 118, Loss: 0.864468, Accuracy: 87.34%\n",
      "Batch 119, Loss: 0.919879, Accuracy: 87.30%\n",
      "Batch 120, Loss: 0.855562, Accuracy: 87.33%\n",
      "Batch 121, Loss: 0.861662, Accuracy: 87.33%\n",
      "Batch 122, Loss: 0.933535, Accuracy: 87.28%\n",
      "Batch 123, Loss: 0.885307, Accuracy: 87.27%\n",
      "Batch 124, Loss: 0.832571, Accuracy: 87.30%\n",
      "Batch 125, Loss: 0.890529, Accuracy: 87.28%\n",
      "Batch 126, Loss: 0.822040, Accuracy: 87.30%\n",
      "Batch 127, Loss: 0.908315, Accuracy: 87.27%\n",
      "Batch 128, Loss: 0.902274, Accuracy: 87.24%\n",
      "Batch 129, Loss: 0.885657, Accuracy: 87.23%\n",
      "Batch 130, Loss: 0.894142, Accuracy: 87.20%\n",
      "Batch 131, Loss: 0.936010, Accuracy: 87.15%\n",
      "Batch 132, Loss: 0.847855, Accuracy: 87.17%\n",
      "Batch 133, Loss: 0.908920, Accuracy: 87.15%\n",
      "Batch 134, Loss: 0.942366, Accuracy: 87.09%\n",
      "Batch 135, Loss: 0.877891, Accuracy: 87.09%\n",
      "Batch 136, Loss: 0.905771, Accuracy: 87.06%\n",
      "Batch 137, Loss: 0.906954, Accuracy: 87.04%\n",
      "Batch 138, Loss: 0.856547, Accuracy: 87.07%\n",
      "Batch 139, Loss: 0.817600, Accuracy: 87.12%\n",
      "Batch 140, Loss: 0.881405, Accuracy: 87.12%\n",
      "Batch 141, Loss: 0.916395, Accuracy: 87.09%\n",
      "Batch 142, Loss: 0.834983, Accuracy: 87.11%\n",
      "Batch 143, Loss: 0.858706, Accuracy: 87.13%\n",
      "Batch 144, Loss: 0.958025, Accuracy: 87.07%\n",
      "Batch 145, Loss: 0.853227, Accuracy: 87.09%\n",
      "Batch 146, Loss: 0.913325, Accuracy: 87.06%\n",
      "Batch 147, Loss: 0.935368, Accuracy: 87.01%\n",
      "Batch 148, Loss: 0.954639, Accuracy: 86.97%\n",
      "Batch 149, Loss: 0.926611, Accuracy: 86.93%\n",
      "Batch 150, Loss: 0.888724, Accuracy: 86.92%\n",
      "Batch 151, Loss: 0.895337, Accuracy: 86.91%\n",
      "Batch 152, Loss: 0.862878, Accuracy: 86.92%\n",
      "Batch 153, Loss: 0.853386, Accuracy: 86.94%\n",
      "Batch 154, Loss: 0.844634, Accuracy: 86.96%\n",
      "Batch 155, Loss: 0.852532, Accuracy: 86.98%\n",
      "Batch 156, Loss: 0.866492, Accuracy: 86.98%\n",
      "Batch 157, Loss: 0.922304, Accuracy: 86.94%\n",
      "Batch 158, Loss: 0.949500, Accuracy: 86.89%\n",
      "Batch 159, Loss: 0.871985, Accuracy: 86.89%\n",
      "Batch 160, Loss: 0.904862, Accuracy: 86.87%\n",
      "Batch 161, Loss: 0.849392, Accuracy: 86.89%\n",
      "Batch 162, Loss: 0.889192, Accuracy: 86.87%\n",
      "Batch 163, Loss: 0.841708, Accuracy: 86.90%\n",
      "Batch 164, Loss: 0.847408, Accuracy: 86.91%\n",
      "Batch 165, Loss: 0.922607, Accuracy: 86.88%\n",
      "Batch 166, Loss: 0.847973, Accuracy: 86.90%\n",
      "Batch 167, Loss: 0.865636, Accuracy: 86.90%\n",
      "Batch 168, Loss: 0.953744, Accuracy: 86.86%\n",
      "Batch 169, Loss: 0.903157, Accuracy: 86.84%\n",
      "Batch 170, Loss: 0.901003, Accuracy: 86.84%\n",
      "Batch 171, Loss: 0.907072, Accuracy: 86.82%\n",
      "Batch 172, Loss: 0.838152, Accuracy: 86.86%\n",
      "Batch 173, Loss: 0.863179, Accuracy: 86.87%\n",
      "Batch 174, Loss: 0.878125, Accuracy: 86.87%\n",
      "Batch 175, Loss: 0.876696, Accuracy: 86.87%\n",
      "Batch 176, Loss: 0.912101, Accuracy: 86.85%\n",
      "Batch 177, Loss: 0.902900, Accuracy: 86.84%\n",
      "Batch 178, Loss: 0.782010, Accuracy: 86.89%\n",
      "Batch 179, Loss: 0.828736, Accuracy: 86.93%\n",
      "Batch 180, Loss: 0.944988, Accuracy: 86.89%\n",
      "Batch 181, Loss: 0.924321, Accuracy: 86.86%\n",
      "Batch 182, Loss: 0.923340, Accuracy: 86.83%\n",
      "Batch 183, Loss: 0.816922, Accuracy: 86.86%\n",
      "Batch 184, Loss: 0.840980, Accuracy: 86.89%\n",
      "Batch 185, Loss: 0.936528, Accuracy: 86.84%\n",
      "Batch 186, Loss: 0.854862, Accuracy: 86.86%\n",
      "Batch 187, Loss: 0.975366, Accuracy: 86.81%\n",
      "Batch 188, Loss: 0.851714, Accuracy: 86.82%\n",
      "Batch 189, Loss: 0.872609, Accuracy: 86.83%\n",
      "Batch 190, Loss: 0.893180, Accuracy: 86.83%\n",
      "Batch 191, Loss: 0.925571, Accuracy: 86.80%\n",
      "Batch 192, Loss: 0.930889, Accuracy: 86.77%\n",
      "Batch 193, Loss: 0.916774, Accuracy: 86.75%\n",
      "Batch 194, Loss: 0.850638, Accuracy: 86.75%\n",
      "Batch 195, Loss: 0.833741, Accuracy: 86.77%\n",
      "Batch 196, Loss: 0.867418, Accuracy: 86.77%\n",
      "Batch 197, Loss: 0.876451, Accuracy: 86.78%\n",
      "Batch 198, Loss: 0.884030, Accuracy: 86.77%\n",
      "Batch 199, Loss: 0.847727, Accuracy: 86.79%\n",
      "Batch 200, Loss: 0.862784, Accuracy: 86.80%\n",
      "Batch 201, Loss: 0.927950, Accuracy: 86.77%\n",
      "Batch 202, Loss: 0.898598, Accuracy: 86.77%\n",
      "Batch 203, Loss: 0.845346, Accuracy: 86.78%\n",
      "Batch 204, Loss: 0.857456, Accuracy: 86.79%\n",
      "Batch 205, Loss: 0.894984, Accuracy: 86.78%\n",
      "Batch 206, Loss: 0.864577, Accuracy: 86.79%\n",
      "Batch 207, Loss: 0.911057, Accuracy: 86.78%\n",
      "Batch 208, Loss: 0.874824, Accuracy: 86.77%\n",
      "Batch 209, Loss: 0.871598, Accuracy: 86.77%\n",
      "Batch 210, Loss: 0.886968, Accuracy: 86.78%\n",
      "Batch 211, Loss: 0.879001, Accuracy: 86.78%\n",
      "Batch 212, Loss: 0.851339, Accuracy: 86.79%\n",
      "Batch 213, Loss: 0.890616, Accuracy: 86.78%\n",
      "Training - Epoch 80, Loss: 0.877571, Accuracy: 86.78%\n",
      "Validation Batch 1, Loss: 0.855676, Accuracy: 89.06%\n",
      "Validation Batch 2, Loss: 0.837851, Accuracy: 91.41%\n",
      "Validation Batch 3, Loss: 0.920660, Accuracy: 88.54%\n",
      "Validation Batch 4, Loss: 0.870049, Accuracy: 88.28%\n",
      "Validation Batch 5, Loss: 0.854139, Accuracy: 88.75%\n",
      "Validation Batch 6, Loss: 0.824959, Accuracy: 89.58%\n",
      "Validation Batch 7, Loss: 0.839407, Accuracy: 89.73%\n",
      "Validation Batch 8, Loss: 0.927202, Accuracy: 88.67%\n",
      "Validation Batch 9, Loss: 0.912753, Accuracy: 88.19%\n",
      "Validation Batch 10, Loss: 0.873249, Accuracy: 88.12%\n",
      "Validation Batch 11, Loss: 0.861811, Accuracy: 88.21%\n",
      "Validation Batch 12, Loss: 0.861205, Accuracy: 88.41%\n",
      "Validation Batch 13, Loss: 0.882051, Accuracy: 88.22%\n",
      "Validation Batch 14, Loss: 0.895881, Accuracy: 87.95%\n",
      "Validation Batch 15, Loss: 0.851866, Accuracy: 87.92%\n",
      "Validation Batch 16, Loss: 0.864160, Accuracy: 88.18%\n",
      "Validation Batch 17, Loss: 0.911184, Accuracy: 87.96%\n",
      "Validation Batch 18, Loss: 0.859308, Accuracy: 88.02%\n",
      "Validation Batch 19, Loss: 0.923971, Accuracy: 87.75%\n",
      "Validation Batch 20, Loss: 0.939616, Accuracy: 87.34%\n",
      "Validation Batch 21, Loss: 0.896491, Accuracy: 87.28%\n",
      "Validation Batch 22, Loss: 0.887365, Accuracy: 87.22%\n",
      "Validation Batch 23, Loss: 0.921072, Accuracy: 87.02%\n",
      "Validation Batch 24, Loss: 0.902188, Accuracy: 86.98%\n",
      "Validation Batch 25, Loss: 0.847040, Accuracy: 87.12%\n",
      "Validation Batch 26, Loss: 0.882992, Accuracy: 87.14%\n",
      "Validation Batch 27, Loss: 0.845979, Accuracy: 87.26%\n",
      "Validation - Epoch 80, Loss: 0.879634, Accuracy: 87.26%\n",
      "Patience—2\n",
      "Epoch 81\n",
      "Batch 1, Loss: 0.919419, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.895651, Accuracy: 83.59%\n",
      "Batch 3, Loss: 0.925482, Accuracy: 83.33%\n",
      "Batch 4, Loss: 0.872900, Accuracy: 84.38%\n",
      "Batch 5, Loss: 0.822982, Accuracy: 85.94%\n",
      "Batch 6, Loss: 0.832984, Accuracy: 86.72%\n",
      "Batch 7, Loss: 0.849015, Accuracy: 87.05%\n",
      "Batch 8, Loss: 0.862379, Accuracy: 87.30%\n",
      "Batch 9, Loss: 0.922361, Accuracy: 86.81%\n",
      "Batch 10, Loss: 0.925432, Accuracy: 86.41%\n",
      "Batch 11, Loss: 0.876796, Accuracy: 86.36%\n",
      "Batch 12, Loss: 0.874680, Accuracy: 86.33%\n",
      "Batch 13, Loss: 0.828829, Accuracy: 86.78%\n",
      "Batch 14, Loss: 0.960507, Accuracy: 86.05%\n",
      "Batch 15, Loss: 0.875095, Accuracy: 86.15%\n",
      "Batch 16, Loss: 0.847722, Accuracy: 86.52%\n",
      "Batch 17, Loss: 0.854381, Accuracy: 86.67%\n",
      "Batch 18, Loss: 0.925251, Accuracy: 86.46%\n",
      "Batch 19, Loss: 0.883396, Accuracy: 86.51%\n",
      "Batch 20, Loss: 0.862523, Accuracy: 86.64%\n",
      "Batch 21, Loss: 0.886818, Accuracy: 86.61%\n",
      "Batch 22, Loss: 0.836572, Accuracy: 86.79%\n",
      "Batch 23, Loss: 0.923811, Accuracy: 86.48%\n",
      "Batch 24, Loss: 0.998441, Accuracy: 86.00%\n",
      "Batch 25, Loss: 0.877245, Accuracy: 86.06%\n",
      "Batch 26, Loss: 0.836560, Accuracy: 86.30%\n",
      "Batch 27, Loss: 0.837410, Accuracy: 86.46%\n",
      "Batch 28, Loss: 0.929419, Accuracy: 86.33%\n",
      "Batch 29, Loss: 0.824582, Accuracy: 86.48%\n",
      "Batch 30, Loss: 0.822050, Accuracy: 86.67%\n",
      "Batch 31, Loss: 0.828286, Accuracy: 86.84%\n",
      "Batch 32, Loss: 0.835982, Accuracy: 86.96%\n",
      "Batch 33, Loss: 0.821733, Accuracy: 87.12%\n",
      "Batch 34, Loss: 0.894455, Accuracy: 87.13%\n",
      "Batch 35, Loss: 0.889867, Accuracy: 87.10%\n",
      "Batch 36, Loss: 0.865939, Accuracy: 87.11%\n",
      "Batch 37, Loss: 0.859523, Accuracy: 87.16%\n",
      "Batch 38, Loss: 0.839817, Accuracy: 87.25%\n",
      "Batch 39, Loss: 0.861057, Accuracy: 87.30%\n",
      "Batch 40, Loss: 0.896455, Accuracy: 87.27%\n",
      "Batch 41, Loss: 0.897899, Accuracy: 87.23%\n",
      "Batch 42, Loss: 0.853988, Accuracy: 87.24%\n",
      "Batch 43, Loss: 0.854268, Accuracy: 87.25%\n",
      "Batch 44, Loss: 0.896485, Accuracy: 87.18%\n",
      "Batch 45, Loss: 0.872783, Accuracy: 87.19%\n",
      "Batch 46, Loss: 0.915657, Accuracy: 87.06%\n",
      "Batch 47, Loss: 0.867887, Accuracy: 87.07%\n",
      "Batch 48, Loss: 0.861123, Accuracy: 87.11%\n",
      "Batch 49, Loss: 0.937591, Accuracy: 86.99%\n",
      "Batch 50, Loss: 0.861693, Accuracy: 87.03%\n",
      "Batch 51, Loss: 0.886899, Accuracy: 87.01%\n",
      "Batch 52, Loss: 0.855433, Accuracy: 87.08%\n",
      "Batch 53, Loss: 0.862754, Accuracy: 87.09%\n",
      "Batch 54, Loss: 0.904165, Accuracy: 87.04%\n",
      "Batch 55, Loss: 0.901147, Accuracy: 86.99%\n",
      "Batch 56, Loss: 0.876298, Accuracy: 86.97%\n",
      "Batch 57, Loss: 0.828326, Accuracy: 87.06%\n",
      "Batch 58, Loss: 0.893981, Accuracy: 86.99%\n",
      "Batch 59, Loss: 0.888215, Accuracy: 86.97%\n",
      "Batch 60, Loss: 0.791231, Accuracy: 87.11%\n",
      "Batch 61, Loss: 0.859438, Accuracy: 87.14%\n",
      "Batch 62, Loss: 0.827034, Accuracy: 87.25%\n",
      "Batch 63, Loss: 0.909086, Accuracy: 87.18%\n",
      "Batch 64, Loss: 0.872086, Accuracy: 87.21%\n",
      "Batch 65, Loss: 0.905047, Accuracy: 87.16%\n",
      "Batch 66, Loss: 0.967007, Accuracy: 87.03%\n",
      "Batch 67, Loss: 0.866588, Accuracy: 87.03%\n",
      "Batch 68, Loss: 0.955093, Accuracy: 86.90%\n",
      "Batch 69, Loss: 0.854096, Accuracy: 86.96%\n",
      "Batch 70, Loss: 0.903925, Accuracy: 86.92%\n",
      "Batch 71, Loss: 0.855704, Accuracy: 86.93%\n",
      "Batch 72, Loss: 0.949813, Accuracy: 86.83%\n",
      "Batch 73, Loss: 0.937747, Accuracy: 86.71%\n",
      "Batch 74, Loss: 0.939560, Accuracy: 86.61%\n",
      "Batch 75, Loss: 0.929213, Accuracy: 86.52%\n",
      "Batch 76, Loss: 0.837646, Accuracy: 86.57%\n",
      "Batch 77, Loss: 0.972205, Accuracy: 86.44%\n",
      "Batch 78, Loss: 0.816156, Accuracy: 86.54%\n",
      "Batch 79, Loss: 0.891297, Accuracy: 86.53%\n",
      "Batch 80, Loss: 0.864220, Accuracy: 86.56%\n",
      "Batch 81, Loss: 0.833833, Accuracy: 86.61%\n",
      "Batch 82, Loss: 0.880159, Accuracy: 86.59%\n",
      "Batch 83, Loss: 0.864405, Accuracy: 86.60%\n",
      "Batch 84, Loss: 0.886374, Accuracy: 86.59%\n",
      "Batch 85, Loss: 0.798280, Accuracy: 86.69%\n",
      "Batch 86, Loss: 0.895186, Accuracy: 86.70%\n",
      "Batch 87, Loss: 0.893392, Accuracy: 86.69%\n",
      "Batch 88, Loss: 0.930064, Accuracy: 86.63%\n",
      "Batch 89, Loss: 0.902000, Accuracy: 86.60%\n",
      "Batch 90, Loss: 0.921528, Accuracy: 86.55%\n",
      "Batch 91, Loss: 0.946237, Accuracy: 86.45%\n",
      "Batch 92, Loss: 0.901673, Accuracy: 86.45%\n",
      "Batch 93, Loss: 0.924968, Accuracy: 86.41%\n",
      "Batch 94, Loss: 0.891935, Accuracy: 86.40%\n",
      "Batch 95, Loss: 0.869339, Accuracy: 86.41%\n",
      "Batch 96, Loss: 0.969334, Accuracy: 86.34%\n",
      "Batch 97, Loss: 0.869343, Accuracy: 86.37%\n",
      "Batch 98, Loss: 0.797000, Accuracy: 86.46%\n",
      "Batch 99, Loss: 0.830132, Accuracy: 86.52%\n",
      "Batch 100, Loss: 0.872824, Accuracy: 86.55%\n",
      "Batch 101, Loss: 0.866643, Accuracy: 86.57%\n",
      "Batch 102, Loss: 0.867549, Accuracy: 86.58%\n",
      "Batch 103, Loss: 0.918984, Accuracy: 86.54%\n",
      "Batch 104, Loss: 0.821209, Accuracy: 86.60%\n",
      "Batch 105, Loss: 0.926903, Accuracy: 86.53%\n",
      "Batch 106, Loss: 0.880746, Accuracy: 86.51%\n",
      "Batch 107, Loss: 0.878203, Accuracy: 86.52%\n",
      "Batch 108, Loss: 0.840389, Accuracy: 86.55%\n",
      "Batch 109, Loss: 0.902789, Accuracy: 86.51%\n",
      "Batch 110, Loss: 0.851483, Accuracy: 86.55%\n",
      "Batch 111, Loss: 0.873396, Accuracy: 86.56%\n",
      "Batch 112, Loss: 0.880722, Accuracy: 86.54%\n",
      "Batch 113, Loss: 0.844158, Accuracy: 86.55%\n",
      "Batch 114, Loss: 0.831360, Accuracy: 86.58%\n",
      "Batch 115, Loss: 0.877919, Accuracy: 86.59%\n",
      "Batch 116, Loss: 0.827403, Accuracy: 86.64%\n",
      "Batch 117, Loss: 0.881093, Accuracy: 86.62%\n",
      "Batch 118, Loss: 0.903561, Accuracy: 86.61%\n",
      "Batch 119, Loss: 0.832567, Accuracy: 86.65%\n",
      "Batch 120, Loss: 0.863206, Accuracy: 86.65%\n",
      "Batch 121, Loss: 0.894261, Accuracy: 86.65%\n",
      "Batch 122, Loss: 0.843564, Accuracy: 86.68%\n",
      "Batch 123, Loss: 0.871429, Accuracy: 86.67%\n",
      "Batch 124, Loss: 0.842719, Accuracy: 86.71%\n",
      "Batch 125, Loss: 0.907060, Accuracy: 86.69%\n",
      "Batch 126, Loss: 0.908776, Accuracy: 86.66%\n",
      "Batch 127, Loss: 0.973945, Accuracy: 86.58%\n",
      "Batch 128, Loss: 0.830176, Accuracy: 86.61%\n",
      "Batch 129, Loss: 0.880410, Accuracy: 86.60%\n",
      "Batch 130, Loss: 0.916385, Accuracy: 86.59%\n",
      "Batch 131, Loss: 0.860862, Accuracy: 86.61%\n",
      "Batch 132, Loss: 0.891908, Accuracy: 86.60%\n",
      "Batch 133, Loss: 0.848674, Accuracy: 86.62%\n",
      "Batch 134, Loss: 0.864123, Accuracy: 86.63%\n",
      "Batch 135, Loss: 0.898104, Accuracy: 86.62%\n",
      "Batch 136, Loss: 0.881281, Accuracy: 86.62%\n",
      "Batch 137, Loss: 0.856063, Accuracy: 86.62%\n",
      "Batch 138, Loss: 0.916447, Accuracy: 86.58%\n",
      "Batch 139, Loss: 0.926115, Accuracy: 86.54%\n",
      "Batch 140, Loss: 0.869657, Accuracy: 86.56%\n",
      "Batch 141, Loss: 0.873930, Accuracy: 86.56%\n",
      "Batch 142, Loss: 0.870751, Accuracy: 86.56%\n",
      "Batch 143, Loss: 0.836537, Accuracy: 86.60%\n",
      "Batch 144, Loss: 0.866322, Accuracy: 86.61%\n",
      "Batch 145, Loss: 0.892889, Accuracy: 86.61%\n",
      "Batch 146, Loss: 0.861679, Accuracy: 86.61%\n",
      "Batch 147, Loss: 0.845634, Accuracy: 86.64%\n",
      "Batch 148, Loss: 0.882074, Accuracy: 86.63%\n",
      "Batch 149, Loss: 0.842925, Accuracy: 86.66%\n",
      "Batch 150, Loss: 0.900933, Accuracy: 86.65%\n",
      "Batch 151, Loss: 0.887423, Accuracy: 86.63%\n",
      "Batch 152, Loss: 0.864456, Accuracy: 86.65%\n",
      "Batch 153, Loss: 0.824067, Accuracy: 86.69%\n",
      "Batch 154, Loss: 0.864364, Accuracy: 86.70%\n",
      "Batch 155, Loss: 0.927785, Accuracy: 86.66%\n",
      "Batch 156, Loss: 0.855437, Accuracy: 86.68%\n",
      "Batch 157, Loss: 0.905461, Accuracy: 86.66%\n",
      "Batch 158, Loss: 0.886449, Accuracy: 86.66%\n",
      "Batch 159, Loss: 0.853610, Accuracy: 86.67%\n",
      "Batch 160, Loss: 0.864972, Accuracy: 86.67%\n",
      "Batch 161, Loss: 0.861068, Accuracy: 86.68%\n",
      "Batch 162, Loss: 0.847984, Accuracy: 86.69%\n",
      "Batch 163, Loss: 0.876843, Accuracy: 86.69%\n",
      "Batch 164, Loss: 0.884590, Accuracy: 86.69%\n",
      "Batch 165, Loss: 0.982757, Accuracy: 86.63%\n",
      "Batch 166, Loss: 0.875302, Accuracy: 86.62%\n",
      "Batch 167, Loss: 0.806713, Accuracy: 86.66%\n",
      "Batch 168, Loss: 0.849450, Accuracy: 86.67%\n",
      "Batch 169, Loss: 0.858405, Accuracy: 86.68%\n",
      "Batch 170, Loss: 0.949266, Accuracy: 86.64%\n",
      "Batch 171, Loss: 0.829246, Accuracy: 86.67%\n",
      "Batch 172, Loss: 0.923581, Accuracy: 86.65%\n",
      "Batch 173, Loss: 0.886356, Accuracy: 86.64%\n",
      "Batch 174, Loss: 0.880066, Accuracy: 86.65%\n",
      "Batch 175, Loss: 0.891771, Accuracy: 86.64%\n",
      "Batch 176, Loss: 0.941511, Accuracy: 86.60%\n",
      "Batch 177, Loss: 0.795544, Accuracy: 86.66%\n",
      "Batch 178, Loss: 0.877271, Accuracy: 86.67%\n",
      "Batch 179, Loss: 0.869922, Accuracy: 86.69%\n",
      "Batch 180, Loss: 0.908599, Accuracy: 86.67%\n",
      "Batch 181, Loss: 0.851752, Accuracy: 86.68%\n",
      "Batch 182, Loss: 0.846137, Accuracy: 86.70%\n",
      "Batch 183, Loss: 0.865589, Accuracy: 86.71%\n",
      "Batch 184, Loss: 0.885650, Accuracy: 86.70%\n",
      "Batch 185, Loss: 0.908517, Accuracy: 86.70%\n",
      "Batch 186, Loss: 0.839515, Accuracy: 86.72%\n",
      "Batch 187, Loss: 0.859017, Accuracy: 86.73%\n",
      "Batch 188, Loss: 0.826154, Accuracy: 86.76%\n",
      "Batch 189, Loss: 0.904778, Accuracy: 86.75%\n",
      "Batch 190, Loss: 0.872259, Accuracy: 86.75%\n",
      "Batch 191, Loss: 0.847988, Accuracy: 86.77%\n",
      "Batch 192, Loss: 0.856641, Accuracy: 86.78%\n",
      "Batch 193, Loss: 0.936565, Accuracy: 86.75%\n",
      "Batch 194, Loss: 0.912854, Accuracy: 86.73%\n",
      "Batch 195, Loss: 0.868946, Accuracy: 86.74%\n",
      "Batch 196, Loss: 0.878345, Accuracy: 86.74%\n",
      "Batch 197, Loss: 0.840213, Accuracy: 86.76%\n",
      "Batch 198, Loss: 0.904927, Accuracy: 86.75%\n",
      "Batch 199, Loss: 0.890185, Accuracy: 86.75%\n",
      "Batch 200, Loss: 0.860441, Accuracy: 86.75%\n",
      "Batch 201, Loss: 0.899309, Accuracy: 86.75%\n",
      "Batch 202, Loss: 0.895705, Accuracy: 86.74%\n",
      "Batch 203, Loss: 0.825811, Accuracy: 86.77%\n",
      "Batch 204, Loss: 0.909481, Accuracy: 86.75%\n",
      "Batch 205, Loss: 0.904662, Accuracy: 86.72%\n",
      "Batch 206, Loss: 0.880048, Accuracy: 86.73%\n",
      "Batch 207, Loss: 0.891576, Accuracy: 86.71%\n",
      "Batch 208, Loss: 0.957233, Accuracy: 86.68%\n",
      "Batch 209, Loss: 0.853399, Accuracy: 86.69%\n",
      "Batch 210, Loss: 0.892822, Accuracy: 86.68%\n",
      "Batch 211, Loss: 0.816294, Accuracy: 86.71%\n",
      "Batch 212, Loss: 0.837063, Accuracy: 86.73%\n",
      "Batch 213, Loss: 0.873735, Accuracy: 86.74%\n",
      "Training - Epoch 81, Loss: 0.878086, Accuracy: 86.74%\n",
      "Validation Batch 1, Loss: 0.850031, Accuracy: 90.62%\n",
      "Validation Batch 2, Loss: 0.832900, Accuracy: 92.19%\n",
      "Validation Batch 3, Loss: 0.916305, Accuracy: 89.58%\n",
      "Validation Batch 4, Loss: 0.868859, Accuracy: 88.67%\n",
      "Validation Batch 5, Loss: 0.854596, Accuracy: 88.75%\n",
      "Validation Batch 6, Loss: 0.821623, Accuracy: 89.58%\n",
      "Validation Batch 7, Loss: 0.833979, Accuracy: 89.73%\n",
      "Validation Batch 8, Loss: 0.922371, Accuracy: 88.87%\n",
      "Validation Batch 9, Loss: 0.913261, Accuracy: 88.37%\n",
      "Validation Batch 10, Loss: 0.860949, Accuracy: 88.28%\n",
      "Validation Batch 11, Loss: 0.856861, Accuracy: 88.35%\n",
      "Validation Batch 12, Loss: 0.866891, Accuracy: 88.41%\n",
      "Validation Batch 13, Loss: 0.871954, Accuracy: 88.34%\n",
      "Validation Batch 14, Loss: 0.894682, Accuracy: 88.06%\n",
      "Validation Batch 15, Loss: 0.847416, Accuracy: 88.12%\n",
      "Validation Batch 16, Loss: 0.856922, Accuracy: 88.28%\n",
      "Validation Batch 17, Loss: 0.901485, Accuracy: 88.05%\n",
      "Validation Batch 18, Loss: 0.860035, Accuracy: 88.02%\n",
      "Validation Batch 19, Loss: 0.914115, Accuracy: 87.75%\n",
      "Validation Batch 20, Loss: 0.932931, Accuracy: 87.42%\n",
      "Validation Batch 21, Loss: 0.892614, Accuracy: 87.35%\n",
      "Validation Batch 22, Loss: 0.880319, Accuracy: 87.29%\n",
      "Validation Batch 23, Loss: 0.900892, Accuracy: 87.23%\n",
      "Validation Batch 24, Loss: 0.885387, Accuracy: 87.17%\n",
      "Validation Batch 25, Loss: 0.845507, Accuracy: 87.31%\n",
      "Validation Batch 26, Loss: 0.879888, Accuracy: 87.32%\n",
      "Validation Batch 27, Loss: 0.831041, Accuracy: 87.43%\n",
      "Validation - Epoch 81, Loss: 0.873845, Accuracy: 87.43%\n",
      "Patience—3\n",
      "Epoch 82\n",
      "Batch 1, Loss: 0.837883, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.847826, Accuracy: 90.62%\n",
      "Batch 3, Loss: 0.824233, Accuracy: 91.15%\n",
      "Batch 4, Loss: 0.806278, Accuracy: 91.80%\n",
      "Batch 5, Loss: 0.982429, Accuracy: 88.44%\n",
      "Batch 6, Loss: 0.868001, Accuracy: 88.28%\n",
      "Batch 7, Loss: 0.914696, Accuracy: 87.50%\n",
      "Batch 8, Loss: 0.904571, Accuracy: 87.11%\n",
      "Batch 9, Loss: 0.826108, Accuracy: 87.67%\n",
      "Batch 10, Loss: 0.924007, Accuracy: 87.03%\n",
      "Batch 11, Loss: 0.916019, Accuracy: 86.79%\n",
      "Batch 12, Loss: 0.867547, Accuracy: 86.85%\n",
      "Batch 13, Loss: 0.901806, Accuracy: 86.54%\n",
      "Batch 14, Loss: 0.876681, Accuracy: 86.38%\n",
      "Batch 15, Loss: 0.873429, Accuracy: 86.35%\n",
      "Batch 16, Loss: 0.832884, Accuracy: 86.72%\n",
      "Batch 17, Loss: 0.949098, Accuracy: 86.12%\n",
      "Batch 18, Loss: 0.885885, Accuracy: 86.02%\n",
      "Batch 19, Loss: 0.906358, Accuracy: 85.94%\n",
      "Batch 20, Loss: 0.948169, Accuracy: 85.55%\n",
      "Batch 21, Loss: 0.926463, Accuracy: 85.27%\n",
      "Batch 22, Loss: 0.934342, Accuracy: 85.01%\n",
      "Batch 23, Loss: 0.867423, Accuracy: 85.12%\n",
      "Batch 24, Loss: 0.944021, Accuracy: 84.83%\n",
      "Batch 25, Loss: 0.897179, Accuracy: 84.88%\n",
      "Batch 26, Loss: 0.860004, Accuracy: 84.92%\n",
      "Batch 27, Loss: 0.905033, Accuracy: 84.84%\n",
      "Batch 28, Loss: 0.842701, Accuracy: 85.04%\n",
      "Batch 29, Loss: 0.887480, Accuracy: 85.08%\n",
      "Batch 30, Loss: 0.882529, Accuracy: 85.05%\n",
      "Batch 31, Loss: 0.836090, Accuracy: 85.23%\n",
      "Batch 32, Loss: 0.876588, Accuracy: 85.30%\n",
      "Batch 33, Loss: 0.841243, Accuracy: 85.46%\n",
      "Batch 34, Loss: 0.833813, Accuracy: 85.62%\n",
      "Batch 35, Loss: 0.861248, Accuracy: 85.67%\n",
      "Batch 36, Loss: 0.864422, Accuracy: 85.72%\n",
      "Batch 37, Loss: 0.833649, Accuracy: 85.90%\n",
      "Batch 38, Loss: 0.881984, Accuracy: 85.86%\n",
      "Batch 39, Loss: 0.882475, Accuracy: 85.90%\n",
      "Batch 40, Loss: 0.858316, Accuracy: 85.98%\n",
      "Batch 41, Loss: 0.933403, Accuracy: 85.79%\n",
      "Batch 42, Loss: 0.928973, Accuracy: 85.68%\n",
      "Batch 43, Loss: 0.912482, Accuracy: 85.61%\n",
      "Batch 44, Loss: 0.828467, Accuracy: 85.76%\n",
      "Batch 45, Loss: 0.866808, Accuracy: 85.80%\n",
      "Batch 46, Loss: 0.902301, Accuracy: 85.77%\n",
      "Batch 47, Loss: 0.863124, Accuracy: 85.80%\n",
      "Batch 48, Loss: 0.884711, Accuracy: 85.81%\n",
      "Batch 49, Loss: 0.807518, Accuracy: 86.00%\n",
      "Batch 50, Loss: 0.905845, Accuracy: 85.88%\n",
      "Batch 51, Loss: 0.920407, Accuracy: 85.81%\n",
      "Batch 52, Loss: 0.831934, Accuracy: 85.94%\n",
      "Batch 53, Loss: 0.791133, Accuracy: 86.14%\n",
      "Batch 54, Loss: 0.867780, Accuracy: 86.17%\n",
      "Batch 55, Loss: 0.922858, Accuracy: 86.05%\n",
      "Batch 56, Loss: 0.885938, Accuracy: 86.08%\n",
      "Batch 57, Loss: 0.874822, Accuracy: 86.07%\n",
      "Batch 58, Loss: 0.895745, Accuracy: 86.02%\n",
      "Batch 59, Loss: 0.883199, Accuracy: 86.04%\n",
      "Batch 60, Loss: 0.888078, Accuracy: 86.07%\n",
      "Batch 61, Loss: 0.823329, Accuracy: 86.17%\n",
      "Batch 62, Loss: 0.876301, Accuracy: 86.16%\n",
      "Batch 63, Loss: 0.854394, Accuracy: 86.24%\n",
      "Batch 64, Loss: 0.876637, Accuracy: 86.25%\n",
      "Batch 65, Loss: 0.864473, Accuracy: 86.27%\n",
      "Batch 66, Loss: 0.821574, Accuracy: 86.34%\n",
      "Batch 67, Loss: 0.878640, Accuracy: 86.36%\n",
      "Batch 68, Loss: 0.857124, Accuracy: 86.40%\n",
      "Batch 69, Loss: 0.888978, Accuracy: 86.35%\n",
      "Batch 70, Loss: 0.818776, Accuracy: 86.45%\n",
      "Batch 71, Loss: 0.853466, Accuracy: 86.49%\n",
      "Batch 72, Loss: 0.851722, Accuracy: 86.52%\n",
      "Batch 73, Loss: 0.867358, Accuracy: 86.54%\n",
      "Batch 74, Loss: 0.859017, Accuracy: 86.55%\n",
      "Batch 75, Loss: 0.851072, Accuracy: 86.60%\n",
      "Batch 76, Loss: 0.842667, Accuracy: 86.66%\n",
      "Batch 77, Loss: 0.903170, Accuracy: 86.61%\n",
      "Batch 78, Loss: 0.832200, Accuracy: 86.68%\n",
      "Batch 79, Loss: 0.891295, Accuracy: 86.65%\n",
      "Batch 80, Loss: 0.882767, Accuracy: 86.64%\n",
      "Batch 81, Loss: 0.904492, Accuracy: 86.61%\n",
      "Batch 82, Loss: 0.948342, Accuracy: 86.53%\n",
      "Batch 83, Loss: 0.866512, Accuracy: 86.54%\n",
      "Batch 84, Loss: 0.862686, Accuracy: 86.55%\n",
      "Batch 85, Loss: 0.882440, Accuracy: 86.53%\n",
      "Batch 86, Loss: 0.854427, Accuracy: 86.56%\n",
      "Batch 87, Loss: 0.867872, Accuracy: 86.58%\n",
      "Batch 88, Loss: 0.862595, Accuracy: 86.59%\n",
      "Batch 89, Loss: 0.925523, Accuracy: 86.55%\n",
      "Batch 90, Loss: 0.975293, Accuracy: 86.46%\n",
      "Batch 91, Loss: 0.885450, Accuracy: 86.45%\n",
      "Batch 92, Loss: 0.857795, Accuracy: 86.48%\n",
      "Batch 93, Loss: 0.885525, Accuracy: 86.46%\n",
      "Batch 94, Loss: 0.911566, Accuracy: 86.44%\n",
      "Batch 95, Loss: 0.838070, Accuracy: 86.50%\n",
      "Batch 96, Loss: 0.866900, Accuracy: 86.51%\n",
      "Batch 97, Loss: 0.953367, Accuracy: 86.42%\n",
      "Batch 98, Loss: 0.843864, Accuracy: 86.46%\n",
      "Batch 99, Loss: 0.854756, Accuracy: 86.51%\n",
      "Batch 100, Loss: 0.852370, Accuracy: 86.53%\n",
      "Batch 101, Loss: 0.904471, Accuracy: 86.53%\n",
      "Batch 102, Loss: 0.835979, Accuracy: 86.57%\n",
      "Batch 103, Loss: 0.914841, Accuracy: 86.54%\n",
      "Batch 104, Loss: 0.842429, Accuracy: 86.58%\n",
      "Batch 105, Loss: 0.815628, Accuracy: 86.67%\n",
      "Batch 106, Loss: 0.923415, Accuracy: 86.60%\n",
      "Batch 107, Loss: 0.838535, Accuracy: 86.64%\n",
      "Batch 108, Loss: 0.878764, Accuracy: 86.65%\n",
      "Batch 109, Loss: 0.923830, Accuracy: 86.61%\n",
      "Batch 110, Loss: 0.874963, Accuracy: 86.62%\n",
      "Batch 111, Loss: 0.902826, Accuracy: 86.60%\n",
      "Batch 112, Loss: 0.911725, Accuracy: 86.57%\n",
      "Batch 113, Loss: 0.915310, Accuracy: 86.52%\n",
      "Batch 114, Loss: 0.858944, Accuracy: 86.53%\n",
      "Batch 115, Loss: 0.843178, Accuracy: 86.55%\n",
      "Batch 116, Loss: 0.862332, Accuracy: 86.57%\n",
      "Batch 117, Loss: 0.825394, Accuracy: 86.61%\n",
      "Batch 118, Loss: 0.909263, Accuracy: 86.56%\n",
      "Batch 119, Loss: 0.887184, Accuracy: 86.55%\n",
      "Batch 120, Loss: 0.837855, Accuracy: 86.59%\n",
      "Batch 121, Loss: 0.861767, Accuracy: 86.62%\n",
      "Batch 122, Loss: 0.895768, Accuracy: 86.60%\n",
      "Batch 123, Loss: 0.881279, Accuracy: 86.60%\n",
      "Batch 124, Loss: 0.829855, Accuracy: 86.64%\n",
      "Batch 125, Loss: 0.809323, Accuracy: 86.70%\n",
      "Batch 126, Loss: 0.914900, Accuracy: 86.67%\n",
      "Batch 127, Loss: 0.835706, Accuracy: 86.70%\n",
      "Batch 128, Loss: 0.871282, Accuracy: 86.71%\n",
      "Batch 129, Loss: 0.883379, Accuracy: 86.70%\n",
      "Batch 130, Loss: 0.905932, Accuracy: 86.68%\n",
      "Batch 131, Loss: 0.888629, Accuracy: 86.68%\n",
      "Batch 132, Loss: 0.870931, Accuracy: 86.68%\n",
      "Batch 133, Loss: 0.863476, Accuracy: 86.69%\n",
      "Batch 134, Loss: 0.868399, Accuracy: 86.68%\n",
      "Batch 135, Loss: 0.906108, Accuracy: 86.67%\n",
      "Batch 136, Loss: 0.804048, Accuracy: 86.73%\n",
      "Batch 137, Loss: 0.849375, Accuracy: 86.76%\n",
      "Batch 138, Loss: 0.827824, Accuracy: 86.80%\n",
      "Batch 139, Loss: 0.861194, Accuracy: 86.80%\n",
      "Batch 140, Loss: 0.912679, Accuracy: 86.79%\n",
      "Batch 141, Loss: 0.910969, Accuracy: 86.76%\n",
      "Batch 142, Loss: 0.920004, Accuracy: 86.73%\n",
      "Batch 143, Loss: 0.883820, Accuracy: 86.72%\n",
      "Batch 144, Loss: 0.821308, Accuracy: 86.77%\n",
      "Batch 145, Loss: 0.930752, Accuracy: 86.72%\n",
      "Batch 146, Loss: 0.901099, Accuracy: 86.70%\n",
      "Batch 147, Loss: 0.890649, Accuracy: 86.68%\n",
      "Batch 148, Loss: 0.865888, Accuracy: 86.69%\n",
      "Batch 149, Loss: 0.926171, Accuracy: 86.66%\n",
      "Batch 150, Loss: 0.994005, Accuracy: 86.57%\n",
      "Batch 151, Loss: 0.799870, Accuracy: 86.62%\n",
      "Batch 152, Loss: 0.819985, Accuracy: 86.67%\n",
      "Batch 153, Loss: 0.854263, Accuracy: 86.69%\n",
      "Batch 154, Loss: 0.904089, Accuracy: 86.67%\n",
      "Batch 155, Loss: 0.894282, Accuracy: 86.66%\n",
      "Batch 156, Loss: 0.875967, Accuracy: 86.67%\n",
      "Batch 157, Loss: 0.890964, Accuracy: 86.66%\n",
      "Batch 158, Loss: 0.823838, Accuracy: 86.70%\n",
      "Batch 159, Loss: 0.919566, Accuracy: 86.67%\n",
      "Batch 160, Loss: 0.885993, Accuracy: 86.66%\n",
      "Batch 161, Loss: 0.911196, Accuracy: 86.64%\n",
      "Batch 162, Loss: 0.922812, Accuracy: 86.61%\n",
      "Batch 163, Loss: 0.957734, Accuracy: 86.57%\n",
      "Batch 164, Loss: 0.837729, Accuracy: 86.59%\n",
      "Batch 165, Loss: 0.826030, Accuracy: 86.63%\n",
      "Batch 166, Loss: 0.886723, Accuracy: 86.62%\n",
      "Batch 167, Loss: 0.853705, Accuracy: 86.64%\n",
      "Batch 168, Loss: 0.836187, Accuracy: 86.67%\n",
      "Batch 169, Loss: 0.869673, Accuracy: 86.69%\n",
      "Batch 170, Loss: 0.878658, Accuracy: 86.68%\n",
      "Batch 171, Loss: 0.940109, Accuracy: 86.63%\n",
      "Batch 172, Loss: 0.974511, Accuracy: 86.57%\n",
      "Batch 173, Loss: 0.872301, Accuracy: 86.59%\n",
      "Batch 174, Loss: 0.907757, Accuracy: 86.57%\n",
      "Batch 175, Loss: 0.859241, Accuracy: 86.58%\n",
      "Batch 176, Loss: 0.924477, Accuracy: 86.55%\n",
      "Batch 177, Loss: 0.878834, Accuracy: 86.56%\n",
      "Batch 178, Loss: 0.867695, Accuracy: 86.56%\n",
      "Batch 179, Loss: 0.822047, Accuracy: 86.59%\n",
      "Batch 180, Loss: 0.845131, Accuracy: 86.61%\n",
      "Batch 181, Loss: 0.914986, Accuracy: 86.57%\n",
      "Batch 182, Loss: 0.876948, Accuracy: 86.56%\n",
      "Batch 183, Loss: 0.884211, Accuracy: 86.57%\n",
      "Batch 184, Loss: 0.858261, Accuracy: 86.59%\n",
      "Batch 185, Loss: 0.858410, Accuracy: 86.61%\n",
      "Batch 186, Loss: 0.865019, Accuracy: 86.60%\n",
      "Batch 187, Loss: 0.831517, Accuracy: 86.64%\n",
      "Batch 188, Loss: 0.827210, Accuracy: 86.67%\n",
      "Batch 189, Loss: 0.907264, Accuracy: 86.65%\n",
      "Batch 190, Loss: 0.879241, Accuracy: 86.65%\n",
      "Batch 191, Loss: 0.851441, Accuracy: 86.67%\n",
      "Batch 192, Loss: 0.889306, Accuracy: 86.65%\n",
      "Batch 193, Loss: 0.853349, Accuracy: 86.68%\n",
      "Batch 194, Loss: 0.871914, Accuracy: 86.68%\n",
      "Batch 195, Loss: 0.845975, Accuracy: 86.68%\n",
      "Batch 196, Loss: 0.903365, Accuracy: 86.67%\n",
      "Batch 197, Loss: 0.881340, Accuracy: 86.68%\n",
      "Batch 198, Loss: 0.840540, Accuracy: 86.70%\n",
      "Batch 199, Loss: 0.934120, Accuracy: 86.68%\n",
      "Batch 200, Loss: 0.892070, Accuracy: 86.66%\n",
      "Batch 201, Loss: 0.888620, Accuracy: 86.66%\n",
      "Batch 202, Loss: 0.888744, Accuracy: 86.66%\n",
      "Batch 203, Loss: 0.886105, Accuracy: 86.65%\n",
      "Batch 204, Loss: 0.888782, Accuracy: 86.64%\n",
      "Batch 205, Loss: 0.985328, Accuracy: 86.58%\n",
      "Batch 206, Loss: 0.886866, Accuracy: 86.57%\n",
      "Batch 207, Loss: 0.915929, Accuracy: 86.55%\n",
      "Batch 208, Loss: 0.851633, Accuracy: 86.56%\n",
      "Batch 209, Loss: 0.840444, Accuracy: 86.59%\n",
      "Batch 210, Loss: 0.858584, Accuracy: 86.60%\n",
      "Batch 211, Loss: 0.835238, Accuracy: 86.62%\n",
      "Batch 212, Loss: 0.850749, Accuracy: 86.64%\n",
      "Batch 213, Loss: 0.854634, Accuracy: 86.65%\n",
      "Training - Epoch 82, Loss: 0.877156, Accuracy: 86.65%\n",
      "Validation Batch 1, Loss: 0.831365, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.816854, Accuracy: 92.97%\n",
      "Validation Batch 3, Loss: 0.908120, Accuracy: 89.58%\n",
      "Validation Batch 4, Loss: 0.844806, Accuracy: 89.84%\n",
      "Validation Batch 5, Loss: 0.840504, Accuracy: 90.00%\n",
      "Validation Batch 6, Loss: 0.804938, Accuracy: 90.62%\n",
      "Validation Batch 7, Loss: 0.817164, Accuracy: 91.07%\n",
      "Validation Batch 8, Loss: 0.912542, Accuracy: 90.23%\n",
      "Validation Batch 9, Loss: 0.899489, Accuracy: 89.58%\n",
      "Validation Batch 10, Loss: 0.846527, Accuracy: 89.84%\n",
      "Validation Batch 11, Loss: 0.849518, Accuracy: 89.91%\n",
      "Validation Batch 12, Loss: 0.835979, Accuracy: 90.10%\n",
      "Validation Batch 13, Loss: 0.852251, Accuracy: 90.14%\n",
      "Validation Batch 14, Loss: 0.876249, Accuracy: 89.73%\n",
      "Validation Batch 15, Loss: 0.837989, Accuracy: 89.79%\n",
      "Validation Batch 16, Loss: 0.846978, Accuracy: 89.84%\n",
      "Validation Batch 17, Loss: 0.891586, Accuracy: 89.61%\n",
      "Validation Batch 18, Loss: 0.835707, Accuracy: 89.67%\n",
      "Validation Batch 19, Loss: 0.889874, Accuracy: 89.47%\n",
      "Validation Batch 20, Loss: 0.884701, Accuracy: 89.30%\n",
      "Validation Batch 21, Loss: 0.875152, Accuracy: 89.21%\n",
      "Validation Batch 22, Loss: 0.865784, Accuracy: 89.06%\n",
      "Validation Batch 23, Loss: 0.882648, Accuracy: 88.99%\n",
      "Validation Batch 24, Loss: 0.871899, Accuracy: 88.93%\n",
      "Validation Batch 25, Loss: 0.829742, Accuracy: 89.00%\n",
      "Validation Batch 26, Loss: 0.854490, Accuracy: 89.00%\n",
      "Validation Batch 27, Loss: 0.811516, Accuracy: 89.14%\n",
      "Validation - Epoch 82, Loss: 0.856088, Accuracy: 89.14%\n",
      "Patience—0\n",
      "Epoch 83\n",
      "Batch 1, Loss: 0.873664, Accuracy: 85.94%\n",
      "Batch 2, Loss: 0.888366, Accuracy: 85.94%\n",
      "Batch 3, Loss: 0.829524, Accuracy: 88.02%\n",
      "Batch 4, Loss: 0.840418, Accuracy: 88.67%\n",
      "Batch 5, Loss: 0.808946, Accuracy: 90.00%\n",
      "Batch 6, Loss: 0.856678, Accuracy: 89.58%\n",
      "Batch 7, Loss: 0.867077, Accuracy: 89.51%\n",
      "Batch 8, Loss: 0.882554, Accuracy: 89.06%\n",
      "Batch 9, Loss: 0.880024, Accuracy: 88.89%\n",
      "Batch 10, Loss: 0.857097, Accuracy: 88.91%\n",
      "Batch 11, Loss: 0.882352, Accuracy: 88.64%\n",
      "Batch 12, Loss: 0.837660, Accuracy: 88.93%\n",
      "Batch 13, Loss: 0.840354, Accuracy: 89.06%\n",
      "Batch 14, Loss: 0.878177, Accuracy: 88.84%\n",
      "Batch 15, Loss: 0.884824, Accuracy: 88.54%\n",
      "Batch 16, Loss: 0.872254, Accuracy: 88.48%\n",
      "Batch 17, Loss: 0.926525, Accuracy: 88.05%\n",
      "Batch 18, Loss: 0.873522, Accuracy: 87.93%\n",
      "Batch 19, Loss: 0.827258, Accuracy: 88.16%\n",
      "Batch 20, Loss: 0.830521, Accuracy: 88.36%\n",
      "Batch 21, Loss: 0.856259, Accuracy: 88.32%\n",
      "Batch 22, Loss: 0.872772, Accuracy: 88.21%\n",
      "Batch 23, Loss: 0.885672, Accuracy: 88.11%\n",
      "Batch 24, Loss: 0.907242, Accuracy: 87.96%\n",
      "Batch 25, Loss: 0.853127, Accuracy: 87.94%\n",
      "Batch 26, Loss: 0.850964, Accuracy: 88.04%\n",
      "Batch 27, Loss: 0.882412, Accuracy: 87.96%\n",
      "Batch 28, Loss: 0.990082, Accuracy: 87.44%\n",
      "Batch 29, Loss: 0.831244, Accuracy: 87.55%\n",
      "Batch 30, Loss: 0.869504, Accuracy: 87.55%\n",
      "Batch 31, Loss: 0.847171, Accuracy: 87.60%\n",
      "Batch 32, Loss: 0.820521, Accuracy: 87.74%\n",
      "Batch 33, Loss: 0.903579, Accuracy: 87.64%\n",
      "Batch 34, Loss: 0.891259, Accuracy: 87.59%\n",
      "Batch 35, Loss: 0.880077, Accuracy: 87.59%\n",
      "Batch 36, Loss: 0.947383, Accuracy: 87.41%\n",
      "Batch 37, Loss: 0.901875, Accuracy: 87.37%\n",
      "Batch 38, Loss: 0.892052, Accuracy: 87.34%\n",
      "Batch 39, Loss: 0.878600, Accuracy: 87.30%\n",
      "Batch 40, Loss: 0.829475, Accuracy: 87.38%\n",
      "Batch 41, Loss: 0.969876, Accuracy: 87.12%\n",
      "Batch 42, Loss: 0.882208, Accuracy: 87.09%\n",
      "Batch 43, Loss: 0.857148, Accuracy: 87.17%\n",
      "Batch 44, Loss: 0.924955, Accuracy: 87.04%\n",
      "Batch 45, Loss: 0.944200, Accuracy: 86.81%\n",
      "Batch 46, Loss: 0.837218, Accuracy: 86.89%\n",
      "Batch 47, Loss: 0.930587, Accuracy: 86.74%\n",
      "Batch 48, Loss: 0.841280, Accuracy: 86.82%\n",
      "Batch 49, Loss: 0.856230, Accuracy: 86.86%\n",
      "Batch 50, Loss: 0.886053, Accuracy: 86.88%\n",
      "Batch 51, Loss: 0.888513, Accuracy: 86.86%\n",
      "Batch 52, Loss: 0.821061, Accuracy: 86.96%\n",
      "Batch 53, Loss: 0.845015, Accuracy: 87.03%\n",
      "Batch 54, Loss: 0.890103, Accuracy: 87.01%\n",
      "Batch 55, Loss: 0.816570, Accuracy: 87.13%\n",
      "Batch 56, Loss: 0.951421, Accuracy: 87.00%\n",
      "Batch 57, Loss: 0.876601, Accuracy: 87.01%\n",
      "Batch 58, Loss: 0.875839, Accuracy: 86.96%\n",
      "Batch 59, Loss: 0.832275, Accuracy: 87.05%\n",
      "Batch 60, Loss: 0.863660, Accuracy: 87.08%\n",
      "Batch 61, Loss: 0.891502, Accuracy: 87.06%\n",
      "Batch 62, Loss: 0.892699, Accuracy: 87.02%\n",
      "Batch 63, Loss: 0.917411, Accuracy: 86.95%\n",
      "Batch 64, Loss: 0.966483, Accuracy: 86.79%\n",
      "Batch 65, Loss: 0.920152, Accuracy: 86.73%\n",
      "Batch 66, Loss: 0.866173, Accuracy: 86.77%\n",
      "Batch 67, Loss: 0.914485, Accuracy: 86.71%\n",
      "Batch 68, Loss: 0.888770, Accuracy: 86.65%\n",
      "Batch 69, Loss: 0.890979, Accuracy: 86.64%\n",
      "Batch 70, Loss: 0.906241, Accuracy: 86.61%\n",
      "Batch 71, Loss: 0.834353, Accuracy: 86.66%\n",
      "Batch 72, Loss: 0.920469, Accuracy: 86.59%\n",
      "Batch 73, Loss: 0.856781, Accuracy: 86.62%\n",
      "Batch 74, Loss: 0.900307, Accuracy: 86.59%\n",
      "Batch 75, Loss: 0.883449, Accuracy: 86.58%\n",
      "Batch 76, Loss: 0.866602, Accuracy: 86.62%\n",
      "Batch 77, Loss: 0.854683, Accuracy: 86.65%\n",
      "Batch 78, Loss: 0.953862, Accuracy: 86.56%\n",
      "Batch 79, Loss: 0.846412, Accuracy: 86.59%\n",
      "Batch 80, Loss: 0.846098, Accuracy: 86.62%\n",
      "Batch 81, Loss: 0.875592, Accuracy: 86.63%\n",
      "Batch 82, Loss: 0.908484, Accuracy: 86.59%\n",
      "Batch 83, Loss: 0.869734, Accuracy: 86.62%\n",
      "Batch 84, Loss: 0.892082, Accuracy: 86.61%\n",
      "Batch 85, Loss: 0.909565, Accuracy: 86.58%\n",
      "Batch 86, Loss: 0.876719, Accuracy: 86.57%\n",
      "Batch 87, Loss: 0.940078, Accuracy: 86.51%\n",
      "Batch 88, Loss: 0.882351, Accuracy: 86.52%\n",
      "Batch 89, Loss: 0.877243, Accuracy: 86.52%\n",
      "Batch 90, Loss: 0.922771, Accuracy: 86.44%\n",
      "Batch 91, Loss: 0.871178, Accuracy: 86.47%\n",
      "Batch 92, Loss: 0.865602, Accuracy: 86.48%\n",
      "Batch 93, Loss: 0.961814, Accuracy: 86.37%\n",
      "Batch 94, Loss: 0.865170, Accuracy: 86.39%\n",
      "Batch 95, Loss: 0.910496, Accuracy: 86.35%\n",
      "Batch 96, Loss: 0.972574, Accuracy: 86.28%\n",
      "Batch 97, Loss: 0.909071, Accuracy: 86.24%\n",
      "Batch 98, Loss: 0.866803, Accuracy: 86.27%\n",
      "Batch 99, Loss: 0.866057, Accuracy: 86.28%\n",
      "Batch 100, Loss: 0.947746, Accuracy: 86.23%\n",
      "Batch 101, Loss: 0.864630, Accuracy: 86.26%\n",
      "Batch 102, Loss: 0.816916, Accuracy: 86.34%\n",
      "Batch 103, Loss: 0.917553, Accuracy: 86.29%\n",
      "Batch 104, Loss: 0.866060, Accuracy: 86.31%\n",
      "Batch 105, Loss: 0.934295, Accuracy: 86.26%\n",
      "Batch 106, Loss: 0.906616, Accuracy: 86.23%\n",
      "Batch 107, Loss: 0.900672, Accuracy: 86.23%\n",
      "Batch 108, Loss: 0.890591, Accuracy: 86.23%\n",
      "Batch 109, Loss: 0.865983, Accuracy: 86.24%\n",
      "Batch 110, Loss: 0.907156, Accuracy: 86.21%\n",
      "Batch 111, Loss: 0.867013, Accuracy: 86.23%\n",
      "Batch 112, Loss: 0.860491, Accuracy: 86.26%\n",
      "Batch 113, Loss: 0.901652, Accuracy: 86.24%\n",
      "Batch 114, Loss: 0.839540, Accuracy: 86.27%\n",
      "Batch 115, Loss: 0.924392, Accuracy: 86.25%\n",
      "Batch 116, Loss: 0.880197, Accuracy: 86.25%\n",
      "Batch 117, Loss: 0.943070, Accuracy: 86.18%\n",
      "Batch 118, Loss: 0.819499, Accuracy: 86.23%\n",
      "Batch 119, Loss: 0.845697, Accuracy: 86.27%\n",
      "Batch 120, Loss: 0.823267, Accuracy: 86.33%\n",
      "Batch 121, Loss: 0.925514, Accuracy: 86.29%\n",
      "Batch 122, Loss: 0.867089, Accuracy: 86.30%\n",
      "Batch 123, Loss: 0.866369, Accuracy: 86.29%\n",
      "Batch 124, Loss: 0.861004, Accuracy: 86.32%\n",
      "Batch 125, Loss: 0.878428, Accuracy: 86.33%\n",
      "Batch 126, Loss: 0.885058, Accuracy: 86.32%\n",
      "Batch 127, Loss: 0.848499, Accuracy: 86.36%\n",
      "Batch 128, Loss: 0.869340, Accuracy: 86.36%\n",
      "Batch 129, Loss: 0.912434, Accuracy: 86.34%\n",
      "Batch 130, Loss: 0.947433, Accuracy: 86.29%\n",
      "Batch 131, Loss: 0.818695, Accuracy: 86.33%\n",
      "Batch 132, Loss: 0.847052, Accuracy: 86.35%\n",
      "Batch 133, Loss: 0.823413, Accuracy: 86.40%\n",
      "Batch 134, Loss: 0.820280, Accuracy: 86.45%\n",
      "Batch 135, Loss: 0.772325, Accuracy: 86.53%\n",
      "Batch 136, Loss: 0.863475, Accuracy: 86.55%\n",
      "Batch 137, Loss: 0.890799, Accuracy: 86.53%\n",
      "Batch 138, Loss: 0.866723, Accuracy: 86.54%\n",
      "Batch 139, Loss: 0.895903, Accuracy: 86.52%\n",
      "Batch 140, Loss: 0.942908, Accuracy: 86.47%\n",
      "Batch 141, Loss: 0.874775, Accuracy: 86.47%\n",
      "Batch 142, Loss: 0.815122, Accuracy: 86.51%\n",
      "Batch 143, Loss: 0.827294, Accuracy: 86.55%\n",
      "Batch 144, Loss: 0.867903, Accuracy: 86.56%\n",
      "Batch 145, Loss: 0.917495, Accuracy: 86.53%\n",
      "Batch 146, Loss: 0.899214, Accuracy: 86.52%\n",
      "Batch 147, Loss: 0.919257, Accuracy: 86.48%\n",
      "Batch 148, Loss: 0.856395, Accuracy: 86.50%\n",
      "Batch 149, Loss: 0.845458, Accuracy: 86.51%\n",
      "Batch 150, Loss: 0.823457, Accuracy: 86.56%\n",
      "Batch 151, Loss: 0.845025, Accuracy: 86.59%\n",
      "Batch 152, Loss: 0.876073, Accuracy: 86.59%\n",
      "Batch 153, Loss: 0.892257, Accuracy: 86.58%\n",
      "Batch 154, Loss: 0.816213, Accuracy: 86.62%\n",
      "Batch 155, Loss: 0.897499, Accuracy: 86.61%\n",
      "Batch 156, Loss: 0.896166, Accuracy: 86.60%\n",
      "Batch 157, Loss: 0.927855, Accuracy: 86.56%\n",
      "Batch 158, Loss: 0.914903, Accuracy: 86.53%\n",
      "Batch 159, Loss: 0.806600, Accuracy: 86.58%\n",
      "Batch 160, Loss: 0.890285, Accuracy: 86.57%\n",
      "Batch 161, Loss: 0.879742, Accuracy: 86.58%\n",
      "Batch 162, Loss: 0.857326, Accuracy: 86.59%\n",
      "Batch 163, Loss: 0.879799, Accuracy: 86.59%\n",
      "Batch 164, Loss: 0.887297, Accuracy: 86.59%\n",
      "Batch 165, Loss: 0.931967, Accuracy: 86.55%\n",
      "Batch 166, Loss: 0.860727, Accuracy: 86.57%\n",
      "Batch 167, Loss: 0.882188, Accuracy: 86.56%\n",
      "Batch 168, Loss: 0.888561, Accuracy: 86.56%\n",
      "Batch 169, Loss: 0.842968, Accuracy: 86.59%\n",
      "Batch 170, Loss: 0.844996, Accuracy: 86.61%\n",
      "Batch 171, Loss: 0.852588, Accuracy: 86.63%\n",
      "Batch 172, Loss: 0.847089, Accuracy: 86.65%\n",
      "Batch 173, Loss: 0.886224, Accuracy: 86.63%\n",
      "Batch 174, Loss: 0.877719, Accuracy: 86.64%\n",
      "Batch 175, Loss: 0.833770, Accuracy: 86.67%\n",
      "Batch 176, Loss: 0.951453, Accuracy: 86.62%\n",
      "Batch 177, Loss: 0.906483, Accuracy: 86.59%\n",
      "Batch 178, Loss: 0.877833, Accuracy: 86.59%\n",
      "Batch 179, Loss: 0.829691, Accuracy: 86.61%\n",
      "Batch 180, Loss: 0.845955, Accuracy: 86.63%\n",
      "Batch 181, Loss: 0.858494, Accuracy: 86.65%\n",
      "Batch 182, Loss: 0.863643, Accuracy: 86.66%\n",
      "Batch 183, Loss: 0.899250, Accuracy: 86.65%\n",
      "Batch 184, Loss: 0.894322, Accuracy: 86.63%\n",
      "Batch 185, Loss: 0.867579, Accuracy: 86.66%\n",
      "Batch 186, Loss: 0.835195, Accuracy: 86.69%\n",
      "Batch 187, Loss: 0.887516, Accuracy: 86.67%\n",
      "Batch 188, Loss: 0.876110, Accuracy: 86.68%\n",
      "Batch 189, Loss: 0.899879, Accuracy: 86.67%\n",
      "Batch 190, Loss: 0.857360, Accuracy: 86.68%\n",
      "Batch 191, Loss: 0.861886, Accuracy: 86.69%\n",
      "Batch 192, Loss: 0.895775, Accuracy: 86.69%\n",
      "Batch 193, Loss: 0.908723, Accuracy: 86.67%\n",
      "Batch 194, Loss: 0.907450, Accuracy: 86.66%\n",
      "Batch 195, Loss: 0.854679, Accuracy: 86.67%\n",
      "Batch 196, Loss: 0.971644, Accuracy: 86.61%\n",
      "Batch 197, Loss: 0.812274, Accuracy: 86.64%\n",
      "Batch 198, Loss: 0.855616, Accuracy: 86.66%\n",
      "Batch 199, Loss: 0.887618, Accuracy: 86.65%\n",
      "Batch 200, Loss: 0.884088, Accuracy: 86.65%\n",
      "Batch 201, Loss: 0.846745, Accuracy: 86.67%\n",
      "Batch 202, Loss: 0.878491, Accuracy: 86.66%\n",
      "Batch 203, Loss: 0.861911, Accuracy: 86.68%\n",
      "Batch 204, Loss: 0.797971, Accuracy: 86.72%\n",
      "Batch 205, Loss: 0.906466, Accuracy: 86.71%\n",
      "Batch 206, Loss: 0.856798, Accuracy: 86.71%\n",
      "Batch 207, Loss: 0.932882, Accuracy: 86.68%\n",
      "Batch 208, Loss: 0.971902, Accuracy: 86.64%\n",
      "Batch 209, Loss: 0.839693, Accuracy: 86.66%\n",
      "Batch 210, Loss: 0.895059, Accuracy: 86.65%\n",
      "Batch 211, Loss: 0.903943, Accuracy: 86.64%\n",
      "Batch 212, Loss: 0.862774, Accuracy: 86.65%\n",
      "Batch 213, Loss: 0.878546, Accuracy: 86.66%\n",
      "Training - Epoch 83, Loss: 0.878370, Accuracy: 86.66%\n",
      "Validation Batch 1, Loss: 0.847082, Accuracy: 90.62%\n",
      "Validation Batch 2, Loss: 0.819297, Accuracy: 92.19%\n",
      "Validation Batch 3, Loss: 0.905930, Accuracy: 89.58%\n",
      "Validation Batch 4, Loss: 0.856827, Accuracy: 89.45%\n",
      "Validation Batch 5, Loss: 0.856117, Accuracy: 89.38%\n",
      "Validation Batch 6, Loss: 0.820701, Accuracy: 90.10%\n",
      "Validation Batch 7, Loss: 0.832084, Accuracy: 90.18%\n",
      "Validation Batch 8, Loss: 0.919329, Accuracy: 89.26%\n",
      "Validation Batch 9, Loss: 0.909368, Accuracy: 88.54%\n",
      "Validation Batch 10, Loss: 0.860908, Accuracy: 88.44%\n",
      "Validation Batch 11, Loss: 0.857392, Accuracy: 88.49%\n",
      "Validation Batch 12, Loss: 0.849478, Accuracy: 88.80%\n",
      "Validation Batch 13, Loss: 0.870173, Accuracy: 88.70%\n",
      "Validation Batch 14, Loss: 0.886413, Accuracy: 88.39%\n",
      "Validation Batch 15, Loss: 0.849090, Accuracy: 88.44%\n",
      "Validation Batch 16, Loss: 0.862627, Accuracy: 88.57%\n",
      "Validation Batch 17, Loss: 0.906201, Accuracy: 88.33%\n",
      "Validation Batch 18, Loss: 0.853838, Accuracy: 88.37%\n",
      "Validation Batch 19, Loss: 0.909043, Accuracy: 88.16%\n",
      "Validation Batch 20, Loss: 0.920337, Accuracy: 87.73%\n",
      "Validation Batch 21, Loss: 0.890799, Accuracy: 87.65%\n",
      "Validation Batch 22, Loss: 0.874262, Accuracy: 87.64%\n",
      "Validation Batch 23, Loss: 0.904646, Accuracy: 87.57%\n",
      "Validation Batch 24, Loss: 0.891667, Accuracy: 87.50%\n",
      "Validation Batch 25, Loss: 0.841624, Accuracy: 87.62%\n",
      "Validation Batch 26, Loss: 0.875322, Accuracy: 87.62%\n",
      "Validation Batch 27, Loss: 0.825440, Accuracy: 87.79%\n",
      "Validation - Epoch 83, Loss: 0.870222, Accuracy: 87.79%\n",
      "Patience—1\n",
      "Epoch 84\n",
      "Batch 1, Loss: 0.904894, Accuracy: 85.94%\n",
      "Batch 2, Loss: 0.894902, Accuracy: 85.16%\n",
      "Batch 3, Loss: 0.941111, Accuracy: 83.33%\n",
      "Batch 4, Loss: 0.898014, Accuracy: 83.20%\n",
      "Batch 5, Loss: 0.845781, Accuracy: 84.38%\n",
      "Batch 6, Loss: 0.859993, Accuracy: 84.90%\n",
      "Batch 7, Loss: 0.912645, Accuracy: 84.60%\n",
      "Batch 8, Loss: 0.843809, Accuracy: 85.35%\n",
      "Batch 9, Loss: 0.799428, Accuracy: 86.46%\n",
      "Batch 10, Loss: 0.864011, Accuracy: 86.72%\n",
      "Batch 11, Loss: 0.907399, Accuracy: 86.36%\n",
      "Batch 12, Loss: 0.871152, Accuracy: 86.33%\n",
      "Batch 13, Loss: 0.876322, Accuracy: 86.30%\n",
      "Batch 14, Loss: 0.895460, Accuracy: 86.27%\n",
      "Batch 15, Loss: 0.869055, Accuracy: 86.25%\n",
      "Batch 16, Loss: 0.930543, Accuracy: 85.94%\n",
      "Batch 17, Loss: 0.902506, Accuracy: 85.75%\n",
      "Batch 18, Loss: 0.932141, Accuracy: 85.59%\n",
      "Batch 19, Loss: 0.791416, Accuracy: 86.10%\n",
      "Batch 20, Loss: 0.831955, Accuracy: 86.41%\n",
      "Batch 21, Loss: 0.872468, Accuracy: 86.53%\n",
      "Batch 22, Loss: 0.939501, Accuracy: 86.22%\n",
      "Batch 23, Loss: 0.960496, Accuracy: 85.80%\n",
      "Batch 24, Loss: 0.918281, Accuracy: 85.61%\n",
      "Batch 25, Loss: 0.863139, Accuracy: 85.75%\n",
      "Batch 26, Loss: 0.872397, Accuracy: 85.82%\n",
      "Batch 27, Loss: 0.875830, Accuracy: 85.82%\n",
      "Batch 28, Loss: 0.869475, Accuracy: 85.83%\n",
      "Batch 29, Loss: 0.923270, Accuracy: 85.72%\n",
      "Batch 30, Loss: 0.901046, Accuracy: 85.68%\n",
      "Batch 31, Loss: 0.865661, Accuracy: 85.74%\n",
      "Batch 32, Loss: 0.885990, Accuracy: 85.74%\n",
      "Batch 33, Loss: 0.857477, Accuracy: 85.80%\n",
      "Batch 34, Loss: 0.910446, Accuracy: 85.71%\n",
      "Batch 35, Loss: 0.862327, Accuracy: 85.80%\n",
      "Batch 36, Loss: 0.966705, Accuracy: 85.55%\n",
      "Batch 37, Loss: 0.914543, Accuracy: 85.52%\n",
      "Batch 38, Loss: 0.865999, Accuracy: 85.57%\n",
      "Batch 39, Loss: 0.916715, Accuracy: 85.50%\n",
      "Batch 40, Loss: 0.865267, Accuracy: 85.55%\n",
      "Batch 41, Loss: 0.859665, Accuracy: 85.63%\n",
      "Batch 42, Loss: 0.843510, Accuracy: 85.75%\n",
      "Batch 43, Loss: 0.884715, Accuracy: 85.79%\n",
      "Batch 44, Loss: 0.832117, Accuracy: 85.94%\n",
      "Batch 45, Loss: 0.833301, Accuracy: 86.04%\n",
      "Batch 46, Loss: 0.906177, Accuracy: 86.01%\n",
      "Batch 47, Loss: 0.885738, Accuracy: 86.00%\n",
      "Batch 48, Loss: 0.879857, Accuracy: 85.97%\n",
      "Batch 49, Loss: 0.925545, Accuracy: 85.87%\n",
      "Batch 50, Loss: 0.856704, Accuracy: 85.88%\n",
      "Batch 51, Loss: 0.960687, Accuracy: 85.72%\n",
      "Batch 52, Loss: 0.863576, Accuracy: 85.76%\n",
      "Batch 53, Loss: 0.951308, Accuracy: 85.67%\n",
      "Batch 54, Loss: 0.873416, Accuracy: 85.68%\n",
      "Batch 55, Loss: 0.861357, Accuracy: 85.74%\n",
      "Batch 56, Loss: 0.878458, Accuracy: 85.77%\n",
      "Batch 57, Loss: 0.871218, Accuracy: 85.77%\n",
      "Batch 58, Loss: 0.911879, Accuracy: 85.67%\n",
      "Batch 59, Loss: 0.836402, Accuracy: 85.75%\n",
      "Batch 60, Loss: 0.854715, Accuracy: 85.83%\n",
      "Batch 61, Loss: 0.916877, Accuracy: 85.78%\n",
      "Batch 62, Loss: 0.899695, Accuracy: 85.76%\n",
      "Batch 63, Loss: 0.846760, Accuracy: 85.84%\n",
      "Batch 64, Loss: 0.872776, Accuracy: 85.86%\n",
      "Batch 65, Loss: 0.919531, Accuracy: 85.79%\n",
      "Batch 66, Loss: 0.854043, Accuracy: 85.84%\n",
      "Batch 67, Loss: 0.921876, Accuracy: 85.77%\n",
      "Batch 68, Loss: 0.840382, Accuracy: 85.85%\n",
      "Batch 69, Loss: 0.861977, Accuracy: 85.89%\n",
      "Batch 70, Loss: 0.854079, Accuracy: 85.92%\n",
      "Batch 71, Loss: 0.825848, Accuracy: 86.00%\n",
      "Batch 72, Loss: 0.838932, Accuracy: 86.09%\n",
      "Batch 73, Loss: 0.874581, Accuracy: 86.09%\n",
      "Batch 74, Loss: 0.850387, Accuracy: 86.11%\n",
      "Batch 75, Loss: 0.834271, Accuracy: 86.19%\n",
      "Batch 76, Loss: 0.833027, Accuracy: 86.27%\n",
      "Batch 77, Loss: 0.851670, Accuracy: 86.30%\n",
      "Batch 78, Loss: 0.924670, Accuracy: 86.22%\n",
      "Batch 79, Loss: 0.939270, Accuracy: 86.16%\n",
      "Batch 80, Loss: 0.895810, Accuracy: 86.11%\n",
      "Batch 81, Loss: 0.877428, Accuracy: 86.09%\n",
      "Batch 82, Loss: 0.942521, Accuracy: 85.99%\n",
      "Batch 83, Loss: 0.833329, Accuracy: 86.05%\n",
      "Batch 84, Loss: 0.874187, Accuracy: 86.05%\n",
      "Batch 85, Loss: 0.879513, Accuracy: 86.05%\n",
      "Batch 86, Loss: 0.828443, Accuracy: 86.10%\n",
      "Batch 87, Loss: 0.863021, Accuracy: 86.14%\n",
      "Batch 88, Loss: 0.839463, Accuracy: 86.19%\n",
      "Batch 89, Loss: 0.846959, Accuracy: 86.24%\n",
      "Batch 90, Loss: 0.828004, Accuracy: 86.30%\n",
      "Batch 91, Loss: 0.935094, Accuracy: 86.25%\n",
      "Batch 92, Loss: 0.866467, Accuracy: 86.26%\n",
      "Batch 93, Loss: 0.857512, Accuracy: 86.29%\n",
      "Batch 94, Loss: 0.847220, Accuracy: 86.34%\n",
      "Batch 95, Loss: 0.876419, Accuracy: 86.35%\n",
      "Batch 96, Loss: 0.831973, Accuracy: 86.39%\n",
      "Batch 97, Loss: 0.874855, Accuracy: 86.40%\n",
      "Batch 98, Loss: 0.858196, Accuracy: 86.43%\n",
      "Batch 99, Loss: 0.842957, Accuracy: 86.47%\n",
      "Batch 100, Loss: 0.861242, Accuracy: 86.48%\n",
      "Batch 101, Loss: 0.965533, Accuracy: 86.39%\n",
      "Batch 102, Loss: 0.847014, Accuracy: 86.41%\n",
      "Batch 103, Loss: 0.862448, Accuracy: 86.41%\n",
      "Batch 104, Loss: 0.911652, Accuracy: 86.37%\n",
      "Batch 105, Loss: 0.856277, Accuracy: 86.40%\n",
      "Batch 106, Loss: 0.880681, Accuracy: 86.39%\n",
      "Batch 107, Loss: 0.892765, Accuracy: 86.38%\n",
      "Batch 108, Loss: 0.858044, Accuracy: 86.40%\n",
      "Batch 109, Loss: 0.906730, Accuracy: 86.37%\n",
      "Batch 110, Loss: 0.869139, Accuracy: 86.38%\n",
      "Batch 111, Loss: 0.887236, Accuracy: 86.39%\n",
      "Batch 112, Loss: 0.891904, Accuracy: 86.37%\n",
      "Batch 113, Loss: 0.887815, Accuracy: 86.37%\n",
      "Batch 114, Loss: 0.895087, Accuracy: 86.36%\n",
      "Batch 115, Loss: 0.853323, Accuracy: 86.40%\n",
      "Batch 116, Loss: 0.827879, Accuracy: 86.45%\n",
      "Batch 117, Loss: 0.920088, Accuracy: 86.40%\n",
      "Batch 118, Loss: 0.809932, Accuracy: 86.47%\n",
      "Batch 119, Loss: 0.898728, Accuracy: 86.45%\n",
      "Batch 120, Loss: 0.882464, Accuracy: 86.46%\n",
      "Batch 121, Loss: 0.881411, Accuracy: 86.44%\n",
      "Batch 122, Loss: 0.811085, Accuracy: 86.49%\n",
      "Batch 123, Loss: 0.820363, Accuracy: 86.55%\n",
      "Batch 124, Loss: 0.852077, Accuracy: 86.57%\n",
      "Batch 125, Loss: 0.823592, Accuracy: 86.61%\n",
      "Batch 126, Loss: 0.929663, Accuracy: 86.57%\n",
      "Batch 127, Loss: 0.867665, Accuracy: 86.58%\n",
      "Batch 128, Loss: 0.824003, Accuracy: 86.61%\n",
      "Batch 129, Loss: 0.904760, Accuracy: 86.59%\n",
      "Batch 130, Loss: 0.811402, Accuracy: 86.66%\n",
      "Batch 131, Loss: 0.877393, Accuracy: 86.67%\n",
      "Batch 132, Loss: 0.872352, Accuracy: 86.67%\n",
      "Batch 133, Loss: 0.925762, Accuracy: 86.64%\n",
      "Batch 134, Loss: 0.909520, Accuracy: 86.61%\n",
      "Batch 135, Loss: 0.976865, Accuracy: 86.52%\n",
      "Batch 136, Loss: 0.948960, Accuracy: 86.45%\n",
      "Batch 137, Loss: 0.875926, Accuracy: 86.46%\n",
      "Batch 138, Loss: 0.821227, Accuracy: 86.50%\n",
      "Batch 139, Loss: 0.859217, Accuracy: 86.52%\n",
      "Batch 140, Loss: 0.865798, Accuracy: 86.54%\n",
      "Batch 141, Loss: 0.935527, Accuracy: 86.49%\n",
      "Batch 142, Loss: 0.888083, Accuracy: 86.48%\n",
      "Batch 143, Loss: 0.861724, Accuracy: 86.49%\n",
      "Batch 144, Loss: 0.880589, Accuracy: 86.49%\n",
      "Batch 145, Loss: 0.890060, Accuracy: 86.50%\n",
      "Batch 146, Loss: 0.881134, Accuracy: 86.49%\n",
      "Batch 147, Loss: 0.875728, Accuracy: 86.49%\n",
      "Batch 148, Loss: 0.901901, Accuracy: 86.48%\n",
      "Batch 149, Loss: 0.850091, Accuracy: 86.50%\n",
      "Batch 150, Loss: 0.861019, Accuracy: 86.51%\n",
      "Batch 151, Loss: 0.898393, Accuracy: 86.50%\n",
      "Batch 152, Loss: 0.893516, Accuracy: 86.48%\n",
      "Batch 153, Loss: 0.920030, Accuracy: 86.46%\n",
      "Batch 154, Loss: 0.849396, Accuracy: 86.48%\n",
      "Batch 155, Loss: 0.883249, Accuracy: 86.47%\n",
      "Batch 156, Loss: 0.796722, Accuracy: 86.53%\n",
      "Batch 157, Loss: 0.828992, Accuracy: 86.55%\n",
      "Batch 158, Loss: 0.906961, Accuracy: 86.53%\n",
      "Batch 159, Loss: 0.906293, Accuracy: 86.52%\n",
      "Batch 160, Loss: 0.867486, Accuracy: 86.53%\n",
      "Batch 161, Loss: 0.904199, Accuracy: 86.51%\n",
      "Batch 162, Loss: 0.954275, Accuracy: 86.47%\n",
      "Batch 163, Loss: 0.892540, Accuracy: 86.46%\n",
      "Batch 164, Loss: 0.933495, Accuracy: 86.43%\n",
      "Batch 165, Loss: 0.867451, Accuracy: 86.44%\n",
      "Batch 166, Loss: 0.775660, Accuracy: 86.50%\n",
      "Batch 167, Loss: 0.885922, Accuracy: 86.50%\n",
      "Batch 168, Loss: 0.945580, Accuracy: 86.45%\n",
      "Batch 169, Loss: 0.888125, Accuracy: 86.45%\n",
      "Batch 170, Loss: 0.817254, Accuracy: 86.48%\n",
      "Batch 171, Loss: 0.839363, Accuracy: 86.50%\n",
      "Batch 172, Loss: 0.837761, Accuracy: 86.52%\n",
      "Batch 173, Loss: 0.839823, Accuracy: 86.54%\n",
      "Batch 174, Loss: 0.852459, Accuracy: 86.56%\n",
      "Batch 175, Loss: 0.920917, Accuracy: 86.53%\n",
      "Batch 176, Loss: 0.819677, Accuracy: 86.57%\n",
      "Batch 177, Loss: 0.824127, Accuracy: 86.60%\n",
      "Batch 178, Loss: 0.909644, Accuracy: 86.59%\n",
      "Batch 179, Loss: 0.823967, Accuracy: 86.62%\n",
      "Batch 180, Loss: 0.822435, Accuracy: 86.66%\n",
      "Batch 181, Loss: 0.959768, Accuracy: 86.61%\n",
      "Batch 182, Loss: 0.855665, Accuracy: 86.63%\n",
      "Batch 183, Loss: 0.869604, Accuracy: 86.65%\n",
      "Batch 184, Loss: 0.885507, Accuracy: 86.64%\n",
      "Batch 185, Loss: 0.873400, Accuracy: 86.63%\n",
      "Batch 186, Loss: 0.837205, Accuracy: 86.65%\n",
      "Batch 187, Loss: 0.850474, Accuracy: 86.67%\n",
      "Batch 188, Loss: 0.817108, Accuracy: 86.71%\n",
      "Batch 189, Loss: 0.866525, Accuracy: 86.71%\n",
      "Batch 190, Loss: 0.975026, Accuracy: 86.66%\n",
      "Batch 191, Loss: 0.961096, Accuracy: 86.62%\n",
      "Batch 192, Loss: 0.874734, Accuracy: 86.61%\n",
      "Batch 193, Loss: 0.857946, Accuracy: 86.62%\n",
      "Batch 194, Loss: 0.928541, Accuracy: 86.59%\n",
      "Batch 195, Loss: 0.835506, Accuracy: 86.61%\n",
      "Batch 196, Loss: 0.850772, Accuracy: 86.62%\n",
      "Batch 197, Loss: 0.826975, Accuracy: 86.65%\n",
      "Batch 198, Loss: 0.909474, Accuracy: 86.64%\n",
      "Batch 199, Loss: 0.841975, Accuracy: 86.66%\n",
      "Batch 200, Loss: 0.919481, Accuracy: 86.64%\n",
      "Batch 201, Loss: 0.882802, Accuracy: 86.64%\n",
      "Batch 202, Loss: 0.887307, Accuracy: 86.64%\n",
      "Batch 203, Loss: 0.793558, Accuracy: 86.68%\n",
      "Batch 204, Loss: 0.804412, Accuracy: 86.72%\n",
      "Batch 205, Loss: 0.826997, Accuracy: 86.75%\n",
      "Batch 206, Loss: 0.862263, Accuracy: 86.75%\n",
      "Batch 207, Loss: 0.911594, Accuracy: 86.73%\n",
      "Batch 208, Loss: 0.924033, Accuracy: 86.70%\n",
      "Batch 209, Loss: 0.803565, Accuracy: 86.74%\n",
      "Batch 210, Loss: 0.870590, Accuracy: 86.75%\n",
      "Batch 211, Loss: 0.814932, Accuracy: 86.77%\n",
      "Batch 212, Loss: 0.918135, Accuracy: 86.76%\n",
      "Batch 213, Loss: 0.859981, Accuracy: 86.77%\n",
      "Training - Epoch 84, Loss: 0.875697, Accuracy: 86.77%\n",
      "Validation Batch 1, Loss: 0.835067, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.814973, Accuracy: 92.97%\n",
      "Validation Batch 3, Loss: 0.891573, Accuracy: 89.58%\n",
      "Validation Batch 4, Loss: 0.845976, Accuracy: 89.84%\n",
      "Validation Batch 5, Loss: 0.841265, Accuracy: 90.00%\n",
      "Validation Batch 6, Loss: 0.810829, Accuracy: 90.62%\n",
      "Validation Batch 7, Loss: 0.817630, Accuracy: 91.07%\n",
      "Validation Batch 8, Loss: 0.909478, Accuracy: 90.04%\n",
      "Validation Batch 9, Loss: 0.901304, Accuracy: 89.41%\n",
      "Validation Batch 10, Loss: 0.844909, Accuracy: 89.53%\n",
      "Validation Batch 11, Loss: 0.845811, Accuracy: 89.63%\n",
      "Validation Batch 12, Loss: 0.839082, Accuracy: 89.84%\n",
      "Validation Batch 13, Loss: 0.858388, Accuracy: 89.66%\n",
      "Validation Batch 14, Loss: 0.873889, Accuracy: 89.40%\n",
      "Validation Batch 15, Loss: 0.831274, Accuracy: 89.58%\n",
      "Validation Batch 16, Loss: 0.842082, Accuracy: 89.75%\n",
      "Validation Batch 17, Loss: 0.894274, Accuracy: 89.52%\n",
      "Validation Batch 18, Loss: 0.838406, Accuracy: 89.50%\n",
      "Validation Batch 19, Loss: 0.896416, Accuracy: 89.23%\n",
      "Validation Batch 20, Loss: 0.887537, Accuracy: 89.06%\n",
      "Validation Batch 21, Loss: 0.879783, Accuracy: 88.91%\n",
      "Validation Batch 22, Loss: 0.859421, Accuracy: 88.85%\n",
      "Validation Batch 23, Loss: 0.885258, Accuracy: 88.79%\n",
      "Validation Batch 24, Loss: 0.874039, Accuracy: 88.67%\n",
      "Validation Batch 25, Loss: 0.830950, Accuracy: 88.75%\n",
      "Validation Batch 26, Loss: 0.863270, Accuracy: 88.70%\n",
      "Validation Batch 27, Loss: 0.811743, Accuracy: 88.84%\n",
      "Validation - Epoch 84, Loss: 0.856468, Accuracy: 88.84%\n",
      "Patience—2\n",
      "Epoch 85\n",
      "Batch 1, Loss: 0.844160, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.928594, Accuracy: 86.72%\n",
      "Batch 3, Loss: 0.842559, Accuracy: 87.50%\n",
      "Batch 4, Loss: 0.916450, Accuracy: 86.33%\n",
      "Batch 5, Loss: 0.890602, Accuracy: 85.94%\n",
      "Batch 6, Loss: 0.925330, Accuracy: 85.42%\n",
      "Batch 7, Loss: 0.884588, Accuracy: 85.71%\n",
      "Batch 8, Loss: 0.876418, Accuracy: 85.94%\n",
      "Batch 9, Loss: 0.863629, Accuracy: 86.28%\n",
      "Batch 10, Loss: 0.885826, Accuracy: 86.25%\n",
      "Batch 11, Loss: 0.837693, Accuracy: 86.65%\n",
      "Batch 12, Loss: 0.893785, Accuracy: 86.33%\n",
      "Batch 13, Loss: 0.922657, Accuracy: 85.94%\n",
      "Batch 14, Loss: 0.887241, Accuracy: 85.71%\n",
      "Batch 15, Loss: 0.890643, Accuracy: 85.62%\n",
      "Batch 16, Loss: 0.828785, Accuracy: 85.94%\n",
      "Batch 17, Loss: 0.932812, Accuracy: 85.66%\n",
      "Batch 18, Loss: 0.880601, Accuracy: 85.68%\n",
      "Batch 19, Loss: 0.880481, Accuracy: 85.69%\n",
      "Batch 20, Loss: 0.945868, Accuracy: 85.39%\n",
      "Batch 21, Loss: 0.925449, Accuracy: 85.34%\n",
      "Batch 22, Loss: 0.892270, Accuracy: 85.30%\n",
      "Batch 23, Loss: 0.887096, Accuracy: 85.26%\n",
      "Batch 24, Loss: 0.856496, Accuracy: 85.42%\n",
      "Batch 25, Loss: 0.845849, Accuracy: 85.56%\n",
      "Batch 26, Loss: 0.849089, Accuracy: 85.76%\n",
      "Batch 27, Loss: 0.761672, Accuracy: 86.23%\n",
      "Batch 28, Loss: 0.822893, Accuracy: 86.44%\n",
      "Batch 29, Loss: 0.867621, Accuracy: 86.48%\n",
      "Batch 30, Loss: 0.920330, Accuracy: 86.30%\n",
      "Batch 31, Loss: 0.833777, Accuracy: 86.44%\n",
      "Batch 32, Loss: 0.918156, Accuracy: 86.28%\n",
      "Batch 33, Loss: 0.876501, Accuracy: 86.27%\n",
      "Batch 34, Loss: 0.862421, Accuracy: 86.35%\n",
      "Batch 35, Loss: 0.877953, Accuracy: 86.38%\n",
      "Batch 36, Loss: 0.852659, Accuracy: 86.46%\n",
      "Batch 37, Loss: 0.855063, Accuracy: 86.57%\n",
      "Batch 38, Loss: 0.854289, Accuracy: 86.64%\n",
      "Batch 39, Loss: 0.870321, Accuracy: 86.66%\n",
      "Batch 40, Loss: 0.938781, Accuracy: 86.48%\n",
      "Batch 41, Loss: 0.818784, Accuracy: 86.62%\n",
      "Batch 42, Loss: 0.824501, Accuracy: 86.79%\n",
      "Batch 43, Loss: 0.914775, Accuracy: 86.74%\n",
      "Batch 44, Loss: 0.842571, Accuracy: 86.83%\n",
      "Batch 45, Loss: 0.859249, Accuracy: 86.84%\n",
      "Batch 46, Loss: 0.925929, Accuracy: 86.72%\n",
      "Batch 47, Loss: 0.874258, Accuracy: 86.74%\n",
      "Batch 48, Loss: 0.859640, Accuracy: 86.78%\n",
      "Batch 49, Loss: 0.858569, Accuracy: 86.83%\n",
      "Batch 50, Loss: 0.904090, Accuracy: 86.78%\n",
      "Batch 51, Loss: 0.871777, Accuracy: 86.80%\n",
      "Batch 52, Loss: 0.919550, Accuracy: 86.69%\n",
      "Batch 53, Loss: 0.848817, Accuracy: 86.79%\n",
      "Batch 54, Loss: 0.921334, Accuracy: 86.75%\n",
      "Batch 55, Loss: 0.937131, Accuracy: 86.62%\n",
      "Batch 56, Loss: 0.815965, Accuracy: 86.72%\n",
      "Batch 57, Loss: 0.820934, Accuracy: 86.84%\n",
      "Batch 58, Loss: 0.871903, Accuracy: 86.85%\n",
      "Batch 59, Loss: 0.873446, Accuracy: 86.84%\n",
      "Batch 60, Loss: 0.888157, Accuracy: 86.82%\n",
      "Batch 61, Loss: 0.857744, Accuracy: 86.83%\n",
      "Batch 62, Loss: 0.890723, Accuracy: 86.82%\n",
      "Batch 63, Loss: 0.823094, Accuracy: 86.93%\n",
      "Batch 64, Loss: 0.870862, Accuracy: 86.94%\n",
      "Batch 65, Loss: 0.875153, Accuracy: 86.95%\n",
      "Batch 66, Loss: 0.921147, Accuracy: 86.86%\n",
      "Batch 67, Loss: 0.896328, Accuracy: 86.85%\n",
      "Batch 68, Loss: 0.879244, Accuracy: 86.83%\n",
      "Batch 69, Loss: 0.870934, Accuracy: 86.84%\n",
      "Batch 70, Loss: 0.885316, Accuracy: 86.83%\n",
      "Batch 71, Loss: 0.886546, Accuracy: 86.84%\n",
      "Batch 72, Loss: 0.800733, Accuracy: 86.96%\n",
      "Batch 73, Loss: 0.868160, Accuracy: 86.96%\n",
      "Batch 74, Loss: 0.873857, Accuracy: 86.97%\n",
      "Batch 75, Loss: 0.874741, Accuracy: 86.98%\n",
      "Batch 76, Loss: 0.870709, Accuracy: 86.99%\n",
      "Batch 77, Loss: 0.901604, Accuracy: 86.97%\n",
      "Batch 78, Loss: 0.908743, Accuracy: 86.92%\n",
      "Batch 79, Loss: 0.910369, Accuracy: 86.87%\n",
      "Batch 80, Loss: 0.889425, Accuracy: 86.86%\n",
      "Batch 81, Loss: 0.868717, Accuracy: 86.84%\n",
      "Batch 82, Loss: 0.863929, Accuracy: 86.85%\n",
      "Batch 83, Loss: 0.863685, Accuracy: 86.86%\n",
      "Batch 84, Loss: 0.888898, Accuracy: 86.85%\n",
      "Batch 85, Loss: 0.877339, Accuracy: 86.86%\n",
      "Batch 86, Loss: 0.834670, Accuracy: 86.90%\n",
      "Batch 87, Loss: 0.916582, Accuracy: 86.89%\n",
      "Batch 88, Loss: 0.893527, Accuracy: 86.86%\n",
      "Batch 89, Loss: 0.887502, Accuracy: 86.85%\n",
      "Batch 90, Loss: 0.903502, Accuracy: 86.82%\n",
      "Batch 91, Loss: 0.896920, Accuracy: 86.80%\n",
      "Batch 92, Loss: 0.838302, Accuracy: 86.84%\n",
      "Batch 93, Loss: 0.942276, Accuracy: 86.78%\n",
      "Batch 94, Loss: 0.854850, Accuracy: 86.80%\n",
      "Batch 95, Loss: 0.913591, Accuracy: 86.74%\n",
      "Batch 96, Loss: 0.879049, Accuracy: 86.75%\n",
      "Batch 97, Loss: 0.936493, Accuracy: 86.68%\n",
      "Batch 98, Loss: 0.858415, Accuracy: 86.72%\n",
      "Batch 99, Loss: 0.840137, Accuracy: 86.76%\n",
      "Batch 100, Loss: 0.893535, Accuracy: 86.72%\n",
      "Batch 101, Loss: 0.884090, Accuracy: 86.70%\n",
      "Batch 102, Loss: 0.890554, Accuracy: 86.69%\n",
      "Batch 103, Loss: 0.843017, Accuracy: 86.73%\n",
      "Batch 104, Loss: 0.931757, Accuracy: 86.67%\n",
      "Batch 105, Loss: 0.842373, Accuracy: 86.71%\n",
      "Batch 106, Loss: 0.842936, Accuracy: 86.75%\n",
      "Batch 107, Loss: 0.921390, Accuracy: 86.68%\n",
      "Batch 108, Loss: 0.870854, Accuracy: 86.68%\n",
      "Batch 109, Loss: 0.886120, Accuracy: 86.65%\n",
      "Batch 110, Loss: 0.912755, Accuracy: 86.65%\n",
      "Batch 111, Loss: 0.844543, Accuracy: 86.68%\n",
      "Batch 112, Loss: 0.860773, Accuracy: 86.70%\n",
      "Batch 113, Loss: 0.927077, Accuracy: 86.66%\n",
      "Batch 114, Loss: 0.958584, Accuracy: 86.61%\n",
      "Batch 115, Loss: 0.892860, Accuracy: 86.59%\n",
      "Batch 116, Loss: 0.891460, Accuracy: 86.58%\n",
      "Batch 117, Loss: 0.861367, Accuracy: 86.59%\n",
      "Batch 118, Loss: 0.838074, Accuracy: 86.64%\n",
      "Batch 119, Loss: 0.797647, Accuracy: 86.73%\n",
      "Batch 120, Loss: 0.928782, Accuracy: 86.69%\n",
      "Batch 121, Loss: 0.858919, Accuracy: 86.71%\n",
      "Batch 122, Loss: 0.949938, Accuracy: 86.64%\n",
      "Batch 123, Loss: 0.816246, Accuracy: 86.70%\n",
      "Batch 124, Loss: 0.926917, Accuracy: 86.64%\n",
      "Batch 125, Loss: 0.898695, Accuracy: 86.62%\n",
      "Batch 126, Loss: 0.799425, Accuracy: 86.69%\n",
      "Batch 127, Loss: 0.904406, Accuracy: 86.68%\n",
      "Batch 128, Loss: 0.914348, Accuracy: 86.66%\n",
      "Batch 129, Loss: 0.905345, Accuracy: 86.65%\n",
      "Batch 130, Loss: 0.820001, Accuracy: 86.69%\n",
      "Batch 131, Loss: 0.882383, Accuracy: 86.70%\n",
      "Batch 132, Loss: 0.880746, Accuracy: 86.70%\n",
      "Batch 133, Loss: 0.826053, Accuracy: 86.72%\n",
      "Batch 134, Loss: 0.854541, Accuracy: 86.74%\n",
      "Batch 135, Loss: 0.879954, Accuracy: 86.74%\n",
      "Batch 136, Loss: 0.864345, Accuracy: 86.75%\n",
      "Batch 137, Loss: 0.870648, Accuracy: 86.75%\n",
      "Batch 138, Loss: 0.851144, Accuracy: 86.75%\n",
      "Batch 139, Loss: 0.908535, Accuracy: 86.74%\n",
      "Batch 140, Loss: 0.883582, Accuracy: 86.73%\n",
      "Batch 141, Loss: 0.894250, Accuracy: 86.72%\n",
      "Batch 142, Loss: 0.831304, Accuracy: 86.77%\n",
      "Batch 143, Loss: 0.866041, Accuracy: 86.77%\n",
      "Batch 144, Loss: 0.913813, Accuracy: 86.73%\n",
      "Batch 145, Loss: 0.884712, Accuracy: 86.73%\n",
      "Batch 146, Loss: 0.918401, Accuracy: 86.72%\n",
      "Batch 147, Loss: 0.890257, Accuracy: 86.71%\n",
      "Batch 148, Loss: 0.810550, Accuracy: 86.76%\n",
      "Batch 149, Loss: 0.871058, Accuracy: 86.77%\n",
      "Batch 150, Loss: 0.917612, Accuracy: 86.73%\n",
      "Batch 151, Loss: 0.808656, Accuracy: 86.79%\n",
      "Batch 152, Loss: 0.836361, Accuracy: 86.82%\n",
      "Batch 153, Loss: 0.840860, Accuracy: 86.85%\n",
      "Batch 154, Loss: 0.837434, Accuracy: 86.86%\n",
      "Batch 155, Loss: 0.809035, Accuracy: 86.92%\n",
      "Batch 156, Loss: 0.838849, Accuracy: 86.95%\n",
      "Batch 157, Loss: 0.813438, Accuracy: 86.99%\n",
      "Batch 158, Loss: 0.858297, Accuracy: 87.01%\n",
      "Batch 159, Loss: 0.909076, Accuracy: 86.99%\n",
      "Batch 160, Loss: 0.931334, Accuracy: 86.95%\n",
      "Batch 161, Loss: 0.911247, Accuracy: 86.93%\n",
      "Batch 162, Loss: 0.854238, Accuracy: 86.95%\n",
      "Batch 163, Loss: 0.845347, Accuracy: 86.96%\n",
      "Batch 164, Loss: 0.872827, Accuracy: 86.96%\n",
      "Batch 165, Loss: 0.938722, Accuracy: 86.91%\n",
      "Batch 166, Loss: 0.845231, Accuracy: 86.94%\n",
      "Batch 167, Loss: 0.852336, Accuracy: 86.95%\n",
      "Batch 168, Loss: 0.858414, Accuracy: 86.96%\n",
      "Batch 169, Loss: 0.874317, Accuracy: 86.95%\n",
      "Batch 170, Loss: 0.818149, Accuracy: 86.99%\n",
      "Batch 171, Loss: 0.887314, Accuracy: 87.00%\n",
      "Batch 172, Loss: 0.835465, Accuracy: 87.02%\n",
      "Batch 173, Loss: 0.873532, Accuracy: 87.02%\n",
      "Batch 174, Loss: 0.884992, Accuracy: 87.02%\n",
      "Batch 175, Loss: 0.928619, Accuracy: 86.99%\n",
      "Batch 176, Loss: 0.914150, Accuracy: 86.97%\n",
      "Batch 177, Loss: 0.830443, Accuracy: 87.01%\n",
      "Batch 178, Loss: 0.923115, Accuracy: 86.97%\n",
      "Batch 179, Loss: 0.857356, Accuracy: 86.98%\n",
      "Batch 180, Loss: 0.882500, Accuracy: 86.97%\n",
      "Batch 181, Loss: 0.840862, Accuracy: 86.98%\n",
      "Batch 182, Loss: 0.934171, Accuracy: 86.94%\n",
      "Batch 183, Loss: 0.933546, Accuracy: 86.90%\n",
      "Batch 184, Loss: 0.974637, Accuracy: 86.85%\n",
      "Batch 185, Loss: 0.827149, Accuracy: 86.88%\n",
      "Batch 186, Loss: 0.870140, Accuracy: 86.88%\n",
      "Batch 187, Loss: 0.826409, Accuracy: 86.91%\n",
      "Batch 188, Loss: 0.885655, Accuracy: 86.91%\n",
      "Batch 189, Loss: 0.898420, Accuracy: 86.90%\n",
      "Batch 190, Loss: 0.891431, Accuracy: 86.89%\n",
      "Batch 191, Loss: 0.888080, Accuracy: 86.89%\n",
      "Batch 192, Loss: 0.923719, Accuracy: 86.86%\n",
      "Batch 193, Loss: 0.849249, Accuracy: 86.88%\n",
      "Batch 194, Loss: 0.833992, Accuracy: 86.90%\n",
      "Batch 195, Loss: 0.867611, Accuracy: 86.90%\n",
      "Batch 196, Loss: 0.863395, Accuracy: 86.90%\n",
      "Batch 197, Loss: 0.911988, Accuracy: 86.87%\n",
      "Batch 198, Loss: 0.854047, Accuracy: 86.88%\n",
      "Batch 199, Loss: 0.896545, Accuracy: 86.88%\n",
      "Batch 200, Loss: 0.845310, Accuracy: 86.91%\n",
      "Batch 201, Loss: 0.852586, Accuracy: 86.92%\n",
      "Batch 202, Loss: 0.849153, Accuracy: 86.92%\n",
      "Batch 203, Loss: 0.882026, Accuracy: 86.92%\n",
      "Batch 204, Loss: 0.880769, Accuracy: 86.90%\n",
      "Batch 205, Loss: 0.875046, Accuracy: 86.91%\n",
      "Batch 206, Loss: 0.893109, Accuracy: 86.90%\n",
      "Batch 207, Loss: 0.885841, Accuracy: 86.90%\n",
      "Batch 208, Loss: 0.884117, Accuracy: 86.89%\n",
      "Batch 209, Loss: 0.808551, Accuracy: 86.92%\n",
      "Batch 210, Loss: 0.818321, Accuracy: 86.96%\n",
      "Batch 211, Loss: 0.841308, Accuracy: 86.98%\n",
      "Batch 212, Loss: 0.878780, Accuracy: 86.98%\n",
      "Batch 213, Loss: 0.912341, Accuracy: 86.96%\n",
      "Training - Epoch 85, Loss: 0.875856, Accuracy: 86.96%\n",
      "Validation Batch 1, Loss: 0.834600, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.807340, Accuracy: 92.97%\n",
      "Validation Batch 3, Loss: 0.891571, Accuracy: 90.10%\n",
      "Validation Batch 4, Loss: 0.845059, Accuracy: 90.23%\n",
      "Validation Batch 5, Loss: 0.841030, Accuracy: 90.31%\n",
      "Validation Batch 6, Loss: 0.810627, Accuracy: 90.89%\n",
      "Validation Batch 7, Loss: 0.818176, Accuracy: 91.29%\n",
      "Validation Batch 8, Loss: 0.917994, Accuracy: 90.04%\n",
      "Validation Batch 9, Loss: 0.900075, Accuracy: 89.41%\n",
      "Validation Batch 10, Loss: 0.844350, Accuracy: 89.69%\n",
      "Validation Batch 11, Loss: 0.845738, Accuracy: 89.63%\n",
      "Validation Batch 12, Loss: 0.839629, Accuracy: 89.84%\n",
      "Validation Batch 13, Loss: 0.859980, Accuracy: 89.66%\n",
      "Validation Batch 14, Loss: 0.871123, Accuracy: 89.40%\n",
      "Validation Batch 15, Loss: 0.835208, Accuracy: 89.58%\n",
      "Validation Batch 16, Loss: 0.844618, Accuracy: 89.75%\n",
      "Validation Batch 17, Loss: 0.897403, Accuracy: 89.43%\n",
      "Validation Batch 18, Loss: 0.831897, Accuracy: 89.50%\n",
      "Validation Batch 19, Loss: 0.899772, Accuracy: 89.23%\n",
      "Validation Batch 20, Loss: 0.894299, Accuracy: 88.98%\n",
      "Validation Batch 21, Loss: 0.880570, Accuracy: 88.84%\n",
      "Validation Batch 22, Loss: 0.860989, Accuracy: 88.78%\n",
      "Validation Batch 23, Loss: 0.888185, Accuracy: 88.72%\n",
      "Validation Batch 24, Loss: 0.880039, Accuracy: 88.61%\n",
      "Validation Batch 25, Loss: 0.831249, Accuracy: 88.69%\n",
      "Validation Batch 26, Loss: 0.862196, Accuracy: 88.64%\n",
      "Validation Batch 27, Loss: 0.816930, Accuracy: 88.78%\n",
      "Validation - Epoch 85, Loss: 0.857431, Accuracy: 88.78%\n",
      "Patience—3\n",
      "Epoch 86\n",
      "Batch 1, Loss: 0.873162, Accuracy: 85.94%\n",
      "Batch 2, Loss: 0.881201, Accuracy: 85.94%\n",
      "Batch 3, Loss: 0.893505, Accuracy: 85.42%\n",
      "Batch 4, Loss: 0.872974, Accuracy: 86.33%\n",
      "Batch 5, Loss: 0.906259, Accuracy: 85.94%\n",
      "Batch 6, Loss: 0.916193, Accuracy: 85.42%\n",
      "Batch 7, Loss: 0.889863, Accuracy: 85.27%\n",
      "Batch 8, Loss: 0.838686, Accuracy: 85.94%\n",
      "Batch 9, Loss: 0.850209, Accuracy: 86.46%\n",
      "Batch 10, Loss: 0.864306, Accuracy: 86.88%\n",
      "Batch 11, Loss: 0.894944, Accuracy: 86.65%\n",
      "Batch 12, Loss: 0.882470, Accuracy: 86.46%\n",
      "Batch 13, Loss: 0.883427, Accuracy: 86.42%\n",
      "Batch 14, Loss: 0.867572, Accuracy: 86.61%\n",
      "Batch 15, Loss: 0.855301, Accuracy: 86.77%\n",
      "Batch 16, Loss: 0.979004, Accuracy: 86.13%\n",
      "Batch 17, Loss: 0.840084, Accuracy: 86.40%\n",
      "Batch 18, Loss: 0.871271, Accuracy: 86.46%\n",
      "Batch 19, Loss: 0.863850, Accuracy: 86.60%\n",
      "Batch 20, Loss: 0.875919, Accuracy: 86.64%\n",
      "Batch 21, Loss: 1.003546, Accuracy: 85.94%\n",
      "Batch 22, Loss: 0.867886, Accuracy: 86.01%\n",
      "Batch 23, Loss: 0.857816, Accuracy: 86.14%\n",
      "Batch 24, Loss: 0.888124, Accuracy: 86.13%\n",
      "Batch 25, Loss: 0.910107, Accuracy: 86.06%\n",
      "Batch 26, Loss: 0.901962, Accuracy: 86.00%\n",
      "Batch 27, Loss: 0.907122, Accuracy: 85.94%\n",
      "Batch 28, Loss: 0.812073, Accuracy: 86.22%\n",
      "Batch 29, Loss: 0.889702, Accuracy: 86.21%\n",
      "Batch 30, Loss: 0.892702, Accuracy: 86.20%\n",
      "Batch 31, Loss: 0.849599, Accuracy: 86.34%\n",
      "Batch 32, Loss: 0.904344, Accuracy: 86.23%\n",
      "Batch 33, Loss: 0.904434, Accuracy: 86.17%\n",
      "Batch 34, Loss: 0.865449, Accuracy: 86.26%\n",
      "Batch 35, Loss: 0.927084, Accuracy: 86.07%\n",
      "Batch 36, Loss: 0.828367, Accuracy: 86.28%\n",
      "Batch 37, Loss: 0.893484, Accuracy: 86.23%\n",
      "Batch 38, Loss: 0.814549, Accuracy: 86.43%\n",
      "Batch 39, Loss: 0.873165, Accuracy: 86.50%\n",
      "Batch 40, Loss: 0.899898, Accuracy: 86.45%\n",
      "Batch 41, Loss: 0.917825, Accuracy: 86.36%\n",
      "Batch 42, Loss: 0.841559, Accuracy: 86.46%\n",
      "Batch 43, Loss: 0.859622, Accuracy: 86.52%\n",
      "Batch 44, Loss: 0.909961, Accuracy: 86.40%\n",
      "Batch 45, Loss: 0.927765, Accuracy: 86.25%\n",
      "Batch 46, Loss: 0.871235, Accuracy: 86.31%\n",
      "Batch 47, Loss: 0.916001, Accuracy: 86.24%\n",
      "Batch 48, Loss: 0.875651, Accuracy: 86.26%\n",
      "Batch 49, Loss: 0.862127, Accuracy: 86.32%\n",
      "Batch 50, Loss: 0.908966, Accuracy: 86.25%\n",
      "Batch 51, Loss: 0.781486, Accuracy: 86.46%\n",
      "Batch 52, Loss: 0.872734, Accuracy: 86.45%\n",
      "Batch 53, Loss: 0.892565, Accuracy: 86.41%\n",
      "Batch 54, Loss: 0.823274, Accuracy: 86.49%\n",
      "Batch 55, Loss: 0.902506, Accuracy: 86.48%\n",
      "Batch 56, Loss: 0.903494, Accuracy: 86.41%\n",
      "Batch 57, Loss: 0.941819, Accuracy: 86.35%\n",
      "Batch 58, Loss: 0.891530, Accuracy: 86.34%\n",
      "Batch 59, Loss: 0.865456, Accuracy: 86.36%\n",
      "Batch 60, Loss: 0.874944, Accuracy: 86.38%\n",
      "Batch 61, Loss: 0.780783, Accuracy: 86.58%\n",
      "Batch 62, Loss: 0.846933, Accuracy: 86.64%\n",
      "Batch 63, Loss: 0.810700, Accuracy: 86.76%\n",
      "Batch 64, Loss: 0.905484, Accuracy: 86.72%\n",
      "Batch 65, Loss: 0.855832, Accuracy: 86.75%\n",
      "Batch 66, Loss: 0.793007, Accuracy: 86.88%\n",
      "Batch 67, Loss: 0.839124, Accuracy: 86.92%\n",
      "Batch 68, Loss: 0.972253, Accuracy: 86.76%\n",
      "Batch 69, Loss: 0.857982, Accuracy: 86.80%\n",
      "Batch 70, Loss: 0.859627, Accuracy: 86.83%\n",
      "Batch 71, Loss: 0.826827, Accuracy: 86.91%\n",
      "Batch 72, Loss: 0.877679, Accuracy: 86.89%\n",
      "Batch 73, Loss: 0.942696, Accuracy: 86.84%\n",
      "Batch 74, Loss: 0.928220, Accuracy: 86.78%\n",
      "Batch 75, Loss: 0.862328, Accuracy: 86.81%\n",
      "Batch 76, Loss: 0.829681, Accuracy: 86.88%\n",
      "Batch 77, Loss: 0.854280, Accuracy: 86.91%\n",
      "Batch 78, Loss: 0.900824, Accuracy: 86.86%\n",
      "Batch 79, Loss: 0.875315, Accuracy: 86.87%\n",
      "Batch 80, Loss: 0.834732, Accuracy: 86.91%\n",
      "Batch 81, Loss: 0.911654, Accuracy: 86.86%\n",
      "Batch 82, Loss: 0.839077, Accuracy: 86.91%\n",
      "Batch 83, Loss: 0.834649, Accuracy: 86.95%\n",
      "Batch 84, Loss: 0.845017, Accuracy: 86.98%\n",
      "Batch 85, Loss: 0.868359, Accuracy: 86.99%\n",
      "Batch 86, Loss: 0.880637, Accuracy: 86.97%\n",
      "Batch 87, Loss: 0.858530, Accuracy: 87.00%\n",
      "Batch 88, Loss: 0.847464, Accuracy: 87.04%\n",
      "Batch 89, Loss: 0.887355, Accuracy: 87.01%\n",
      "Batch 90, Loss: 0.808401, Accuracy: 87.10%\n",
      "Batch 91, Loss: 0.856777, Accuracy: 87.12%\n",
      "Batch 92, Loss: 0.871764, Accuracy: 87.11%\n",
      "Batch 93, Loss: 0.840219, Accuracy: 87.13%\n",
      "Batch 94, Loss: 0.850724, Accuracy: 87.15%\n",
      "Batch 95, Loss: 0.847778, Accuracy: 87.19%\n",
      "Batch 96, Loss: 0.884463, Accuracy: 87.17%\n",
      "Batch 97, Loss: 0.860628, Accuracy: 87.19%\n",
      "Batch 98, Loss: 0.861079, Accuracy: 87.21%\n",
      "Batch 99, Loss: 0.962446, Accuracy: 87.12%\n",
      "Batch 100, Loss: 0.893675, Accuracy: 87.09%\n",
      "Batch 101, Loss: 0.925419, Accuracy: 87.05%\n",
      "Batch 102, Loss: 0.896486, Accuracy: 87.01%\n",
      "Batch 103, Loss: 0.826832, Accuracy: 87.06%\n",
      "Batch 104, Loss: 0.849826, Accuracy: 87.09%\n",
      "Batch 105, Loss: 0.847440, Accuracy: 87.13%\n",
      "Batch 106, Loss: 0.823558, Accuracy: 87.18%\n",
      "Batch 107, Loss: 0.897411, Accuracy: 87.15%\n",
      "Batch 108, Loss: 0.921483, Accuracy: 87.11%\n",
      "Batch 109, Loss: 0.925529, Accuracy: 87.06%\n",
      "Batch 110, Loss: 0.862670, Accuracy: 87.06%\n",
      "Batch 111, Loss: 0.899911, Accuracy: 87.05%\n",
      "Batch 112, Loss: 0.916348, Accuracy: 87.01%\n",
      "Batch 113, Loss: 0.869885, Accuracy: 87.02%\n",
      "Batch 114, Loss: 0.825210, Accuracy: 87.05%\n",
      "Batch 115, Loss: 0.954465, Accuracy: 86.97%\n",
      "Batch 116, Loss: 0.821552, Accuracy: 87.02%\n",
      "Batch 117, Loss: 0.841314, Accuracy: 87.05%\n",
      "Batch 118, Loss: 0.863137, Accuracy: 87.06%\n",
      "Batch 119, Loss: 0.843948, Accuracy: 87.09%\n",
      "Batch 120, Loss: 0.859228, Accuracy: 87.10%\n",
      "Batch 121, Loss: 0.880229, Accuracy: 87.09%\n",
      "Batch 122, Loss: 0.855614, Accuracy: 87.10%\n",
      "Batch 123, Loss: 0.879593, Accuracy: 87.11%\n",
      "Batch 124, Loss: 0.930838, Accuracy: 87.05%\n",
      "Batch 125, Loss: 0.868240, Accuracy: 87.05%\n",
      "Batch 126, Loss: 0.886632, Accuracy: 87.04%\n",
      "Batch 127, Loss: 0.849119, Accuracy: 87.06%\n",
      "Batch 128, Loss: 0.894341, Accuracy: 87.05%\n",
      "Batch 129, Loss: 0.898600, Accuracy: 87.03%\n",
      "Batch 130, Loss: 0.902814, Accuracy: 87.01%\n",
      "Batch 131, Loss: 0.899017, Accuracy: 86.98%\n",
      "Batch 132, Loss: 0.903058, Accuracy: 86.96%\n",
      "Batch 133, Loss: 0.940038, Accuracy: 86.90%\n",
      "Batch 134, Loss: 0.847599, Accuracy: 86.93%\n",
      "Batch 135, Loss: 0.934833, Accuracy: 86.89%\n",
      "Batch 136, Loss: 0.859468, Accuracy: 86.90%\n",
      "Batch 137, Loss: 0.867740, Accuracy: 86.92%\n",
      "Batch 138, Loss: 0.825003, Accuracy: 86.95%\n",
      "Batch 139, Loss: 0.856880, Accuracy: 86.96%\n",
      "Batch 140, Loss: 0.851307, Accuracy: 86.99%\n",
      "Batch 141, Loss: 0.879727, Accuracy: 86.97%\n",
      "Batch 142, Loss: 0.886426, Accuracy: 86.95%\n",
      "Batch 143, Loss: 0.895794, Accuracy: 86.94%\n",
      "Batch 144, Loss: 0.832526, Accuracy: 86.98%\n",
      "Batch 145, Loss: 0.840066, Accuracy: 87.00%\n",
      "Batch 146, Loss: 0.917622, Accuracy: 86.98%\n",
      "Batch 147, Loss: 0.841119, Accuracy: 87.00%\n",
      "Batch 148, Loss: 0.875572, Accuracy: 87.00%\n",
      "Batch 149, Loss: 0.916829, Accuracy: 86.95%\n",
      "Batch 150, Loss: 0.920164, Accuracy: 86.93%\n",
      "Batch 151, Loss: 0.858844, Accuracy: 86.94%\n",
      "Batch 152, Loss: 0.815165, Accuracy: 86.99%\n",
      "Batch 153, Loss: 0.818070, Accuracy: 87.02%\n",
      "Batch 154, Loss: 0.896513, Accuracy: 87.01%\n",
      "Batch 155, Loss: 0.896696, Accuracy: 87.01%\n",
      "Batch 156, Loss: 0.872685, Accuracy: 87.00%\n",
      "Batch 157, Loss: 0.872636, Accuracy: 87.00%\n",
      "Batch 158, Loss: 0.839455, Accuracy: 87.03%\n",
      "Batch 159, Loss: 0.908214, Accuracy: 87.01%\n",
      "Batch 160, Loss: 0.889752, Accuracy: 87.00%\n",
      "Batch 161, Loss: 0.920044, Accuracy: 86.97%\n",
      "Batch 162, Loss: 0.780674, Accuracy: 87.04%\n",
      "Batch 163, Loss: 0.870068, Accuracy: 87.03%\n",
      "Batch 164, Loss: 0.877101, Accuracy: 87.03%\n",
      "Batch 165, Loss: 0.872975, Accuracy: 87.04%\n",
      "Batch 166, Loss: 0.943407, Accuracy: 87.00%\n",
      "Batch 167, Loss: 0.937863, Accuracy: 86.96%\n",
      "Batch 168, Loss: 0.917023, Accuracy: 86.93%\n",
      "Batch 169, Loss: 0.860394, Accuracy: 86.95%\n",
      "Batch 170, Loss: 0.875086, Accuracy: 86.95%\n",
      "Batch 171, Loss: 0.810759, Accuracy: 86.99%\n",
      "Batch 172, Loss: 0.883555, Accuracy: 86.98%\n",
      "Batch 173, Loss: 0.942575, Accuracy: 86.95%\n",
      "Batch 174, Loss: 0.878775, Accuracy: 86.94%\n",
      "Batch 175, Loss: 0.926693, Accuracy: 86.91%\n",
      "Batch 176, Loss: 0.955059, Accuracy: 86.87%\n",
      "Batch 177, Loss: 0.842267, Accuracy: 86.89%\n",
      "Batch 178, Loss: 0.834487, Accuracy: 86.92%\n",
      "Batch 179, Loss: 0.868847, Accuracy: 86.92%\n",
      "Batch 180, Loss: 0.827049, Accuracy: 86.95%\n",
      "Batch 181, Loss: 0.860651, Accuracy: 86.96%\n",
      "Batch 182, Loss: 0.864448, Accuracy: 86.97%\n",
      "Batch 183, Loss: 0.843861, Accuracy: 86.99%\n",
      "Batch 184, Loss: 0.857635, Accuracy: 87.00%\n",
      "Batch 185, Loss: 0.879445, Accuracy: 87.00%\n",
      "Batch 186, Loss: 0.896978, Accuracy: 86.99%\n",
      "Batch 187, Loss: 0.878964, Accuracy: 86.98%\n",
      "Batch 188, Loss: 0.872733, Accuracy: 86.98%\n",
      "Batch 189, Loss: 0.917388, Accuracy: 86.97%\n",
      "Batch 190, Loss: 0.911507, Accuracy: 86.96%\n",
      "Batch 191, Loss: 0.846664, Accuracy: 86.97%\n",
      "Batch 192, Loss: 0.800508, Accuracy: 87.01%\n",
      "Batch 193, Loss: 0.878525, Accuracy: 87.01%\n",
      "Batch 194, Loss: 0.840824, Accuracy: 87.02%\n",
      "Batch 195, Loss: 0.850485, Accuracy: 87.04%\n",
      "Batch 196, Loss: 0.865815, Accuracy: 87.04%\n",
      "Batch 197, Loss: 0.924603, Accuracy: 87.01%\n",
      "Batch 198, Loss: 0.900401, Accuracy: 87.00%\n",
      "Batch 199, Loss: 0.831257, Accuracy: 87.02%\n",
      "Batch 200, Loss: 0.799750, Accuracy: 87.06%\n",
      "Batch 201, Loss: 0.942852, Accuracy: 87.04%\n",
      "Batch 202, Loss: 0.839033, Accuracy: 87.07%\n",
      "Batch 203, Loss: 0.816563, Accuracy: 87.11%\n",
      "Batch 204, Loss: 0.870548, Accuracy: 87.12%\n",
      "Batch 205, Loss: 0.850031, Accuracy: 87.13%\n",
      "Batch 206, Loss: 0.819028, Accuracy: 87.15%\n",
      "Batch 207, Loss: 0.811149, Accuracy: 87.18%\n",
      "Batch 208, Loss: 0.878885, Accuracy: 87.18%\n",
      "Batch 209, Loss: 0.865897, Accuracy: 87.19%\n",
      "Batch 210, Loss: 0.889156, Accuracy: 87.18%\n",
      "Batch 211, Loss: 0.816581, Accuracy: 87.20%\n",
      "Batch 212, Loss: 0.829384, Accuracy: 87.23%\n",
      "Batch 213, Loss: 0.918941, Accuracy: 87.20%\n",
      "Training - Epoch 86, Loss: 0.874032, Accuracy: 87.20%\n",
      "Validation Batch 1, Loss: 0.834832, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.808765, Accuracy: 92.97%\n",
      "Validation Batch 3, Loss: 0.889938, Accuracy: 90.10%\n",
      "Validation Batch 4, Loss: 0.851956, Accuracy: 90.23%\n",
      "Validation Batch 5, Loss: 0.839127, Accuracy: 90.31%\n",
      "Validation Batch 6, Loss: 0.812817, Accuracy: 90.89%\n",
      "Validation Batch 7, Loss: 0.818989, Accuracy: 91.29%\n",
      "Validation Batch 8, Loss: 0.917096, Accuracy: 90.23%\n",
      "Validation Batch 9, Loss: 0.906751, Accuracy: 89.58%\n",
      "Validation Batch 10, Loss: 0.841698, Accuracy: 89.84%\n",
      "Validation Batch 11, Loss: 0.847913, Accuracy: 89.77%\n",
      "Validation Batch 12, Loss: 0.842216, Accuracy: 89.97%\n",
      "Validation Batch 13, Loss: 0.858711, Accuracy: 89.78%\n",
      "Validation Batch 14, Loss: 0.871863, Accuracy: 89.51%\n",
      "Validation Batch 15, Loss: 0.837329, Accuracy: 89.69%\n",
      "Validation Batch 16, Loss: 0.849054, Accuracy: 89.75%\n",
      "Validation Batch 17, Loss: 0.896490, Accuracy: 89.52%\n",
      "Validation Batch 18, Loss: 0.827687, Accuracy: 89.58%\n",
      "Validation Batch 19, Loss: 0.902290, Accuracy: 89.23%\n",
      "Validation Batch 20, Loss: 0.900166, Accuracy: 88.91%\n",
      "Validation Batch 21, Loss: 0.879621, Accuracy: 88.84%\n",
      "Validation Batch 22, Loss: 0.863100, Accuracy: 88.78%\n",
      "Validation Batch 23, Loss: 0.885205, Accuracy: 88.72%\n",
      "Validation Batch 24, Loss: 0.882008, Accuracy: 88.61%\n",
      "Validation Batch 25, Loss: 0.833467, Accuracy: 88.69%\n",
      "Validation Batch 26, Loss: 0.870425, Accuracy: 88.64%\n",
      "Validation Batch 27, Loss: 0.820542, Accuracy: 88.73%\n",
      "Validation - Epoch 86, Loss: 0.858891, Accuracy: 88.73%\n",
      "Patience—4\n",
      "Epoch 87\n",
      "Batch 1, Loss: 0.849845, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.941435, Accuracy: 85.94%\n",
      "Batch 3, Loss: 0.906110, Accuracy: 84.90%\n",
      "Batch 4, Loss: 0.856280, Accuracy: 85.55%\n",
      "Batch 5, Loss: 0.875320, Accuracy: 86.25%\n",
      "Batch 6, Loss: 0.849825, Accuracy: 86.46%\n",
      "Batch 7, Loss: 0.863623, Accuracy: 86.61%\n",
      "Batch 8, Loss: 0.920365, Accuracy: 86.33%\n",
      "Batch 9, Loss: 0.945519, Accuracy: 85.42%\n",
      "Batch 10, Loss: 0.902625, Accuracy: 85.31%\n",
      "Batch 11, Loss: 0.813907, Accuracy: 85.94%\n",
      "Batch 12, Loss: 0.859211, Accuracy: 86.33%\n",
      "Batch 13, Loss: 0.853149, Accuracy: 86.66%\n",
      "Batch 14, Loss: 0.827369, Accuracy: 87.05%\n",
      "Batch 15, Loss: 0.813490, Accuracy: 87.40%\n",
      "Batch 16, Loss: 0.849130, Accuracy: 87.60%\n",
      "Batch 17, Loss: 0.870196, Accuracy: 87.50%\n",
      "Batch 18, Loss: 0.898986, Accuracy: 87.33%\n",
      "Batch 19, Loss: 0.848573, Accuracy: 87.42%\n",
      "Batch 20, Loss: 0.921495, Accuracy: 87.11%\n",
      "Batch 21, Loss: 0.815913, Accuracy: 87.35%\n",
      "Batch 22, Loss: 0.868914, Accuracy: 87.43%\n",
      "Batch 23, Loss: 0.823176, Accuracy: 87.70%\n",
      "Batch 24, Loss: 0.935516, Accuracy: 87.50%\n",
      "Batch 25, Loss: 0.838377, Accuracy: 87.69%\n",
      "Batch 26, Loss: 0.824684, Accuracy: 87.80%\n",
      "Batch 27, Loss: 0.811289, Accuracy: 88.02%\n",
      "Batch 28, Loss: 0.824357, Accuracy: 88.23%\n",
      "Batch 29, Loss: 0.846188, Accuracy: 88.25%\n",
      "Batch 30, Loss: 0.878927, Accuracy: 88.23%\n",
      "Batch 31, Loss: 0.873917, Accuracy: 88.21%\n",
      "Batch 32, Loss: 0.881525, Accuracy: 88.13%\n",
      "Batch 33, Loss: 0.883971, Accuracy: 88.02%\n",
      "Batch 34, Loss: 0.881020, Accuracy: 88.01%\n",
      "Batch 35, Loss: 0.905951, Accuracy: 87.90%\n",
      "Batch 36, Loss: 0.860618, Accuracy: 87.93%\n",
      "Batch 37, Loss: 0.896733, Accuracy: 87.84%\n",
      "Batch 38, Loss: 0.849497, Accuracy: 87.91%\n",
      "Batch 39, Loss: 0.916039, Accuracy: 87.78%\n",
      "Batch 40, Loss: 0.820880, Accuracy: 87.93%\n",
      "Batch 41, Loss: 0.895922, Accuracy: 87.84%\n",
      "Batch 42, Loss: 0.887847, Accuracy: 87.80%\n",
      "Batch 43, Loss: 0.843205, Accuracy: 87.86%\n",
      "Batch 44, Loss: 0.908279, Accuracy: 87.75%\n",
      "Batch 45, Loss: 0.879306, Accuracy: 87.71%\n",
      "Batch 46, Loss: 0.903350, Accuracy: 87.64%\n",
      "Batch 47, Loss: 0.883753, Accuracy: 87.60%\n",
      "Batch 48, Loss: 0.865973, Accuracy: 87.60%\n",
      "Batch 49, Loss: 0.937615, Accuracy: 87.50%\n",
      "Batch 50, Loss: 0.824991, Accuracy: 87.59%\n",
      "Batch 51, Loss: 0.871014, Accuracy: 87.59%\n",
      "Batch 52, Loss: 0.896894, Accuracy: 87.56%\n",
      "Batch 53, Loss: 0.853631, Accuracy: 87.62%\n",
      "Batch 54, Loss: 0.911803, Accuracy: 87.53%\n",
      "Batch 55, Loss: 0.837312, Accuracy: 87.59%\n",
      "Batch 56, Loss: 0.922944, Accuracy: 87.44%\n",
      "Batch 57, Loss: 0.838067, Accuracy: 87.47%\n",
      "Batch 58, Loss: 0.905065, Accuracy: 87.39%\n",
      "Batch 59, Loss: 0.827839, Accuracy: 87.50%\n",
      "Batch 60, Loss: 0.843970, Accuracy: 87.55%\n",
      "Batch 61, Loss: 0.855494, Accuracy: 87.58%\n",
      "Batch 62, Loss: 0.849425, Accuracy: 87.65%\n",
      "Batch 63, Loss: 0.884634, Accuracy: 87.62%\n",
      "Batch 64, Loss: 0.814785, Accuracy: 87.70%\n",
      "Batch 65, Loss: 0.851576, Accuracy: 87.74%\n",
      "Batch 66, Loss: 0.838943, Accuracy: 87.81%\n",
      "Batch 67, Loss: 0.887091, Accuracy: 87.78%\n",
      "Batch 68, Loss: 0.864889, Accuracy: 87.78%\n",
      "Batch 69, Loss: 0.899911, Accuracy: 87.75%\n",
      "Batch 70, Loss: 0.857818, Accuracy: 87.77%\n",
      "Batch 71, Loss: 0.848709, Accuracy: 87.79%\n",
      "Batch 72, Loss: 0.846529, Accuracy: 87.83%\n",
      "Batch 73, Loss: 0.833965, Accuracy: 87.86%\n",
      "Batch 74, Loss: 0.918593, Accuracy: 87.80%\n",
      "Batch 75, Loss: 0.853243, Accuracy: 87.81%\n",
      "Batch 76, Loss: 0.867358, Accuracy: 87.81%\n",
      "Batch 77, Loss: 0.935774, Accuracy: 87.72%\n",
      "Batch 78, Loss: 0.990182, Accuracy: 87.54%\n",
      "Batch 79, Loss: 0.808766, Accuracy: 87.62%\n",
      "Batch 80, Loss: 0.838541, Accuracy: 87.66%\n",
      "Batch 81, Loss: 0.941611, Accuracy: 87.56%\n",
      "Batch 82, Loss: 0.877985, Accuracy: 87.52%\n",
      "Batch 83, Loss: 0.880118, Accuracy: 87.52%\n",
      "Batch 84, Loss: 0.879920, Accuracy: 87.50%\n",
      "Batch 85, Loss: 0.894978, Accuracy: 87.46%\n",
      "Batch 86, Loss: 0.847797, Accuracy: 87.50%\n",
      "Batch 87, Loss: 0.883674, Accuracy: 87.50%\n",
      "Batch 88, Loss: 0.945398, Accuracy: 87.39%\n",
      "Batch 89, Loss: 0.864951, Accuracy: 87.39%\n",
      "Batch 90, Loss: 0.821760, Accuracy: 87.45%\n",
      "Batch 91, Loss: 0.859346, Accuracy: 87.48%\n",
      "Batch 92, Loss: 0.919528, Accuracy: 87.43%\n",
      "Batch 93, Loss: 0.907278, Accuracy: 87.42%\n",
      "Batch 94, Loss: 0.886290, Accuracy: 87.40%\n",
      "Batch 95, Loss: 0.872716, Accuracy: 87.38%\n",
      "Batch 96, Loss: 0.828971, Accuracy: 87.42%\n",
      "Batch 97, Loss: 0.880417, Accuracy: 87.39%\n",
      "Batch 98, Loss: 0.853082, Accuracy: 87.40%\n",
      "Batch 99, Loss: 0.884952, Accuracy: 87.39%\n",
      "Batch 100, Loss: 0.882920, Accuracy: 87.36%\n",
      "Batch 101, Loss: 0.898316, Accuracy: 87.33%\n",
      "Batch 102, Loss: 0.866045, Accuracy: 87.35%\n",
      "Batch 103, Loss: 0.900536, Accuracy: 87.32%\n",
      "Batch 104, Loss: 0.892651, Accuracy: 87.30%\n",
      "Batch 105, Loss: 0.826650, Accuracy: 87.35%\n",
      "Batch 106, Loss: 0.872066, Accuracy: 87.34%\n",
      "Batch 107, Loss: 0.885962, Accuracy: 87.31%\n",
      "Batch 108, Loss: 0.866460, Accuracy: 87.31%\n",
      "Batch 109, Loss: 0.860049, Accuracy: 87.33%\n",
      "Batch 110, Loss: 0.848750, Accuracy: 87.34%\n",
      "Batch 111, Loss: 0.941867, Accuracy: 87.26%\n",
      "Batch 112, Loss: 0.827044, Accuracy: 87.30%\n",
      "Batch 113, Loss: 0.917069, Accuracy: 87.26%\n",
      "Batch 114, Loss: 0.938533, Accuracy: 87.18%\n",
      "Batch 115, Loss: 0.815292, Accuracy: 87.24%\n",
      "Batch 116, Loss: 0.869723, Accuracy: 87.23%\n",
      "Batch 117, Loss: 0.891650, Accuracy: 87.22%\n",
      "Batch 118, Loss: 0.899666, Accuracy: 87.20%\n",
      "Batch 119, Loss: 0.881046, Accuracy: 87.18%\n",
      "Batch 120, Loss: 0.779596, Accuracy: 87.27%\n",
      "Batch 121, Loss: 0.850343, Accuracy: 87.29%\n",
      "Batch 122, Loss: 0.780165, Accuracy: 87.38%\n",
      "Batch 123, Loss: 0.876144, Accuracy: 87.39%\n",
      "Batch 124, Loss: 0.863962, Accuracy: 87.40%\n",
      "Batch 125, Loss: 0.893438, Accuracy: 87.36%\n",
      "Batch 126, Loss: 0.912336, Accuracy: 87.33%\n",
      "Batch 127, Loss: 0.824375, Accuracy: 87.36%\n",
      "Batch 128, Loss: 0.854988, Accuracy: 87.38%\n",
      "Batch 129, Loss: 0.881459, Accuracy: 87.35%\n",
      "Batch 130, Loss: 0.842816, Accuracy: 87.38%\n",
      "Batch 131, Loss: 0.847704, Accuracy: 87.40%\n",
      "Batch 132, Loss: 0.840864, Accuracy: 87.44%\n",
      "Batch 133, Loss: 0.877744, Accuracy: 87.43%\n",
      "Batch 134, Loss: 0.937934, Accuracy: 87.37%\n",
      "Batch 135, Loss: 0.908157, Accuracy: 87.35%\n",
      "Batch 136, Loss: 0.853907, Accuracy: 87.36%\n",
      "Batch 137, Loss: 0.895272, Accuracy: 87.34%\n",
      "Batch 138, Loss: 0.876212, Accuracy: 87.34%\n",
      "Batch 139, Loss: 0.924164, Accuracy: 87.30%\n",
      "Batch 140, Loss: 0.921119, Accuracy: 87.25%\n",
      "Batch 141, Loss: 0.862680, Accuracy: 87.26%\n",
      "Batch 142, Loss: 0.940948, Accuracy: 87.20%\n",
      "Batch 143, Loss: 0.881635, Accuracy: 87.19%\n",
      "Batch 144, Loss: 0.837803, Accuracy: 87.22%\n",
      "Batch 145, Loss: 0.871684, Accuracy: 87.23%\n",
      "Batch 146, Loss: 0.892391, Accuracy: 87.23%\n",
      "Batch 147, Loss: 0.900514, Accuracy: 87.21%\n",
      "Batch 148, Loss: 0.859778, Accuracy: 87.23%\n",
      "Batch 149, Loss: 0.888995, Accuracy: 87.21%\n",
      "Batch 150, Loss: 0.855582, Accuracy: 87.22%\n",
      "Batch 151, Loss: 0.879534, Accuracy: 87.21%\n",
      "Batch 152, Loss: 0.868236, Accuracy: 87.20%\n",
      "Batch 153, Loss: 0.880589, Accuracy: 87.20%\n",
      "Batch 154, Loss: 0.852388, Accuracy: 87.23%\n",
      "Batch 155, Loss: 0.894080, Accuracy: 87.22%\n",
      "Batch 156, Loss: 0.888554, Accuracy: 87.20%\n",
      "Batch 157, Loss: 0.902207, Accuracy: 87.18%\n",
      "Batch 158, Loss: 0.833180, Accuracy: 87.21%\n",
      "Batch 159, Loss: 0.849534, Accuracy: 87.22%\n",
      "Batch 160, Loss: 0.849750, Accuracy: 87.24%\n",
      "Batch 161, Loss: 0.875330, Accuracy: 87.24%\n",
      "Batch 162, Loss: 0.876477, Accuracy: 87.23%\n",
      "Batch 163, Loss: 0.894243, Accuracy: 87.21%\n",
      "Batch 164, Loss: 0.917563, Accuracy: 87.18%\n",
      "Batch 165, Loss: 0.846718, Accuracy: 87.20%\n",
      "Batch 166, Loss: 0.832139, Accuracy: 87.23%\n",
      "Batch 167, Loss: 0.869353, Accuracy: 87.24%\n",
      "Batch 168, Loss: 0.867816, Accuracy: 87.24%\n",
      "Batch 169, Loss: 0.849841, Accuracy: 87.25%\n",
      "Batch 170, Loss: 0.815193, Accuracy: 87.29%\n",
      "Batch 171, Loss: 0.875095, Accuracy: 87.29%\n",
      "Batch 172, Loss: 0.895415, Accuracy: 87.27%\n",
      "Batch 173, Loss: 0.882104, Accuracy: 87.27%\n",
      "Batch 174, Loss: 0.961874, Accuracy: 87.20%\n",
      "Batch 175, Loss: 0.924316, Accuracy: 87.16%\n",
      "Batch 176, Loss: 0.900574, Accuracy: 87.13%\n",
      "Batch 177, Loss: 0.886733, Accuracy: 87.11%\n",
      "Batch 178, Loss: 0.875328, Accuracy: 87.12%\n",
      "Batch 179, Loss: 0.951007, Accuracy: 87.08%\n",
      "Batch 180, Loss: 0.820851, Accuracy: 87.12%\n",
      "Batch 181, Loss: 0.887095, Accuracy: 87.11%\n",
      "Batch 182, Loss: 0.899280, Accuracy: 87.10%\n",
      "Batch 183, Loss: 0.849906, Accuracy: 87.12%\n",
      "Batch 184, Loss: 0.838556, Accuracy: 87.13%\n",
      "Batch 185, Loss: 0.875265, Accuracy: 87.12%\n",
      "Batch 186, Loss: 0.866918, Accuracy: 87.13%\n",
      "Batch 187, Loss: 0.986163, Accuracy: 87.07%\n",
      "Batch 188, Loss: 0.908487, Accuracy: 87.05%\n",
      "Batch 189, Loss: 0.850255, Accuracy: 87.06%\n",
      "Batch 190, Loss: 0.956434, Accuracy: 87.01%\n",
      "Batch 191, Loss: 0.871665, Accuracy: 87.03%\n",
      "Batch 192, Loss: 0.884960, Accuracy: 87.03%\n",
      "Batch 193, Loss: 0.924665, Accuracy: 87.00%\n",
      "Batch 194, Loss: 0.913042, Accuracy: 86.97%\n",
      "Batch 195, Loss: 0.907151, Accuracy: 86.95%\n",
      "Batch 196, Loss: 0.890595, Accuracy: 86.93%\n",
      "Batch 197, Loss: 0.813393, Accuracy: 86.97%\n",
      "Batch 198, Loss: 0.836350, Accuracy: 86.99%\n",
      "Batch 199, Loss: 0.858922, Accuracy: 86.99%\n",
      "Batch 200, Loss: 0.822974, Accuracy: 87.02%\n",
      "Batch 201, Loss: 0.975332, Accuracy: 86.97%\n",
      "Batch 202, Loss: 0.837855, Accuracy: 87.00%\n",
      "Batch 203, Loss: 0.881741, Accuracy: 86.99%\n",
      "Batch 204, Loss: 0.844198, Accuracy: 87.01%\n",
      "Batch 205, Loss: 0.881778, Accuracy: 87.00%\n",
      "Batch 206, Loss: 0.876017, Accuracy: 87.00%\n",
      "Batch 207, Loss: 0.865373, Accuracy: 87.00%\n",
      "Batch 208, Loss: 0.802781, Accuracy: 87.03%\n",
      "Batch 209, Loss: 0.860061, Accuracy: 87.04%\n",
      "Batch 210, Loss: 0.855441, Accuracy: 87.05%\n",
      "Batch 211, Loss: 0.852313, Accuracy: 87.06%\n",
      "Batch 212, Loss: 0.880142, Accuracy: 87.06%\n",
      "Batch 213, Loss: 0.807266, Accuracy: 87.09%\n",
      "Training - Epoch 87, Loss: 0.873780, Accuracy: 87.09%\n",
      "Validation Batch 1, Loss: 0.836780, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.813423, Accuracy: 92.97%\n",
      "Validation Batch 3, Loss: 0.887331, Accuracy: 90.10%\n",
      "Validation Batch 4, Loss: 0.852032, Accuracy: 90.23%\n",
      "Validation Batch 5, Loss: 0.844148, Accuracy: 90.00%\n",
      "Validation Batch 6, Loss: 0.813435, Accuracy: 90.62%\n",
      "Validation Batch 7, Loss: 0.821541, Accuracy: 90.85%\n",
      "Validation Batch 8, Loss: 0.913777, Accuracy: 90.04%\n",
      "Validation Batch 9, Loss: 0.902535, Accuracy: 89.41%\n",
      "Validation Batch 10, Loss: 0.849901, Accuracy: 89.53%\n",
      "Validation Batch 11, Loss: 0.847666, Accuracy: 89.77%\n",
      "Validation Batch 12, Loss: 0.843283, Accuracy: 89.97%\n",
      "Validation Batch 13, Loss: 0.864043, Accuracy: 89.78%\n",
      "Validation Batch 14, Loss: 0.872734, Accuracy: 89.40%\n",
      "Validation Batch 15, Loss: 0.835169, Accuracy: 89.58%\n",
      "Validation Batch 16, Loss: 0.844667, Accuracy: 89.75%\n",
      "Validation Batch 17, Loss: 0.894196, Accuracy: 89.52%\n",
      "Validation Batch 18, Loss: 0.829087, Accuracy: 89.50%\n",
      "Validation Batch 19, Loss: 0.896127, Accuracy: 89.31%\n",
      "Validation Batch 20, Loss: 0.904623, Accuracy: 88.98%\n",
      "Validation Batch 21, Loss: 0.878799, Accuracy: 88.91%\n",
      "Validation Batch 22, Loss: 0.863598, Accuracy: 88.78%\n",
      "Validation Batch 23, Loss: 0.895454, Accuracy: 88.65%\n",
      "Validation Batch 24, Loss: 0.884006, Accuracy: 88.54%\n",
      "Validation Batch 25, Loss: 0.835169, Accuracy: 88.62%\n",
      "Validation Batch 26, Loss: 0.866122, Accuracy: 88.58%\n",
      "Validation Batch 27, Loss: 0.815780, Accuracy: 88.73%\n",
      "Validation - Epoch 87, Loss: 0.859460, Accuracy: 88.73%\n",
      "Patience—5\n",
      "Epoch 88\n",
      "Batch 1, Loss: 0.835958, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.968535, Accuracy: 84.38%\n",
      "Batch 3, Loss: 0.900878, Accuracy: 84.38%\n",
      "Batch 4, Loss: 0.915929, Accuracy: 84.38%\n",
      "Batch 5, Loss: 0.876710, Accuracy: 85.00%\n",
      "Batch 6, Loss: 0.895515, Accuracy: 85.16%\n",
      "Batch 7, Loss: 0.814915, Accuracy: 86.16%\n",
      "Batch 8, Loss: 0.831871, Accuracy: 86.72%\n",
      "Batch 9, Loss: 0.867181, Accuracy: 86.81%\n",
      "Batch 10, Loss: 0.879713, Accuracy: 86.88%\n",
      "Batch 11, Loss: 0.902453, Accuracy: 86.79%\n",
      "Batch 12, Loss: 0.840195, Accuracy: 87.11%\n",
      "Batch 13, Loss: 0.863556, Accuracy: 87.02%\n",
      "Batch 14, Loss: 0.876291, Accuracy: 86.94%\n",
      "Batch 15, Loss: 0.825014, Accuracy: 87.29%\n",
      "Batch 16, Loss: 0.897954, Accuracy: 87.21%\n",
      "Batch 17, Loss: 0.977332, Accuracy: 86.49%\n",
      "Batch 18, Loss: 0.881578, Accuracy: 86.46%\n",
      "Batch 19, Loss: 0.836475, Accuracy: 86.76%\n",
      "Batch 20, Loss: 0.883932, Accuracy: 86.80%\n",
      "Batch 21, Loss: 0.851125, Accuracy: 86.98%\n",
      "Batch 22, Loss: 0.872405, Accuracy: 87.07%\n",
      "Batch 23, Loss: 0.848090, Accuracy: 87.23%\n",
      "Batch 24, Loss: 0.867853, Accuracy: 87.24%\n",
      "Batch 25, Loss: 0.898971, Accuracy: 87.12%\n",
      "Batch 26, Loss: 0.883749, Accuracy: 87.08%\n",
      "Batch 27, Loss: 0.896045, Accuracy: 86.92%\n",
      "Batch 28, Loss: 0.844737, Accuracy: 87.11%\n",
      "Batch 29, Loss: 0.891910, Accuracy: 87.12%\n",
      "Batch 30, Loss: 0.815774, Accuracy: 87.29%\n",
      "Batch 31, Loss: 0.872118, Accuracy: 87.30%\n",
      "Batch 32, Loss: 0.920010, Accuracy: 87.16%\n",
      "Batch 33, Loss: 0.872997, Accuracy: 87.12%\n",
      "Batch 34, Loss: 0.796607, Accuracy: 87.36%\n",
      "Batch 35, Loss: 0.835416, Accuracy: 87.46%\n",
      "Batch 36, Loss: 0.866920, Accuracy: 87.46%\n",
      "Batch 37, Loss: 0.847749, Accuracy: 87.54%\n",
      "Batch 38, Loss: 0.890322, Accuracy: 87.50%\n",
      "Batch 39, Loss: 0.890334, Accuracy: 87.38%\n",
      "Batch 40, Loss: 0.798186, Accuracy: 87.62%\n",
      "Batch 41, Loss: 0.834369, Accuracy: 87.73%\n",
      "Batch 42, Loss: 0.840930, Accuracy: 87.80%\n",
      "Batch 43, Loss: 0.812754, Accuracy: 87.94%\n",
      "Batch 44, Loss: 0.882859, Accuracy: 87.89%\n",
      "Batch 45, Loss: 0.830353, Accuracy: 87.95%\n",
      "Batch 46, Loss: 0.940964, Accuracy: 87.74%\n",
      "Batch 47, Loss: 0.852030, Accuracy: 87.80%\n",
      "Batch 48, Loss: 0.887066, Accuracy: 87.76%\n",
      "Batch 49, Loss: 0.865159, Accuracy: 87.72%\n",
      "Batch 50, Loss: 0.882161, Accuracy: 87.69%\n",
      "Batch 51, Loss: 0.848358, Accuracy: 87.71%\n",
      "Batch 52, Loss: 0.879488, Accuracy: 87.74%\n",
      "Batch 53, Loss: 0.884036, Accuracy: 87.71%\n",
      "Batch 54, Loss: 0.865583, Accuracy: 87.73%\n",
      "Batch 55, Loss: 0.900668, Accuracy: 87.67%\n",
      "Batch 56, Loss: 0.906091, Accuracy: 87.58%\n",
      "Batch 57, Loss: 0.827627, Accuracy: 87.66%\n",
      "Batch 58, Loss: 0.820321, Accuracy: 87.74%\n",
      "Batch 59, Loss: 0.881053, Accuracy: 87.74%\n",
      "Batch 60, Loss: 0.871522, Accuracy: 87.76%\n",
      "Batch 61, Loss: 0.873093, Accuracy: 87.76%\n",
      "Batch 62, Loss: 0.899736, Accuracy: 87.70%\n",
      "Batch 63, Loss: 0.859404, Accuracy: 87.72%\n",
      "Batch 64, Loss: 0.858503, Accuracy: 87.74%\n",
      "Batch 65, Loss: 0.831390, Accuracy: 87.81%\n",
      "Batch 66, Loss: 0.890809, Accuracy: 87.78%\n",
      "Batch 67, Loss: 0.844571, Accuracy: 87.80%\n",
      "Batch 68, Loss: 0.930448, Accuracy: 87.73%\n",
      "Batch 69, Loss: 0.849495, Accuracy: 87.73%\n",
      "Batch 70, Loss: 0.943125, Accuracy: 87.59%\n",
      "Batch 71, Loss: 0.842943, Accuracy: 87.63%\n",
      "Batch 72, Loss: 0.896040, Accuracy: 87.61%\n",
      "Batch 73, Loss: 0.878141, Accuracy: 87.61%\n",
      "Batch 74, Loss: 0.921657, Accuracy: 87.54%\n",
      "Batch 75, Loss: 0.847337, Accuracy: 87.58%\n",
      "Batch 76, Loss: 0.842648, Accuracy: 87.60%\n",
      "Batch 77, Loss: 0.930310, Accuracy: 87.52%\n",
      "Batch 78, Loss: 0.817793, Accuracy: 87.58%\n",
      "Batch 79, Loss: 0.865744, Accuracy: 87.58%\n",
      "Batch 80, Loss: 0.891150, Accuracy: 87.56%\n",
      "Batch 81, Loss: 0.910703, Accuracy: 87.50%\n",
      "Batch 82, Loss: 0.870474, Accuracy: 87.50%\n",
      "Batch 83, Loss: 0.852622, Accuracy: 87.54%\n",
      "Batch 84, Loss: 0.858227, Accuracy: 87.56%\n",
      "Batch 85, Loss: 0.903174, Accuracy: 87.50%\n",
      "Batch 86, Loss: 0.846267, Accuracy: 87.50%\n",
      "Batch 87, Loss: 0.901725, Accuracy: 87.46%\n",
      "Batch 88, Loss: 0.899132, Accuracy: 87.41%\n",
      "Batch 89, Loss: 0.872870, Accuracy: 87.41%\n",
      "Batch 90, Loss: 0.937930, Accuracy: 87.33%\n",
      "Batch 91, Loss: 0.935236, Accuracy: 87.26%\n",
      "Batch 92, Loss: 0.855015, Accuracy: 87.28%\n",
      "Batch 93, Loss: 0.872324, Accuracy: 87.26%\n",
      "Batch 94, Loss: 0.982085, Accuracy: 87.13%\n",
      "Batch 95, Loss: 0.854641, Accuracy: 87.15%\n",
      "Batch 96, Loss: 0.895445, Accuracy: 87.14%\n",
      "Batch 97, Loss: 0.907248, Accuracy: 87.10%\n",
      "Batch 98, Loss: 0.914822, Accuracy: 87.02%\n",
      "Batch 99, Loss: 0.895238, Accuracy: 86.98%\n",
      "Batch 100, Loss: 0.891686, Accuracy: 86.95%\n",
      "Batch 101, Loss: 0.901234, Accuracy: 86.91%\n",
      "Batch 102, Loss: 0.846940, Accuracy: 86.93%\n",
      "Batch 103, Loss: 0.818712, Accuracy: 87.00%\n",
      "Batch 104, Loss: 0.893527, Accuracy: 86.97%\n",
      "Batch 105, Loss: 0.901241, Accuracy: 86.95%\n",
      "Batch 106, Loss: 0.859053, Accuracy: 86.97%\n",
      "Batch 107, Loss: 0.887791, Accuracy: 86.96%\n",
      "Batch 108, Loss: 0.831397, Accuracy: 86.99%\n",
      "Batch 109, Loss: 0.887988, Accuracy: 86.97%\n",
      "Batch 110, Loss: 0.891198, Accuracy: 86.96%\n",
      "Batch 111, Loss: 0.836731, Accuracy: 87.02%\n",
      "Batch 112, Loss: 0.840444, Accuracy: 87.05%\n",
      "Batch 113, Loss: 0.803991, Accuracy: 87.13%\n",
      "Batch 114, Loss: 0.927345, Accuracy: 87.06%\n",
      "Batch 115, Loss: 0.892562, Accuracy: 87.05%\n",
      "Batch 116, Loss: 0.791171, Accuracy: 87.14%\n",
      "Batch 117, Loss: 0.831838, Accuracy: 87.17%\n",
      "Batch 118, Loss: 0.845919, Accuracy: 87.18%\n",
      "Batch 119, Loss: 0.896555, Accuracy: 87.16%\n",
      "Batch 120, Loss: 0.900827, Accuracy: 87.14%\n",
      "Batch 121, Loss: 0.827807, Accuracy: 87.18%\n",
      "Batch 122, Loss: 0.917717, Accuracy: 87.14%\n",
      "Batch 123, Loss: 0.855492, Accuracy: 87.16%\n",
      "Batch 124, Loss: 0.898446, Accuracy: 87.12%\n",
      "Batch 125, Loss: 0.873974, Accuracy: 87.12%\n",
      "Batch 126, Loss: 0.810867, Accuracy: 87.18%\n",
      "Batch 127, Loss: 0.856305, Accuracy: 87.19%\n",
      "Batch 128, Loss: 0.919281, Accuracy: 87.16%\n",
      "Batch 129, Loss: 0.867863, Accuracy: 87.16%\n",
      "Batch 130, Loss: 0.832752, Accuracy: 87.20%\n",
      "Batch 131, Loss: 0.846905, Accuracy: 87.23%\n",
      "Batch 132, Loss: 0.924792, Accuracy: 87.17%\n",
      "Batch 133, Loss: 0.884846, Accuracy: 87.16%\n",
      "Batch 134, Loss: 0.849859, Accuracy: 87.17%\n",
      "Batch 135, Loss: 0.864653, Accuracy: 87.19%\n",
      "Batch 136, Loss: 0.878352, Accuracy: 87.18%\n",
      "Batch 137, Loss: 0.923510, Accuracy: 87.14%\n",
      "Batch 138, Loss: 0.890497, Accuracy: 87.13%\n",
      "Batch 139, Loss: 0.934785, Accuracy: 87.07%\n",
      "Batch 140, Loss: 0.877732, Accuracy: 87.08%\n",
      "Batch 141, Loss: 0.869732, Accuracy: 87.09%\n",
      "Batch 142, Loss: 0.907876, Accuracy: 87.07%\n",
      "Batch 143, Loss: 0.884489, Accuracy: 87.06%\n",
      "Batch 144, Loss: 0.831703, Accuracy: 87.09%\n",
      "Batch 145, Loss: 0.924931, Accuracy: 87.05%\n",
      "Batch 146, Loss: 0.797976, Accuracy: 87.10%\n",
      "Batch 147, Loss: 0.829166, Accuracy: 87.14%\n",
      "Batch 148, Loss: 0.851361, Accuracy: 87.15%\n",
      "Batch 149, Loss: 0.872197, Accuracy: 87.15%\n",
      "Batch 150, Loss: 0.880154, Accuracy: 87.15%\n",
      "Batch 151, Loss: 0.921465, Accuracy: 87.12%\n",
      "Batch 152, Loss: 0.845995, Accuracy: 87.13%\n",
      "Batch 153, Loss: 0.879118, Accuracy: 87.13%\n",
      "Batch 154, Loss: 0.878415, Accuracy: 87.13%\n",
      "Batch 155, Loss: 0.963471, Accuracy: 87.08%\n",
      "Batch 156, Loss: 0.902290, Accuracy: 87.06%\n",
      "Batch 157, Loss: 0.848050, Accuracy: 87.08%\n",
      "Batch 158, Loss: 0.796490, Accuracy: 87.13%\n",
      "Batch 159, Loss: 0.851851, Accuracy: 87.15%\n",
      "Batch 160, Loss: 0.886627, Accuracy: 87.14%\n",
      "Batch 161, Loss: 0.873755, Accuracy: 87.15%\n",
      "Batch 162, Loss: 0.871235, Accuracy: 87.14%\n",
      "Batch 163, Loss: 0.920979, Accuracy: 87.11%\n",
      "Batch 164, Loss: 0.836743, Accuracy: 87.13%\n",
      "Batch 165, Loss: 0.907036, Accuracy: 87.11%\n",
      "Batch 166, Loss: 0.907072, Accuracy: 87.10%\n",
      "Batch 167, Loss: 0.859994, Accuracy: 87.10%\n",
      "Batch 168, Loss: 0.869125, Accuracy: 87.10%\n",
      "Batch 169, Loss: 0.847592, Accuracy: 87.10%\n",
      "Batch 170, Loss: 0.899245, Accuracy: 87.10%\n",
      "Batch 171, Loss: 0.851919, Accuracy: 87.11%\n",
      "Batch 172, Loss: 0.889423, Accuracy: 87.10%\n",
      "Batch 173, Loss: 0.870061, Accuracy: 87.11%\n",
      "Batch 174, Loss: 0.890147, Accuracy: 87.10%\n",
      "Batch 175, Loss: 0.847930, Accuracy: 87.12%\n",
      "Batch 176, Loss: 0.829270, Accuracy: 87.14%\n",
      "Batch 177, Loss: 0.860716, Accuracy: 87.16%\n",
      "Batch 178, Loss: 0.950388, Accuracy: 87.11%\n",
      "Batch 179, Loss: 0.888908, Accuracy: 87.11%\n",
      "Batch 180, Loss: 0.881654, Accuracy: 87.09%\n",
      "Batch 181, Loss: 0.842455, Accuracy: 87.11%\n",
      "Batch 182, Loss: 0.866468, Accuracy: 87.11%\n",
      "Batch 183, Loss: 0.833560, Accuracy: 87.13%\n",
      "Batch 184, Loss: 0.940140, Accuracy: 87.09%\n",
      "Batch 185, Loss: 0.874308, Accuracy: 87.08%\n",
      "Batch 186, Loss: 0.827732, Accuracy: 87.11%\n",
      "Batch 187, Loss: 0.842013, Accuracy: 87.12%\n",
      "Batch 188, Loss: 0.850460, Accuracy: 87.13%\n",
      "Batch 189, Loss: 0.853004, Accuracy: 87.14%\n",
      "Batch 190, Loss: 0.899338, Accuracy: 87.12%\n",
      "Batch 191, Loss: 0.946264, Accuracy: 87.08%\n",
      "Batch 192, Loss: 0.875027, Accuracy: 87.08%\n",
      "Batch 193, Loss: 0.822032, Accuracy: 87.10%\n",
      "Batch 194, Loss: 0.878361, Accuracy: 87.10%\n",
      "Batch 195, Loss: 0.960773, Accuracy: 87.05%\n",
      "Batch 196, Loss: 0.931625, Accuracy: 87.02%\n",
      "Batch 197, Loss: 0.852453, Accuracy: 87.03%\n",
      "Batch 198, Loss: 0.831679, Accuracy: 87.06%\n",
      "Batch 199, Loss: 0.971189, Accuracy: 87.01%\n",
      "Batch 200, Loss: 0.909280, Accuracy: 87.00%\n",
      "Batch 201, Loss: 0.897901, Accuracy: 86.99%\n",
      "Batch 202, Loss: 0.876063, Accuracy: 87.00%\n",
      "Batch 203, Loss: 0.878916, Accuracy: 86.99%\n",
      "Batch 204, Loss: 0.920140, Accuracy: 86.97%\n",
      "Batch 205, Loss: 0.799983, Accuracy: 87.01%\n",
      "Batch 206, Loss: 0.849693, Accuracy: 87.03%\n",
      "Batch 207, Loss: 0.897501, Accuracy: 87.02%\n",
      "Batch 208, Loss: 0.857550, Accuracy: 87.04%\n",
      "Batch 209, Loss: 0.870251, Accuracy: 87.04%\n",
      "Batch 210, Loss: 0.868105, Accuracy: 87.05%\n",
      "Batch 211, Loss: 0.896001, Accuracy: 87.03%\n",
      "Batch 212, Loss: 0.838872, Accuracy: 87.06%\n",
      "Batch 213, Loss: 0.831975, Accuracy: 87.08%\n",
      "Training - Epoch 88, Loss: 0.874218, Accuracy: 87.08%\n",
      "Validation Batch 1, Loss: 0.841889, Accuracy: 90.62%\n",
      "Validation Batch 2, Loss: 0.819573, Accuracy: 92.19%\n",
      "Validation Batch 3, Loss: 0.896812, Accuracy: 89.58%\n",
      "Validation Batch 4, Loss: 0.861205, Accuracy: 89.45%\n",
      "Validation Batch 5, Loss: 0.848561, Accuracy: 89.38%\n",
      "Validation Batch 6, Loss: 0.819215, Accuracy: 90.10%\n",
      "Validation Batch 7, Loss: 0.823752, Accuracy: 90.62%\n",
      "Validation Batch 8, Loss: 0.917326, Accuracy: 89.65%\n",
      "Validation Batch 9, Loss: 0.905011, Accuracy: 89.06%\n",
      "Validation Batch 10, Loss: 0.849798, Accuracy: 89.22%\n",
      "Validation Batch 11, Loss: 0.852516, Accuracy: 89.20%\n",
      "Validation Batch 12, Loss: 0.848733, Accuracy: 89.45%\n",
      "Validation Batch 13, Loss: 0.868598, Accuracy: 89.30%\n",
      "Validation Batch 14, Loss: 0.878529, Accuracy: 88.95%\n",
      "Validation Batch 15, Loss: 0.837977, Accuracy: 89.17%\n",
      "Validation Batch 16, Loss: 0.855547, Accuracy: 89.26%\n",
      "Validation Batch 17, Loss: 0.902172, Accuracy: 88.97%\n",
      "Validation Batch 18, Loss: 0.836641, Accuracy: 88.98%\n",
      "Validation Batch 19, Loss: 0.906241, Accuracy: 88.73%\n",
      "Validation Batch 20, Loss: 0.917761, Accuracy: 88.44%\n",
      "Validation Batch 21, Loss: 0.883677, Accuracy: 88.39%\n",
      "Validation Batch 22, Loss: 0.870364, Accuracy: 88.28%\n",
      "Validation Batch 23, Loss: 0.896801, Accuracy: 88.18%\n",
      "Validation Batch 24, Loss: 0.890454, Accuracy: 88.09%\n",
      "Validation Batch 25, Loss: 0.840287, Accuracy: 88.19%\n",
      "Validation Batch 26, Loss: 0.875228, Accuracy: 88.10%\n",
      "Validation Batch 27, Loss: 0.827437, Accuracy: 88.26%\n",
      "Validation - Epoch 88, Loss: 0.865633, Accuracy: 88.26%\n",
      "Patience—6\n",
      "Epoch 89\n",
      "Batch 1, Loss: 0.847852, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.850341, Accuracy: 89.06%\n",
      "Batch 3, Loss: 0.870336, Accuracy: 89.06%\n",
      "Batch 4, Loss: 0.851415, Accuracy: 89.45%\n",
      "Batch 5, Loss: 0.922495, Accuracy: 88.12%\n",
      "Batch 6, Loss: 0.862459, Accuracy: 88.02%\n",
      "Batch 7, Loss: 0.817343, Accuracy: 89.06%\n",
      "Batch 8, Loss: 0.827736, Accuracy: 89.45%\n",
      "Batch 9, Loss: 0.867725, Accuracy: 89.24%\n",
      "Batch 10, Loss: 0.924906, Accuracy: 88.59%\n",
      "Batch 11, Loss: 0.871808, Accuracy: 88.64%\n",
      "Batch 12, Loss: 0.828301, Accuracy: 88.80%\n",
      "Batch 13, Loss: 0.843504, Accuracy: 88.94%\n",
      "Batch 14, Loss: 0.850098, Accuracy: 88.95%\n",
      "Batch 15, Loss: 0.845461, Accuracy: 89.06%\n",
      "Batch 16, Loss: 0.865345, Accuracy: 88.96%\n",
      "Batch 17, Loss: 0.950463, Accuracy: 88.33%\n",
      "Batch 18, Loss: 0.808775, Accuracy: 88.72%\n",
      "Batch 19, Loss: 0.921168, Accuracy: 88.40%\n",
      "Batch 20, Loss: 0.907122, Accuracy: 88.20%\n",
      "Batch 21, Loss: 0.902522, Accuracy: 87.95%\n",
      "Batch 22, Loss: 0.839316, Accuracy: 88.07%\n",
      "Batch 23, Loss: 0.860536, Accuracy: 88.04%\n",
      "Batch 24, Loss: 0.927514, Accuracy: 87.76%\n",
      "Batch 25, Loss: 0.948920, Accuracy: 87.38%\n",
      "Batch 26, Loss: 0.838941, Accuracy: 87.50%\n",
      "Batch 27, Loss: 0.913097, Accuracy: 87.33%\n",
      "Batch 28, Loss: 0.896254, Accuracy: 87.28%\n",
      "Batch 29, Loss: 0.912872, Accuracy: 87.12%\n",
      "Batch 30, Loss: 0.873167, Accuracy: 87.14%\n",
      "Batch 31, Loss: 0.871888, Accuracy: 87.15%\n",
      "Batch 32, Loss: 0.832857, Accuracy: 87.30%\n",
      "Batch 33, Loss: 0.923272, Accuracy: 87.17%\n",
      "Batch 34, Loss: 0.888599, Accuracy: 87.13%\n",
      "Batch 35, Loss: 0.915400, Accuracy: 87.05%\n",
      "Batch 36, Loss: 0.960289, Accuracy: 86.81%\n",
      "Batch 37, Loss: 0.902261, Accuracy: 86.74%\n",
      "Batch 38, Loss: 0.867417, Accuracy: 86.80%\n",
      "Batch 39, Loss: 0.921519, Accuracy: 86.66%\n",
      "Batch 40, Loss: 0.886048, Accuracy: 86.64%\n",
      "Batch 41, Loss: 0.885016, Accuracy: 86.62%\n",
      "Batch 42, Loss: 0.886720, Accuracy: 86.61%\n",
      "Batch 43, Loss: 0.869592, Accuracy: 86.66%\n",
      "Batch 44, Loss: 0.852646, Accuracy: 86.72%\n",
      "Batch 45, Loss: 0.896422, Accuracy: 86.67%\n",
      "Batch 46, Loss: 0.864259, Accuracy: 86.65%\n",
      "Batch 47, Loss: 0.808940, Accuracy: 86.80%\n",
      "Batch 48, Loss: 0.836236, Accuracy: 86.91%\n",
      "Batch 49, Loss: 0.836725, Accuracy: 86.99%\n",
      "Batch 50, Loss: 0.868320, Accuracy: 87.03%\n",
      "Batch 51, Loss: 0.799702, Accuracy: 87.16%\n",
      "Batch 52, Loss: 0.832919, Accuracy: 87.26%\n",
      "Batch 53, Loss: 0.862154, Accuracy: 87.26%\n",
      "Batch 54, Loss: 0.856027, Accuracy: 87.30%\n",
      "Batch 55, Loss: 0.874970, Accuracy: 87.33%\n",
      "Batch 56, Loss: 0.883346, Accuracy: 87.30%\n",
      "Batch 57, Loss: 0.873535, Accuracy: 87.28%\n",
      "Batch 58, Loss: 0.886569, Accuracy: 87.23%\n",
      "Batch 59, Loss: 0.923851, Accuracy: 87.16%\n",
      "Batch 60, Loss: 0.897347, Accuracy: 87.08%\n",
      "Batch 61, Loss: 0.785524, Accuracy: 87.24%\n",
      "Batch 62, Loss: 0.943483, Accuracy: 87.12%\n",
      "Batch 63, Loss: 0.895450, Accuracy: 87.05%\n",
      "Batch 64, Loss: 0.863328, Accuracy: 87.06%\n",
      "Batch 65, Loss: 0.820363, Accuracy: 87.16%\n",
      "Batch 66, Loss: 0.946488, Accuracy: 87.00%\n",
      "Batch 67, Loss: 1.000891, Accuracy: 86.78%\n",
      "Batch 68, Loss: 0.900830, Accuracy: 86.74%\n",
      "Batch 69, Loss: 0.836537, Accuracy: 86.80%\n",
      "Batch 70, Loss: 0.850335, Accuracy: 86.85%\n",
      "Batch 71, Loss: 0.891495, Accuracy: 86.84%\n",
      "Batch 72, Loss: 0.911920, Accuracy: 86.78%\n",
      "Batch 73, Loss: 0.877893, Accuracy: 86.77%\n",
      "Batch 74, Loss: 0.897406, Accuracy: 86.76%\n",
      "Batch 75, Loss: 0.888388, Accuracy: 86.73%\n",
      "Batch 76, Loss: 0.819978, Accuracy: 86.80%\n",
      "Batch 77, Loss: 0.830045, Accuracy: 86.89%\n",
      "Batch 78, Loss: 0.871851, Accuracy: 86.90%\n",
      "Batch 79, Loss: 0.877400, Accuracy: 86.91%\n",
      "Batch 80, Loss: 0.816326, Accuracy: 86.97%\n",
      "Batch 81, Loss: 0.841582, Accuracy: 87.02%\n",
      "Batch 82, Loss: 0.869313, Accuracy: 87.02%\n",
      "Batch 83, Loss: 0.878690, Accuracy: 87.03%\n",
      "Batch 84, Loss: 0.925766, Accuracy: 86.94%\n",
      "Batch 85, Loss: 0.848642, Accuracy: 86.99%\n",
      "Batch 86, Loss: 0.842580, Accuracy: 87.03%\n",
      "Batch 87, Loss: 0.934508, Accuracy: 86.94%\n",
      "Batch 88, Loss: 0.868598, Accuracy: 86.93%\n",
      "Batch 89, Loss: 0.846667, Accuracy: 86.96%\n",
      "Batch 90, Loss: 0.892785, Accuracy: 86.93%\n",
      "Batch 91, Loss: 0.837453, Accuracy: 87.00%\n",
      "Batch 92, Loss: 0.919096, Accuracy: 86.96%\n",
      "Batch 93, Loss: 0.862430, Accuracy: 86.98%\n",
      "Batch 94, Loss: 0.883055, Accuracy: 86.97%\n",
      "Batch 95, Loss: 0.884747, Accuracy: 86.94%\n",
      "Batch 96, Loss: 0.871543, Accuracy: 86.95%\n",
      "Batch 97, Loss: 0.898043, Accuracy: 86.90%\n",
      "Batch 98, Loss: 0.929969, Accuracy: 86.85%\n",
      "Batch 99, Loss: 0.800443, Accuracy: 86.93%\n",
      "Batch 100, Loss: 0.869110, Accuracy: 86.94%\n",
      "Batch 101, Loss: 0.848697, Accuracy: 86.97%\n",
      "Batch 102, Loss: 0.904298, Accuracy: 86.95%\n",
      "Batch 103, Loss: 0.896561, Accuracy: 86.94%\n",
      "Batch 104, Loss: 0.803881, Accuracy: 87.02%\n",
      "Batch 105, Loss: 0.840133, Accuracy: 87.05%\n",
      "Batch 106, Loss: 0.961465, Accuracy: 86.95%\n",
      "Batch 107, Loss: 0.879333, Accuracy: 86.95%\n",
      "Batch 108, Loss: 0.901386, Accuracy: 86.91%\n",
      "Batch 109, Loss: 0.921855, Accuracy: 86.85%\n",
      "Batch 110, Loss: 0.873690, Accuracy: 86.86%\n",
      "Batch 111, Loss: 0.853653, Accuracy: 86.87%\n",
      "Batch 112, Loss: 0.959566, Accuracy: 86.80%\n",
      "Batch 113, Loss: 0.873846, Accuracy: 86.81%\n",
      "Batch 114, Loss: 0.890091, Accuracy: 86.80%\n",
      "Batch 115, Loss: 0.895540, Accuracy: 86.78%\n",
      "Batch 116, Loss: 0.903785, Accuracy: 86.76%\n",
      "Batch 117, Loss: 0.909267, Accuracy: 86.73%\n",
      "Batch 118, Loss: 0.846604, Accuracy: 86.75%\n",
      "Batch 119, Loss: 0.857583, Accuracy: 86.76%\n",
      "Batch 120, Loss: 0.852950, Accuracy: 86.77%\n",
      "Batch 121, Loss: 0.890323, Accuracy: 86.76%\n",
      "Batch 122, Loss: 0.925259, Accuracy: 86.72%\n",
      "Batch 123, Loss: 0.904558, Accuracy: 86.70%\n",
      "Batch 124, Loss: 0.866072, Accuracy: 86.68%\n",
      "Batch 125, Loss: 0.853407, Accuracy: 86.70%\n",
      "Batch 126, Loss: 0.921739, Accuracy: 86.66%\n",
      "Batch 127, Loss: 0.807695, Accuracy: 86.71%\n",
      "Batch 128, Loss: 0.911177, Accuracy: 86.68%\n",
      "Batch 129, Loss: 0.841643, Accuracy: 86.71%\n",
      "Batch 130, Loss: 0.866320, Accuracy: 86.73%\n",
      "Batch 131, Loss: 0.828352, Accuracy: 86.77%\n",
      "Batch 132, Loss: 0.861400, Accuracy: 86.79%\n",
      "Batch 133, Loss: 0.865236, Accuracy: 86.81%\n",
      "Batch 134, Loss: 0.862663, Accuracy: 86.82%\n",
      "Batch 135, Loss: 0.924135, Accuracy: 86.78%\n",
      "Batch 136, Loss: 0.851306, Accuracy: 86.81%\n",
      "Batch 137, Loss: 0.836072, Accuracy: 86.84%\n",
      "Batch 138, Loss: 0.810896, Accuracy: 86.89%\n",
      "Batch 139, Loss: 0.890010, Accuracy: 86.88%\n",
      "Batch 140, Loss: 0.851206, Accuracy: 86.91%\n",
      "Batch 141, Loss: 0.840346, Accuracy: 86.93%\n",
      "Batch 142, Loss: 0.879173, Accuracy: 86.92%\n",
      "Batch 143, Loss: 0.807389, Accuracy: 86.96%\n",
      "Batch 144, Loss: 0.815695, Accuracy: 87.00%\n",
      "Batch 145, Loss: 0.872326, Accuracy: 87.00%\n",
      "Batch 146, Loss: 0.832104, Accuracy: 87.03%\n",
      "Batch 147, Loss: 0.925824, Accuracy: 87.00%\n",
      "Batch 148, Loss: 0.852239, Accuracy: 87.01%\n",
      "Batch 149, Loss: 0.851139, Accuracy: 87.03%\n",
      "Batch 150, Loss: 0.940202, Accuracy: 86.98%\n",
      "Batch 151, Loss: 0.912318, Accuracy: 86.95%\n",
      "Batch 152, Loss: 0.851377, Accuracy: 86.97%\n",
      "Batch 153, Loss: 0.851421, Accuracy: 86.97%\n",
      "Batch 154, Loss: 0.895057, Accuracy: 86.96%\n",
      "Batch 155, Loss: 0.862072, Accuracy: 86.96%\n",
      "Batch 156, Loss: 0.843712, Accuracy: 86.98%\n",
      "Batch 157, Loss: 0.887324, Accuracy: 86.97%\n",
      "Batch 158, Loss: 0.887299, Accuracy: 86.95%\n",
      "Batch 159, Loss: 0.919277, Accuracy: 86.92%\n",
      "Batch 160, Loss: 0.849827, Accuracy: 86.94%\n",
      "Batch 161, Loss: 0.924974, Accuracy: 86.91%\n",
      "Batch 162, Loss: 0.906013, Accuracy: 86.89%\n",
      "Batch 163, Loss: 0.957155, Accuracy: 86.83%\n",
      "Batch 164, Loss: 0.873385, Accuracy: 86.83%\n",
      "Batch 165, Loss: 0.894383, Accuracy: 86.81%\n",
      "Batch 166, Loss: 0.927283, Accuracy: 86.78%\n",
      "Batch 167, Loss: 0.837010, Accuracy: 86.80%\n",
      "Batch 168, Loss: 0.843275, Accuracy: 86.82%\n",
      "Batch 169, Loss: 0.852953, Accuracy: 86.84%\n",
      "Batch 170, Loss: 0.823728, Accuracy: 86.87%\n",
      "Batch 171, Loss: 0.867699, Accuracy: 86.88%\n",
      "Batch 172, Loss: 0.945933, Accuracy: 86.84%\n",
      "Batch 173, Loss: 0.868654, Accuracy: 86.84%\n",
      "Batch 174, Loss: 0.934247, Accuracy: 86.80%\n",
      "Batch 175, Loss: 0.814238, Accuracy: 86.84%\n",
      "Batch 176, Loss: 0.816301, Accuracy: 86.87%\n",
      "Batch 177, Loss: 0.875565, Accuracy: 86.86%\n",
      "Batch 178, Loss: 0.863088, Accuracy: 86.87%\n",
      "Batch 179, Loss: 0.834234, Accuracy: 86.90%\n",
      "Batch 180, Loss: 0.889350, Accuracy: 86.89%\n",
      "Batch 181, Loss: 0.855342, Accuracy: 86.90%\n",
      "Batch 182, Loss: 0.887895, Accuracy: 86.89%\n",
      "Batch 183, Loss: 0.879987, Accuracy: 86.90%\n",
      "Batch 184, Loss: 0.849978, Accuracy: 86.91%\n",
      "Batch 185, Loss: 0.872705, Accuracy: 86.91%\n",
      "Batch 186, Loss: 0.890003, Accuracy: 86.90%\n",
      "Batch 187, Loss: 0.854349, Accuracy: 86.92%\n",
      "Batch 188, Loss: 0.897521, Accuracy: 86.91%\n",
      "Batch 189, Loss: 0.842097, Accuracy: 86.94%\n",
      "Batch 190, Loss: 0.914343, Accuracy: 86.91%\n",
      "Batch 191, Loss: 0.849825, Accuracy: 86.93%\n",
      "Batch 192, Loss: 0.896874, Accuracy: 86.91%\n",
      "Batch 193, Loss: 0.891881, Accuracy: 86.91%\n",
      "Batch 194, Loss: 0.882927, Accuracy: 86.90%\n",
      "Batch 195, Loss: 0.823789, Accuracy: 86.92%\n",
      "Batch 196, Loss: 0.894480, Accuracy: 86.89%\n",
      "Batch 197, Loss: 0.900796, Accuracy: 86.88%\n",
      "Batch 198, Loss: 0.861590, Accuracy: 86.88%\n",
      "Batch 199, Loss: 0.901160, Accuracy: 86.87%\n",
      "Batch 200, Loss: 0.907550, Accuracy: 86.86%\n",
      "Batch 201, Loss: 0.857862, Accuracy: 86.86%\n",
      "Batch 202, Loss: 0.870658, Accuracy: 86.87%\n",
      "Batch 203, Loss: 0.828867, Accuracy: 86.89%\n",
      "Batch 204, Loss: 0.834504, Accuracy: 86.91%\n",
      "Batch 205, Loss: 0.903030, Accuracy: 86.89%\n",
      "Batch 206, Loss: 0.878672, Accuracy: 86.89%\n",
      "Batch 207, Loss: 0.873218, Accuracy: 86.90%\n",
      "Batch 208, Loss: 0.845946, Accuracy: 86.91%\n",
      "Batch 209, Loss: 0.886955, Accuracy: 86.91%\n",
      "Batch 210, Loss: 0.846176, Accuracy: 86.92%\n",
      "Batch 211, Loss: 0.880578, Accuracy: 86.91%\n",
      "Batch 212, Loss: 0.787637, Accuracy: 86.95%\n",
      "Batch 213, Loss: 0.846327, Accuracy: 86.96%\n",
      "Training - Epoch 89, Loss: 0.874218, Accuracy: 86.96%\n",
      "Validation Batch 1, Loss: 0.822546, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.802089, Accuracy: 95.31%\n",
      "Validation Batch 3, Loss: 0.876638, Accuracy: 91.67%\n",
      "Validation Batch 4, Loss: 0.842483, Accuracy: 91.41%\n",
      "Validation Batch 5, Loss: 0.826926, Accuracy: 91.56%\n",
      "Validation Batch 6, Loss: 0.801382, Accuracy: 91.93%\n",
      "Validation Batch 7, Loss: 0.810155, Accuracy: 92.19%\n",
      "Validation Batch 8, Loss: 0.905307, Accuracy: 90.82%\n",
      "Validation Batch 9, Loss: 0.899977, Accuracy: 90.10%\n",
      "Validation Batch 10, Loss: 0.823215, Accuracy: 90.31%\n",
      "Validation Batch 11, Loss: 0.842062, Accuracy: 90.34%\n",
      "Validation Batch 12, Loss: 0.832926, Accuracy: 90.49%\n",
      "Validation Batch 13, Loss: 0.848502, Accuracy: 90.38%\n",
      "Validation Batch 14, Loss: 0.861173, Accuracy: 90.18%\n",
      "Validation Batch 15, Loss: 0.824637, Accuracy: 90.31%\n",
      "Validation Batch 16, Loss: 0.837926, Accuracy: 90.43%\n",
      "Validation Batch 17, Loss: 0.888207, Accuracy: 90.17%\n",
      "Validation Batch 18, Loss: 0.815579, Accuracy: 90.36%\n",
      "Validation Batch 19, Loss: 0.889330, Accuracy: 90.05%\n",
      "Validation Batch 20, Loss: 0.879922, Accuracy: 89.92%\n",
      "Validation Batch 21, Loss: 0.866162, Accuracy: 89.81%\n",
      "Validation Batch 22, Loss: 0.851169, Accuracy: 89.70%\n",
      "Validation Batch 23, Loss: 0.873083, Accuracy: 89.61%\n",
      "Validation Batch 24, Loss: 0.856351, Accuracy: 89.52%\n",
      "Validation Batch 25, Loss: 0.824558, Accuracy: 89.56%\n",
      "Validation Batch 26, Loss: 0.851399, Accuracy: 89.60%\n",
      "Validation Batch 27, Loss: 0.802482, Accuracy: 89.72%\n",
      "Validation - Epoch 89, Loss: 0.846525, Accuracy: 89.72%\n",
      "Patience—0\n",
      "Epoch 90\n",
      "Batch 1, Loss: 0.841257, Accuracy: 89.06%\n",
      "Batch 2, Loss: 0.911900, Accuracy: 85.16%\n",
      "Batch 3, Loss: 0.880256, Accuracy: 85.42%\n",
      "Batch 4, Loss: 0.918256, Accuracy: 85.16%\n",
      "Batch 5, Loss: 0.837006, Accuracy: 86.25%\n",
      "Batch 6, Loss: 0.879807, Accuracy: 85.94%\n",
      "Batch 7, Loss: 0.888275, Accuracy: 85.94%\n",
      "Batch 8, Loss: 0.882347, Accuracy: 85.94%\n",
      "Batch 9, Loss: 0.818087, Accuracy: 86.81%\n",
      "Batch 10, Loss: 0.807493, Accuracy: 87.34%\n",
      "Batch 11, Loss: 0.878940, Accuracy: 87.22%\n",
      "Batch 12, Loss: 0.862208, Accuracy: 87.24%\n",
      "Batch 13, Loss: 0.874075, Accuracy: 87.14%\n",
      "Batch 14, Loss: 0.920842, Accuracy: 86.83%\n",
      "Batch 15, Loss: 0.960004, Accuracy: 86.35%\n",
      "Batch 16, Loss: 0.843863, Accuracy: 86.62%\n",
      "Batch 17, Loss: 0.879079, Accuracy: 86.58%\n",
      "Batch 18, Loss: 0.831672, Accuracy: 86.89%\n",
      "Batch 19, Loss: 0.856108, Accuracy: 87.01%\n",
      "Batch 20, Loss: 0.858320, Accuracy: 87.03%\n",
      "Batch 21, Loss: 0.846366, Accuracy: 87.20%\n",
      "Batch 22, Loss: 0.933191, Accuracy: 86.93%\n",
      "Batch 23, Loss: 0.870220, Accuracy: 86.96%\n",
      "Batch 24, Loss: 0.902378, Accuracy: 86.85%\n",
      "Batch 25, Loss: 0.888574, Accuracy: 86.81%\n",
      "Batch 26, Loss: 0.895300, Accuracy: 86.72%\n",
      "Batch 27, Loss: 0.900659, Accuracy: 86.63%\n",
      "Batch 28, Loss: 0.800617, Accuracy: 86.94%\n",
      "Batch 29, Loss: 0.800856, Accuracy: 87.23%\n",
      "Batch 30, Loss: 0.789339, Accuracy: 87.55%\n",
      "Batch 31, Loss: 0.780367, Accuracy: 87.85%\n",
      "Batch 32, Loss: 0.816525, Accuracy: 87.99%\n",
      "Batch 33, Loss: 0.881096, Accuracy: 87.93%\n",
      "Batch 34, Loss: 0.834254, Accuracy: 88.01%\n",
      "Batch 35, Loss: 0.889971, Accuracy: 87.90%\n",
      "Batch 36, Loss: 0.852301, Accuracy: 87.93%\n",
      "Batch 37, Loss: 0.853375, Accuracy: 87.96%\n",
      "Batch 38, Loss: 0.868350, Accuracy: 87.91%\n",
      "Batch 39, Loss: 0.867840, Accuracy: 87.90%\n",
      "Batch 40, Loss: 0.833899, Accuracy: 87.97%\n",
      "Batch 41, Loss: 0.910726, Accuracy: 87.84%\n",
      "Batch 42, Loss: 0.876310, Accuracy: 87.80%\n",
      "Batch 43, Loss: 0.873902, Accuracy: 87.79%\n",
      "Batch 44, Loss: 0.907188, Accuracy: 87.71%\n",
      "Batch 45, Loss: 0.900608, Accuracy: 87.64%\n",
      "Batch 46, Loss: 0.860485, Accuracy: 87.64%\n",
      "Batch 47, Loss: 0.845716, Accuracy: 87.70%\n",
      "Batch 48, Loss: 0.812014, Accuracy: 87.83%\n",
      "Batch 49, Loss: 0.919551, Accuracy: 87.72%\n",
      "Batch 50, Loss: 0.881772, Accuracy: 87.69%\n",
      "Batch 51, Loss: 0.910462, Accuracy: 87.62%\n",
      "Batch 52, Loss: 0.915014, Accuracy: 87.56%\n",
      "Batch 53, Loss: 0.911737, Accuracy: 87.47%\n",
      "Batch 54, Loss: 0.876005, Accuracy: 87.44%\n",
      "Batch 55, Loss: 0.907512, Accuracy: 87.39%\n",
      "Batch 56, Loss: 0.838575, Accuracy: 87.44%\n",
      "Batch 57, Loss: 0.893762, Accuracy: 87.39%\n",
      "Batch 58, Loss: 0.881431, Accuracy: 87.37%\n",
      "Batch 59, Loss: 0.834596, Accuracy: 87.45%\n",
      "Batch 60, Loss: 0.811040, Accuracy: 87.55%\n",
      "Batch 61, Loss: 0.786825, Accuracy: 87.68%\n",
      "Batch 62, Loss: 0.940014, Accuracy: 87.58%\n",
      "Batch 63, Loss: 0.886830, Accuracy: 87.55%\n",
      "Batch 64, Loss: 0.853993, Accuracy: 87.57%\n",
      "Batch 65, Loss: 0.834586, Accuracy: 87.64%\n",
      "Batch 66, Loss: 0.937946, Accuracy: 87.55%\n",
      "Batch 67, Loss: 0.895593, Accuracy: 87.48%\n",
      "Batch 68, Loss: 0.883227, Accuracy: 87.45%\n",
      "Batch 69, Loss: 0.864974, Accuracy: 87.43%\n",
      "Batch 70, Loss: 0.900380, Accuracy: 87.39%\n",
      "Batch 71, Loss: 0.848571, Accuracy: 87.41%\n",
      "Batch 72, Loss: 0.882605, Accuracy: 87.37%\n",
      "Batch 73, Loss: 0.882879, Accuracy: 87.37%\n",
      "Batch 74, Loss: 0.926127, Accuracy: 87.31%\n",
      "Batch 75, Loss: 0.861523, Accuracy: 87.31%\n",
      "Batch 76, Loss: 0.840676, Accuracy: 87.36%\n",
      "Batch 77, Loss: 0.876465, Accuracy: 87.34%\n",
      "Batch 78, Loss: 0.861063, Accuracy: 87.36%\n",
      "Batch 79, Loss: 0.919716, Accuracy: 87.28%\n",
      "Batch 80, Loss: 0.823550, Accuracy: 87.34%\n",
      "Batch 81, Loss: 0.854934, Accuracy: 87.38%\n",
      "Batch 82, Loss: 0.820505, Accuracy: 87.46%\n",
      "Batch 83, Loss: 0.871239, Accuracy: 87.46%\n",
      "Batch 84, Loss: 0.900278, Accuracy: 87.44%\n",
      "Batch 85, Loss: 0.796384, Accuracy: 87.54%\n",
      "Batch 86, Loss: 0.902803, Accuracy: 87.50%\n",
      "Batch 87, Loss: 0.830196, Accuracy: 87.54%\n",
      "Batch 88, Loss: 0.899855, Accuracy: 87.50%\n",
      "Batch 89, Loss: 0.896334, Accuracy: 87.46%\n",
      "Batch 90, Loss: 0.801815, Accuracy: 87.55%\n",
      "Batch 91, Loss: 0.879368, Accuracy: 87.55%\n",
      "Batch 92, Loss: 0.883690, Accuracy: 87.53%\n",
      "Batch 93, Loss: 0.912183, Accuracy: 87.50%\n",
      "Batch 94, Loss: 0.816009, Accuracy: 87.55%\n",
      "Batch 95, Loss: 0.890578, Accuracy: 87.52%\n",
      "Batch 96, Loss: 0.823933, Accuracy: 87.57%\n",
      "Batch 97, Loss: 0.870326, Accuracy: 87.56%\n",
      "Batch 98, Loss: 0.895733, Accuracy: 87.55%\n",
      "Batch 99, Loss: 0.886323, Accuracy: 87.53%\n",
      "Batch 100, Loss: 0.872216, Accuracy: 87.53%\n",
      "Batch 101, Loss: 0.826014, Accuracy: 87.56%\n",
      "Batch 102, Loss: 0.857743, Accuracy: 87.58%\n",
      "Batch 103, Loss: 0.904658, Accuracy: 87.53%\n",
      "Batch 104, Loss: 0.860639, Accuracy: 87.55%\n",
      "Batch 105, Loss: 0.908286, Accuracy: 87.50%\n",
      "Batch 106, Loss: 0.876005, Accuracy: 87.50%\n",
      "Batch 107, Loss: 0.899279, Accuracy: 87.47%\n",
      "Batch 108, Loss: 0.855022, Accuracy: 87.49%\n",
      "Batch 109, Loss: 0.816737, Accuracy: 87.54%\n",
      "Batch 110, Loss: 0.959974, Accuracy: 87.44%\n",
      "Batch 111, Loss: 0.885350, Accuracy: 87.43%\n",
      "Batch 112, Loss: 0.882992, Accuracy: 87.40%\n",
      "Batch 113, Loss: 0.884340, Accuracy: 87.38%\n",
      "Batch 114, Loss: 0.848413, Accuracy: 87.39%\n",
      "Batch 115, Loss: 0.861404, Accuracy: 87.39%\n",
      "Batch 116, Loss: 0.870525, Accuracy: 87.38%\n",
      "Batch 117, Loss: 0.911342, Accuracy: 87.34%\n",
      "Batch 118, Loss: 0.836730, Accuracy: 87.38%\n",
      "Batch 119, Loss: 0.828637, Accuracy: 87.43%\n",
      "Batch 120, Loss: 0.899329, Accuracy: 87.41%\n",
      "Batch 121, Loss: 0.862284, Accuracy: 87.41%\n",
      "Batch 122, Loss: 0.900355, Accuracy: 87.40%\n",
      "Batch 123, Loss: 0.837784, Accuracy: 87.41%\n",
      "Batch 124, Loss: 0.887050, Accuracy: 87.39%\n",
      "Batch 125, Loss: 0.845744, Accuracy: 87.41%\n",
      "Batch 126, Loss: 0.920725, Accuracy: 87.35%\n",
      "Batch 127, Loss: 0.852269, Accuracy: 87.38%\n",
      "Batch 128, Loss: 0.865671, Accuracy: 87.38%\n",
      "Batch 129, Loss: 0.924488, Accuracy: 87.34%\n",
      "Batch 130, Loss: 0.883755, Accuracy: 87.34%\n",
      "Batch 131, Loss: 0.934590, Accuracy: 87.30%\n",
      "Batch 132, Loss: 0.893969, Accuracy: 87.29%\n",
      "Batch 133, Loss: 0.830959, Accuracy: 87.32%\n",
      "Batch 134, Loss: 0.844312, Accuracy: 87.34%\n",
      "Batch 135, Loss: 0.836938, Accuracy: 87.37%\n",
      "Batch 136, Loss: 0.866639, Accuracy: 87.39%\n",
      "Batch 137, Loss: 0.893729, Accuracy: 87.36%\n",
      "Batch 138, Loss: 0.901585, Accuracy: 87.33%\n",
      "Batch 139, Loss: 0.960818, Accuracy: 87.26%\n",
      "Batch 140, Loss: 0.904394, Accuracy: 87.23%\n",
      "Batch 141, Loss: 0.924499, Accuracy: 87.19%\n",
      "Batch 142, Loss: 0.839493, Accuracy: 87.21%\n",
      "Batch 143, Loss: 0.879424, Accuracy: 87.19%\n",
      "Batch 144, Loss: 0.845040, Accuracy: 87.21%\n",
      "Batch 145, Loss: 0.893707, Accuracy: 87.21%\n",
      "Batch 146, Loss: 0.892325, Accuracy: 87.19%\n",
      "Batch 147, Loss: 0.839148, Accuracy: 87.21%\n",
      "Batch 148, Loss: 0.918600, Accuracy: 87.17%\n",
      "Batch 149, Loss: 0.864719, Accuracy: 87.17%\n",
      "Batch 150, Loss: 0.776506, Accuracy: 87.25%\n",
      "Batch 151, Loss: 0.878708, Accuracy: 87.23%\n",
      "Batch 152, Loss: 0.819855, Accuracy: 87.26%\n",
      "Batch 153, Loss: 0.898172, Accuracy: 87.25%\n",
      "Batch 154, Loss: 0.890790, Accuracy: 87.25%\n",
      "Batch 155, Loss: 0.876217, Accuracy: 87.23%\n",
      "Batch 156, Loss: 0.944640, Accuracy: 87.18%\n",
      "Batch 157, Loss: 0.902891, Accuracy: 87.16%\n",
      "Batch 158, Loss: 0.861780, Accuracy: 87.17%\n",
      "Batch 159, Loss: 0.940900, Accuracy: 87.12%\n",
      "Batch 160, Loss: 0.836093, Accuracy: 87.14%\n",
      "Batch 161, Loss: 0.872639, Accuracy: 87.13%\n",
      "Batch 162, Loss: 0.825690, Accuracy: 87.16%\n",
      "Batch 163, Loss: 0.873747, Accuracy: 87.15%\n",
      "Batch 164, Loss: 0.881453, Accuracy: 87.14%\n",
      "Batch 165, Loss: 0.937955, Accuracy: 87.08%\n",
      "Batch 166, Loss: 0.872718, Accuracy: 87.08%\n",
      "Batch 167, Loss: 0.882016, Accuracy: 87.08%\n",
      "Batch 168, Loss: 0.855489, Accuracy: 87.09%\n",
      "Batch 169, Loss: 0.872777, Accuracy: 87.09%\n",
      "Batch 170, Loss: 0.854269, Accuracy: 87.10%\n",
      "Batch 171, Loss: 0.858837, Accuracy: 87.13%\n",
      "Batch 172, Loss: 0.865064, Accuracy: 87.14%\n",
      "Batch 173, Loss: 0.814158, Accuracy: 87.17%\n",
      "Batch 174, Loss: 0.931285, Accuracy: 87.14%\n",
      "Batch 175, Loss: 0.886718, Accuracy: 87.13%\n",
      "Batch 176, Loss: 0.850950, Accuracy: 87.15%\n",
      "Batch 177, Loss: 0.905791, Accuracy: 87.14%\n",
      "Batch 178, Loss: 0.904052, Accuracy: 87.12%\n",
      "Batch 179, Loss: 0.903671, Accuracy: 87.11%\n",
      "Batch 180, Loss: 0.865121, Accuracy: 87.12%\n",
      "Batch 181, Loss: 0.868804, Accuracy: 87.12%\n",
      "Batch 182, Loss: 0.802576, Accuracy: 87.16%\n",
      "Batch 183, Loss: 0.840635, Accuracy: 87.18%\n",
      "Batch 184, Loss: 0.879745, Accuracy: 87.16%\n",
      "Batch 185, Loss: 0.848455, Accuracy: 87.18%\n",
      "Batch 186, Loss: 0.917573, Accuracy: 87.15%\n",
      "Batch 187, Loss: 0.891392, Accuracy: 87.13%\n",
      "Batch 188, Loss: 0.950546, Accuracy: 87.08%\n",
      "Batch 189, Loss: 0.892747, Accuracy: 87.07%\n",
      "Batch 190, Loss: 0.842101, Accuracy: 87.09%\n",
      "Batch 191, Loss: 0.951020, Accuracy: 87.05%\n",
      "Batch 192, Loss: 0.807655, Accuracy: 87.08%\n",
      "Batch 193, Loss: 0.856085, Accuracy: 87.10%\n",
      "Batch 194, Loss: 0.888248, Accuracy: 87.09%\n",
      "Batch 195, Loss: 0.919075, Accuracy: 87.06%\n",
      "Batch 196, Loss: 0.906632, Accuracy: 87.05%\n",
      "Batch 197, Loss: 0.868456, Accuracy: 87.04%\n",
      "Batch 198, Loss: 0.845730, Accuracy: 87.05%\n",
      "Batch 199, Loss: 0.918685, Accuracy: 87.02%\n",
      "Batch 200, Loss: 0.881683, Accuracy: 87.02%\n",
      "Batch 201, Loss: 0.911485, Accuracy: 87.00%\n",
      "Batch 202, Loss: 0.937412, Accuracy: 86.97%\n",
      "Batch 203, Loss: 0.899496, Accuracy: 86.96%\n",
      "Batch 204, Loss: 0.832902, Accuracy: 86.98%\n",
      "Batch 205, Loss: 0.870413, Accuracy: 86.98%\n",
      "Batch 206, Loss: 0.864825, Accuracy: 86.99%\n",
      "Batch 207, Loss: 0.917869, Accuracy: 86.97%\n",
      "Batch 208, Loss: 0.944782, Accuracy: 86.94%\n",
      "Batch 209, Loss: 0.967923, Accuracy: 86.89%\n",
      "Batch 210, Loss: 0.874948, Accuracy: 86.89%\n",
      "Batch 211, Loss: 0.929758, Accuracy: 86.86%\n",
      "Batch 212, Loss: 0.878709, Accuracy: 86.87%\n",
      "Batch 213, Loss: 0.873226, Accuracy: 86.87%\n",
      "Training - Epoch 90, Loss: 0.874915, Accuracy: 86.87%\n",
      "Validation Batch 1, Loss: 0.827998, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.810346, Accuracy: 93.75%\n",
      "Validation Batch 3, Loss: 0.875298, Accuracy: 90.62%\n",
      "Validation Batch 4, Loss: 0.849809, Accuracy: 90.62%\n",
      "Validation Batch 5, Loss: 0.831216, Accuracy: 90.94%\n",
      "Validation Batch 6, Loss: 0.809188, Accuracy: 91.41%\n",
      "Validation Batch 7, Loss: 0.814923, Accuracy: 91.74%\n",
      "Validation Batch 8, Loss: 0.911782, Accuracy: 90.43%\n",
      "Validation Batch 9, Loss: 0.899170, Accuracy: 89.76%\n",
      "Validation Batch 10, Loss: 0.833328, Accuracy: 90.00%\n",
      "Validation Batch 11, Loss: 0.845545, Accuracy: 90.06%\n",
      "Validation Batch 12, Loss: 0.842013, Accuracy: 90.23%\n",
      "Validation Batch 13, Loss: 0.855860, Accuracy: 90.14%\n",
      "Validation Batch 14, Loss: 0.861176, Accuracy: 90.07%\n",
      "Validation Batch 15, Loss: 0.825115, Accuracy: 90.21%\n",
      "Validation Batch 16, Loss: 0.837290, Accuracy: 90.33%\n",
      "Validation Batch 17, Loss: 0.892810, Accuracy: 89.98%\n",
      "Validation Batch 18, Loss: 0.826727, Accuracy: 90.02%\n",
      "Validation Batch 19, Loss: 0.894585, Accuracy: 89.80%\n",
      "Validation Batch 20, Loss: 0.900406, Accuracy: 89.53%\n",
      "Validation Batch 21, Loss: 0.871914, Accuracy: 89.43%\n",
      "Validation Batch 22, Loss: 0.852451, Accuracy: 89.42%\n",
      "Validation Batch 23, Loss: 0.883311, Accuracy: 89.33%\n",
      "Validation Batch 24, Loss: 0.861008, Accuracy: 89.26%\n",
      "Validation Batch 25, Loss: 0.826307, Accuracy: 89.31%\n",
      "Validation Batch 26, Loss: 0.859975, Accuracy: 89.30%\n",
      "Validation Batch 27, Loss: 0.805863, Accuracy: 89.49%\n",
      "Validation - Epoch 90, Loss: 0.852052, Accuracy: 89.49%\n",
      "Patience—1\n",
      "Epoch 91\n",
      "Batch 1, Loss: 0.885595, Accuracy: 85.94%\n",
      "Batch 2, Loss: 0.890595, Accuracy: 85.94%\n",
      "Batch 3, Loss: 0.888519, Accuracy: 85.94%\n",
      "Batch 4, Loss: 0.864527, Accuracy: 85.94%\n",
      "Batch 5, Loss: 0.832415, Accuracy: 87.19%\n",
      "Batch 6, Loss: 0.876281, Accuracy: 86.98%\n",
      "Batch 7, Loss: 0.915113, Accuracy: 86.38%\n",
      "Batch 8, Loss: 0.834276, Accuracy: 86.91%\n",
      "Batch 9, Loss: 0.815686, Accuracy: 87.67%\n",
      "Batch 10, Loss: 0.896885, Accuracy: 87.34%\n",
      "Batch 11, Loss: 0.813884, Accuracy: 87.93%\n",
      "Batch 12, Loss: 0.815140, Accuracy: 88.41%\n",
      "Batch 13, Loss: 0.861673, Accuracy: 88.34%\n",
      "Batch 14, Loss: 0.841862, Accuracy: 88.50%\n",
      "Batch 15, Loss: 0.899710, Accuracy: 88.23%\n",
      "Batch 16, Loss: 0.838881, Accuracy: 88.48%\n",
      "Batch 17, Loss: 0.876197, Accuracy: 88.42%\n",
      "Batch 18, Loss: 0.869926, Accuracy: 88.37%\n",
      "Batch 19, Loss: 0.880897, Accuracy: 88.24%\n",
      "Batch 20, Loss: 0.902275, Accuracy: 88.05%\n",
      "Batch 21, Loss: 0.883336, Accuracy: 87.95%\n",
      "Batch 22, Loss: 0.889818, Accuracy: 87.78%\n",
      "Batch 23, Loss: 0.835749, Accuracy: 87.91%\n",
      "Batch 24, Loss: 0.857865, Accuracy: 87.96%\n",
      "Batch 25, Loss: 0.846642, Accuracy: 88.00%\n",
      "Batch 26, Loss: 0.890381, Accuracy: 87.86%\n",
      "Batch 27, Loss: 0.871220, Accuracy: 87.85%\n",
      "Batch 28, Loss: 0.861280, Accuracy: 87.83%\n",
      "Batch 29, Loss: 0.882110, Accuracy: 87.82%\n",
      "Batch 30, Loss: 0.890649, Accuracy: 87.71%\n",
      "Batch 31, Loss: 0.838734, Accuracy: 87.75%\n",
      "Batch 32, Loss: 0.877720, Accuracy: 87.74%\n",
      "Batch 33, Loss: 0.871810, Accuracy: 87.74%\n",
      "Batch 34, Loss: 0.870793, Accuracy: 87.73%\n",
      "Batch 35, Loss: 0.895256, Accuracy: 87.68%\n",
      "Batch 36, Loss: 0.921731, Accuracy: 87.54%\n",
      "Batch 37, Loss: 0.878048, Accuracy: 87.58%\n",
      "Batch 38, Loss: 0.889625, Accuracy: 87.50%\n",
      "Batch 39, Loss: 0.895365, Accuracy: 87.46%\n",
      "Batch 40, Loss: 0.873340, Accuracy: 87.46%\n",
      "Batch 41, Loss: 0.936698, Accuracy: 87.27%\n",
      "Batch 42, Loss: 0.900980, Accuracy: 87.20%\n",
      "Batch 43, Loss: 0.857705, Accuracy: 87.25%\n",
      "Batch 44, Loss: 0.885975, Accuracy: 87.25%\n",
      "Batch 45, Loss: 0.845353, Accuracy: 87.29%\n",
      "Batch 46, Loss: 0.882029, Accuracy: 87.26%\n",
      "Batch 47, Loss: 0.925214, Accuracy: 87.17%\n",
      "Batch 48, Loss: 0.865266, Accuracy: 87.17%\n",
      "Batch 49, Loss: 0.837384, Accuracy: 87.28%\n",
      "Batch 50, Loss: 0.878298, Accuracy: 87.25%\n",
      "Batch 51, Loss: 0.834763, Accuracy: 87.35%\n",
      "Batch 52, Loss: 0.897256, Accuracy: 87.29%\n",
      "Batch 53, Loss: 0.837976, Accuracy: 87.35%\n",
      "Batch 54, Loss: 0.804747, Accuracy: 87.47%\n",
      "Batch 55, Loss: 0.813356, Accuracy: 87.61%\n",
      "Batch 56, Loss: 0.856098, Accuracy: 87.67%\n",
      "Batch 57, Loss: 0.861912, Accuracy: 87.66%\n",
      "Batch 58, Loss: 0.819623, Accuracy: 87.74%\n",
      "Batch 59, Loss: 0.934356, Accuracy: 87.63%\n",
      "Batch 60, Loss: 0.879158, Accuracy: 87.63%\n",
      "Batch 61, Loss: 0.837480, Accuracy: 87.70%\n",
      "Batch 62, Loss: 0.849962, Accuracy: 87.73%\n",
      "Batch 63, Loss: 0.864861, Accuracy: 87.70%\n",
      "Batch 64, Loss: 0.852479, Accuracy: 87.72%\n",
      "Batch 65, Loss: 0.940327, Accuracy: 87.60%\n",
      "Batch 66, Loss: 0.867364, Accuracy: 87.55%\n",
      "Batch 67, Loss: 0.916312, Accuracy: 87.45%\n",
      "Batch 68, Loss: 0.900172, Accuracy: 87.41%\n",
      "Batch 69, Loss: 0.873768, Accuracy: 87.39%\n",
      "Batch 70, Loss: 0.807985, Accuracy: 87.50%\n",
      "Batch 71, Loss: 0.844445, Accuracy: 87.54%\n",
      "Batch 72, Loss: 0.881276, Accuracy: 87.54%\n",
      "Batch 73, Loss: 0.851819, Accuracy: 87.56%\n",
      "Batch 74, Loss: 0.891394, Accuracy: 87.52%\n",
      "Batch 75, Loss: 0.893358, Accuracy: 87.50%\n",
      "Batch 76, Loss: 0.920577, Accuracy: 87.42%\n",
      "Batch 77, Loss: 0.939602, Accuracy: 87.32%\n",
      "Batch 78, Loss: 0.844429, Accuracy: 87.34%\n",
      "Batch 79, Loss: 0.883354, Accuracy: 87.32%\n",
      "Batch 80, Loss: 0.895068, Accuracy: 87.29%\n",
      "Batch 81, Loss: 0.929730, Accuracy: 87.21%\n",
      "Batch 82, Loss: 0.824226, Accuracy: 87.27%\n",
      "Batch 83, Loss: 0.804937, Accuracy: 87.33%\n",
      "Batch 84, Loss: 0.897059, Accuracy: 87.28%\n",
      "Batch 85, Loss: 0.913990, Accuracy: 87.22%\n",
      "Batch 86, Loss: 0.920539, Accuracy: 87.15%\n",
      "Batch 87, Loss: 0.891728, Accuracy: 87.14%\n",
      "Batch 88, Loss: 0.933915, Accuracy: 87.04%\n",
      "Batch 89, Loss: 0.873644, Accuracy: 87.03%\n",
      "Batch 90, Loss: 0.845638, Accuracy: 87.07%\n",
      "Batch 91, Loss: 0.833052, Accuracy: 87.12%\n",
      "Batch 92, Loss: 0.929946, Accuracy: 87.06%\n",
      "Batch 93, Loss: 0.861977, Accuracy: 87.08%\n",
      "Batch 94, Loss: 0.885483, Accuracy: 87.05%\n",
      "Batch 95, Loss: 0.838688, Accuracy: 87.09%\n",
      "Batch 96, Loss: 0.843162, Accuracy: 87.13%\n",
      "Batch 97, Loss: 0.969448, Accuracy: 87.03%\n",
      "Batch 98, Loss: 0.823817, Accuracy: 87.10%\n",
      "Batch 99, Loss: 0.880790, Accuracy: 87.09%\n",
      "Batch 100, Loss: 0.859090, Accuracy: 87.09%\n",
      "Batch 101, Loss: 0.846374, Accuracy: 87.11%\n",
      "Batch 102, Loss: 0.840568, Accuracy: 87.16%\n",
      "Batch 103, Loss: 0.861545, Accuracy: 87.15%\n",
      "Batch 104, Loss: 0.926035, Accuracy: 87.09%\n",
      "Batch 105, Loss: 0.815921, Accuracy: 87.16%\n",
      "Batch 106, Loss: 0.832880, Accuracy: 87.19%\n",
      "Batch 107, Loss: 0.856966, Accuracy: 87.21%\n",
      "Batch 108, Loss: 0.867376, Accuracy: 87.20%\n",
      "Batch 109, Loss: 0.841589, Accuracy: 87.23%\n",
      "Batch 110, Loss: 0.824028, Accuracy: 87.27%\n",
      "Batch 111, Loss: 0.818160, Accuracy: 87.33%\n",
      "Batch 112, Loss: 0.888471, Accuracy: 87.32%\n",
      "Batch 113, Loss: 0.871570, Accuracy: 87.32%\n",
      "Batch 114, Loss: 0.850538, Accuracy: 87.35%\n",
      "Batch 115, Loss: 0.802156, Accuracy: 87.42%\n",
      "Batch 116, Loss: 0.812718, Accuracy: 87.47%\n",
      "Batch 117, Loss: 0.887140, Accuracy: 87.46%\n",
      "Batch 118, Loss: 0.821205, Accuracy: 87.49%\n",
      "Batch 119, Loss: 0.913162, Accuracy: 87.45%\n",
      "Batch 120, Loss: 0.880913, Accuracy: 87.43%\n",
      "Batch 121, Loss: 0.829453, Accuracy: 87.46%\n",
      "Batch 122, Loss: 0.848452, Accuracy: 87.49%\n",
      "Batch 123, Loss: 0.897287, Accuracy: 87.46%\n",
      "Batch 124, Loss: 0.892800, Accuracy: 87.45%\n",
      "Batch 125, Loss: 0.833287, Accuracy: 87.49%\n",
      "Batch 126, Loss: 0.867365, Accuracy: 87.49%\n",
      "Batch 127, Loss: 0.839090, Accuracy: 87.51%\n",
      "Batch 128, Loss: 0.863040, Accuracy: 87.51%\n",
      "Batch 129, Loss: 0.826063, Accuracy: 87.54%\n",
      "Batch 130, Loss: 0.897887, Accuracy: 87.51%\n",
      "Batch 131, Loss: 0.863422, Accuracy: 87.51%\n",
      "Batch 132, Loss: 0.877650, Accuracy: 87.52%\n",
      "Batch 133, Loss: 0.884329, Accuracy: 87.51%\n",
      "Batch 134, Loss: 0.897173, Accuracy: 87.49%\n",
      "Batch 135, Loss: 0.897053, Accuracy: 87.45%\n",
      "Batch 136, Loss: 0.914524, Accuracy: 87.42%\n",
      "Batch 137, Loss: 0.913075, Accuracy: 87.39%\n",
      "Batch 138, Loss: 0.867944, Accuracy: 87.39%\n",
      "Batch 139, Loss: 0.881993, Accuracy: 87.38%\n",
      "Batch 140, Loss: 0.923198, Accuracy: 87.33%\n",
      "Batch 141, Loss: 0.951760, Accuracy: 87.27%\n",
      "Batch 142, Loss: 0.811267, Accuracy: 87.32%\n",
      "Batch 143, Loss: 0.844142, Accuracy: 87.34%\n",
      "Batch 144, Loss: 0.850838, Accuracy: 87.35%\n",
      "Batch 145, Loss: 0.833456, Accuracy: 87.37%\n",
      "Batch 146, Loss: 0.825586, Accuracy: 87.41%\n",
      "Batch 147, Loss: 0.879006, Accuracy: 87.40%\n",
      "Batch 148, Loss: 0.920313, Accuracy: 87.36%\n",
      "Batch 149, Loss: 0.820403, Accuracy: 87.41%\n",
      "Batch 150, Loss: 0.874336, Accuracy: 87.41%\n",
      "Batch 151, Loss: 0.872014, Accuracy: 87.41%\n",
      "Batch 152, Loss: 0.859442, Accuracy: 87.43%\n",
      "Batch 153, Loss: 0.878303, Accuracy: 87.42%\n",
      "Batch 154, Loss: 0.875281, Accuracy: 87.42%\n",
      "Batch 155, Loss: 0.882396, Accuracy: 87.42%\n",
      "Batch 156, Loss: 0.890849, Accuracy: 87.40%\n",
      "Batch 157, Loss: 0.885533, Accuracy: 87.39%\n",
      "Batch 158, Loss: 0.883100, Accuracy: 87.38%\n",
      "Batch 159, Loss: 0.884946, Accuracy: 87.37%\n",
      "Batch 160, Loss: 0.881318, Accuracy: 87.35%\n",
      "Batch 161, Loss: 0.906992, Accuracy: 87.33%\n",
      "Batch 162, Loss: 0.852557, Accuracy: 87.34%\n",
      "Batch 163, Loss: 0.813510, Accuracy: 87.37%\n",
      "Batch 164, Loss: 0.853700, Accuracy: 87.39%\n",
      "Batch 165, Loss: 0.828193, Accuracy: 87.41%\n",
      "Batch 166, Loss: 0.986650, Accuracy: 87.33%\n",
      "Batch 167, Loss: 0.886340, Accuracy: 87.32%\n",
      "Batch 168, Loss: 0.817448, Accuracy: 87.35%\n",
      "Batch 169, Loss: 0.875750, Accuracy: 87.34%\n",
      "Batch 170, Loss: 0.897145, Accuracy: 87.32%\n",
      "Batch 171, Loss: 0.853210, Accuracy: 87.34%\n",
      "Batch 172, Loss: 0.918037, Accuracy: 87.31%\n",
      "Batch 173, Loss: 0.851009, Accuracy: 87.32%\n",
      "Batch 174, Loss: 0.909606, Accuracy: 87.29%\n",
      "Batch 175, Loss: 0.872738, Accuracy: 87.29%\n",
      "Batch 176, Loss: 0.838408, Accuracy: 87.31%\n",
      "Batch 177, Loss: 0.849763, Accuracy: 87.31%\n",
      "Batch 178, Loss: 0.786937, Accuracy: 87.36%\n",
      "Batch 179, Loss: 0.931398, Accuracy: 87.33%\n",
      "Batch 180, Loss: 0.877842, Accuracy: 87.32%\n",
      "Batch 181, Loss: 0.865763, Accuracy: 87.32%\n",
      "Batch 182, Loss: 0.850349, Accuracy: 87.34%\n",
      "Batch 183, Loss: 0.930026, Accuracy: 87.31%\n",
      "Batch 184, Loss: 0.831194, Accuracy: 87.33%\n",
      "Batch 185, Loss: 0.858293, Accuracy: 87.34%\n",
      "Batch 186, Loss: 0.861418, Accuracy: 87.34%\n",
      "Batch 187, Loss: 0.824790, Accuracy: 87.37%\n",
      "Batch 188, Loss: 0.934410, Accuracy: 87.33%\n",
      "Batch 189, Loss: 0.857324, Accuracy: 87.33%\n",
      "Batch 190, Loss: 0.882467, Accuracy: 87.33%\n",
      "Batch 191, Loss: 0.824131, Accuracy: 87.34%\n",
      "Batch 192, Loss: 0.853203, Accuracy: 87.35%\n",
      "Batch 193, Loss: 0.856085, Accuracy: 87.36%\n",
      "Batch 194, Loss: 0.827972, Accuracy: 87.39%\n",
      "Batch 195, Loss: 0.901159, Accuracy: 87.36%\n",
      "Batch 196, Loss: 0.976647, Accuracy: 87.31%\n",
      "Batch 197, Loss: 0.864439, Accuracy: 87.32%\n",
      "Batch 198, Loss: 0.869864, Accuracy: 87.32%\n",
      "Batch 199, Loss: 0.888040, Accuracy: 87.32%\n",
      "Batch 200, Loss: 0.816975, Accuracy: 87.35%\n",
      "Batch 201, Loss: 0.869352, Accuracy: 87.35%\n",
      "Batch 202, Loss: 0.860219, Accuracy: 87.36%\n",
      "Batch 203, Loss: 0.847750, Accuracy: 87.38%\n",
      "Batch 204, Loss: 0.929273, Accuracy: 87.35%\n",
      "Batch 205, Loss: 0.900970, Accuracy: 87.33%\n",
      "Batch 206, Loss: 0.833920, Accuracy: 87.35%\n",
      "Batch 207, Loss: 0.892306, Accuracy: 87.33%\n",
      "Batch 208, Loss: 0.877062, Accuracy: 87.33%\n",
      "Batch 209, Loss: 0.800863, Accuracy: 87.36%\n",
      "Batch 210, Loss: 0.903677, Accuracy: 87.34%\n",
      "Batch 211, Loss: 0.844104, Accuracy: 87.36%\n",
      "Batch 212, Loss: 0.911873, Accuracy: 87.33%\n",
      "Batch 213, Loss: 0.921083, Accuracy: 87.30%\n",
      "Training - Epoch 91, Loss: 0.870637, Accuracy: 87.30%\n",
      "Validation Batch 1, Loss: 0.835526, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.817001, Accuracy: 92.97%\n",
      "Validation Batch 3, Loss: 0.883089, Accuracy: 90.10%\n",
      "Validation Batch 4, Loss: 0.850563, Accuracy: 90.23%\n",
      "Validation Batch 5, Loss: 0.839841, Accuracy: 90.62%\n",
      "Validation Batch 6, Loss: 0.814895, Accuracy: 91.15%\n",
      "Validation Batch 7, Loss: 0.814265, Accuracy: 91.74%\n",
      "Validation Batch 8, Loss: 0.908635, Accuracy: 91.02%\n",
      "Validation Batch 9, Loss: 0.905436, Accuracy: 90.28%\n",
      "Validation Batch 10, Loss: 0.837071, Accuracy: 90.47%\n",
      "Validation Batch 11, Loss: 0.854101, Accuracy: 90.20%\n",
      "Validation Batch 12, Loss: 0.843067, Accuracy: 90.36%\n",
      "Validation Batch 13, Loss: 0.863544, Accuracy: 90.14%\n",
      "Validation Batch 14, Loss: 0.868307, Accuracy: 90.07%\n",
      "Validation Batch 15, Loss: 0.829295, Accuracy: 90.21%\n",
      "Validation Batch 16, Loss: 0.852637, Accuracy: 90.23%\n",
      "Validation Batch 17, Loss: 0.900998, Accuracy: 89.89%\n",
      "Validation Batch 18, Loss: 0.831963, Accuracy: 89.93%\n",
      "Validation Batch 19, Loss: 0.907974, Accuracy: 89.56%\n",
      "Validation Batch 20, Loss: 0.901300, Accuracy: 89.30%\n",
      "Validation Batch 21, Loss: 0.879074, Accuracy: 89.21%\n",
      "Validation Batch 22, Loss: 0.863135, Accuracy: 89.06%\n",
      "Validation Batch 23, Loss: 0.894441, Accuracy: 88.99%\n",
      "Validation Batch 24, Loss: 0.878480, Accuracy: 88.87%\n",
      "Validation Batch 25, Loss: 0.828505, Accuracy: 89.00%\n",
      "Validation Batch 26, Loss: 0.866295, Accuracy: 88.94%\n",
      "Validation Batch 27, Loss: 0.823151, Accuracy: 89.02%\n",
      "Validation - Epoch 91, Loss: 0.858985, Accuracy: 89.02%\n",
      "Patience—2\n",
      "Epoch 92\n",
      "Batch 1, Loss: 0.921069, Accuracy: 79.69%\n",
      "Batch 2, Loss: 0.867818, Accuracy: 83.59%\n",
      "Batch 3, Loss: 0.847044, Accuracy: 85.42%\n",
      "Batch 4, Loss: 0.870262, Accuracy: 85.55%\n",
      "Batch 5, Loss: 0.829733, Accuracy: 86.56%\n",
      "Batch 6, Loss: 0.911612, Accuracy: 85.94%\n",
      "Batch 7, Loss: 0.825478, Accuracy: 87.05%\n",
      "Batch 8, Loss: 0.863751, Accuracy: 87.11%\n",
      "Batch 9, Loss: 0.850353, Accuracy: 87.50%\n",
      "Batch 10, Loss: 0.912093, Accuracy: 87.19%\n",
      "Batch 11, Loss: 0.905653, Accuracy: 86.93%\n",
      "Batch 12, Loss: 0.837068, Accuracy: 87.37%\n",
      "Batch 13, Loss: 0.818944, Accuracy: 87.74%\n",
      "Batch 14, Loss: 0.918453, Accuracy: 87.39%\n",
      "Batch 15, Loss: 0.875760, Accuracy: 87.29%\n",
      "Batch 16, Loss: 0.860322, Accuracy: 87.40%\n",
      "Batch 17, Loss: 0.875848, Accuracy: 87.32%\n",
      "Batch 18, Loss: 0.858767, Accuracy: 87.41%\n",
      "Batch 19, Loss: 0.834129, Accuracy: 87.66%\n",
      "Batch 20, Loss: 0.897614, Accuracy: 87.50%\n",
      "Batch 21, Loss: 0.819600, Accuracy: 87.80%\n",
      "Batch 22, Loss: 0.832554, Accuracy: 88.00%\n",
      "Batch 23, Loss: 0.827484, Accuracy: 88.11%\n",
      "Batch 24, Loss: 0.830532, Accuracy: 88.28%\n",
      "Batch 25, Loss: 0.815092, Accuracy: 88.50%\n",
      "Batch 26, Loss: 0.859497, Accuracy: 88.52%\n",
      "Batch 27, Loss: 0.829080, Accuracy: 88.72%\n",
      "Batch 28, Loss: 0.878778, Accuracy: 88.67%\n",
      "Batch 29, Loss: 0.843572, Accuracy: 88.74%\n",
      "Batch 30, Loss: 0.914952, Accuracy: 88.54%\n",
      "Batch 31, Loss: 0.842614, Accuracy: 88.66%\n",
      "Batch 32, Loss: 0.852948, Accuracy: 88.67%\n",
      "Batch 33, Loss: 0.896554, Accuracy: 88.54%\n",
      "Batch 34, Loss: 0.828084, Accuracy: 88.69%\n",
      "Batch 35, Loss: 0.922252, Accuracy: 88.48%\n",
      "Batch 36, Loss: 0.862868, Accuracy: 88.41%\n",
      "Batch 37, Loss: 0.818488, Accuracy: 88.51%\n",
      "Batch 38, Loss: 0.882543, Accuracy: 88.49%\n",
      "Batch 39, Loss: 0.787733, Accuracy: 88.70%\n",
      "Batch 40, Loss: 0.849969, Accuracy: 88.75%\n",
      "Batch 41, Loss: 0.886827, Accuracy: 88.68%\n",
      "Batch 42, Loss: 0.853749, Accuracy: 88.69%\n",
      "Batch 43, Loss: 0.834533, Accuracy: 88.74%\n",
      "Batch 44, Loss: 0.837864, Accuracy: 88.78%\n",
      "Batch 45, Loss: 0.859530, Accuracy: 88.82%\n",
      "Batch 46, Loss: 0.799016, Accuracy: 88.96%\n",
      "Batch 47, Loss: 0.877960, Accuracy: 88.93%\n",
      "Batch 48, Loss: 0.854296, Accuracy: 88.90%\n",
      "Batch 49, Loss: 0.909685, Accuracy: 88.81%\n",
      "Batch 50, Loss: 0.943768, Accuracy: 88.66%\n",
      "Batch 51, Loss: 0.799217, Accuracy: 88.79%\n",
      "Batch 52, Loss: 0.960755, Accuracy: 88.58%\n",
      "Batch 53, Loss: 0.860551, Accuracy: 88.59%\n",
      "Batch 54, Loss: 0.815669, Accuracy: 88.69%\n",
      "Batch 55, Loss: 0.836533, Accuracy: 88.72%\n",
      "Batch 56, Loss: 0.869970, Accuracy: 88.70%\n",
      "Batch 57, Loss: 0.873823, Accuracy: 88.65%\n",
      "Batch 58, Loss: 0.935014, Accuracy: 88.52%\n",
      "Batch 59, Loss: 0.877108, Accuracy: 88.51%\n",
      "Batch 60, Loss: 0.877513, Accuracy: 88.46%\n",
      "Batch 61, Loss: 0.896726, Accuracy: 88.40%\n",
      "Batch 62, Loss: 0.895094, Accuracy: 88.36%\n",
      "Batch 63, Loss: 0.861829, Accuracy: 88.34%\n",
      "Batch 64, Loss: 0.890553, Accuracy: 88.31%\n",
      "Batch 65, Loss: 0.891281, Accuracy: 88.25%\n",
      "Batch 66, Loss: 0.834166, Accuracy: 88.30%\n",
      "Batch 67, Loss: 0.832521, Accuracy: 88.36%\n",
      "Batch 68, Loss: 0.838169, Accuracy: 88.40%\n",
      "Batch 69, Loss: 0.857403, Accuracy: 88.38%\n",
      "Batch 70, Loss: 0.897442, Accuracy: 88.33%\n",
      "Batch 71, Loss: 0.867350, Accuracy: 88.31%\n",
      "Batch 72, Loss: 0.887283, Accuracy: 88.26%\n",
      "Batch 73, Loss: 0.822446, Accuracy: 88.33%\n",
      "Batch 74, Loss: 0.867272, Accuracy: 88.30%\n",
      "Batch 75, Loss: 0.905039, Accuracy: 88.25%\n",
      "Batch 76, Loss: 0.836267, Accuracy: 88.28%\n",
      "Batch 77, Loss: 0.904626, Accuracy: 88.23%\n",
      "Batch 78, Loss: 0.838246, Accuracy: 88.26%\n",
      "Batch 79, Loss: 0.872474, Accuracy: 88.25%\n",
      "Batch 80, Loss: 0.883950, Accuracy: 88.22%\n",
      "Batch 81, Loss: 0.872148, Accuracy: 88.21%\n",
      "Batch 82, Loss: 0.845996, Accuracy: 88.22%\n",
      "Batch 83, Loss: 0.820212, Accuracy: 88.27%\n",
      "Batch 84, Loss: 0.874940, Accuracy: 88.23%\n",
      "Batch 85, Loss: 0.859964, Accuracy: 88.22%\n",
      "Batch 86, Loss: 0.880020, Accuracy: 88.19%\n",
      "Batch 87, Loss: 0.847114, Accuracy: 88.22%\n",
      "Batch 88, Loss: 0.881794, Accuracy: 88.21%\n",
      "Batch 89, Loss: 0.873336, Accuracy: 88.20%\n",
      "Batch 90, Loss: 0.959721, Accuracy: 88.07%\n",
      "Batch 91, Loss: 0.936814, Accuracy: 88.00%\n",
      "Batch 92, Loss: 0.901547, Accuracy: 87.98%\n",
      "Batch 93, Loss: 0.865277, Accuracy: 87.97%\n",
      "Batch 94, Loss: 0.872261, Accuracy: 87.95%\n",
      "Batch 95, Loss: 0.850480, Accuracy: 87.94%\n",
      "Batch 96, Loss: 0.888027, Accuracy: 87.92%\n",
      "Batch 97, Loss: 0.876222, Accuracy: 87.89%\n",
      "Batch 98, Loss: 0.882105, Accuracy: 87.85%\n",
      "Batch 99, Loss: 0.805253, Accuracy: 87.93%\n",
      "Batch 100, Loss: 0.909658, Accuracy: 87.91%\n",
      "Batch 101, Loss: 0.891109, Accuracy: 87.89%\n",
      "Batch 102, Loss: 0.852200, Accuracy: 87.90%\n",
      "Batch 103, Loss: 0.886056, Accuracy: 87.86%\n",
      "Batch 104, Loss: 0.921495, Accuracy: 87.82%\n",
      "Batch 105, Loss: 0.904185, Accuracy: 87.78%\n",
      "Batch 106, Loss: 0.848770, Accuracy: 87.81%\n",
      "Batch 107, Loss: 0.850186, Accuracy: 87.84%\n",
      "Batch 108, Loss: 0.884081, Accuracy: 87.82%\n",
      "Batch 109, Loss: 0.868754, Accuracy: 87.82%\n",
      "Batch 110, Loss: 0.864212, Accuracy: 87.81%\n",
      "Batch 111, Loss: 0.866753, Accuracy: 87.81%\n",
      "Batch 112, Loss: 0.876099, Accuracy: 87.81%\n",
      "Batch 113, Loss: 0.896813, Accuracy: 87.79%\n",
      "Batch 114, Loss: 0.876658, Accuracy: 87.77%\n",
      "Batch 115, Loss: 0.844849, Accuracy: 87.80%\n",
      "Batch 116, Loss: 0.937756, Accuracy: 87.73%\n",
      "Batch 117, Loss: 0.917312, Accuracy: 87.69%\n",
      "Batch 118, Loss: 0.879619, Accuracy: 87.67%\n",
      "Batch 119, Loss: 0.860391, Accuracy: 87.67%\n",
      "Batch 120, Loss: 0.840088, Accuracy: 87.71%\n",
      "Batch 121, Loss: 0.824334, Accuracy: 87.76%\n",
      "Batch 122, Loss: 0.882266, Accuracy: 87.74%\n",
      "Batch 123, Loss: 0.892662, Accuracy: 87.72%\n",
      "Batch 124, Loss: 0.908591, Accuracy: 87.69%\n",
      "Batch 125, Loss: 0.846578, Accuracy: 87.71%\n",
      "Batch 126, Loss: 0.861543, Accuracy: 87.71%\n",
      "Batch 127, Loss: 0.878232, Accuracy: 87.70%\n",
      "Batch 128, Loss: 0.860357, Accuracy: 87.70%\n",
      "Batch 129, Loss: 0.864479, Accuracy: 87.71%\n",
      "Batch 130, Loss: 0.889409, Accuracy: 87.69%\n",
      "Batch 131, Loss: 0.880561, Accuracy: 87.67%\n",
      "Batch 132, Loss: 0.863823, Accuracy: 87.68%\n",
      "Batch 133, Loss: 0.848653, Accuracy: 87.69%\n",
      "Batch 134, Loss: 0.848189, Accuracy: 87.70%\n",
      "Batch 135, Loss: 0.794873, Accuracy: 87.75%\n",
      "Batch 136, Loss: 0.891925, Accuracy: 87.73%\n",
      "Batch 137, Loss: 0.861141, Accuracy: 87.74%\n",
      "Batch 138, Loss: 0.838493, Accuracy: 87.76%\n",
      "Batch 139, Loss: 0.862797, Accuracy: 87.78%\n",
      "Batch 140, Loss: 0.911206, Accuracy: 87.76%\n",
      "Batch 141, Loss: 0.881852, Accuracy: 87.74%\n",
      "Batch 142, Loss: 0.916692, Accuracy: 87.69%\n",
      "Batch 143, Loss: 0.837499, Accuracy: 87.72%\n",
      "Batch 144, Loss: 0.838986, Accuracy: 87.74%\n",
      "Batch 145, Loss: 0.890795, Accuracy: 87.72%\n",
      "Batch 146, Loss: 0.834553, Accuracy: 87.74%\n",
      "Batch 147, Loss: 0.859203, Accuracy: 87.73%\n",
      "Batch 148, Loss: 0.881625, Accuracy: 87.72%\n",
      "Batch 149, Loss: 0.851960, Accuracy: 87.73%\n",
      "Batch 150, Loss: 0.911951, Accuracy: 87.70%\n",
      "Batch 151, Loss: 0.907584, Accuracy: 87.68%\n",
      "Batch 152, Loss: 0.963671, Accuracy: 87.60%\n",
      "Batch 153, Loss: 0.800743, Accuracy: 87.65%\n",
      "Batch 154, Loss: 0.886672, Accuracy: 87.65%\n",
      "Batch 155, Loss: 0.870960, Accuracy: 87.66%\n",
      "Batch 156, Loss: 0.898661, Accuracy: 87.64%\n",
      "Batch 157, Loss: 0.825257, Accuracy: 87.67%\n",
      "Batch 158, Loss: 0.841215, Accuracy: 87.68%\n",
      "Batch 159, Loss: 0.894725, Accuracy: 87.67%\n",
      "Batch 160, Loss: 0.840997, Accuracy: 87.69%\n",
      "Batch 161, Loss: 0.897746, Accuracy: 87.65%\n",
      "Batch 162, Loss: 0.892600, Accuracy: 87.63%\n",
      "Batch 163, Loss: 0.839580, Accuracy: 87.65%\n",
      "Batch 164, Loss: 0.814072, Accuracy: 87.70%\n",
      "Batch 165, Loss: 0.863580, Accuracy: 87.70%\n",
      "Batch 166, Loss: 0.828554, Accuracy: 87.73%\n",
      "Batch 167, Loss: 0.941338, Accuracy: 87.69%\n",
      "Batch 168, Loss: 0.918326, Accuracy: 87.66%\n",
      "Batch 169, Loss: 0.869438, Accuracy: 87.65%\n",
      "Batch 170, Loss: 0.894617, Accuracy: 87.64%\n",
      "Batch 171, Loss: 0.848530, Accuracy: 87.66%\n",
      "Batch 172, Loss: 0.820827, Accuracy: 87.68%\n",
      "Batch 173, Loss: 0.908118, Accuracy: 87.65%\n",
      "Batch 174, Loss: 0.814898, Accuracy: 87.69%\n",
      "Batch 175, Loss: 0.873089, Accuracy: 87.67%\n",
      "Batch 176, Loss: 0.830061, Accuracy: 87.70%\n",
      "Batch 177, Loss: 0.832878, Accuracy: 87.72%\n",
      "Batch 178, Loss: 0.855235, Accuracy: 87.74%\n",
      "Batch 179, Loss: 0.913123, Accuracy: 87.71%\n",
      "Batch 180, Loss: 0.988079, Accuracy: 87.64%\n",
      "Batch 181, Loss: 0.851766, Accuracy: 87.65%\n",
      "Batch 182, Loss: 0.840280, Accuracy: 87.66%\n",
      "Batch 183, Loss: 0.905176, Accuracy: 87.64%\n",
      "Batch 184, Loss: 0.854430, Accuracy: 87.64%\n",
      "Batch 185, Loss: 0.903123, Accuracy: 87.63%\n",
      "Batch 186, Loss: 0.917966, Accuracy: 87.60%\n",
      "Batch 187, Loss: 0.906273, Accuracy: 87.58%\n",
      "Batch 188, Loss: 0.858868, Accuracy: 87.58%\n",
      "Batch 189, Loss: 0.895284, Accuracy: 87.57%\n",
      "Batch 190, Loss: 0.982055, Accuracy: 87.51%\n",
      "Batch 191, Loss: 0.985214, Accuracy: 87.45%\n",
      "Batch 192, Loss: 0.934858, Accuracy: 87.41%\n",
      "Batch 193, Loss: 0.896310, Accuracy: 87.40%\n",
      "Batch 194, Loss: 0.779389, Accuracy: 87.45%\n",
      "Batch 195, Loss: 0.845894, Accuracy: 87.47%\n",
      "Batch 196, Loss: 0.841611, Accuracy: 87.48%\n",
      "Batch 197, Loss: 0.875358, Accuracy: 87.48%\n",
      "Batch 198, Loss: 0.887906, Accuracy: 87.47%\n",
      "Batch 199, Loss: 0.810213, Accuracy: 87.50%\n",
      "Batch 200, Loss: 0.836504, Accuracy: 87.52%\n",
      "Batch 201, Loss: 0.896384, Accuracy: 87.50%\n",
      "Batch 202, Loss: 0.898832, Accuracy: 87.48%\n",
      "Batch 203, Loss: 0.934008, Accuracy: 87.45%\n",
      "Batch 204, Loss: 0.858498, Accuracy: 87.45%\n",
      "Batch 205, Loss: 0.823358, Accuracy: 87.48%\n",
      "Batch 206, Loss: 0.872669, Accuracy: 87.48%\n",
      "Batch 207, Loss: 0.924897, Accuracy: 87.45%\n",
      "Batch 208, Loss: 0.861172, Accuracy: 87.45%\n",
      "Batch 209, Loss: 0.819585, Accuracy: 87.48%\n",
      "Batch 210, Loss: 0.834840, Accuracy: 87.49%\n",
      "Batch 211, Loss: 0.848796, Accuracy: 87.51%\n",
      "Batch 212, Loss: 0.818676, Accuracy: 87.54%\n",
      "Batch 213, Loss: 0.881868, Accuracy: 87.53%\n",
      "Training - Epoch 92, Loss: 0.869780, Accuracy: 87.53%\n",
      "Validation Batch 1, Loss: 0.835182, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.817642, Accuracy: 93.75%\n",
      "Validation Batch 3, Loss: 0.886642, Accuracy: 90.62%\n",
      "Validation Batch 4, Loss: 0.860624, Accuracy: 90.23%\n",
      "Validation Batch 5, Loss: 0.840055, Accuracy: 90.31%\n",
      "Validation Batch 6, Loss: 0.813128, Accuracy: 90.89%\n",
      "Validation Batch 7, Loss: 0.819490, Accuracy: 91.07%\n",
      "Validation Batch 8, Loss: 0.910072, Accuracy: 90.04%\n",
      "Validation Batch 9, Loss: 0.909566, Accuracy: 89.41%\n",
      "Validation Batch 10, Loss: 0.840231, Accuracy: 89.53%\n",
      "Validation Batch 11, Loss: 0.850357, Accuracy: 89.63%\n",
      "Validation Batch 12, Loss: 0.843249, Accuracy: 89.84%\n",
      "Validation Batch 13, Loss: 0.860554, Accuracy: 89.66%\n",
      "Validation Batch 14, Loss: 0.867617, Accuracy: 89.62%\n",
      "Validation Batch 15, Loss: 0.829415, Accuracy: 89.79%\n",
      "Validation Batch 16, Loss: 0.850055, Accuracy: 89.84%\n",
      "Validation Batch 17, Loss: 0.900359, Accuracy: 89.52%\n",
      "Validation Batch 18, Loss: 0.830868, Accuracy: 89.58%\n",
      "Validation Batch 19, Loss: 0.902389, Accuracy: 89.31%\n",
      "Validation Batch 20, Loss: 0.913240, Accuracy: 88.98%\n",
      "Validation Batch 21, Loss: 0.877609, Accuracy: 88.99%\n",
      "Validation Batch 22, Loss: 0.864867, Accuracy: 88.92%\n",
      "Validation Batch 23, Loss: 0.889357, Accuracy: 88.86%\n",
      "Validation Batch 24, Loss: 0.880128, Accuracy: 88.74%\n",
      "Validation Batch 25, Loss: 0.833840, Accuracy: 88.88%\n",
      "Validation Batch 26, Loss: 0.870384, Accuracy: 88.76%\n",
      "Validation Batch 27, Loss: 0.820046, Accuracy: 88.90%\n",
      "Validation - Epoch 92, Loss: 0.859888, Accuracy: 88.90%\n",
      "Patience—3\n",
      "Epoch 93\n",
      "Batch 1, Loss: 0.914615, Accuracy: 84.38%\n",
      "Batch 2, Loss: 0.877807, Accuracy: 84.38%\n",
      "Batch 3, Loss: 0.796297, Accuracy: 88.02%\n",
      "Batch 4, Loss: 0.899086, Accuracy: 87.11%\n",
      "Batch 5, Loss: 0.892548, Accuracy: 86.56%\n",
      "Batch 6, Loss: 0.871996, Accuracy: 86.72%\n",
      "Batch 7, Loss: 0.888245, Accuracy: 86.38%\n",
      "Batch 8, Loss: 0.865961, Accuracy: 86.72%\n",
      "Batch 9, Loss: 0.839574, Accuracy: 87.15%\n",
      "Batch 10, Loss: 0.818517, Accuracy: 87.81%\n",
      "Batch 11, Loss: 0.828917, Accuracy: 88.07%\n",
      "Batch 12, Loss: 0.854752, Accuracy: 88.15%\n",
      "Batch 13, Loss: 0.855782, Accuracy: 88.22%\n",
      "Batch 14, Loss: 0.830971, Accuracy: 88.39%\n",
      "Batch 15, Loss: 0.864969, Accuracy: 88.33%\n",
      "Batch 16, Loss: 0.916154, Accuracy: 87.89%\n",
      "Batch 17, Loss: 0.797379, Accuracy: 88.24%\n",
      "Batch 18, Loss: 0.829694, Accuracy: 88.45%\n",
      "Batch 19, Loss: 0.839770, Accuracy: 88.49%\n",
      "Batch 20, Loss: 0.861079, Accuracy: 88.52%\n",
      "Batch 21, Loss: 0.920447, Accuracy: 88.24%\n",
      "Batch 22, Loss: 0.876576, Accuracy: 88.14%\n",
      "Batch 23, Loss: 0.925982, Accuracy: 87.91%\n",
      "Batch 24, Loss: 0.893223, Accuracy: 87.83%\n",
      "Batch 25, Loss: 0.826324, Accuracy: 88.00%\n",
      "Batch 26, Loss: 0.902590, Accuracy: 87.80%\n",
      "Batch 27, Loss: 0.780946, Accuracy: 88.19%\n",
      "Batch 28, Loss: 0.886573, Accuracy: 88.11%\n",
      "Batch 29, Loss: 0.874737, Accuracy: 88.09%\n",
      "Batch 30, Loss: 0.831477, Accuracy: 88.23%\n",
      "Batch 31, Loss: 0.833816, Accuracy: 88.36%\n",
      "Batch 32, Loss: 0.870959, Accuracy: 88.38%\n",
      "Batch 33, Loss: 0.839140, Accuracy: 88.45%\n",
      "Batch 34, Loss: 0.935108, Accuracy: 88.24%\n",
      "Batch 35, Loss: 0.826098, Accuracy: 88.39%\n",
      "Batch 36, Loss: 0.926892, Accuracy: 88.24%\n",
      "Batch 37, Loss: 0.892259, Accuracy: 88.13%\n",
      "Batch 38, Loss: 0.908681, Accuracy: 88.03%\n",
      "Batch 39, Loss: 0.900223, Accuracy: 87.90%\n",
      "Batch 40, Loss: 0.884075, Accuracy: 87.81%\n",
      "Batch 41, Loss: 0.892665, Accuracy: 87.73%\n",
      "Batch 42, Loss: 0.892645, Accuracy: 87.69%\n",
      "Batch 43, Loss: 0.911436, Accuracy: 87.61%\n",
      "Batch 44, Loss: 0.876417, Accuracy: 87.57%\n",
      "Batch 45, Loss: 0.894384, Accuracy: 87.57%\n",
      "Batch 46, Loss: 0.823786, Accuracy: 87.67%\n",
      "Batch 47, Loss: 0.903645, Accuracy: 87.60%\n",
      "Batch 48, Loss: 0.855031, Accuracy: 87.63%\n",
      "Batch 49, Loss: 0.878930, Accuracy: 87.63%\n",
      "Batch 50, Loss: 0.891999, Accuracy: 87.59%\n",
      "Batch 51, Loss: 0.856711, Accuracy: 87.62%\n",
      "Batch 52, Loss: 0.883689, Accuracy: 87.59%\n",
      "Batch 53, Loss: 0.814948, Accuracy: 87.71%\n",
      "Batch 54, Loss: 0.863138, Accuracy: 87.70%\n",
      "Batch 55, Loss: 0.830490, Accuracy: 87.78%\n",
      "Batch 56, Loss: 0.879080, Accuracy: 87.75%\n",
      "Batch 57, Loss: 0.835603, Accuracy: 87.80%\n",
      "Batch 58, Loss: 0.864361, Accuracy: 87.82%\n",
      "Batch 59, Loss: 0.864436, Accuracy: 87.84%\n",
      "Batch 60, Loss: 0.873940, Accuracy: 87.81%\n",
      "Batch 61, Loss: 0.900482, Accuracy: 87.76%\n",
      "Batch 62, Loss: 0.904155, Accuracy: 87.68%\n",
      "Batch 63, Loss: 0.936147, Accuracy: 87.57%\n",
      "Batch 64, Loss: 0.922402, Accuracy: 87.48%\n",
      "Batch 65, Loss: 0.836907, Accuracy: 87.55%\n",
      "Batch 66, Loss: 0.930936, Accuracy: 87.45%\n",
      "Batch 67, Loss: 0.918011, Accuracy: 87.38%\n",
      "Batch 68, Loss: 0.883173, Accuracy: 87.36%\n",
      "Batch 69, Loss: 0.918970, Accuracy: 87.32%\n",
      "Batch 70, Loss: 0.858480, Accuracy: 87.34%\n",
      "Batch 71, Loss: 0.924569, Accuracy: 87.26%\n",
      "Batch 72, Loss: 0.886239, Accuracy: 87.26%\n",
      "Batch 73, Loss: 0.913429, Accuracy: 87.22%\n",
      "Batch 74, Loss: 0.889500, Accuracy: 87.18%\n",
      "Batch 75, Loss: 0.823314, Accuracy: 87.25%\n",
      "Batch 76, Loss: 0.839363, Accuracy: 87.29%\n",
      "Batch 77, Loss: 0.873791, Accuracy: 87.28%\n",
      "Batch 78, Loss: 0.903244, Accuracy: 87.26%\n",
      "Batch 79, Loss: 0.837467, Accuracy: 87.30%\n",
      "Batch 80, Loss: 0.851112, Accuracy: 87.32%\n",
      "Batch 81, Loss: 0.896124, Accuracy: 87.29%\n",
      "Batch 82, Loss: 0.868857, Accuracy: 87.29%\n",
      "Batch 83, Loss: 0.823317, Accuracy: 87.35%\n",
      "Batch 84, Loss: 0.817021, Accuracy: 87.41%\n",
      "Batch 85, Loss: 0.923798, Accuracy: 87.33%\n",
      "Batch 86, Loss: 0.882441, Accuracy: 87.30%\n",
      "Batch 87, Loss: 0.896870, Accuracy: 87.27%\n",
      "Batch 88, Loss: 0.837544, Accuracy: 87.30%\n",
      "Batch 89, Loss: 0.795136, Accuracy: 87.39%\n",
      "Batch 90, Loss: 0.810183, Accuracy: 87.47%\n",
      "Batch 91, Loss: 0.901039, Accuracy: 87.41%\n",
      "Batch 92, Loss: 0.886724, Accuracy: 87.40%\n",
      "Batch 93, Loss: 0.898290, Accuracy: 87.37%\n",
      "Batch 94, Loss: 0.909499, Accuracy: 87.35%\n",
      "Batch 95, Loss: 0.830932, Accuracy: 87.40%\n",
      "Batch 96, Loss: 0.921992, Accuracy: 87.35%\n",
      "Batch 97, Loss: 0.868867, Accuracy: 87.36%\n",
      "Batch 98, Loss: 0.931041, Accuracy: 87.29%\n",
      "Batch 99, Loss: 0.822891, Accuracy: 87.33%\n",
      "Batch 100, Loss: 0.866833, Accuracy: 87.33%\n",
      "Batch 101, Loss: 0.795448, Accuracy: 87.41%\n",
      "Batch 102, Loss: 0.834376, Accuracy: 87.44%\n",
      "Batch 103, Loss: 0.861346, Accuracy: 87.45%\n",
      "Batch 104, Loss: 0.794377, Accuracy: 87.52%\n",
      "Batch 105, Loss: 0.862624, Accuracy: 87.51%\n",
      "Batch 106, Loss: 0.833303, Accuracy: 87.56%\n",
      "Batch 107, Loss: 0.864908, Accuracy: 87.57%\n",
      "Batch 108, Loss: 0.893416, Accuracy: 87.56%\n",
      "Batch 109, Loss: 0.830778, Accuracy: 87.60%\n",
      "Batch 110, Loss: 0.869774, Accuracy: 87.61%\n",
      "Batch 111, Loss: 0.855676, Accuracy: 87.63%\n",
      "Batch 112, Loss: 0.908332, Accuracy: 87.58%\n",
      "Batch 113, Loss: 0.883130, Accuracy: 87.57%\n",
      "Batch 114, Loss: 0.918499, Accuracy: 87.53%\n",
      "Batch 115, Loss: 0.935397, Accuracy: 87.47%\n",
      "Batch 116, Loss: 0.904501, Accuracy: 87.42%\n",
      "Batch 117, Loss: 0.794521, Accuracy: 87.50%\n",
      "Batch 118, Loss: 0.923404, Accuracy: 87.46%\n",
      "Batch 119, Loss: 0.835090, Accuracy: 87.49%\n",
      "Batch 120, Loss: 0.882474, Accuracy: 87.46%\n",
      "Batch 121, Loss: 0.871103, Accuracy: 87.46%\n",
      "Batch 122, Loss: 0.822180, Accuracy: 87.50%\n",
      "Batch 123, Loss: 0.837254, Accuracy: 87.53%\n",
      "Batch 124, Loss: 0.920494, Accuracy: 87.47%\n",
      "Batch 125, Loss: 0.867760, Accuracy: 87.47%\n",
      "Batch 126, Loss: 0.796619, Accuracy: 87.54%\n",
      "Batch 127, Loss: 0.959795, Accuracy: 87.46%\n",
      "Batch 128, Loss: 0.813487, Accuracy: 87.52%\n",
      "Batch 129, Loss: 0.819461, Accuracy: 87.57%\n",
      "Batch 130, Loss: 0.881118, Accuracy: 87.56%\n",
      "Batch 131, Loss: 0.833543, Accuracy: 87.58%\n",
      "Batch 132, Loss: 0.855197, Accuracy: 87.58%\n",
      "Batch 133, Loss: 0.861950, Accuracy: 87.59%\n",
      "Batch 134, Loss: 0.875639, Accuracy: 87.60%\n",
      "Batch 135, Loss: 0.847924, Accuracy: 87.62%\n",
      "Batch 136, Loss: 0.821737, Accuracy: 87.65%\n",
      "Batch 137, Loss: 0.896310, Accuracy: 87.61%\n",
      "Batch 138, Loss: 0.865458, Accuracy: 87.61%\n",
      "Batch 139, Loss: 0.909298, Accuracy: 87.59%\n",
      "Batch 140, Loss: 0.916602, Accuracy: 87.53%\n",
      "Batch 141, Loss: 0.883205, Accuracy: 87.52%\n",
      "Batch 142, Loss: 0.838407, Accuracy: 87.54%\n",
      "Batch 143, Loss: 0.882057, Accuracy: 87.54%\n",
      "Batch 144, Loss: 0.911025, Accuracy: 87.51%\n",
      "Batch 145, Loss: 0.901786, Accuracy: 87.48%\n",
      "Batch 146, Loss: 0.873761, Accuracy: 87.48%\n",
      "Batch 147, Loss: 0.866959, Accuracy: 87.48%\n",
      "Batch 148, Loss: 0.890082, Accuracy: 87.47%\n",
      "Batch 149, Loss: 0.915193, Accuracy: 87.44%\n",
      "Batch 150, Loss: 0.922184, Accuracy: 87.40%\n",
      "Batch 151, Loss: 0.858037, Accuracy: 87.42%\n",
      "Batch 152, Loss: 0.907322, Accuracy: 87.40%\n",
      "Batch 153, Loss: 0.922595, Accuracy: 87.36%\n",
      "Batch 154, Loss: 0.926767, Accuracy: 87.32%\n",
      "Batch 155, Loss: 0.891807, Accuracy: 87.31%\n",
      "Batch 156, Loss: 0.863422, Accuracy: 87.31%\n",
      "Batch 157, Loss: 0.868725, Accuracy: 87.31%\n",
      "Batch 158, Loss: 0.818233, Accuracy: 87.35%\n",
      "Batch 159, Loss: 0.838876, Accuracy: 87.37%\n",
      "Batch 160, Loss: 0.879950, Accuracy: 87.36%\n",
      "Batch 161, Loss: 0.908541, Accuracy: 87.34%\n",
      "Batch 162, Loss: 0.830600, Accuracy: 87.36%\n",
      "Batch 163, Loss: 0.859016, Accuracy: 87.37%\n",
      "Batch 164, Loss: 0.806117, Accuracy: 87.40%\n",
      "Batch 165, Loss: 0.912311, Accuracy: 87.38%\n",
      "Batch 166, Loss: 0.847100, Accuracy: 87.40%\n",
      "Batch 167, Loss: 0.799833, Accuracy: 87.44%\n",
      "Batch 168, Loss: 0.872738, Accuracy: 87.45%\n",
      "Batch 169, Loss: 0.846994, Accuracy: 87.46%\n",
      "Batch 170, Loss: 0.886557, Accuracy: 87.45%\n",
      "Batch 171, Loss: 0.850849, Accuracy: 87.45%\n",
      "Batch 172, Loss: 0.808468, Accuracy: 87.49%\n",
      "Batch 173, Loss: 0.817128, Accuracy: 87.52%\n",
      "Batch 174, Loss: 0.955355, Accuracy: 87.46%\n",
      "Batch 175, Loss: 0.812317, Accuracy: 87.50%\n",
      "Batch 176, Loss: 0.851661, Accuracy: 87.51%\n",
      "Batch 177, Loss: 0.953844, Accuracy: 87.46%\n",
      "Batch 178, Loss: 0.986234, Accuracy: 87.40%\n",
      "Batch 179, Loss: 0.862059, Accuracy: 87.40%\n",
      "Batch 180, Loss: 0.860233, Accuracy: 87.41%\n",
      "Batch 181, Loss: 0.863490, Accuracy: 87.41%\n",
      "Batch 182, Loss: 0.846054, Accuracy: 87.43%\n",
      "Batch 183, Loss: 0.884381, Accuracy: 87.42%\n",
      "Batch 184, Loss: 0.945134, Accuracy: 87.38%\n",
      "Batch 185, Loss: 0.885949, Accuracy: 87.37%\n",
      "Batch 186, Loss: 0.816604, Accuracy: 87.40%\n",
      "Batch 187, Loss: 0.906666, Accuracy: 87.38%\n",
      "Batch 188, Loss: 0.860379, Accuracy: 87.39%\n",
      "Batch 189, Loss: 0.856471, Accuracy: 87.40%\n",
      "Batch 190, Loss: 0.894941, Accuracy: 87.39%\n",
      "Batch 191, Loss: 0.876726, Accuracy: 87.39%\n",
      "Batch 192, Loss: 0.869149, Accuracy: 87.39%\n",
      "Batch 193, Loss: 0.892080, Accuracy: 87.38%\n",
      "Batch 194, Loss: 0.930296, Accuracy: 87.35%\n",
      "Batch 195, Loss: 0.841117, Accuracy: 87.36%\n",
      "Batch 196, Loss: 0.829600, Accuracy: 87.38%\n",
      "Batch 197, Loss: 0.864462, Accuracy: 87.38%\n",
      "Batch 198, Loss: 0.841404, Accuracy: 87.40%\n",
      "Batch 199, Loss: 0.883080, Accuracy: 87.38%\n",
      "Batch 200, Loss: 0.848059, Accuracy: 87.39%\n",
      "Batch 201, Loss: 0.788210, Accuracy: 87.44%\n",
      "Batch 202, Loss: 0.909050, Accuracy: 87.42%\n",
      "Batch 203, Loss: 0.878082, Accuracy: 87.42%\n",
      "Batch 204, Loss: 0.908158, Accuracy: 87.40%\n",
      "Batch 205, Loss: 0.846495, Accuracy: 87.40%\n",
      "Batch 206, Loss: 0.872274, Accuracy: 87.40%\n",
      "Batch 207, Loss: 0.864530, Accuracy: 87.40%\n",
      "Batch 208, Loss: 0.849233, Accuracy: 87.42%\n",
      "Batch 209, Loss: 0.842221, Accuracy: 87.43%\n",
      "Batch 210, Loss: 0.927895, Accuracy: 87.41%\n",
      "Batch 211, Loss: 0.929997, Accuracy: 87.39%\n",
      "Batch 212, Loss: 0.886329, Accuracy: 87.38%\n",
      "Batch 213, Loss: 0.844300, Accuracy: 87.40%\n",
      "Training - Epoch 93, Loss: 0.871052, Accuracy: 87.40%\n",
      "Validation Batch 1, Loss: 0.823672, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.803894, Accuracy: 93.75%\n",
      "Validation Batch 3, Loss: 0.861287, Accuracy: 91.67%\n",
      "Validation Batch 4, Loss: 0.844185, Accuracy: 91.41%\n",
      "Validation Batch 5, Loss: 0.827211, Accuracy: 91.56%\n",
      "Validation Batch 6, Loss: 0.796819, Accuracy: 92.19%\n",
      "Validation Batch 7, Loss: 0.806776, Accuracy: 92.63%\n",
      "Validation Batch 8, Loss: 0.897840, Accuracy: 91.60%\n",
      "Validation Batch 9, Loss: 0.903270, Accuracy: 90.62%\n",
      "Validation Batch 10, Loss: 0.822406, Accuracy: 90.78%\n",
      "Validation Batch 11, Loss: 0.837208, Accuracy: 90.91%\n",
      "Validation Batch 12, Loss: 0.836768, Accuracy: 91.02%\n",
      "Validation Batch 13, Loss: 0.842132, Accuracy: 90.87%\n",
      "Validation Batch 14, Loss: 0.851612, Accuracy: 90.85%\n",
      "Validation Batch 15, Loss: 0.820787, Accuracy: 90.94%\n",
      "Validation Batch 16, Loss: 0.832179, Accuracy: 91.02%\n",
      "Validation Batch 17, Loss: 0.888162, Accuracy: 90.72%\n",
      "Validation Batch 18, Loss: 0.817734, Accuracy: 90.71%\n",
      "Validation Batch 19, Loss: 0.884030, Accuracy: 90.46%\n",
      "Validation Batch 20, Loss: 0.881447, Accuracy: 90.31%\n",
      "Validation Batch 21, Loss: 0.867905, Accuracy: 90.18%\n",
      "Validation Batch 22, Loss: 0.848625, Accuracy: 90.13%\n",
      "Validation Batch 23, Loss: 0.870733, Accuracy: 90.01%\n",
      "Validation Batch 24, Loss: 0.848753, Accuracy: 90.04%\n",
      "Validation Batch 25, Loss: 0.822854, Accuracy: 90.06%\n",
      "Validation Batch 26, Loss: 0.853489, Accuracy: 90.02%\n",
      "Validation Batch 27, Loss: 0.798802, Accuracy: 90.14%\n",
      "Validation - Epoch 93, Loss: 0.844095, Accuracy: 90.14%\n",
      "Patience—0\n",
      "Epoch 94\n",
      "Batch 1, Loss: 0.873820, Accuracy: 84.38%\n",
      "Batch 2, Loss: 0.890199, Accuracy: 83.59%\n",
      "Batch 3, Loss: 0.885396, Accuracy: 84.38%\n",
      "Batch 4, Loss: 0.842812, Accuracy: 86.33%\n",
      "Batch 5, Loss: 0.847090, Accuracy: 87.19%\n",
      "Batch 6, Loss: 0.807808, Accuracy: 88.28%\n",
      "Batch 7, Loss: 0.837007, Accuracy: 88.39%\n",
      "Batch 8, Loss: 0.820487, Accuracy: 88.87%\n",
      "Batch 9, Loss: 0.842168, Accuracy: 89.41%\n",
      "Batch 10, Loss: 0.839119, Accuracy: 89.53%\n",
      "Batch 11, Loss: 0.853713, Accuracy: 89.49%\n",
      "Batch 12, Loss: 0.895439, Accuracy: 89.06%\n",
      "Batch 13, Loss: 0.848802, Accuracy: 88.94%\n",
      "Batch 14, Loss: 0.871017, Accuracy: 88.73%\n",
      "Batch 15, Loss: 0.860004, Accuracy: 88.85%\n",
      "Batch 16, Loss: 0.848042, Accuracy: 88.96%\n",
      "Batch 17, Loss: 0.851969, Accuracy: 89.06%\n",
      "Batch 18, Loss: 0.801175, Accuracy: 89.41%\n",
      "Batch 19, Loss: 0.942558, Accuracy: 88.82%\n",
      "Batch 20, Loss: 0.894023, Accuracy: 88.59%\n",
      "Batch 21, Loss: 0.873851, Accuracy: 88.54%\n",
      "Batch 22, Loss: 0.812827, Accuracy: 88.78%\n",
      "Batch 23, Loss: 0.849632, Accuracy: 88.79%\n",
      "Batch 24, Loss: 0.788627, Accuracy: 89.06%\n",
      "Batch 25, Loss: 0.819326, Accuracy: 89.25%\n",
      "Batch 26, Loss: 0.841552, Accuracy: 89.24%\n",
      "Batch 27, Loss: 0.911038, Accuracy: 89.06%\n",
      "Batch 28, Loss: 0.888098, Accuracy: 88.90%\n",
      "Batch 29, Loss: 0.866617, Accuracy: 88.85%\n",
      "Batch 30, Loss: 0.895379, Accuracy: 88.70%\n",
      "Batch 31, Loss: 0.851117, Accuracy: 88.71%\n",
      "Batch 32, Loss: 0.979413, Accuracy: 88.33%\n",
      "Batch 33, Loss: 0.832959, Accuracy: 88.40%\n",
      "Batch 34, Loss: 0.889713, Accuracy: 88.33%\n",
      "Batch 35, Loss: 0.899964, Accuracy: 88.21%\n",
      "Batch 36, Loss: 0.842167, Accuracy: 88.28%\n",
      "Batch 37, Loss: 0.857136, Accuracy: 88.26%\n",
      "Batch 38, Loss: 0.822662, Accuracy: 88.36%\n",
      "Batch 39, Loss: 0.850958, Accuracy: 88.34%\n",
      "Batch 40, Loss: 0.830310, Accuracy: 88.44%\n",
      "Batch 41, Loss: 0.859865, Accuracy: 88.45%\n",
      "Batch 42, Loss: 0.844023, Accuracy: 88.50%\n",
      "Batch 43, Loss: 0.893981, Accuracy: 88.44%\n",
      "Batch 44, Loss: 0.867640, Accuracy: 88.46%\n",
      "Batch 45, Loss: 0.910800, Accuracy: 88.33%\n",
      "Batch 46, Loss: 0.900490, Accuracy: 88.21%\n",
      "Batch 47, Loss: 0.856549, Accuracy: 88.23%\n",
      "Batch 48, Loss: 0.800593, Accuracy: 88.38%\n",
      "Batch 49, Loss: 0.887109, Accuracy: 88.30%\n",
      "Batch 50, Loss: 0.830866, Accuracy: 88.38%\n",
      "Batch 51, Loss: 0.880948, Accuracy: 88.33%\n",
      "Batch 52, Loss: 0.892349, Accuracy: 88.25%\n",
      "Batch 53, Loss: 0.842102, Accuracy: 88.30%\n",
      "Batch 54, Loss: 0.879621, Accuracy: 88.25%\n",
      "Batch 55, Loss: 0.870023, Accuracy: 88.24%\n",
      "Batch 56, Loss: 0.841598, Accuracy: 88.28%\n",
      "Batch 57, Loss: 0.830194, Accuracy: 88.35%\n",
      "Batch 58, Loss: 0.863638, Accuracy: 88.34%\n",
      "Batch 59, Loss: 0.888707, Accuracy: 88.32%\n",
      "Batch 60, Loss: 0.888513, Accuracy: 88.28%\n",
      "Batch 61, Loss: 0.836761, Accuracy: 88.32%\n",
      "Batch 62, Loss: 0.933227, Accuracy: 88.16%\n",
      "Batch 63, Loss: 0.927267, Accuracy: 88.07%\n",
      "Batch 64, Loss: 0.845828, Accuracy: 88.11%\n",
      "Batch 65, Loss: 0.884158, Accuracy: 88.08%\n",
      "Batch 66, Loss: 0.868346, Accuracy: 88.09%\n",
      "Batch 67, Loss: 0.898913, Accuracy: 87.99%\n",
      "Batch 68, Loss: 0.843507, Accuracy: 88.03%\n",
      "Batch 69, Loss: 0.943694, Accuracy: 87.93%\n",
      "Batch 70, Loss: 0.811151, Accuracy: 88.01%\n",
      "Batch 71, Loss: 0.888902, Accuracy: 87.98%\n",
      "Batch 72, Loss: 0.908689, Accuracy: 87.91%\n",
      "Batch 73, Loss: 0.800691, Accuracy: 87.99%\n",
      "Batch 74, Loss: 0.854176, Accuracy: 87.99%\n",
      "Batch 75, Loss: 0.871899, Accuracy: 87.98%\n",
      "Batch 76, Loss: 0.823079, Accuracy: 88.01%\n",
      "Batch 77, Loss: 0.873767, Accuracy: 87.97%\n",
      "Batch 78, Loss: 0.874941, Accuracy: 87.96%\n",
      "Batch 79, Loss: 0.891468, Accuracy: 87.94%\n",
      "Batch 80, Loss: 0.859836, Accuracy: 87.95%\n",
      "Batch 81, Loss: 0.884222, Accuracy: 87.92%\n",
      "Batch 82, Loss: 0.859933, Accuracy: 87.92%\n",
      "Batch 83, Loss: 0.854010, Accuracy: 87.91%\n",
      "Batch 84, Loss: 0.825616, Accuracy: 87.97%\n",
      "Batch 85, Loss: 0.877438, Accuracy: 87.94%\n",
      "Batch 86, Loss: 0.844144, Accuracy: 87.97%\n",
      "Batch 87, Loss: 0.833517, Accuracy: 88.02%\n",
      "Batch 88, Loss: 0.865806, Accuracy: 88.00%\n",
      "Batch 89, Loss: 0.809235, Accuracy: 88.08%\n",
      "Batch 90, Loss: 0.875293, Accuracy: 88.06%\n",
      "Batch 91, Loss: 0.863633, Accuracy: 88.05%\n",
      "Batch 92, Loss: 0.864120, Accuracy: 88.06%\n",
      "Batch 93, Loss: 0.950212, Accuracy: 87.95%\n",
      "Batch 94, Loss: 0.846192, Accuracy: 87.97%\n",
      "Batch 95, Loss: 0.801060, Accuracy: 88.04%\n",
      "Batch 96, Loss: 0.885400, Accuracy: 88.02%\n",
      "Batch 97, Loss: 0.795994, Accuracy: 88.10%\n",
      "Batch 98, Loss: 0.886550, Accuracy: 88.06%\n",
      "Batch 99, Loss: 0.869660, Accuracy: 88.05%\n",
      "Batch 100, Loss: 0.908657, Accuracy: 88.02%\n",
      "Batch 101, Loss: 0.861524, Accuracy: 88.03%\n",
      "Batch 102, Loss: 0.852185, Accuracy: 88.05%\n",
      "Batch 103, Loss: 0.903979, Accuracy: 88.00%\n",
      "Batch 104, Loss: 0.942882, Accuracy: 87.92%\n",
      "Batch 105, Loss: 0.811485, Accuracy: 87.98%\n",
      "Batch 106, Loss: 0.908326, Accuracy: 87.94%\n",
      "Batch 107, Loss: 0.860655, Accuracy: 87.94%\n",
      "Batch 108, Loss: 0.884945, Accuracy: 87.92%\n",
      "Batch 109, Loss: 0.913804, Accuracy: 87.87%\n",
      "Batch 110, Loss: 0.914826, Accuracy: 87.86%\n",
      "Batch 111, Loss: 0.927962, Accuracy: 87.80%\n",
      "Batch 112, Loss: 0.855272, Accuracy: 87.79%\n",
      "Batch 113, Loss: 0.932062, Accuracy: 87.72%\n",
      "Batch 114, Loss: 0.861569, Accuracy: 87.72%\n",
      "Batch 115, Loss: 0.866062, Accuracy: 87.72%\n",
      "Batch 116, Loss: 0.916600, Accuracy: 87.68%\n",
      "Batch 117, Loss: 0.900790, Accuracy: 87.65%\n",
      "Batch 118, Loss: 0.821597, Accuracy: 87.70%\n",
      "Batch 119, Loss: 0.884968, Accuracy: 87.68%\n",
      "Batch 120, Loss: 0.848027, Accuracy: 87.71%\n",
      "Batch 121, Loss: 0.920854, Accuracy: 87.65%\n",
      "Batch 122, Loss: 0.857430, Accuracy: 87.67%\n",
      "Batch 123, Loss: 0.888471, Accuracy: 87.65%\n",
      "Batch 124, Loss: 0.923225, Accuracy: 87.61%\n",
      "Batch 125, Loss: 0.858853, Accuracy: 87.64%\n",
      "Batch 126, Loss: 0.873336, Accuracy: 87.64%\n",
      "Batch 127, Loss: 0.915923, Accuracy: 87.60%\n",
      "Batch 128, Loss: 0.849497, Accuracy: 87.61%\n",
      "Batch 129, Loss: 0.880576, Accuracy: 87.61%\n",
      "Batch 130, Loss: 0.900007, Accuracy: 87.58%\n",
      "Batch 131, Loss: 0.818266, Accuracy: 87.63%\n",
      "Batch 132, Loss: 0.842580, Accuracy: 87.65%\n",
      "Batch 133, Loss: 0.886196, Accuracy: 87.64%\n",
      "Batch 134, Loss: 0.918115, Accuracy: 87.60%\n",
      "Batch 135, Loss: 0.849901, Accuracy: 87.62%\n",
      "Batch 136, Loss: 0.885068, Accuracy: 87.60%\n",
      "Batch 137, Loss: 0.846012, Accuracy: 87.61%\n",
      "Batch 138, Loss: 0.879353, Accuracy: 87.60%\n",
      "Batch 139, Loss: 0.879254, Accuracy: 87.59%\n",
      "Batch 140, Loss: 0.894462, Accuracy: 87.57%\n",
      "Batch 141, Loss: 0.887671, Accuracy: 87.56%\n",
      "Batch 142, Loss: 0.893944, Accuracy: 87.53%\n",
      "Batch 143, Loss: 0.851837, Accuracy: 87.55%\n",
      "Batch 144, Loss: 0.847757, Accuracy: 87.57%\n",
      "Batch 145, Loss: 0.933269, Accuracy: 87.51%\n",
      "Batch 146, Loss: 0.836867, Accuracy: 87.54%\n",
      "Batch 147, Loss: 0.890108, Accuracy: 87.53%\n",
      "Batch 148, Loss: 0.894252, Accuracy: 87.52%\n",
      "Batch 149, Loss: 0.902938, Accuracy: 87.49%\n",
      "Batch 150, Loss: 0.852981, Accuracy: 87.50%\n",
      "Batch 151, Loss: 0.817289, Accuracy: 87.54%\n",
      "Batch 152, Loss: 0.911797, Accuracy: 87.51%\n",
      "Batch 153, Loss: 0.916316, Accuracy: 87.48%\n",
      "Batch 154, Loss: 0.928712, Accuracy: 87.43%\n",
      "Batch 155, Loss: 0.897237, Accuracy: 87.41%\n",
      "Batch 156, Loss: 0.880780, Accuracy: 87.40%\n",
      "Batch 157, Loss: 0.920183, Accuracy: 87.37%\n",
      "Batch 158, Loss: 0.820155, Accuracy: 87.40%\n",
      "Batch 159, Loss: 0.871575, Accuracy: 87.39%\n",
      "Batch 160, Loss: 0.878000, Accuracy: 87.39%\n",
      "Batch 161, Loss: 0.908668, Accuracy: 87.36%\n",
      "Batch 162, Loss: 0.883421, Accuracy: 87.36%\n",
      "Batch 163, Loss: 0.896743, Accuracy: 87.33%\n",
      "Batch 164, Loss: 0.855980, Accuracy: 87.34%\n",
      "Batch 165, Loss: 0.908951, Accuracy: 87.31%\n",
      "Batch 166, Loss: 0.865676, Accuracy: 87.31%\n",
      "Batch 167, Loss: 0.867646, Accuracy: 87.32%\n",
      "Batch 168, Loss: 0.872193, Accuracy: 87.33%\n",
      "Batch 169, Loss: 0.811124, Accuracy: 87.37%\n",
      "Batch 170, Loss: 0.853076, Accuracy: 87.39%\n",
      "Batch 171, Loss: 0.833412, Accuracy: 87.41%\n",
      "Batch 172, Loss: 0.825198, Accuracy: 87.44%\n",
      "Batch 173, Loss: 0.855475, Accuracy: 87.45%\n",
      "Batch 174, Loss: 0.902274, Accuracy: 87.42%\n",
      "Batch 175, Loss: 0.825134, Accuracy: 87.45%\n",
      "Batch 176, Loss: 0.917266, Accuracy: 87.42%\n",
      "Batch 177, Loss: 0.955765, Accuracy: 87.37%\n",
      "Batch 178, Loss: 0.855790, Accuracy: 87.37%\n",
      "Batch 179, Loss: 0.907308, Accuracy: 87.34%\n",
      "Batch 180, Loss: 0.850155, Accuracy: 87.36%\n",
      "Batch 181, Loss: 0.900856, Accuracy: 87.34%\n",
      "Batch 182, Loss: 0.822975, Accuracy: 87.37%\n",
      "Batch 183, Loss: 0.860543, Accuracy: 87.38%\n",
      "Batch 184, Loss: 0.847714, Accuracy: 87.39%\n",
      "Batch 185, Loss: 0.881490, Accuracy: 87.38%\n",
      "Batch 186, Loss: 0.887960, Accuracy: 87.37%\n",
      "Batch 187, Loss: 0.923488, Accuracy: 87.35%\n",
      "Batch 188, Loss: 0.855665, Accuracy: 87.35%\n",
      "Batch 189, Loss: 0.877831, Accuracy: 87.35%\n",
      "Batch 190, Loss: 0.872362, Accuracy: 87.35%\n",
      "Batch 191, Loss: 0.820732, Accuracy: 87.39%\n",
      "Batch 192, Loss: 0.864733, Accuracy: 87.39%\n",
      "Batch 193, Loss: 0.924884, Accuracy: 87.35%\n",
      "Batch 194, Loss: 0.858248, Accuracy: 87.35%\n",
      "Batch 195, Loss: 0.865451, Accuracy: 87.35%\n",
      "Batch 196, Loss: 0.866424, Accuracy: 87.36%\n",
      "Batch 197, Loss: 0.855385, Accuracy: 87.37%\n",
      "Batch 198, Loss: 0.856968, Accuracy: 87.37%\n",
      "Batch 199, Loss: 0.838403, Accuracy: 87.38%\n",
      "Batch 200, Loss: 0.850840, Accuracy: 87.39%\n",
      "Batch 201, Loss: 0.882505, Accuracy: 87.38%\n",
      "Batch 202, Loss: 0.883563, Accuracy: 87.37%\n",
      "Batch 203, Loss: 0.879757, Accuracy: 87.36%\n",
      "Batch 204, Loss: 0.805686, Accuracy: 87.39%\n",
      "Batch 205, Loss: 0.888008, Accuracy: 87.38%\n",
      "Batch 206, Loss: 0.836468, Accuracy: 87.40%\n",
      "Batch 207, Loss: 0.805248, Accuracy: 87.43%\n",
      "Batch 208, Loss: 0.850917, Accuracy: 87.44%\n",
      "Batch 209, Loss: 0.893220, Accuracy: 87.43%\n",
      "Batch 210, Loss: 0.820344, Accuracy: 87.46%\n",
      "Batch 211, Loss: 0.871780, Accuracy: 87.46%\n",
      "Batch 212, Loss: 0.912431, Accuracy: 87.43%\n",
      "Batch 213, Loss: 0.782674, Accuracy: 87.48%\n",
      "Training - Epoch 94, Loss: 0.868986, Accuracy: 87.48%\n",
      "Validation Batch 1, Loss: 0.827161, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.805858, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.869561, Accuracy: 92.19%\n",
      "Validation Batch 4, Loss: 0.844142, Accuracy: 91.80%\n",
      "Validation Batch 5, Loss: 0.837352, Accuracy: 91.56%\n",
      "Validation Batch 6, Loss: 0.804630, Accuracy: 91.93%\n",
      "Validation Batch 7, Loss: 0.806861, Accuracy: 92.41%\n",
      "Validation Batch 8, Loss: 0.898833, Accuracy: 91.60%\n",
      "Validation Batch 9, Loss: 0.900097, Accuracy: 90.80%\n",
      "Validation Batch 10, Loss: 0.829350, Accuracy: 90.94%\n",
      "Validation Batch 11, Loss: 0.839118, Accuracy: 90.91%\n",
      "Validation Batch 12, Loss: 0.832037, Accuracy: 91.02%\n",
      "Validation Batch 13, Loss: 0.853125, Accuracy: 90.87%\n",
      "Validation Batch 14, Loss: 0.854581, Accuracy: 90.62%\n",
      "Validation Batch 15, Loss: 0.825024, Accuracy: 90.73%\n",
      "Validation Batch 16, Loss: 0.840364, Accuracy: 90.72%\n",
      "Validation Batch 17, Loss: 0.890914, Accuracy: 90.35%\n",
      "Validation Batch 18, Loss: 0.822510, Accuracy: 90.36%\n",
      "Validation Batch 19, Loss: 0.895613, Accuracy: 90.05%\n",
      "Validation Batch 20, Loss: 0.886938, Accuracy: 89.77%\n",
      "Validation Batch 21, Loss: 0.875235, Accuracy: 89.58%\n",
      "Validation Batch 22, Loss: 0.855495, Accuracy: 89.49%\n",
      "Validation Batch 23, Loss: 0.879214, Accuracy: 89.40%\n",
      "Validation Batch 24, Loss: 0.863640, Accuracy: 89.39%\n",
      "Validation Batch 25, Loss: 0.825583, Accuracy: 89.50%\n",
      "Validation Batch 26, Loss: 0.857100, Accuracy: 89.48%\n",
      "Validation Batch 27, Loss: 0.812173, Accuracy: 89.61%\n",
      "Validation - Epoch 94, Loss: 0.849352, Accuracy: 89.61%\n",
      "Patience—1\n",
      "Epoch 95\n",
      "Batch 1, Loss: 0.867056, Accuracy: 87.50%\n",
      "Batch 2, Loss: 0.847259, Accuracy: 88.28%\n",
      "Batch 3, Loss: 0.841814, Accuracy: 89.06%\n",
      "Batch 4, Loss: 0.824326, Accuracy: 90.23%\n",
      "Batch 5, Loss: 0.802038, Accuracy: 90.94%\n",
      "Batch 6, Loss: 0.820950, Accuracy: 91.15%\n",
      "Batch 7, Loss: 0.871398, Accuracy: 90.62%\n",
      "Batch 8, Loss: 0.820135, Accuracy: 90.82%\n",
      "Batch 9, Loss: 0.869171, Accuracy: 90.45%\n",
      "Batch 10, Loss: 0.891531, Accuracy: 90.00%\n",
      "Batch 11, Loss: 0.880119, Accuracy: 89.49%\n",
      "Batch 12, Loss: 0.889978, Accuracy: 89.32%\n",
      "Batch 13, Loss: 0.916049, Accuracy: 88.82%\n",
      "Batch 14, Loss: 0.811876, Accuracy: 89.17%\n",
      "Batch 15, Loss: 0.924535, Accuracy: 88.65%\n",
      "Batch 16, Loss: 0.881017, Accuracy: 88.57%\n",
      "Batch 17, Loss: 0.844411, Accuracy: 88.60%\n",
      "Batch 18, Loss: 0.909760, Accuracy: 88.28%\n",
      "Batch 19, Loss: 0.866499, Accuracy: 88.40%\n",
      "Batch 20, Loss: 0.838991, Accuracy: 88.52%\n",
      "Batch 21, Loss: 0.925034, Accuracy: 88.17%\n",
      "Batch 22, Loss: 0.853658, Accuracy: 88.21%\n",
      "Batch 23, Loss: 0.869283, Accuracy: 88.18%\n",
      "Batch 24, Loss: 0.891741, Accuracy: 88.15%\n",
      "Batch 25, Loss: 0.910580, Accuracy: 87.94%\n",
      "Batch 26, Loss: 0.846997, Accuracy: 88.04%\n",
      "Batch 27, Loss: 0.865983, Accuracy: 88.08%\n",
      "Batch 28, Loss: 0.879298, Accuracy: 88.00%\n",
      "Batch 29, Loss: 0.846808, Accuracy: 88.04%\n",
      "Batch 30, Loss: 0.820312, Accuracy: 88.18%\n",
      "Batch 31, Loss: 0.842988, Accuracy: 88.26%\n",
      "Batch 32, Loss: 0.836519, Accuracy: 88.33%\n",
      "Batch 33, Loss: 0.886863, Accuracy: 88.26%\n",
      "Batch 34, Loss: 0.835926, Accuracy: 88.33%\n",
      "Batch 35, Loss: 0.847267, Accuracy: 88.35%\n",
      "Batch 36, Loss: 0.925271, Accuracy: 88.15%\n",
      "Batch 37, Loss: 0.905864, Accuracy: 88.05%\n",
      "Batch 38, Loss: 0.839529, Accuracy: 88.16%\n",
      "Batch 39, Loss: 0.846510, Accuracy: 88.18%\n",
      "Batch 40, Loss: 0.893727, Accuracy: 88.09%\n",
      "Batch 41, Loss: 0.850667, Accuracy: 88.11%\n",
      "Batch 42, Loss: 0.926546, Accuracy: 87.95%\n",
      "Batch 43, Loss: 0.841706, Accuracy: 88.01%\n",
      "Batch 44, Loss: 0.830285, Accuracy: 88.10%\n",
      "Batch 45, Loss: 0.901129, Accuracy: 87.99%\n",
      "Batch 46, Loss: 0.795239, Accuracy: 88.18%\n",
      "Batch 47, Loss: 0.810939, Accuracy: 88.30%\n",
      "Batch 48, Loss: 0.959129, Accuracy: 88.12%\n",
      "Batch 49, Loss: 0.905088, Accuracy: 87.98%\n",
      "Batch 50, Loss: 0.887840, Accuracy: 87.94%\n",
      "Batch 51, Loss: 0.863078, Accuracy: 87.96%\n",
      "Batch 52, Loss: 0.834408, Accuracy: 88.01%\n",
      "Batch 53, Loss: 0.889953, Accuracy: 87.94%\n",
      "Batch 54, Loss: 0.842761, Accuracy: 87.99%\n",
      "Batch 55, Loss: 0.824852, Accuracy: 88.07%\n",
      "Batch 56, Loss: 0.868331, Accuracy: 88.06%\n",
      "Batch 57, Loss: 0.877473, Accuracy: 88.02%\n",
      "Batch 58, Loss: 0.876853, Accuracy: 88.01%\n",
      "Batch 59, Loss: 0.877849, Accuracy: 88.00%\n",
      "Batch 60, Loss: 0.845554, Accuracy: 88.05%\n",
      "Batch 61, Loss: 0.879689, Accuracy: 88.01%\n",
      "Batch 62, Loss: 0.853474, Accuracy: 88.03%\n",
      "Batch 63, Loss: 0.868744, Accuracy: 88.02%\n",
      "Batch 64, Loss: 0.824371, Accuracy: 88.09%\n",
      "Batch 65, Loss: 0.874317, Accuracy: 88.08%\n",
      "Batch 66, Loss: 0.825089, Accuracy: 88.16%\n",
      "Batch 67, Loss: 0.821781, Accuracy: 88.25%\n",
      "Batch 68, Loss: 0.912960, Accuracy: 88.17%\n",
      "Batch 69, Loss: 0.822889, Accuracy: 88.22%\n",
      "Batch 70, Loss: 0.946037, Accuracy: 88.08%\n",
      "Batch 71, Loss: 0.857335, Accuracy: 88.09%\n",
      "Batch 72, Loss: 0.909960, Accuracy: 88.04%\n",
      "Batch 73, Loss: 0.852895, Accuracy: 88.06%\n",
      "Batch 74, Loss: 0.898284, Accuracy: 88.01%\n",
      "Batch 75, Loss: 0.924646, Accuracy: 87.92%\n",
      "Batch 76, Loss: 0.860220, Accuracy: 87.93%\n",
      "Batch 77, Loss: 0.832255, Accuracy: 87.99%\n",
      "Batch 78, Loss: 0.865140, Accuracy: 87.98%\n",
      "Batch 79, Loss: 0.838941, Accuracy: 88.03%\n",
      "Batch 80, Loss: 0.815324, Accuracy: 88.11%\n",
      "Batch 81, Loss: 0.836787, Accuracy: 88.14%\n",
      "Batch 82, Loss: 0.888391, Accuracy: 88.11%\n",
      "Batch 83, Loss: 0.845354, Accuracy: 88.16%\n",
      "Batch 84, Loss: 0.857042, Accuracy: 88.17%\n",
      "Batch 85, Loss: 0.873804, Accuracy: 88.18%\n",
      "Batch 86, Loss: 0.961646, Accuracy: 88.05%\n",
      "Batch 87, Loss: 0.919993, Accuracy: 87.98%\n",
      "Batch 88, Loss: 0.913010, Accuracy: 87.93%\n",
      "Batch 89, Loss: 0.825793, Accuracy: 87.97%\n",
      "Batch 90, Loss: 0.911829, Accuracy: 87.93%\n",
      "Batch 91, Loss: 0.836528, Accuracy: 87.96%\n",
      "Batch 92, Loss: 0.846094, Accuracy: 87.99%\n",
      "Batch 93, Loss: 0.810280, Accuracy: 88.05%\n",
      "Batch 94, Loss: 0.830919, Accuracy: 88.07%\n",
      "Batch 95, Loss: 0.937947, Accuracy: 87.98%\n",
      "Batch 96, Loss: 0.826992, Accuracy: 88.02%\n",
      "Batch 97, Loss: 0.853860, Accuracy: 88.03%\n",
      "Batch 98, Loss: 0.921860, Accuracy: 87.98%\n",
      "Batch 99, Loss: 0.871113, Accuracy: 87.96%\n",
      "Batch 100, Loss: 0.909058, Accuracy: 87.91%\n",
      "Batch 101, Loss: 0.861702, Accuracy: 87.90%\n",
      "Batch 102, Loss: 0.891673, Accuracy: 87.88%\n",
      "Batch 103, Loss: 0.850751, Accuracy: 87.91%\n",
      "Batch 104, Loss: 0.838074, Accuracy: 87.94%\n",
      "Batch 105, Loss: 0.899832, Accuracy: 87.89%\n",
      "Batch 106, Loss: 0.834760, Accuracy: 87.91%\n",
      "Batch 107, Loss: 0.904203, Accuracy: 87.87%\n",
      "Batch 108, Loss: 0.950059, Accuracy: 87.77%\n",
      "Batch 109, Loss: 0.893639, Accuracy: 87.76%\n",
      "Batch 110, Loss: 0.919463, Accuracy: 87.70%\n",
      "Batch 111, Loss: 0.861679, Accuracy: 87.70%\n",
      "Batch 112, Loss: 0.863169, Accuracy: 87.70%\n",
      "Batch 113, Loss: 0.896755, Accuracy: 87.67%\n",
      "Batch 114, Loss: 0.915963, Accuracy: 87.62%\n",
      "Batch 115, Loss: 0.845917, Accuracy: 87.65%\n",
      "Batch 116, Loss: 0.891274, Accuracy: 87.62%\n",
      "Batch 117, Loss: 0.842973, Accuracy: 87.63%\n",
      "Batch 118, Loss: 0.849217, Accuracy: 87.66%\n",
      "Batch 119, Loss: 0.867828, Accuracy: 87.64%\n",
      "Batch 120, Loss: 0.806480, Accuracy: 87.70%\n",
      "Batch 121, Loss: 0.885372, Accuracy: 87.69%\n",
      "Batch 122, Loss: 0.851744, Accuracy: 87.70%\n",
      "Batch 123, Loss: 0.962809, Accuracy: 87.63%\n",
      "Batch 124, Loss: 0.821903, Accuracy: 87.68%\n",
      "Batch 125, Loss: 0.875486, Accuracy: 87.66%\n",
      "Batch 126, Loss: 0.855621, Accuracy: 87.67%\n",
      "Batch 127, Loss: 0.885672, Accuracy: 87.65%\n",
      "Batch 128, Loss: 0.854987, Accuracy: 87.66%\n",
      "Batch 129, Loss: 0.884370, Accuracy: 87.65%\n",
      "Batch 130, Loss: 0.919822, Accuracy: 87.61%\n",
      "Batch 131, Loss: 0.895100, Accuracy: 87.57%\n",
      "Batch 132, Loss: 0.879364, Accuracy: 87.56%\n",
      "Batch 133, Loss: 0.887470, Accuracy: 87.55%\n",
      "Batch 134, Loss: 0.901255, Accuracy: 87.52%\n",
      "Batch 135, Loss: 0.898746, Accuracy: 87.50%\n",
      "Batch 136, Loss: 0.857893, Accuracy: 87.52%\n",
      "Batch 137, Loss: 0.872026, Accuracy: 87.52%\n",
      "Batch 138, Loss: 0.891215, Accuracy: 87.49%\n",
      "Batch 139, Loss: 0.878539, Accuracy: 87.48%\n",
      "Batch 140, Loss: 0.887478, Accuracy: 87.46%\n",
      "Batch 141, Loss: 0.865673, Accuracy: 87.46%\n",
      "Batch 142, Loss: 0.864312, Accuracy: 87.46%\n",
      "Batch 143, Loss: 0.853135, Accuracy: 87.47%\n",
      "Batch 144, Loss: 0.912211, Accuracy: 87.43%\n",
      "Batch 145, Loss: 0.831586, Accuracy: 87.47%\n",
      "Batch 146, Loss: 0.905454, Accuracy: 87.45%\n",
      "Batch 147, Loss: 0.830284, Accuracy: 87.48%\n",
      "Batch 148, Loss: 0.837379, Accuracy: 87.50%\n",
      "Batch 149, Loss: 0.799833, Accuracy: 87.55%\n",
      "Batch 150, Loss: 0.897294, Accuracy: 87.52%\n",
      "Batch 151, Loss: 0.967607, Accuracy: 87.47%\n",
      "Batch 152, Loss: 0.869402, Accuracy: 87.47%\n",
      "Batch 153, Loss: 0.879423, Accuracy: 87.46%\n",
      "Batch 154, Loss: 0.862694, Accuracy: 87.48%\n",
      "Batch 155, Loss: 0.852813, Accuracy: 87.49%\n",
      "Batch 156, Loss: 0.900235, Accuracy: 87.47%\n",
      "Batch 157, Loss: 0.867507, Accuracy: 87.46%\n",
      "Batch 158, Loss: 0.893036, Accuracy: 87.44%\n",
      "Batch 159, Loss: 0.832932, Accuracy: 87.46%\n",
      "Batch 160, Loss: 0.852897, Accuracy: 87.47%\n",
      "Batch 161, Loss: 0.828377, Accuracy: 87.49%\n",
      "Batch 162, Loss: 0.912595, Accuracy: 87.47%\n",
      "Batch 163, Loss: 0.841339, Accuracy: 87.49%\n",
      "Batch 164, Loss: 0.784289, Accuracy: 87.56%\n",
      "Batch 165, Loss: 0.873802, Accuracy: 87.55%\n",
      "Batch 166, Loss: 0.902729, Accuracy: 87.53%\n",
      "Batch 167, Loss: 0.831769, Accuracy: 87.56%\n",
      "Batch 168, Loss: 0.869663, Accuracy: 87.56%\n",
      "Batch 169, Loss: 0.861293, Accuracy: 87.56%\n",
      "Batch 170, Loss: 0.873524, Accuracy: 87.56%\n",
      "Batch 171, Loss: 0.914230, Accuracy: 87.53%\n",
      "Batch 172, Loss: 0.872129, Accuracy: 87.53%\n",
      "Batch 173, Loss: 0.898862, Accuracy: 87.51%\n",
      "Batch 174, Loss: 0.883035, Accuracy: 87.50%\n",
      "Batch 175, Loss: 0.805355, Accuracy: 87.54%\n",
      "Batch 176, Loss: 0.826997, Accuracy: 87.56%\n",
      "Batch 177, Loss: 0.827975, Accuracy: 87.59%\n",
      "Batch 178, Loss: 0.963292, Accuracy: 87.54%\n",
      "Batch 179, Loss: 0.855613, Accuracy: 87.54%\n",
      "Batch 180, Loss: 0.849824, Accuracy: 87.56%\n",
      "Batch 181, Loss: 0.844142, Accuracy: 87.59%\n",
      "Batch 182, Loss: 0.820094, Accuracy: 87.62%\n",
      "Batch 183, Loss: 0.917608, Accuracy: 87.59%\n",
      "Batch 184, Loss: 0.845460, Accuracy: 87.61%\n",
      "Batch 185, Loss: 0.840021, Accuracy: 87.64%\n",
      "Batch 186, Loss: 0.885545, Accuracy: 87.62%\n",
      "Batch 187, Loss: 0.828546, Accuracy: 87.64%\n",
      "Batch 188, Loss: 0.835722, Accuracy: 87.67%\n",
      "Batch 189, Loss: 0.857164, Accuracy: 87.67%\n",
      "Batch 190, Loss: 0.843017, Accuracy: 87.69%\n",
      "Batch 191, Loss: 0.819564, Accuracy: 87.72%\n",
      "Batch 192, Loss: 0.882589, Accuracy: 87.72%\n",
      "Batch 193, Loss: 0.921308, Accuracy: 87.69%\n",
      "Batch 194, Loss: 0.894664, Accuracy: 87.67%\n",
      "Batch 195, Loss: 0.813784, Accuracy: 87.70%\n",
      "Batch 196, Loss: 0.854226, Accuracy: 87.71%\n",
      "Batch 197, Loss: 0.863645, Accuracy: 87.71%\n",
      "Batch 198, Loss: 0.832072, Accuracy: 87.74%\n",
      "Batch 199, Loss: 0.863862, Accuracy: 87.74%\n",
      "Batch 200, Loss: 0.838321, Accuracy: 87.76%\n",
      "Batch 201, Loss: 0.877614, Accuracy: 87.76%\n",
      "Batch 202, Loss: 0.865304, Accuracy: 87.76%\n",
      "Batch 203, Loss: 0.849907, Accuracy: 87.76%\n",
      "Batch 204, Loss: 0.798245, Accuracy: 87.80%\n",
      "Batch 205, Loss: 0.884501, Accuracy: 87.79%\n",
      "Batch 206, Loss: 0.897528, Accuracy: 87.77%\n",
      "Batch 207, Loss: 0.886886, Accuracy: 87.77%\n",
      "Batch 208, Loss: 0.852216, Accuracy: 87.78%\n",
      "Batch 209, Loss: 0.857644, Accuracy: 87.79%\n",
      "Batch 210, Loss: 0.905024, Accuracy: 87.78%\n",
      "Batch 211, Loss: 0.830350, Accuracy: 87.80%\n",
      "Batch 212, Loss: 0.823433, Accuracy: 87.82%\n",
      "Batch 213, Loss: 0.862241, Accuracy: 87.83%\n",
      "Training - Epoch 95, Loss: 0.867316, Accuracy: 87.83%\n",
      "Validation Batch 1, Loss: 0.821544, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.799408, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.868913, Accuracy: 91.67%\n",
      "Validation Batch 4, Loss: 0.843319, Accuracy: 91.41%\n",
      "Validation Batch 5, Loss: 0.829405, Accuracy: 91.56%\n",
      "Validation Batch 6, Loss: 0.800045, Accuracy: 91.93%\n",
      "Validation Batch 7, Loss: 0.804006, Accuracy: 92.41%\n",
      "Validation Batch 8, Loss: 0.892159, Accuracy: 91.60%\n",
      "Validation Batch 9, Loss: 0.897487, Accuracy: 90.80%\n",
      "Validation Batch 10, Loss: 0.828533, Accuracy: 90.94%\n",
      "Validation Batch 11, Loss: 0.834274, Accuracy: 91.19%\n",
      "Validation Batch 12, Loss: 0.832824, Accuracy: 91.28%\n",
      "Validation Batch 13, Loss: 0.850523, Accuracy: 90.99%\n",
      "Validation Batch 14, Loss: 0.853463, Accuracy: 90.85%\n",
      "Validation Batch 15, Loss: 0.823505, Accuracy: 90.94%\n",
      "Validation Batch 16, Loss: 0.834614, Accuracy: 91.02%\n",
      "Validation Batch 17, Loss: 0.879999, Accuracy: 90.81%\n",
      "Validation Batch 18, Loss: 0.818634, Accuracy: 90.80%\n",
      "Validation Batch 19, Loss: 0.886959, Accuracy: 90.46%\n",
      "Validation Batch 20, Loss: 0.878164, Accuracy: 90.31%\n",
      "Validation Batch 21, Loss: 0.869930, Accuracy: 90.18%\n",
      "Validation Batch 22, Loss: 0.851009, Accuracy: 90.13%\n",
      "Validation Batch 23, Loss: 0.875998, Accuracy: 90.01%\n",
      "Validation Batch 24, Loss: 0.853703, Accuracy: 89.97%\n",
      "Validation Batch 25, Loss: 0.823484, Accuracy: 90.00%\n",
      "Validation Batch 26, Loss: 0.853193, Accuracy: 90.02%\n",
      "Validation Batch 27, Loss: 0.806113, Accuracy: 90.14%\n",
      "Validation - Epoch 95, Loss: 0.844860, Accuracy: 90.14%\n",
      "Patience—2\n",
      "Epoch 96\n",
      "Batch 1, Loss: 0.838640, Accuracy: 92.19%\n",
      "Batch 2, Loss: 0.917164, Accuracy: 87.50%\n",
      "Batch 3, Loss: 0.965453, Accuracy: 83.85%\n",
      "Batch 4, Loss: 0.888203, Accuracy: 84.38%\n",
      "Batch 5, Loss: 0.822213, Accuracy: 85.62%\n",
      "Batch 6, Loss: 0.818707, Accuracy: 87.24%\n",
      "Batch 7, Loss: 0.835333, Accuracy: 87.72%\n",
      "Batch 8, Loss: 0.856951, Accuracy: 87.89%\n",
      "Batch 9, Loss: 0.808616, Accuracy: 88.54%\n",
      "Batch 10, Loss: 0.844831, Accuracy: 88.75%\n",
      "Batch 11, Loss: 0.840616, Accuracy: 88.92%\n",
      "Batch 12, Loss: 0.966272, Accuracy: 87.76%\n",
      "Batch 13, Loss: 0.910551, Accuracy: 87.26%\n",
      "Batch 14, Loss: 0.865571, Accuracy: 87.28%\n",
      "Batch 15, Loss: 0.916125, Accuracy: 86.98%\n",
      "Batch 16, Loss: 0.884233, Accuracy: 86.91%\n",
      "Batch 17, Loss: 0.888544, Accuracy: 86.86%\n",
      "Batch 18, Loss: 0.804446, Accuracy: 87.24%\n",
      "Batch 19, Loss: 0.851059, Accuracy: 87.34%\n",
      "Batch 20, Loss: 0.853477, Accuracy: 87.34%\n",
      "Batch 21, Loss: 0.848217, Accuracy: 87.50%\n",
      "Batch 22, Loss: 0.920705, Accuracy: 87.07%\n",
      "Batch 23, Loss: 0.824658, Accuracy: 87.30%\n",
      "Batch 24, Loss: 0.823777, Accuracy: 87.50%\n",
      "Batch 25, Loss: 0.819864, Accuracy: 87.69%\n",
      "Batch 26, Loss: 0.848516, Accuracy: 87.80%\n",
      "Batch 27, Loss: 0.837324, Accuracy: 87.96%\n",
      "Batch 28, Loss: 0.844665, Accuracy: 88.11%\n",
      "Batch 29, Loss: 0.909253, Accuracy: 87.98%\n",
      "Batch 30, Loss: 0.862166, Accuracy: 87.97%\n",
      "Batch 31, Loss: 0.851866, Accuracy: 88.00%\n",
      "Batch 32, Loss: 0.881868, Accuracy: 87.99%\n",
      "Batch 33, Loss: 0.886007, Accuracy: 87.88%\n",
      "Batch 34, Loss: 0.869302, Accuracy: 87.91%\n",
      "Batch 35, Loss: 0.862730, Accuracy: 87.90%\n",
      "Batch 36, Loss: 0.809181, Accuracy: 88.06%\n",
      "Batch 37, Loss: 0.802056, Accuracy: 88.26%\n",
      "Batch 38, Loss: 0.931777, Accuracy: 88.03%\n",
      "Batch 39, Loss: 0.866782, Accuracy: 88.06%\n",
      "Batch 40, Loss: 0.790130, Accuracy: 88.24%\n",
      "Batch 41, Loss: 0.874637, Accuracy: 88.15%\n",
      "Batch 42, Loss: 0.907385, Accuracy: 87.98%\n",
      "Batch 43, Loss: 0.928404, Accuracy: 87.83%\n",
      "Batch 44, Loss: 0.896989, Accuracy: 87.75%\n",
      "Batch 45, Loss: 0.863702, Accuracy: 87.71%\n",
      "Batch 46, Loss: 0.884294, Accuracy: 87.67%\n",
      "Batch 47, Loss: 0.888850, Accuracy: 87.67%\n",
      "Batch 48, Loss: 0.882798, Accuracy: 87.60%\n",
      "Batch 49, Loss: 0.812605, Accuracy: 87.72%\n",
      "Batch 50, Loss: 0.904202, Accuracy: 87.66%\n",
      "Batch 51, Loss: 0.931243, Accuracy: 87.56%\n",
      "Batch 52, Loss: 0.845508, Accuracy: 87.62%\n",
      "Batch 53, Loss: 0.826640, Accuracy: 87.71%\n",
      "Batch 54, Loss: 0.908449, Accuracy: 87.64%\n",
      "Batch 55, Loss: 0.889990, Accuracy: 87.61%\n",
      "Batch 56, Loss: 0.867699, Accuracy: 87.64%\n",
      "Batch 57, Loss: 0.874227, Accuracy: 87.61%\n",
      "Batch 58, Loss: 0.861243, Accuracy: 87.63%\n",
      "Batch 59, Loss: 0.826822, Accuracy: 87.71%\n",
      "Batch 60, Loss: 0.895277, Accuracy: 87.68%\n",
      "Batch 61, Loss: 0.863944, Accuracy: 87.68%\n",
      "Batch 62, Loss: 0.856284, Accuracy: 87.70%\n",
      "Batch 63, Loss: 0.830533, Accuracy: 87.77%\n",
      "Batch 64, Loss: 0.893312, Accuracy: 87.72%\n",
      "Batch 65, Loss: 0.827210, Accuracy: 87.79%\n",
      "Batch 66, Loss: 0.903835, Accuracy: 87.74%\n",
      "Batch 67, Loss: 0.977568, Accuracy: 87.59%\n",
      "Batch 68, Loss: 0.866221, Accuracy: 87.57%\n",
      "Batch 69, Loss: 0.861995, Accuracy: 87.59%\n",
      "Batch 70, Loss: 0.875335, Accuracy: 87.59%\n",
      "Batch 71, Loss: 0.883822, Accuracy: 87.54%\n",
      "Batch 72, Loss: 0.933310, Accuracy: 87.43%\n",
      "Batch 73, Loss: 0.857662, Accuracy: 87.46%\n",
      "Batch 74, Loss: 0.903715, Accuracy: 87.42%\n",
      "Batch 75, Loss: 0.866190, Accuracy: 87.44%\n",
      "Batch 76, Loss: 0.925723, Accuracy: 87.38%\n",
      "Batch 77, Loss: 0.853258, Accuracy: 87.40%\n",
      "Batch 78, Loss: 0.860959, Accuracy: 87.40%\n",
      "Batch 79, Loss: 0.871931, Accuracy: 87.40%\n",
      "Batch 80, Loss: 0.855037, Accuracy: 87.42%\n",
      "Batch 81, Loss: 0.865380, Accuracy: 87.42%\n",
      "Batch 82, Loss: 0.896554, Accuracy: 87.40%\n",
      "Batch 83, Loss: 0.859092, Accuracy: 87.41%\n",
      "Batch 84, Loss: 0.921125, Accuracy: 87.31%\n",
      "Batch 85, Loss: 0.848807, Accuracy: 87.35%\n",
      "Batch 86, Loss: 0.886327, Accuracy: 87.34%\n",
      "Batch 87, Loss: 0.834556, Accuracy: 87.41%\n",
      "Batch 88, Loss: 0.867582, Accuracy: 87.43%\n",
      "Batch 89, Loss: 0.858484, Accuracy: 87.45%\n",
      "Batch 90, Loss: 0.920580, Accuracy: 87.40%\n",
      "Batch 91, Loss: 0.833165, Accuracy: 87.45%\n",
      "Batch 92, Loss: 0.919514, Accuracy: 87.40%\n",
      "Batch 93, Loss: 0.930991, Accuracy: 87.33%\n",
      "Batch 94, Loss: 0.885297, Accuracy: 87.33%\n",
      "Batch 95, Loss: 0.814659, Accuracy: 87.38%\n",
      "Batch 96, Loss: 0.886978, Accuracy: 87.39%\n",
      "Batch 97, Loss: 0.901691, Accuracy: 87.34%\n",
      "Batch 98, Loss: 0.904411, Accuracy: 87.29%\n",
      "Batch 99, Loss: 0.930458, Accuracy: 87.22%\n",
      "Batch 100, Loss: 0.788864, Accuracy: 87.31%\n",
      "Batch 101, Loss: 0.824815, Accuracy: 87.36%\n",
      "Batch 102, Loss: 0.856155, Accuracy: 87.38%\n",
      "Batch 103, Loss: 0.915852, Accuracy: 87.32%\n",
      "Batch 104, Loss: 0.875066, Accuracy: 87.30%\n",
      "Batch 105, Loss: 0.871886, Accuracy: 87.32%\n",
      "Batch 106, Loss: 0.822847, Accuracy: 87.38%\n",
      "Batch 107, Loss: 0.801440, Accuracy: 87.44%\n",
      "Batch 108, Loss: 0.830325, Accuracy: 87.50%\n",
      "Batch 109, Loss: 0.886404, Accuracy: 87.49%\n",
      "Batch 110, Loss: 0.799026, Accuracy: 87.56%\n",
      "Batch 111, Loss: 0.878720, Accuracy: 87.56%\n",
      "Batch 112, Loss: 0.904491, Accuracy: 87.51%\n",
      "Batch 113, Loss: 0.832015, Accuracy: 87.56%\n",
      "Batch 114, Loss: 0.888359, Accuracy: 87.54%\n",
      "Batch 115, Loss: 0.822804, Accuracy: 87.58%\n",
      "Batch 116, Loss: 0.952757, Accuracy: 87.50%\n",
      "Batch 117, Loss: 0.914761, Accuracy: 87.47%\n",
      "Batch 118, Loss: 0.794869, Accuracy: 87.53%\n",
      "Batch 119, Loss: 0.880646, Accuracy: 87.53%\n",
      "Batch 120, Loss: 0.903775, Accuracy: 87.50%\n",
      "Batch 121, Loss: 0.859908, Accuracy: 87.51%\n",
      "Batch 122, Loss: 0.903077, Accuracy: 87.47%\n",
      "Batch 123, Loss: 0.880091, Accuracy: 87.47%\n",
      "Batch 124, Loss: 0.896695, Accuracy: 87.46%\n",
      "Batch 125, Loss: 0.795615, Accuracy: 87.53%\n",
      "Batch 126, Loss: 0.875690, Accuracy: 87.52%\n",
      "Batch 127, Loss: 0.856410, Accuracy: 87.55%\n",
      "Batch 128, Loss: 0.852627, Accuracy: 87.56%\n",
      "Batch 129, Loss: 0.827595, Accuracy: 87.60%\n",
      "Batch 130, Loss: 0.800496, Accuracy: 87.66%\n",
      "Batch 131, Loss: 0.904388, Accuracy: 87.62%\n",
      "Batch 132, Loss: 0.809525, Accuracy: 87.67%\n",
      "Batch 133, Loss: 0.871498, Accuracy: 87.65%\n",
      "Batch 134, Loss: 0.870929, Accuracy: 87.65%\n",
      "Batch 135, Loss: 0.819483, Accuracy: 87.70%\n",
      "Batch 136, Loss: 0.781818, Accuracy: 87.76%\n",
      "Batch 137, Loss: 0.906721, Accuracy: 87.74%\n",
      "Batch 138, Loss: 0.847326, Accuracy: 87.76%\n",
      "Batch 139, Loss: 0.833563, Accuracy: 87.78%\n",
      "Batch 140, Loss: 0.846290, Accuracy: 87.79%\n",
      "Batch 141, Loss: 0.809435, Accuracy: 87.82%\n",
      "Batch 142, Loss: 0.876572, Accuracy: 87.83%\n",
      "Batch 143, Loss: 0.839915, Accuracy: 87.85%\n",
      "Batch 144, Loss: 0.845984, Accuracy: 87.85%\n",
      "Batch 145, Loss: 0.884284, Accuracy: 87.81%\n",
      "Batch 146, Loss: 0.914698, Accuracy: 87.79%\n",
      "Batch 147, Loss: 0.856395, Accuracy: 87.80%\n",
      "Batch 148, Loss: 0.815288, Accuracy: 87.83%\n",
      "Batch 149, Loss: 0.859717, Accuracy: 87.84%\n",
      "Batch 150, Loss: 0.844795, Accuracy: 87.85%\n",
      "Batch 151, Loss: 0.799919, Accuracy: 87.90%\n",
      "Batch 152, Loss: 0.840064, Accuracy: 87.92%\n",
      "Batch 153, Loss: 0.860923, Accuracy: 87.93%\n",
      "Batch 154, Loss: 0.806199, Accuracy: 87.98%\n",
      "Batch 155, Loss: 0.826906, Accuracy: 87.99%\n",
      "Batch 156, Loss: 0.877957, Accuracy: 87.98%\n",
      "Batch 157, Loss: 0.838141, Accuracy: 88.01%\n",
      "Batch 158, Loss: 0.870516, Accuracy: 88.00%\n",
      "Batch 159, Loss: 0.849406, Accuracy: 88.01%\n",
      "Batch 160, Loss: 0.913612, Accuracy: 87.98%\n",
      "Batch 161, Loss: 0.843138, Accuracy: 87.99%\n",
      "Batch 162, Loss: 0.911281, Accuracy: 87.97%\n",
      "Batch 163, Loss: 0.822858, Accuracy: 88.00%\n",
      "Batch 164, Loss: 0.853942, Accuracy: 88.01%\n",
      "Batch 165, Loss: 0.865992, Accuracy: 88.01%\n",
      "Batch 166, Loss: 0.883494, Accuracy: 88.00%\n",
      "Batch 167, Loss: 0.900311, Accuracy: 87.98%\n",
      "Batch 168, Loss: 0.832801, Accuracy: 87.99%\n",
      "Batch 169, Loss: 0.828171, Accuracy: 88.01%\n",
      "Batch 170, Loss: 0.873384, Accuracy: 88.01%\n",
      "Batch 171, Loss: 0.889955, Accuracy: 87.99%\n",
      "Batch 172, Loss: 0.807407, Accuracy: 88.03%\n",
      "Batch 173, Loss: 0.866383, Accuracy: 88.02%\n",
      "Batch 174, Loss: 0.818446, Accuracy: 88.06%\n",
      "Batch 175, Loss: 0.896514, Accuracy: 88.04%\n",
      "Batch 176, Loss: 0.883196, Accuracy: 88.02%\n",
      "Batch 177, Loss: 0.850011, Accuracy: 88.03%\n",
      "Batch 178, Loss: 0.850279, Accuracy: 88.04%\n",
      "Batch 179, Loss: 0.871864, Accuracy: 88.04%\n",
      "Batch 180, Loss: 0.806574, Accuracy: 88.08%\n",
      "Batch 181, Loss: 0.854228, Accuracy: 88.09%\n",
      "Batch 182, Loss: 0.810100, Accuracy: 88.12%\n",
      "Batch 183, Loss: 0.811753, Accuracy: 88.15%\n",
      "Batch 184, Loss: 0.868120, Accuracy: 88.15%\n",
      "Batch 185, Loss: 0.869940, Accuracy: 88.15%\n",
      "Batch 186, Loss: 0.890747, Accuracy: 88.14%\n",
      "Batch 187, Loss: 0.798788, Accuracy: 88.18%\n",
      "Batch 188, Loss: 0.869716, Accuracy: 88.17%\n",
      "Batch 189, Loss: 0.823744, Accuracy: 88.19%\n",
      "Batch 190, Loss: 0.875214, Accuracy: 88.19%\n",
      "Batch 191, Loss: 0.843712, Accuracy: 88.20%\n",
      "Batch 192, Loss: 0.924356, Accuracy: 88.16%\n",
      "Batch 193, Loss: 0.905966, Accuracy: 88.14%\n",
      "Batch 194, Loss: 0.847104, Accuracy: 88.15%\n",
      "Batch 195, Loss: 0.851871, Accuracy: 88.16%\n",
      "Batch 196, Loss: 0.831958, Accuracy: 88.18%\n",
      "Batch 197, Loss: 0.879653, Accuracy: 88.17%\n",
      "Batch 198, Loss: 0.925469, Accuracy: 88.13%\n",
      "Batch 199, Loss: 0.853238, Accuracy: 88.13%\n",
      "Batch 200, Loss: 0.825181, Accuracy: 88.15%\n",
      "Batch 201, Loss: 0.882189, Accuracy: 88.15%\n",
      "Batch 202, Loss: 0.903113, Accuracy: 88.12%\n",
      "Batch 203, Loss: 0.882722, Accuracy: 88.10%\n",
      "Batch 204, Loss: 0.890638, Accuracy: 88.09%\n",
      "Batch 205, Loss: 0.961366, Accuracy: 88.03%\n",
      "Batch 206, Loss: 0.886605, Accuracy: 88.03%\n",
      "Batch 207, Loss: 0.887911, Accuracy: 88.01%\n",
      "Batch 208, Loss: 0.888146, Accuracy: 88.01%\n",
      "Batch 209, Loss: 0.855373, Accuracy: 88.02%\n",
      "Batch 210, Loss: 0.933149, Accuracy: 87.98%\n",
      "Batch 211, Loss: 0.846230, Accuracy: 87.98%\n",
      "Batch 212, Loss: 0.877988, Accuracy: 87.96%\n",
      "Batch 213, Loss: 0.909464, Accuracy: 87.95%\n",
      "Training - Epoch 96, Loss: 0.865777, Accuracy: 87.95%\n",
      "Validation Batch 1, Loss: 0.816678, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.792307, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.857993, Accuracy: 91.67%\n",
      "Validation Batch 4, Loss: 0.834696, Accuracy: 91.41%\n",
      "Validation Batch 5, Loss: 0.822977, Accuracy: 91.56%\n",
      "Validation Batch 6, Loss: 0.792122, Accuracy: 92.19%\n",
      "Validation Batch 7, Loss: 0.797761, Accuracy: 92.63%\n",
      "Validation Batch 8, Loss: 0.889017, Accuracy: 91.99%\n",
      "Validation Batch 9, Loss: 0.890896, Accuracy: 91.32%\n",
      "Validation Batch 10, Loss: 0.822669, Accuracy: 91.41%\n",
      "Validation Batch 11, Loss: 0.833227, Accuracy: 91.48%\n",
      "Validation Batch 12, Loss: 0.827807, Accuracy: 91.54%\n",
      "Validation Batch 13, Loss: 0.841327, Accuracy: 91.47%\n",
      "Validation Batch 14, Loss: 0.843221, Accuracy: 91.52%\n",
      "Validation Batch 15, Loss: 0.819701, Accuracy: 91.56%\n",
      "Validation Batch 16, Loss: 0.830658, Accuracy: 91.60%\n",
      "Validation Batch 17, Loss: 0.876251, Accuracy: 91.36%\n",
      "Validation Batch 18, Loss: 0.807726, Accuracy: 91.49%\n",
      "Validation Batch 19, Loss: 0.880519, Accuracy: 91.20%\n",
      "Validation Batch 20, Loss: 0.850231, Accuracy: 91.25%\n",
      "Validation Batch 21, Loss: 0.866568, Accuracy: 91.07%\n",
      "Validation Batch 22, Loss: 0.848238, Accuracy: 90.91%\n",
      "Validation Batch 23, Loss: 0.869186, Accuracy: 90.76%\n",
      "Validation Batch 24, Loss: 0.843113, Accuracy: 90.76%\n",
      "Validation Batch 25, Loss: 0.815734, Accuracy: 90.75%\n",
      "Validation Batch 26, Loss: 0.849656, Accuracy: 90.75%\n",
      "Validation Batch 27, Loss: 0.801044, Accuracy: 90.84%\n",
      "Validation - Epoch 96, Loss: 0.837827, Accuracy: 90.84%\n",
      "Patience—0\n",
      "Epoch 97\n",
      "Batch 1, Loss: 0.829478, Accuracy: 92.19%\n",
      "Batch 2, Loss: 0.790904, Accuracy: 93.75%\n",
      "Batch 3, Loss: 0.849958, Accuracy: 92.19%\n",
      "Batch 4, Loss: 0.891189, Accuracy: 90.62%\n",
      "Batch 5, Loss: 0.855721, Accuracy: 89.69%\n",
      "Batch 6, Loss: 0.846271, Accuracy: 89.84%\n",
      "Batch 7, Loss: 0.804641, Accuracy: 90.62%\n",
      "Batch 8, Loss: 0.928303, Accuracy: 89.45%\n",
      "Batch 9, Loss: 0.862495, Accuracy: 89.41%\n",
      "Batch 10, Loss: 0.867711, Accuracy: 89.22%\n",
      "Batch 11, Loss: 0.837535, Accuracy: 89.49%\n",
      "Batch 12, Loss: 0.870342, Accuracy: 89.32%\n",
      "Batch 13, Loss: 0.845273, Accuracy: 89.30%\n",
      "Batch 14, Loss: 0.851819, Accuracy: 89.40%\n",
      "Batch 15, Loss: 0.839329, Accuracy: 89.48%\n",
      "Batch 16, Loss: 0.821082, Accuracy: 89.75%\n",
      "Batch 17, Loss: 0.896815, Accuracy: 89.34%\n",
      "Batch 18, Loss: 0.847654, Accuracy: 89.41%\n",
      "Batch 19, Loss: 0.896879, Accuracy: 89.14%\n",
      "Batch 20, Loss: 0.901524, Accuracy: 88.91%\n",
      "Batch 21, Loss: 0.820445, Accuracy: 89.06%\n",
      "Batch 22, Loss: 0.929373, Accuracy: 88.78%\n",
      "Batch 23, Loss: 0.838629, Accuracy: 88.93%\n",
      "Batch 24, Loss: 0.854590, Accuracy: 88.87%\n",
      "Batch 25, Loss: 0.896539, Accuracy: 88.69%\n",
      "Batch 26, Loss: 0.942863, Accuracy: 88.34%\n",
      "Batch 27, Loss: 0.839626, Accuracy: 88.43%\n",
      "Batch 28, Loss: 0.829946, Accuracy: 88.56%\n",
      "Batch 29, Loss: 0.884019, Accuracy: 88.47%\n",
      "Batch 30, Loss: 0.835731, Accuracy: 88.59%\n",
      "Batch 31, Loss: 0.892114, Accuracy: 88.46%\n",
      "Batch 32, Loss: 0.796814, Accuracy: 88.67%\n",
      "Batch 33, Loss: 0.867450, Accuracy: 88.68%\n",
      "Batch 34, Loss: 0.847003, Accuracy: 88.74%\n",
      "Batch 35, Loss: 0.871389, Accuracy: 88.66%\n",
      "Batch 36, Loss: 0.874419, Accuracy: 88.59%\n",
      "Batch 37, Loss: 0.895597, Accuracy: 88.47%\n",
      "Batch 38, Loss: 0.871423, Accuracy: 88.45%\n",
      "Batch 39, Loss: 0.863426, Accuracy: 88.42%\n",
      "Batch 40, Loss: 0.810390, Accuracy: 88.55%\n",
      "Batch 41, Loss: 0.922544, Accuracy: 88.41%\n",
      "Batch 42, Loss: 0.918972, Accuracy: 88.21%\n",
      "Batch 43, Loss: 0.861218, Accuracy: 88.23%\n",
      "Batch 44, Loss: 0.833488, Accuracy: 88.28%\n",
      "Batch 45, Loss: 0.844365, Accuracy: 88.30%\n",
      "Batch 46, Loss: 0.946044, Accuracy: 88.11%\n",
      "Batch 47, Loss: 0.838380, Accuracy: 88.16%\n",
      "Batch 48, Loss: 0.897035, Accuracy: 88.09%\n",
      "Batch 49, Loss: 0.835672, Accuracy: 88.14%\n",
      "Batch 50, Loss: 0.952976, Accuracy: 87.97%\n",
      "Batch 51, Loss: 0.841657, Accuracy: 88.02%\n",
      "Batch 52, Loss: 0.906218, Accuracy: 87.92%\n",
      "Batch 53, Loss: 0.864516, Accuracy: 87.91%\n",
      "Batch 54, Loss: 0.864855, Accuracy: 87.91%\n",
      "Batch 55, Loss: 0.904311, Accuracy: 87.87%\n",
      "Batch 56, Loss: 0.889460, Accuracy: 87.83%\n",
      "Batch 57, Loss: 0.924829, Accuracy: 87.75%\n",
      "Batch 58, Loss: 0.867411, Accuracy: 87.74%\n",
      "Batch 59, Loss: 0.930297, Accuracy: 87.63%\n",
      "Batch 60, Loss: 0.862889, Accuracy: 87.66%\n",
      "Batch 61, Loss: 0.841314, Accuracy: 87.70%\n",
      "Batch 62, Loss: 0.938828, Accuracy: 87.60%\n",
      "Batch 63, Loss: 0.815586, Accuracy: 87.72%\n",
      "Batch 64, Loss: 0.866034, Accuracy: 87.70%\n",
      "Batch 65, Loss: 0.850160, Accuracy: 87.74%\n",
      "Batch 66, Loss: 0.870039, Accuracy: 87.74%\n",
      "Batch 67, Loss: 0.881087, Accuracy: 87.73%\n",
      "Batch 68, Loss: 0.862796, Accuracy: 87.73%\n",
      "Batch 69, Loss: 0.906968, Accuracy: 87.66%\n",
      "Batch 70, Loss: 0.857486, Accuracy: 87.66%\n",
      "Batch 71, Loss: 0.892787, Accuracy: 87.61%\n",
      "Batch 72, Loss: 0.849161, Accuracy: 87.63%\n",
      "Batch 73, Loss: 0.913601, Accuracy: 87.56%\n",
      "Batch 74, Loss: 0.882581, Accuracy: 87.56%\n",
      "Batch 75, Loss: 0.884345, Accuracy: 87.52%\n",
      "Batch 76, Loss: 0.890769, Accuracy: 87.48%\n",
      "Batch 77, Loss: 0.819895, Accuracy: 87.54%\n",
      "Batch 78, Loss: 0.888082, Accuracy: 87.50%\n",
      "Batch 79, Loss: 0.917521, Accuracy: 87.44%\n",
      "Batch 80, Loss: 0.843434, Accuracy: 87.50%\n",
      "Batch 81, Loss: 0.834061, Accuracy: 87.56%\n",
      "Batch 82, Loss: 0.851747, Accuracy: 87.58%\n",
      "Batch 83, Loss: 0.823511, Accuracy: 87.65%\n",
      "Batch 84, Loss: 0.842399, Accuracy: 87.69%\n",
      "Batch 85, Loss: 0.855784, Accuracy: 87.70%\n",
      "Batch 86, Loss: 0.864182, Accuracy: 87.70%\n",
      "Batch 87, Loss: 0.847446, Accuracy: 87.70%\n",
      "Batch 88, Loss: 0.871978, Accuracy: 87.70%\n",
      "Batch 89, Loss: 0.893438, Accuracy: 87.66%\n",
      "Batch 90, Loss: 0.939112, Accuracy: 87.57%\n",
      "Batch 91, Loss: 0.854472, Accuracy: 87.59%\n",
      "Batch 92, Loss: 0.859708, Accuracy: 87.58%\n",
      "Batch 93, Loss: 0.864593, Accuracy: 87.58%\n",
      "Batch 94, Loss: 0.837660, Accuracy: 87.62%\n",
      "Batch 95, Loss: 0.808656, Accuracy: 87.70%\n",
      "Batch 96, Loss: 0.813579, Accuracy: 87.76%\n",
      "Batch 97, Loss: 0.881062, Accuracy: 87.74%\n",
      "Batch 98, Loss: 0.897568, Accuracy: 87.74%\n",
      "Batch 99, Loss: 0.899351, Accuracy: 87.72%\n",
      "Batch 100, Loss: 0.817001, Accuracy: 87.78%\n",
      "Batch 101, Loss: 0.811734, Accuracy: 87.86%\n",
      "Batch 102, Loss: 0.893210, Accuracy: 87.82%\n",
      "Batch 103, Loss: 0.943127, Accuracy: 87.76%\n",
      "Batch 104, Loss: 0.895831, Accuracy: 87.74%\n",
      "Batch 105, Loss: 0.844727, Accuracy: 87.75%\n",
      "Batch 106, Loss: 0.851797, Accuracy: 87.78%\n",
      "Batch 107, Loss: 0.868503, Accuracy: 87.78%\n",
      "Batch 108, Loss: 0.887055, Accuracy: 87.76%\n",
      "Batch 109, Loss: 0.871101, Accuracy: 87.76%\n",
      "Batch 110, Loss: 0.816879, Accuracy: 87.80%\n",
      "Batch 111, Loss: 0.894786, Accuracy: 87.75%\n",
      "Batch 112, Loss: 0.863867, Accuracy: 87.75%\n",
      "Batch 113, Loss: 0.864726, Accuracy: 87.76%\n",
      "Batch 114, Loss: 0.787995, Accuracy: 87.83%\n",
      "Batch 115, Loss: 0.866609, Accuracy: 87.84%\n",
      "Batch 116, Loss: 0.800434, Accuracy: 87.90%\n",
      "Batch 117, Loss: 0.818197, Accuracy: 87.94%\n",
      "Batch 118, Loss: 0.974896, Accuracy: 87.84%\n",
      "Batch 119, Loss: 0.820603, Accuracy: 87.88%\n",
      "Batch 120, Loss: 0.838894, Accuracy: 87.90%\n",
      "Batch 121, Loss: 0.856976, Accuracy: 87.91%\n",
      "Batch 122, Loss: 0.895926, Accuracy: 87.91%\n",
      "Batch 123, Loss: 0.773379, Accuracy: 87.98%\n",
      "Batch 124, Loss: 0.854652, Accuracy: 87.99%\n",
      "Batch 125, Loss: 0.839948, Accuracy: 88.01%\n",
      "Batch 126, Loss: 0.815485, Accuracy: 88.05%\n",
      "Batch 127, Loss: 0.862686, Accuracy: 88.03%\n",
      "Batch 128, Loss: 0.808527, Accuracy: 88.07%\n",
      "Batch 129, Loss: 0.904871, Accuracy: 88.06%\n",
      "Batch 130, Loss: 0.871603, Accuracy: 88.05%\n",
      "Batch 131, Loss: 0.861440, Accuracy: 88.06%\n",
      "Batch 132, Loss: 0.890655, Accuracy: 88.03%\n",
      "Batch 133, Loss: 0.897224, Accuracy: 88.01%\n",
      "Batch 134, Loss: 0.867423, Accuracy: 88.00%\n",
      "Batch 135, Loss: 0.856644, Accuracy: 88.01%\n",
      "Batch 136, Loss: 0.832205, Accuracy: 88.04%\n",
      "Batch 137, Loss: 0.854353, Accuracy: 88.05%\n",
      "Batch 138, Loss: 0.916416, Accuracy: 88.02%\n",
      "Batch 139, Loss: 0.868315, Accuracy: 88.02%\n",
      "Batch 140, Loss: 0.857856, Accuracy: 88.02%\n",
      "Batch 141, Loss: 0.918376, Accuracy: 87.98%\n",
      "Batch 142, Loss: 0.869393, Accuracy: 87.97%\n",
      "Batch 143, Loss: 0.906477, Accuracy: 87.94%\n",
      "Batch 144, Loss: 0.887101, Accuracy: 87.91%\n",
      "Batch 145, Loss: 0.905826, Accuracy: 87.89%\n",
      "Batch 146, Loss: 0.830481, Accuracy: 87.91%\n",
      "Batch 147, Loss: 0.809243, Accuracy: 87.95%\n",
      "Batch 148, Loss: 0.896727, Accuracy: 87.93%\n",
      "Batch 149, Loss: 0.878450, Accuracy: 87.92%\n",
      "Batch 150, Loss: 0.811514, Accuracy: 87.95%\n",
      "Batch 151, Loss: 0.924790, Accuracy: 87.89%\n",
      "Batch 152, Loss: 0.873150, Accuracy: 87.89%\n",
      "Batch 153, Loss: 0.872981, Accuracy: 87.88%\n",
      "Batch 154, Loss: 0.864644, Accuracy: 87.87%\n",
      "Batch 155, Loss: 0.880449, Accuracy: 87.86%\n",
      "Batch 156, Loss: 0.847635, Accuracy: 87.88%\n",
      "Batch 157, Loss: 0.929306, Accuracy: 87.84%\n",
      "Batch 158, Loss: 0.898063, Accuracy: 87.81%\n",
      "Batch 159, Loss: 0.837447, Accuracy: 87.83%\n",
      "Batch 160, Loss: 0.896368, Accuracy: 87.82%\n",
      "Batch 161, Loss: 0.857160, Accuracy: 87.83%\n",
      "Batch 162, Loss: 0.887059, Accuracy: 87.83%\n",
      "Batch 163, Loss: 0.907601, Accuracy: 87.80%\n",
      "Batch 164, Loss: 0.851388, Accuracy: 87.81%\n",
      "Batch 165, Loss: 0.913578, Accuracy: 87.78%\n",
      "Batch 166, Loss: 0.829022, Accuracy: 87.80%\n",
      "Batch 167, Loss: 0.820396, Accuracy: 87.82%\n",
      "Batch 168, Loss: 0.869059, Accuracy: 87.83%\n",
      "Batch 169, Loss: 0.881437, Accuracy: 87.81%\n",
      "Batch 170, Loss: 0.893556, Accuracy: 87.78%\n",
      "Batch 171, Loss: 0.848612, Accuracy: 87.79%\n",
      "Batch 172, Loss: 0.856484, Accuracy: 87.81%\n",
      "Batch 173, Loss: 0.822329, Accuracy: 87.83%\n",
      "Batch 174, Loss: 0.833255, Accuracy: 87.86%\n",
      "Batch 175, Loss: 0.824708, Accuracy: 87.88%\n",
      "Batch 176, Loss: 0.864168, Accuracy: 87.88%\n",
      "Batch 177, Loss: 0.808053, Accuracy: 87.91%\n",
      "Batch 178, Loss: 0.869464, Accuracy: 87.90%\n",
      "Batch 179, Loss: 0.778408, Accuracy: 87.95%\n",
      "Batch 180, Loss: 0.873605, Accuracy: 87.96%\n",
      "Batch 181, Loss: 0.897600, Accuracy: 87.95%\n",
      "Batch 182, Loss: 0.901264, Accuracy: 87.92%\n",
      "Batch 183, Loss: 0.910536, Accuracy: 87.89%\n",
      "Batch 184, Loss: 0.895056, Accuracy: 87.88%\n",
      "Batch 185, Loss: 0.879725, Accuracy: 87.87%\n",
      "Batch 186, Loss: 0.873810, Accuracy: 87.87%\n",
      "Batch 187, Loss: 0.860797, Accuracy: 87.87%\n",
      "Batch 188, Loss: 0.842817, Accuracy: 87.88%\n",
      "Batch 189, Loss: 0.853326, Accuracy: 87.90%\n",
      "Batch 190, Loss: 0.905620, Accuracy: 87.87%\n",
      "Batch 191, Loss: 0.869311, Accuracy: 87.87%\n",
      "Batch 192, Loss: 0.881939, Accuracy: 87.86%\n",
      "Batch 193, Loss: 0.878592, Accuracy: 87.85%\n",
      "Batch 194, Loss: 0.929605, Accuracy: 87.81%\n",
      "Batch 195, Loss: 0.912312, Accuracy: 87.79%\n",
      "Batch 196, Loss: 0.848911, Accuracy: 87.80%\n",
      "Batch 197, Loss: 0.872372, Accuracy: 87.79%\n",
      "Batch 198, Loss: 0.841042, Accuracy: 87.80%\n",
      "Batch 199, Loss: 0.984446, Accuracy: 87.74%\n",
      "Batch 200, Loss: 0.836946, Accuracy: 87.77%\n",
      "Batch 201, Loss: 0.866278, Accuracy: 87.76%\n",
      "Batch 202, Loss: 0.884087, Accuracy: 87.76%\n",
      "Batch 203, Loss: 0.819907, Accuracy: 87.78%\n",
      "Batch 204, Loss: 0.790793, Accuracy: 87.81%\n",
      "Batch 205, Loss: 0.866121, Accuracy: 87.82%\n",
      "Batch 206, Loss: 0.814951, Accuracy: 87.84%\n",
      "Batch 207, Loss: 0.874916, Accuracy: 87.84%\n",
      "Batch 208, Loss: 0.828733, Accuracy: 87.86%\n",
      "Batch 209, Loss: 0.935857, Accuracy: 87.83%\n",
      "Batch 210, Loss: 0.922684, Accuracy: 87.81%\n",
      "Batch 211, Loss: 0.849811, Accuracy: 87.81%\n",
      "Batch 212, Loss: 0.928696, Accuracy: 87.77%\n",
      "Batch 213, Loss: 0.900391, Accuracy: 87.76%\n",
      "Training - Epoch 97, Loss: 0.867240, Accuracy: 87.76%\n",
      "Validation Batch 1, Loss: 0.822526, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.813820, Accuracy: 93.75%\n",
      "Validation Batch 3, Loss: 0.872191, Accuracy: 91.67%\n",
      "Validation Batch 4, Loss: 0.840856, Accuracy: 91.41%\n",
      "Validation Batch 5, Loss: 0.829766, Accuracy: 91.56%\n",
      "Validation Batch 6, Loss: 0.806193, Accuracy: 91.93%\n",
      "Validation Batch 7, Loss: 0.801615, Accuracy: 92.41%\n",
      "Validation Batch 8, Loss: 0.888586, Accuracy: 91.60%\n",
      "Validation Batch 9, Loss: 0.899905, Accuracy: 90.80%\n",
      "Validation Batch 10, Loss: 0.831799, Accuracy: 90.94%\n",
      "Validation Batch 11, Loss: 0.834987, Accuracy: 91.05%\n",
      "Validation Batch 12, Loss: 0.832603, Accuracy: 91.15%\n",
      "Validation Batch 13, Loss: 0.854942, Accuracy: 91.11%\n",
      "Validation Batch 14, Loss: 0.857621, Accuracy: 91.07%\n",
      "Validation Batch 15, Loss: 0.821797, Accuracy: 91.15%\n",
      "Validation Batch 16, Loss: 0.845129, Accuracy: 91.11%\n",
      "Validation Batch 17, Loss: 0.884327, Accuracy: 90.81%\n",
      "Validation Batch 18, Loss: 0.817306, Accuracy: 90.89%\n",
      "Validation Batch 19, Loss: 0.898526, Accuracy: 90.54%\n",
      "Validation Batch 20, Loss: 0.865574, Accuracy: 90.47%\n",
      "Validation Batch 21, Loss: 0.873984, Accuracy: 90.33%\n",
      "Validation Batch 22, Loss: 0.857805, Accuracy: 90.27%\n",
      "Validation Batch 23, Loss: 0.883974, Accuracy: 90.08%\n",
      "Validation Batch 24, Loss: 0.861004, Accuracy: 89.97%\n",
      "Validation Batch 25, Loss: 0.825370, Accuracy: 90.06%\n",
      "Validation Batch 26, Loss: 0.861904, Accuracy: 89.96%\n",
      "Validation Batch 27, Loss: 0.808917, Accuracy: 90.02%\n",
      "Validation - Epoch 97, Loss: 0.847890, Accuracy: 90.02%\n",
      "Patience—1\n",
      "Epoch 98\n",
      "Batch 1, Loss: 0.891608, Accuracy: 85.94%\n",
      "Batch 2, Loss: 0.820682, Accuracy: 89.84%\n",
      "Batch 3, Loss: 0.837299, Accuracy: 90.10%\n",
      "Batch 4, Loss: 0.831460, Accuracy: 90.23%\n",
      "Batch 5, Loss: 0.911542, Accuracy: 88.75%\n",
      "Batch 6, Loss: 0.839830, Accuracy: 89.32%\n",
      "Batch 7, Loss: 0.832470, Accuracy: 89.73%\n",
      "Batch 8, Loss: 0.873190, Accuracy: 89.45%\n",
      "Batch 9, Loss: 0.832031, Accuracy: 89.76%\n",
      "Batch 10, Loss: 0.833827, Accuracy: 90.00%\n",
      "Batch 11, Loss: 0.795567, Accuracy: 90.48%\n",
      "Batch 12, Loss: 0.869835, Accuracy: 90.10%\n",
      "Batch 13, Loss: 0.940396, Accuracy: 89.42%\n",
      "Batch 14, Loss: 0.844236, Accuracy: 89.51%\n",
      "Batch 15, Loss: 0.819202, Accuracy: 89.69%\n",
      "Batch 16, Loss: 0.880370, Accuracy: 89.45%\n",
      "Batch 17, Loss: 0.846644, Accuracy: 89.43%\n",
      "Batch 18, Loss: 0.907938, Accuracy: 89.06%\n",
      "Batch 19, Loss: 0.838059, Accuracy: 89.23%\n",
      "Batch 20, Loss: 0.792446, Accuracy: 89.53%\n",
      "Batch 21, Loss: 0.858858, Accuracy: 89.51%\n",
      "Batch 22, Loss: 0.836518, Accuracy: 89.56%\n",
      "Batch 23, Loss: 0.865967, Accuracy: 89.54%\n",
      "Batch 24, Loss: 0.916045, Accuracy: 89.26%\n",
      "Batch 25, Loss: 0.758992, Accuracy: 89.62%\n",
      "Batch 26, Loss: 0.886110, Accuracy: 89.48%\n",
      "Batch 27, Loss: 0.907054, Accuracy: 89.24%\n",
      "Batch 28, Loss: 0.915425, Accuracy: 89.01%\n",
      "Batch 29, Loss: 0.848989, Accuracy: 89.01%\n",
      "Batch 30, Loss: 0.904381, Accuracy: 88.85%\n",
      "Batch 31, Loss: 0.826516, Accuracy: 88.96%\n",
      "Batch 32, Loss: 0.886236, Accuracy: 88.87%\n",
      "Batch 33, Loss: 0.904775, Accuracy: 88.73%\n",
      "Batch 34, Loss: 0.898310, Accuracy: 88.60%\n",
      "Batch 35, Loss: 0.864333, Accuracy: 88.57%\n",
      "Batch 36, Loss: 0.822543, Accuracy: 88.67%\n",
      "Batch 37, Loss: 0.896304, Accuracy: 88.51%\n",
      "Batch 38, Loss: 0.792768, Accuracy: 88.69%\n",
      "Batch 39, Loss: 0.929461, Accuracy: 88.42%\n",
      "Batch 40, Loss: 0.821053, Accuracy: 88.52%\n",
      "Batch 41, Loss: 0.910312, Accuracy: 88.38%\n",
      "Batch 42, Loss: 0.879910, Accuracy: 88.32%\n",
      "Batch 43, Loss: 0.849255, Accuracy: 88.37%\n",
      "Batch 44, Loss: 0.903878, Accuracy: 88.28%\n",
      "Batch 45, Loss: 0.935341, Accuracy: 88.12%\n",
      "Batch 46, Loss: 0.905528, Accuracy: 88.08%\n",
      "Batch 47, Loss: 0.893318, Accuracy: 88.03%\n",
      "Batch 48, Loss: 0.850411, Accuracy: 88.09%\n",
      "Batch 49, Loss: 0.838606, Accuracy: 88.14%\n",
      "Batch 50, Loss: 0.891859, Accuracy: 88.09%\n",
      "Batch 51, Loss: 0.855933, Accuracy: 88.11%\n",
      "Batch 52, Loss: 0.859619, Accuracy: 88.13%\n",
      "Batch 53, Loss: 0.863327, Accuracy: 88.15%\n",
      "Batch 54, Loss: 0.791640, Accuracy: 88.31%\n",
      "Batch 55, Loss: 0.863380, Accuracy: 88.27%\n",
      "Batch 56, Loss: 0.851150, Accuracy: 88.25%\n",
      "Batch 57, Loss: 0.862706, Accuracy: 88.27%\n",
      "Batch 58, Loss: 0.888057, Accuracy: 88.23%\n",
      "Batch 59, Loss: 0.903952, Accuracy: 88.16%\n",
      "Batch 60, Loss: 0.889632, Accuracy: 88.12%\n",
      "Batch 61, Loss: 0.921823, Accuracy: 88.01%\n",
      "Batch 62, Loss: 0.794073, Accuracy: 88.10%\n",
      "Batch 63, Loss: 0.828414, Accuracy: 88.14%\n",
      "Batch 64, Loss: 0.900051, Accuracy: 88.09%\n",
      "Batch 65, Loss: 0.912347, Accuracy: 88.03%\n",
      "Batch 66, Loss: 0.853730, Accuracy: 88.04%\n",
      "Batch 67, Loss: 0.887465, Accuracy: 88.01%\n",
      "Batch 68, Loss: 0.889982, Accuracy: 87.96%\n",
      "Batch 69, Loss: 0.932482, Accuracy: 87.86%\n",
      "Batch 70, Loss: 0.858054, Accuracy: 87.86%\n",
      "Batch 71, Loss: 0.886955, Accuracy: 87.83%\n",
      "Batch 72, Loss: 0.819007, Accuracy: 87.89%\n",
      "Batch 73, Loss: 0.879132, Accuracy: 87.86%\n",
      "Batch 74, Loss: 0.845011, Accuracy: 87.88%\n",
      "Batch 75, Loss: 0.922574, Accuracy: 87.79%\n",
      "Batch 76, Loss: 0.870767, Accuracy: 87.79%\n",
      "Batch 77, Loss: 0.794948, Accuracy: 87.91%\n",
      "Batch 78, Loss: 0.853092, Accuracy: 87.94%\n",
      "Batch 79, Loss: 0.870736, Accuracy: 87.94%\n",
      "Batch 80, Loss: 0.885097, Accuracy: 87.91%\n",
      "Batch 81, Loss: 0.863009, Accuracy: 87.91%\n",
      "Batch 82, Loss: 0.947118, Accuracy: 87.82%\n",
      "Batch 83, Loss: 0.870390, Accuracy: 87.82%\n",
      "Batch 84, Loss: 0.851996, Accuracy: 87.85%\n",
      "Batch 85, Loss: 0.901446, Accuracy: 87.81%\n",
      "Batch 86, Loss: 0.851658, Accuracy: 87.83%\n",
      "Batch 87, Loss: 0.855015, Accuracy: 87.82%\n",
      "Batch 88, Loss: 0.883981, Accuracy: 87.82%\n",
      "Batch 89, Loss: 0.857231, Accuracy: 87.82%\n",
      "Batch 90, Loss: 0.878632, Accuracy: 87.80%\n",
      "Batch 91, Loss: 0.829780, Accuracy: 87.84%\n",
      "Batch 92, Loss: 0.928197, Accuracy: 87.79%\n",
      "Batch 93, Loss: 0.836616, Accuracy: 87.84%\n",
      "Batch 94, Loss: 0.876732, Accuracy: 87.83%\n",
      "Batch 95, Loss: 0.911285, Accuracy: 87.78%\n",
      "Batch 96, Loss: 0.866226, Accuracy: 87.78%\n",
      "Batch 97, Loss: 0.827791, Accuracy: 87.82%\n",
      "Batch 98, Loss: 0.894901, Accuracy: 87.80%\n",
      "Batch 99, Loss: 0.857852, Accuracy: 87.80%\n",
      "Batch 100, Loss: 0.862414, Accuracy: 87.80%\n",
      "Batch 101, Loss: 0.863632, Accuracy: 87.81%\n",
      "Batch 102, Loss: 0.817828, Accuracy: 87.87%\n",
      "Batch 103, Loss: 0.831725, Accuracy: 87.89%\n",
      "Batch 104, Loss: 0.856848, Accuracy: 87.91%\n",
      "Batch 105, Loss: 0.866306, Accuracy: 87.90%\n",
      "Batch 106, Loss: 0.857360, Accuracy: 87.90%\n",
      "Batch 107, Loss: 0.829145, Accuracy: 87.94%\n",
      "Batch 108, Loss: 0.858006, Accuracy: 87.95%\n",
      "Batch 109, Loss: 0.902324, Accuracy: 87.90%\n",
      "Batch 110, Loss: 0.899498, Accuracy: 87.87%\n",
      "Batch 111, Loss: 0.892030, Accuracy: 87.84%\n",
      "Batch 112, Loss: 0.894705, Accuracy: 87.81%\n",
      "Batch 113, Loss: 0.924386, Accuracy: 87.75%\n",
      "Batch 114, Loss: 0.911915, Accuracy: 87.71%\n",
      "Batch 115, Loss: 0.896204, Accuracy: 87.69%\n",
      "Batch 116, Loss: 0.856699, Accuracy: 87.69%\n",
      "Batch 117, Loss: 0.853899, Accuracy: 87.71%\n",
      "Batch 118, Loss: 0.857415, Accuracy: 87.71%\n",
      "Batch 119, Loss: 0.835177, Accuracy: 87.75%\n",
      "Batch 120, Loss: 0.834885, Accuracy: 87.79%\n",
      "Batch 121, Loss: 0.976920, Accuracy: 87.69%\n",
      "Batch 122, Loss: 0.836963, Accuracy: 87.72%\n",
      "Batch 123, Loss: 0.851088, Accuracy: 87.72%\n",
      "Batch 124, Loss: 0.914239, Accuracy: 87.68%\n",
      "Batch 125, Loss: 0.879729, Accuracy: 87.65%\n",
      "Batch 126, Loss: 0.877061, Accuracy: 87.62%\n",
      "Batch 127, Loss: 0.883021, Accuracy: 87.61%\n",
      "Batch 128, Loss: 0.864740, Accuracy: 87.61%\n",
      "Batch 129, Loss: 0.849387, Accuracy: 87.62%\n",
      "Batch 130, Loss: 0.888534, Accuracy: 87.62%\n",
      "Batch 131, Loss: 0.860289, Accuracy: 87.63%\n",
      "Batch 132, Loss: 0.887804, Accuracy: 87.62%\n",
      "Batch 133, Loss: 0.885727, Accuracy: 87.61%\n",
      "Batch 134, Loss: 0.910982, Accuracy: 87.59%\n",
      "Batch 135, Loss: 0.867131, Accuracy: 87.59%\n",
      "Batch 136, Loss: 0.844822, Accuracy: 87.60%\n",
      "Batch 137, Loss: 0.832393, Accuracy: 87.63%\n",
      "Batch 138, Loss: 0.872814, Accuracy: 87.62%\n",
      "Batch 139, Loss: 0.921069, Accuracy: 87.58%\n",
      "Batch 140, Loss: 0.832265, Accuracy: 87.61%\n",
      "Batch 141, Loss: 0.875214, Accuracy: 87.61%\n",
      "Batch 142, Loss: 0.858948, Accuracy: 87.62%\n",
      "Batch 143, Loss: 0.882617, Accuracy: 87.61%\n",
      "Batch 144, Loss: 0.859405, Accuracy: 87.61%\n",
      "Batch 145, Loss: 0.845890, Accuracy: 87.64%\n",
      "Batch 146, Loss: 0.895343, Accuracy: 87.63%\n",
      "Batch 147, Loss: 0.822587, Accuracy: 87.66%\n",
      "Batch 148, Loss: 0.847356, Accuracy: 87.68%\n",
      "Batch 149, Loss: 0.863614, Accuracy: 87.69%\n",
      "Batch 150, Loss: 0.888178, Accuracy: 87.68%\n",
      "Batch 151, Loss: 0.831231, Accuracy: 87.70%\n",
      "Batch 152, Loss: 0.815956, Accuracy: 87.74%\n",
      "Batch 153, Loss: 0.908897, Accuracy: 87.70%\n",
      "Batch 154, Loss: 0.841705, Accuracy: 87.72%\n",
      "Batch 155, Loss: 0.788851, Accuracy: 87.77%\n",
      "Batch 156, Loss: 0.805389, Accuracy: 87.82%\n",
      "Batch 157, Loss: 0.847908, Accuracy: 87.83%\n",
      "Batch 158, Loss: 0.850427, Accuracy: 87.85%\n",
      "Batch 159, Loss: 0.942410, Accuracy: 87.79%\n",
      "Batch 160, Loss: 0.850478, Accuracy: 87.81%\n",
      "Batch 161, Loss: 0.812083, Accuracy: 87.85%\n",
      "Batch 162, Loss: 0.843826, Accuracy: 87.87%\n",
      "Batch 163, Loss: 0.860183, Accuracy: 87.85%\n",
      "Batch 164, Loss: 0.913149, Accuracy: 87.83%\n",
      "Batch 165, Loss: 0.871997, Accuracy: 87.83%\n",
      "Batch 166, Loss: 0.953756, Accuracy: 87.78%\n",
      "Batch 167, Loss: 0.874726, Accuracy: 87.77%\n",
      "Batch 168, Loss: 0.851248, Accuracy: 87.80%\n",
      "Batch 169, Loss: 0.904076, Accuracy: 87.78%\n",
      "Batch 170, Loss: 0.828247, Accuracy: 87.80%\n",
      "Batch 171, Loss: 0.837389, Accuracy: 87.82%\n",
      "Batch 172, Loss: 0.899251, Accuracy: 87.79%\n",
      "Batch 173, Loss: 0.863723, Accuracy: 87.80%\n",
      "Batch 174, Loss: 0.873542, Accuracy: 87.80%\n",
      "Batch 175, Loss: 0.815855, Accuracy: 87.83%\n",
      "Batch 176, Loss: 0.818629, Accuracy: 87.86%\n",
      "Batch 177, Loss: 0.843764, Accuracy: 87.88%\n",
      "Batch 178, Loss: 0.869174, Accuracy: 87.87%\n",
      "Batch 179, Loss: 0.871448, Accuracy: 87.87%\n",
      "Batch 180, Loss: 0.862478, Accuracy: 87.86%\n",
      "Batch 181, Loss: 0.851402, Accuracy: 87.86%\n",
      "Batch 182, Loss: 0.861114, Accuracy: 87.86%\n",
      "Batch 183, Loss: 0.793299, Accuracy: 87.90%\n",
      "Batch 184, Loss: 0.881642, Accuracy: 87.88%\n",
      "Batch 185, Loss: 0.918331, Accuracy: 87.85%\n",
      "Batch 186, Loss: 0.788238, Accuracy: 87.90%\n",
      "Batch 187, Loss: 0.825475, Accuracy: 87.93%\n",
      "Batch 188, Loss: 0.830199, Accuracy: 87.96%\n",
      "Batch 189, Loss: 0.910620, Accuracy: 87.93%\n",
      "Batch 190, Loss: 0.932738, Accuracy: 87.89%\n",
      "Batch 191, Loss: 0.879987, Accuracy: 87.90%\n",
      "Batch 192, Loss: 0.886137, Accuracy: 87.89%\n",
      "Batch 193, Loss: 0.864851, Accuracy: 87.89%\n",
      "Batch 194, Loss: 0.881757, Accuracy: 87.89%\n",
      "Batch 195, Loss: 0.859971, Accuracy: 87.89%\n",
      "Batch 196, Loss: 0.881052, Accuracy: 87.87%\n",
      "Batch 197, Loss: 0.842487, Accuracy: 87.90%\n",
      "Batch 198, Loss: 0.905769, Accuracy: 87.87%\n",
      "Batch 199, Loss: 0.903374, Accuracy: 87.85%\n",
      "Batch 200, Loss: 0.836330, Accuracy: 87.88%\n",
      "Batch 201, Loss: 0.912827, Accuracy: 87.85%\n",
      "Batch 202, Loss: 0.883694, Accuracy: 87.84%\n",
      "Batch 203, Loss: 0.875386, Accuracy: 87.83%\n",
      "Batch 204, Loss: 0.954186, Accuracy: 87.78%\n",
      "Batch 205, Loss: 0.890553, Accuracy: 87.78%\n",
      "Batch 206, Loss: 0.872803, Accuracy: 87.78%\n",
      "Batch 207, Loss: 0.772597, Accuracy: 87.83%\n",
      "Batch 208, Loss: 0.872056, Accuracy: 87.82%\n",
      "Batch 209, Loss: 0.854868, Accuracy: 87.81%\n",
      "Batch 210, Loss: 0.807446, Accuracy: 87.84%\n",
      "Batch 211, Loss: 0.914741, Accuracy: 87.83%\n",
      "Batch 212, Loss: 0.822967, Accuracy: 87.85%\n",
      "Batch 213, Loss: 0.849901, Accuracy: 87.84%\n",
      "Training - Epoch 98, Loss: 0.866722, Accuracy: 87.84%\n",
      "Validation Batch 1, Loss: 0.828707, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.807741, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.863808, Accuracy: 92.19%\n",
      "Validation Batch 4, Loss: 0.846211, Accuracy: 91.80%\n",
      "Validation Batch 5, Loss: 0.831224, Accuracy: 91.88%\n",
      "Validation Batch 6, Loss: 0.805315, Accuracy: 92.19%\n",
      "Validation Batch 7, Loss: 0.803196, Accuracy: 92.63%\n",
      "Validation Batch 8, Loss: 0.888066, Accuracy: 91.60%\n",
      "Validation Batch 9, Loss: 0.899897, Accuracy: 90.80%\n",
      "Validation Batch 10, Loss: 0.829943, Accuracy: 90.94%\n",
      "Validation Batch 11, Loss: 0.832796, Accuracy: 91.05%\n",
      "Validation Batch 12, Loss: 0.833776, Accuracy: 91.15%\n",
      "Validation Batch 13, Loss: 0.852971, Accuracy: 90.99%\n",
      "Validation Batch 14, Loss: 0.856791, Accuracy: 90.74%\n",
      "Validation Batch 15, Loss: 0.823226, Accuracy: 90.83%\n",
      "Validation Batch 16, Loss: 0.841398, Accuracy: 90.82%\n",
      "Validation Batch 17, Loss: 0.881608, Accuracy: 90.53%\n",
      "Validation Batch 18, Loss: 0.820694, Accuracy: 90.54%\n",
      "Validation Batch 19, Loss: 0.891941, Accuracy: 90.30%\n",
      "Validation Batch 20, Loss: 0.874792, Accuracy: 90.16%\n",
      "Validation Batch 21, Loss: 0.873196, Accuracy: 90.03%\n",
      "Validation Batch 22, Loss: 0.853226, Accuracy: 89.91%\n",
      "Validation Batch 23, Loss: 0.881272, Accuracy: 89.74%\n",
      "Validation Batch 24, Loss: 0.858391, Accuracy: 89.65%\n",
      "Validation Batch 25, Loss: 0.826026, Accuracy: 89.75%\n",
      "Validation Batch 26, Loss: 0.860339, Accuracy: 89.72%\n",
      "Validation Batch 27, Loss: 0.803319, Accuracy: 89.84%\n",
      "Validation - Epoch 98, Loss: 0.847032, Accuracy: 89.84%\n",
      "Patience—2\n",
      "Epoch 99\n",
      "Batch 1, Loss: 0.795739, Accuracy: 93.75%\n",
      "Batch 2, Loss: 0.850612, Accuracy: 91.41%\n",
      "Batch 3, Loss: 0.875174, Accuracy: 90.10%\n",
      "Batch 4, Loss: 0.903081, Accuracy: 88.28%\n",
      "Batch 5, Loss: 0.905949, Accuracy: 87.50%\n",
      "Batch 6, Loss: 0.852625, Accuracy: 87.76%\n",
      "Batch 7, Loss: 0.889056, Accuracy: 87.28%\n",
      "Batch 8, Loss: 0.902110, Accuracy: 86.91%\n",
      "Batch 9, Loss: 0.834499, Accuracy: 87.33%\n",
      "Batch 10, Loss: 0.814786, Accuracy: 87.81%\n",
      "Batch 11, Loss: 0.863301, Accuracy: 87.64%\n",
      "Batch 12, Loss: 0.882298, Accuracy: 87.37%\n",
      "Batch 13, Loss: 0.783947, Accuracy: 88.10%\n",
      "Batch 14, Loss: 0.861521, Accuracy: 88.06%\n",
      "Batch 15, Loss: 0.844058, Accuracy: 88.23%\n",
      "Batch 16, Loss: 0.834861, Accuracy: 88.38%\n",
      "Batch 17, Loss: 0.917736, Accuracy: 88.14%\n",
      "Batch 18, Loss: 0.898838, Accuracy: 87.93%\n",
      "Batch 19, Loss: 0.852977, Accuracy: 87.99%\n",
      "Batch 20, Loss: 0.877311, Accuracy: 87.89%\n",
      "Batch 21, Loss: 0.893554, Accuracy: 87.80%\n",
      "Batch 22, Loss: 0.880387, Accuracy: 87.71%\n",
      "Batch 23, Loss: 0.892007, Accuracy: 87.57%\n",
      "Batch 24, Loss: 0.839978, Accuracy: 87.70%\n",
      "Batch 25, Loss: 0.847777, Accuracy: 87.75%\n",
      "Batch 26, Loss: 0.846772, Accuracy: 87.80%\n",
      "Batch 27, Loss: 0.933905, Accuracy: 87.50%\n",
      "Batch 28, Loss: 0.821371, Accuracy: 87.72%\n",
      "Batch 29, Loss: 0.913221, Accuracy: 87.50%\n",
      "Batch 30, Loss: 0.869391, Accuracy: 87.45%\n",
      "Batch 31, Loss: 0.864597, Accuracy: 87.45%\n",
      "Batch 32, Loss: 0.931756, Accuracy: 87.26%\n",
      "Batch 33, Loss: 0.861569, Accuracy: 87.26%\n",
      "Batch 34, Loss: 0.820647, Accuracy: 87.41%\n",
      "Batch 35, Loss: 0.905718, Accuracy: 87.28%\n",
      "Batch 36, Loss: 0.864569, Accuracy: 87.33%\n",
      "Batch 37, Loss: 0.855019, Accuracy: 87.37%\n",
      "Batch 38, Loss: 0.847338, Accuracy: 87.42%\n",
      "Batch 39, Loss: 0.849901, Accuracy: 87.50%\n",
      "Batch 40, Loss: 0.894161, Accuracy: 87.46%\n",
      "Batch 41, Loss: 0.831109, Accuracy: 87.58%\n",
      "Batch 42, Loss: 0.894930, Accuracy: 87.50%\n",
      "Batch 43, Loss: 0.921884, Accuracy: 87.39%\n",
      "Batch 44, Loss: 0.944109, Accuracy: 87.22%\n",
      "Batch 45, Loss: 0.854916, Accuracy: 87.26%\n",
      "Batch 46, Loss: 0.884117, Accuracy: 87.23%\n",
      "Batch 47, Loss: 0.844862, Accuracy: 87.27%\n",
      "Batch 48, Loss: 0.809533, Accuracy: 87.40%\n",
      "Batch 49, Loss: 0.860998, Accuracy: 87.44%\n",
      "Batch 50, Loss: 0.845727, Accuracy: 87.47%\n",
      "Batch 51, Loss: 0.894066, Accuracy: 87.41%\n",
      "Batch 52, Loss: 0.904453, Accuracy: 87.35%\n",
      "Batch 53, Loss: 0.884498, Accuracy: 87.35%\n",
      "Batch 54, Loss: 0.806171, Accuracy: 87.47%\n",
      "Batch 55, Loss: 0.851617, Accuracy: 87.53%\n",
      "Batch 56, Loss: 0.826703, Accuracy: 87.61%\n",
      "Batch 57, Loss: 0.855071, Accuracy: 87.64%\n",
      "Batch 58, Loss: 0.830029, Accuracy: 87.69%\n",
      "Batch 59, Loss: 0.856648, Accuracy: 87.71%\n",
      "Batch 60, Loss: 0.911424, Accuracy: 87.66%\n",
      "Batch 61, Loss: 0.871358, Accuracy: 87.65%\n",
      "Batch 62, Loss: 0.841565, Accuracy: 87.73%\n",
      "Batch 63, Loss: 0.834506, Accuracy: 87.80%\n",
      "Batch 64, Loss: 0.883950, Accuracy: 87.77%\n",
      "Batch 65, Loss: 0.844736, Accuracy: 87.84%\n",
      "Batch 66, Loss: 0.864019, Accuracy: 87.86%\n",
      "Batch 67, Loss: 0.852531, Accuracy: 87.90%\n",
      "Batch 68, Loss: 0.936345, Accuracy: 87.75%\n",
      "Batch 69, Loss: 0.864392, Accuracy: 87.75%\n",
      "Batch 70, Loss: 0.908260, Accuracy: 87.68%\n",
      "Batch 71, Loss: 0.890118, Accuracy: 87.68%\n",
      "Batch 72, Loss: 0.809568, Accuracy: 87.76%\n",
      "Batch 73, Loss: 0.886720, Accuracy: 87.74%\n",
      "Batch 74, Loss: 0.852713, Accuracy: 87.73%\n",
      "Batch 75, Loss: 0.857562, Accuracy: 87.75%\n",
      "Batch 76, Loss: 0.821596, Accuracy: 87.83%\n",
      "Batch 77, Loss: 0.920751, Accuracy: 87.74%\n",
      "Batch 78, Loss: 0.858126, Accuracy: 87.76%\n",
      "Batch 79, Loss: 0.873702, Accuracy: 87.76%\n",
      "Batch 80, Loss: 0.934406, Accuracy: 87.68%\n",
      "Batch 81, Loss: 0.865595, Accuracy: 87.65%\n",
      "Batch 82, Loss: 0.825503, Accuracy: 87.71%\n",
      "Batch 83, Loss: 0.887540, Accuracy: 87.69%\n",
      "Batch 84, Loss: 0.846135, Accuracy: 87.72%\n",
      "Batch 85, Loss: 0.857124, Accuracy: 87.74%\n",
      "Batch 86, Loss: 0.965941, Accuracy: 87.61%\n",
      "Batch 87, Loss: 0.843966, Accuracy: 87.64%\n",
      "Batch 88, Loss: 0.829766, Accuracy: 87.70%\n",
      "Batch 89, Loss: 0.870510, Accuracy: 87.68%\n",
      "Batch 90, Loss: 0.851204, Accuracy: 87.69%\n",
      "Batch 91, Loss: 0.839245, Accuracy: 87.72%\n",
      "Batch 92, Loss: 0.808705, Accuracy: 87.79%\n",
      "Batch 93, Loss: 0.847560, Accuracy: 87.80%\n",
      "Batch 94, Loss: 0.890130, Accuracy: 87.77%\n",
      "Batch 95, Loss: 0.858006, Accuracy: 87.76%\n",
      "Batch 96, Loss: 0.835927, Accuracy: 87.78%\n",
      "Batch 97, Loss: 0.847495, Accuracy: 87.79%\n",
      "Batch 98, Loss: 0.818507, Accuracy: 87.85%\n",
      "Batch 99, Loss: 0.911979, Accuracy: 87.80%\n",
      "Batch 100, Loss: 0.951910, Accuracy: 87.72%\n",
      "Batch 101, Loss: 0.908653, Accuracy: 87.69%\n",
      "Batch 102, Loss: 0.895370, Accuracy: 87.65%\n",
      "Batch 103, Loss: 0.894449, Accuracy: 87.62%\n",
      "Batch 104, Loss: 0.849116, Accuracy: 87.65%\n",
      "Batch 105, Loss: 0.837203, Accuracy: 87.68%\n",
      "Batch 106, Loss: 0.863517, Accuracy: 87.68%\n",
      "Batch 107, Loss: 0.883642, Accuracy: 87.68%\n",
      "Batch 108, Loss: 0.847803, Accuracy: 87.70%\n",
      "Batch 109, Loss: 0.888721, Accuracy: 87.70%\n",
      "Batch 110, Loss: 0.820214, Accuracy: 87.76%\n",
      "Batch 111, Loss: 0.843295, Accuracy: 87.77%\n",
      "Batch 112, Loss: 0.843918, Accuracy: 87.81%\n",
      "Batch 113, Loss: 0.824926, Accuracy: 87.85%\n",
      "Batch 114, Loss: 0.933152, Accuracy: 87.77%\n",
      "Batch 115, Loss: 0.863363, Accuracy: 87.77%\n",
      "Batch 116, Loss: 0.888915, Accuracy: 87.76%\n",
      "Batch 117, Loss: 0.919187, Accuracy: 87.71%\n",
      "Batch 118, Loss: 0.858090, Accuracy: 87.71%\n",
      "Batch 119, Loss: 0.833902, Accuracy: 87.74%\n",
      "Batch 120, Loss: 0.901432, Accuracy: 87.72%\n",
      "Batch 121, Loss: 0.841555, Accuracy: 87.73%\n",
      "Batch 122, Loss: 0.857373, Accuracy: 87.73%\n",
      "Batch 123, Loss: 0.874528, Accuracy: 87.74%\n",
      "Batch 124, Loss: 0.957642, Accuracy: 87.66%\n",
      "Batch 125, Loss: 0.954030, Accuracy: 87.58%\n",
      "Batch 126, Loss: 0.884583, Accuracy: 87.57%\n",
      "Batch 127, Loss: 0.800866, Accuracy: 87.64%\n",
      "Batch 128, Loss: 0.834334, Accuracy: 87.67%\n",
      "Batch 129, Loss: 0.801231, Accuracy: 87.73%\n",
      "Batch 130, Loss: 0.927189, Accuracy: 87.68%\n",
      "Batch 131, Loss: 0.821244, Accuracy: 87.71%\n",
      "Batch 132, Loss: 0.844075, Accuracy: 87.72%\n",
      "Batch 133, Loss: 0.852189, Accuracy: 87.72%\n",
      "Batch 134, Loss: 0.869109, Accuracy: 87.71%\n",
      "Batch 135, Loss: 0.828113, Accuracy: 87.74%\n",
      "Batch 136, Loss: 0.865327, Accuracy: 87.74%\n",
      "Batch 137, Loss: 0.838823, Accuracy: 87.76%\n",
      "Batch 138, Loss: 0.795545, Accuracy: 87.82%\n",
      "Batch 139, Loss: 0.879099, Accuracy: 87.80%\n",
      "Batch 140, Loss: 0.924268, Accuracy: 87.76%\n",
      "Batch 141, Loss: 0.869988, Accuracy: 87.75%\n",
      "Batch 142, Loss: 0.866592, Accuracy: 87.76%\n",
      "Batch 143, Loss: 0.846466, Accuracy: 87.77%\n",
      "Batch 144, Loss: 0.861562, Accuracy: 87.77%\n",
      "Batch 145, Loss: 0.839305, Accuracy: 87.79%\n",
      "Batch 146, Loss: 0.852314, Accuracy: 87.79%\n",
      "Batch 147, Loss: 0.822921, Accuracy: 87.83%\n",
      "Batch 148, Loss: 0.812965, Accuracy: 87.87%\n",
      "Batch 149, Loss: 0.847334, Accuracy: 87.89%\n",
      "Batch 150, Loss: 0.916075, Accuracy: 87.86%\n",
      "Batch 151, Loss: 0.908214, Accuracy: 87.83%\n",
      "Batch 152, Loss: 0.887283, Accuracy: 87.82%\n",
      "Batch 153, Loss: 0.916203, Accuracy: 87.79%\n",
      "Batch 154, Loss: 0.802238, Accuracy: 87.83%\n",
      "Batch 155, Loss: 0.844347, Accuracy: 87.85%\n",
      "Batch 156, Loss: 0.826685, Accuracy: 87.89%\n",
      "Batch 157, Loss: 0.824792, Accuracy: 87.92%\n",
      "Batch 158, Loss: 0.901743, Accuracy: 87.90%\n",
      "Batch 159, Loss: 0.849764, Accuracy: 87.90%\n",
      "Batch 160, Loss: 0.857764, Accuracy: 87.91%\n",
      "Batch 161, Loss: 0.853140, Accuracy: 87.92%\n",
      "Batch 162, Loss: 0.853440, Accuracy: 87.93%\n",
      "Batch 163, Loss: 0.920574, Accuracy: 87.89%\n",
      "Batch 164, Loss: 0.838497, Accuracy: 87.92%\n",
      "Batch 165, Loss: 0.866503, Accuracy: 87.92%\n",
      "Batch 166, Loss: 0.849318, Accuracy: 87.93%\n",
      "Batch 167, Loss: 0.883670, Accuracy: 87.93%\n",
      "Batch 168, Loss: 0.828411, Accuracy: 87.94%\n",
      "Batch 169, Loss: 0.847525, Accuracy: 87.94%\n",
      "Batch 170, Loss: 0.855093, Accuracy: 87.95%\n",
      "Batch 171, Loss: 0.867718, Accuracy: 87.95%\n",
      "Batch 172, Loss: 0.821498, Accuracy: 87.97%\n",
      "Batch 173, Loss: 0.907434, Accuracy: 87.94%\n",
      "Batch 174, Loss: 0.858796, Accuracy: 87.94%\n",
      "Batch 175, Loss: 0.924308, Accuracy: 87.90%\n",
      "Batch 176, Loss: 0.821348, Accuracy: 87.93%\n",
      "Batch 177, Loss: 0.898784, Accuracy: 87.90%\n",
      "Batch 178, Loss: 0.854927, Accuracy: 87.91%\n",
      "Batch 179, Loss: 0.840343, Accuracy: 87.93%\n",
      "Batch 180, Loss: 0.826118, Accuracy: 87.95%\n",
      "Batch 181, Loss: 0.805438, Accuracy: 87.98%\n",
      "Batch 182, Loss: 0.857288, Accuracy: 87.99%\n",
      "Batch 183, Loss: 0.831799, Accuracy: 88.01%\n",
      "Batch 184, Loss: 0.882688, Accuracy: 87.99%\n",
      "Batch 185, Loss: 0.822256, Accuracy: 88.02%\n",
      "Batch 186, Loss: 0.844138, Accuracy: 88.03%\n",
      "Batch 187, Loss: 0.846133, Accuracy: 88.04%\n",
      "Batch 188, Loss: 0.917853, Accuracy: 88.02%\n",
      "Batch 189, Loss: 0.909112, Accuracy: 88.00%\n",
      "Batch 190, Loss: 0.858910, Accuracy: 88.00%\n",
      "Batch 191, Loss: 0.899675, Accuracy: 87.99%\n",
      "Batch 192, Loss: 0.817115, Accuracy: 88.01%\n",
      "Batch 193, Loss: 0.891086, Accuracy: 88.00%\n",
      "Batch 194, Loss: 0.857389, Accuracy: 88.01%\n",
      "Batch 195, Loss: 0.927578, Accuracy: 87.97%\n",
      "Batch 196, Loss: 0.860964, Accuracy: 87.98%\n",
      "Batch 197, Loss: 0.826426, Accuracy: 88.01%\n",
      "Batch 198, Loss: 0.813232, Accuracy: 88.04%\n",
      "Batch 199, Loss: 0.895478, Accuracy: 88.02%\n",
      "Batch 200, Loss: 0.817246, Accuracy: 88.04%\n",
      "Batch 201, Loss: 0.825571, Accuracy: 88.06%\n",
      "Batch 202, Loss: 0.807128, Accuracy: 88.10%\n",
      "Batch 203, Loss: 0.860390, Accuracy: 88.09%\n",
      "Batch 204, Loss: 0.835448, Accuracy: 88.11%\n",
      "Batch 205, Loss: 0.835640, Accuracy: 88.12%\n",
      "Batch 206, Loss: 0.902842, Accuracy: 88.10%\n",
      "Batch 207, Loss: 0.852788, Accuracy: 88.10%\n",
      "Batch 208, Loss: 0.942444, Accuracy: 88.06%\n",
      "Batch 209, Loss: 0.869490, Accuracy: 88.05%\n",
      "Batch 210, Loss: 0.865406, Accuracy: 88.06%\n",
      "Batch 211, Loss: 0.894958, Accuracy: 88.05%\n",
      "Batch 212, Loss: 0.821618, Accuracy: 88.07%\n",
      "Batch 213, Loss: 0.868002, Accuracy: 88.07%\n",
      "Training - Epoch 99, Loss: 0.864001, Accuracy: 88.07%\n",
      "Validation Batch 1, Loss: 0.820535, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.797608, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.856109, Accuracy: 92.71%\n",
      "Validation Batch 4, Loss: 0.838701, Accuracy: 92.19%\n",
      "Validation Batch 5, Loss: 0.827461, Accuracy: 91.88%\n",
      "Validation Batch 6, Loss: 0.795142, Accuracy: 92.45%\n",
      "Validation Batch 7, Loss: 0.801179, Accuracy: 92.86%\n",
      "Validation Batch 8, Loss: 0.885467, Accuracy: 91.80%\n",
      "Validation Batch 9, Loss: 0.892890, Accuracy: 91.15%\n",
      "Validation Batch 10, Loss: 0.823975, Accuracy: 91.25%\n",
      "Validation Batch 11, Loss: 0.832103, Accuracy: 91.34%\n",
      "Validation Batch 12, Loss: 0.830458, Accuracy: 91.41%\n",
      "Validation Batch 13, Loss: 0.842303, Accuracy: 91.35%\n",
      "Validation Batch 14, Loss: 0.844673, Accuracy: 91.29%\n",
      "Validation Batch 15, Loss: 0.825086, Accuracy: 91.35%\n",
      "Validation Batch 16, Loss: 0.834558, Accuracy: 91.41%\n",
      "Validation Batch 17, Loss: 0.879719, Accuracy: 91.18%\n",
      "Validation Batch 18, Loss: 0.813326, Accuracy: 91.23%\n",
      "Validation Batch 19, Loss: 0.883934, Accuracy: 90.95%\n",
      "Validation Batch 20, Loss: 0.860561, Accuracy: 90.86%\n",
      "Validation Batch 21, Loss: 0.868984, Accuracy: 90.70%\n",
      "Validation Batch 22, Loss: 0.844621, Accuracy: 90.62%\n",
      "Validation Batch 23, Loss: 0.871236, Accuracy: 90.49%\n",
      "Validation Batch 24, Loss: 0.839479, Accuracy: 90.56%\n",
      "Validation Batch 25, Loss: 0.820561, Accuracy: 90.62%\n",
      "Validation Batch 26, Loss: 0.853671, Accuracy: 90.62%\n",
      "Validation Batch 27, Loss: 0.804042, Accuracy: 90.72%\n",
      "Validation - Epoch 99, Loss: 0.840310, Accuracy: 90.72%\n",
      "Patience—3\n",
      "Epoch 100\n",
      "Batch 1, Loss: 0.966439, Accuracy: 78.12%\n",
      "Batch 2, Loss: 0.801233, Accuracy: 85.94%\n",
      "Batch 3, Loss: 0.904325, Accuracy: 84.90%\n",
      "Batch 4, Loss: 0.791869, Accuracy: 87.50%\n",
      "Batch 5, Loss: 0.830397, Accuracy: 88.75%\n",
      "Batch 6, Loss: 0.827796, Accuracy: 89.58%\n",
      "Batch 7, Loss: 0.833259, Accuracy: 89.73%\n",
      "Batch 8, Loss: 0.882600, Accuracy: 89.26%\n",
      "Batch 9, Loss: 0.841606, Accuracy: 89.58%\n",
      "Batch 10, Loss: 0.876085, Accuracy: 89.38%\n",
      "Batch 11, Loss: 0.947066, Accuracy: 88.49%\n",
      "Batch 12, Loss: 0.837584, Accuracy: 88.54%\n",
      "Batch 13, Loss: 0.842944, Accuracy: 88.70%\n",
      "Batch 14, Loss: 0.852495, Accuracy: 88.73%\n",
      "Batch 15, Loss: 0.940392, Accuracy: 88.02%\n",
      "Batch 16, Loss: 0.838395, Accuracy: 88.18%\n",
      "Batch 17, Loss: 0.933748, Accuracy: 87.78%\n",
      "Batch 18, Loss: 0.838471, Accuracy: 87.93%\n",
      "Batch 19, Loss: 0.865262, Accuracy: 87.91%\n",
      "Batch 20, Loss: 0.875389, Accuracy: 87.81%\n",
      "Batch 21, Loss: 0.805432, Accuracy: 88.10%\n",
      "Batch 22, Loss: 0.806501, Accuracy: 88.35%\n",
      "Batch 23, Loss: 0.817177, Accuracy: 88.52%\n",
      "Batch 24, Loss: 0.810993, Accuracy: 88.74%\n",
      "Batch 25, Loss: 0.916582, Accuracy: 88.50%\n",
      "Batch 26, Loss: 0.827615, Accuracy: 88.64%\n",
      "Batch 27, Loss: 0.857651, Accuracy: 88.60%\n",
      "Batch 28, Loss: 0.901024, Accuracy: 88.50%\n",
      "Batch 29, Loss: 0.980799, Accuracy: 88.09%\n",
      "Batch 30, Loss: 0.841931, Accuracy: 88.12%\n",
      "Batch 31, Loss: 0.842342, Accuracy: 88.16%\n",
      "Batch 32, Loss: 0.903708, Accuracy: 88.04%\n",
      "Batch 33, Loss: 0.840935, Accuracy: 88.12%\n",
      "Batch 34, Loss: 0.871879, Accuracy: 88.05%\n",
      "Batch 35, Loss: 0.933824, Accuracy: 87.86%\n",
      "Batch 36, Loss: 0.835368, Accuracy: 87.93%\n",
      "Batch 37, Loss: 0.850334, Accuracy: 87.92%\n",
      "Batch 38, Loss: 0.781196, Accuracy: 88.12%\n",
      "Batch 39, Loss: 0.817235, Accuracy: 88.22%\n",
      "Batch 40, Loss: 0.871320, Accuracy: 88.20%\n",
      "Batch 41, Loss: 0.867910, Accuracy: 88.22%\n",
      "Batch 42, Loss: 0.827148, Accuracy: 88.32%\n",
      "Batch 43, Loss: 0.814630, Accuracy: 88.44%\n",
      "Batch 44, Loss: 0.903762, Accuracy: 88.35%\n",
      "Batch 45, Loss: 0.860149, Accuracy: 88.37%\n",
      "Batch 46, Loss: 0.934679, Accuracy: 88.18%\n",
      "Batch 47, Loss: 0.871172, Accuracy: 88.16%\n",
      "Batch 48, Loss: 0.870365, Accuracy: 88.18%\n",
      "Batch 49, Loss: 0.911033, Accuracy: 88.11%\n",
      "Batch 50, Loss: 0.843000, Accuracy: 88.16%\n",
      "Batch 51, Loss: 0.817205, Accuracy: 88.27%\n",
      "Batch 52, Loss: 0.909108, Accuracy: 88.16%\n",
      "Batch 53, Loss: 0.899231, Accuracy: 88.12%\n",
      "Batch 54, Loss: 0.866938, Accuracy: 88.11%\n",
      "Batch 55, Loss: 0.801860, Accuracy: 88.21%\n",
      "Batch 56, Loss: 0.855616, Accuracy: 88.23%\n",
      "Batch 57, Loss: 0.891045, Accuracy: 88.21%\n",
      "Batch 58, Loss: 0.909024, Accuracy: 88.15%\n",
      "Batch 59, Loss: 0.934683, Accuracy: 88.00%\n",
      "Batch 60, Loss: 0.858439, Accuracy: 88.02%\n",
      "Batch 61, Loss: 0.836581, Accuracy: 88.06%\n",
      "Batch 62, Loss: 0.848676, Accuracy: 88.08%\n",
      "Batch 63, Loss: 0.861141, Accuracy: 88.10%\n",
      "Batch 64, Loss: 0.808033, Accuracy: 88.18%\n",
      "Batch 65, Loss: 0.789719, Accuracy: 88.32%\n",
      "Batch 66, Loss: 0.857755, Accuracy: 88.33%\n",
      "Batch 67, Loss: 0.827456, Accuracy: 88.36%\n",
      "Batch 68, Loss: 0.887367, Accuracy: 88.33%\n",
      "Batch 69, Loss: 0.923712, Accuracy: 88.25%\n",
      "Batch 70, Loss: 0.807093, Accuracy: 88.35%\n",
      "Batch 71, Loss: 0.890848, Accuracy: 88.29%\n",
      "Batch 72, Loss: 0.897197, Accuracy: 88.24%\n",
      "Batch 73, Loss: 0.954515, Accuracy: 88.10%\n",
      "Batch 74, Loss: 0.960269, Accuracy: 87.96%\n",
      "Batch 75, Loss: 0.863731, Accuracy: 87.96%\n",
      "Batch 76, Loss: 0.875594, Accuracy: 87.95%\n",
      "Batch 77, Loss: 0.882672, Accuracy: 87.95%\n",
      "Batch 78, Loss: 0.841439, Accuracy: 87.98%\n",
      "Batch 79, Loss: 0.936388, Accuracy: 87.90%\n",
      "Batch 80, Loss: 0.875535, Accuracy: 87.87%\n",
      "Batch 81, Loss: 0.845838, Accuracy: 87.89%\n",
      "Batch 82, Loss: 0.801483, Accuracy: 87.96%\n",
      "Batch 83, Loss: 0.837382, Accuracy: 87.99%\n",
      "Batch 84, Loss: 0.847784, Accuracy: 88.00%\n",
      "Batch 85, Loss: 0.881890, Accuracy: 87.98%\n",
      "Batch 86, Loss: 0.847431, Accuracy: 88.01%\n",
      "Batch 87, Loss: 0.905377, Accuracy: 87.97%\n",
      "Batch 88, Loss: 0.919808, Accuracy: 87.91%\n",
      "Batch 89, Loss: 0.841546, Accuracy: 87.94%\n",
      "Batch 90, Loss: 0.888476, Accuracy: 87.90%\n",
      "Batch 91, Loss: 0.863284, Accuracy: 87.89%\n",
      "Batch 92, Loss: 0.869161, Accuracy: 87.89%\n",
      "Batch 93, Loss: 0.841577, Accuracy: 87.92%\n",
      "Batch 94, Loss: 0.877800, Accuracy: 87.92%\n",
      "Batch 95, Loss: 0.844915, Accuracy: 87.96%\n",
      "Batch 96, Loss: 0.873876, Accuracy: 87.96%\n",
      "Batch 97, Loss: 0.907782, Accuracy: 87.90%\n",
      "Batch 98, Loss: 0.830381, Accuracy: 87.96%\n",
      "Batch 99, Loss: 0.874607, Accuracy: 87.94%\n",
      "Batch 100, Loss: 0.812079, Accuracy: 88.00%\n",
      "Batch 101, Loss: 0.864282, Accuracy: 88.01%\n",
      "Batch 102, Loss: 0.878527, Accuracy: 88.01%\n",
      "Batch 103, Loss: 0.843101, Accuracy: 88.05%\n",
      "Batch 104, Loss: 0.899715, Accuracy: 88.01%\n",
      "Batch 105, Loss: 0.853524, Accuracy: 88.01%\n",
      "Batch 106, Loss: 0.848389, Accuracy: 88.03%\n",
      "Batch 107, Loss: 0.873307, Accuracy: 88.01%\n",
      "Batch 108, Loss: 0.885975, Accuracy: 87.99%\n",
      "Batch 109, Loss: 0.849331, Accuracy: 88.00%\n",
      "Batch 110, Loss: 0.870073, Accuracy: 87.98%\n",
      "Batch 111, Loss: 0.897256, Accuracy: 87.95%\n",
      "Batch 112, Loss: 0.888740, Accuracy: 87.92%\n",
      "Batch 113, Loss: 0.835501, Accuracy: 87.94%\n",
      "Batch 114, Loss: 0.854409, Accuracy: 87.97%\n",
      "Batch 115, Loss: 0.836550, Accuracy: 87.99%\n",
      "Batch 116, Loss: 0.851958, Accuracy: 88.00%\n",
      "Batch 117, Loss: 0.826495, Accuracy: 88.03%\n",
      "Batch 118, Loss: 0.855141, Accuracy: 88.04%\n",
      "Batch 119, Loss: 0.860555, Accuracy: 88.05%\n",
      "Batch 120, Loss: 0.793864, Accuracy: 88.11%\n",
      "Batch 121, Loss: 0.891662, Accuracy: 88.09%\n",
      "Batch 122, Loss: 0.837106, Accuracy: 88.11%\n",
      "Batch 123, Loss: 0.795330, Accuracy: 88.17%\n",
      "Batch 124, Loss: 0.822261, Accuracy: 88.22%\n",
      "Batch 125, Loss: 0.866026, Accuracy: 88.21%\n",
      "Batch 126, Loss: 0.836491, Accuracy: 88.24%\n",
      "Batch 127, Loss: 0.859056, Accuracy: 88.24%\n",
      "Batch 128, Loss: 0.884026, Accuracy: 88.21%\n",
      "Batch 129, Loss: 0.905792, Accuracy: 88.18%\n",
      "Batch 130, Loss: 0.844369, Accuracy: 88.19%\n",
      "Batch 131, Loss: 0.808344, Accuracy: 88.23%\n",
      "Batch 132, Loss: 0.814194, Accuracy: 88.26%\n",
      "Batch 133, Loss: 0.888431, Accuracy: 88.24%\n",
      "Batch 134, Loss: 0.804532, Accuracy: 88.28%\n",
      "Batch 135, Loss: 0.855262, Accuracy: 88.29%\n",
      "Batch 136, Loss: 0.847101, Accuracy: 88.30%\n",
      "Batch 137, Loss: 0.901189, Accuracy: 88.28%\n",
      "Batch 138, Loss: 0.843851, Accuracy: 88.28%\n",
      "Batch 139, Loss: 0.860617, Accuracy: 88.28%\n",
      "Batch 140, Loss: 0.832304, Accuracy: 88.30%\n",
      "Batch 141, Loss: 0.845163, Accuracy: 88.32%\n",
      "Batch 142, Loss: 0.835150, Accuracy: 88.35%\n",
      "Batch 143, Loss: 0.925035, Accuracy: 88.30%\n",
      "Batch 144, Loss: 0.814647, Accuracy: 88.32%\n",
      "Batch 145, Loss: 0.913011, Accuracy: 88.30%\n",
      "Batch 146, Loss: 0.823034, Accuracy: 88.32%\n",
      "Batch 147, Loss: 0.901035, Accuracy: 88.30%\n",
      "Batch 148, Loss: 0.792247, Accuracy: 88.34%\n",
      "Batch 149, Loss: 0.861755, Accuracy: 88.35%\n",
      "Batch 150, Loss: 0.853389, Accuracy: 88.35%\n",
      "Batch 151, Loss: 0.934599, Accuracy: 88.30%\n",
      "Batch 152, Loss: 0.873974, Accuracy: 88.29%\n",
      "Batch 153, Loss: 0.834940, Accuracy: 88.31%\n",
      "Batch 154, Loss: 0.845347, Accuracy: 88.31%\n",
      "Batch 155, Loss: 0.824084, Accuracy: 88.35%\n",
      "Batch 156, Loss: 0.916426, Accuracy: 88.31%\n",
      "Batch 157, Loss: 0.899912, Accuracy: 88.29%\n",
      "Batch 158, Loss: 0.911771, Accuracy: 88.25%\n",
      "Batch 159, Loss: 0.906004, Accuracy: 88.22%\n",
      "Batch 160, Loss: 0.897271, Accuracy: 88.19%\n",
      "Batch 161, Loss: 0.898234, Accuracy: 88.17%\n",
      "Batch 162, Loss: 0.816807, Accuracy: 88.20%\n",
      "Batch 163, Loss: 0.836870, Accuracy: 88.22%\n",
      "Batch 164, Loss: 0.910395, Accuracy: 88.20%\n",
      "Batch 165, Loss: 0.854208, Accuracy: 88.19%\n",
      "Batch 166, Loss: 0.880100, Accuracy: 88.18%\n",
      "Batch 167, Loss: 0.862521, Accuracy: 88.17%\n",
      "Batch 168, Loss: 0.798357, Accuracy: 88.22%\n",
      "Batch 169, Loss: 0.838871, Accuracy: 88.23%\n",
      "Batch 170, Loss: 0.941540, Accuracy: 88.18%\n",
      "Batch 171, Loss: 0.893928, Accuracy: 88.16%\n",
      "Batch 172, Loss: 0.889113, Accuracy: 88.14%\n",
      "Batch 173, Loss: 0.876065, Accuracy: 88.13%\n",
      "Batch 174, Loss: 0.843099, Accuracy: 88.15%\n",
      "Batch 175, Loss: 0.890342, Accuracy: 88.12%\n",
      "Batch 176, Loss: 0.854134, Accuracy: 88.14%\n",
      "Batch 177, Loss: 0.837258, Accuracy: 88.15%\n",
      "Batch 178, Loss: 0.899967, Accuracy: 88.13%\n",
      "Batch 179, Loss: 0.799187, Accuracy: 88.16%\n",
      "Batch 180, Loss: 0.849586, Accuracy: 88.18%\n",
      "Batch 181, Loss: 0.840893, Accuracy: 88.19%\n",
      "Batch 182, Loss: 0.804145, Accuracy: 88.22%\n",
      "Batch 183, Loss: 0.814063, Accuracy: 88.24%\n",
      "Batch 184, Loss: 0.876788, Accuracy: 88.23%\n",
      "Batch 185, Loss: 0.811859, Accuracy: 88.26%\n",
      "Batch 186, Loss: 0.919041, Accuracy: 88.23%\n",
      "Batch 187, Loss: 0.772104, Accuracy: 88.29%\n",
      "Batch 188, Loss: 0.793581, Accuracy: 88.32%\n",
      "Batch 189, Loss: 0.904675, Accuracy: 88.29%\n",
      "Batch 190, Loss: 0.875148, Accuracy: 88.29%\n",
      "Batch 191, Loss: 0.841118, Accuracy: 88.30%\n",
      "Batch 192, Loss: 0.885087, Accuracy: 88.28%\n",
      "Batch 193, Loss: 0.895659, Accuracy: 88.25%\n",
      "Batch 194, Loss: 0.900589, Accuracy: 88.24%\n",
      "Batch 195, Loss: 0.890096, Accuracy: 88.23%\n",
      "Batch 196, Loss: 0.819477, Accuracy: 88.25%\n",
      "Batch 197, Loss: 0.882238, Accuracy: 88.25%\n",
      "Batch 198, Loss: 0.816207, Accuracy: 88.27%\n",
      "Batch 199, Loss: 0.813158, Accuracy: 88.29%\n",
      "Batch 200, Loss: 0.831124, Accuracy: 88.31%\n",
      "Batch 201, Loss: 0.909469, Accuracy: 88.29%\n",
      "Batch 202, Loss: 0.863846, Accuracy: 88.28%\n",
      "Batch 203, Loss: 0.848780, Accuracy: 88.29%\n",
      "Batch 204, Loss: 0.901874, Accuracy: 88.27%\n",
      "Batch 205, Loss: 0.835605, Accuracy: 88.29%\n",
      "Batch 206, Loss: 0.833630, Accuracy: 88.31%\n",
      "Batch 207, Loss: 0.879932, Accuracy: 88.31%\n",
      "Batch 208, Loss: 0.883983, Accuracy: 88.30%\n",
      "Batch 209, Loss: 0.803495, Accuracy: 88.32%\n",
      "Batch 210, Loss: 0.854982, Accuracy: 88.33%\n",
      "Batch 211, Loss: 0.826158, Accuracy: 88.36%\n",
      "Batch 212, Loss: 0.854947, Accuracy: 88.36%\n",
      "Batch 213, Loss: 0.816564, Accuracy: 88.39%\n",
      "Training - Epoch 100, Loss: 0.861365, Accuracy: 88.39%\n",
      "Validation Batch 1, Loss: 0.824942, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.802309, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.858691, Accuracy: 92.71%\n",
      "Validation Batch 4, Loss: 0.849506, Accuracy: 92.19%\n",
      "Validation Batch 5, Loss: 0.836967, Accuracy: 91.88%\n",
      "Validation Batch 6, Loss: 0.799273, Accuracy: 92.45%\n",
      "Validation Batch 7, Loss: 0.808593, Accuracy: 92.86%\n",
      "Validation Batch 8, Loss: 0.888384, Accuracy: 91.99%\n",
      "Validation Batch 9, Loss: 0.902062, Accuracy: 91.15%\n",
      "Validation Batch 10, Loss: 0.827958, Accuracy: 91.25%\n",
      "Validation Batch 11, Loss: 0.839023, Accuracy: 91.34%\n",
      "Validation Batch 12, Loss: 0.837528, Accuracy: 91.41%\n",
      "Validation Batch 13, Loss: 0.845106, Accuracy: 91.23%\n",
      "Validation Batch 14, Loss: 0.860047, Accuracy: 90.96%\n",
      "Validation Batch 15, Loss: 0.831784, Accuracy: 91.04%\n",
      "Validation Batch 16, Loss: 0.840804, Accuracy: 91.02%\n",
      "Validation Batch 17, Loss: 0.872044, Accuracy: 90.90%\n",
      "Validation Batch 18, Loss: 0.820591, Accuracy: 90.89%\n",
      "Validation Batch 19, Loss: 0.885632, Accuracy: 90.71%\n",
      "Validation Batch 20, Loss: 0.874361, Accuracy: 90.47%\n",
      "Validation Batch 21, Loss: 0.872623, Accuracy: 90.33%\n",
      "Validation Batch 22, Loss: 0.851469, Accuracy: 90.20%\n",
      "Validation Batch 23, Loss: 0.878831, Accuracy: 90.01%\n",
      "Validation Batch 24, Loss: 0.852478, Accuracy: 89.97%\n",
      "Validation Batch 25, Loss: 0.825644, Accuracy: 90.00%\n",
      "Validation Batch 26, Loss: 0.864384, Accuracy: 89.90%\n",
      "Validation Batch 27, Loss: 0.806809, Accuracy: 90.02%\n",
      "Validation - Epoch 100, Loss: 0.846587, Accuracy: 90.02%\n",
      "Patience—4\n",
      "Epoch 101\n",
      "Batch 1, Loss: 0.850846, Accuracy: 89.06%\n",
      "Batch 2, Loss: 0.837386, Accuracy: 90.62%\n",
      "Batch 3, Loss: 0.891404, Accuracy: 89.06%\n",
      "Batch 4, Loss: 0.864936, Accuracy: 89.06%\n",
      "Batch 5, Loss: 0.865512, Accuracy: 89.06%\n",
      "Batch 6, Loss: 0.844966, Accuracy: 89.06%\n",
      "Batch 7, Loss: 0.899012, Accuracy: 88.17%\n",
      "Batch 8, Loss: 0.832856, Accuracy: 88.67%\n",
      "Batch 9, Loss: 0.890694, Accuracy: 88.02%\n",
      "Batch 10, Loss: 0.920057, Accuracy: 87.50%\n",
      "Batch 11, Loss: 0.853348, Accuracy: 87.64%\n",
      "Batch 12, Loss: 0.834709, Accuracy: 87.89%\n",
      "Batch 13, Loss: 0.893841, Accuracy: 87.74%\n",
      "Batch 14, Loss: 0.794860, Accuracy: 88.28%\n",
      "Batch 15, Loss: 0.868847, Accuracy: 88.33%\n",
      "Batch 16, Loss: 0.824617, Accuracy: 88.48%\n",
      "Batch 17, Loss: 0.904194, Accuracy: 88.24%\n",
      "Batch 18, Loss: 0.971864, Accuracy: 87.67%\n",
      "Batch 19, Loss: 0.817509, Accuracy: 87.99%\n",
      "Batch 20, Loss: 0.857515, Accuracy: 88.12%\n",
      "Batch 21, Loss: 0.875415, Accuracy: 88.02%\n",
      "Batch 22, Loss: 0.924902, Accuracy: 87.71%\n",
      "Batch 23, Loss: 0.852839, Accuracy: 87.70%\n",
      "Batch 24, Loss: 0.872682, Accuracy: 87.76%\n",
      "Batch 25, Loss: 0.843651, Accuracy: 87.81%\n",
      "Batch 26, Loss: 0.794026, Accuracy: 88.10%\n",
      "Batch 27, Loss: 0.880186, Accuracy: 88.02%\n",
      "Batch 28, Loss: 0.851391, Accuracy: 88.06%\n",
      "Batch 29, Loss: 0.809448, Accuracy: 88.25%\n",
      "Batch 30, Loss: 0.859583, Accuracy: 88.28%\n",
      "Batch 31, Loss: 0.858781, Accuracy: 88.31%\n",
      "Batch 32, Loss: 0.883456, Accuracy: 88.28%\n",
      "Batch 33, Loss: 0.913214, Accuracy: 88.12%\n",
      "Batch 34, Loss: 0.871323, Accuracy: 88.10%\n",
      "Batch 35, Loss: 0.867725, Accuracy: 88.08%\n",
      "Batch 36, Loss: 0.859939, Accuracy: 88.11%\n",
      "Batch 37, Loss: 0.836117, Accuracy: 88.22%\n",
      "Batch 38, Loss: 0.983577, Accuracy: 87.91%\n",
      "Batch 39, Loss: 0.844556, Accuracy: 87.94%\n",
      "Batch 40, Loss: 0.912048, Accuracy: 87.81%\n",
      "Batch 41, Loss: 0.867382, Accuracy: 87.80%\n",
      "Batch 42, Loss: 0.877037, Accuracy: 87.76%\n",
      "Batch 43, Loss: 0.891799, Accuracy: 87.68%\n",
      "Batch 44, Loss: 0.847737, Accuracy: 87.71%\n",
      "Batch 45, Loss: 0.911559, Accuracy: 87.57%\n",
      "Batch 46, Loss: 0.795972, Accuracy: 87.70%\n",
      "Batch 47, Loss: 0.825161, Accuracy: 87.80%\n",
      "Batch 48, Loss: 0.873743, Accuracy: 87.76%\n",
      "Batch 49, Loss: 0.875871, Accuracy: 87.76%\n",
      "Batch 50, Loss: 0.795300, Accuracy: 87.88%\n",
      "Batch 51, Loss: 0.912670, Accuracy: 87.81%\n",
      "Batch 52, Loss: 0.899585, Accuracy: 87.74%\n",
      "Batch 53, Loss: 0.878117, Accuracy: 87.71%\n",
      "Batch 54, Loss: 0.932320, Accuracy: 87.59%\n",
      "Batch 55, Loss: 0.820598, Accuracy: 87.67%\n",
      "Batch 56, Loss: 0.840448, Accuracy: 87.72%\n",
      "Batch 57, Loss: 0.898928, Accuracy: 87.66%\n",
      "Batch 58, Loss: 0.875693, Accuracy: 87.69%\n",
      "Batch 59, Loss: 0.852478, Accuracy: 87.74%\n",
      "Batch 60, Loss: 0.873238, Accuracy: 87.73%\n",
      "Batch 61, Loss: 0.886217, Accuracy: 87.73%\n",
      "Batch 62, Loss: 0.932951, Accuracy: 87.63%\n",
      "Batch 63, Loss: 0.873426, Accuracy: 87.60%\n",
      "Batch 64, Loss: 0.777227, Accuracy: 87.74%\n",
      "Batch 65, Loss: 0.894246, Accuracy: 87.72%\n",
      "Batch 66, Loss: 0.885727, Accuracy: 87.69%\n",
      "Batch 67, Loss: 0.880469, Accuracy: 87.66%\n",
      "Batch 68, Loss: 0.858792, Accuracy: 87.66%\n",
      "Batch 69, Loss: 0.851327, Accuracy: 87.68%\n",
      "Batch 70, Loss: 0.830535, Accuracy: 87.72%\n",
      "Batch 71, Loss: 0.863583, Accuracy: 87.72%\n",
      "Batch 72, Loss: 0.834139, Accuracy: 87.78%\n",
      "Batch 73, Loss: 0.912396, Accuracy: 87.71%\n",
      "Batch 74, Loss: 0.838537, Accuracy: 87.77%\n",
      "Batch 75, Loss: 0.849660, Accuracy: 87.77%\n",
      "Batch 76, Loss: 0.803593, Accuracy: 87.85%\n",
      "Batch 77, Loss: 0.899005, Accuracy: 87.78%\n",
      "Batch 78, Loss: 0.850818, Accuracy: 87.80%\n",
      "Batch 79, Loss: 0.868649, Accuracy: 87.80%\n",
      "Batch 80, Loss: 0.877330, Accuracy: 87.79%\n",
      "Batch 81, Loss: 0.889261, Accuracy: 87.75%\n",
      "Batch 82, Loss: 0.811062, Accuracy: 87.80%\n",
      "Batch 83, Loss: 0.843778, Accuracy: 87.82%\n",
      "Batch 84, Loss: 0.865574, Accuracy: 87.80%\n",
      "Batch 85, Loss: 0.910268, Accuracy: 87.76%\n",
      "Batch 86, Loss: 0.822201, Accuracy: 87.81%\n",
      "Batch 87, Loss: 0.834269, Accuracy: 87.84%\n",
      "Batch 88, Loss: 0.817484, Accuracy: 87.91%\n",
      "Batch 89, Loss: 0.833202, Accuracy: 87.96%\n",
      "Batch 90, Loss: 0.811113, Accuracy: 88.02%\n",
      "Batch 91, Loss: 0.884535, Accuracy: 88.00%\n",
      "Batch 92, Loss: 0.858518, Accuracy: 87.99%\n",
      "Batch 93, Loss: 0.872609, Accuracy: 87.99%\n",
      "Batch 94, Loss: 0.851524, Accuracy: 88.00%\n",
      "Batch 95, Loss: 0.864352, Accuracy: 88.01%\n",
      "Batch 96, Loss: 0.911192, Accuracy: 87.96%\n",
      "Batch 97, Loss: 0.791576, Accuracy: 88.03%\n",
      "Batch 98, Loss: 0.815342, Accuracy: 88.09%\n",
      "Batch 99, Loss: 0.830324, Accuracy: 88.12%\n",
      "Batch 100, Loss: 0.880272, Accuracy: 88.11%\n",
      "Batch 101, Loss: 0.945776, Accuracy: 88.04%\n",
      "Batch 102, Loss: 0.877271, Accuracy: 88.02%\n",
      "Batch 103, Loss: 0.906860, Accuracy: 87.97%\n",
      "Batch 104, Loss: 0.846287, Accuracy: 88.00%\n",
      "Batch 105, Loss: 0.889696, Accuracy: 87.96%\n",
      "Batch 106, Loss: 0.874101, Accuracy: 87.96%\n",
      "Batch 107, Loss: 0.891109, Accuracy: 87.94%\n",
      "Batch 108, Loss: 0.886933, Accuracy: 87.92%\n",
      "Batch 109, Loss: 0.805260, Accuracy: 87.97%\n",
      "Batch 110, Loss: 0.874462, Accuracy: 87.95%\n",
      "Batch 111, Loss: 0.908589, Accuracy: 87.91%\n",
      "Batch 112, Loss: 0.884689, Accuracy: 87.90%\n",
      "Batch 113, Loss: 0.877821, Accuracy: 87.87%\n",
      "Batch 114, Loss: 0.932114, Accuracy: 87.80%\n",
      "Batch 115, Loss: 0.910306, Accuracy: 87.74%\n",
      "Batch 116, Loss: 0.894559, Accuracy: 87.73%\n",
      "Batch 117, Loss: 0.905473, Accuracy: 87.69%\n",
      "Batch 118, Loss: 0.799210, Accuracy: 87.75%\n",
      "Batch 119, Loss: 0.870386, Accuracy: 87.75%\n",
      "Batch 120, Loss: 0.862853, Accuracy: 87.75%\n",
      "Batch 121, Loss: 0.896141, Accuracy: 87.73%\n",
      "Batch 122, Loss: 0.892371, Accuracy: 87.72%\n",
      "Batch 123, Loss: 0.825020, Accuracy: 87.75%\n",
      "Batch 124, Loss: 0.892157, Accuracy: 87.74%\n",
      "Batch 125, Loss: 0.957079, Accuracy: 87.66%\n",
      "Batch 126, Loss: 0.824006, Accuracy: 87.71%\n",
      "Batch 127, Loss: 0.875180, Accuracy: 87.71%\n",
      "Batch 128, Loss: 0.874590, Accuracy: 87.70%\n",
      "Batch 129, Loss: 0.886763, Accuracy: 87.71%\n",
      "Batch 130, Loss: 0.826433, Accuracy: 87.74%\n",
      "Batch 131, Loss: 0.832065, Accuracy: 87.76%\n",
      "Batch 132, Loss: 0.847429, Accuracy: 87.78%\n",
      "Batch 133, Loss: 0.781396, Accuracy: 87.84%\n",
      "Batch 134, Loss: 0.868264, Accuracy: 87.84%\n",
      "Batch 135, Loss: 0.801089, Accuracy: 87.89%\n",
      "Batch 136, Loss: 0.802674, Accuracy: 87.94%\n",
      "Batch 137, Loss: 0.838255, Accuracy: 87.96%\n",
      "Batch 138, Loss: 0.842321, Accuracy: 87.99%\n",
      "Batch 139, Loss: 0.861852, Accuracy: 87.98%\n",
      "Batch 140, Loss: 0.887802, Accuracy: 87.95%\n",
      "Batch 141, Loss: 0.907693, Accuracy: 87.92%\n",
      "Batch 142, Loss: 0.788623, Accuracy: 87.98%\n",
      "Batch 143, Loss: 0.881104, Accuracy: 87.97%\n",
      "Batch 144, Loss: 0.884679, Accuracy: 87.94%\n",
      "Batch 145, Loss: 0.857387, Accuracy: 87.96%\n",
      "Batch 146, Loss: 0.910698, Accuracy: 87.93%\n",
      "Batch 147, Loss: 0.841562, Accuracy: 87.95%\n",
      "Batch 148, Loss: 0.784156, Accuracy: 88.01%\n",
      "Batch 149, Loss: 0.858809, Accuracy: 88.01%\n",
      "Batch 150, Loss: 0.892868, Accuracy: 87.99%\n",
      "Batch 151, Loss: 0.964383, Accuracy: 87.90%\n",
      "Batch 152, Loss: 0.908908, Accuracy: 87.87%\n",
      "Batch 153, Loss: 0.917458, Accuracy: 87.84%\n",
      "Batch 154, Loss: 0.872796, Accuracy: 87.82%\n",
      "Batch 155, Loss: 0.935954, Accuracy: 87.77%\n",
      "Batch 156, Loss: 0.859582, Accuracy: 87.77%\n",
      "Batch 157, Loss: 0.837593, Accuracy: 87.79%\n",
      "Batch 158, Loss: 0.886984, Accuracy: 87.78%\n",
      "Batch 159, Loss: 0.920582, Accuracy: 87.74%\n",
      "Batch 160, Loss: 0.884511, Accuracy: 87.71%\n",
      "Batch 161, Loss: 0.826983, Accuracy: 87.74%\n",
      "Batch 162, Loss: 0.851508, Accuracy: 87.75%\n",
      "Batch 163, Loss: 0.887076, Accuracy: 87.73%\n",
      "Batch 164, Loss: 0.836625, Accuracy: 87.75%\n",
      "Batch 165, Loss: 0.832194, Accuracy: 87.77%\n",
      "Batch 166, Loss: 0.908462, Accuracy: 87.73%\n",
      "Batch 167, Loss: 0.882832, Accuracy: 87.71%\n",
      "Batch 168, Loss: 0.837228, Accuracy: 87.71%\n",
      "Batch 169, Loss: 0.857790, Accuracy: 87.72%\n",
      "Batch 170, Loss: 0.837494, Accuracy: 87.74%\n",
      "Batch 171, Loss: 0.830655, Accuracy: 87.76%\n",
      "Batch 172, Loss: 0.908894, Accuracy: 87.74%\n",
      "Batch 173, Loss: 0.854327, Accuracy: 87.74%\n",
      "Batch 174, Loss: 0.787433, Accuracy: 87.79%\n",
      "Batch 175, Loss: 0.842392, Accuracy: 87.79%\n",
      "Batch 176, Loss: 0.825148, Accuracy: 87.82%\n",
      "Batch 177, Loss: 0.815362, Accuracy: 87.85%\n",
      "Batch 178, Loss: 0.840043, Accuracy: 87.87%\n",
      "Batch 179, Loss: 0.947307, Accuracy: 87.81%\n",
      "Batch 180, Loss: 0.926937, Accuracy: 87.77%\n",
      "Batch 181, Loss: 0.892923, Accuracy: 87.74%\n",
      "Batch 182, Loss: 0.856175, Accuracy: 87.75%\n",
      "Batch 183, Loss: 0.865203, Accuracy: 87.75%\n",
      "Batch 184, Loss: 0.849118, Accuracy: 87.75%\n",
      "Batch 185, Loss: 0.902439, Accuracy: 87.73%\n",
      "Batch 186, Loss: 0.888196, Accuracy: 87.72%\n",
      "Batch 187, Loss: 0.882307, Accuracy: 87.71%\n",
      "Batch 188, Loss: 0.875803, Accuracy: 87.70%\n",
      "Batch 189, Loss: 0.974602, Accuracy: 87.63%\n",
      "Batch 190, Loss: 0.852288, Accuracy: 87.65%\n",
      "Batch 191, Loss: 0.877495, Accuracy: 87.64%\n",
      "Batch 192, Loss: 0.861411, Accuracy: 87.64%\n",
      "Batch 193, Loss: 0.808431, Accuracy: 87.67%\n",
      "Batch 194, Loss: 0.855550, Accuracy: 87.69%\n",
      "Batch 195, Loss: 0.963880, Accuracy: 87.63%\n",
      "Batch 196, Loss: 0.792509, Accuracy: 87.67%\n",
      "Batch 197, Loss: 0.867774, Accuracy: 87.67%\n",
      "Batch 198, Loss: 0.830260, Accuracy: 87.69%\n",
      "Batch 199, Loss: 0.844572, Accuracy: 87.70%\n",
      "Batch 200, Loss: 0.837862, Accuracy: 87.72%\n",
      "Batch 201, Loss: 0.871833, Accuracy: 87.72%\n",
      "Batch 202, Loss: 0.794049, Accuracy: 87.76%\n",
      "Batch 203, Loss: 0.867260, Accuracy: 87.75%\n",
      "Batch 204, Loss: 0.866673, Accuracy: 87.76%\n",
      "Batch 205, Loss: 0.875683, Accuracy: 87.76%\n",
      "Batch 206, Loss: 0.861602, Accuracy: 87.77%\n",
      "Batch 207, Loss: 0.899369, Accuracy: 87.75%\n",
      "Batch 208, Loss: 0.853233, Accuracy: 87.76%\n",
      "Batch 209, Loss: 0.838952, Accuracy: 87.77%\n",
      "Batch 210, Loss: 0.866947, Accuracy: 87.77%\n",
      "Batch 211, Loss: 0.843355, Accuracy: 87.77%\n",
      "Batch 212, Loss: 0.852530, Accuracy: 87.78%\n",
      "Batch 213, Loss: 0.886585, Accuracy: 87.76%\n",
      "Training - Epoch 101, Loss: 0.865601, Accuracy: 87.76%\n",
      "Validation Batch 1, Loss: 0.813561, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.791007, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.849286, Accuracy: 92.71%\n",
      "Validation Batch 4, Loss: 0.833973, Accuracy: 92.19%\n",
      "Validation Batch 5, Loss: 0.824012, Accuracy: 91.88%\n",
      "Validation Batch 6, Loss: 0.789071, Accuracy: 92.71%\n",
      "Validation Batch 7, Loss: 0.795951, Accuracy: 93.08%\n",
      "Validation Batch 8, Loss: 0.877126, Accuracy: 92.19%\n",
      "Validation Batch 9, Loss: 0.886349, Accuracy: 91.67%\n",
      "Validation Batch 10, Loss: 0.819418, Accuracy: 91.72%\n",
      "Validation Batch 11, Loss: 0.827557, Accuracy: 91.76%\n",
      "Validation Batch 12, Loss: 0.826849, Accuracy: 91.80%\n",
      "Validation Batch 13, Loss: 0.828617, Accuracy: 91.95%\n",
      "Validation Batch 14, Loss: 0.840615, Accuracy: 91.96%\n",
      "Validation Batch 15, Loss: 0.819216, Accuracy: 92.08%\n",
      "Validation Batch 16, Loss: 0.828330, Accuracy: 92.09%\n",
      "Validation Batch 17, Loss: 0.870839, Accuracy: 91.82%\n",
      "Validation Batch 18, Loss: 0.802413, Accuracy: 91.93%\n",
      "Validation Batch 19, Loss: 0.878405, Accuracy: 91.61%\n",
      "Validation Batch 20, Loss: 0.839721, Accuracy: 91.64%\n",
      "Validation Batch 21, Loss: 0.864411, Accuracy: 91.44%\n",
      "Validation Batch 22, Loss: 0.837528, Accuracy: 91.41%\n",
      "Validation Batch 23, Loss: 0.865495, Accuracy: 91.30%\n",
      "Validation Batch 24, Loss: 0.835016, Accuracy: 91.34%\n",
      "Validation Batch 25, Loss: 0.814288, Accuracy: 91.38%\n",
      "Validation Batch 26, Loss: 0.850781, Accuracy: 91.29%\n",
      "Validation Batch 27, Loss: 0.796579, Accuracy: 91.37%\n",
      "Validation - Epoch 101, Loss: 0.833571, Accuracy: 91.37%\n",
      "Patience—0\n",
      "Epoch 102\n",
      "Batch 1, Loss: 0.865612, Accuracy: 87.50%\n",
      "Batch 2, Loss: 0.892208, Accuracy: 85.94%\n",
      "Batch 3, Loss: 0.886130, Accuracy: 85.94%\n",
      "Batch 4, Loss: 0.819253, Accuracy: 87.89%\n",
      "Batch 5, Loss: 0.943339, Accuracy: 85.94%\n",
      "Batch 6, Loss: 0.792727, Accuracy: 87.50%\n",
      "Batch 7, Loss: 0.908083, Accuracy: 86.61%\n",
      "Batch 8, Loss: 0.904076, Accuracy: 86.33%\n",
      "Batch 9, Loss: 0.859765, Accuracy: 86.63%\n",
      "Batch 10, Loss: 0.890607, Accuracy: 86.41%\n",
      "Batch 11, Loss: 0.909585, Accuracy: 86.08%\n",
      "Batch 12, Loss: 0.794907, Accuracy: 86.85%\n",
      "Batch 13, Loss: 0.917685, Accuracy: 86.42%\n",
      "Batch 14, Loss: 0.919802, Accuracy: 86.05%\n",
      "Batch 15, Loss: 0.814535, Accuracy: 86.56%\n",
      "Batch 16, Loss: 0.891258, Accuracy: 86.33%\n",
      "Batch 17, Loss: 0.833369, Accuracy: 86.67%\n",
      "Batch 18, Loss: 0.863197, Accuracy: 86.72%\n",
      "Batch 19, Loss: 0.855587, Accuracy: 86.84%\n",
      "Batch 20, Loss: 0.914911, Accuracy: 86.64%\n",
      "Batch 21, Loss: 0.812297, Accuracy: 86.90%\n",
      "Batch 22, Loss: 0.863809, Accuracy: 86.93%\n",
      "Batch 23, Loss: 0.825564, Accuracy: 87.16%\n",
      "Batch 24, Loss: 0.854863, Accuracy: 87.30%\n",
      "Batch 25, Loss: 0.863694, Accuracy: 87.25%\n",
      "Batch 26, Loss: 0.940084, Accuracy: 87.02%\n",
      "Batch 27, Loss: 0.869353, Accuracy: 87.04%\n",
      "Batch 28, Loss: 0.910472, Accuracy: 86.89%\n",
      "Batch 29, Loss: 0.821556, Accuracy: 87.07%\n",
      "Batch 30, Loss: 0.806513, Accuracy: 87.24%\n",
      "Batch 31, Loss: 0.917727, Accuracy: 87.05%\n",
      "Batch 32, Loss: 0.791409, Accuracy: 87.30%\n",
      "Batch 33, Loss: 0.885664, Accuracy: 87.26%\n",
      "Batch 34, Loss: 0.843773, Accuracy: 87.36%\n",
      "Batch 35, Loss: 0.866086, Accuracy: 87.28%\n",
      "Batch 36, Loss: 0.909857, Accuracy: 87.15%\n",
      "Batch 37, Loss: 0.800027, Accuracy: 87.33%\n",
      "Batch 38, Loss: 0.869561, Accuracy: 87.34%\n",
      "Batch 39, Loss: 0.870539, Accuracy: 87.34%\n",
      "Batch 40, Loss: 0.854938, Accuracy: 87.34%\n",
      "Batch 41, Loss: 0.898807, Accuracy: 87.27%\n",
      "Batch 42, Loss: 0.873351, Accuracy: 87.28%\n",
      "Batch 43, Loss: 0.963920, Accuracy: 87.06%\n",
      "Batch 44, Loss: 0.776261, Accuracy: 87.29%\n",
      "Batch 45, Loss: 0.769686, Accuracy: 87.53%\n",
      "Batch 46, Loss: 0.890279, Accuracy: 87.47%\n",
      "Batch 47, Loss: 0.843374, Accuracy: 87.50%\n",
      "Batch 48, Loss: 0.819513, Accuracy: 87.60%\n",
      "Batch 49, Loss: 0.933335, Accuracy: 87.47%\n",
      "Batch 50, Loss: 0.861164, Accuracy: 87.47%\n",
      "Batch 51, Loss: 0.842272, Accuracy: 87.53%\n",
      "Batch 52, Loss: 0.817148, Accuracy: 87.65%\n",
      "Batch 53, Loss: 0.815304, Accuracy: 87.74%\n",
      "Batch 54, Loss: 0.888341, Accuracy: 87.70%\n",
      "Batch 55, Loss: 0.816422, Accuracy: 87.78%\n",
      "Batch 56, Loss: 0.884658, Accuracy: 87.75%\n",
      "Batch 57, Loss: 0.909605, Accuracy: 87.64%\n",
      "Batch 58, Loss: 0.841980, Accuracy: 87.66%\n",
      "Batch 59, Loss: 0.862958, Accuracy: 87.69%\n",
      "Batch 60, Loss: 0.865550, Accuracy: 87.71%\n",
      "Batch 61, Loss: 0.851695, Accuracy: 87.73%\n",
      "Batch 62, Loss: 0.839944, Accuracy: 87.75%\n",
      "Batch 63, Loss: 0.879101, Accuracy: 87.72%\n",
      "Batch 64, Loss: 0.833557, Accuracy: 87.77%\n",
      "Batch 65, Loss: 0.839538, Accuracy: 87.81%\n",
      "Batch 66, Loss: 0.838307, Accuracy: 87.86%\n",
      "Batch 67, Loss: 0.882992, Accuracy: 87.80%\n",
      "Batch 68, Loss: 0.824964, Accuracy: 87.84%\n",
      "Batch 69, Loss: 0.946543, Accuracy: 87.73%\n",
      "Batch 70, Loss: 0.836502, Accuracy: 87.77%\n",
      "Batch 71, Loss: 0.917105, Accuracy: 87.70%\n",
      "Batch 72, Loss: 0.829742, Accuracy: 87.74%\n",
      "Batch 73, Loss: 0.919125, Accuracy: 87.67%\n",
      "Batch 74, Loss: 0.890785, Accuracy: 87.65%\n",
      "Batch 75, Loss: 0.848140, Accuracy: 87.67%\n",
      "Batch 76, Loss: 0.853662, Accuracy: 87.71%\n",
      "Batch 77, Loss: 0.845679, Accuracy: 87.72%\n",
      "Batch 78, Loss: 0.925349, Accuracy: 87.64%\n",
      "Batch 79, Loss: 0.851996, Accuracy: 87.66%\n",
      "Batch 80, Loss: 0.832789, Accuracy: 87.70%\n",
      "Batch 81, Loss: 0.937099, Accuracy: 87.62%\n",
      "Batch 82, Loss: 0.914706, Accuracy: 87.54%\n",
      "Batch 83, Loss: 0.891422, Accuracy: 87.52%\n",
      "Batch 84, Loss: 0.825460, Accuracy: 87.57%\n",
      "Batch 85, Loss: 0.859961, Accuracy: 87.57%\n",
      "Batch 86, Loss: 0.808497, Accuracy: 87.65%\n",
      "Batch 87, Loss: 0.847745, Accuracy: 87.68%\n",
      "Batch 88, Loss: 0.901507, Accuracy: 87.64%\n",
      "Batch 89, Loss: 0.894667, Accuracy: 87.59%\n",
      "Batch 90, Loss: 0.899475, Accuracy: 87.55%\n",
      "Batch 91, Loss: 0.854495, Accuracy: 87.59%\n",
      "Batch 92, Loss: 0.849241, Accuracy: 87.62%\n",
      "Batch 93, Loss: 0.886037, Accuracy: 87.60%\n",
      "Batch 94, Loss: 0.814639, Accuracy: 87.67%\n",
      "Batch 95, Loss: 0.843568, Accuracy: 87.70%\n",
      "Batch 96, Loss: 0.905698, Accuracy: 87.66%\n",
      "Batch 97, Loss: 0.900347, Accuracy: 87.63%\n",
      "Batch 98, Loss: 0.871093, Accuracy: 87.64%\n",
      "Batch 99, Loss: 0.931780, Accuracy: 87.58%\n",
      "Batch 100, Loss: 0.948600, Accuracy: 87.50%\n",
      "Batch 101, Loss: 0.852963, Accuracy: 87.52%\n",
      "Batch 102, Loss: 0.820384, Accuracy: 87.58%\n",
      "Batch 103, Loss: 0.902747, Accuracy: 87.55%\n",
      "Batch 104, Loss: 0.860432, Accuracy: 87.56%\n",
      "Batch 105, Loss: 0.810718, Accuracy: 87.62%\n",
      "Batch 106, Loss: 0.925538, Accuracy: 87.56%\n",
      "Batch 107, Loss: 0.931142, Accuracy: 87.47%\n",
      "Batch 108, Loss: 0.811206, Accuracy: 87.53%\n",
      "Batch 109, Loss: 0.847544, Accuracy: 87.54%\n",
      "Batch 110, Loss: 0.813016, Accuracy: 87.60%\n",
      "Batch 111, Loss: 0.859769, Accuracy: 87.58%\n",
      "Batch 112, Loss: 0.860024, Accuracy: 87.57%\n",
      "Batch 113, Loss: 0.858796, Accuracy: 87.58%\n",
      "Batch 114, Loss: 0.833509, Accuracy: 87.62%\n",
      "Batch 115, Loss: 0.935867, Accuracy: 87.57%\n",
      "Batch 116, Loss: 0.854370, Accuracy: 87.57%\n",
      "Batch 117, Loss: 0.870319, Accuracy: 87.58%\n",
      "Batch 118, Loss: 0.862260, Accuracy: 87.58%\n",
      "Batch 119, Loss: 0.826135, Accuracy: 87.62%\n",
      "Batch 120, Loss: 0.774238, Accuracy: 87.71%\n",
      "Batch 121, Loss: 0.885010, Accuracy: 87.68%\n",
      "Batch 122, Loss: 0.901085, Accuracy: 87.65%\n",
      "Batch 123, Loss: 0.842771, Accuracy: 87.67%\n",
      "Batch 124, Loss: 0.845137, Accuracy: 87.68%\n",
      "Batch 125, Loss: 0.882579, Accuracy: 87.67%\n",
      "Batch 126, Loss: 0.805077, Accuracy: 87.72%\n",
      "Batch 127, Loss: 0.964237, Accuracy: 87.64%\n",
      "Batch 128, Loss: 0.871221, Accuracy: 87.63%\n",
      "Batch 129, Loss: 0.875993, Accuracy: 87.63%\n",
      "Batch 130, Loss: 0.854437, Accuracy: 87.64%\n",
      "Batch 131, Loss: 0.847165, Accuracy: 87.66%\n",
      "Batch 132, Loss: 0.818448, Accuracy: 87.69%\n",
      "Batch 133, Loss: 0.826089, Accuracy: 87.72%\n",
      "Batch 134, Loss: 0.812265, Accuracy: 87.77%\n",
      "Batch 135, Loss: 0.873473, Accuracy: 87.77%\n",
      "Batch 136, Loss: 0.842587, Accuracy: 87.79%\n",
      "Batch 137, Loss: 0.854497, Accuracy: 87.80%\n",
      "Batch 138, Loss: 0.858761, Accuracy: 87.81%\n",
      "Batch 139, Loss: 0.896065, Accuracy: 87.79%\n",
      "Batch 140, Loss: 0.822797, Accuracy: 87.82%\n",
      "Batch 141, Loss: 0.967547, Accuracy: 87.74%\n",
      "Batch 142, Loss: 0.861976, Accuracy: 87.74%\n",
      "Batch 143, Loss: 0.927210, Accuracy: 87.70%\n",
      "Batch 144, Loss: 0.925396, Accuracy: 87.64%\n",
      "Batch 145, Loss: 0.841907, Accuracy: 87.66%\n",
      "Batch 146, Loss: 0.818011, Accuracy: 87.70%\n",
      "Batch 147, Loss: 0.826046, Accuracy: 87.72%\n",
      "Batch 148, Loss: 0.816293, Accuracy: 87.76%\n",
      "Batch 149, Loss: 0.805010, Accuracy: 87.80%\n",
      "Batch 150, Loss: 0.861091, Accuracy: 87.80%\n",
      "Batch 151, Loss: 0.860171, Accuracy: 87.80%\n",
      "Batch 152, Loss: 0.840892, Accuracy: 87.82%\n",
      "Batch 153, Loss: 0.839635, Accuracy: 87.84%\n",
      "Batch 154, Loss: 0.890844, Accuracy: 87.81%\n",
      "Batch 155, Loss: 0.875109, Accuracy: 87.80%\n",
      "Batch 156, Loss: 0.853770, Accuracy: 87.81%\n",
      "Batch 157, Loss: 0.875112, Accuracy: 87.80%\n",
      "Batch 158, Loss: 0.882352, Accuracy: 87.79%\n",
      "Batch 159, Loss: 0.857723, Accuracy: 87.79%\n",
      "Batch 160, Loss: 0.890798, Accuracy: 87.79%\n",
      "Batch 161, Loss: 0.847531, Accuracy: 87.79%\n",
      "Batch 162, Loss: 0.830018, Accuracy: 87.82%\n",
      "Batch 163, Loss: 0.828787, Accuracy: 87.85%\n",
      "Batch 164, Loss: 0.838521, Accuracy: 87.86%\n",
      "Batch 165, Loss: 0.844172, Accuracy: 87.87%\n",
      "Batch 166, Loss: 0.835855, Accuracy: 87.90%\n",
      "Batch 167, Loss: 0.830156, Accuracy: 87.92%\n",
      "Batch 168, Loss: 0.845426, Accuracy: 87.94%\n",
      "Batch 169, Loss: 0.927020, Accuracy: 87.91%\n",
      "Batch 170, Loss: 0.852355, Accuracy: 87.91%\n",
      "Batch 171, Loss: 0.874989, Accuracy: 87.90%\n",
      "Batch 172, Loss: 0.901983, Accuracy: 87.87%\n",
      "Batch 173, Loss: 0.931322, Accuracy: 87.83%\n",
      "Batch 174, Loss: 0.896905, Accuracy: 87.81%\n",
      "Batch 175, Loss: 0.844406, Accuracy: 87.80%\n",
      "Batch 176, Loss: 0.895549, Accuracy: 87.79%\n",
      "Batch 177, Loss: 0.811754, Accuracy: 87.83%\n",
      "Batch 178, Loss: 0.847212, Accuracy: 87.84%\n",
      "Batch 179, Loss: 0.843043, Accuracy: 87.86%\n",
      "Batch 180, Loss: 0.948120, Accuracy: 87.81%\n",
      "Batch 181, Loss: 0.878889, Accuracy: 87.80%\n",
      "Batch 182, Loss: 0.890940, Accuracy: 87.79%\n",
      "Batch 183, Loss: 0.901167, Accuracy: 87.77%\n",
      "Batch 184, Loss: 0.862225, Accuracy: 87.78%\n",
      "Batch 185, Loss: 0.808354, Accuracy: 87.80%\n",
      "Batch 186, Loss: 0.812266, Accuracy: 87.83%\n",
      "Batch 187, Loss: 0.901903, Accuracy: 87.81%\n",
      "Batch 188, Loss: 0.851413, Accuracy: 87.82%\n",
      "Batch 189, Loss: 0.812073, Accuracy: 87.85%\n",
      "Batch 190, Loss: 0.816238, Accuracy: 87.87%\n",
      "Batch 191, Loss: 0.912783, Accuracy: 87.84%\n",
      "Batch 192, Loss: 0.832130, Accuracy: 87.86%\n",
      "Batch 193, Loss: 0.843118, Accuracy: 87.88%\n",
      "Batch 194, Loss: 0.836568, Accuracy: 87.89%\n",
      "Batch 195, Loss: 0.890147, Accuracy: 87.88%\n",
      "Batch 196, Loss: 0.835813, Accuracy: 87.90%\n",
      "Batch 197, Loss: 0.821516, Accuracy: 87.92%\n",
      "Batch 198, Loss: 0.808330, Accuracy: 87.95%\n",
      "Batch 199, Loss: 0.839400, Accuracy: 87.96%\n",
      "Batch 200, Loss: 0.883787, Accuracy: 87.95%\n",
      "Batch 201, Loss: 0.883718, Accuracy: 87.93%\n",
      "Batch 202, Loss: 0.852315, Accuracy: 87.93%\n",
      "Batch 203, Loss: 0.909053, Accuracy: 87.91%\n",
      "Batch 204, Loss: 0.914036, Accuracy: 87.88%\n",
      "Batch 205, Loss: 0.868623, Accuracy: 87.88%\n",
      "Batch 206, Loss: 0.923073, Accuracy: 87.85%\n",
      "Batch 207, Loss: 0.868132, Accuracy: 87.85%\n",
      "Batch 208, Loss: 0.827500, Accuracy: 87.87%\n",
      "Batch 209, Loss: 0.820677, Accuracy: 87.89%\n",
      "Batch 210, Loss: 0.982053, Accuracy: 87.83%\n",
      "Batch 211, Loss: 0.906602, Accuracy: 87.81%\n",
      "Batch 212, Loss: 0.894401, Accuracy: 87.79%\n",
      "Batch 213, Loss: 0.856686, Accuracy: 87.80%\n",
      "Training - Epoch 102, Loss: 0.864463, Accuracy: 87.80%\n",
      "Validation Batch 1, Loss: 0.820105, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.805372, Accuracy: 93.75%\n",
      "Validation Batch 3, Loss: 0.851836, Accuracy: 92.19%\n",
      "Validation Batch 4, Loss: 0.840786, Accuracy: 91.80%\n",
      "Validation Batch 5, Loss: 0.834471, Accuracy: 91.56%\n",
      "Validation Batch 6, Loss: 0.796856, Accuracy: 92.19%\n",
      "Validation Batch 7, Loss: 0.805005, Accuracy: 92.63%\n",
      "Validation Batch 8, Loss: 0.885832, Accuracy: 91.60%\n",
      "Validation Batch 9, Loss: 0.892165, Accuracy: 90.80%\n",
      "Validation Batch 10, Loss: 0.820840, Accuracy: 90.94%\n",
      "Validation Batch 11, Loss: 0.829817, Accuracy: 91.05%\n",
      "Validation Batch 12, Loss: 0.829713, Accuracy: 91.15%\n",
      "Validation Batch 13, Loss: 0.839322, Accuracy: 91.11%\n",
      "Validation Batch 14, Loss: 0.848435, Accuracy: 91.18%\n",
      "Validation Batch 15, Loss: 0.829085, Accuracy: 91.25%\n",
      "Validation Batch 16, Loss: 0.835177, Accuracy: 91.21%\n",
      "Validation Batch 17, Loss: 0.873560, Accuracy: 91.08%\n",
      "Validation Batch 18, Loss: 0.816331, Accuracy: 91.15%\n",
      "Validation Batch 19, Loss: 0.884250, Accuracy: 90.87%\n",
      "Validation Batch 20, Loss: 0.854340, Accuracy: 90.70%\n",
      "Validation Batch 21, Loss: 0.871300, Accuracy: 90.55%\n",
      "Validation Batch 22, Loss: 0.848743, Accuracy: 90.41%\n",
      "Validation Batch 23, Loss: 0.872973, Accuracy: 90.29%\n",
      "Validation Batch 24, Loss: 0.847125, Accuracy: 90.30%\n",
      "Validation Batch 25, Loss: 0.825821, Accuracy: 90.38%\n",
      "Validation Batch 26, Loss: 0.854393, Accuracy: 90.32%\n",
      "Validation Batch 27, Loss: 0.807132, Accuracy: 90.37%\n",
      "Validation - Epoch 102, Loss: 0.841511, Accuracy: 90.37%\n",
      "Patience—1\n",
      "Epoch 103\n",
      "Batch 1, Loss: 0.924149, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.853022, Accuracy: 85.94%\n",
      "Batch 3, Loss: 0.829726, Accuracy: 88.02%\n",
      "Batch 4, Loss: 0.834093, Accuracy: 88.67%\n",
      "Batch 5, Loss: 0.928259, Accuracy: 87.19%\n",
      "Batch 6, Loss: 0.833433, Accuracy: 87.50%\n",
      "Batch 7, Loss: 0.815392, Accuracy: 88.39%\n",
      "Batch 8, Loss: 0.912598, Accuracy: 87.70%\n",
      "Batch 9, Loss: 0.825724, Accuracy: 88.19%\n",
      "Batch 10, Loss: 0.855929, Accuracy: 88.28%\n",
      "Batch 11, Loss: 0.850705, Accuracy: 88.35%\n",
      "Batch 12, Loss: 0.839748, Accuracy: 88.67%\n",
      "Batch 13, Loss: 0.793970, Accuracy: 89.18%\n",
      "Batch 14, Loss: 0.857835, Accuracy: 89.17%\n",
      "Batch 15, Loss: 0.877934, Accuracy: 88.85%\n",
      "Batch 16, Loss: 0.854347, Accuracy: 88.96%\n",
      "Batch 17, Loss: 0.842546, Accuracy: 89.06%\n",
      "Batch 18, Loss: 0.851550, Accuracy: 89.06%\n",
      "Batch 19, Loss: 0.894950, Accuracy: 88.90%\n",
      "Batch 20, Loss: 0.794895, Accuracy: 89.22%\n",
      "Batch 21, Loss: 0.852307, Accuracy: 89.21%\n",
      "Batch 22, Loss: 0.803482, Accuracy: 89.42%\n",
      "Batch 23, Loss: 0.885177, Accuracy: 89.27%\n",
      "Batch 24, Loss: 0.903449, Accuracy: 89.06%\n",
      "Batch 25, Loss: 0.827300, Accuracy: 89.12%\n",
      "Batch 26, Loss: 0.877261, Accuracy: 89.06%\n",
      "Batch 27, Loss: 0.840111, Accuracy: 89.12%\n",
      "Batch 28, Loss: 0.804373, Accuracy: 89.34%\n",
      "Batch 29, Loss: 0.851666, Accuracy: 89.39%\n",
      "Batch 30, Loss: 0.850120, Accuracy: 89.43%\n",
      "Batch 31, Loss: 0.831440, Accuracy: 89.47%\n",
      "Batch 32, Loss: 0.832039, Accuracy: 89.55%\n",
      "Batch 33, Loss: 0.880492, Accuracy: 89.49%\n",
      "Batch 34, Loss: 0.817722, Accuracy: 89.61%\n",
      "Batch 35, Loss: 0.919423, Accuracy: 89.46%\n",
      "Batch 36, Loss: 0.797208, Accuracy: 89.63%\n",
      "Batch 37, Loss: 0.861977, Accuracy: 89.57%\n",
      "Batch 38, Loss: 0.852726, Accuracy: 89.56%\n",
      "Batch 39, Loss: 0.871129, Accuracy: 89.54%\n",
      "Batch 40, Loss: 0.860444, Accuracy: 89.49%\n",
      "Batch 41, Loss: 0.941018, Accuracy: 89.25%\n",
      "Batch 42, Loss: 0.839899, Accuracy: 89.29%\n",
      "Batch 43, Loss: 0.862434, Accuracy: 89.24%\n",
      "Batch 44, Loss: 0.804410, Accuracy: 89.35%\n",
      "Batch 45, Loss: 0.927226, Accuracy: 89.17%\n",
      "Batch 46, Loss: 0.802140, Accuracy: 89.27%\n",
      "Batch 47, Loss: 0.915241, Accuracy: 89.13%\n",
      "Batch 48, Loss: 0.841416, Accuracy: 89.16%\n",
      "Batch 49, Loss: 0.796035, Accuracy: 89.29%\n",
      "Batch 50, Loss: 0.842210, Accuracy: 89.34%\n",
      "Batch 51, Loss: 0.871760, Accuracy: 89.31%\n",
      "Batch 52, Loss: 0.809719, Accuracy: 89.39%\n",
      "Batch 53, Loss: 0.917772, Accuracy: 89.24%\n",
      "Batch 54, Loss: 0.924009, Accuracy: 89.09%\n",
      "Batch 55, Loss: 0.894265, Accuracy: 89.01%\n",
      "Batch 56, Loss: 0.838241, Accuracy: 89.03%\n",
      "Batch 57, Loss: 0.869974, Accuracy: 89.04%\n",
      "Batch 58, Loss: 0.834063, Accuracy: 89.09%\n",
      "Batch 59, Loss: 0.908838, Accuracy: 89.01%\n",
      "Batch 60, Loss: 0.866272, Accuracy: 89.01%\n",
      "Batch 61, Loss: 0.933272, Accuracy: 88.88%\n",
      "Batch 62, Loss: 0.883242, Accuracy: 88.86%\n",
      "Batch 63, Loss: 0.878998, Accuracy: 88.79%\n",
      "Batch 64, Loss: 0.874153, Accuracy: 88.75%\n",
      "Batch 65, Loss: 0.839003, Accuracy: 88.77%\n",
      "Batch 66, Loss: 0.831633, Accuracy: 88.83%\n",
      "Batch 67, Loss: 0.905670, Accuracy: 88.74%\n",
      "Batch 68, Loss: 0.854434, Accuracy: 88.74%\n",
      "Batch 69, Loss: 0.824999, Accuracy: 88.79%\n",
      "Batch 70, Loss: 0.808412, Accuracy: 88.86%\n",
      "Batch 71, Loss: 0.820309, Accuracy: 88.93%\n",
      "Batch 72, Loss: 0.926734, Accuracy: 88.85%\n",
      "Batch 73, Loss: 0.927474, Accuracy: 88.76%\n",
      "Batch 74, Loss: 0.877454, Accuracy: 88.75%\n",
      "Batch 75, Loss: 0.912579, Accuracy: 88.69%\n",
      "Batch 76, Loss: 0.880564, Accuracy: 88.65%\n",
      "Batch 77, Loss: 0.824271, Accuracy: 88.70%\n",
      "Batch 78, Loss: 0.920089, Accuracy: 88.62%\n",
      "Batch 79, Loss: 0.911228, Accuracy: 88.55%\n",
      "Batch 80, Loss: 0.777909, Accuracy: 88.65%\n",
      "Batch 81, Loss: 0.950873, Accuracy: 88.52%\n",
      "Batch 82, Loss: 0.841958, Accuracy: 88.51%\n",
      "Batch 83, Loss: 0.836223, Accuracy: 88.54%\n",
      "Batch 84, Loss: 0.795999, Accuracy: 88.62%\n",
      "Batch 85, Loss: 0.898533, Accuracy: 88.57%\n",
      "Batch 86, Loss: 0.879659, Accuracy: 88.55%\n",
      "Batch 87, Loss: 0.894938, Accuracy: 88.51%\n",
      "Batch 88, Loss: 0.864463, Accuracy: 88.49%\n",
      "Batch 89, Loss: 0.827957, Accuracy: 88.54%\n",
      "Batch 90, Loss: 0.884131, Accuracy: 88.51%\n",
      "Batch 91, Loss: 0.835523, Accuracy: 88.53%\n",
      "Batch 92, Loss: 0.837409, Accuracy: 88.57%\n",
      "Batch 93, Loss: 0.898548, Accuracy: 88.51%\n",
      "Batch 94, Loss: 0.847381, Accuracy: 88.51%\n",
      "Batch 95, Loss: 0.856584, Accuracy: 88.52%\n",
      "Batch 96, Loss: 0.869393, Accuracy: 88.49%\n",
      "Batch 97, Loss: 0.883236, Accuracy: 88.48%\n",
      "Batch 98, Loss: 0.915130, Accuracy: 88.44%\n",
      "Batch 99, Loss: 0.879636, Accuracy: 88.43%\n",
      "Batch 100, Loss: 0.851307, Accuracy: 88.45%\n",
      "Batch 101, Loss: 0.877076, Accuracy: 88.43%\n",
      "Batch 102, Loss: 0.860128, Accuracy: 88.42%\n",
      "Batch 103, Loss: 0.868420, Accuracy: 88.43%\n",
      "Batch 104, Loss: 0.905631, Accuracy: 88.39%\n",
      "Batch 105, Loss: 0.833085, Accuracy: 88.41%\n",
      "Batch 106, Loss: 0.863329, Accuracy: 88.40%\n",
      "Batch 107, Loss: 0.855787, Accuracy: 88.39%\n",
      "Batch 108, Loss: 0.885385, Accuracy: 88.37%\n",
      "Batch 109, Loss: 0.820228, Accuracy: 88.42%\n",
      "Batch 110, Loss: 0.869152, Accuracy: 88.41%\n",
      "Batch 111, Loss: 0.848606, Accuracy: 88.41%\n",
      "Batch 112, Loss: 0.826572, Accuracy: 88.45%\n",
      "Batch 113, Loss: 0.854523, Accuracy: 88.47%\n",
      "Batch 114, Loss: 0.892819, Accuracy: 88.43%\n",
      "Batch 115, Loss: 0.888864, Accuracy: 88.41%\n",
      "Batch 116, Loss: 0.844504, Accuracy: 88.42%\n",
      "Batch 117, Loss: 0.872827, Accuracy: 88.41%\n",
      "Batch 118, Loss: 0.855607, Accuracy: 88.41%\n",
      "Batch 119, Loss: 0.840792, Accuracy: 88.42%\n",
      "Batch 120, Loss: 0.830994, Accuracy: 88.45%\n",
      "Batch 121, Loss: 0.857507, Accuracy: 88.47%\n",
      "Batch 122, Loss: 0.875351, Accuracy: 88.43%\n",
      "Batch 123, Loss: 0.836055, Accuracy: 88.45%\n",
      "Batch 124, Loss: 0.817192, Accuracy: 88.51%\n",
      "Batch 125, Loss: 0.894207, Accuracy: 88.47%\n",
      "Batch 126, Loss: 0.799613, Accuracy: 88.53%\n",
      "Batch 127, Loss: 0.844248, Accuracy: 88.55%\n",
      "Batch 128, Loss: 0.808513, Accuracy: 88.59%\n",
      "Batch 129, Loss: 0.866235, Accuracy: 88.58%\n",
      "Batch 130, Loss: 0.818761, Accuracy: 88.62%\n",
      "Batch 131, Loss: 0.843032, Accuracy: 88.63%\n",
      "Batch 132, Loss: 0.867078, Accuracy: 88.62%\n",
      "Batch 133, Loss: 0.930500, Accuracy: 88.58%\n",
      "Batch 134, Loss: 0.825283, Accuracy: 88.62%\n",
      "Batch 135, Loss: 0.892930, Accuracy: 88.58%\n",
      "Batch 136, Loss: 0.909357, Accuracy: 88.53%\n",
      "Batch 137, Loss: 0.846947, Accuracy: 88.53%\n",
      "Batch 138, Loss: 0.806780, Accuracy: 88.56%\n",
      "Batch 139, Loss: 0.868416, Accuracy: 88.55%\n",
      "Batch 140, Loss: 0.881128, Accuracy: 88.53%\n",
      "Batch 141, Loss: 0.844863, Accuracy: 88.54%\n",
      "Batch 142, Loss: 0.864908, Accuracy: 88.53%\n",
      "Batch 143, Loss: 0.927005, Accuracy: 88.48%\n",
      "Batch 144, Loss: 0.852258, Accuracy: 88.48%\n",
      "Batch 145, Loss: 0.833026, Accuracy: 88.50%\n",
      "Batch 146, Loss: 0.833908, Accuracy: 88.52%\n",
      "Batch 147, Loss: 0.797661, Accuracy: 88.55%\n",
      "Batch 148, Loss: 0.819569, Accuracy: 88.59%\n",
      "Batch 149, Loss: 0.819237, Accuracy: 88.62%\n",
      "Batch 150, Loss: 0.833327, Accuracy: 88.64%\n",
      "Batch 151, Loss: 0.815248, Accuracy: 88.66%\n",
      "Batch 152, Loss: 0.851783, Accuracy: 88.65%\n",
      "Batch 153, Loss: 0.904993, Accuracy: 88.62%\n",
      "Batch 154, Loss: 0.863169, Accuracy: 88.62%\n",
      "Batch 155, Loss: 0.871995, Accuracy: 88.61%\n",
      "Batch 156, Loss: 0.780643, Accuracy: 88.67%\n",
      "Batch 157, Loss: 0.891365, Accuracy: 88.65%\n",
      "Batch 158, Loss: 0.818824, Accuracy: 88.69%\n",
      "Batch 159, Loss: 0.903232, Accuracy: 88.65%\n",
      "Batch 160, Loss: 0.855069, Accuracy: 88.65%\n",
      "Batch 161, Loss: 0.878626, Accuracy: 88.64%\n",
      "Batch 162, Loss: 0.847282, Accuracy: 88.64%\n",
      "Batch 163, Loss: 0.848941, Accuracy: 88.65%\n",
      "Batch 164, Loss: 0.863127, Accuracy: 88.65%\n",
      "Batch 165, Loss: 0.854213, Accuracy: 88.65%\n",
      "Batch 166, Loss: 0.901190, Accuracy: 88.61%\n",
      "Batch 167, Loss: 0.879303, Accuracy: 88.59%\n",
      "Batch 168, Loss: 0.872470, Accuracy: 88.58%\n",
      "Batch 169, Loss: 0.896351, Accuracy: 88.55%\n",
      "Batch 170, Loss: 0.882648, Accuracy: 88.54%\n",
      "Batch 171, Loss: 0.881273, Accuracy: 88.52%\n",
      "Batch 172, Loss: 0.848777, Accuracy: 88.53%\n",
      "Batch 173, Loss: 0.794959, Accuracy: 88.57%\n",
      "Batch 174, Loss: 0.896748, Accuracy: 88.54%\n",
      "Batch 175, Loss: 0.821238, Accuracy: 88.56%\n",
      "Batch 176, Loss: 0.881560, Accuracy: 88.55%\n",
      "Batch 177, Loss: 0.837006, Accuracy: 88.56%\n",
      "Batch 178, Loss: 0.915553, Accuracy: 88.53%\n",
      "Batch 179, Loss: 0.857302, Accuracy: 88.53%\n",
      "Batch 180, Loss: 0.905011, Accuracy: 88.50%\n",
      "Batch 181, Loss: 0.842327, Accuracy: 88.50%\n",
      "Batch 182, Loss: 0.833127, Accuracy: 88.51%\n",
      "Batch 183, Loss: 0.856012, Accuracy: 88.50%\n",
      "Batch 184, Loss: 0.857467, Accuracy: 88.50%\n",
      "Batch 185, Loss: 0.831738, Accuracy: 88.52%\n",
      "Batch 186, Loss: 0.929096, Accuracy: 88.48%\n",
      "Batch 187, Loss: 0.831860, Accuracy: 88.49%\n",
      "Batch 188, Loss: 0.889651, Accuracy: 88.48%\n",
      "Batch 189, Loss: 0.847160, Accuracy: 88.48%\n",
      "Batch 190, Loss: 0.866007, Accuracy: 88.49%\n",
      "Batch 191, Loss: 0.891002, Accuracy: 88.47%\n",
      "Batch 192, Loss: 0.877901, Accuracy: 88.46%\n",
      "Batch 193, Loss: 0.846737, Accuracy: 88.46%\n",
      "Batch 194, Loss: 0.882637, Accuracy: 88.46%\n",
      "Batch 195, Loss: 0.828648, Accuracy: 88.48%\n",
      "Batch 196, Loss: 0.847159, Accuracy: 88.48%\n",
      "Batch 197, Loss: 0.902414, Accuracy: 88.47%\n",
      "Batch 198, Loss: 0.879482, Accuracy: 88.45%\n",
      "Batch 199, Loss: 0.858472, Accuracy: 88.46%\n",
      "Batch 200, Loss: 0.882978, Accuracy: 88.45%\n",
      "Batch 201, Loss: 0.877530, Accuracy: 88.44%\n",
      "Batch 202, Loss: 0.887335, Accuracy: 88.43%\n",
      "Batch 203, Loss: 0.869907, Accuracy: 88.42%\n",
      "Batch 204, Loss: 0.906068, Accuracy: 88.40%\n",
      "Batch 205, Loss: 0.847879, Accuracy: 88.40%\n",
      "Batch 206, Loss: 0.817062, Accuracy: 88.42%\n",
      "Batch 207, Loss: 0.854966, Accuracy: 88.42%\n",
      "Batch 208, Loss: 0.953849, Accuracy: 88.37%\n",
      "Batch 209, Loss: 0.854924, Accuracy: 88.37%\n",
      "Batch 210, Loss: 0.782368, Accuracy: 88.41%\n",
      "Batch 211, Loss: 0.846286, Accuracy: 88.41%\n",
      "Batch 212, Loss: 0.853807, Accuracy: 88.41%\n",
      "Batch 213, Loss: 0.817490, Accuracy: 88.44%\n",
      "Training - Epoch 103, Loss: 0.860279, Accuracy: 88.44%\n",
      "Validation Batch 1, Loss: 0.827088, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.804076, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.852365, Accuracy: 92.71%\n",
      "Validation Batch 4, Loss: 0.854598, Accuracy: 91.80%\n",
      "Validation Batch 5, Loss: 0.837847, Accuracy: 91.56%\n",
      "Validation Batch 6, Loss: 0.800336, Accuracy: 92.19%\n",
      "Validation Batch 7, Loss: 0.810880, Accuracy: 92.41%\n",
      "Validation Batch 8, Loss: 0.886736, Accuracy: 91.41%\n",
      "Validation Batch 9, Loss: 0.899261, Accuracy: 90.45%\n",
      "Validation Batch 10, Loss: 0.815619, Accuracy: 90.78%\n",
      "Validation Batch 11, Loss: 0.835346, Accuracy: 90.91%\n",
      "Validation Batch 12, Loss: 0.835620, Accuracy: 91.02%\n",
      "Validation Batch 13, Loss: 0.846293, Accuracy: 90.87%\n",
      "Validation Batch 14, Loss: 0.851502, Accuracy: 90.62%\n",
      "Validation Batch 15, Loss: 0.821388, Accuracy: 90.73%\n",
      "Validation Batch 16, Loss: 0.843046, Accuracy: 90.72%\n",
      "Validation Batch 17, Loss: 0.874345, Accuracy: 90.53%\n",
      "Validation Batch 18, Loss: 0.818759, Accuracy: 90.62%\n",
      "Validation Batch 19, Loss: 0.885470, Accuracy: 90.46%\n",
      "Validation Batch 20, Loss: 0.868138, Accuracy: 90.31%\n",
      "Validation Batch 21, Loss: 0.869977, Accuracy: 90.18%\n",
      "Validation Batch 22, Loss: 0.840315, Accuracy: 90.20%\n",
      "Validation Batch 23, Loss: 0.875730, Accuracy: 90.08%\n",
      "Validation Batch 24, Loss: 0.854994, Accuracy: 90.04%\n",
      "Validation Batch 25, Loss: 0.825426, Accuracy: 90.06%\n",
      "Validation Batch 26, Loss: 0.856248, Accuracy: 89.96%\n",
      "Validation Batch 27, Loss: 0.805069, Accuracy: 90.08%\n",
      "Validation - Epoch 103, Loss: 0.844314, Accuracy: 90.08%\n",
      "Patience—2\n",
      "Epoch 104\n",
      "Batch 1, Loss: 0.868599, Accuracy: 85.94%\n",
      "Batch 2, Loss: 0.804288, Accuracy: 89.84%\n",
      "Batch 3, Loss: 0.869801, Accuracy: 88.54%\n",
      "Batch 4, Loss: 0.809187, Accuracy: 89.84%\n",
      "Batch 5, Loss: 0.824089, Accuracy: 90.31%\n",
      "Batch 6, Loss: 0.880124, Accuracy: 89.32%\n",
      "Batch 7, Loss: 0.848693, Accuracy: 89.06%\n",
      "Batch 8, Loss: 0.847602, Accuracy: 89.06%\n",
      "Batch 9, Loss: 0.861992, Accuracy: 89.06%\n",
      "Batch 10, Loss: 0.787996, Accuracy: 89.69%\n",
      "Batch 11, Loss: 0.870933, Accuracy: 89.49%\n",
      "Batch 12, Loss: 0.871533, Accuracy: 89.32%\n",
      "Batch 13, Loss: 0.843728, Accuracy: 89.30%\n",
      "Batch 14, Loss: 0.878991, Accuracy: 89.06%\n",
      "Batch 15, Loss: 0.929247, Accuracy: 88.54%\n",
      "Batch 16, Loss: 0.883664, Accuracy: 88.38%\n",
      "Batch 17, Loss: 0.884968, Accuracy: 88.24%\n",
      "Batch 18, Loss: 0.822579, Accuracy: 88.45%\n",
      "Batch 19, Loss: 0.849241, Accuracy: 88.49%\n",
      "Batch 20, Loss: 0.891964, Accuracy: 88.36%\n",
      "Batch 21, Loss: 0.887026, Accuracy: 88.24%\n",
      "Batch 22, Loss: 0.849778, Accuracy: 88.28%\n",
      "Batch 23, Loss: 0.867378, Accuracy: 88.32%\n",
      "Batch 24, Loss: 0.812283, Accuracy: 88.48%\n",
      "Batch 25, Loss: 0.840586, Accuracy: 88.56%\n",
      "Batch 26, Loss: 0.857642, Accuracy: 88.52%\n",
      "Batch 27, Loss: 0.832002, Accuracy: 88.60%\n",
      "Batch 28, Loss: 0.817287, Accuracy: 88.78%\n",
      "Batch 29, Loss: 0.935252, Accuracy: 88.42%\n",
      "Batch 30, Loss: 0.880422, Accuracy: 88.33%\n",
      "Batch 31, Loss: 0.890787, Accuracy: 88.26%\n",
      "Batch 32, Loss: 0.947955, Accuracy: 87.99%\n",
      "Batch 33, Loss: 0.870767, Accuracy: 87.93%\n",
      "Batch 34, Loss: 0.853661, Accuracy: 87.96%\n",
      "Batch 35, Loss: 0.779409, Accuracy: 88.21%\n",
      "Batch 36, Loss: 0.850709, Accuracy: 88.24%\n",
      "Batch 37, Loss: 0.883448, Accuracy: 88.13%\n",
      "Batch 38, Loss: 0.857624, Accuracy: 88.16%\n",
      "Batch 39, Loss: 0.863661, Accuracy: 88.18%\n",
      "Batch 40, Loss: 0.890258, Accuracy: 88.12%\n",
      "Batch 41, Loss: 0.836648, Accuracy: 88.19%\n",
      "Batch 42, Loss: 0.862842, Accuracy: 88.17%\n",
      "Batch 43, Loss: 0.886749, Accuracy: 88.08%\n",
      "Batch 44, Loss: 0.821606, Accuracy: 88.21%\n",
      "Batch 45, Loss: 0.847043, Accuracy: 88.26%\n",
      "Batch 46, Loss: 0.820284, Accuracy: 88.35%\n",
      "Batch 47, Loss: 0.857894, Accuracy: 88.33%\n",
      "Batch 48, Loss: 0.921239, Accuracy: 88.22%\n",
      "Batch 49, Loss: 0.912128, Accuracy: 88.11%\n",
      "Batch 50, Loss: 0.900933, Accuracy: 88.00%\n",
      "Batch 51, Loss: 0.871571, Accuracy: 87.99%\n",
      "Batch 52, Loss: 0.818556, Accuracy: 88.07%\n",
      "Batch 53, Loss: 0.859701, Accuracy: 88.06%\n",
      "Batch 54, Loss: 0.889376, Accuracy: 88.02%\n",
      "Batch 55, Loss: 0.897833, Accuracy: 87.95%\n",
      "Batch 56, Loss: 0.930193, Accuracy: 87.83%\n",
      "Batch 57, Loss: 0.869330, Accuracy: 87.83%\n",
      "Batch 58, Loss: 0.840688, Accuracy: 87.85%\n",
      "Batch 59, Loss: 0.882088, Accuracy: 87.84%\n",
      "Batch 60, Loss: 0.868597, Accuracy: 87.84%\n",
      "Batch 61, Loss: 0.834759, Accuracy: 87.91%\n",
      "Batch 62, Loss: 0.830552, Accuracy: 87.98%\n",
      "Batch 63, Loss: 0.808454, Accuracy: 88.05%\n",
      "Batch 64, Loss: 0.837477, Accuracy: 88.09%\n",
      "Batch 65, Loss: 0.812137, Accuracy: 88.17%\n",
      "Batch 66, Loss: 0.779145, Accuracy: 88.30%\n",
      "Batch 67, Loss: 0.859012, Accuracy: 88.29%\n",
      "Batch 68, Loss: 0.856996, Accuracy: 88.30%\n",
      "Batch 69, Loss: 0.851448, Accuracy: 88.32%\n",
      "Batch 70, Loss: 0.819020, Accuracy: 88.37%\n",
      "Batch 71, Loss: 0.859126, Accuracy: 88.36%\n",
      "Batch 72, Loss: 0.834981, Accuracy: 88.41%\n",
      "Batch 73, Loss: 0.898596, Accuracy: 88.38%\n",
      "Batch 74, Loss: 0.857653, Accuracy: 88.37%\n",
      "Batch 75, Loss: 0.831936, Accuracy: 88.40%\n",
      "Batch 76, Loss: 0.848953, Accuracy: 88.45%\n",
      "Batch 77, Loss: 0.881944, Accuracy: 88.39%\n",
      "Batch 78, Loss: 0.814868, Accuracy: 88.46%\n",
      "Batch 79, Loss: 0.876846, Accuracy: 88.41%\n",
      "Batch 80, Loss: 0.890586, Accuracy: 88.38%\n",
      "Batch 81, Loss: 0.898472, Accuracy: 88.33%\n",
      "Batch 82, Loss: 0.872586, Accuracy: 88.30%\n",
      "Batch 83, Loss: 0.876840, Accuracy: 88.29%\n",
      "Batch 84, Loss: 0.832989, Accuracy: 88.34%\n",
      "Batch 85, Loss: 0.888159, Accuracy: 88.31%\n",
      "Batch 86, Loss: 0.839973, Accuracy: 88.34%\n",
      "Batch 87, Loss: 0.901858, Accuracy: 88.29%\n",
      "Batch 88, Loss: 0.836821, Accuracy: 88.32%\n",
      "Batch 89, Loss: 0.830370, Accuracy: 88.36%\n",
      "Batch 90, Loss: 0.884380, Accuracy: 88.33%\n",
      "Batch 91, Loss: 0.900132, Accuracy: 88.29%\n",
      "Batch 92, Loss: 0.834118, Accuracy: 88.33%\n",
      "Batch 93, Loss: 0.910405, Accuracy: 88.27%\n",
      "Batch 94, Loss: 0.910590, Accuracy: 88.21%\n",
      "Batch 95, Loss: 0.966322, Accuracy: 88.11%\n",
      "Batch 96, Loss: 0.838392, Accuracy: 88.12%\n",
      "Batch 97, Loss: 0.886454, Accuracy: 88.06%\n",
      "Batch 98, Loss: 0.831924, Accuracy: 88.09%\n",
      "Batch 99, Loss: 0.957743, Accuracy: 87.99%\n",
      "Batch 100, Loss: 0.901829, Accuracy: 87.95%\n",
      "Batch 101, Loss: 0.830514, Accuracy: 87.98%\n",
      "Batch 102, Loss: 0.912691, Accuracy: 87.94%\n",
      "Batch 103, Loss: 0.836062, Accuracy: 87.99%\n",
      "Batch 104, Loss: 0.849923, Accuracy: 88.00%\n",
      "Batch 105, Loss: 0.829202, Accuracy: 88.04%\n",
      "Batch 106, Loss: 0.774365, Accuracy: 88.13%\n",
      "Batch 107, Loss: 0.888779, Accuracy: 88.11%\n",
      "Batch 108, Loss: 0.930914, Accuracy: 88.05%\n",
      "Batch 109, Loss: 0.823262, Accuracy: 88.09%\n",
      "Batch 110, Loss: 0.815130, Accuracy: 88.14%\n",
      "Batch 111, Loss: 0.868034, Accuracy: 88.12%\n",
      "Batch 112, Loss: 0.830677, Accuracy: 88.14%\n",
      "Batch 113, Loss: 0.788118, Accuracy: 88.21%\n",
      "Batch 114, Loss: 0.809485, Accuracy: 88.24%\n",
      "Batch 115, Loss: 0.890445, Accuracy: 88.22%\n",
      "Batch 116, Loss: 0.916010, Accuracy: 88.16%\n",
      "Batch 117, Loss: 0.814157, Accuracy: 88.21%\n",
      "Batch 118, Loss: 0.846814, Accuracy: 88.22%\n",
      "Batch 119, Loss: 0.887975, Accuracy: 88.18%\n",
      "Batch 120, Loss: 0.834649, Accuracy: 88.20%\n",
      "Batch 121, Loss: 0.908928, Accuracy: 88.17%\n",
      "Batch 122, Loss: 0.865228, Accuracy: 88.17%\n",
      "Batch 123, Loss: 0.863131, Accuracy: 88.16%\n",
      "Batch 124, Loss: 0.870585, Accuracy: 88.17%\n",
      "Batch 125, Loss: 0.922391, Accuracy: 88.11%\n",
      "Batch 126, Loss: 0.879539, Accuracy: 88.10%\n",
      "Batch 127, Loss: 0.864956, Accuracy: 88.09%\n",
      "Batch 128, Loss: 0.809672, Accuracy: 88.13%\n",
      "Batch 129, Loss: 0.834111, Accuracy: 88.17%\n",
      "Batch 130, Loss: 0.940858, Accuracy: 88.11%\n",
      "Batch 131, Loss: 0.865456, Accuracy: 88.12%\n",
      "Batch 132, Loss: 0.926566, Accuracy: 88.07%\n",
      "Batch 133, Loss: 0.833706, Accuracy: 88.09%\n",
      "Batch 134, Loss: 0.864916, Accuracy: 88.08%\n",
      "Batch 135, Loss: 0.862212, Accuracy: 88.08%\n",
      "Batch 136, Loss: 0.872720, Accuracy: 88.07%\n",
      "Batch 137, Loss: 0.850591, Accuracy: 88.08%\n",
      "Batch 138, Loss: 0.809269, Accuracy: 88.12%\n",
      "Batch 139, Loss: 0.814838, Accuracy: 88.15%\n",
      "Batch 140, Loss: 0.869761, Accuracy: 88.15%\n",
      "Batch 141, Loss: 0.819898, Accuracy: 88.18%\n",
      "Batch 142, Loss: 0.838815, Accuracy: 88.19%\n",
      "Batch 143, Loss: 0.904153, Accuracy: 88.17%\n",
      "Batch 144, Loss: 0.810945, Accuracy: 88.22%\n",
      "Batch 145, Loss: 0.835081, Accuracy: 88.24%\n",
      "Batch 146, Loss: 0.859240, Accuracy: 88.25%\n",
      "Batch 147, Loss: 0.851379, Accuracy: 88.25%\n",
      "Batch 148, Loss: 0.929097, Accuracy: 88.21%\n",
      "Batch 149, Loss: 0.837757, Accuracy: 88.23%\n",
      "Batch 150, Loss: 0.861657, Accuracy: 88.25%\n",
      "Batch 151, Loss: 0.885376, Accuracy: 88.25%\n",
      "Batch 152, Loss: 0.850727, Accuracy: 88.26%\n",
      "Batch 153, Loss: 0.853555, Accuracy: 88.27%\n",
      "Batch 154, Loss: 0.909230, Accuracy: 88.23%\n",
      "Batch 155, Loss: 0.856235, Accuracy: 88.24%\n",
      "Batch 156, Loss: 0.856864, Accuracy: 88.24%\n",
      "Batch 157, Loss: 0.865589, Accuracy: 88.25%\n",
      "Batch 158, Loss: 0.850443, Accuracy: 88.25%\n",
      "Batch 159, Loss: 0.823765, Accuracy: 88.28%\n",
      "Batch 160, Loss: 0.856367, Accuracy: 88.27%\n",
      "Batch 161, Loss: 0.909603, Accuracy: 88.25%\n",
      "Batch 162, Loss: 0.887235, Accuracy: 88.23%\n",
      "Batch 163, Loss: 0.809021, Accuracy: 88.27%\n",
      "Batch 164, Loss: 0.893576, Accuracy: 88.24%\n",
      "Batch 165, Loss: 0.790019, Accuracy: 88.29%\n",
      "Batch 166, Loss: 0.828595, Accuracy: 88.31%\n",
      "Batch 167, Loss: 0.893035, Accuracy: 88.28%\n",
      "Batch 168, Loss: 0.859482, Accuracy: 88.28%\n",
      "Batch 169, Loss: 0.825570, Accuracy: 88.31%\n",
      "Batch 170, Loss: 0.804333, Accuracy: 88.35%\n",
      "Batch 171, Loss: 0.844423, Accuracy: 88.37%\n",
      "Batch 172, Loss: 0.862239, Accuracy: 88.38%\n",
      "Batch 173, Loss: 0.846093, Accuracy: 88.39%\n",
      "Batch 174, Loss: 0.859303, Accuracy: 88.40%\n",
      "Batch 175, Loss: 0.903756, Accuracy: 88.37%\n",
      "Batch 176, Loss: 0.853361, Accuracy: 88.37%\n",
      "Batch 177, Loss: 0.874001, Accuracy: 88.37%\n",
      "Batch 178, Loss: 0.917747, Accuracy: 88.33%\n",
      "Batch 179, Loss: 0.898266, Accuracy: 88.31%\n",
      "Batch 180, Loss: 0.846171, Accuracy: 88.32%\n",
      "Batch 181, Loss: 0.934074, Accuracy: 88.28%\n",
      "Batch 182, Loss: 0.890683, Accuracy: 88.26%\n",
      "Batch 183, Loss: 0.873155, Accuracy: 88.27%\n",
      "Batch 184, Loss: 0.872888, Accuracy: 88.26%\n",
      "Batch 185, Loss: 0.843991, Accuracy: 88.27%\n",
      "Batch 186, Loss: 0.863439, Accuracy: 88.27%\n",
      "Batch 187, Loss: 0.837826, Accuracy: 88.29%\n",
      "Batch 188, Loss: 0.889532, Accuracy: 88.27%\n",
      "Batch 189, Loss: 0.850329, Accuracy: 88.28%\n",
      "Batch 190, Loss: 0.876918, Accuracy: 88.26%\n",
      "Batch 191, Loss: 0.910527, Accuracy: 88.24%\n",
      "Batch 192, Loss: 0.900611, Accuracy: 88.21%\n",
      "Batch 193, Loss: 0.879970, Accuracy: 88.20%\n",
      "Batch 194, Loss: 0.841496, Accuracy: 88.21%\n",
      "Batch 195, Loss: 0.802477, Accuracy: 88.24%\n",
      "Batch 196, Loss: 0.823557, Accuracy: 88.26%\n",
      "Batch 197, Loss: 0.837360, Accuracy: 88.27%\n",
      "Batch 198, Loss: 0.815626, Accuracy: 88.30%\n",
      "Batch 199, Loss: 0.826965, Accuracy: 88.31%\n",
      "Batch 200, Loss: 0.841464, Accuracy: 88.32%\n",
      "Batch 201, Loss: 0.835183, Accuracy: 88.33%\n",
      "Batch 202, Loss: 0.845332, Accuracy: 88.34%\n",
      "Batch 203, Loss: 0.889735, Accuracy: 88.32%\n",
      "Batch 204, Loss: 0.912403, Accuracy: 88.29%\n",
      "Batch 205, Loss: 0.832955, Accuracy: 88.30%\n",
      "Batch 206, Loss: 0.845635, Accuracy: 88.31%\n",
      "Batch 207, Loss: 0.881018, Accuracy: 88.30%\n",
      "Batch 208, Loss: 0.788428, Accuracy: 88.35%\n",
      "Batch 209, Loss: 0.873483, Accuracy: 88.34%\n",
      "Batch 210, Loss: 0.884799, Accuracy: 88.34%\n",
      "Batch 211, Loss: 0.840091, Accuracy: 88.36%\n",
      "Batch 212, Loss: 0.863860, Accuracy: 88.35%\n",
      "Batch 213, Loss: 0.890493, Accuracy: 88.34%\n",
      "Training - Epoch 104, Loss: 0.860762, Accuracy: 88.34%\n",
      "Validation Batch 1, Loss: 0.817366, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.788469, Accuracy: 95.31%\n",
      "Validation Batch 3, Loss: 0.832781, Accuracy: 94.27%\n",
      "Validation Batch 4, Loss: 0.840506, Accuracy: 92.97%\n",
      "Validation Batch 5, Loss: 0.827972, Accuracy: 92.81%\n",
      "Validation Batch 6, Loss: 0.786319, Accuracy: 93.49%\n",
      "Validation Batch 7, Loss: 0.798259, Accuracy: 93.75%\n",
      "Validation Batch 8, Loss: 0.871530, Accuracy: 92.97%\n",
      "Validation Batch 9, Loss: 0.884658, Accuracy: 92.36%\n",
      "Validation Batch 10, Loss: 0.802734, Accuracy: 92.66%\n",
      "Validation Batch 11, Loss: 0.824652, Accuracy: 92.76%\n",
      "Validation Batch 12, Loss: 0.828064, Accuracy: 92.71%\n",
      "Validation Batch 13, Loss: 0.828265, Accuracy: 92.79%\n",
      "Validation Batch 14, Loss: 0.837763, Accuracy: 92.63%\n",
      "Validation Batch 15, Loss: 0.815541, Accuracy: 92.71%\n",
      "Validation Batch 16, Loss: 0.824781, Accuracy: 92.68%\n",
      "Validation Batch 17, Loss: 0.866296, Accuracy: 92.37%\n",
      "Validation Batch 18, Loss: 0.804751, Accuracy: 92.45%\n",
      "Validation Batch 19, Loss: 0.869489, Accuracy: 92.19%\n",
      "Validation Batch 20, Loss: 0.840772, Accuracy: 92.19%\n",
      "Validation Batch 21, Loss: 0.864069, Accuracy: 92.04%\n",
      "Validation Batch 22, Loss: 0.829914, Accuracy: 91.97%\n",
      "Validation Batch 23, Loss: 0.863599, Accuracy: 91.85%\n",
      "Validation Batch 24, Loss: 0.833792, Accuracy: 91.93%\n",
      "Validation Batch 25, Loss: 0.811549, Accuracy: 91.94%\n",
      "Validation Batch 26, Loss: 0.844137, Accuracy: 91.89%\n",
      "Validation Batch 27, Loss: 0.787112, Accuracy: 92.01%\n",
      "Validation - Epoch 104, Loss: 0.830561, Accuracy: 92.01%\n",
      "Patience—0\n",
      "Epoch 105\n",
      "Batch 1, Loss: 0.839992, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.850080, Accuracy: 89.84%\n",
      "Batch 3, Loss: 0.891930, Accuracy: 88.02%\n",
      "Batch 4, Loss: 0.844859, Accuracy: 88.67%\n",
      "Batch 5, Loss: 0.850299, Accuracy: 88.75%\n",
      "Batch 6, Loss: 0.855662, Accuracy: 89.06%\n",
      "Batch 7, Loss: 0.856450, Accuracy: 89.06%\n",
      "Batch 8, Loss: 0.814554, Accuracy: 89.65%\n",
      "Batch 9, Loss: 0.877686, Accuracy: 89.41%\n",
      "Batch 10, Loss: 0.858222, Accuracy: 89.22%\n",
      "Batch 11, Loss: 0.853435, Accuracy: 89.20%\n",
      "Batch 12, Loss: 0.853023, Accuracy: 89.19%\n",
      "Batch 13, Loss: 0.902255, Accuracy: 88.70%\n",
      "Batch 14, Loss: 0.784328, Accuracy: 89.17%\n",
      "Batch 15, Loss: 0.806829, Accuracy: 89.48%\n",
      "Batch 16, Loss: 0.832451, Accuracy: 89.55%\n",
      "Batch 17, Loss: 0.848014, Accuracy: 89.52%\n",
      "Batch 18, Loss: 0.810393, Accuracy: 89.76%\n",
      "Batch 19, Loss: 0.807704, Accuracy: 89.97%\n",
      "Batch 20, Loss: 0.926315, Accuracy: 89.61%\n",
      "Batch 21, Loss: 0.919389, Accuracy: 89.21%\n",
      "Batch 22, Loss: 0.867556, Accuracy: 89.06%\n",
      "Batch 23, Loss: 0.831819, Accuracy: 89.13%\n",
      "Batch 24, Loss: 0.888072, Accuracy: 89.00%\n",
      "Batch 25, Loss: 0.931995, Accuracy: 88.69%\n",
      "Batch 26, Loss: 0.851335, Accuracy: 88.70%\n",
      "Batch 27, Loss: 0.897882, Accuracy: 88.54%\n",
      "Batch 28, Loss: 0.853619, Accuracy: 88.62%\n",
      "Batch 29, Loss: 0.860648, Accuracy: 88.63%\n",
      "Batch 30, Loss: 0.846451, Accuracy: 88.70%\n",
      "Batch 31, Loss: 0.866177, Accuracy: 88.71%\n",
      "Batch 32, Loss: 0.803285, Accuracy: 88.87%\n",
      "Batch 33, Loss: 0.809794, Accuracy: 89.02%\n",
      "Batch 34, Loss: 0.860943, Accuracy: 88.97%\n",
      "Batch 35, Loss: 0.906119, Accuracy: 88.79%\n",
      "Batch 36, Loss: 0.848242, Accuracy: 88.85%\n",
      "Batch 37, Loss: 0.931441, Accuracy: 88.68%\n",
      "Batch 38, Loss: 0.881954, Accuracy: 88.65%\n",
      "Batch 39, Loss: 0.881055, Accuracy: 88.54%\n",
      "Batch 40, Loss: 0.854388, Accuracy: 88.52%\n",
      "Batch 41, Loss: 0.944165, Accuracy: 88.30%\n",
      "Batch 42, Loss: 0.870252, Accuracy: 88.24%\n",
      "Batch 43, Loss: 0.846879, Accuracy: 88.30%\n",
      "Batch 44, Loss: 0.905323, Accuracy: 88.17%\n",
      "Batch 45, Loss: 0.818386, Accuracy: 88.26%\n",
      "Batch 46, Loss: 0.874583, Accuracy: 88.21%\n",
      "Batch 47, Loss: 0.842016, Accuracy: 88.26%\n",
      "Batch 48, Loss: 0.820438, Accuracy: 88.35%\n",
      "Batch 49, Loss: 0.881565, Accuracy: 88.30%\n",
      "Batch 50, Loss: 0.861437, Accuracy: 88.28%\n",
      "Batch 51, Loss: 0.870919, Accuracy: 88.27%\n",
      "Batch 52, Loss: 0.911032, Accuracy: 88.16%\n",
      "Batch 53, Loss: 0.858001, Accuracy: 88.15%\n",
      "Batch 54, Loss: 0.818530, Accuracy: 88.25%\n",
      "Batch 55, Loss: 0.871768, Accuracy: 88.24%\n",
      "Batch 56, Loss: 0.851960, Accuracy: 88.25%\n",
      "Batch 57, Loss: 0.809793, Accuracy: 88.35%\n",
      "Batch 58, Loss: 0.817863, Accuracy: 88.44%\n",
      "Batch 59, Loss: 0.820604, Accuracy: 88.51%\n",
      "Batch 60, Loss: 0.791723, Accuracy: 88.65%\n",
      "Batch 61, Loss: 0.811516, Accuracy: 88.73%\n",
      "Batch 62, Loss: 0.838217, Accuracy: 88.73%\n",
      "Batch 63, Loss: 0.830179, Accuracy: 88.76%\n",
      "Batch 64, Loss: 0.848237, Accuracy: 88.77%\n",
      "Batch 65, Loss: 0.873663, Accuracy: 88.75%\n",
      "Batch 66, Loss: 0.855251, Accuracy: 88.75%\n",
      "Batch 67, Loss: 0.950282, Accuracy: 88.62%\n",
      "Batch 68, Loss: 0.856221, Accuracy: 88.60%\n",
      "Batch 69, Loss: 0.833949, Accuracy: 88.65%\n",
      "Batch 70, Loss: 0.823962, Accuracy: 88.71%\n",
      "Batch 71, Loss: 0.891376, Accuracy: 88.62%\n",
      "Batch 72, Loss: 0.840689, Accuracy: 88.63%\n",
      "Batch 73, Loss: 0.884810, Accuracy: 88.61%\n",
      "Batch 74, Loss: 0.848392, Accuracy: 88.62%\n",
      "Batch 75, Loss: 0.827278, Accuracy: 88.67%\n",
      "Batch 76, Loss: 0.844478, Accuracy: 88.69%\n",
      "Batch 77, Loss: 0.922678, Accuracy: 88.60%\n",
      "Batch 78, Loss: 0.902777, Accuracy: 88.54%\n",
      "Batch 79, Loss: 0.878649, Accuracy: 88.53%\n",
      "Batch 80, Loss: 0.867365, Accuracy: 88.52%\n",
      "Batch 81, Loss: 0.781123, Accuracy: 88.62%\n",
      "Batch 82, Loss: 0.833984, Accuracy: 88.64%\n",
      "Batch 83, Loss: 0.902702, Accuracy: 88.63%\n",
      "Batch 84, Loss: 0.785580, Accuracy: 88.73%\n",
      "Batch 85, Loss: 0.808731, Accuracy: 88.79%\n",
      "Batch 86, Loss: 0.837219, Accuracy: 88.81%\n",
      "Batch 87, Loss: 0.857728, Accuracy: 88.81%\n",
      "Batch 88, Loss: 0.850732, Accuracy: 88.81%\n",
      "Batch 89, Loss: 0.921558, Accuracy: 88.73%\n",
      "Batch 90, Loss: 0.862758, Accuracy: 88.72%\n",
      "Batch 91, Loss: 0.843701, Accuracy: 88.74%\n",
      "Batch 92, Loss: 0.825752, Accuracy: 88.76%\n",
      "Batch 93, Loss: 0.929675, Accuracy: 88.66%\n",
      "Batch 94, Loss: 0.876915, Accuracy: 88.65%\n",
      "Batch 95, Loss: 0.848097, Accuracy: 88.67%\n",
      "Batch 96, Loss: 0.930918, Accuracy: 88.59%\n",
      "Batch 97, Loss: 0.846586, Accuracy: 88.60%\n",
      "Batch 98, Loss: 0.853673, Accuracy: 88.60%\n",
      "Batch 99, Loss: 0.823796, Accuracy: 88.64%\n",
      "Batch 100, Loss: 0.889044, Accuracy: 88.61%\n",
      "Batch 101, Loss: 0.908206, Accuracy: 88.58%\n",
      "Batch 102, Loss: 0.816768, Accuracy: 88.62%\n",
      "Batch 103, Loss: 0.877197, Accuracy: 88.61%\n",
      "Batch 104, Loss: 0.849357, Accuracy: 88.61%\n",
      "Batch 105, Loss: 0.825346, Accuracy: 88.66%\n",
      "Batch 106, Loss: 0.847554, Accuracy: 88.66%\n",
      "Batch 107, Loss: 0.873097, Accuracy: 88.67%\n",
      "Batch 108, Loss: 0.898567, Accuracy: 88.61%\n",
      "Batch 109, Loss: 0.828558, Accuracy: 88.65%\n",
      "Batch 110, Loss: 0.873212, Accuracy: 88.64%\n",
      "Batch 111, Loss: 0.863106, Accuracy: 88.64%\n",
      "Batch 112, Loss: 0.866969, Accuracy: 88.63%\n",
      "Batch 113, Loss: 0.904225, Accuracy: 88.58%\n",
      "Batch 114, Loss: 0.843245, Accuracy: 88.60%\n",
      "Batch 115, Loss: 0.854943, Accuracy: 88.60%\n",
      "Batch 116, Loss: 0.867792, Accuracy: 88.60%\n",
      "Batch 117, Loss: 0.845554, Accuracy: 88.61%\n",
      "Batch 118, Loss: 0.931826, Accuracy: 88.55%\n",
      "Batch 119, Loss: 0.890229, Accuracy: 88.52%\n",
      "Batch 120, Loss: 0.913641, Accuracy: 88.48%\n",
      "Batch 121, Loss: 0.858396, Accuracy: 88.48%\n",
      "Batch 122, Loss: 0.886699, Accuracy: 88.47%\n",
      "Batch 123, Loss: 0.890063, Accuracy: 88.47%\n",
      "Batch 124, Loss: 0.819965, Accuracy: 88.51%\n",
      "Batch 125, Loss: 0.886520, Accuracy: 88.46%\n",
      "Batch 126, Loss: 0.864097, Accuracy: 88.45%\n",
      "Batch 127, Loss: 0.824554, Accuracy: 88.48%\n",
      "Batch 128, Loss: 0.864958, Accuracy: 88.48%\n",
      "Batch 129, Loss: 0.808194, Accuracy: 88.53%\n",
      "Batch 130, Loss: 0.857702, Accuracy: 88.52%\n",
      "Batch 131, Loss: 0.857580, Accuracy: 88.53%\n",
      "Batch 132, Loss: 0.889623, Accuracy: 88.49%\n",
      "Batch 133, Loss: 0.827092, Accuracy: 88.52%\n",
      "Batch 134, Loss: 0.955343, Accuracy: 88.44%\n",
      "Batch 135, Loss: 0.952637, Accuracy: 88.37%\n",
      "Batch 136, Loss: 0.878192, Accuracy: 88.36%\n",
      "Batch 137, Loss: 0.833356, Accuracy: 88.39%\n",
      "Batch 138, Loss: 0.930822, Accuracy: 88.34%\n",
      "Batch 139, Loss: 0.868580, Accuracy: 88.34%\n",
      "Batch 140, Loss: 0.809153, Accuracy: 88.38%\n",
      "Batch 141, Loss: 0.879195, Accuracy: 88.38%\n",
      "Batch 142, Loss: 0.831802, Accuracy: 88.40%\n",
      "Batch 143, Loss: 0.847727, Accuracy: 88.41%\n",
      "Batch 144, Loss: 0.810319, Accuracy: 88.44%\n",
      "Batch 145, Loss: 0.861079, Accuracy: 88.46%\n",
      "Batch 146, Loss: 0.965469, Accuracy: 88.38%\n",
      "Batch 147, Loss: 0.894450, Accuracy: 88.35%\n",
      "Batch 148, Loss: 0.864739, Accuracy: 88.34%\n",
      "Batch 149, Loss: 0.862132, Accuracy: 88.34%\n",
      "Batch 150, Loss: 0.944747, Accuracy: 88.27%\n",
      "Batch 151, Loss: 0.876104, Accuracy: 88.27%\n",
      "Batch 152, Loss: 0.850524, Accuracy: 88.27%\n",
      "Batch 153, Loss: 0.836128, Accuracy: 88.29%\n",
      "Batch 154, Loss: 0.874209, Accuracy: 88.28%\n",
      "Batch 155, Loss: 0.868161, Accuracy: 88.28%\n",
      "Batch 156, Loss: 0.839395, Accuracy: 88.29%\n",
      "Batch 157, Loss: 0.883390, Accuracy: 88.28%\n",
      "Batch 158, Loss: 0.883768, Accuracy: 88.26%\n",
      "Batch 159, Loss: 0.795636, Accuracy: 88.31%\n",
      "Batch 160, Loss: 0.898470, Accuracy: 88.27%\n",
      "Batch 161, Loss: 0.826780, Accuracy: 88.30%\n",
      "Batch 162, Loss: 0.855737, Accuracy: 88.30%\n",
      "Batch 163, Loss: 0.866756, Accuracy: 88.31%\n",
      "Batch 164, Loss: 0.789607, Accuracy: 88.35%\n",
      "Batch 165, Loss: 0.802940, Accuracy: 88.39%\n",
      "Batch 166, Loss: 0.846527, Accuracy: 88.41%\n",
      "Batch 167, Loss: 0.864567, Accuracy: 88.41%\n",
      "Batch 168, Loss: 0.828181, Accuracy: 88.43%\n",
      "Batch 169, Loss: 0.860606, Accuracy: 88.43%\n",
      "Batch 170, Loss: 0.872771, Accuracy: 88.43%\n",
      "Batch 171, Loss: 0.903236, Accuracy: 88.40%\n",
      "Batch 172, Loss: 0.864628, Accuracy: 88.40%\n",
      "Batch 173, Loss: 0.886564, Accuracy: 88.38%\n",
      "Batch 174, Loss: 0.821239, Accuracy: 88.41%\n",
      "Batch 175, Loss: 0.874199, Accuracy: 88.39%\n",
      "Batch 176, Loss: 0.813410, Accuracy: 88.42%\n",
      "Batch 177, Loss: 0.873695, Accuracy: 88.41%\n",
      "Batch 178, Loss: 0.796530, Accuracy: 88.45%\n",
      "Batch 179, Loss: 0.882683, Accuracy: 88.44%\n",
      "Batch 180, Loss: 0.869706, Accuracy: 88.43%\n",
      "Batch 181, Loss: 0.830156, Accuracy: 88.45%\n",
      "Batch 182, Loss: 0.891862, Accuracy: 88.43%\n",
      "Batch 183, Loss: 0.843106, Accuracy: 88.43%\n",
      "Batch 184, Loss: 0.849775, Accuracy: 88.44%\n",
      "Batch 185, Loss: 0.866377, Accuracy: 88.45%\n",
      "Batch 186, Loss: 0.902018, Accuracy: 88.41%\n",
      "Batch 187, Loss: 0.856896, Accuracy: 88.41%\n",
      "Batch 188, Loss: 0.833075, Accuracy: 88.42%\n",
      "Batch 189, Loss: 0.871418, Accuracy: 88.41%\n",
      "Batch 190, Loss: 0.850493, Accuracy: 88.42%\n",
      "Batch 191, Loss: 0.863624, Accuracy: 88.42%\n",
      "Batch 192, Loss: 0.858406, Accuracy: 88.41%\n",
      "Batch 193, Loss: 0.831984, Accuracy: 88.42%\n",
      "Batch 194, Loss: 0.862261, Accuracy: 88.43%\n",
      "Batch 195, Loss: 0.864120, Accuracy: 88.42%\n",
      "Batch 196, Loss: 0.874997, Accuracy: 88.41%\n",
      "Batch 197, Loss: 0.905038, Accuracy: 88.39%\n",
      "Batch 198, Loss: 0.898120, Accuracy: 88.38%\n",
      "Batch 199, Loss: 0.830991, Accuracy: 88.39%\n",
      "Batch 200, Loss: 0.840210, Accuracy: 88.40%\n",
      "Batch 201, Loss: 0.869598, Accuracy: 88.39%\n",
      "Batch 202, Loss: 0.876870, Accuracy: 88.39%\n",
      "Batch 203, Loss: 0.860430, Accuracy: 88.39%\n",
      "Batch 204, Loss: 0.806461, Accuracy: 88.43%\n",
      "Batch 205, Loss: 0.857015, Accuracy: 88.43%\n",
      "Batch 206, Loss: 0.858676, Accuracy: 88.43%\n",
      "Batch 207, Loss: 0.831951, Accuracy: 88.44%\n",
      "Batch 208, Loss: 0.838577, Accuracy: 88.45%\n",
      "Batch 209, Loss: 0.832200, Accuracy: 88.47%\n",
      "Batch 210, Loss: 0.860158, Accuracy: 88.47%\n",
      "Batch 211, Loss: 0.820594, Accuracy: 88.49%\n",
      "Batch 212, Loss: 0.948269, Accuracy: 88.44%\n",
      "Batch 213, Loss: 0.861780, Accuracy: 88.44%\n",
      "Training - Epoch 105, Loss: 0.860293, Accuracy: 88.44%\n",
      "Validation Batch 1, Loss: 0.814081, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.789979, Accuracy: 95.31%\n",
      "Validation Batch 3, Loss: 0.840907, Accuracy: 93.23%\n",
      "Validation Batch 4, Loss: 0.834031, Accuracy: 92.97%\n",
      "Validation Batch 5, Loss: 0.829140, Accuracy: 92.50%\n",
      "Validation Batch 6, Loss: 0.789016, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.797362, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.872778, Accuracy: 92.77%\n",
      "Validation Batch 9, Loss: 0.883755, Accuracy: 92.01%\n",
      "Validation Batch 10, Loss: 0.815216, Accuracy: 92.03%\n",
      "Validation Batch 11, Loss: 0.827071, Accuracy: 92.05%\n",
      "Validation Batch 12, Loss: 0.826147, Accuracy: 92.06%\n",
      "Validation Batch 13, Loss: 0.831943, Accuracy: 92.07%\n",
      "Validation Batch 14, Loss: 0.834276, Accuracy: 92.08%\n",
      "Validation Batch 15, Loss: 0.824962, Accuracy: 92.08%\n",
      "Validation Batch 16, Loss: 0.831096, Accuracy: 92.09%\n",
      "Validation Batch 17, Loss: 0.870428, Accuracy: 91.82%\n",
      "Validation Batch 18, Loss: 0.803230, Accuracy: 91.93%\n",
      "Validation Batch 19, Loss: 0.878599, Accuracy: 91.61%\n",
      "Validation Batch 20, Loss: 0.835749, Accuracy: 91.72%\n",
      "Validation Batch 21, Loss: 0.863467, Accuracy: 91.59%\n",
      "Validation Batch 22, Loss: 0.840326, Accuracy: 91.55%\n",
      "Validation Batch 23, Loss: 0.870086, Accuracy: 91.37%\n",
      "Validation Batch 24, Loss: 0.838740, Accuracy: 91.41%\n",
      "Validation Batch 25, Loss: 0.813331, Accuracy: 91.44%\n",
      "Validation Batch 26, Loss: 0.846456, Accuracy: 91.41%\n",
      "Validation Batch 27, Loss: 0.799957, Accuracy: 91.49%\n",
      "Validation - Epoch 105, Loss: 0.833412, Accuracy: 91.49%\n",
      "Patience—1\n",
      "Epoch 106\n",
      "Batch 1, Loss: 0.826845, Accuracy: 92.19%\n",
      "Batch 2, Loss: 0.882700, Accuracy: 89.06%\n",
      "Batch 3, Loss: 0.882545, Accuracy: 88.54%\n",
      "Batch 4, Loss: 0.847430, Accuracy: 89.06%\n",
      "Batch 5, Loss: 0.810511, Accuracy: 90.00%\n",
      "Batch 6, Loss: 0.827380, Accuracy: 90.62%\n",
      "Batch 7, Loss: 0.885966, Accuracy: 89.73%\n",
      "Batch 8, Loss: 0.855410, Accuracy: 89.45%\n",
      "Batch 9, Loss: 0.839719, Accuracy: 89.58%\n",
      "Batch 10, Loss: 0.823360, Accuracy: 89.84%\n",
      "Batch 11, Loss: 0.875318, Accuracy: 89.49%\n",
      "Batch 12, Loss: 0.873084, Accuracy: 89.32%\n",
      "Batch 13, Loss: 0.893514, Accuracy: 89.06%\n",
      "Batch 14, Loss: 0.825345, Accuracy: 89.40%\n",
      "Batch 15, Loss: 0.806270, Accuracy: 89.58%\n",
      "Batch 16, Loss: 0.933524, Accuracy: 89.06%\n",
      "Batch 17, Loss: 0.860573, Accuracy: 89.06%\n",
      "Batch 18, Loss: 0.892728, Accuracy: 88.72%\n",
      "Batch 19, Loss: 0.847023, Accuracy: 88.82%\n",
      "Batch 20, Loss: 0.854787, Accuracy: 88.83%\n",
      "Batch 21, Loss: 0.855059, Accuracy: 88.84%\n",
      "Batch 22, Loss: 0.847075, Accuracy: 88.92%\n",
      "Batch 23, Loss: 0.901546, Accuracy: 88.72%\n",
      "Batch 24, Loss: 0.912507, Accuracy: 88.48%\n",
      "Batch 25, Loss: 0.884688, Accuracy: 88.38%\n",
      "Batch 26, Loss: 0.853611, Accuracy: 88.34%\n",
      "Batch 27, Loss: 0.828279, Accuracy: 88.48%\n",
      "Batch 28, Loss: 0.841621, Accuracy: 88.56%\n",
      "Batch 29, Loss: 0.909088, Accuracy: 88.42%\n",
      "Batch 30, Loss: 0.839765, Accuracy: 88.44%\n",
      "Batch 31, Loss: 0.859526, Accuracy: 88.41%\n",
      "Batch 32, Loss: 0.807355, Accuracy: 88.62%\n",
      "Batch 33, Loss: 0.856685, Accuracy: 88.64%\n",
      "Batch 34, Loss: 0.807151, Accuracy: 88.79%\n",
      "Batch 35, Loss: 0.872398, Accuracy: 88.71%\n",
      "Batch 36, Loss: 0.784660, Accuracy: 88.93%\n",
      "Batch 37, Loss: 0.890346, Accuracy: 88.81%\n",
      "Batch 38, Loss: 0.847862, Accuracy: 88.82%\n",
      "Batch 39, Loss: 0.821734, Accuracy: 88.90%\n",
      "Batch 40, Loss: 0.876454, Accuracy: 88.87%\n",
      "Batch 41, Loss: 0.884655, Accuracy: 88.80%\n",
      "Batch 42, Loss: 0.849625, Accuracy: 88.84%\n",
      "Batch 43, Loss: 0.860862, Accuracy: 88.84%\n",
      "Batch 44, Loss: 0.858234, Accuracy: 88.81%\n",
      "Batch 45, Loss: 0.856755, Accuracy: 88.78%\n",
      "Batch 46, Loss: 0.937876, Accuracy: 88.62%\n",
      "Batch 47, Loss: 0.871053, Accuracy: 88.56%\n",
      "Batch 48, Loss: 0.832717, Accuracy: 88.61%\n",
      "Batch 49, Loss: 0.899452, Accuracy: 88.52%\n",
      "Batch 50, Loss: 0.893900, Accuracy: 88.47%\n",
      "Batch 51, Loss: 0.850708, Accuracy: 88.51%\n",
      "Batch 52, Loss: 0.945798, Accuracy: 88.34%\n",
      "Batch 53, Loss: 0.860377, Accuracy: 88.35%\n",
      "Batch 54, Loss: 0.847969, Accuracy: 88.40%\n",
      "Batch 55, Loss: 0.843744, Accuracy: 88.41%\n",
      "Batch 56, Loss: 0.825907, Accuracy: 88.48%\n",
      "Batch 57, Loss: 0.886965, Accuracy: 88.43%\n",
      "Batch 58, Loss: 0.851762, Accuracy: 88.44%\n",
      "Batch 59, Loss: 0.835293, Accuracy: 88.48%\n",
      "Batch 60, Loss: 0.874130, Accuracy: 88.44%\n",
      "Batch 61, Loss: 0.875779, Accuracy: 88.40%\n",
      "Batch 62, Loss: 0.889545, Accuracy: 88.33%\n",
      "Batch 63, Loss: 0.848065, Accuracy: 88.37%\n",
      "Batch 64, Loss: 0.813932, Accuracy: 88.45%\n",
      "Batch 65, Loss: 0.934072, Accuracy: 88.34%\n",
      "Batch 66, Loss: 0.826551, Accuracy: 88.40%\n",
      "Batch 67, Loss: 0.886448, Accuracy: 88.34%\n",
      "Batch 68, Loss: 0.890928, Accuracy: 88.30%\n",
      "Batch 69, Loss: 0.891892, Accuracy: 88.25%\n",
      "Batch 70, Loss: 0.845277, Accuracy: 88.28%\n",
      "Batch 71, Loss: 0.894492, Accuracy: 88.23%\n",
      "Batch 72, Loss: 0.862059, Accuracy: 88.22%\n",
      "Batch 73, Loss: 0.879996, Accuracy: 88.18%\n",
      "Batch 74, Loss: 0.804508, Accuracy: 88.28%\n",
      "Batch 75, Loss: 0.827180, Accuracy: 88.31%\n",
      "Batch 76, Loss: 0.908379, Accuracy: 88.28%\n",
      "Batch 77, Loss: 0.944374, Accuracy: 88.17%\n",
      "Batch 78, Loss: 0.791909, Accuracy: 88.26%\n",
      "Batch 79, Loss: 0.860827, Accuracy: 88.27%\n",
      "Batch 80, Loss: 0.847737, Accuracy: 88.32%\n",
      "Batch 81, Loss: 0.816844, Accuracy: 88.39%\n",
      "Batch 82, Loss: 0.831177, Accuracy: 88.41%\n",
      "Batch 83, Loss: 0.846212, Accuracy: 88.42%\n",
      "Batch 84, Loss: 0.847814, Accuracy: 88.45%\n",
      "Batch 85, Loss: 0.888437, Accuracy: 88.42%\n",
      "Batch 86, Loss: 0.819297, Accuracy: 88.48%\n",
      "Batch 87, Loss: 0.906412, Accuracy: 88.42%\n",
      "Batch 88, Loss: 0.895011, Accuracy: 88.37%\n",
      "Batch 89, Loss: 0.893143, Accuracy: 88.33%\n",
      "Batch 90, Loss: 0.835264, Accuracy: 88.37%\n",
      "Batch 91, Loss: 0.872592, Accuracy: 88.36%\n",
      "Batch 92, Loss: 0.882519, Accuracy: 88.33%\n",
      "Batch 93, Loss: 0.842515, Accuracy: 88.34%\n",
      "Batch 94, Loss: 0.808268, Accuracy: 88.40%\n",
      "Batch 95, Loss: 0.877863, Accuracy: 88.39%\n",
      "Batch 96, Loss: 0.810672, Accuracy: 88.44%\n",
      "Batch 97, Loss: 0.879707, Accuracy: 88.42%\n",
      "Batch 98, Loss: 0.908284, Accuracy: 88.36%\n",
      "Batch 99, Loss: 0.894000, Accuracy: 88.34%\n",
      "Batch 100, Loss: 0.806146, Accuracy: 88.39%\n",
      "Batch 101, Loss: 0.912393, Accuracy: 88.32%\n",
      "Batch 102, Loss: 0.854013, Accuracy: 88.31%\n",
      "Batch 103, Loss: 0.814178, Accuracy: 88.36%\n",
      "Batch 104, Loss: 0.850390, Accuracy: 88.36%\n",
      "Batch 105, Loss: 0.817836, Accuracy: 88.39%\n",
      "Batch 106, Loss: 0.796956, Accuracy: 88.46%\n",
      "Batch 107, Loss: 0.863294, Accuracy: 88.45%\n",
      "Batch 108, Loss: 0.824455, Accuracy: 88.47%\n",
      "Batch 109, Loss: 0.838209, Accuracy: 88.50%\n",
      "Batch 110, Loss: 0.845608, Accuracy: 88.51%\n",
      "Batch 111, Loss: 0.900491, Accuracy: 88.47%\n",
      "Batch 112, Loss: 0.892562, Accuracy: 88.45%\n",
      "Batch 113, Loss: 0.839820, Accuracy: 88.47%\n",
      "Batch 114, Loss: 0.785138, Accuracy: 88.54%\n",
      "Batch 115, Loss: 0.855334, Accuracy: 88.55%\n",
      "Batch 116, Loss: 0.883796, Accuracy: 88.51%\n",
      "Batch 117, Loss: 0.837837, Accuracy: 88.53%\n",
      "Batch 118, Loss: 0.822646, Accuracy: 88.56%\n",
      "Batch 119, Loss: 0.846532, Accuracy: 88.58%\n",
      "Batch 120, Loss: 0.823176, Accuracy: 88.61%\n",
      "Batch 121, Loss: 0.875965, Accuracy: 88.60%\n",
      "Batch 122, Loss: 0.873275, Accuracy: 88.58%\n",
      "Batch 123, Loss: 0.866801, Accuracy: 88.55%\n",
      "Batch 124, Loss: 0.933185, Accuracy: 88.48%\n",
      "Batch 125, Loss: 0.866310, Accuracy: 88.47%\n",
      "Batch 126, Loss: 0.850194, Accuracy: 88.48%\n",
      "Batch 127, Loss: 0.897002, Accuracy: 88.45%\n",
      "Batch 128, Loss: 0.916503, Accuracy: 88.40%\n",
      "Batch 129, Loss: 0.872563, Accuracy: 88.41%\n",
      "Batch 130, Loss: 0.892116, Accuracy: 88.38%\n",
      "Batch 131, Loss: 0.848568, Accuracy: 88.39%\n",
      "Batch 132, Loss: 0.881342, Accuracy: 88.38%\n",
      "Batch 133, Loss: 0.898025, Accuracy: 88.36%\n",
      "Batch 134, Loss: 0.854800, Accuracy: 88.36%\n",
      "Batch 135, Loss: 0.859506, Accuracy: 88.37%\n",
      "Batch 136, Loss: 0.868608, Accuracy: 88.36%\n",
      "Batch 137, Loss: 0.809941, Accuracy: 88.40%\n",
      "Batch 138, Loss: 0.869864, Accuracy: 88.39%\n",
      "Batch 139, Loss: 0.834879, Accuracy: 88.42%\n",
      "Batch 140, Loss: 0.865537, Accuracy: 88.43%\n",
      "Batch 141, Loss: 0.840596, Accuracy: 88.43%\n",
      "Batch 142, Loss: 0.864849, Accuracy: 88.44%\n",
      "Batch 143, Loss: 0.937139, Accuracy: 88.39%\n",
      "Batch 144, Loss: 0.824612, Accuracy: 88.41%\n",
      "Batch 145, Loss: 0.847361, Accuracy: 88.43%\n",
      "Batch 146, Loss: 0.838368, Accuracy: 88.44%\n",
      "Batch 147, Loss: 0.881029, Accuracy: 88.42%\n",
      "Batch 148, Loss: 0.930970, Accuracy: 88.38%\n",
      "Batch 149, Loss: 0.855052, Accuracy: 88.38%\n",
      "Batch 150, Loss: 0.832858, Accuracy: 88.40%\n",
      "Batch 151, Loss: 0.845208, Accuracy: 88.41%\n",
      "Batch 152, Loss: 0.877502, Accuracy: 88.38%\n",
      "Batch 153, Loss: 0.827323, Accuracy: 88.41%\n",
      "Batch 154, Loss: 0.815229, Accuracy: 88.44%\n",
      "Batch 155, Loss: 0.912502, Accuracy: 88.42%\n",
      "Batch 156, Loss: 0.878357, Accuracy: 88.39%\n",
      "Batch 157, Loss: 0.822729, Accuracy: 88.42%\n",
      "Batch 158, Loss: 0.849753, Accuracy: 88.43%\n",
      "Batch 159, Loss: 0.831745, Accuracy: 88.45%\n",
      "Batch 160, Loss: 0.974704, Accuracy: 88.38%\n",
      "Batch 161, Loss: 0.793658, Accuracy: 88.42%\n",
      "Batch 162, Loss: 0.856278, Accuracy: 88.43%\n",
      "Batch 163, Loss: 0.941276, Accuracy: 88.38%\n",
      "Batch 164, Loss: 0.876071, Accuracy: 88.37%\n",
      "Batch 165, Loss: 0.882119, Accuracy: 88.36%\n",
      "Batch 166, Loss: 0.946823, Accuracy: 88.31%\n",
      "Batch 167, Loss: 0.901975, Accuracy: 88.29%\n",
      "Batch 168, Loss: 0.940635, Accuracy: 88.23%\n",
      "Batch 169, Loss: 0.815698, Accuracy: 88.27%\n",
      "Batch 170, Loss: 0.840936, Accuracy: 88.28%\n",
      "Batch 171, Loss: 0.870733, Accuracy: 88.27%\n",
      "Batch 172, Loss: 0.843857, Accuracy: 88.27%\n",
      "Batch 173, Loss: 0.843754, Accuracy: 88.29%\n",
      "Batch 174, Loss: 0.847684, Accuracy: 88.30%\n",
      "Batch 175, Loss: 0.831401, Accuracy: 88.31%\n",
      "Batch 176, Loss: 0.868852, Accuracy: 88.31%\n",
      "Batch 177, Loss: 0.865673, Accuracy: 88.30%\n",
      "Batch 178, Loss: 0.878063, Accuracy: 88.29%\n",
      "Batch 179, Loss: 0.845267, Accuracy: 88.30%\n",
      "Batch 180, Loss: 0.841115, Accuracy: 88.32%\n",
      "Batch 181, Loss: 0.848984, Accuracy: 88.33%\n",
      "Batch 182, Loss: 0.903887, Accuracy: 88.30%\n",
      "Batch 183, Loss: 0.801398, Accuracy: 88.33%\n",
      "Batch 184, Loss: 0.846421, Accuracy: 88.33%\n",
      "Batch 185, Loss: 0.857120, Accuracy: 88.34%\n",
      "Batch 186, Loss: 0.830804, Accuracy: 88.36%\n",
      "Batch 187, Loss: 0.855112, Accuracy: 88.37%\n",
      "Batch 188, Loss: 0.902714, Accuracy: 88.35%\n",
      "Batch 189, Loss: 0.945089, Accuracy: 88.30%\n",
      "Batch 190, Loss: 0.859050, Accuracy: 88.31%\n",
      "Batch 191, Loss: 0.871433, Accuracy: 88.30%\n",
      "Batch 192, Loss: 0.863191, Accuracy: 88.31%\n",
      "Batch 193, Loss: 0.882411, Accuracy: 88.30%\n",
      "Batch 194, Loss: 0.861819, Accuracy: 88.31%\n",
      "Batch 195, Loss: 0.847594, Accuracy: 88.31%\n",
      "Batch 196, Loss: 0.859287, Accuracy: 88.31%\n",
      "Batch 197, Loss: 0.864249, Accuracy: 88.32%\n",
      "Batch 198, Loss: 0.822237, Accuracy: 88.34%\n",
      "Batch 199, Loss: 0.882126, Accuracy: 88.32%\n",
      "Batch 200, Loss: 0.782793, Accuracy: 88.37%\n",
      "Batch 201, Loss: 0.815609, Accuracy: 88.39%\n",
      "Batch 202, Loss: 0.875394, Accuracy: 88.37%\n",
      "Batch 203, Loss: 0.891761, Accuracy: 88.36%\n",
      "Batch 204, Loss: 0.860139, Accuracy: 88.37%\n",
      "Batch 205, Loss: 0.845770, Accuracy: 88.37%\n",
      "Batch 206, Loss: 0.877424, Accuracy: 88.36%\n",
      "Batch 207, Loss: 0.831802, Accuracy: 88.37%\n",
      "Batch 208, Loss: 0.859604, Accuracy: 88.38%\n",
      "Batch 209, Loss: 0.847789, Accuracy: 88.37%\n",
      "Batch 210, Loss: 0.845107, Accuracy: 88.39%\n",
      "Batch 211, Loss: 0.918451, Accuracy: 88.35%\n",
      "Batch 212, Loss: 0.918586, Accuracy: 88.33%\n",
      "Batch 213, Loss: 0.856744, Accuracy: 88.33%\n",
      "Training - Epoch 106, Loss: 0.861440, Accuracy: 88.33%\n",
      "Validation Batch 1, Loss: 0.824334, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.801670, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.855722, Accuracy: 92.71%\n",
      "Validation Batch 4, Loss: 0.854907, Accuracy: 91.80%\n",
      "Validation Batch 5, Loss: 0.841029, Accuracy: 91.25%\n",
      "Validation Batch 6, Loss: 0.803287, Accuracy: 91.93%\n",
      "Validation Batch 7, Loss: 0.807478, Accuracy: 92.41%\n",
      "Validation Batch 8, Loss: 0.889565, Accuracy: 91.41%\n",
      "Validation Batch 9, Loss: 0.900618, Accuracy: 90.45%\n",
      "Validation Batch 10, Loss: 0.825811, Accuracy: 90.78%\n",
      "Validation Batch 11, Loss: 0.841494, Accuracy: 90.91%\n",
      "Validation Batch 12, Loss: 0.836592, Accuracy: 91.02%\n",
      "Validation Batch 13, Loss: 0.846822, Accuracy: 90.87%\n",
      "Validation Batch 14, Loss: 0.851353, Accuracy: 90.74%\n",
      "Validation Batch 15, Loss: 0.826908, Accuracy: 90.83%\n",
      "Validation Batch 16, Loss: 0.845855, Accuracy: 90.82%\n",
      "Validation Batch 17, Loss: 0.872295, Accuracy: 90.72%\n",
      "Validation Batch 18, Loss: 0.818526, Accuracy: 90.89%\n",
      "Validation Batch 19, Loss: 0.884620, Accuracy: 90.62%\n",
      "Validation Batch 20, Loss: 0.873537, Accuracy: 90.39%\n",
      "Validation Batch 21, Loss: 0.867313, Accuracy: 90.33%\n",
      "Validation Batch 22, Loss: 0.852836, Accuracy: 90.27%\n",
      "Validation Batch 23, Loss: 0.879084, Accuracy: 90.15%\n",
      "Validation Batch 24, Loss: 0.858050, Accuracy: 90.04%\n",
      "Validation Batch 25, Loss: 0.825758, Accuracy: 90.12%\n",
      "Validation Batch 26, Loss: 0.860108, Accuracy: 90.02%\n",
      "Validation Batch 27, Loss: 0.808112, Accuracy: 90.14%\n",
      "Validation - Epoch 106, Loss: 0.846433, Accuracy: 90.14%\n",
      "Patience—2\n",
      "Epoch 107\n",
      "Batch 1, Loss: 0.875249, Accuracy: 87.50%\n",
      "Batch 2, Loss: 0.880641, Accuracy: 86.72%\n",
      "Batch 3, Loss: 0.878118, Accuracy: 86.98%\n",
      "Batch 4, Loss: 0.867283, Accuracy: 87.11%\n",
      "Batch 5, Loss: 0.888222, Accuracy: 86.88%\n",
      "Batch 6, Loss: 0.843241, Accuracy: 87.24%\n",
      "Batch 7, Loss: 0.850423, Accuracy: 87.50%\n",
      "Batch 8, Loss: 0.853350, Accuracy: 87.89%\n",
      "Batch 9, Loss: 0.877897, Accuracy: 87.85%\n",
      "Batch 10, Loss: 0.859420, Accuracy: 87.97%\n",
      "Batch 11, Loss: 0.868023, Accuracy: 87.93%\n",
      "Batch 12, Loss: 0.808023, Accuracy: 88.41%\n",
      "Batch 13, Loss: 0.822451, Accuracy: 88.70%\n",
      "Batch 14, Loss: 0.846871, Accuracy: 88.84%\n",
      "Batch 15, Loss: 0.916014, Accuracy: 88.44%\n",
      "Batch 16, Loss: 0.875443, Accuracy: 88.38%\n",
      "Batch 17, Loss: 0.828453, Accuracy: 88.60%\n",
      "Batch 18, Loss: 0.799716, Accuracy: 88.98%\n",
      "Batch 19, Loss: 0.910874, Accuracy: 88.57%\n",
      "Batch 20, Loss: 0.810129, Accuracy: 88.91%\n",
      "Batch 21, Loss: 0.838721, Accuracy: 88.99%\n",
      "Batch 22, Loss: 0.876475, Accuracy: 88.92%\n",
      "Batch 23, Loss: 0.814947, Accuracy: 89.13%\n",
      "Batch 24, Loss: 0.853899, Accuracy: 89.19%\n",
      "Batch 25, Loss: 0.872074, Accuracy: 89.12%\n",
      "Batch 26, Loss: 0.872836, Accuracy: 89.06%\n",
      "Batch 27, Loss: 0.858847, Accuracy: 89.06%\n",
      "Batch 28, Loss: 0.827062, Accuracy: 89.17%\n",
      "Batch 29, Loss: 0.947647, Accuracy: 88.79%\n",
      "Batch 30, Loss: 0.860099, Accuracy: 88.80%\n",
      "Batch 31, Loss: 0.827563, Accuracy: 88.91%\n",
      "Batch 32, Loss: 0.836426, Accuracy: 88.96%\n",
      "Batch 33, Loss: 0.825948, Accuracy: 89.06%\n",
      "Batch 34, Loss: 0.857971, Accuracy: 89.06%\n",
      "Batch 35, Loss: 0.829397, Accuracy: 89.15%\n",
      "Batch 36, Loss: 0.867045, Accuracy: 89.11%\n",
      "Batch 37, Loss: 0.882824, Accuracy: 89.06%\n",
      "Batch 38, Loss: 0.806216, Accuracy: 89.14%\n",
      "Batch 39, Loss: 0.849980, Accuracy: 89.14%\n",
      "Batch 40, Loss: 0.899196, Accuracy: 89.02%\n",
      "Batch 41, Loss: 0.908849, Accuracy: 88.83%\n",
      "Batch 42, Loss: 0.899949, Accuracy: 88.73%\n",
      "Batch 43, Loss: 0.847276, Accuracy: 88.70%\n",
      "Batch 44, Loss: 0.840480, Accuracy: 88.74%\n",
      "Batch 45, Loss: 0.805979, Accuracy: 88.89%\n",
      "Batch 46, Loss: 0.834763, Accuracy: 88.93%\n",
      "Batch 47, Loss: 0.840380, Accuracy: 88.96%\n",
      "Batch 48, Loss: 0.806105, Accuracy: 89.06%\n",
      "Batch 49, Loss: 0.915134, Accuracy: 88.93%\n",
      "Batch 50, Loss: 0.816471, Accuracy: 89.03%\n",
      "Batch 51, Loss: 0.903596, Accuracy: 88.94%\n",
      "Batch 52, Loss: 0.928030, Accuracy: 88.79%\n",
      "Batch 53, Loss: 0.854881, Accuracy: 88.80%\n",
      "Batch 54, Loss: 0.856382, Accuracy: 88.80%\n",
      "Batch 55, Loss: 0.842639, Accuracy: 88.84%\n",
      "Batch 56, Loss: 0.882367, Accuracy: 88.81%\n",
      "Batch 57, Loss: 0.870146, Accuracy: 88.76%\n",
      "Batch 58, Loss: 0.829860, Accuracy: 88.79%\n",
      "Batch 59, Loss: 0.891998, Accuracy: 88.72%\n",
      "Batch 60, Loss: 0.863993, Accuracy: 88.72%\n",
      "Batch 61, Loss: 0.898742, Accuracy: 88.68%\n",
      "Batch 62, Loss: 0.819603, Accuracy: 88.76%\n",
      "Batch 63, Loss: 0.827711, Accuracy: 88.81%\n",
      "Batch 64, Loss: 0.851631, Accuracy: 88.82%\n",
      "Batch 65, Loss: 0.854018, Accuracy: 88.82%\n",
      "Batch 66, Loss: 0.942216, Accuracy: 88.71%\n",
      "Batch 67, Loss: 0.855964, Accuracy: 88.71%\n",
      "Batch 68, Loss: 0.811916, Accuracy: 88.79%\n",
      "Batch 69, Loss: 0.878510, Accuracy: 88.75%\n",
      "Batch 70, Loss: 0.838945, Accuracy: 88.79%\n",
      "Batch 71, Loss: 0.903650, Accuracy: 88.73%\n",
      "Batch 72, Loss: 0.804421, Accuracy: 88.82%\n",
      "Batch 73, Loss: 0.922326, Accuracy: 88.74%\n",
      "Batch 74, Loss: 0.907302, Accuracy: 88.66%\n",
      "Batch 75, Loss: 0.864643, Accuracy: 88.62%\n",
      "Batch 76, Loss: 0.854508, Accuracy: 88.63%\n",
      "Batch 77, Loss: 0.872094, Accuracy: 88.62%\n",
      "Batch 78, Loss: 0.856867, Accuracy: 88.60%\n",
      "Batch 79, Loss: 0.859719, Accuracy: 88.61%\n",
      "Batch 80, Loss: 0.886656, Accuracy: 88.59%\n",
      "Batch 81, Loss: 0.917322, Accuracy: 88.48%\n",
      "Batch 82, Loss: 0.880314, Accuracy: 88.47%\n",
      "Batch 83, Loss: 0.837832, Accuracy: 88.50%\n",
      "Batch 84, Loss: 0.827603, Accuracy: 88.54%\n",
      "Batch 85, Loss: 0.889522, Accuracy: 88.51%\n",
      "Batch 86, Loss: 0.842659, Accuracy: 88.54%\n",
      "Batch 87, Loss: 0.876668, Accuracy: 88.51%\n",
      "Batch 88, Loss: 0.914647, Accuracy: 88.42%\n",
      "Batch 89, Loss: 0.878594, Accuracy: 88.41%\n",
      "Batch 90, Loss: 0.875363, Accuracy: 88.40%\n",
      "Batch 91, Loss: 0.874382, Accuracy: 88.38%\n",
      "Batch 92, Loss: 0.872639, Accuracy: 88.35%\n",
      "Batch 93, Loss: 0.797957, Accuracy: 88.42%\n",
      "Batch 94, Loss: 0.834316, Accuracy: 88.46%\n",
      "Batch 95, Loss: 0.826203, Accuracy: 88.50%\n",
      "Batch 96, Loss: 0.852071, Accuracy: 88.51%\n",
      "Batch 97, Loss: 0.858454, Accuracy: 88.50%\n",
      "Batch 98, Loss: 0.871108, Accuracy: 88.49%\n",
      "Batch 99, Loss: 0.883541, Accuracy: 88.46%\n",
      "Batch 100, Loss: 0.861113, Accuracy: 88.45%\n",
      "Batch 101, Loss: 0.817769, Accuracy: 88.49%\n",
      "Batch 102, Loss: 0.863295, Accuracy: 88.50%\n",
      "Batch 103, Loss: 0.851004, Accuracy: 88.49%\n",
      "Batch 104, Loss: 0.832670, Accuracy: 88.51%\n",
      "Batch 105, Loss: 0.850297, Accuracy: 88.51%\n",
      "Batch 106, Loss: 0.783652, Accuracy: 88.59%\n",
      "Batch 107, Loss: 0.858606, Accuracy: 88.57%\n",
      "Batch 108, Loss: 0.765689, Accuracy: 88.66%\n",
      "Batch 109, Loss: 0.803905, Accuracy: 88.70%\n",
      "Batch 110, Loss: 0.834997, Accuracy: 88.72%\n",
      "Batch 111, Loss: 0.808319, Accuracy: 88.77%\n",
      "Batch 112, Loss: 0.821956, Accuracy: 88.80%\n",
      "Batch 113, Loss: 0.879933, Accuracy: 88.79%\n",
      "Batch 114, Loss: 0.903567, Accuracy: 88.75%\n",
      "Batch 115, Loss: 0.929037, Accuracy: 88.68%\n",
      "Batch 116, Loss: 0.865675, Accuracy: 88.67%\n",
      "Batch 117, Loss: 0.840478, Accuracy: 88.68%\n",
      "Batch 118, Loss: 0.874194, Accuracy: 88.67%\n",
      "Batch 119, Loss: 0.889871, Accuracy: 88.64%\n",
      "Batch 120, Loss: 0.875949, Accuracy: 88.62%\n",
      "Batch 121, Loss: 0.836239, Accuracy: 88.65%\n",
      "Batch 122, Loss: 0.838809, Accuracy: 88.67%\n",
      "Batch 123, Loss: 0.771431, Accuracy: 88.74%\n",
      "Batch 124, Loss: 0.868294, Accuracy: 88.75%\n",
      "Batch 125, Loss: 0.877617, Accuracy: 88.72%\n",
      "Batch 126, Loss: 0.842317, Accuracy: 88.74%\n",
      "Batch 127, Loss: 0.876343, Accuracy: 88.73%\n",
      "Batch 128, Loss: 0.878779, Accuracy: 88.71%\n",
      "Batch 129, Loss: 0.926494, Accuracy: 88.64%\n",
      "Batch 130, Loss: 0.804626, Accuracy: 88.68%\n",
      "Batch 131, Loss: 0.829793, Accuracy: 88.70%\n",
      "Batch 132, Loss: 0.908032, Accuracy: 88.66%\n",
      "Batch 133, Loss: 0.925532, Accuracy: 88.60%\n",
      "Batch 134, Loss: 0.839330, Accuracy: 88.62%\n",
      "Batch 135, Loss: 0.884733, Accuracy: 88.60%\n",
      "Batch 136, Loss: 0.818442, Accuracy: 88.63%\n",
      "Batch 137, Loss: 0.953936, Accuracy: 88.55%\n",
      "Batch 138, Loss: 0.873932, Accuracy: 88.53%\n",
      "Batch 139, Loss: 0.826222, Accuracy: 88.55%\n",
      "Batch 140, Loss: 0.869474, Accuracy: 88.54%\n",
      "Batch 141, Loss: 0.820407, Accuracy: 88.56%\n",
      "Batch 142, Loss: 0.875013, Accuracy: 88.56%\n",
      "Batch 143, Loss: 0.836548, Accuracy: 88.57%\n",
      "Batch 144, Loss: 0.803801, Accuracy: 88.61%\n",
      "Batch 145, Loss: 0.882745, Accuracy: 88.59%\n",
      "Batch 146, Loss: 0.870301, Accuracy: 88.57%\n",
      "Batch 147, Loss: 0.849747, Accuracy: 88.58%\n",
      "Batch 148, Loss: 0.874758, Accuracy: 88.58%\n",
      "Batch 149, Loss: 0.901954, Accuracy: 88.56%\n",
      "Batch 150, Loss: 0.882720, Accuracy: 88.54%\n",
      "Batch 151, Loss: 0.852083, Accuracy: 88.55%\n",
      "Batch 152, Loss: 0.874981, Accuracy: 88.54%\n",
      "Batch 153, Loss: 0.827989, Accuracy: 88.56%\n",
      "Batch 154, Loss: 0.859929, Accuracy: 88.57%\n",
      "Batch 155, Loss: 0.898213, Accuracy: 88.54%\n",
      "Batch 156, Loss: 0.901052, Accuracy: 88.51%\n",
      "Batch 157, Loss: 0.859272, Accuracy: 88.50%\n",
      "Batch 158, Loss: 0.819677, Accuracy: 88.53%\n",
      "Batch 159, Loss: 0.855646, Accuracy: 88.53%\n",
      "Batch 160, Loss: 0.874522, Accuracy: 88.53%\n",
      "Batch 161, Loss: 0.885730, Accuracy: 88.51%\n",
      "Batch 162, Loss: 0.819853, Accuracy: 88.54%\n",
      "Batch 163, Loss: 0.822208, Accuracy: 88.56%\n",
      "Batch 164, Loss: 0.841557, Accuracy: 88.58%\n",
      "Batch 165, Loss: 0.903361, Accuracy: 88.54%\n",
      "Batch 166, Loss: 0.913543, Accuracy: 88.51%\n",
      "Batch 167, Loss: 0.902064, Accuracy: 88.48%\n",
      "Batch 168, Loss: 0.898154, Accuracy: 88.46%\n",
      "Batch 169, Loss: 0.834041, Accuracy: 88.48%\n",
      "Batch 170, Loss: 0.928353, Accuracy: 88.44%\n",
      "Batch 171, Loss: 0.814921, Accuracy: 88.45%\n",
      "Batch 172, Loss: 0.836287, Accuracy: 88.46%\n",
      "Batch 173, Loss: 0.822663, Accuracy: 88.48%\n",
      "Batch 174, Loss: 0.819880, Accuracy: 88.50%\n",
      "Batch 175, Loss: 0.847239, Accuracy: 88.51%\n",
      "Batch 176, Loss: 0.827817, Accuracy: 88.53%\n",
      "Batch 177, Loss: 0.850276, Accuracy: 88.53%\n",
      "Batch 178, Loss: 0.925958, Accuracy: 88.50%\n",
      "Batch 179, Loss: 0.874326, Accuracy: 88.50%\n",
      "Batch 180, Loss: 0.826825, Accuracy: 88.52%\n",
      "Batch 181, Loss: 0.811586, Accuracy: 88.55%\n",
      "Batch 182, Loss: 0.803288, Accuracy: 88.59%\n",
      "Batch 183, Loss: 0.906242, Accuracy: 88.57%\n",
      "Batch 184, Loss: 0.895524, Accuracy: 88.55%\n",
      "Batch 185, Loss: 0.867789, Accuracy: 88.55%\n",
      "Batch 186, Loss: 0.858813, Accuracy: 88.55%\n",
      "Batch 187, Loss: 0.902085, Accuracy: 88.53%\n",
      "Batch 188, Loss: 0.826818, Accuracy: 88.54%\n",
      "Batch 189, Loss: 0.883339, Accuracy: 88.53%\n",
      "Batch 190, Loss: 0.868366, Accuracy: 88.52%\n",
      "Batch 191, Loss: 0.829140, Accuracy: 88.53%\n",
      "Batch 192, Loss: 0.877778, Accuracy: 88.52%\n",
      "Batch 193, Loss: 0.817439, Accuracy: 88.54%\n",
      "Batch 194, Loss: 0.825921, Accuracy: 88.56%\n",
      "Batch 195, Loss: 0.818698, Accuracy: 88.57%\n",
      "Batch 196, Loss: 0.859896, Accuracy: 88.57%\n",
      "Batch 197, Loss: 0.847435, Accuracy: 88.57%\n",
      "Batch 198, Loss: 0.814507, Accuracy: 88.59%\n",
      "Batch 199, Loss: 0.921851, Accuracy: 88.55%\n",
      "Batch 200, Loss: 0.867210, Accuracy: 88.55%\n",
      "Batch 201, Loss: 0.861413, Accuracy: 88.55%\n",
      "Batch 202, Loss: 0.766596, Accuracy: 88.60%\n",
      "Batch 203, Loss: 0.841879, Accuracy: 88.61%\n",
      "Batch 204, Loss: 0.813423, Accuracy: 88.63%\n",
      "Batch 205, Loss: 0.851812, Accuracy: 88.64%\n",
      "Batch 206, Loss: 0.801943, Accuracy: 88.67%\n",
      "Batch 207, Loss: 0.903319, Accuracy: 88.65%\n",
      "Batch 208, Loss: 0.845542, Accuracy: 88.66%\n",
      "Batch 209, Loss: 0.794649, Accuracy: 88.70%\n",
      "Batch 210, Loss: 0.819401, Accuracy: 88.72%\n",
      "Batch 211, Loss: 0.901373, Accuracy: 88.69%\n",
      "Batch 212, Loss: 0.868585, Accuracy: 88.69%\n",
      "Batch 213, Loss: 0.894970, Accuracy: 88.67%\n",
      "Training - Epoch 107, Loss: 0.858095, Accuracy: 88.67%\n",
      "Validation Batch 1, Loss: 0.830541, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.810145, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.862618, Accuracy: 92.19%\n",
      "Validation Batch 4, Loss: 0.857867, Accuracy: 91.41%\n",
      "Validation Batch 5, Loss: 0.848444, Accuracy: 90.94%\n",
      "Validation Batch 6, Loss: 0.806465, Accuracy: 91.67%\n",
      "Validation Batch 7, Loss: 0.811491, Accuracy: 91.96%\n",
      "Validation Batch 8, Loss: 0.893361, Accuracy: 90.82%\n",
      "Validation Batch 9, Loss: 0.903243, Accuracy: 89.93%\n",
      "Validation Batch 10, Loss: 0.827099, Accuracy: 90.16%\n",
      "Validation Batch 11, Loss: 0.842248, Accuracy: 90.34%\n",
      "Validation Batch 12, Loss: 0.845921, Accuracy: 90.49%\n",
      "Validation Batch 13, Loss: 0.852901, Accuracy: 90.38%\n",
      "Validation Batch 14, Loss: 0.856586, Accuracy: 90.29%\n",
      "Validation Batch 15, Loss: 0.828710, Accuracy: 90.31%\n",
      "Validation Batch 16, Loss: 0.850477, Accuracy: 90.33%\n",
      "Validation Batch 17, Loss: 0.873879, Accuracy: 90.26%\n",
      "Validation Batch 18, Loss: 0.823687, Accuracy: 90.28%\n",
      "Validation Batch 19, Loss: 0.892043, Accuracy: 90.05%\n",
      "Validation Batch 20, Loss: 0.884422, Accuracy: 89.77%\n",
      "Validation Batch 21, Loss: 0.870374, Accuracy: 89.73%\n",
      "Validation Batch 22, Loss: 0.856878, Accuracy: 89.63%\n",
      "Validation Batch 23, Loss: 0.885673, Accuracy: 89.54%\n",
      "Validation Batch 24, Loss: 0.867057, Accuracy: 89.45%\n",
      "Validation Batch 25, Loss: 0.829123, Accuracy: 89.56%\n",
      "Validation Batch 26, Loss: 0.859790, Accuracy: 89.54%\n",
      "Validation Batch 27, Loss: 0.814752, Accuracy: 89.67%\n",
      "Validation - Epoch 107, Loss: 0.851326, Accuracy: 89.67%\n",
      "Patience—3\n",
      "Epoch 108\n",
      "Batch 1, Loss: 0.834950, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.817867, Accuracy: 91.41%\n",
      "Batch 3, Loss: 0.886090, Accuracy: 89.06%\n",
      "Batch 4, Loss: 0.838016, Accuracy: 89.45%\n",
      "Batch 5, Loss: 0.794226, Accuracy: 90.94%\n",
      "Batch 6, Loss: 0.891870, Accuracy: 89.84%\n",
      "Batch 7, Loss: 0.832665, Accuracy: 89.96%\n",
      "Batch 8, Loss: 0.868609, Accuracy: 89.65%\n",
      "Batch 9, Loss: 0.864942, Accuracy: 89.58%\n",
      "Batch 10, Loss: 0.833815, Accuracy: 89.69%\n",
      "Batch 11, Loss: 0.872876, Accuracy: 89.49%\n",
      "Batch 12, Loss: 0.824329, Accuracy: 89.71%\n",
      "Batch 13, Loss: 0.902352, Accuracy: 89.30%\n",
      "Batch 14, Loss: 0.880774, Accuracy: 89.06%\n",
      "Batch 15, Loss: 0.865079, Accuracy: 89.06%\n",
      "Batch 16, Loss: 0.828860, Accuracy: 89.36%\n",
      "Batch 17, Loss: 0.868340, Accuracy: 89.15%\n",
      "Batch 18, Loss: 0.847567, Accuracy: 89.24%\n",
      "Batch 19, Loss: 0.903843, Accuracy: 88.98%\n",
      "Batch 20, Loss: 0.825478, Accuracy: 89.14%\n",
      "Batch 21, Loss: 0.843916, Accuracy: 89.29%\n",
      "Batch 22, Loss: 0.888782, Accuracy: 89.06%\n",
      "Batch 23, Loss: 0.858820, Accuracy: 89.06%\n",
      "Batch 24, Loss: 0.816526, Accuracy: 89.26%\n",
      "Batch 25, Loss: 0.860829, Accuracy: 89.25%\n",
      "Batch 26, Loss: 0.847587, Accuracy: 89.24%\n",
      "Batch 27, Loss: 0.869220, Accuracy: 89.18%\n",
      "Batch 28, Loss: 0.892384, Accuracy: 89.06%\n",
      "Batch 29, Loss: 0.902534, Accuracy: 88.90%\n",
      "Batch 30, Loss: 0.837925, Accuracy: 88.96%\n",
      "Batch 31, Loss: 0.887235, Accuracy: 88.86%\n",
      "Batch 32, Loss: 0.878895, Accuracy: 88.82%\n",
      "Batch 33, Loss: 0.948975, Accuracy: 88.49%\n",
      "Batch 34, Loss: 0.907932, Accuracy: 88.33%\n",
      "Batch 35, Loss: 0.960659, Accuracy: 87.99%\n",
      "Batch 36, Loss: 0.974628, Accuracy: 87.63%\n",
      "Batch 37, Loss: 0.816243, Accuracy: 87.80%\n",
      "Batch 38, Loss: 0.829992, Accuracy: 87.91%\n",
      "Batch 39, Loss: 0.880946, Accuracy: 87.86%\n",
      "Batch 40, Loss: 0.913585, Accuracy: 87.73%\n",
      "Batch 41, Loss: 0.828865, Accuracy: 87.84%\n",
      "Batch 42, Loss: 0.863742, Accuracy: 87.83%\n",
      "Batch 43, Loss: 0.840095, Accuracy: 87.90%\n",
      "Batch 44, Loss: 0.810082, Accuracy: 88.03%\n",
      "Batch 45, Loss: 0.847051, Accuracy: 88.09%\n",
      "Batch 46, Loss: 0.879101, Accuracy: 88.04%\n",
      "Batch 47, Loss: 0.827534, Accuracy: 88.10%\n",
      "Batch 48, Loss: 0.848304, Accuracy: 88.12%\n",
      "Batch 49, Loss: 0.807590, Accuracy: 88.20%\n",
      "Batch 50, Loss: 0.898872, Accuracy: 88.16%\n",
      "Batch 51, Loss: 0.857416, Accuracy: 88.14%\n",
      "Batch 52, Loss: 0.834701, Accuracy: 88.19%\n",
      "Batch 53, Loss: 0.861457, Accuracy: 88.18%\n",
      "Batch 54, Loss: 0.847176, Accuracy: 88.19%\n",
      "Batch 55, Loss: 0.827541, Accuracy: 88.24%\n",
      "Batch 56, Loss: 0.909407, Accuracy: 88.14%\n",
      "Batch 57, Loss: 0.836193, Accuracy: 88.21%\n",
      "Batch 58, Loss: 0.919159, Accuracy: 88.12%\n",
      "Batch 59, Loss: 0.828017, Accuracy: 88.16%\n",
      "Batch 60, Loss: 0.870648, Accuracy: 88.15%\n",
      "Batch 61, Loss: 0.893762, Accuracy: 88.09%\n",
      "Batch 62, Loss: 0.836595, Accuracy: 88.18%\n",
      "Batch 63, Loss: 0.890498, Accuracy: 88.14%\n",
      "Batch 64, Loss: 0.857409, Accuracy: 88.13%\n",
      "Batch 65, Loss: 0.836375, Accuracy: 88.20%\n",
      "Batch 66, Loss: 0.870396, Accuracy: 88.16%\n",
      "Batch 67, Loss: 0.819483, Accuracy: 88.22%\n",
      "Batch 68, Loss: 0.844412, Accuracy: 88.26%\n",
      "Batch 69, Loss: 0.847200, Accuracy: 88.29%\n",
      "Batch 70, Loss: 0.822685, Accuracy: 88.35%\n",
      "Batch 71, Loss: 0.848520, Accuracy: 88.38%\n",
      "Batch 72, Loss: 0.880482, Accuracy: 88.35%\n",
      "Batch 73, Loss: 0.863268, Accuracy: 88.33%\n",
      "Batch 74, Loss: 0.860580, Accuracy: 88.32%\n",
      "Batch 75, Loss: 0.881784, Accuracy: 88.29%\n",
      "Batch 76, Loss: 0.847341, Accuracy: 88.30%\n",
      "Batch 77, Loss: 0.859464, Accuracy: 88.31%\n",
      "Batch 78, Loss: 0.862312, Accuracy: 88.32%\n",
      "Batch 79, Loss: 0.884439, Accuracy: 88.29%\n",
      "Batch 80, Loss: 0.904114, Accuracy: 88.24%\n",
      "Batch 81, Loss: 0.851927, Accuracy: 88.25%\n",
      "Batch 82, Loss: 0.828718, Accuracy: 88.28%\n",
      "Batch 83, Loss: 0.887601, Accuracy: 88.25%\n",
      "Batch 84, Loss: 0.944946, Accuracy: 88.15%\n",
      "Batch 85, Loss: 0.832889, Accuracy: 88.20%\n",
      "Batch 86, Loss: 0.826296, Accuracy: 88.24%\n",
      "Batch 87, Loss: 0.839003, Accuracy: 88.27%\n",
      "Batch 88, Loss: 0.929349, Accuracy: 88.17%\n",
      "Batch 89, Loss: 0.808019, Accuracy: 88.24%\n",
      "Batch 90, Loss: 0.924517, Accuracy: 88.18%\n",
      "Batch 91, Loss: 0.830925, Accuracy: 88.20%\n",
      "Batch 92, Loss: 0.846102, Accuracy: 88.21%\n",
      "Batch 93, Loss: 0.832571, Accuracy: 88.24%\n",
      "Batch 94, Loss: 0.876152, Accuracy: 88.21%\n",
      "Batch 95, Loss: 0.848377, Accuracy: 88.24%\n",
      "Batch 96, Loss: 0.815199, Accuracy: 88.26%\n",
      "Batch 97, Loss: 0.866383, Accuracy: 88.26%\n",
      "Batch 98, Loss: 0.796394, Accuracy: 88.33%\n",
      "Batch 99, Loss: 0.824118, Accuracy: 88.37%\n",
      "Batch 100, Loss: 0.877034, Accuracy: 88.36%\n",
      "Batch 101, Loss: 0.820340, Accuracy: 88.38%\n",
      "Batch 102, Loss: 0.853528, Accuracy: 88.39%\n",
      "Batch 103, Loss: 0.867647, Accuracy: 88.38%\n",
      "Batch 104, Loss: 0.825029, Accuracy: 88.43%\n",
      "Batch 105, Loss: 0.804215, Accuracy: 88.48%\n",
      "Batch 106, Loss: 0.834452, Accuracy: 88.50%\n",
      "Batch 107, Loss: 0.875268, Accuracy: 88.46%\n",
      "Batch 108, Loss: 0.841315, Accuracy: 88.48%\n",
      "Batch 109, Loss: 0.814799, Accuracy: 88.52%\n",
      "Batch 110, Loss: 0.920523, Accuracy: 88.45%\n",
      "Batch 111, Loss: 0.857192, Accuracy: 88.44%\n",
      "Batch 112, Loss: 0.881530, Accuracy: 88.43%\n",
      "Batch 113, Loss: 0.876844, Accuracy: 88.43%\n",
      "Batch 114, Loss: 0.865141, Accuracy: 88.42%\n",
      "Batch 115, Loss: 0.917298, Accuracy: 88.37%\n",
      "Batch 116, Loss: 0.857694, Accuracy: 88.38%\n",
      "Batch 117, Loss: 0.831642, Accuracy: 88.41%\n",
      "Batch 118, Loss: 0.847008, Accuracy: 88.41%\n",
      "Batch 119, Loss: 0.820825, Accuracy: 88.45%\n",
      "Batch 120, Loss: 0.827096, Accuracy: 88.48%\n",
      "Batch 121, Loss: 0.876346, Accuracy: 88.47%\n",
      "Batch 122, Loss: 0.814820, Accuracy: 88.51%\n",
      "Batch 123, Loss: 0.902332, Accuracy: 88.48%\n",
      "Batch 124, Loss: 0.825519, Accuracy: 88.51%\n",
      "Batch 125, Loss: 0.841230, Accuracy: 88.51%\n",
      "Batch 126, Loss: 0.892187, Accuracy: 88.47%\n",
      "Batch 127, Loss: 0.872792, Accuracy: 88.47%\n",
      "Batch 128, Loss: 0.873380, Accuracy: 88.46%\n",
      "Batch 129, Loss: 0.911832, Accuracy: 88.42%\n",
      "Batch 130, Loss: 0.918432, Accuracy: 88.38%\n",
      "Batch 131, Loss: 0.957627, Accuracy: 88.29%\n",
      "Batch 132, Loss: 0.823199, Accuracy: 88.30%\n",
      "Batch 133, Loss: 0.894612, Accuracy: 88.28%\n",
      "Batch 134, Loss: 0.850327, Accuracy: 88.28%\n",
      "Batch 135, Loss: 0.881424, Accuracy: 88.26%\n",
      "Batch 136, Loss: 0.802463, Accuracy: 88.32%\n",
      "Batch 137, Loss: 0.873178, Accuracy: 88.30%\n",
      "Batch 138, Loss: 0.883582, Accuracy: 88.28%\n",
      "Batch 139, Loss: 0.889054, Accuracy: 88.25%\n",
      "Batch 140, Loss: 0.835662, Accuracy: 88.27%\n",
      "Batch 141, Loss: 0.861117, Accuracy: 88.28%\n",
      "Batch 142, Loss: 0.909546, Accuracy: 88.24%\n",
      "Batch 143, Loss: 0.854949, Accuracy: 88.25%\n",
      "Batch 144, Loss: 0.843109, Accuracy: 88.27%\n",
      "Batch 145, Loss: 0.842971, Accuracy: 88.30%\n",
      "Batch 146, Loss: 0.863911, Accuracy: 88.29%\n",
      "Batch 147, Loss: 0.839475, Accuracy: 88.31%\n",
      "Batch 148, Loss: 0.847050, Accuracy: 88.32%\n",
      "Batch 149, Loss: 0.858957, Accuracy: 88.33%\n",
      "Batch 150, Loss: 0.863095, Accuracy: 88.32%\n",
      "Batch 151, Loss: 0.824788, Accuracy: 88.35%\n",
      "Batch 152, Loss: 0.838501, Accuracy: 88.37%\n",
      "Batch 153, Loss: 0.826822, Accuracy: 88.40%\n",
      "Batch 154, Loss: 0.886924, Accuracy: 88.38%\n",
      "Batch 155, Loss: 0.900823, Accuracy: 88.36%\n",
      "Batch 156, Loss: 0.906714, Accuracy: 88.33%\n",
      "Batch 157, Loss: 0.846590, Accuracy: 88.36%\n",
      "Batch 158, Loss: 0.811476, Accuracy: 88.39%\n",
      "Batch 159, Loss: 0.892764, Accuracy: 88.36%\n",
      "Batch 160, Loss: 0.849412, Accuracy: 88.38%\n",
      "Batch 161, Loss: 0.868658, Accuracy: 88.37%\n",
      "Batch 162, Loss: 0.882077, Accuracy: 88.36%\n",
      "Batch 163, Loss: 0.813063, Accuracy: 88.39%\n",
      "Batch 164, Loss: 0.840747, Accuracy: 88.41%\n",
      "Batch 165, Loss: 0.926123, Accuracy: 88.37%\n",
      "Batch 166, Loss: 0.857909, Accuracy: 88.38%\n",
      "Batch 167, Loss: 0.857115, Accuracy: 88.38%\n",
      "Batch 168, Loss: 0.900590, Accuracy: 88.36%\n",
      "Batch 169, Loss: 0.795644, Accuracy: 88.41%\n",
      "Batch 170, Loss: 0.829641, Accuracy: 88.44%\n",
      "Batch 171, Loss: 0.835110, Accuracy: 88.45%\n",
      "Batch 172, Loss: 0.905408, Accuracy: 88.42%\n",
      "Batch 173, Loss: 0.830449, Accuracy: 88.44%\n",
      "Batch 174, Loss: 0.842661, Accuracy: 88.44%\n",
      "Batch 175, Loss: 0.938546, Accuracy: 88.39%\n",
      "Batch 176, Loss: 0.839766, Accuracy: 88.41%\n",
      "Batch 177, Loss: 0.797286, Accuracy: 88.44%\n",
      "Batch 178, Loss: 0.826624, Accuracy: 88.47%\n",
      "Batch 179, Loss: 0.900057, Accuracy: 88.45%\n",
      "Batch 180, Loss: 0.848603, Accuracy: 88.45%\n",
      "Batch 181, Loss: 0.882572, Accuracy: 88.44%\n",
      "Batch 182, Loss: 0.860287, Accuracy: 88.44%\n",
      "Batch 183, Loss: 0.867002, Accuracy: 88.45%\n",
      "Batch 184, Loss: 0.897801, Accuracy: 88.43%\n",
      "Batch 185, Loss: 0.820026, Accuracy: 88.45%\n",
      "Batch 186, Loss: 0.895815, Accuracy: 88.43%\n",
      "Batch 187, Loss: 0.863303, Accuracy: 88.44%\n",
      "Batch 188, Loss: 0.905613, Accuracy: 88.40%\n",
      "Batch 189, Loss: 0.881183, Accuracy: 88.38%\n",
      "Batch 190, Loss: 0.883157, Accuracy: 88.36%\n",
      "Batch 191, Loss: 0.856100, Accuracy: 88.36%\n",
      "Batch 192, Loss: 0.889778, Accuracy: 88.34%\n",
      "Batch 193, Loss: 0.855221, Accuracy: 88.34%\n",
      "Batch 194, Loss: 0.891409, Accuracy: 88.33%\n",
      "Batch 195, Loss: 0.826014, Accuracy: 88.34%\n",
      "Batch 196, Loss: 0.916223, Accuracy: 88.31%\n",
      "Batch 197, Loss: 0.828897, Accuracy: 88.33%\n",
      "Batch 198, Loss: 0.861543, Accuracy: 88.34%\n",
      "Batch 199, Loss: 0.887371, Accuracy: 88.33%\n",
      "Batch 200, Loss: 0.926805, Accuracy: 88.30%\n",
      "Batch 201, Loss: 0.820106, Accuracy: 88.32%\n",
      "Batch 202, Loss: 0.818584, Accuracy: 88.35%\n",
      "Batch 203, Loss: 0.847693, Accuracy: 88.35%\n",
      "Batch 204, Loss: 0.859109, Accuracy: 88.34%\n",
      "Batch 205, Loss: 0.874879, Accuracy: 88.34%\n",
      "Batch 206, Loss: 0.870917, Accuracy: 88.33%\n",
      "Batch 207, Loss: 0.812427, Accuracy: 88.35%\n",
      "Batch 208, Loss: 0.825536, Accuracy: 88.38%\n",
      "Batch 209, Loss: 0.846720, Accuracy: 88.39%\n",
      "Batch 210, Loss: 0.816113, Accuracy: 88.42%\n",
      "Batch 211, Loss: 0.902144, Accuracy: 88.39%\n",
      "Batch 212, Loss: 0.870179, Accuracy: 88.38%\n",
      "Batch 213, Loss: 0.868926, Accuracy: 88.37%\n",
      "Training - Epoch 108, Loss: 0.860726, Accuracy: 88.37%\n",
      "Validation Batch 1, Loss: 0.816821, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.779966, Accuracy: 95.31%\n",
      "Validation Batch 3, Loss: 0.830160, Accuracy: 94.27%\n",
      "Validation Batch 4, Loss: 0.836153, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.827591, Accuracy: 92.81%\n",
      "Validation Batch 6, Loss: 0.788029, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.798406, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.875425, Accuracy: 92.58%\n",
      "Validation Batch 9, Loss: 0.886175, Accuracy: 91.84%\n",
      "Validation Batch 10, Loss: 0.802945, Accuracy: 92.19%\n",
      "Validation Batch 11, Loss: 0.828769, Accuracy: 92.05%\n",
      "Validation Batch 12, Loss: 0.827070, Accuracy: 92.06%\n",
      "Validation Batch 13, Loss: 0.832649, Accuracy: 92.07%\n",
      "Validation Batch 14, Loss: 0.836705, Accuracy: 91.96%\n",
      "Validation Batch 15, Loss: 0.817962, Accuracy: 91.98%\n",
      "Validation Batch 16, Loss: 0.828068, Accuracy: 91.99%\n",
      "Validation Batch 17, Loss: 0.867777, Accuracy: 91.73%\n",
      "Validation Batch 18, Loss: 0.800727, Accuracy: 91.93%\n",
      "Validation Batch 19, Loss: 0.877134, Accuracy: 91.61%\n",
      "Validation Batch 20, Loss: 0.846413, Accuracy: 91.48%\n",
      "Validation Batch 21, Loss: 0.862437, Accuracy: 91.37%\n",
      "Validation Batch 22, Loss: 0.824675, Accuracy: 91.48%\n",
      "Validation Batch 23, Loss: 0.864362, Accuracy: 91.30%\n",
      "Validation Batch 24, Loss: 0.836393, Accuracy: 91.28%\n",
      "Validation Batch 25, Loss: 0.806175, Accuracy: 91.38%\n",
      "Validation Batch 26, Loss: 0.841043, Accuracy: 91.41%\n",
      "Validation Batch 27, Loss: 0.792845, Accuracy: 91.49%\n",
      "Validation - Epoch 108, Loss: 0.830847, Accuracy: 91.49%\n",
      "Patience—4\n",
      "Epoch 109\n",
      "Batch 1, Loss: 0.837695, Accuracy: 87.50%\n",
      "Batch 2, Loss: 0.822968, Accuracy: 90.62%\n",
      "Batch 3, Loss: 0.838436, Accuracy: 91.15%\n",
      "Batch 4, Loss: 0.878873, Accuracy: 89.84%\n",
      "Batch 5, Loss: 0.892166, Accuracy: 88.75%\n",
      "Batch 6, Loss: 0.887127, Accuracy: 88.54%\n",
      "Batch 7, Loss: 0.875940, Accuracy: 88.39%\n",
      "Batch 8, Loss: 0.848230, Accuracy: 88.67%\n",
      "Batch 9, Loss: 0.886479, Accuracy: 88.37%\n",
      "Batch 10, Loss: 0.844648, Accuracy: 88.44%\n",
      "Batch 11, Loss: 0.845779, Accuracy: 88.49%\n",
      "Batch 12, Loss: 0.830747, Accuracy: 88.80%\n",
      "Batch 13, Loss: 0.852527, Accuracy: 88.94%\n",
      "Batch 14, Loss: 0.827105, Accuracy: 89.17%\n",
      "Batch 15, Loss: 0.815038, Accuracy: 89.48%\n",
      "Batch 16, Loss: 0.864972, Accuracy: 89.26%\n",
      "Batch 17, Loss: 0.819576, Accuracy: 89.43%\n",
      "Batch 18, Loss: 0.877513, Accuracy: 89.24%\n",
      "Batch 19, Loss: 0.847386, Accuracy: 89.23%\n",
      "Batch 20, Loss: 0.830024, Accuracy: 89.30%\n",
      "Batch 21, Loss: 0.851735, Accuracy: 89.21%\n",
      "Batch 22, Loss: 0.865653, Accuracy: 89.20%\n",
      "Batch 23, Loss: 0.914822, Accuracy: 88.86%\n",
      "Batch 24, Loss: 0.849748, Accuracy: 88.87%\n",
      "Batch 25, Loss: 0.824255, Accuracy: 89.00%\n",
      "Batch 26, Loss: 0.881131, Accuracy: 88.94%\n",
      "Batch 27, Loss: 0.858671, Accuracy: 88.95%\n",
      "Batch 28, Loss: 0.844779, Accuracy: 89.01%\n",
      "Batch 29, Loss: 0.917649, Accuracy: 88.74%\n",
      "Batch 30, Loss: 0.822397, Accuracy: 88.85%\n",
      "Batch 31, Loss: 0.886727, Accuracy: 88.76%\n",
      "Batch 32, Loss: 0.864201, Accuracy: 88.72%\n",
      "Batch 33, Loss: 0.818312, Accuracy: 88.83%\n",
      "Batch 34, Loss: 0.843404, Accuracy: 88.92%\n",
      "Batch 35, Loss: 0.840958, Accuracy: 88.93%\n",
      "Batch 36, Loss: 0.913712, Accuracy: 88.76%\n",
      "Batch 37, Loss: 0.891463, Accuracy: 88.68%\n",
      "Batch 38, Loss: 0.927568, Accuracy: 88.49%\n",
      "Batch 39, Loss: 0.785947, Accuracy: 88.66%\n",
      "Batch 40, Loss: 0.777537, Accuracy: 88.87%\n",
      "Batch 41, Loss: 0.892342, Accuracy: 88.76%\n",
      "Batch 42, Loss: 0.864145, Accuracy: 88.76%\n",
      "Batch 43, Loss: 0.840215, Accuracy: 88.77%\n",
      "Batch 44, Loss: 0.876488, Accuracy: 88.71%\n",
      "Batch 45, Loss: 0.802151, Accuracy: 88.85%\n",
      "Batch 46, Loss: 0.901911, Accuracy: 88.72%\n",
      "Batch 47, Loss: 0.820561, Accuracy: 88.80%\n",
      "Batch 48, Loss: 0.797886, Accuracy: 88.90%\n",
      "Batch 49, Loss: 0.889874, Accuracy: 88.81%\n",
      "Batch 50, Loss: 0.862443, Accuracy: 88.75%\n",
      "Batch 51, Loss: 0.951800, Accuracy: 88.54%\n",
      "Batch 52, Loss: 0.797931, Accuracy: 88.67%\n",
      "Batch 53, Loss: 0.872464, Accuracy: 88.68%\n",
      "Batch 54, Loss: 0.912379, Accuracy: 88.54%\n",
      "Batch 55, Loss: 0.795703, Accuracy: 88.69%\n",
      "Batch 56, Loss: 0.891101, Accuracy: 88.62%\n",
      "Batch 57, Loss: 0.874184, Accuracy: 88.57%\n",
      "Batch 58, Loss: 0.857792, Accuracy: 88.58%\n",
      "Batch 59, Loss: 0.857768, Accuracy: 88.56%\n",
      "Batch 60, Loss: 0.864983, Accuracy: 88.54%\n",
      "Batch 61, Loss: 0.810652, Accuracy: 88.63%\n",
      "Batch 62, Loss: 0.843372, Accuracy: 88.68%\n",
      "Batch 63, Loss: 0.882451, Accuracy: 88.67%\n",
      "Batch 64, Loss: 0.847512, Accuracy: 88.67%\n",
      "Batch 65, Loss: 0.839380, Accuracy: 88.70%\n",
      "Batch 66, Loss: 0.838956, Accuracy: 88.73%\n",
      "Batch 67, Loss: 0.809975, Accuracy: 88.81%\n",
      "Batch 68, Loss: 0.844152, Accuracy: 88.81%\n",
      "Batch 69, Loss: 0.892386, Accuracy: 88.77%\n",
      "Batch 70, Loss: 0.846703, Accuracy: 88.79%\n",
      "Batch 71, Loss: 0.821572, Accuracy: 88.86%\n",
      "Batch 72, Loss: 0.831494, Accuracy: 88.89%\n",
      "Batch 73, Loss: 0.863095, Accuracy: 88.89%\n",
      "Batch 74, Loss: 0.843886, Accuracy: 88.91%\n",
      "Batch 75, Loss: 0.836390, Accuracy: 88.96%\n",
      "Batch 76, Loss: 0.859566, Accuracy: 88.96%\n",
      "Batch 77, Loss: 0.885003, Accuracy: 88.92%\n",
      "Batch 78, Loss: 0.847092, Accuracy: 88.90%\n",
      "Batch 79, Loss: 0.891087, Accuracy: 88.84%\n",
      "Batch 80, Loss: 0.809483, Accuracy: 88.91%\n",
      "Batch 81, Loss: 0.811175, Accuracy: 88.97%\n",
      "Batch 82, Loss: 0.840645, Accuracy: 88.99%\n",
      "Batch 83, Loss: 0.880308, Accuracy: 88.95%\n",
      "Batch 84, Loss: 0.881256, Accuracy: 88.91%\n",
      "Batch 85, Loss: 0.848131, Accuracy: 88.93%\n",
      "Batch 86, Loss: 0.850335, Accuracy: 88.95%\n",
      "Batch 87, Loss: 0.865622, Accuracy: 88.94%\n",
      "Batch 88, Loss: 0.878465, Accuracy: 88.92%\n",
      "Batch 89, Loss: 0.852218, Accuracy: 88.92%\n",
      "Batch 90, Loss: 0.906736, Accuracy: 88.87%\n",
      "Batch 91, Loss: 0.870474, Accuracy: 88.86%\n",
      "Batch 92, Loss: 0.960235, Accuracy: 88.76%\n",
      "Batch 93, Loss: 0.837799, Accuracy: 88.78%\n",
      "Batch 94, Loss: 0.885305, Accuracy: 88.73%\n",
      "Batch 95, Loss: 0.867213, Accuracy: 88.72%\n",
      "Batch 96, Loss: 0.872505, Accuracy: 88.69%\n",
      "Batch 97, Loss: 0.845718, Accuracy: 88.69%\n",
      "Batch 98, Loss: 0.953346, Accuracy: 88.57%\n",
      "Batch 99, Loss: 0.913450, Accuracy: 88.51%\n",
      "Batch 100, Loss: 0.868859, Accuracy: 88.50%\n",
      "Batch 101, Loss: 0.827989, Accuracy: 88.54%\n",
      "Batch 102, Loss: 0.852814, Accuracy: 88.54%\n",
      "Batch 103, Loss: 0.858690, Accuracy: 88.55%\n",
      "Batch 104, Loss: 0.896950, Accuracy: 88.52%\n",
      "Batch 105, Loss: 0.832268, Accuracy: 88.54%\n",
      "Batch 106, Loss: 0.860272, Accuracy: 88.55%\n",
      "Batch 107, Loss: 0.856117, Accuracy: 88.55%\n",
      "Batch 108, Loss: 0.870328, Accuracy: 88.53%\n",
      "Batch 109, Loss: 0.885418, Accuracy: 88.50%\n",
      "Batch 110, Loss: 0.813331, Accuracy: 88.55%\n",
      "Batch 111, Loss: 0.833205, Accuracy: 88.57%\n",
      "Batch 112, Loss: 0.826480, Accuracy: 88.59%\n",
      "Batch 113, Loss: 0.872949, Accuracy: 88.56%\n",
      "Batch 114, Loss: 0.881806, Accuracy: 88.54%\n",
      "Batch 115, Loss: 0.802826, Accuracy: 88.59%\n",
      "Batch 116, Loss: 0.841586, Accuracy: 88.62%\n",
      "Batch 117, Loss: 0.810648, Accuracy: 88.66%\n",
      "Batch 118, Loss: 0.799042, Accuracy: 88.72%\n",
      "Batch 119, Loss: 0.880765, Accuracy: 88.69%\n",
      "Batch 120, Loss: 0.888894, Accuracy: 88.67%\n",
      "Batch 121, Loss: 0.842776, Accuracy: 88.69%\n",
      "Batch 122, Loss: 0.811730, Accuracy: 88.73%\n",
      "Batch 123, Loss: 0.817916, Accuracy: 88.76%\n",
      "Batch 124, Loss: 0.836331, Accuracy: 88.77%\n",
      "Batch 125, Loss: 0.848507, Accuracy: 88.79%\n",
      "Batch 126, Loss: 0.875154, Accuracy: 88.79%\n",
      "Batch 127, Loss: 0.874942, Accuracy: 88.75%\n",
      "Batch 128, Loss: 0.858206, Accuracy: 88.77%\n",
      "Batch 129, Loss: 0.910341, Accuracy: 88.74%\n",
      "Batch 130, Loss: 0.831195, Accuracy: 88.75%\n",
      "Batch 131, Loss: 0.968078, Accuracy: 88.67%\n",
      "Batch 132, Loss: 0.825747, Accuracy: 88.70%\n",
      "Batch 133, Loss: 0.862188, Accuracy: 88.67%\n",
      "Batch 134, Loss: 0.886492, Accuracy: 88.64%\n",
      "Batch 135, Loss: 0.956344, Accuracy: 88.58%\n",
      "Batch 136, Loss: 0.888384, Accuracy: 88.56%\n",
      "Batch 137, Loss: 0.863659, Accuracy: 88.55%\n",
      "Batch 138, Loss: 0.833977, Accuracy: 88.56%\n",
      "Batch 139, Loss: 0.820032, Accuracy: 88.60%\n",
      "Batch 140, Loss: 0.873305, Accuracy: 88.59%\n",
      "Batch 141, Loss: 0.860755, Accuracy: 88.59%\n",
      "Batch 142, Loss: 0.839767, Accuracy: 88.61%\n",
      "Batch 143, Loss: 0.806278, Accuracy: 88.66%\n",
      "Batch 144, Loss: 0.811145, Accuracy: 88.68%\n",
      "Batch 145, Loss: 0.896207, Accuracy: 88.65%\n",
      "Batch 146, Loss: 0.860996, Accuracy: 88.65%\n",
      "Batch 147, Loss: 0.822283, Accuracy: 88.67%\n",
      "Batch 148, Loss: 0.815884, Accuracy: 88.70%\n",
      "Batch 149, Loss: 0.833316, Accuracy: 88.73%\n",
      "Batch 150, Loss: 0.833856, Accuracy: 88.75%\n",
      "Batch 151, Loss: 0.798334, Accuracy: 88.77%\n",
      "Batch 152, Loss: 0.879508, Accuracy: 88.75%\n",
      "Batch 153, Loss: 0.830172, Accuracy: 88.78%\n",
      "Batch 154, Loss: 0.869232, Accuracy: 88.77%\n",
      "Batch 155, Loss: 0.890414, Accuracy: 88.75%\n",
      "Batch 156, Loss: 0.868031, Accuracy: 88.74%\n",
      "Batch 157, Loss: 0.877346, Accuracy: 88.73%\n",
      "Batch 158, Loss: 0.923831, Accuracy: 88.69%\n",
      "Batch 159, Loss: 0.788197, Accuracy: 88.73%\n",
      "Batch 160, Loss: 0.917065, Accuracy: 88.68%\n",
      "Batch 161, Loss: 0.828348, Accuracy: 88.70%\n",
      "Batch 162, Loss: 0.908356, Accuracy: 88.68%\n",
      "Batch 163, Loss: 0.902894, Accuracy: 88.65%\n",
      "Batch 164, Loss: 0.893256, Accuracy: 88.62%\n",
      "Batch 165, Loss: 0.805204, Accuracy: 88.66%\n",
      "Batch 166, Loss: 0.897410, Accuracy: 88.64%\n",
      "Batch 167, Loss: 0.821129, Accuracy: 88.66%\n",
      "Batch 168, Loss: 0.910012, Accuracy: 88.63%\n",
      "Batch 169, Loss: 0.859278, Accuracy: 88.63%\n",
      "Batch 170, Loss: 0.874767, Accuracy: 88.62%\n",
      "Batch 171, Loss: 0.914855, Accuracy: 88.59%\n",
      "Batch 172, Loss: 0.925160, Accuracy: 88.55%\n",
      "Batch 173, Loss: 0.857337, Accuracy: 88.55%\n",
      "Batch 174, Loss: 0.839534, Accuracy: 88.57%\n",
      "Batch 175, Loss: 0.938612, Accuracy: 88.53%\n",
      "Batch 176, Loss: 0.866300, Accuracy: 88.53%\n",
      "Batch 177, Loss: 0.942336, Accuracy: 88.48%\n",
      "Batch 178, Loss: 0.859956, Accuracy: 88.48%\n",
      "Batch 179, Loss: 0.880557, Accuracy: 88.48%\n",
      "Batch 180, Loss: 0.821451, Accuracy: 88.50%\n",
      "Batch 181, Loss: 0.839394, Accuracy: 88.51%\n",
      "Batch 182, Loss: 0.788491, Accuracy: 88.55%\n",
      "Batch 183, Loss: 0.865749, Accuracy: 88.53%\n",
      "Batch 184, Loss: 0.850936, Accuracy: 88.54%\n",
      "Batch 185, Loss: 0.811344, Accuracy: 88.57%\n",
      "Batch 186, Loss: 0.919567, Accuracy: 88.55%\n",
      "Batch 187, Loss: 0.891091, Accuracy: 88.54%\n",
      "Batch 188, Loss: 0.865371, Accuracy: 88.53%\n",
      "Batch 189, Loss: 0.859712, Accuracy: 88.53%\n",
      "Batch 190, Loss: 0.798300, Accuracy: 88.55%\n",
      "Batch 191, Loss: 0.828772, Accuracy: 88.57%\n",
      "Batch 192, Loss: 0.869184, Accuracy: 88.57%\n",
      "Batch 193, Loss: 0.860325, Accuracy: 88.57%\n",
      "Batch 194, Loss: 0.866288, Accuracy: 88.56%\n",
      "Batch 195, Loss: 0.797943, Accuracy: 88.60%\n",
      "Batch 196, Loss: 0.883230, Accuracy: 88.58%\n",
      "Batch 197, Loss: 0.831746, Accuracy: 88.59%\n",
      "Batch 198, Loss: 0.860769, Accuracy: 88.60%\n",
      "Batch 199, Loss: 0.854308, Accuracy: 88.60%\n",
      "Batch 200, Loss: 0.868416, Accuracy: 88.58%\n",
      "Batch 201, Loss: 0.886828, Accuracy: 88.56%\n",
      "Batch 202, Loss: 0.911494, Accuracy: 88.54%\n",
      "Batch 203, Loss: 0.882301, Accuracy: 88.52%\n",
      "Batch 204, Loss: 0.853321, Accuracy: 88.53%\n",
      "Batch 205, Loss: 0.799642, Accuracy: 88.56%\n",
      "Batch 206, Loss: 0.860623, Accuracy: 88.56%\n",
      "Batch 207, Loss: 0.826034, Accuracy: 88.58%\n",
      "Batch 208, Loss: 0.826233, Accuracy: 88.59%\n",
      "Batch 209, Loss: 0.868756, Accuracy: 88.58%\n",
      "Batch 210, Loss: 0.815311, Accuracy: 88.60%\n",
      "Batch 211, Loss: 0.906102, Accuracy: 88.57%\n",
      "Batch 212, Loss: 0.849352, Accuracy: 88.58%\n",
      "Batch 213, Loss: 0.867022, Accuracy: 88.58%\n",
      "Training - Epoch 109, Loss: 0.858559, Accuracy: 88.58%\n",
      "Validation Batch 1, Loss: 0.812095, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.783210, Accuracy: 95.31%\n",
      "Validation Batch 3, Loss: 0.827448, Accuracy: 94.27%\n",
      "Validation Batch 4, Loss: 0.828714, Accuracy: 93.75%\n",
      "Validation Batch 5, Loss: 0.816267, Accuracy: 93.75%\n",
      "Validation Batch 6, Loss: 0.786758, Accuracy: 94.27%\n",
      "Validation Batch 7, Loss: 0.794757, Accuracy: 94.42%\n",
      "Validation Batch 8, Loss: 0.860836, Accuracy: 93.75%\n",
      "Validation Batch 9, Loss: 0.882730, Accuracy: 93.06%\n",
      "Validation Batch 10, Loss: 0.803875, Accuracy: 93.12%\n",
      "Validation Batch 11, Loss: 0.822677, Accuracy: 93.18%\n",
      "Validation Batch 12, Loss: 0.826826, Accuracy: 93.10%\n",
      "Validation Batch 13, Loss: 0.827857, Accuracy: 93.15%\n",
      "Validation Batch 14, Loss: 0.837140, Accuracy: 92.97%\n",
      "Validation Batch 15, Loss: 0.813774, Accuracy: 93.02%\n",
      "Validation Batch 16, Loss: 0.821214, Accuracy: 93.07%\n",
      "Validation Batch 17, Loss: 0.865455, Accuracy: 92.74%\n",
      "Validation Batch 18, Loss: 0.798618, Accuracy: 92.80%\n",
      "Validation Batch 19, Loss: 0.872921, Accuracy: 92.43%\n",
      "Validation Batch 20, Loss: 0.821815, Accuracy: 92.50%\n",
      "Validation Batch 21, Loss: 0.859430, Accuracy: 92.34%\n",
      "Validation Batch 22, Loss: 0.815707, Accuracy: 92.33%\n",
      "Validation Batch 23, Loss: 0.858123, Accuracy: 92.12%\n",
      "Validation Batch 24, Loss: 0.822946, Accuracy: 92.12%\n",
      "Validation Batch 25, Loss: 0.800160, Accuracy: 92.25%\n",
      "Validation Batch 26, Loss: 0.840578, Accuracy: 92.25%\n",
      "Validation Batch 27, Loss: 0.788565, Accuracy: 92.37%\n",
      "Validation - Epoch 109, Loss: 0.825574, Accuracy: 92.37%\n",
      "Patience—0\n",
      "Epoch 110\n",
      "Batch 1, Loss: 0.828796, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.866200, Accuracy: 89.06%\n",
      "Batch 3, Loss: 0.856051, Accuracy: 89.06%\n",
      "Batch 4, Loss: 0.852897, Accuracy: 89.06%\n",
      "Batch 5, Loss: 0.892549, Accuracy: 88.12%\n",
      "Batch 6, Loss: 0.892166, Accuracy: 87.50%\n",
      "Batch 7, Loss: 0.929491, Accuracy: 86.38%\n",
      "Batch 8, Loss: 0.917736, Accuracy: 85.74%\n",
      "Batch 9, Loss: 0.863701, Accuracy: 86.11%\n",
      "Batch 10, Loss: 0.878776, Accuracy: 85.94%\n",
      "Batch 11, Loss: 0.829762, Accuracy: 86.65%\n",
      "Batch 12, Loss: 0.886138, Accuracy: 86.59%\n",
      "Batch 13, Loss: 0.909102, Accuracy: 86.42%\n",
      "Batch 14, Loss: 0.861792, Accuracy: 86.61%\n",
      "Batch 15, Loss: 0.816636, Accuracy: 86.98%\n",
      "Batch 16, Loss: 0.908494, Accuracy: 86.82%\n",
      "Batch 17, Loss: 0.879640, Accuracy: 86.86%\n",
      "Batch 18, Loss: 0.843035, Accuracy: 87.15%\n",
      "Batch 19, Loss: 0.890675, Accuracy: 87.09%\n",
      "Batch 20, Loss: 0.827740, Accuracy: 87.27%\n",
      "Batch 21, Loss: 0.872860, Accuracy: 87.20%\n",
      "Batch 22, Loss: 0.888755, Accuracy: 87.14%\n",
      "Batch 23, Loss: 0.919763, Accuracy: 86.96%\n",
      "Batch 24, Loss: 0.817250, Accuracy: 87.17%\n",
      "Batch 25, Loss: 0.829975, Accuracy: 87.31%\n",
      "Batch 26, Loss: 0.859755, Accuracy: 87.32%\n",
      "Batch 27, Loss: 0.869464, Accuracy: 87.33%\n",
      "Batch 28, Loss: 0.785638, Accuracy: 87.67%\n",
      "Batch 29, Loss: 0.870418, Accuracy: 87.66%\n",
      "Batch 30, Loss: 0.821333, Accuracy: 87.76%\n",
      "Batch 31, Loss: 0.868048, Accuracy: 87.75%\n",
      "Batch 32, Loss: 0.896538, Accuracy: 87.65%\n",
      "Batch 33, Loss: 0.846106, Accuracy: 87.69%\n",
      "Batch 34, Loss: 0.831940, Accuracy: 87.78%\n",
      "Batch 35, Loss: 0.874517, Accuracy: 87.77%\n",
      "Batch 36, Loss: 0.829532, Accuracy: 87.89%\n",
      "Batch 37, Loss: 0.813803, Accuracy: 88.01%\n",
      "Batch 38, Loss: 0.810131, Accuracy: 88.16%\n",
      "Batch 39, Loss: 0.819501, Accuracy: 88.30%\n",
      "Batch 40, Loss: 0.933816, Accuracy: 88.16%\n",
      "Batch 41, Loss: 0.884309, Accuracy: 88.11%\n",
      "Batch 42, Loss: 0.876984, Accuracy: 88.06%\n",
      "Batch 43, Loss: 0.844989, Accuracy: 88.12%\n",
      "Batch 44, Loss: 0.815895, Accuracy: 88.21%\n",
      "Batch 45, Loss: 0.910149, Accuracy: 88.12%\n",
      "Batch 46, Loss: 0.875676, Accuracy: 88.11%\n",
      "Batch 47, Loss: 0.800551, Accuracy: 88.26%\n",
      "Batch 48, Loss: 0.824305, Accuracy: 88.35%\n",
      "Batch 49, Loss: 0.867770, Accuracy: 88.33%\n",
      "Batch 50, Loss: 0.856535, Accuracy: 88.34%\n",
      "Batch 51, Loss: 0.822850, Accuracy: 88.42%\n",
      "Batch 52, Loss: 0.873939, Accuracy: 88.34%\n",
      "Batch 53, Loss: 0.823296, Accuracy: 88.44%\n",
      "Batch 54, Loss: 0.873575, Accuracy: 88.43%\n",
      "Batch 55, Loss: 0.850891, Accuracy: 88.44%\n",
      "Batch 56, Loss: 0.837625, Accuracy: 88.48%\n",
      "Batch 57, Loss: 0.895899, Accuracy: 88.40%\n",
      "Batch 58, Loss: 0.819627, Accuracy: 88.50%\n",
      "Batch 59, Loss: 0.868690, Accuracy: 88.48%\n",
      "Batch 60, Loss: 0.853655, Accuracy: 88.46%\n",
      "Batch 61, Loss: 0.833135, Accuracy: 88.50%\n",
      "Batch 62, Loss: 0.851673, Accuracy: 88.51%\n",
      "Batch 63, Loss: 0.891551, Accuracy: 88.44%\n",
      "Batch 64, Loss: 0.858147, Accuracy: 88.45%\n",
      "Batch 65, Loss: 0.862558, Accuracy: 88.46%\n",
      "Batch 66, Loss: 0.881986, Accuracy: 88.45%\n",
      "Batch 67, Loss: 0.831267, Accuracy: 88.50%\n",
      "Batch 68, Loss: 0.864189, Accuracy: 88.51%\n",
      "Batch 69, Loss: 0.878059, Accuracy: 88.47%\n",
      "Batch 70, Loss: 0.794835, Accuracy: 88.57%\n",
      "Batch 71, Loss: 0.905139, Accuracy: 88.49%\n",
      "Batch 72, Loss: 0.843029, Accuracy: 88.50%\n",
      "Batch 73, Loss: 0.806959, Accuracy: 88.57%\n",
      "Batch 74, Loss: 0.892441, Accuracy: 88.51%\n",
      "Batch 75, Loss: 0.822405, Accuracy: 88.54%\n",
      "Batch 76, Loss: 0.829644, Accuracy: 88.59%\n",
      "Batch 77, Loss: 0.881635, Accuracy: 88.56%\n",
      "Batch 78, Loss: 0.846993, Accuracy: 88.56%\n",
      "Batch 79, Loss: 0.876351, Accuracy: 88.53%\n",
      "Batch 80, Loss: 0.821259, Accuracy: 88.57%\n",
      "Batch 81, Loss: 0.854423, Accuracy: 88.58%\n",
      "Batch 82, Loss: 0.887403, Accuracy: 88.55%\n",
      "Batch 83, Loss: 0.793347, Accuracy: 88.63%\n",
      "Batch 84, Loss: 0.956438, Accuracy: 88.52%\n",
      "Batch 85, Loss: 0.930026, Accuracy: 88.44%\n",
      "Batch 86, Loss: 0.857846, Accuracy: 88.41%\n",
      "Batch 87, Loss: 0.875689, Accuracy: 88.38%\n",
      "Batch 88, Loss: 0.916925, Accuracy: 88.32%\n",
      "Batch 89, Loss: 0.810012, Accuracy: 88.38%\n",
      "Batch 90, Loss: 0.814485, Accuracy: 88.40%\n",
      "Batch 91, Loss: 0.783952, Accuracy: 88.50%\n",
      "Batch 92, Loss: 0.839609, Accuracy: 88.52%\n",
      "Batch 93, Loss: 0.878433, Accuracy: 88.51%\n",
      "Batch 94, Loss: 0.901117, Accuracy: 88.45%\n",
      "Batch 95, Loss: 0.775952, Accuracy: 88.54%\n",
      "Batch 96, Loss: 0.830912, Accuracy: 88.54%\n",
      "Batch 97, Loss: 0.825607, Accuracy: 88.58%\n",
      "Batch 98, Loss: 0.881569, Accuracy: 88.55%\n",
      "Batch 99, Loss: 0.842752, Accuracy: 88.57%\n",
      "Batch 100, Loss: 0.877708, Accuracy: 88.55%\n",
      "Batch 101, Loss: 0.884407, Accuracy: 88.51%\n",
      "Batch 102, Loss: 0.898569, Accuracy: 88.47%\n",
      "Batch 103, Loss: 0.815006, Accuracy: 88.52%\n",
      "Batch 104, Loss: 0.826584, Accuracy: 88.55%\n",
      "Batch 105, Loss: 0.830897, Accuracy: 88.59%\n",
      "Batch 106, Loss: 0.852437, Accuracy: 88.61%\n",
      "Batch 107, Loss: 0.839630, Accuracy: 88.61%\n",
      "Batch 108, Loss: 0.830973, Accuracy: 88.64%\n",
      "Batch 109, Loss: 0.841212, Accuracy: 88.66%\n",
      "Batch 110, Loss: 0.846351, Accuracy: 88.68%\n",
      "Batch 111, Loss: 0.887917, Accuracy: 88.65%\n",
      "Batch 112, Loss: 0.870832, Accuracy: 88.63%\n",
      "Batch 113, Loss: 0.889405, Accuracy: 88.61%\n",
      "Batch 114, Loss: 0.855658, Accuracy: 88.61%\n",
      "Batch 115, Loss: 0.875214, Accuracy: 88.59%\n",
      "Batch 116, Loss: 0.843593, Accuracy: 88.60%\n",
      "Batch 117, Loss: 0.829424, Accuracy: 88.64%\n",
      "Batch 118, Loss: 0.859484, Accuracy: 88.63%\n",
      "Batch 119, Loss: 0.869034, Accuracy: 88.62%\n",
      "Batch 120, Loss: 0.868195, Accuracy: 88.62%\n",
      "Batch 121, Loss: 0.813321, Accuracy: 88.66%\n",
      "Batch 122, Loss: 0.800612, Accuracy: 88.72%\n",
      "Batch 123, Loss: 0.816678, Accuracy: 88.74%\n",
      "Batch 124, Loss: 0.857414, Accuracy: 88.75%\n",
      "Batch 125, Loss: 0.846110, Accuracy: 88.75%\n",
      "Batch 126, Loss: 0.819020, Accuracy: 88.78%\n",
      "Batch 127, Loss: 0.825025, Accuracy: 88.80%\n",
      "Batch 128, Loss: 0.808529, Accuracy: 88.85%\n",
      "Batch 129, Loss: 0.898348, Accuracy: 88.82%\n",
      "Batch 130, Loss: 0.864109, Accuracy: 88.80%\n",
      "Batch 131, Loss: 0.845751, Accuracy: 88.80%\n",
      "Batch 132, Loss: 0.912909, Accuracy: 88.75%\n",
      "Batch 133, Loss: 0.820218, Accuracy: 88.78%\n",
      "Batch 134, Loss: 0.819680, Accuracy: 88.79%\n",
      "Batch 135, Loss: 0.874544, Accuracy: 88.78%\n",
      "Batch 136, Loss: 0.855133, Accuracy: 88.80%\n",
      "Batch 137, Loss: 0.855576, Accuracy: 88.80%\n",
      "Batch 138, Loss: 0.859290, Accuracy: 88.79%\n",
      "Batch 139, Loss: 0.868161, Accuracy: 88.78%\n",
      "Batch 140, Loss: 0.776306, Accuracy: 88.84%\n",
      "Batch 141, Loss: 0.903029, Accuracy: 88.81%\n",
      "Batch 142, Loss: 0.854153, Accuracy: 88.82%\n",
      "Batch 143, Loss: 0.876775, Accuracy: 88.80%\n",
      "Batch 144, Loss: 0.849998, Accuracy: 88.80%\n",
      "Batch 145, Loss: 0.840470, Accuracy: 88.81%\n",
      "Batch 146, Loss: 0.842273, Accuracy: 88.83%\n",
      "Batch 147, Loss: 0.896892, Accuracy: 88.80%\n",
      "Batch 148, Loss: 0.864655, Accuracy: 88.80%\n",
      "Batch 149, Loss: 0.818402, Accuracy: 88.84%\n",
      "Batch 150, Loss: 0.819920, Accuracy: 88.88%\n",
      "Batch 151, Loss: 0.804652, Accuracy: 88.92%\n",
      "Batch 152, Loss: 0.840817, Accuracy: 88.93%\n",
      "Batch 153, Loss: 0.960840, Accuracy: 88.86%\n",
      "Batch 154, Loss: 0.913956, Accuracy: 88.82%\n",
      "Batch 155, Loss: 0.859668, Accuracy: 88.81%\n",
      "Batch 156, Loss: 0.857891, Accuracy: 88.81%\n",
      "Batch 157, Loss: 0.869078, Accuracy: 88.79%\n",
      "Batch 158, Loss: 0.869931, Accuracy: 88.79%\n",
      "Batch 159, Loss: 0.844251, Accuracy: 88.80%\n",
      "Batch 160, Loss: 0.834795, Accuracy: 88.82%\n",
      "Batch 161, Loss: 0.845567, Accuracy: 88.83%\n",
      "Batch 162, Loss: 0.835462, Accuracy: 88.85%\n",
      "Batch 163, Loss: 0.848558, Accuracy: 88.85%\n",
      "Batch 164, Loss: 0.813779, Accuracy: 88.89%\n",
      "Batch 165, Loss: 0.898064, Accuracy: 88.86%\n",
      "Batch 166, Loss: 0.845250, Accuracy: 88.86%\n",
      "Batch 167, Loss: 0.881628, Accuracy: 88.87%\n",
      "Batch 168, Loss: 0.819780, Accuracy: 88.90%\n",
      "Batch 169, Loss: 0.869797, Accuracy: 88.88%\n",
      "Batch 170, Loss: 0.826592, Accuracy: 88.90%\n",
      "Batch 171, Loss: 0.860544, Accuracy: 88.90%\n",
      "Batch 172, Loss: 0.866251, Accuracy: 88.89%\n",
      "Batch 173, Loss: 0.946921, Accuracy: 88.84%\n",
      "Batch 174, Loss: 0.856578, Accuracy: 88.84%\n",
      "Batch 175, Loss: 0.810468, Accuracy: 88.87%\n",
      "Batch 176, Loss: 0.875931, Accuracy: 88.85%\n",
      "Batch 177, Loss: 0.818769, Accuracy: 88.87%\n",
      "Batch 178, Loss: 0.885458, Accuracy: 88.84%\n",
      "Batch 179, Loss: 0.853849, Accuracy: 88.84%\n",
      "Batch 180, Loss: 0.848219, Accuracy: 88.85%\n",
      "Batch 181, Loss: 0.900827, Accuracy: 88.82%\n",
      "Batch 182, Loss: 0.841166, Accuracy: 88.82%\n",
      "Batch 183, Loss: 0.816856, Accuracy: 88.85%\n",
      "Batch 184, Loss: 0.920512, Accuracy: 88.82%\n",
      "Batch 185, Loss: 0.903026, Accuracy: 88.80%\n",
      "Batch 186, Loss: 0.827019, Accuracy: 88.82%\n",
      "Batch 187, Loss: 0.830977, Accuracy: 88.83%\n",
      "Batch 188, Loss: 0.852703, Accuracy: 88.84%\n",
      "Batch 189, Loss: 0.823259, Accuracy: 88.86%\n",
      "Batch 190, Loss: 0.780230, Accuracy: 88.91%\n",
      "Batch 191, Loss: 0.833341, Accuracy: 88.92%\n",
      "Batch 192, Loss: 0.855841, Accuracy: 88.93%\n",
      "Batch 193, Loss: 0.836002, Accuracy: 88.95%\n",
      "Batch 194, Loss: 0.906973, Accuracy: 88.92%\n",
      "Batch 195, Loss: 0.827762, Accuracy: 88.93%\n",
      "Batch 196, Loss: 0.863943, Accuracy: 88.93%\n",
      "Batch 197, Loss: 0.843375, Accuracy: 88.94%\n",
      "Batch 198, Loss: 0.870022, Accuracy: 88.93%\n",
      "Batch 199, Loss: 0.883448, Accuracy: 88.91%\n",
      "Batch 200, Loss: 0.824946, Accuracy: 88.93%\n",
      "Batch 201, Loss: 0.919224, Accuracy: 88.90%\n",
      "Batch 202, Loss: 0.864680, Accuracy: 88.89%\n",
      "Batch 203, Loss: 0.791021, Accuracy: 88.92%\n",
      "Batch 204, Loss: 0.872167, Accuracy: 88.91%\n",
      "Batch 205, Loss: 0.882461, Accuracy: 88.90%\n",
      "Batch 206, Loss: 0.879168, Accuracy: 88.89%\n",
      "Batch 207, Loss: 0.832133, Accuracy: 88.90%\n",
      "Batch 208, Loss: 0.875377, Accuracy: 88.89%\n",
      "Batch 209, Loss: 0.859359, Accuracy: 88.88%\n",
      "Batch 210, Loss: 0.867306, Accuracy: 88.88%\n",
      "Batch 211, Loss: 0.899222, Accuracy: 88.86%\n",
      "Batch 212, Loss: 0.934146, Accuracy: 88.82%\n",
      "Batch 213, Loss: 0.852579, Accuracy: 88.81%\n",
      "Training - Epoch 110, Loss: 0.856468, Accuracy: 88.81%\n",
      "Validation Batch 1, Loss: 0.813328, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.806666, Accuracy: 93.75%\n",
      "Validation Batch 3, Loss: 0.837800, Accuracy: 92.71%\n",
      "Validation Batch 4, Loss: 0.834771, Accuracy: 92.19%\n",
      "Validation Batch 5, Loss: 0.821608, Accuracy: 92.50%\n",
      "Validation Batch 6, Loss: 0.797436, Accuracy: 92.97%\n",
      "Validation Batch 7, Loss: 0.799651, Accuracy: 93.30%\n",
      "Validation Batch 8, Loss: 0.863309, Accuracy: 92.58%\n",
      "Validation Batch 9, Loss: 0.891237, Accuracy: 91.84%\n",
      "Validation Batch 10, Loss: 0.812603, Accuracy: 92.03%\n",
      "Validation Batch 11, Loss: 0.828297, Accuracy: 92.05%\n",
      "Validation Batch 12, Loss: 0.828542, Accuracy: 92.06%\n",
      "Validation Batch 13, Loss: 0.846183, Accuracy: 91.95%\n",
      "Validation Batch 14, Loss: 0.841509, Accuracy: 91.85%\n",
      "Validation Batch 15, Loss: 0.821556, Accuracy: 91.98%\n",
      "Validation Batch 16, Loss: 0.828752, Accuracy: 92.09%\n",
      "Validation Batch 17, Loss: 0.866494, Accuracy: 91.91%\n",
      "Validation Batch 18, Loss: 0.803641, Accuracy: 92.01%\n",
      "Validation Batch 19, Loss: 0.882067, Accuracy: 91.69%\n",
      "Validation Batch 20, Loss: 0.832784, Accuracy: 91.72%\n",
      "Validation Batch 21, Loss: 0.863196, Accuracy: 91.59%\n",
      "Validation Batch 22, Loss: 0.831279, Accuracy: 91.62%\n",
      "Validation Batch 23, Loss: 0.871861, Accuracy: 91.44%\n",
      "Validation Batch 24, Loss: 0.836334, Accuracy: 91.41%\n",
      "Validation Batch 25, Loss: 0.814839, Accuracy: 91.44%\n",
      "Validation Batch 26, Loss: 0.854306, Accuracy: 91.29%\n",
      "Validation Batch 27, Loss: 0.802175, Accuracy: 91.37%\n",
      "Validation - Epoch 110, Loss: 0.834527, Accuracy: 91.37%\n",
      "Patience—1\n",
      "Epoch 111\n",
      "Batch 1, Loss: 0.902732, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.874826, Accuracy: 85.16%\n",
      "Batch 3, Loss: 0.943038, Accuracy: 82.81%\n",
      "Batch 4, Loss: 0.804541, Accuracy: 85.55%\n",
      "Batch 5, Loss: 0.882643, Accuracy: 85.62%\n",
      "Batch 6, Loss: 0.853176, Accuracy: 86.20%\n",
      "Batch 7, Loss: 0.868677, Accuracy: 86.61%\n",
      "Batch 8, Loss: 0.834553, Accuracy: 87.11%\n",
      "Batch 9, Loss: 0.951770, Accuracy: 86.28%\n",
      "Batch 10, Loss: 0.839825, Accuracy: 86.72%\n",
      "Batch 11, Loss: 0.901772, Accuracy: 86.51%\n",
      "Batch 12, Loss: 0.877361, Accuracy: 86.72%\n",
      "Batch 13, Loss: 0.950157, Accuracy: 86.06%\n",
      "Batch 14, Loss: 0.927905, Accuracy: 85.71%\n",
      "Batch 15, Loss: 0.845190, Accuracy: 86.04%\n",
      "Batch 16, Loss: 0.926377, Accuracy: 85.84%\n",
      "Batch 17, Loss: 0.884374, Accuracy: 85.75%\n",
      "Batch 18, Loss: 0.882610, Accuracy: 85.76%\n",
      "Batch 19, Loss: 0.872703, Accuracy: 85.77%\n",
      "Batch 20, Loss: 0.853737, Accuracy: 86.02%\n",
      "Batch 21, Loss: 0.825156, Accuracy: 86.31%\n",
      "Batch 22, Loss: 0.896490, Accuracy: 86.22%\n",
      "Batch 23, Loss: 0.861302, Accuracy: 86.28%\n",
      "Batch 24, Loss: 0.847380, Accuracy: 86.39%\n",
      "Batch 25, Loss: 0.857288, Accuracy: 86.50%\n",
      "Batch 26, Loss: 0.860302, Accuracy: 86.66%\n",
      "Batch 27, Loss: 0.903812, Accuracy: 86.57%\n",
      "Batch 28, Loss: 0.851833, Accuracy: 86.66%\n",
      "Batch 29, Loss: 0.861332, Accuracy: 86.75%\n",
      "Batch 30, Loss: 0.828398, Accuracy: 86.93%\n",
      "Batch 31, Loss: 0.882697, Accuracy: 86.90%\n",
      "Batch 32, Loss: 0.818870, Accuracy: 87.06%\n",
      "Batch 33, Loss: 0.822676, Accuracy: 87.22%\n",
      "Batch 34, Loss: 0.857136, Accuracy: 87.27%\n",
      "Batch 35, Loss: 0.821660, Accuracy: 87.46%\n",
      "Batch 36, Loss: 0.832078, Accuracy: 87.54%\n",
      "Batch 37, Loss: 0.928301, Accuracy: 87.33%\n",
      "Batch 38, Loss: 0.874701, Accuracy: 87.42%\n",
      "Batch 39, Loss: 0.880064, Accuracy: 87.42%\n",
      "Batch 40, Loss: 0.814490, Accuracy: 87.58%\n",
      "Batch 41, Loss: 0.843056, Accuracy: 87.58%\n",
      "Batch 42, Loss: 0.848589, Accuracy: 87.65%\n",
      "Batch 43, Loss: 0.846533, Accuracy: 87.72%\n",
      "Batch 44, Loss: 0.854902, Accuracy: 87.71%\n",
      "Batch 45, Loss: 0.824513, Accuracy: 87.81%\n",
      "Batch 46, Loss: 0.885129, Accuracy: 87.77%\n",
      "Batch 47, Loss: 0.829659, Accuracy: 87.87%\n",
      "Batch 48, Loss: 0.793859, Accuracy: 88.02%\n",
      "Batch 49, Loss: 0.810991, Accuracy: 88.17%\n",
      "Batch 50, Loss: 0.928064, Accuracy: 88.03%\n",
      "Batch 51, Loss: 0.860479, Accuracy: 88.08%\n",
      "Batch 52, Loss: 0.791426, Accuracy: 88.22%\n",
      "Batch 53, Loss: 0.934152, Accuracy: 88.09%\n",
      "Batch 54, Loss: 0.978134, Accuracy: 87.88%\n",
      "Batch 55, Loss: 0.782818, Accuracy: 88.04%\n",
      "Batch 56, Loss: 0.863075, Accuracy: 88.06%\n",
      "Batch 57, Loss: 0.829862, Accuracy: 88.10%\n",
      "Batch 58, Loss: 0.873029, Accuracy: 88.09%\n",
      "Batch 59, Loss: 0.851862, Accuracy: 88.11%\n",
      "Batch 60, Loss: 0.914731, Accuracy: 88.05%\n",
      "Batch 61, Loss: 0.924593, Accuracy: 87.96%\n",
      "Batch 62, Loss: 0.847681, Accuracy: 87.98%\n",
      "Batch 63, Loss: 0.811372, Accuracy: 88.02%\n",
      "Batch 64, Loss: 0.931218, Accuracy: 87.89%\n",
      "Batch 65, Loss: 0.873259, Accuracy: 87.88%\n",
      "Batch 66, Loss: 0.852141, Accuracy: 87.90%\n",
      "Batch 67, Loss: 0.865555, Accuracy: 87.90%\n",
      "Batch 68, Loss: 0.828761, Accuracy: 87.96%\n",
      "Batch 69, Loss: 0.878309, Accuracy: 87.91%\n",
      "Batch 70, Loss: 0.851263, Accuracy: 87.90%\n",
      "Batch 71, Loss: 0.821645, Accuracy: 87.96%\n",
      "Batch 72, Loss: 0.937208, Accuracy: 87.85%\n",
      "Batch 73, Loss: 0.825282, Accuracy: 87.91%\n",
      "Batch 74, Loss: 0.833103, Accuracy: 87.92%\n",
      "Batch 75, Loss: 0.837133, Accuracy: 87.96%\n",
      "Batch 76, Loss: 0.866246, Accuracy: 87.95%\n",
      "Batch 77, Loss: 0.826925, Accuracy: 88.01%\n",
      "Batch 78, Loss: 0.856859, Accuracy: 88.02%\n",
      "Batch 79, Loss: 0.786219, Accuracy: 88.13%\n",
      "Batch 80, Loss: 0.834652, Accuracy: 88.16%\n",
      "Batch 81, Loss: 0.910458, Accuracy: 88.10%\n",
      "Batch 82, Loss: 0.839422, Accuracy: 88.13%\n",
      "Batch 83, Loss: 0.786682, Accuracy: 88.22%\n",
      "Batch 84, Loss: 0.808945, Accuracy: 88.30%\n",
      "Batch 85, Loss: 0.848301, Accuracy: 88.31%\n",
      "Batch 86, Loss: 0.923589, Accuracy: 88.23%\n",
      "Batch 87, Loss: 0.883252, Accuracy: 88.20%\n",
      "Batch 88, Loss: 0.816380, Accuracy: 88.26%\n",
      "Batch 89, Loss: 0.863966, Accuracy: 88.24%\n",
      "Batch 90, Loss: 0.850701, Accuracy: 88.26%\n",
      "Batch 91, Loss: 0.857121, Accuracy: 88.27%\n",
      "Batch 92, Loss: 0.817644, Accuracy: 88.33%\n",
      "Batch 93, Loss: 0.897713, Accuracy: 88.27%\n",
      "Batch 94, Loss: 0.804057, Accuracy: 88.35%\n",
      "Batch 95, Loss: 0.839092, Accuracy: 88.37%\n",
      "Batch 96, Loss: 0.827635, Accuracy: 88.40%\n",
      "Batch 97, Loss: 0.843253, Accuracy: 88.40%\n",
      "Batch 98, Loss: 0.897619, Accuracy: 88.36%\n",
      "Batch 99, Loss: 0.891998, Accuracy: 88.30%\n",
      "Batch 100, Loss: 0.922905, Accuracy: 88.25%\n",
      "Batch 101, Loss: 0.845859, Accuracy: 88.27%\n",
      "Batch 102, Loss: 0.861969, Accuracy: 88.28%\n",
      "Batch 103, Loss: 0.833075, Accuracy: 88.30%\n",
      "Batch 104, Loss: 0.802285, Accuracy: 88.37%\n",
      "Batch 105, Loss: 0.854307, Accuracy: 88.38%\n",
      "Batch 106, Loss: 0.869011, Accuracy: 88.35%\n",
      "Batch 107, Loss: 0.919681, Accuracy: 88.29%\n",
      "Batch 108, Loss: 0.897608, Accuracy: 88.25%\n",
      "Batch 109, Loss: 0.837314, Accuracy: 88.27%\n",
      "Batch 110, Loss: 0.793693, Accuracy: 88.34%\n",
      "Batch 111, Loss: 0.854215, Accuracy: 88.34%\n",
      "Batch 112, Loss: 0.867334, Accuracy: 88.34%\n",
      "Batch 113, Loss: 0.855599, Accuracy: 88.33%\n",
      "Batch 114, Loss: 0.856158, Accuracy: 88.34%\n",
      "Batch 115, Loss: 0.824129, Accuracy: 88.37%\n",
      "Batch 116, Loss: 0.843486, Accuracy: 88.39%\n",
      "Batch 117, Loss: 0.880671, Accuracy: 88.37%\n",
      "Batch 118, Loss: 0.850242, Accuracy: 88.37%\n",
      "Batch 119, Loss: 0.825937, Accuracy: 88.41%\n",
      "Batch 120, Loss: 0.830050, Accuracy: 88.44%\n",
      "Batch 121, Loss: 0.829915, Accuracy: 88.47%\n",
      "Batch 122, Loss: 0.847023, Accuracy: 88.49%\n",
      "Batch 123, Loss: 0.797484, Accuracy: 88.53%\n",
      "Batch 124, Loss: 0.873057, Accuracy: 88.52%\n",
      "Batch 125, Loss: 0.890193, Accuracy: 88.49%\n",
      "Batch 126, Loss: 0.803804, Accuracy: 88.53%\n",
      "Batch 127, Loss: 0.918518, Accuracy: 88.47%\n",
      "Batch 128, Loss: 0.885167, Accuracy: 88.45%\n",
      "Batch 129, Loss: 0.834754, Accuracy: 88.47%\n",
      "Batch 130, Loss: 0.816833, Accuracy: 88.51%\n",
      "Batch 131, Loss: 0.868903, Accuracy: 88.50%\n",
      "Batch 132, Loss: 0.914076, Accuracy: 88.45%\n",
      "Batch 133, Loss: 0.839040, Accuracy: 88.46%\n",
      "Batch 134, Loss: 0.877635, Accuracy: 88.44%\n",
      "Batch 135, Loss: 0.924213, Accuracy: 88.39%\n",
      "Batch 136, Loss: 0.819633, Accuracy: 88.43%\n",
      "Batch 137, Loss: 0.821487, Accuracy: 88.46%\n",
      "Batch 138, Loss: 0.784804, Accuracy: 88.52%\n",
      "Batch 139, Loss: 0.841605, Accuracy: 88.53%\n",
      "Batch 140, Loss: 0.850505, Accuracy: 88.55%\n",
      "Batch 141, Loss: 0.863729, Accuracy: 88.55%\n",
      "Batch 142, Loss: 0.825064, Accuracy: 88.58%\n",
      "Batch 143, Loss: 0.895085, Accuracy: 88.56%\n",
      "Batch 144, Loss: 0.826482, Accuracy: 88.59%\n",
      "Batch 145, Loss: 0.805589, Accuracy: 88.63%\n",
      "Batch 146, Loss: 0.866791, Accuracy: 88.62%\n",
      "Batch 147, Loss: 0.920205, Accuracy: 88.57%\n",
      "Batch 148, Loss: 0.864063, Accuracy: 88.57%\n",
      "Batch 149, Loss: 0.878408, Accuracy: 88.55%\n",
      "Batch 150, Loss: 0.816244, Accuracy: 88.58%\n",
      "Batch 151, Loss: 0.872294, Accuracy: 88.58%\n",
      "Batch 152, Loss: 0.909446, Accuracy: 88.55%\n",
      "Batch 153, Loss: 0.857826, Accuracy: 88.55%\n",
      "Batch 154, Loss: 0.810250, Accuracy: 88.59%\n",
      "Batch 155, Loss: 0.861249, Accuracy: 88.58%\n",
      "Batch 156, Loss: 0.822490, Accuracy: 88.61%\n",
      "Batch 157, Loss: 0.883620, Accuracy: 88.58%\n",
      "Batch 158, Loss: 0.808172, Accuracy: 88.63%\n",
      "Batch 159, Loss: 0.926593, Accuracy: 88.58%\n",
      "Batch 160, Loss: 0.937048, Accuracy: 88.53%\n",
      "Batch 161, Loss: 0.872032, Accuracy: 88.51%\n",
      "Batch 162, Loss: 0.810997, Accuracy: 88.54%\n",
      "Batch 163, Loss: 0.933203, Accuracy: 88.49%\n",
      "Batch 164, Loss: 0.831508, Accuracy: 88.50%\n",
      "Batch 165, Loss: 0.821110, Accuracy: 88.53%\n",
      "Batch 166, Loss: 0.913754, Accuracy: 88.50%\n",
      "Batch 167, Loss: 0.873885, Accuracy: 88.48%\n",
      "Batch 168, Loss: 0.854282, Accuracy: 88.49%\n",
      "Batch 169, Loss: 0.834493, Accuracy: 88.51%\n",
      "Batch 170, Loss: 0.832633, Accuracy: 88.54%\n",
      "Batch 171, Loss: 0.831952, Accuracy: 88.56%\n",
      "Batch 172, Loss: 0.888096, Accuracy: 88.54%\n",
      "Batch 173, Loss: 0.858055, Accuracy: 88.56%\n",
      "Batch 174, Loss: 0.840349, Accuracy: 88.56%\n",
      "Batch 175, Loss: 0.827839, Accuracy: 88.58%\n",
      "Batch 176, Loss: 0.815007, Accuracy: 88.60%\n",
      "Batch 177, Loss: 0.801624, Accuracy: 88.63%\n",
      "Batch 178, Loss: 0.916829, Accuracy: 88.59%\n",
      "Batch 179, Loss: 0.848247, Accuracy: 88.60%\n",
      "Batch 180, Loss: 0.806677, Accuracy: 88.63%\n",
      "Batch 181, Loss: 0.860075, Accuracy: 88.61%\n",
      "Batch 182, Loss: 0.808317, Accuracy: 88.64%\n",
      "Batch 183, Loss: 0.877242, Accuracy: 88.63%\n",
      "Batch 184, Loss: 0.846388, Accuracy: 88.64%\n",
      "Batch 185, Loss: 0.866245, Accuracy: 88.65%\n",
      "Batch 186, Loss: 0.804116, Accuracy: 88.68%\n",
      "Batch 187, Loss: 0.902496, Accuracy: 88.64%\n",
      "Batch 188, Loss: 0.861942, Accuracy: 88.65%\n",
      "Batch 189, Loss: 0.843783, Accuracy: 88.65%\n",
      "Batch 190, Loss: 0.852088, Accuracy: 88.64%\n",
      "Batch 191, Loss: 0.834191, Accuracy: 88.66%\n",
      "Batch 192, Loss: 0.910096, Accuracy: 88.64%\n",
      "Batch 193, Loss: 0.872910, Accuracy: 88.63%\n",
      "Batch 194, Loss: 0.869935, Accuracy: 88.61%\n",
      "Batch 195, Loss: 0.784853, Accuracy: 88.65%\n",
      "Batch 196, Loss: 0.819830, Accuracy: 88.67%\n",
      "Batch 197, Loss: 0.844154, Accuracy: 88.68%\n",
      "Batch 198, Loss: 0.844165, Accuracy: 88.68%\n",
      "Batch 199, Loss: 0.839320, Accuracy: 88.69%\n",
      "Batch 200, Loss: 0.856628, Accuracy: 88.70%\n",
      "Batch 201, Loss: 0.884853, Accuracy: 88.68%\n",
      "Batch 202, Loss: 0.833696, Accuracy: 88.68%\n",
      "Batch 203, Loss: 0.834915, Accuracy: 88.70%\n",
      "Batch 204, Loss: 0.855702, Accuracy: 88.70%\n",
      "Batch 205, Loss: 0.830105, Accuracy: 88.72%\n",
      "Batch 206, Loss: 0.858738, Accuracy: 88.71%\n",
      "Batch 207, Loss: 0.868455, Accuracy: 88.70%\n",
      "Batch 208, Loss: 0.908100, Accuracy: 88.66%\n",
      "Batch 209, Loss: 0.778681, Accuracy: 88.70%\n",
      "Batch 210, Loss: 0.825771, Accuracy: 88.72%\n",
      "Batch 211, Loss: 0.866545, Accuracy: 88.71%\n",
      "Batch 212, Loss: 0.901248, Accuracy: 88.69%\n",
      "Batch 213, Loss: 0.846364, Accuracy: 88.69%\n",
      "Training - Epoch 111, Loss: 0.857114, Accuracy: 88.69%\n",
      "Validation Batch 1, Loss: 0.816954, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.805805, Accuracy: 93.75%\n",
      "Validation Batch 3, Loss: 0.835487, Accuracy: 93.23%\n",
      "Validation Batch 4, Loss: 0.836812, Accuracy: 92.58%\n",
      "Validation Batch 5, Loss: 0.820901, Accuracy: 92.81%\n",
      "Validation Batch 6, Loss: 0.796671, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.799887, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.864460, Accuracy: 92.77%\n",
      "Validation Batch 9, Loss: 0.895785, Accuracy: 91.84%\n",
      "Validation Batch 10, Loss: 0.808395, Accuracy: 92.03%\n",
      "Validation Batch 11, Loss: 0.831360, Accuracy: 92.05%\n",
      "Validation Batch 12, Loss: 0.829152, Accuracy: 92.06%\n",
      "Validation Batch 13, Loss: 0.846446, Accuracy: 91.83%\n",
      "Validation Batch 14, Loss: 0.844326, Accuracy: 91.74%\n",
      "Validation Batch 15, Loss: 0.812518, Accuracy: 91.77%\n",
      "Validation Batch 16, Loss: 0.831915, Accuracy: 91.70%\n",
      "Validation Batch 17, Loss: 0.868438, Accuracy: 91.45%\n",
      "Validation Batch 18, Loss: 0.807212, Accuracy: 91.67%\n",
      "Validation Batch 19, Loss: 0.874732, Accuracy: 91.45%\n",
      "Validation Batch 20, Loss: 0.841504, Accuracy: 91.33%\n",
      "Validation Batch 21, Loss: 0.865986, Accuracy: 91.22%\n",
      "Validation Batch 22, Loss: 0.830218, Accuracy: 91.19%\n",
      "Validation Batch 23, Loss: 0.869606, Accuracy: 91.03%\n",
      "Validation Batch 24, Loss: 0.835857, Accuracy: 91.02%\n",
      "Validation Batch 25, Loss: 0.813201, Accuracy: 91.06%\n",
      "Validation Batch 26, Loss: 0.849041, Accuracy: 91.05%\n",
      "Validation Batch 27, Loss: 0.801154, Accuracy: 91.13%\n",
      "Validation - Epoch 111, Loss: 0.834586, Accuracy: 91.13%\n",
      "Patience—2\n",
      "Epoch 112\n",
      "Batch 1, Loss: 0.849389, Accuracy: 87.50%\n",
      "Batch 2, Loss: 0.876301, Accuracy: 87.50%\n",
      "Batch 3, Loss: 0.849086, Accuracy: 88.02%\n",
      "Batch 4, Loss: 0.847507, Accuracy: 88.28%\n",
      "Batch 5, Loss: 0.877918, Accuracy: 88.12%\n",
      "Batch 6, Loss: 0.867713, Accuracy: 88.02%\n",
      "Batch 7, Loss: 0.863705, Accuracy: 87.95%\n",
      "Batch 8, Loss: 1.005023, Accuracy: 85.74%\n",
      "Batch 9, Loss: 0.796624, Accuracy: 86.81%\n",
      "Batch 10, Loss: 0.796063, Accuracy: 87.66%\n",
      "Batch 11, Loss: 0.883780, Accuracy: 87.50%\n",
      "Batch 12, Loss: 0.825355, Accuracy: 87.89%\n",
      "Batch 13, Loss: 0.809392, Accuracy: 88.46%\n",
      "Batch 14, Loss: 0.816496, Accuracy: 88.84%\n",
      "Batch 15, Loss: 0.863103, Accuracy: 88.65%\n",
      "Batch 16, Loss: 0.889564, Accuracy: 88.38%\n",
      "Batch 17, Loss: 0.826213, Accuracy: 88.60%\n",
      "Batch 18, Loss: 0.827189, Accuracy: 88.80%\n",
      "Batch 19, Loss: 0.841778, Accuracy: 88.82%\n",
      "Batch 20, Loss: 0.932113, Accuracy: 88.52%\n",
      "Batch 21, Loss: 0.850953, Accuracy: 88.62%\n",
      "Batch 22, Loss: 0.902198, Accuracy: 88.35%\n",
      "Batch 23, Loss: 0.854927, Accuracy: 88.38%\n",
      "Batch 24, Loss: 0.860894, Accuracy: 88.35%\n",
      "Batch 25, Loss: 0.833004, Accuracy: 88.44%\n",
      "Batch 26, Loss: 0.776266, Accuracy: 88.76%\n",
      "Batch 27, Loss: 0.792435, Accuracy: 88.95%\n",
      "Batch 28, Loss: 0.840567, Accuracy: 88.90%\n",
      "Batch 29, Loss: 0.837645, Accuracy: 88.95%\n",
      "Batch 30, Loss: 0.843668, Accuracy: 88.96%\n",
      "Batch 31, Loss: 0.845290, Accuracy: 89.01%\n",
      "Batch 32, Loss: 0.882096, Accuracy: 88.92%\n",
      "Batch 33, Loss: 0.915291, Accuracy: 88.73%\n",
      "Batch 34, Loss: 0.802938, Accuracy: 88.92%\n",
      "Batch 35, Loss: 0.892242, Accuracy: 88.79%\n",
      "Batch 36, Loss: 0.891199, Accuracy: 88.63%\n",
      "Batch 37, Loss: 0.836646, Accuracy: 88.68%\n",
      "Batch 38, Loss: 0.877254, Accuracy: 88.61%\n",
      "Batch 39, Loss: 0.850238, Accuracy: 88.62%\n",
      "Batch 40, Loss: 0.855246, Accuracy: 88.63%\n",
      "Batch 41, Loss: 0.829971, Accuracy: 88.68%\n",
      "Batch 42, Loss: 0.870153, Accuracy: 88.65%\n",
      "Batch 43, Loss: 0.806536, Accuracy: 88.77%\n",
      "Batch 44, Loss: 0.821356, Accuracy: 88.85%\n",
      "Batch 45, Loss: 0.817982, Accuracy: 88.92%\n",
      "Batch 46, Loss: 0.796113, Accuracy: 89.06%\n",
      "Batch 47, Loss: 0.921029, Accuracy: 88.96%\n",
      "Batch 48, Loss: 0.876246, Accuracy: 88.90%\n",
      "Batch 49, Loss: 0.846796, Accuracy: 88.90%\n",
      "Batch 50, Loss: 0.843466, Accuracy: 88.94%\n",
      "Batch 51, Loss: 0.874769, Accuracy: 88.91%\n",
      "Batch 52, Loss: 0.845623, Accuracy: 88.94%\n",
      "Batch 53, Loss: 0.883737, Accuracy: 88.89%\n",
      "Batch 54, Loss: 0.819773, Accuracy: 88.95%\n",
      "Batch 55, Loss: 0.945954, Accuracy: 88.75%\n",
      "Batch 56, Loss: 0.864185, Accuracy: 88.76%\n",
      "Batch 57, Loss: 0.850067, Accuracy: 88.73%\n",
      "Batch 58, Loss: 0.885989, Accuracy: 88.69%\n",
      "Batch 59, Loss: 0.878988, Accuracy: 88.64%\n",
      "Batch 60, Loss: 0.869595, Accuracy: 88.62%\n",
      "Batch 61, Loss: 0.797314, Accuracy: 88.73%\n",
      "Batch 62, Loss: 0.893691, Accuracy: 88.68%\n",
      "Batch 63, Loss: 0.877553, Accuracy: 88.67%\n",
      "Batch 64, Loss: 0.842503, Accuracy: 88.70%\n",
      "Batch 65, Loss: 0.851387, Accuracy: 88.70%\n",
      "Batch 66, Loss: 0.858065, Accuracy: 88.71%\n",
      "Batch 67, Loss: 0.792386, Accuracy: 88.83%\n",
      "Batch 68, Loss: 0.902120, Accuracy: 88.76%\n",
      "Batch 69, Loss: 0.898993, Accuracy: 88.68%\n",
      "Batch 70, Loss: 0.782715, Accuracy: 88.79%\n",
      "Batch 71, Loss: 0.871575, Accuracy: 88.78%\n",
      "Batch 72, Loss: 0.786356, Accuracy: 88.87%\n",
      "Batch 73, Loss: 0.892676, Accuracy: 88.81%\n",
      "Batch 74, Loss: 0.833918, Accuracy: 88.85%\n",
      "Batch 75, Loss: 0.838281, Accuracy: 88.85%\n",
      "Batch 76, Loss: 0.902702, Accuracy: 88.77%\n",
      "Batch 77, Loss: 0.821383, Accuracy: 88.84%\n",
      "Batch 78, Loss: 0.871493, Accuracy: 88.82%\n",
      "Batch 79, Loss: 0.861031, Accuracy: 88.83%\n",
      "Batch 80, Loss: 0.911740, Accuracy: 88.73%\n",
      "Batch 81, Loss: 0.817028, Accuracy: 88.77%\n",
      "Batch 82, Loss: 0.826365, Accuracy: 88.81%\n",
      "Batch 83, Loss: 0.899826, Accuracy: 88.76%\n",
      "Batch 84, Loss: 0.898837, Accuracy: 88.71%\n",
      "Batch 85, Loss: 0.843703, Accuracy: 88.73%\n",
      "Batch 86, Loss: 0.847655, Accuracy: 88.74%\n",
      "Batch 87, Loss: 0.847093, Accuracy: 88.74%\n",
      "Batch 88, Loss: 0.880863, Accuracy: 88.71%\n",
      "Batch 89, Loss: 0.851835, Accuracy: 88.71%\n",
      "Batch 90, Loss: 0.855395, Accuracy: 88.72%\n",
      "Batch 91, Loss: 0.799176, Accuracy: 88.79%\n",
      "Batch 92, Loss: 0.805208, Accuracy: 88.86%\n",
      "Batch 93, Loss: 0.859421, Accuracy: 88.84%\n",
      "Batch 94, Loss: 0.933865, Accuracy: 88.76%\n",
      "Batch 95, Loss: 0.898032, Accuracy: 88.72%\n",
      "Batch 96, Loss: 0.909299, Accuracy: 88.66%\n",
      "Batch 97, Loss: 0.850052, Accuracy: 88.68%\n",
      "Batch 98, Loss: 0.803551, Accuracy: 88.73%\n",
      "Batch 99, Loss: 0.894864, Accuracy: 88.68%\n",
      "Batch 100, Loss: 0.829771, Accuracy: 88.72%\n",
      "Batch 101, Loss: 0.850358, Accuracy: 88.74%\n",
      "Batch 102, Loss: 0.844961, Accuracy: 88.74%\n",
      "Batch 103, Loss: 0.879628, Accuracy: 88.71%\n",
      "Batch 104, Loss: 0.852552, Accuracy: 88.70%\n",
      "Batch 105, Loss: 0.861606, Accuracy: 88.69%\n",
      "Batch 106, Loss: 0.845974, Accuracy: 88.69%\n",
      "Batch 107, Loss: 0.837520, Accuracy: 88.71%\n",
      "Batch 108, Loss: 0.892756, Accuracy: 88.69%\n",
      "Batch 109, Loss: 0.812660, Accuracy: 88.73%\n",
      "Batch 110, Loss: 0.783106, Accuracy: 88.81%\n",
      "Batch 111, Loss: 0.837304, Accuracy: 88.82%\n",
      "Batch 112, Loss: 0.830826, Accuracy: 88.84%\n",
      "Batch 113, Loss: 0.841867, Accuracy: 88.84%\n",
      "Batch 114, Loss: 0.834878, Accuracy: 88.86%\n",
      "Batch 115, Loss: 0.829917, Accuracy: 88.89%\n",
      "Batch 116, Loss: 0.824009, Accuracy: 88.91%\n",
      "Batch 117, Loss: 0.927501, Accuracy: 88.84%\n",
      "Batch 118, Loss: 0.836541, Accuracy: 88.85%\n",
      "Batch 119, Loss: 0.851047, Accuracy: 88.85%\n",
      "Batch 120, Loss: 0.884505, Accuracy: 88.83%\n",
      "Batch 121, Loss: 0.851386, Accuracy: 88.83%\n",
      "Batch 122, Loss: 0.886553, Accuracy: 88.82%\n",
      "Batch 123, Loss: 0.825279, Accuracy: 88.85%\n",
      "Batch 124, Loss: 0.890505, Accuracy: 88.82%\n",
      "Batch 125, Loss: 0.876570, Accuracy: 88.81%\n",
      "Batch 126, Loss: 0.855245, Accuracy: 88.81%\n",
      "Batch 127, Loss: 0.840735, Accuracy: 88.83%\n",
      "Batch 128, Loss: 0.896449, Accuracy: 88.79%\n",
      "Batch 129, Loss: 0.826518, Accuracy: 88.81%\n",
      "Batch 130, Loss: 0.797032, Accuracy: 88.86%\n",
      "Batch 131, Loss: 0.815020, Accuracy: 88.90%\n",
      "Batch 132, Loss: 0.838149, Accuracy: 88.92%\n",
      "Batch 133, Loss: 0.896340, Accuracy: 88.89%\n",
      "Batch 134, Loss: 0.834276, Accuracy: 88.91%\n",
      "Batch 135, Loss: 0.835781, Accuracy: 88.92%\n",
      "Batch 136, Loss: 0.850457, Accuracy: 88.91%\n",
      "Batch 137, Loss: 0.865761, Accuracy: 88.91%\n",
      "Batch 138, Loss: 0.873281, Accuracy: 88.89%\n",
      "Batch 139, Loss: 0.856353, Accuracy: 88.89%\n",
      "Batch 140, Loss: 0.834689, Accuracy: 88.92%\n",
      "Batch 141, Loss: 0.934437, Accuracy: 88.85%\n",
      "Batch 142, Loss: 0.889148, Accuracy: 88.82%\n",
      "Batch 143, Loss: 0.822391, Accuracy: 88.85%\n",
      "Batch 144, Loss: 0.820130, Accuracy: 88.88%\n",
      "Batch 145, Loss: 0.812859, Accuracy: 88.91%\n",
      "Batch 146, Loss: 0.844268, Accuracy: 88.92%\n",
      "Batch 147, Loss: 0.869707, Accuracy: 88.91%\n",
      "Batch 148, Loss: 0.815278, Accuracy: 88.94%\n",
      "Batch 149, Loss: 0.863172, Accuracy: 88.94%\n",
      "Batch 150, Loss: 0.811557, Accuracy: 88.98%\n",
      "Batch 151, Loss: 0.800516, Accuracy: 89.02%\n",
      "Batch 152, Loss: 0.829090, Accuracy: 89.05%\n",
      "Batch 153, Loss: 0.881689, Accuracy: 89.04%\n",
      "Batch 154, Loss: 0.894943, Accuracy: 89.01%\n",
      "Batch 155, Loss: 0.876820, Accuracy: 88.99%\n",
      "Batch 156, Loss: 0.925834, Accuracy: 88.94%\n",
      "Batch 157, Loss: 0.819965, Accuracy: 88.96%\n",
      "Batch 158, Loss: 0.840448, Accuracy: 88.96%\n",
      "Batch 159, Loss: 0.855496, Accuracy: 88.96%\n",
      "Batch 160, Loss: 0.838886, Accuracy: 88.97%\n",
      "Batch 161, Loss: 0.886529, Accuracy: 88.95%\n",
      "Batch 162, Loss: 0.833169, Accuracy: 88.96%\n",
      "Batch 163, Loss: 0.855362, Accuracy: 88.96%\n",
      "Batch 164, Loss: 0.857943, Accuracy: 88.94%\n",
      "Batch 165, Loss: 0.828182, Accuracy: 88.96%\n",
      "Batch 166, Loss: 0.843763, Accuracy: 88.96%\n",
      "Batch 167, Loss: 0.900653, Accuracy: 88.91%\n",
      "Batch 168, Loss: 0.840720, Accuracy: 88.92%\n",
      "Batch 169, Loss: 0.854102, Accuracy: 88.92%\n",
      "Batch 170, Loss: 0.882070, Accuracy: 88.92%\n",
      "Batch 171, Loss: 0.805741, Accuracy: 88.94%\n",
      "Batch 172, Loss: 0.860910, Accuracy: 88.94%\n",
      "Batch 173, Loss: 0.864313, Accuracy: 88.95%\n",
      "Batch 174, Loss: 0.897107, Accuracy: 88.92%\n",
      "Batch 175, Loss: 0.854708, Accuracy: 88.93%\n",
      "Batch 176, Loss: 0.789888, Accuracy: 88.96%\n",
      "Batch 177, Loss: 0.839768, Accuracy: 88.97%\n",
      "Batch 178, Loss: 0.934981, Accuracy: 88.92%\n",
      "Batch 179, Loss: 0.844244, Accuracy: 88.93%\n",
      "Batch 180, Loss: 0.813238, Accuracy: 88.96%\n",
      "Batch 181, Loss: 0.904835, Accuracy: 88.92%\n",
      "Batch 182, Loss: 0.866205, Accuracy: 88.92%\n",
      "Batch 183, Loss: 0.830332, Accuracy: 88.93%\n",
      "Batch 184, Loss: 0.875403, Accuracy: 88.92%\n",
      "Batch 185, Loss: 0.897767, Accuracy: 88.89%\n",
      "Batch 186, Loss: 0.797550, Accuracy: 88.94%\n",
      "Batch 187, Loss: 0.816120, Accuracy: 88.96%\n",
      "Batch 188, Loss: 0.862349, Accuracy: 88.97%\n",
      "Batch 189, Loss: 0.862192, Accuracy: 88.96%\n",
      "Batch 190, Loss: 0.892747, Accuracy: 88.94%\n",
      "Batch 191, Loss: 0.859707, Accuracy: 88.94%\n",
      "Batch 192, Loss: 0.835469, Accuracy: 88.95%\n",
      "Batch 193, Loss: 0.925586, Accuracy: 88.90%\n",
      "Batch 194, Loss: 0.857934, Accuracy: 88.89%\n",
      "Batch 195, Loss: 0.806387, Accuracy: 88.92%\n",
      "Batch 196, Loss: 0.865333, Accuracy: 88.91%\n",
      "Batch 197, Loss: 0.953231, Accuracy: 88.86%\n",
      "Batch 198, Loss: 0.884569, Accuracy: 88.85%\n",
      "Batch 199, Loss: 0.831759, Accuracy: 88.87%\n",
      "Batch 200, Loss: 0.901799, Accuracy: 88.84%\n",
      "Batch 201, Loss: 0.866539, Accuracy: 88.83%\n",
      "Batch 202, Loss: 0.918928, Accuracy: 88.80%\n",
      "Batch 203, Loss: 0.850116, Accuracy: 88.80%\n",
      "Batch 204, Loss: 0.869665, Accuracy: 88.79%\n",
      "Batch 205, Loss: 0.843454, Accuracy: 88.80%\n",
      "Batch 206, Loss: 0.891342, Accuracy: 88.78%\n",
      "Batch 207, Loss: 0.834669, Accuracy: 88.80%\n",
      "Batch 208, Loss: 0.809453, Accuracy: 88.81%\n",
      "Batch 209, Loss: 0.800074, Accuracy: 88.84%\n",
      "Batch 210, Loss: 0.887087, Accuracy: 88.82%\n",
      "Batch 211, Loss: 0.897331, Accuracy: 88.80%\n",
      "Batch 212, Loss: 0.812266, Accuracy: 88.83%\n",
      "Batch 213, Loss: 0.810008, Accuracy: 88.84%\n",
      "Training - Epoch 112, Loss: 0.855077, Accuracy: 88.84%\n",
      "Validation Batch 1, Loss: 0.823045, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.801239, Accuracy: 93.75%\n",
      "Validation Batch 3, Loss: 0.841671, Accuracy: 92.71%\n",
      "Validation Batch 4, Loss: 0.847418, Accuracy: 91.80%\n",
      "Validation Batch 5, Loss: 0.826782, Accuracy: 91.88%\n",
      "Validation Batch 6, Loss: 0.798272, Accuracy: 92.45%\n",
      "Validation Batch 7, Loss: 0.802913, Accuracy: 92.86%\n",
      "Validation Batch 8, Loss: 0.872499, Accuracy: 92.19%\n",
      "Validation Batch 9, Loss: 0.899424, Accuracy: 91.15%\n",
      "Validation Batch 10, Loss: 0.806387, Accuracy: 91.41%\n",
      "Validation Batch 11, Loss: 0.838015, Accuracy: 91.48%\n",
      "Validation Batch 12, Loss: 0.834537, Accuracy: 91.54%\n",
      "Validation Batch 13, Loss: 0.847037, Accuracy: 91.35%\n",
      "Validation Batch 14, Loss: 0.847525, Accuracy: 91.29%\n",
      "Validation Batch 15, Loss: 0.816835, Accuracy: 91.35%\n",
      "Validation Batch 16, Loss: 0.840142, Accuracy: 91.31%\n",
      "Validation Batch 17, Loss: 0.870241, Accuracy: 91.08%\n",
      "Validation Batch 18, Loss: 0.808979, Accuracy: 91.23%\n",
      "Validation Batch 19, Loss: 0.878438, Accuracy: 91.12%\n",
      "Validation Batch 20, Loss: 0.852463, Accuracy: 91.02%\n",
      "Validation Batch 21, Loss: 0.864645, Accuracy: 90.92%\n",
      "Validation Batch 22, Loss: 0.825174, Accuracy: 91.05%\n",
      "Validation Batch 23, Loss: 0.872586, Accuracy: 90.90%\n",
      "Validation Batch 24, Loss: 0.843802, Accuracy: 90.82%\n",
      "Validation Batch 25, Loss: 0.809577, Accuracy: 90.88%\n",
      "Validation Batch 26, Loss: 0.850342, Accuracy: 90.81%\n",
      "Validation Batch 27, Loss: 0.802952, Accuracy: 90.90%\n",
      "Validation - Epoch 112, Loss: 0.837887, Accuracy: 90.90%\n",
      "Patience—3\n",
      "Epoch 113\n",
      "Batch 1, Loss: 0.796961, Accuracy: 95.31%\n",
      "Batch 2, Loss: 0.879597, Accuracy: 90.62%\n",
      "Batch 3, Loss: 0.861042, Accuracy: 90.62%\n",
      "Batch 4, Loss: 0.854848, Accuracy: 90.23%\n",
      "Batch 5, Loss: 0.795003, Accuracy: 91.25%\n",
      "Batch 6, Loss: 0.893116, Accuracy: 90.10%\n",
      "Batch 7, Loss: 0.831836, Accuracy: 90.40%\n",
      "Batch 8, Loss: 0.849542, Accuracy: 90.23%\n",
      "Batch 9, Loss: 0.885136, Accuracy: 89.76%\n",
      "Batch 10, Loss: 0.854285, Accuracy: 89.84%\n",
      "Batch 11, Loss: 0.845738, Accuracy: 89.91%\n",
      "Batch 12, Loss: 0.839011, Accuracy: 89.97%\n",
      "Batch 13, Loss: 0.827946, Accuracy: 90.14%\n",
      "Batch 14, Loss: 0.844106, Accuracy: 90.18%\n",
      "Batch 15, Loss: 0.861669, Accuracy: 90.10%\n",
      "Batch 16, Loss: 0.862780, Accuracy: 89.94%\n",
      "Batch 17, Loss: 0.844757, Accuracy: 90.07%\n",
      "Batch 18, Loss: 0.816131, Accuracy: 90.19%\n",
      "Batch 19, Loss: 0.837058, Accuracy: 90.21%\n",
      "Batch 20, Loss: 0.852710, Accuracy: 90.08%\n",
      "Batch 21, Loss: 0.924097, Accuracy: 89.81%\n",
      "Batch 22, Loss: 0.844520, Accuracy: 89.84%\n",
      "Batch 23, Loss: 0.858972, Accuracy: 89.81%\n",
      "Batch 24, Loss: 0.879344, Accuracy: 89.65%\n",
      "Batch 25, Loss: 0.836904, Accuracy: 89.69%\n",
      "Batch 26, Loss: 0.891919, Accuracy: 89.42%\n",
      "Batch 27, Loss: 0.875744, Accuracy: 89.35%\n",
      "Batch 28, Loss: 0.874741, Accuracy: 89.29%\n",
      "Batch 29, Loss: 0.837304, Accuracy: 89.33%\n",
      "Batch 30, Loss: 0.873676, Accuracy: 89.27%\n",
      "Batch 31, Loss: 0.808606, Accuracy: 89.42%\n",
      "Batch 32, Loss: 0.854080, Accuracy: 89.36%\n",
      "Batch 33, Loss: 0.958035, Accuracy: 89.02%\n",
      "Batch 34, Loss: 0.840288, Accuracy: 89.02%\n",
      "Batch 35, Loss: 0.837786, Accuracy: 89.02%\n",
      "Batch 36, Loss: 0.839584, Accuracy: 89.06%\n",
      "Batch 37, Loss: 0.842031, Accuracy: 89.10%\n",
      "Batch 38, Loss: 0.827995, Accuracy: 89.19%\n",
      "Batch 39, Loss: 0.877870, Accuracy: 89.10%\n",
      "Batch 40, Loss: 0.856393, Accuracy: 89.02%\n",
      "Batch 41, Loss: 0.842478, Accuracy: 89.02%\n",
      "Batch 42, Loss: 0.804056, Accuracy: 89.14%\n",
      "Batch 43, Loss: 0.864150, Accuracy: 89.14%\n",
      "Batch 44, Loss: 0.898815, Accuracy: 88.99%\n",
      "Batch 45, Loss: 0.864310, Accuracy: 88.96%\n",
      "Batch 46, Loss: 0.884237, Accuracy: 88.93%\n",
      "Batch 47, Loss: 0.872319, Accuracy: 88.86%\n",
      "Batch 48, Loss: 0.850382, Accuracy: 88.87%\n",
      "Batch 49, Loss: 0.857070, Accuracy: 88.87%\n",
      "Batch 50, Loss: 0.855447, Accuracy: 88.88%\n",
      "Batch 51, Loss: 0.852557, Accuracy: 88.88%\n",
      "Batch 52, Loss: 0.800344, Accuracy: 89.03%\n",
      "Batch 53, Loss: 0.839041, Accuracy: 89.03%\n",
      "Batch 54, Loss: 0.852039, Accuracy: 89.00%\n",
      "Batch 55, Loss: 0.827997, Accuracy: 89.06%\n",
      "Batch 56, Loss: 0.874182, Accuracy: 89.01%\n",
      "Batch 57, Loss: 0.865035, Accuracy: 88.98%\n",
      "Batch 58, Loss: 0.835685, Accuracy: 89.01%\n",
      "Batch 59, Loss: 0.942950, Accuracy: 88.82%\n",
      "Batch 60, Loss: 0.821899, Accuracy: 88.88%\n",
      "Batch 61, Loss: 0.845803, Accuracy: 88.91%\n",
      "Batch 62, Loss: 0.840649, Accuracy: 88.94%\n",
      "Batch 63, Loss: 0.812114, Accuracy: 89.01%\n",
      "Batch 64, Loss: 0.927609, Accuracy: 88.89%\n",
      "Batch 65, Loss: 0.824090, Accuracy: 88.94%\n",
      "Batch 66, Loss: 0.817963, Accuracy: 88.99%\n",
      "Batch 67, Loss: 0.875114, Accuracy: 88.97%\n",
      "Batch 68, Loss: 0.904456, Accuracy: 88.92%\n",
      "Batch 69, Loss: 0.918608, Accuracy: 88.86%\n",
      "Batch 70, Loss: 0.889975, Accuracy: 88.82%\n",
      "Batch 71, Loss: 0.887638, Accuracy: 88.75%\n",
      "Batch 72, Loss: 0.843799, Accuracy: 88.76%\n",
      "Batch 73, Loss: 0.888848, Accuracy: 88.72%\n",
      "Batch 74, Loss: 0.824487, Accuracy: 88.77%\n",
      "Batch 75, Loss: 0.797781, Accuracy: 88.85%\n",
      "Batch 76, Loss: 0.821845, Accuracy: 88.92%\n",
      "Batch 77, Loss: 0.849673, Accuracy: 88.92%\n",
      "Batch 78, Loss: 0.861260, Accuracy: 88.92%\n",
      "Batch 79, Loss: 0.849463, Accuracy: 88.92%\n",
      "Batch 80, Loss: 0.857827, Accuracy: 88.93%\n",
      "Batch 81, Loss: 0.830902, Accuracy: 88.97%\n",
      "Batch 82, Loss: 0.859505, Accuracy: 88.97%\n",
      "Batch 83, Loss: 0.912832, Accuracy: 88.91%\n",
      "Batch 84, Loss: 0.808738, Accuracy: 88.97%\n",
      "Batch 85, Loss: 0.862038, Accuracy: 88.95%\n",
      "Batch 86, Loss: 0.868545, Accuracy: 88.94%\n",
      "Batch 87, Loss: 0.904077, Accuracy: 88.88%\n",
      "Batch 88, Loss: 0.855991, Accuracy: 88.87%\n",
      "Batch 89, Loss: 0.808329, Accuracy: 88.92%\n",
      "Batch 90, Loss: 0.881975, Accuracy: 88.89%\n",
      "Batch 91, Loss: 0.826710, Accuracy: 88.94%\n",
      "Batch 92, Loss: 0.820092, Accuracy: 88.99%\n",
      "Batch 93, Loss: 0.929094, Accuracy: 88.91%\n",
      "Batch 94, Loss: 0.811091, Accuracy: 88.96%\n",
      "Batch 95, Loss: 0.814216, Accuracy: 89.01%\n",
      "Batch 96, Loss: 0.887769, Accuracy: 88.98%\n",
      "Batch 97, Loss: 0.849477, Accuracy: 88.98%\n",
      "Batch 98, Loss: 0.887959, Accuracy: 88.95%\n",
      "Batch 99, Loss: 0.830705, Accuracy: 88.98%\n",
      "Batch 100, Loss: 0.958699, Accuracy: 88.88%\n",
      "Batch 101, Loss: 0.818996, Accuracy: 88.91%\n",
      "Batch 102, Loss: 0.897651, Accuracy: 88.86%\n",
      "Batch 103, Loss: 0.860480, Accuracy: 88.87%\n",
      "Batch 104, Loss: 0.813480, Accuracy: 88.91%\n",
      "Batch 105, Loss: 0.840472, Accuracy: 88.91%\n",
      "Batch 106, Loss: 0.839081, Accuracy: 88.94%\n",
      "Batch 107, Loss: 0.865906, Accuracy: 88.93%\n",
      "Batch 108, Loss: 0.881587, Accuracy: 88.90%\n",
      "Batch 109, Loss: 0.828039, Accuracy: 88.95%\n",
      "Batch 110, Loss: 0.852885, Accuracy: 88.96%\n",
      "Batch 111, Loss: 0.844770, Accuracy: 88.98%\n",
      "Batch 112, Loss: 0.885570, Accuracy: 88.95%\n",
      "Batch 113, Loss: 0.884949, Accuracy: 88.92%\n",
      "Batch 114, Loss: 0.837861, Accuracy: 88.94%\n",
      "Batch 115, Loss: 0.859883, Accuracy: 88.94%\n",
      "Batch 116, Loss: 0.828375, Accuracy: 88.95%\n",
      "Batch 117, Loss: 0.880053, Accuracy: 88.94%\n",
      "Batch 118, Loss: 0.865330, Accuracy: 88.94%\n",
      "Batch 119, Loss: 0.856993, Accuracy: 88.94%\n",
      "Batch 120, Loss: 0.827349, Accuracy: 88.97%\n",
      "Batch 121, Loss: 0.890220, Accuracy: 88.95%\n",
      "Batch 122, Loss: 0.808604, Accuracy: 88.99%\n",
      "Batch 123, Loss: 0.848725, Accuracy: 89.00%\n",
      "Batch 124, Loss: 0.894977, Accuracy: 88.97%\n",
      "Batch 125, Loss: 0.856019, Accuracy: 88.97%\n",
      "Batch 126, Loss: 0.818610, Accuracy: 89.00%\n",
      "Batch 127, Loss: 0.813112, Accuracy: 89.04%\n",
      "Batch 128, Loss: 0.824713, Accuracy: 89.07%\n",
      "Batch 129, Loss: 0.865886, Accuracy: 89.07%\n",
      "Batch 130, Loss: 0.880440, Accuracy: 89.05%\n",
      "Batch 131, Loss: 0.835384, Accuracy: 89.06%\n",
      "Batch 132, Loss: 0.854129, Accuracy: 89.06%\n",
      "Batch 133, Loss: 0.881072, Accuracy: 89.04%\n",
      "Batch 134, Loss: 0.863125, Accuracy: 89.02%\n",
      "Batch 135, Loss: 0.862004, Accuracy: 89.03%\n",
      "Batch 136, Loss: 0.859007, Accuracy: 89.02%\n",
      "Batch 137, Loss: 0.818199, Accuracy: 89.04%\n",
      "Batch 138, Loss: 0.841468, Accuracy: 89.04%\n",
      "Batch 139, Loss: 0.902993, Accuracy: 89.01%\n",
      "Batch 140, Loss: 0.801612, Accuracy: 89.05%\n",
      "Batch 141, Loss: 0.914683, Accuracy: 89.01%\n",
      "Batch 142, Loss: 0.830093, Accuracy: 89.03%\n",
      "Batch 143, Loss: 0.804133, Accuracy: 89.07%\n",
      "Batch 144, Loss: 0.873010, Accuracy: 89.05%\n",
      "Batch 145, Loss: 0.864329, Accuracy: 89.03%\n",
      "Batch 146, Loss: 0.868909, Accuracy: 89.01%\n",
      "Batch 147, Loss: 0.821146, Accuracy: 89.04%\n",
      "Batch 148, Loss: 0.866111, Accuracy: 89.03%\n",
      "Batch 149, Loss: 0.921460, Accuracy: 88.99%\n",
      "Batch 150, Loss: 0.875873, Accuracy: 88.98%\n",
      "Batch 151, Loss: 0.863461, Accuracy: 88.96%\n",
      "Batch 152, Loss: 0.860215, Accuracy: 88.95%\n",
      "Batch 153, Loss: 0.877440, Accuracy: 88.93%\n",
      "Batch 154, Loss: 0.829509, Accuracy: 88.95%\n",
      "Batch 155, Loss: 0.845048, Accuracy: 88.96%\n",
      "Batch 156, Loss: 0.812563, Accuracy: 88.99%\n",
      "Batch 157, Loss: 0.919335, Accuracy: 88.94%\n",
      "Batch 158, Loss: 0.882252, Accuracy: 88.92%\n",
      "Batch 159, Loss: 0.808092, Accuracy: 88.94%\n",
      "Batch 160, Loss: 0.842029, Accuracy: 88.95%\n",
      "Batch 161, Loss: 0.930944, Accuracy: 88.89%\n",
      "Batch 162, Loss: 0.894558, Accuracy: 88.87%\n",
      "Batch 163, Loss: 0.793363, Accuracy: 88.91%\n",
      "Batch 164, Loss: 0.864339, Accuracy: 88.88%\n",
      "Batch 165, Loss: 0.829615, Accuracy: 88.90%\n",
      "Batch 166, Loss: 0.853352, Accuracy: 88.90%\n",
      "Batch 167, Loss: 0.867630, Accuracy: 88.89%\n",
      "Batch 168, Loss: 0.820354, Accuracy: 88.91%\n",
      "Batch 169, Loss: 0.875624, Accuracy: 88.91%\n",
      "Batch 170, Loss: 0.869856, Accuracy: 88.90%\n",
      "Batch 171, Loss: 0.826074, Accuracy: 88.91%\n",
      "Batch 172, Loss: 0.844575, Accuracy: 88.91%\n",
      "Batch 173, Loss: 0.835505, Accuracy: 88.92%\n",
      "Batch 174, Loss: 0.875200, Accuracy: 88.90%\n",
      "Batch 175, Loss: 0.895576, Accuracy: 88.88%\n",
      "Batch 176, Loss: 0.871689, Accuracy: 88.87%\n",
      "Batch 177, Loss: 0.899646, Accuracy: 88.83%\n",
      "Batch 178, Loss: 0.814165, Accuracy: 88.84%\n",
      "Batch 179, Loss: 0.871348, Accuracy: 88.84%\n",
      "Batch 180, Loss: 0.821597, Accuracy: 88.85%\n",
      "Batch 181, Loss: 0.787684, Accuracy: 88.89%\n",
      "Batch 182, Loss: 0.860747, Accuracy: 88.88%\n",
      "Batch 183, Loss: 0.858019, Accuracy: 88.89%\n",
      "Batch 184, Loss: 0.819401, Accuracy: 88.91%\n",
      "Batch 185, Loss: 0.874225, Accuracy: 88.89%\n",
      "Batch 186, Loss: 0.890795, Accuracy: 88.87%\n",
      "Batch 187, Loss: 0.914121, Accuracy: 88.85%\n",
      "Batch 188, Loss: 0.839448, Accuracy: 88.86%\n",
      "Batch 189, Loss: 0.826473, Accuracy: 88.88%\n",
      "Batch 190, Loss: 0.754296, Accuracy: 88.94%\n",
      "Batch 191, Loss: 0.863891, Accuracy: 88.93%\n",
      "Batch 192, Loss: 0.873149, Accuracy: 88.92%\n",
      "Batch 193, Loss: 0.863833, Accuracy: 88.92%\n",
      "Batch 194, Loss: 0.942247, Accuracy: 88.88%\n",
      "Batch 195, Loss: 0.808901, Accuracy: 88.90%\n",
      "Batch 196, Loss: 0.841686, Accuracy: 88.90%\n",
      "Batch 197, Loss: 0.862104, Accuracy: 88.90%\n",
      "Batch 198, Loss: 0.900024, Accuracy: 88.88%\n",
      "Batch 199, Loss: 0.845733, Accuracy: 88.89%\n",
      "Batch 200, Loss: 0.870025, Accuracy: 88.88%\n",
      "Batch 201, Loss: 0.810325, Accuracy: 88.91%\n",
      "Batch 202, Loss: 0.810212, Accuracy: 88.92%\n",
      "Batch 203, Loss: 0.826138, Accuracy: 88.93%\n",
      "Batch 204, Loss: 0.835136, Accuracy: 88.95%\n",
      "Batch 205, Loss: 0.825738, Accuracy: 88.96%\n",
      "Batch 206, Loss: 0.892195, Accuracy: 88.93%\n",
      "Batch 207, Loss: 0.878305, Accuracy: 88.92%\n",
      "Batch 208, Loss: 0.850943, Accuracy: 88.92%\n",
      "Batch 209, Loss: 0.794770, Accuracy: 88.95%\n",
      "Batch 210, Loss: 0.841030, Accuracy: 88.96%\n",
      "Batch 211, Loss: 0.854676, Accuracy: 88.95%\n",
      "Batch 212, Loss: 0.831886, Accuracy: 88.96%\n",
      "Batch 213, Loss: 0.773242, Accuracy: 88.99%\n",
      "Training - Epoch 113, Loss: 0.854329, Accuracy: 88.99%\n",
      "Validation Batch 1, Loss: 0.813939, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.780838, Accuracy: 96.09%\n",
      "Validation Batch 3, Loss: 0.819967, Accuracy: 94.79%\n",
      "Validation Batch 4, Loss: 0.830068, Accuracy: 93.75%\n",
      "Validation Batch 5, Loss: 0.818615, Accuracy: 93.44%\n",
      "Validation Batch 6, Loss: 0.787026, Accuracy: 93.75%\n",
      "Validation Batch 7, Loss: 0.795347, Accuracy: 93.97%\n",
      "Validation Batch 8, Loss: 0.857345, Accuracy: 93.36%\n",
      "Validation Batch 9, Loss: 0.884161, Accuracy: 92.53%\n",
      "Validation Batch 10, Loss: 0.795825, Accuracy: 92.81%\n",
      "Validation Batch 11, Loss: 0.827333, Accuracy: 92.76%\n",
      "Validation Batch 12, Loss: 0.828075, Accuracy: 92.71%\n",
      "Validation Batch 13, Loss: 0.834524, Accuracy: 92.67%\n",
      "Validation Batch 14, Loss: 0.829425, Accuracy: 92.63%\n",
      "Validation Batch 15, Loss: 0.813701, Accuracy: 92.60%\n",
      "Validation Batch 16, Loss: 0.825060, Accuracy: 92.68%\n",
      "Validation Batch 17, Loss: 0.870108, Accuracy: 92.37%\n",
      "Validation Batch 18, Loss: 0.797315, Accuracy: 92.53%\n",
      "Validation Batch 19, Loss: 0.867452, Accuracy: 92.35%\n",
      "Validation Batch 20, Loss: 0.809505, Accuracy: 92.58%\n",
      "Validation Batch 21, Loss: 0.863107, Accuracy: 92.41%\n",
      "Validation Batch 22, Loss: 0.811417, Accuracy: 92.47%\n",
      "Validation Batch 23, Loss: 0.853934, Accuracy: 92.39%\n",
      "Validation Batch 24, Loss: 0.828500, Accuracy: 92.32%\n",
      "Validation Batch 25, Loss: 0.793715, Accuracy: 92.50%\n",
      "Validation Batch 26, Loss: 0.834058, Accuracy: 92.49%\n",
      "Validation Batch 27, Loss: 0.791731, Accuracy: 92.54%\n",
      "Validation - Epoch 113, Loss: 0.824522, Accuracy: 92.54%\n",
      "Patience—0\n",
      "Epoch 114\n",
      "Batch 1, Loss: 0.863196, Accuracy: 89.06%\n",
      "Batch 2, Loss: 0.882385, Accuracy: 88.28%\n",
      "Batch 3, Loss: 0.840331, Accuracy: 89.06%\n",
      "Batch 4, Loss: 0.788325, Accuracy: 90.62%\n",
      "Batch 5, Loss: 0.863341, Accuracy: 90.00%\n",
      "Batch 6, Loss: 0.899093, Accuracy: 89.06%\n",
      "Batch 7, Loss: 0.870230, Accuracy: 88.84%\n",
      "Batch 8, Loss: 0.823247, Accuracy: 89.45%\n",
      "Batch 9, Loss: 0.845820, Accuracy: 89.58%\n",
      "Batch 10, Loss: 0.775678, Accuracy: 90.31%\n",
      "Batch 11, Loss: 0.835932, Accuracy: 90.48%\n",
      "Batch 12, Loss: 0.880042, Accuracy: 90.10%\n",
      "Batch 13, Loss: 0.874294, Accuracy: 89.90%\n",
      "Batch 14, Loss: 0.879726, Accuracy: 89.62%\n",
      "Batch 15, Loss: 0.818304, Accuracy: 89.79%\n",
      "Batch 16, Loss: 0.847750, Accuracy: 89.75%\n",
      "Batch 17, Loss: 0.882518, Accuracy: 89.52%\n",
      "Batch 18, Loss: 0.889849, Accuracy: 89.32%\n",
      "Batch 19, Loss: 0.846094, Accuracy: 89.39%\n",
      "Batch 20, Loss: 0.958578, Accuracy: 88.75%\n",
      "Batch 21, Loss: 0.845533, Accuracy: 88.84%\n",
      "Batch 22, Loss: 0.889095, Accuracy: 88.64%\n",
      "Batch 23, Loss: 0.883647, Accuracy: 88.59%\n",
      "Batch 24, Loss: 0.856555, Accuracy: 88.61%\n",
      "Batch 25, Loss: 0.917307, Accuracy: 88.31%\n",
      "Batch 26, Loss: 0.824818, Accuracy: 88.52%\n",
      "Batch 27, Loss: 0.833953, Accuracy: 88.66%\n",
      "Batch 28, Loss: 0.820247, Accuracy: 88.78%\n",
      "Batch 29, Loss: 0.774989, Accuracy: 89.06%\n",
      "Batch 30, Loss: 0.854969, Accuracy: 89.06%\n",
      "Batch 31, Loss: 0.811240, Accuracy: 89.16%\n",
      "Batch 32, Loss: 0.828672, Accuracy: 89.31%\n",
      "Batch 33, Loss: 0.880945, Accuracy: 89.20%\n",
      "Batch 34, Loss: 0.893798, Accuracy: 89.02%\n",
      "Batch 35, Loss: 0.861273, Accuracy: 88.97%\n",
      "Batch 36, Loss: 0.874200, Accuracy: 88.89%\n",
      "Batch 37, Loss: 0.896144, Accuracy: 88.77%\n",
      "Batch 38, Loss: 0.877088, Accuracy: 88.69%\n",
      "Batch 39, Loss: 0.868807, Accuracy: 88.70%\n",
      "Batch 40, Loss: 0.857836, Accuracy: 88.71%\n",
      "Batch 41, Loss: 0.824908, Accuracy: 88.80%\n",
      "Batch 42, Loss: 0.875009, Accuracy: 88.76%\n",
      "Batch 43, Loss: 0.864401, Accuracy: 88.74%\n",
      "Batch 44, Loss: 0.810831, Accuracy: 88.85%\n",
      "Batch 45, Loss: 0.812079, Accuracy: 88.92%\n",
      "Batch 46, Loss: 0.912276, Accuracy: 88.79%\n",
      "Batch 47, Loss: 0.856959, Accuracy: 88.76%\n",
      "Batch 48, Loss: 0.863925, Accuracy: 88.77%\n",
      "Batch 49, Loss: 0.850944, Accuracy: 88.74%\n",
      "Batch 50, Loss: 0.876272, Accuracy: 88.72%\n",
      "Batch 51, Loss: 0.862383, Accuracy: 88.73%\n",
      "Batch 52, Loss: 0.839533, Accuracy: 88.76%\n",
      "Batch 53, Loss: 0.785443, Accuracy: 88.92%\n",
      "Batch 54, Loss: 0.927154, Accuracy: 88.74%\n",
      "Batch 55, Loss: 0.909329, Accuracy: 88.66%\n",
      "Batch 56, Loss: 0.859519, Accuracy: 88.67%\n",
      "Batch 57, Loss: 0.865428, Accuracy: 88.68%\n",
      "Batch 58, Loss: 0.824375, Accuracy: 88.74%\n",
      "Batch 59, Loss: 0.892701, Accuracy: 88.67%\n",
      "Batch 60, Loss: 0.825588, Accuracy: 88.72%\n",
      "Batch 61, Loss: 0.847006, Accuracy: 88.76%\n",
      "Batch 62, Loss: 0.805106, Accuracy: 88.84%\n",
      "Batch 63, Loss: 0.799540, Accuracy: 88.91%\n",
      "Batch 64, Loss: 0.877906, Accuracy: 88.87%\n",
      "Batch 65, Loss: 0.797493, Accuracy: 88.94%\n",
      "Batch 66, Loss: 0.876997, Accuracy: 88.92%\n",
      "Batch 67, Loss: 0.867708, Accuracy: 88.90%\n",
      "Batch 68, Loss: 0.896087, Accuracy: 88.86%\n",
      "Batch 69, Loss: 0.933365, Accuracy: 88.75%\n",
      "Batch 70, Loss: 0.867757, Accuracy: 88.73%\n",
      "Batch 71, Loss: 0.925726, Accuracy: 88.64%\n",
      "Batch 72, Loss: 0.809235, Accuracy: 88.72%\n",
      "Batch 73, Loss: 0.927866, Accuracy: 88.63%\n",
      "Batch 74, Loss: 0.883162, Accuracy: 88.60%\n",
      "Batch 75, Loss: 0.868008, Accuracy: 88.58%\n",
      "Batch 76, Loss: 0.830628, Accuracy: 88.61%\n",
      "Batch 77, Loss: 0.875234, Accuracy: 88.58%\n",
      "Batch 78, Loss: 0.807939, Accuracy: 88.64%\n",
      "Batch 79, Loss: 0.864996, Accuracy: 88.63%\n",
      "Batch 80, Loss: 0.857504, Accuracy: 88.63%\n",
      "Batch 81, Loss: 0.845885, Accuracy: 88.64%\n",
      "Batch 82, Loss: 0.806026, Accuracy: 88.72%\n",
      "Batch 83, Loss: 0.797564, Accuracy: 88.80%\n",
      "Batch 84, Loss: 0.879675, Accuracy: 88.76%\n",
      "Batch 85, Loss: 0.914370, Accuracy: 88.69%\n",
      "Batch 86, Loss: 0.827760, Accuracy: 88.74%\n",
      "Batch 87, Loss: 0.890228, Accuracy: 88.69%\n",
      "Batch 88, Loss: 0.824996, Accuracy: 88.73%\n",
      "Batch 89, Loss: 0.829348, Accuracy: 88.75%\n",
      "Batch 90, Loss: 0.892072, Accuracy: 88.72%\n",
      "Batch 91, Loss: 0.829905, Accuracy: 88.75%\n",
      "Batch 92, Loss: 0.845549, Accuracy: 88.76%\n",
      "Batch 93, Loss: 0.847205, Accuracy: 88.76%\n",
      "Batch 94, Loss: 0.828109, Accuracy: 88.80%\n",
      "Batch 95, Loss: 0.849280, Accuracy: 88.80%\n",
      "Batch 96, Loss: 0.827396, Accuracy: 88.83%\n",
      "Batch 97, Loss: 0.860711, Accuracy: 88.82%\n",
      "Batch 98, Loss: 0.815439, Accuracy: 88.86%\n",
      "Batch 99, Loss: 0.908726, Accuracy: 88.81%\n",
      "Batch 100, Loss: 0.827902, Accuracy: 88.81%\n",
      "Batch 101, Loss: 0.894124, Accuracy: 88.77%\n",
      "Batch 102, Loss: 0.876184, Accuracy: 88.76%\n",
      "Batch 103, Loss: 0.807125, Accuracy: 88.82%\n",
      "Batch 104, Loss: 0.899786, Accuracy: 88.76%\n",
      "Batch 105, Loss: 0.923900, Accuracy: 88.71%\n",
      "Batch 106, Loss: 0.888196, Accuracy: 88.69%\n",
      "Batch 107, Loss: 0.870678, Accuracy: 88.68%\n",
      "Batch 108, Loss: 0.894534, Accuracy: 88.64%\n",
      "Batch 109, Loss: 0.817958, Accuracy: 88.68%\n",
      "Batch 110, Loss: 0.864501, Accuracy: 88.66%\n",
      "Batch 111, Loss: 0.930171, Accuracy: 88.60%\n",
      "Batch 112, Loss: 0.849009, Accuracy: 88.62%\n",
      "Batch 113, Loss: 0.847751, Accuracy: 88.63%\n",
      "Batch 114, Loss: 0.789615, Accuracy: 88.69%\n",
      "Batch 115, Loss: 0.860783, Accuracy: 88.70%\n",
      "Batch 116, Loss: 0.808845, Accuracy: 88.74%\n",
      "Batch 117, Loss: 0.809950, Accuracy: 88.80%\n",
      "Batch 118, Loss: 0.831551, Accuracy: 88.82%\n",
      "Batch 119, Loss: 0.837236, Accuracy: 88.83%\n",
      "Batch 120, Loss: 0.902641, Accuracy: 88.78%\n",
      "Batch 121, Loss: 0.850694, Accuracy: 88.78%\n",
      "Batch 122, Loss: 0.838348, Accuracy: 88.79%\n",
      "Batch 123, Loss: 0.878887, Accuracy: 88.78%\n",
      "Batch 124, Loss: 0.853599, Accuracy: 88.79%\n",
      "Batch 125, Loss: 0.871205, Accuracy: 88.76%\n",
      "Batch 126, Loss: 0.898236, Accuracy: 88.72%\n",
      "Batch 127, Loss: 0.821250, Accuracy: 88.74%\n",
      "Batch 128, Loss: 0.867702, Accuracy: 88.75%\n",
      "Batch 129, Loss: 0.786048, Accuracy: 88.80%\n",
      "Batch 130, Loss: 0.844602, Accuracy: 88.81%\n",
      "Batch 131, Loss: 0.869058, Accuracy: 88.79%\n",
      "Batch 132, Loss: 0.859249, Accuracy: 88.78%\n",
      "Batch 133, Loss: 0.868683, Accuracy: 88.77%\n",
      "Batch 134, Loss: 0.847956, Accuracy: 88.77%\n",
      "Batch 135, Loss: 0.843326, Accuracy: 88.77%\n",
      "Batch 136, Loss: 0.924598, Accuracy: 88.73%\n",
      "Batch 137, Loss: 0.822916, Accuracy: 88.74%\n",
      "Batch 138, Loss: 0.866266, Accuracy: 88.76%\n",
      "Batch 139, Loss: 0.878611, Accuracy: 88.73%\n",
      "Batch 140, Loss: 0.804417, Accuracy: 88.76%\n",
      "Batch 141, Loss: 0.908486, Accuracy: 88.72%\n",
      "Batch 142, Loss: 0.844701, Accuracy: 88.73%\n",
      "Batch 143, Loss: 0.833886, Accuracy: 88.75%\n",
      "Batch 144, Loss: 0.869797, Accuracy: 88.74%\n",
      "Batch 145, Loss: 0.861328, Accuracy: 88.74%\n",
      "Batch 146, Loss: 0.824117, Accuracy: 88.76%\n",
      "Batch 147, Loss: 0.873397, Accuracy: 88.75%\n",
      "Batch 148, Loss: 0.897178, Accuracy: 88.74%\n",
      "Batch 149, Loss: 0.853211, Accuracy: 88.74%\n",
      "Batch 150, Loss: 0.823709, Accuracy: 88.75%\n",
      "Batch 151, Loss: 0.832961, Accuracy: 88.76%\n",
      "Batch 152, Loss: 0.806387, Accuracy: 88.78%\n",
      "Batch 153, Loss: 0.870081, Accuracy: 88.78%\n",
      "Batch 154, Loss: 0.847602, Accuracy: 88.78%\n",
      "Batch 155, Loss: 0.899365, Accuracy: 88.75%\n",
      "Batch 156, Loss: 0.802385, Accuracy: 88.78%\n",
      "Batch 157, Loss: 0.908121, Accuracy: 88.74%\n",
      "Batch 158, Loss: 0.880693, Accuracy: 88.73%\n",
      "Batch 159, Loss: 0.825061, Accuracy: 88.75%\n",
      "Batch 160, Loss: 0.864027, Accuracy: 88.76%\n",
      "Batch 161, Loss: 0.826787, Accuracy: 88.78%\n",
      "Batch 162, Loss: 0.938828, Accuracy: 88.72%\n",
      "Batch 163, Loss: 0.826555, Accuracy: 88.75%\n",
      "Batch 164, Loss: 0.806101, Accuracy: 88.77%\n",
      "Batch 165, Loss: 0.897241, Accuracy: 88.74%\n",
      "Batch 166, Loss: 0.846015, Accuracy: 88.74%\n",
      "Batch 167, Loss: 0.904409, Accuracy: 88.71%\n",
      "Batch 168, Loss: 0.847929, Accuracy: 88.72%\n",
      "Batch 169, Loss: 0.900856, Accuracy: 88.69%\n",
      "Batch 170, Loss: 0.810442, Accuracy: 88.72%\n",
      "Batch 171, Loss: 0.811169, Accuracy: 88.74%\n",
      "Batch 172, Loss: 0.914000, Accuracy: 88.71%\n",
      "Batch 173, Loss: 0.858166, Accuracy: 88.71%\n",
      "Batch 174, Loss: 0.906138, Accuracy: 88.69%\n",
      "Batch 175, Loss: 0.878906, Accuracy: 88.68%\n",
      "Batch 176, Loss: 0.829609, Accuracy: 88.69%\n",
      "Batch 177, Loss: 0.863724, Accuracy: 88.68%\n",
      "Batch 178, Loss: 0.865336, Accuracy: 88.68%\n",
      "Batch 179, Loss: 0.822437, Accuracy: 88.70%\n",
      "Batch 180, Loss: 0.864428, Accuracy: 88.70%\n",
      "Batch 181, Loss: 0.885331, Accuracy: 88.67%\n",
      "Batch 182, Loss: 0.880103, Accuracy: 88.65%\n",
      "Batch 183, Loss: 0.888408, Accuracy: 88.64%\n",
      "Batch 184, Loss: 0.819752, Accuracy: 88.65%\n",
      "Batch 185, Loss: 0.878080, Accuracy: 88.64%\n",
      "Batch 186, Loss: 0.958803, Accuracy: 88.58%\n",
      "Batch 187, Loss: 0.841671, Accuracy: 88.59%\n",
      "Batch 188, Loss: 0.873118, Accuracy: 88.58%\n",
      "Batch 189, Loss: 0.871043, Accuracy: 88.57%\n",
      "Batch 190, Loss: 0.824544, Accuracy: 88.59%\n",
      "Batch 191, Loss: 0.930215, Accuracy: 88.55%\n",
      "Batch 192, Loss: 0.833292, Accuracy: 88.56%\n",
      "Batch 193, Loss: 0.858609, Accuracy: 88.56%\n",
      "Batch 194, Loss: 0.790645, Accuracy: 88.60%\n",
      "Batch 195, Loss: 0.838569, Accuracy: 88.61%\n",
      "Batch 196, Loss: 0.862763, Accuracy: 88.61%\n",
      "Batch 197, Loss: 0.902919, Accuracy: 88.57%\n",
      "Batch 198, Loss: 0.808413, Accuracy: 88.60%\n",
      "Batch 199, Loss: 0.824145, Accuracy: 88.61%\n",
      "Batch 200, Loss: 0.855940, Accuracy: 88.62%\n",
      "Batch 201, Loss: 0.847108, Accuracy: 88.63%\n",
      "Batch 202, Loss: 0.840588, Accuracy: 88.64%\n",
      "Batch 203, Loss: 0.845424, Accuracy: 88.64%\n",
      "Batch 204, Loss: 0.888006, Accuracy: 88.63%\n",
      "Batch 205, Loss: 0.854314, Accuracy: 88.63%\n",
      "Batch 206, Loss: 0.863135, Accuracy: 88.61%\n",
      "Batch 207, Loss: 0.811353, Accuracy: 88.64%\n",
      "Batch 208, Loss: 0.807847, Accuracy: 88.66%\n",
      "Batch 209, Loss: 0.840329, Accuracy: 88.67%\n",
      "Batch 210, Loss: 0.842986, Accuracy: 88.68%\n",
      "Batch 211, Loss: 0.889969, Accuracy: 88.66%\n",
      "Batch 212, Loss: 0.883766, Accuracy: 88.65%\n",
      "Batch 213, Loss: 0.841757, Accuracy: 88.65%\n",
      "Training - Epoch 114, Loss: 0.857047, Accuracy: 88.65%\n",
      "Validation Batch 1, Loss: 0.814319, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.788656, Accuracy: 95.31%\n",
      "Validation Batch 3, Loss: 0.822319, Accuracy: 94.27%\n",
      "Validation Batch 4, Loss: 0.832277, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.814297, Accuracy: 93.44%\n",
      "Validation Batch 6, Loss: 0.790204, Accuracy: 93.75%\n",
      "Validation Batch 7, Loss: 0.796694, Accuracy: 93.97%\n",
      "Validation Batch 8, Loss: 0.857756, Accuracy: 93.36%\n",
      "Validation Batch 9, Loss: 0.883284, Accuracy: 92.71%\n",
      "Validation Batch 10, Loss: 0.799420, Accuracy: 92.97%\n",
      "Validation Batch 11, Loss: 0.819811, Accuracy: 93.04%\n",
      "Validation Batch 12, Loss: 0.828483, Accuracy: 92.97%\n",
      "Validation Batch 13, Loss: 0.833592, Accuracy: 92.79%\n",
      "Validation Batch 14, Loss: 0.833560, Accuracy: 92.75%\n",
      "Validation Batch 15, Loss: 0.810321, Accuracy: 92.81%\n",
      "Validation Batch 16, Loss: 0.821169, Accuracy: 92.87%\n",
      "Validation Batch 17, Loss: 0.867059, Accuracy: 92.56%\n",
      "Validation Batch 18, Loss: 0.802795, Accuracy: 92.71%\n",
      "Validation Batch 19, Loss: 0.863526, Accuracy: 92.60%\n",
      "Validation Batch 20, Loss: 0.816238, Accuracy: 92.73%\n",
      "Validation Batch 21, Loss: 0.862144, Accuracy: 92.56%\n",
      "Validation Batch 22, Loss: 0.813046, Accuracy: 92.61%\n",
      "Validation Batch 23, Loss: 0.858227, Accuracy: 92.46%\n",
      "Validation Batch 24, Loss: 0.821748, Accuracy: 92.51%\n",
      "Validation Batch 25, Loss: 0.799489, Accuracy: 92.62%\n",
      "Validation Batch 26, Loss: 0.831817, Accuracy: 92.61%\n",
      "Validation Batch 27, Loss: 0.791860, Accuracy: 92.66%\n",
      "Validation - Epoch 114, Loss: 0.824967, Accuracy: 92.66%\n",
      "Patience—1\n",
      "Epoch 115\n",
      "Batch 1, Loss: 0.862465, Accuracy: 87.50%\n",
      "Batch 2, Loss: 0.923580, Accuracy: 84.38%\n",
      "Batch 3, Loss: 0.823777, Accuracy: 86.98%\n",
      "Batch 4, Loss: 0.868987, Accuracy: 87.11%\n",
      "Batch 5, Loss: 0.859235, Accuracy: 87.19%\n",
      "Batch 6, Loss: 0.879575, Accuracy: 86.98%\n",
      "Batch 7, Loss: 0.851389, Accuracy: 87.28%\n",
      "Batch 8, Loss: 0.866563, Accuracy: 87.50%\n",
      "Batch 9, Loss: 0.817925, Accuracy: 88.19%\n",
      "Batch 10, Loss: 0.844774, Accuracy: 88.28%\n",
      "Batch 11, Loss: 0.804484, Accuracy: 88.78%\n",
      "Batch 12, Loss: 0.844784, Accuracy: 88.80%\n",
      "Batch 13, Loss: 0.838820, Accuracy: 89.06%\n",
      "Batch 14, Loss: 0.865773, Accuracy: 89.06%\n",
      "Batch 15, Loss: 0.830905, Accuracy: 89.27%\n",
      "Batch 16, Loss: 0.893314, Accuracy: 89.06%\n",
      "Batch 17, Loss: 0.876588, Accuracy: 88.97%\n",
      "Batch 18, Loss: 0.836282, Accuracy: 89.06%\n",
      "Batch 19, Loss: 0.905017, Accuracy: 88.82%\n",
      "Batch 20, Loss: 0.815184, Accuracy: 88.98%\n",
      "Batch 21, Loss: 0.817558, Accuracy: 89.21%\n",
      "Batch 22, Loss: 0.894506, Accuracy: 88.99%\n",
      "Batch 23, Loss: 0.864892, Accuracy: 88.93%\n",
      "Batch 24, Loss: 0.914012, Accuracy: 88.54%\n",
      "Batch 25, Loss: 0.842749, Accuracy: 88.62%\n",
      "Batch 26, Loss: 0.870997, Accuracy: 88.58%\n",
      "Batch 27, Loss: 0.904998, Accuracy: 88.37%\n",
      "Batch 28, Loss: 0.872049, Accuracy: 88.34%\n",
      "Batch 29, Loss: 0.787025, Accuracy: 88.63%\n",
      "Batch 30, Loss: 0.834444, Accuracy: 88.70%\n",
      "Batch 31, Loss: 0.812410, Accuracy: 88.81%\n",
      "Batch 32, Loss: 0.781463, Accuracy: 89.06%\n",
      "Batch 33, Loss: 0.856001, Accuracy: 89.06%\n",
      "Batch 34, Loss: 0.862037, Accuracy: 89.06%\n",
      "Batch 35, Loss: 0.869629, Accuracy: 89.02%\n",
      "Batch 36, Loss: 0.900383, Accuracy: 88.93%\n",
      "Batch 37, Loss: 0.828085, Accuracy: 89.02%\n",
      "Batch 38, Loss: 0.886116, Accuracy: 88.94%\n",
      "Batch 39, Loss: 0.847366, Accuracy: 88.94%\n",
      "Batch 40, Loss: 0.811304, Accuracy: 89.06%\n",
      "Batch 41, Loss: 0.800785, Accuracy: 89.21%\n",
      "Batch 42, Loss: 0.857041, Accuracy: 89.17%\n",
      "Batch 43, Loss: 0.788543, Accuracy: 89.35%\n",
      "Batch 44, Loss: 0.862714, Accuracy: 89.38%\n",
      "Batch 45, Loss: 0.825961, Accuracy: 89.44%\n",
      "Batch 46, Loss: 0.924070, Accuracy: 89.27%\n",
      "Batch 47, Loss: 0.894156, Accuracy: 89.20%\n",
      "Batch 48, Loss: 0.836774, Accuracy: 89.23%\n",
      "Batch 49, Loss: 0.808173, Accuracy: 89.32%\n",
      "Batch 50, Loss: 0.841281, Accuracy: 89.31%\n",
      "Batch 51, Loss: 0.791209, Accuracy: 89.43%\n",
      "Batch 52, Loss: 0.904169, Accuracy: 89.33%\n",
      "Batch 53, Loss: 0.890281, Accuracy: 89.21%\n",
      "Batch 54, Loss: 0.869428, Accuracy: 89.18%\n",
      "Batch 55, Loss: 0.830240, Accuracy: 89.23%\n",
      "Batch 56, Loss: 0.853985, Accuracy: 89.20%\n",
      "Batch 57, Loss: 0.833915, Accuracy: 89.23%\n",
      "Batch 58, Loss: 0.849574, Accuracy: 89.22%\n",
      "Batch 59, Loss: 0.823443, Accuracy: 89.25%\n",
      "Batch 60, Loss: 0.854378, Accuracy: 89.24%\n",
      "Batch 61, Loss: 0.835297, Accuracy: 89.29%\n",
      "Batch 62, Loss: 0.832913, Accuracy: 89.31%\n",
      "Batch 63, Loss: 0.847468, Accuracy: 89.34%\n",
      "Batch 64, Loss: 0.864259, Accuracy: 89.33%\n",
      "Batch 65, Loss: 0.818622, Accuracy: 89.40%\n",
      "Batch 66, Loss: 0.796975, Accuracy: 89.51%\n",
      "Batch 67, Loss: 0.921311, Accuracy: 89.41%\n",
      "Batch 68, Loss: 0.828014, Accuracy: 89.43%\n",
      "Batch 69, Loss: 0.874281, Accuracy: 89.40%\n",
      "Batch 70, Loss: 0.795677, Accuracy: 89.49%\n",
      "Batch 71, Loss: 0.897824, Accuracy: 89.44%\n",
      "Batch 72, Loss: 0.863277, Accuracy: 89.43%\n",
      "Batch 73, Loss: 0.814788, Accuracy: 89.47%\n",
      "Batch 74, Loss: 0.896988, Accuracy: 89.38%\n",
      "Batch 75, Loss: 0.810950, Accuracy: 89.44%\n",
      "Batch 76, Loss: 0.834309, Accuracy: 89.45%\n",
      "Batch 77, Loss: 0.822312, Accuracy: 89.53%\n",
      "Batch 78, Loss: 0.840806, Accuracy: 89.54%\n",
      "Batch 79, Loss: 0.828501, Accuracy: 89.58%\n",
      "Batch 80, Loss: 0.854232, Accuracy: 89.57%\n",
      "Batch 81, Loss: 0.867272, Accuracy: 89.54%\n",
      "Batch 82, Loss: 0.873618, Accuracy: 89.52%\n",
      "Batch 83, Loss: 0.860976, Accuracy: 89.50%\n",
      "Batch 84, Loss: 0.864501, Accuracy: 89.45%\n",
      "Batch 85, Loss: 0.847008, Accuracy: 89.45%\n",
      "Batch 86, Loss: 0.832794, Accuracy: 89.48%\n",
      "Batch 87, Loss: 0.806053, Accuracy: 89.53%\n",
      "Batch 88, Loss: 0.927711, Accuracy: 89.44%\n",
      "Batch 89, Loss: 0.822846, Accuracy: 89.47%\n",
      "Batch 90, Loss: 0.872229, Accuracy: 89.43%\n",
      "Batch 91, Loss: 0.811457, Accuracy: 89.46%\n",
      "Batch 92, Loss: 0.850607, Accuracy: 89.45%\n",
      "Batch 93, Loss: 0.919268, Accuracy: 89.38%\n",
      "Batch 94, Loss: 0.855051, Accuracy: 89.38%\n",
      "Batch 95, Loss: 0.869801, Accuracy: 89.34%\n",
      "Batch 96, Loss: 0.867144, Accuracy: 89.32%\n",
      "Batch 97, Loss: 0.868494, Accuracy: 89.30%\n",
      "Batch 98, Loss: 0.838872, Accuracy: 89.33%\n",
      "Batch 99, Loss: 0.841191, Accuracy: 89.35%\n",
      "Batch 100, Loss: 0.828749, Accuracy: 89.39%\n",
      "Batch 101, Loss: 0.818067, Accuracy: 89.42%\n",
      "Batch 102, Loss: 0.913374, Accuracy: 89.34%\n",
      "Batch 103, Loss: 0.864029, Accuracy: 89.32%\n",
      "Batch 104, Loss: 0.897055, Accuracy: 89.27%\n",
      "Batch 105, Loss: 0.849708, Accuracy: 89.27%\n",
      "Batch 106, Loss: 0.888620, Accuracy: 89.24%\n",
      "Batch 107, Loss: 0.889170, Accuracy: 89.21%\n",
      "Batch 108, Loss: 0.866260, Accuracy: 89.19%\n",
      "Batch 109, Loss: 0.826147, Accuracy: 89.22%\n",
      "Batch 110, Loss: 0.856614, Accuracy: 89.22%\n",
      "Batch 111, Loss: 0.840445, Accuracy: 89.23%\n",
      "Batch 112, Loss: 0.842492, Accuracy: 89.24%\n",
      "Batch 113, Loss: 0.887604, Accuracy: 89.21%\n",
      "Batch 114, Loss: 0.831222, Accuracy: 89.23%\n",
      "Batch 115, Loss: 0.796313, Accuracy: 89.29%\n",
      "Batch 116, Loss: 0.927768, Accuracy: 89.24%\n",
      "Batch 117, Loss: 0.944460, Accuracy: 89.16%\n",
      "Batch 118, Loss: 0.866773, Accuracy: 89.14%\n",
      "Batch 119, Loss: 0.844570, Accuracy: 89.15%\n",
      "Batch 120, Loss: 0.889958, Accuracy: 89.13%\n",
      "Batch 121, Loss: 0.833638, Accuracy: 89.14%\n",
      "Batch 122, Loss: 0.867164, Accuracy: 89.14%\n",
      "Batch 123, Loss: 0.809139, Accuracy: 89.18%\n",
      "Batch 124, Loss: 0.851381, Accuracy: 89.18%\n",
      "Batch 125, Loss: 0.866969, Accuracy: 89.16%\n",
      "Batch 126, Loss: 0.829385, Accuracy: 89.17%\n",
      "Batch 127, Loss: 0.786388, Accuracy: 89.22%\n",
      "Batch 128, Loss: 0.860314, Accuracy: 89.22%\n",
      "Batch 129, Loss: 0.909964, Accuracy: 89.20%\n",
      "Batch 130, Loss: 0.833506, Accuracy: 89.21%\n",
      "Batch 131, Loss: 0.904595, Accuracy: 89.17%\n",
      "Batch 132, Loss: 0.784828, Accuracy: 89.22%\n",
      "Batch 133, Loss: 0.873473, Accuracy: 89.20%\n",
      "Batch 134, Loss: 0.860165, Accuracy: 89.20%\n",
      "Batch 135, Loss: 0.830195, Accuracy: 89.22%\n",
      "Batch 136, Loss: 0.887011, Accuracy: 89.18%\n",
      "Batch 137, Loss: 0.805393, Accuracy: 89.21%\n",
      "Batch 138, Loss: 0.809120, Accuracy: 89.24%\n",
      "Batch 139, Loss: 0.862462, Accuracy: 89.23%\n",
      "Batch 140, Loss: 0.865879, Accuracy: 89.21%\n",
      "Batch 141, Loss: 0.817277, Accuracy: 89.24%\n",
      "Batch 142, Loss: 0.906351, Accuracy: 89.19%\n",
      "Batch 143, Loss: 0.864928, Accuracy: 89.18%\n",
      "Batch 144, Loss: 0.844619, Accuracy: 89.19%\n",
      "Batch 145, Loss: 0.855057, Accuracy: 89.20%\n",
      "Batch 146, Loss: 0.853294, Accuracy: 89.19%\n",
      "Batch 147, Loss: 0.852942, Accuracy: 89.19%\n",
      "Batch 148, Loss: 0.833058, Accuracy: 89.19%\n",
      "Batch 149, Loss: 0.814244, Accuracy: 89.22%\n",
      "Batch 150, Loss: 0.802855, Accuracy: 89.25%\n",
      "Batch 151, Loss: 0.836449, Accuracy: 89.25%\n",
      "Batch 152, Loss: 0.846542, Accuracy: 89.25%\n",
      "Batch 153, Loss: 0.907744, Accuracy: 89.21%\n",
      "Batch 154, Loss: 0.820139, Accuracy: 89.23%\n",
      "Batch 155, Loss: 0.818880, Accuracy: 89.26%\n",
      "Batch 156, Loss: 0.814671, Accuracy: 89.29%\n",
      "Batch 157, Loss: 0.841909, Accuracy: 89.30%\n",
      "Batch 158, Loss: 0.866558, Accuracy: 89.30%\n",
      "Batch 159, Loss: 0.822631, Accuracy: 89.32%\n",
      "Batch 160, Loss: 0.840709, Accuracy: 89.33%\n",
      "Batch 161, Loss: 0.855076, Accuracy: 89.32%\n",
      "Batch 162, Loss: 0.854774, Accuracy: 89.32%\n",
      "Batch 163, Loss: 0.883060, Accuracy: 89.30%\n",
      "Batch 164, Loss: 0.802359, Accuracy: 89.34%\n",
      "Batch 165, Loss: 0.837329, Accuracy: 89.35%\n",
      "Batch 166, Loss: 0.818689, Accuracy: 89.36%\n",
      "Batch 167, Loss: 0.849399, Accuracy: 89.37%\n",
      "Batch 168, Loss: 0.822328, Accuracy: 89.40%\n",
      "Batch 169, Loss: 0.860786, Accuracy: 89.38%\n",
      "Batch 170, Loss: 0.813563, Accuracy: 89.39%\n",
      "Batch 171, Loss: 0.895441, Accuracy: 89.35%\n",
      "Batch 172, Loss: 0.842813, Accuracy: 89.36%\n",
      "Batch 173, Loss: 0.846613, Accuracy: 89.37%\n",
      "Batch 174, Loss: 0.920381, Accuracy: 89.33%\n",
      "Batch 175, Loss: 0.840191, Accuracy: 89.34%\n",
      "Batch 176, Loss: 0.878608, Accuracy: 89.34%\n",
      "Batch 177, Loss: 0.898101, Accuracy: 89.30%\n",
      "Batch 178, Loss: 0.922056, Accuracy: 89.26%\n",
      "Batch 179, Loss: 0.951883, Accuracy: 89.21%\n",
      "Batch 180, Loss: 0.757975, Accuracy: 89.27%\n",
      "Batch 181, Loss: 0.859110, Accuracy: 89.27%\n",
      "Batch 182, Loss: 0.821755, Accuracy: 89.29%\n",
      "Batch 183, Loss: 0.915374, Accuracy: 89.26%\n",
      "Batch 184, Loss: 0.840461, Accuracy: 89.27%\n",
      "Batch 185, Loss: 0.820700, Accuracy: 89.29%\n",
      "Batch 186, Loss: 0.838973, Accuracy: 89.31%\n",
      "Batch 187, Loss: 0.909492, Accuracy: 89.27%\n",
      "Batch 188, Loss: 0.817043, Accuracy: 89.30%\n",
      "Batch 189, Loss: 0.906327, Accuracy: 89.26%\n",
      "Batch 190, Loss: 0.874517, Accuracy: 89.24%\n",
      "Batch 191, Loss: 0.839737, Accuracy: 89.25%\n",
      "Batch 192, Loss: 0.803994, Accuracy: 89.27%\n",
      "Batch 193, Loss: 0.871160, Accuracy: 89.26%\n",
      "Batch 194, Loss: 0.841803, Accuracy: 89.26%\n",
      "Batch 195, Loss: 0.877592, Accuracy: 89.25%\n",
      "Batch 196, Loss: 0.903961, Accuracy: 89.21%\n",
      "Batch 197, Loss: 0.841188, Accuracy: 89.22%\n",
      "Batch 198, Loss: 0.834039, Accuracy: 89.23%\n",
      "Batch 199, Loss: 0.862322, Accuracy: 89.23%\n",
      "Batch 200, Loss: 0.831435, Accuracy: 89.24%\n",
      "Batch 201, Loss: 0.853677, Accuracy: 89.24%\n",
      "Batch 202, Loss: 0.813961, Accuracy: 89.26%\n",
      "Batch 203, Loss: 0.818869, Accuracy: 89.27%\n",
      "Batch 204, Loss: 0.891812, Accuracy: 89.25%\n",
      "Batch 205, Loss: 0.827622, Accuracy: 89.27%\n",
      "Batch 206, Loss: 0.871449, Accuracy: 89.26%\n",
      "Batch 207, Loss: 0.863196, Accuracy: 89.25%\n",
      "Batch 208, Loss: 0.895112, Accuracy: 89.22%\n",
      "Batch 209, Loss: 0.897840, Accuracy: 89.20%\n",
      "Batch 210, Loss: 0.806516, Accuracy: 89.23%\n",
      "Batch 211, Loss: 0.805173, Accuracy: 89.26%\n",
      "Batch 212, Loss: 0.860738, Accuracy: 89.25%\n",
      "Batch 213, Loss: 0.817975, Accuracy: 89.27%\n",
      "Training - Epoch 115, Loss: 0.852257, Accuracy: 89.27%\n",
      "Validation Batch 1, Loss: 0.813265, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.789376, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.817266, Accuracy: 94.27%\n",
      "Validation Batch 4, Loss: 0.826932, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.809211, Accuracy: 93.75%\n",
      "Validation Batch 6, Loss: 0.791358, Accuracy: 94.01%\n",
      "Validation Batch 7, Loss: 0.795670, Accuracy: 94.20%\n",
      "Validation Batch 8, Loss: 0.856655, Accuracy: 93.55%\n",
      "Validation Batch 9, Loss: 0.880152, Accuracy: 92.88%\n",
      "Validation Batch 10, Loss: 0.796464, Accuracy: 93.12%\n",
      "Validation Batch 11, Loss: 0.818958, Accuracy: 93.18%\n",
      "Validation Batch 12, Loss: 0.828237, Accuracy: 93.10%\n",
      "Validation Batch 13, Loss: 0.828952, Accuracy: 93.15%\n",
      "Validation Batch 14, Loss: 0.833544, Accuracy: 93.19%\n",
      "Validation Batch 15, Loss: 0.804561, Accuracy: 93.23%\n",
      "Validation Batch 16, Loss: 0.820088, Accuracy: 93.26%\n",
      "Validation Batch 17, Loss: 0.866905, Accuracy: 92.92%\n",
      "Validation Batch 18, Loss: 0.798052, Accuracy: 93.06%\n",
      "Validation Batch 19, Loss: 0.866147, Accuracy: 92.93%\n",
      "Validation Batch 20, Loss: 0.810705, Accuracy: 93.05%\n",
      "Validation Batch 21, Loss: 0.858435, Accuracy: 92.86%\n",
      "Validation Batch 22, Loss: 0.809555, Accuracy: 92.90%\n",
      "Validation Batch 23, Loss: 0.854363, Accuracy: 92.73%\n",
      "Validation Batch 24, Loss: 0.819280, Accuracy: 92.77%\n",
      "Validation Batch 25, Loss: 0.799567, Accuracy: 92.94%\n",
      "Validation Batch 26, Loss: 0.832209, Accuracy: 92.91%\n",
      "Validation Batch 27, Loss: 0.791372, Accuracy: 92.95%\n",
      "Validation - Epoch 115, Loss: 0.822862, Accuracy: 92.95%\n",
      "Patience—0\n",
      "Epoch 116\n",
      "Batch 1, Loss: 0.811267, Accuracy: 92.19%\n",
      "Batch 2, Loss: 0.862197, Accuracy: 89.84%\n",
      "Batch 3, Loss: 0.907564, Accuracy: 87.50%\n",
      "Batch 4, Loss: 0.785513, Accuracy: 89.84%\n",
      "Batch 5, Loss: 0.820767, Accuracy: 90.31%\n",
      "Batch 6, Loss: 0.818009, Accuracy: 90.89%\n",
      "Batch 7, Loss: 0.889623, Accuracy: 90.18%\n",
      "Batch 8, Loss: 0.839148, Accuracy: 90.23%\n",
      "Batch 9, Loss: 0.887392, Accuracy: 89.58%\n",
      "Batch 10, Loss: 0.866039, Accuracy: 89.38%\n",
      "Batch 11, Loss: 0.843113, Accuracy: 89.49%\n",
      "Batch 12, Loss: 0.826117, Accuracy: 89.58%\n",
      "Batch 13, Loss: 0.835018, Accuracy: 89.66%\n",
      "Batch 14, Loss: 0.817034, Accuracy: 89.84%\n",
      "Batch 15, Loss: 0.827420, Accuracy: 90.00%\n",
      "Batch 16, Loss: 0.898473, Accuracy: 89.65%\n",
      "Batch 17, Loss: 0.824281, Accuracy: 89.80%\n",
      "Batch 18, Loss: 0.891581, Accuracy: 89.50%\n",
      "Batch 19, Loss: 0.813244, Accuracy: 89.64%\n",
      "Batch 20, Loss: 0.895573, Accuracy: 89.45%\n",
      "Batch 21, Loss: 0.835284, Accuracy: 89.58%\n",
      "Batch 22, Loss: 0.829291, Accuracy: 89.70%\n",
      "Batch 23, Loss: 0.894502, Accuracy: 89.47%\n",
      "Batch 24, Loss: 0.837652, Accuracy: 89.52%\n",
      "Batch 25, Loss: 0.880671, Accuracy: 89.38%\n",
      "Batch 26, Loss: 0.870491, Accuracy: 89.30%\n",
      "Batch 27, Loss: 0.818846, Accuracy: 89.47%\n",
      "Batch 28, Loss: 0.845304, Accuracy: 89.51%\n",
      "Batch 29, Loss: 0.860945, Accuracy: 89.44%\n",
      "Batch 30, Loss: 0.871480, Accuracy: 89.38%\n",
      "Batch 31, Loss: 0.828227, Accuracy: 89.47%\n",
      "Batch 32, Loss: 0.820925, Accuracy: 89.55%\n",
      "Batch 33, Loss: 0.904027, Accuracy: 89.35%\n",
      "Batch 34, Loss: 0.802154, Accuracy: 89.52%\n",
      "Batch 35, Loss: 0.891292, Accuracy: 89.38%\n",
      "Batch 36, Loss: 0.819362, Accuracy: 89.45%\n",
      "Batch 37, Loss: 0.829945, Accuracy: 89.53%\n",
      "Batch 38, Loss: 0.862460, Accuracy: 89.47%\n",
      "Batch 39, Loss: 0.880582, Accuracy: 89.38%\n",
      "Batch 40, Loss: 0.841443, Accuracy: 89.41%\n",
      "Batch 41, Loss: 0.795994, Accuracy: 89.56%\n",
      "Batch 42, Loss: 0.793666, Accuracy: 89.69%\n",
      "Batch 43, Loss: 0.879710, Accuracy: 89.64%\n",
      "Batch 44, Loss: 0.891002, Accuracy: 89.56%\n",
      "Batch 45, Loss: 0.803967, Accuracy: 89.69%\n",
      "Batch 46, Loss: 0.849457, Accuracy: 89.67%\n",
      "Batch 47, Loss: 0.765602, Accuracy: 89.86%\n",
      "Batch 48, Loss: 0.827595, Accuracy: 89.91%\n",
      "Batch 49, Loss: 0.866456, Accuracy: 89.89%\n",
      "Batch 50, Loss: 0.873312, Accuracy: 89.84%\n",
      "Batch 51, Loss: 0.838363, Accuracy: 89.86%\n",
      "Batch 52, Loss: 0.865991, Accuracy: 89.81%\n",
      "Batch 53, Loss: 0.800905, Accuracy: 89.92%\n",
      "Batch 54, Loss: 0.836947, Accuracy: 89.93%\n",
      "Batch 55, Loss: 0.811004, Accuracy: 89.97%\n",
      "Batch 56, Loss: 0.903433, Accuracy: 89.87%\n",
      "Batch 57, Loss: 0.832865, Accuracy: 89.91%\n",
      "Batch 58, Loss: 0.793457, Accuracy: 90.01%\n",
      "Batch 59, Loss: 0.829847, Accuracy: 90.04%\n",
      "Batch 60, Loss: 0.931795, Accuracy: 89.87%\n",
      "Batch 61, Loss: 0.842062, Accuracy: 89.88%\n",
      "Batch 62, Loss: 0.865410, Accuracy: 89.84%\n",
      "Batch 63, Loss: 0.855377, Accuracy: 89.83%\n",
      "Batch 64, Loss: 0.909914, Accuracy: 89.70%\n",
      "Batch 65, Loss: 0.862876, Accuracy: 89.69%\n",
      "Batch 66, Loss: 0.836585, Accuracy: 89.70%\n",
      "Batch 67, Loss: 0.885183, Accuracy: 89.65%\n",
      "Batch 68, Loss: 0.881382, Accuracy: 89.61%\n",
      "Batch 69, Loss: 0.850259, Accuracy: 89.61%\n",
      "Batch 70, Loss: 0.809337, Accuracy: 89.67%\n",
      "Batch 71, Loss: 0.835440, Accuracy: 89.70%\n",
      "Batch 72, Loss: 0.893409, Accuracy: 89.63%\n",
      "Batch 73, Loss: 0.843740, Accuracy: 89.64%\n",
      "Batch 74, Loss: 0.842008, Accuracy: 89.65%\n",
      "Batch 75, Loss: 0.812715, Accuracy: 89.71%\n",
      "Batch 76, Loss: 0.854064, Accuracy: 89.68%\n",
      "Batch 77, Loss: 0.921832, Accuracy: 89.57%\n",
      "Batch 78, Loss: 0.838825, Accuracy: 89.58%\n",
      "Batch 79, Loss: 0.853104, Accuracy: 89.58%\n",
      "Batch 80, Loss: 0.857881, Accuracy: 89.57%\n",
      "Batch 81, Loss: 0.890761, Accuracy: 89.53%\n",
      "Batch 82, Loss: 0.878797, Accuracy: 89.48%\n",
      "Batch 83, Loss: 0.754263, Accuracy: 89.61%\n",
      "Batch 84, Loss: 0.888313, Accuracy: 89.56%\n",
      "Batch 85, Loss: 0.840621, Accuracy: 89.58%\n",
      "Batch 86, Loss: 0.814173, Accuracy: 89.63%\n",
      "Batch 87, Loss: 0.870509, Accuracy: 89.60%\n",
      "Batch 88, Loss: 0.799721, Accuracy: 89.67%\n",
      "Batch 89, Loss: 0.832535, Accuracy: 89.68%\n",
      "Batch 90, Loss: 0.863526, Accuracy: 89.67%\n",
      "Batch 91, Loss: 0.892371, Accuracy: 89.61%\n",
      "Batch 92, Loss: 0.860017, Accuracy: 89.59%\n",
      "Batch 93, Loss: 0.865151, Accuracy: 89.55%\n",
      "Batch 94, Loss: 0.845953, Accuracy: 89.56%\n",
      "Batch 95, Loss: 0.823197, Accuracy: 89.59%\n",
      "Batch 96, Loss: 0.889557, Accuracy: 89.53%\n",
      "Batch 97, Loss: 0.873403, Accuracy: 89.51%\n",
      "Batch 98, Loss: 0.857196, Accuracy: 89.51%\n",
      "Batch 99, Loss: 0.808942, Accuracy: 89.55%\n",
      "Batch 100, Loss: 0.802450, Accuracy: 89.61%\n",
      "Batch 101, Loss: 0.836283, Accuracy: 89.62%\n",
      "Batch 102, Loss: 0.936362, Accuracy: 89.55%\n",
      "Batch 103, Loss: 0.829134, Accuracy: 89.58%\n",
      "Batch 104, Loss: 0.873073, Accuracy: 89.56%\n",
      "Batch 105, Loss: 0.866121, Accuracy: 89.54%\n",
      "Batch 106, Loss: 0.845413, Accuracy: 89.53%\n",
      "Batch 107, Loss: 0.852214, Accuracy: 89.54%\n",
      "Batch 108, Loss: 0.845688, Accuracy: 89.54%\n",
      "Batch 109, Loss: 0.900462, Accuracy: 89.49%\n",
      "Batch 110, Loss: 0.884655, Accuracy: 89.45%\n",
      "Batch 111, Loss: 0.825673, Accuracy: 89.47%\n",
      "Batch 112, Loss: 0.871056, Accuracy: 89.45%\n",
      "Batch 113, Loss: 0.848112, Accuracy: 89.45%\n",
      "Batch 114, Loss: 0.818234, Accuracy: 89.49%\n",
      "Batch 115, Loss: 0.838587, Accuracy: 89.50%\n",
      "Batch 116, Loss: 0.828788, Accuracy: 89.51%\n",
      "Batch 117, Loss: 0.838657, Accuracy: 89.52%\n",
      "Batch 118, Loss: 0.767847, Accuracy: 89.59%\n",
      "Batch 119, Loss: 0.887329, Accuracy: 89.56%\n",
      "Batch 120, Loss: 0.875511, Accuracy: 89.54%\n",
      "Batch 121, Loss: 0.849205, Accuracy: 89.54%\n",
      "Batch 122, Loss: 0.830381, Accuracy: 89.55%\n",
      "Batch 123, Loss: 0.886634, Accuracy: 89.52%\n",
      "Batch 124, Loss: 0.866001, Accuracy: 89.49%\n",
      "Batch 125, Loss: 0.811703, Accuracy: 89.53%\n",
      "Batch 126, Loss: 0.808798, Accuracy: 89.56%\n",
      "Batch 127, Loss: 0.814858, Accuracy: 89.59%\n",
      "Batch 128, Loss: 0.825122, Accuracy: 89.61%\n",
      "Batch 129, Loss: 0.926521, Accuracy: 89.55%\n",
      "Batch 130, Loss: 0.929962, Accuracy: 89.46%\n",
      "Batch 131, Loss: 0.883120, Accuracy: 89.43%\n",
      "Batch 132, Loss: 0.831475, Accuracy: 89.45%\n",
      "Batch 133, Loss: 0.834832, Accuracy: 89.46%\n",
      "Batch 134, Loss: 0.910894, Accuracy: 89.42%\n",
      "Batch 135, Loss: 0.895392, Accuracy: 89.40%\n",
      "Batch 136, Loss: 0.905138, Accuracy: 89.37%\n",
      "Batch 137, Loss: 0.852113, Accuracy: 89.37%\n",
      "Batch 138, Loss: 0.872404, Accuracy: 89.37%\n",
      "Batch 139, Loss: 0.924764, Accuracy: 89.31%\n",
      "Batch 140, Loss: 0.810432, Accuracy: 89.34%\n",
      "Batch 141, Loss: 0.829157, Accuracy: 89.36%\n",
      "Batch 142, Loss: 0.872974, Accuracy: 89.35%\n",
      "Batch 143, Loss: 0.884371, Accuracy: 89.32%\n",
      "Batch 144, Loss: 0.868469, Accuracy: 89.31%\n",
      "Batch 145, Loss: 0.846243, Accuracy: 89.32%\n",
      "Batch 146, Loss: 0.851423, Accuracy: 89.32%\n",
      "Batch 147, Loss: 0.915922, Accuracy: 89.26%\n",
      "Batch 148, Loss: 0.876221, Accuracy: 89.25%\n",
      "Batch 149, Loss: 0.860475, Accuracy: 89.25%\n",
      "Batch 150, Loss: 0.923441, Accuracy: 89.21%\n",
      "Batch 151, Loss: 0.853445, Accuracy: 89.21%\n",
      "Batch 152, Loss: 0.868572, Accuracy: 89.20%\n",
      "Batch 153, Loss: 0.843398, Accuracy: 89.22%\n",
      "Batch 154, Loss: 0.843746, Accuracy: 89.23%\n",
      "Batch 155, Loss: 0.811464, Accuracy: 89.26%\n",
      "Batch 156, Loss: 0.885993, Accuracy: 89.24%\n",
      "Batch 157, Loss: 0.842513, Accuracy: 89.25%\n",
      "Batch 158, Loss: 0.873219, Accuracy: 89.24%\n",
      "Batch 159, Loss: 0.829085, Accuracy: 89.26%\n",
      "Batch 160, Loss: 0.788799, Accuracy: 89.31%\n",
      "Batch 161, Loss: 0.877369, Accuracy: 89.30%\n",
      "Batch 162, Loss: 0.935331, Accuracy: 89.25%\n",
      "Batch 163, Loss: 0.888365, Accuracy: 89.24%\n",
      "Batch 164, Loss: 0.851569, Accuracy: 89.23%\n",
      "Batch 165, Loss: 0.786684, Accuracy: 89.28%\n",
      "Batch 166, Loss: 0.875805, Accuracy: 89.27%\n",
      "Batch 167, Loss: 0.869146, Accuracy: 89.26%\n",
      "Batch 168, Loss: 0.853785, Accuracy: 89.27%\n",
      "Batch 169, Loss: 0.837334, Accuracy: 89.27%\n",
      "Batch 170, Loss: 0.849924, Accuracy: 89.26%\n",
      "Batch 171, Loss: 0.820152, Accuracy: 89.28%\n",
      "Batch 172, Loss: 0.878233, Accuracy: 89.26%\n",
      "Batch 173, Loss: 0.826227, Accuracy: 89.28%\n",
      "Batch 174, Loss: 0.826316, Accuracy: 89.30%\n",
      "Batch 175, Loss: 0.821611, Accuracy: 89.31%\n",
      "Batch 176, Loss: 0.836076, Accuracy: 89.32%\n",
      "Batch 177, Loss: 0.805264, Accuracy: 89.34%\n",
      "Batch 178, Loss: 0.874818, Accuracy: 89.34%\n",
      "Batch 179, Loss: 0.857126, Accuracy: 89.34%\n",
      "Batch 180, Loss: 0.873713, Accuracy: 89.33%\n",
      "Batch 181, Loss: 0.856891, Accuracy: 89.33%\n",
      "Batch 182, Loss: 0.901526, Accuracy: 89.29%\n",
      "Batch 183, Loss: 0.849789, Accuracy: 89.30%\n",
      "Batch 184, Loss: 0.830778, Accuracy: 89.31%\n",
      "Batch 185, Loss: 0.842502, Accuracy: 89.32%\n",
      "Batch 186, Loss: 0.868532, Accuracy: 89.31%\n",
      "Batch 187, Loss: 0.836034, Accuracy: 89.31%\n",
      "Batch 188, Loss: 0.820927, Accuracy: 89.33%\n",
      "Batch 189, Loss: 0.887328, Accuracy: 89.30%\n",
      "Batch 190, Loss: 0.824016, Accuracy: 89.32%\n",
      "Batch 191, Loss: 0.868509, Accuracy: 89.30%\n",
      "Batch 192, Loss: 0.854065, Accuracy: 89.30%\n",
      "Batch 193, Loss: 0.857984, Accuracy: 89.30%\n",
      "Batch 194, Loss: 0.924775, Accuracy: 89.26%\n",
      "Batch 195, Loss: 0.847044, Accuracy: 89.25%\n",
      "Batch 196, Loss: 0.880270, Accuracy: 89.24%\n",
      "Batch 197, Loss: 0.898131, Accuracy: 89.21%\n",
      "Batch 198, Loss: 0.842432, Accuracy: 89.20%\n",
      "Batch 199, Loss: 0.851598, Accuracy: 89.20%\n",
      "Batch 200, Loss: 0.839465, Accuracy: 89.21%\n",
      "Batch 201, Loss: 0.924476, Accuracy: 89.17%\n",
      "Batch 202, Loss: 0.900952, Accuracy: 89.15%\n",
      "Batch 203, Loss: 0.878840, Accuracy: 89.13%\n",
      "Batch 204, Loss: 0.888306, Accuracy: 89.12%\n",
      "Batch 205, Loss: 0.859825, Accuracy: 89.11%\n",
      "Batch 206, Loss: 0.856674, Accuracy: 89.11%\n",
      "Batch 207, Loss: 0.822759, Accuracy: 89.12%\n",
      "Batch 208, Loss: 0.838061, Accuracy: 89.13%\n",
      "Batch 209, Loss: 0.868722, Accuracy: 89.11%\n",
      "Batch 210, Loss: 0.865347, Accuracy: 89.11%\n",
      "Batch 211, Loss: 0.925950, Accuracy: 89.07%\n",
      "Batch 212, Loss: 0.825640, Accuracy: 89.08%\n",
      "Batch 213, Loss: 0.895839, Accuracy: 89.07%\n",
      "Training - Epoch 116, Loss: 0.854297, Accuracy: 89.07%\n",
      "Validation Batch 1, Loss: 0.823081, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.792515, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.835923, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.842951, Accuracy: 92.58%\n",
      "Validation Batch 5, Loss: 0.821115, Accuracy: 92.50%\n",
      "Validation Batch 6, Loss: 0.797776, Accuracy: 92.97%\n",
      "Validation Batch 7, Loss: 0.804383, Accuracy: 93.30%\n",
      "Validation Batch 8, Loss: 0.867487, Accuracy: 92.58%\n",
      "Validation Batch 9, Loss: 0.894486, Accuracy: 91.67%\n",
      "Validation Batch 10, Loss: 0.806514, Accuracy: 91.88%\n",
      "Validation Batch 11, Loss: 0.829169, Accuracy: 91.90%\n",
      "Validation Batch 12, Loss: 0.840772, Accuracy: 91.93%\n",
      "Validation Batch 13, Loss: 0.843834, Accuracy: 91.83%\n",
      "Validation Batch 14, Loss: 0.842235, Accuracy: 91.74%\n",
      "Validation Batch 15, Loss: 0.813404, Accuracy: 91.77%\n",
      "Validation Batch 16, Loss: 0.843466, Accuracy: 91.70%\n",
      "Validation Batch 17, Loss: 0.869125, Accuracy: 91.54%\n",
      "Validation Batch 18, Loss: 0.806041, Accuracy: 91.75%\n",
      "Validation Batch 19, Loss: 0.876169, Accuracy: 91.61%\n",
      "Validation Batch 20, Loss: 0.838592, Accuracy: 91.56%\n",
      "Validation Batch 21, Loss: 0.863424, Accuracy: 91.44%\n",
      "Validation Batch 22, Loss: 0.818733, Accuracy: 91.55%\n",
      "Validation Batch 23, Loss: 0.866215, Accuracy: 91.44%\n",
      "Validation Batch 24, Loss: 0.841995, Accuracy: 91.34%\n",
      "Validation Batch 25, Loss: 0.813194, Accuracy: 91.38%\n",
      "Validation Batch 26, Loss: 0.845972, Accuracy: 91.35%\n",
      "Validation Batch 27, Loss: 0.809290, Accuracy: 91.43%\n",
      "Validation - Epoch 116, Loss: 0.835106, Accuracy: 91.43%\n",
      "Patience—1\n",
      "Epoch 117\n",
      "Batch 1, Loss: 0.874220, Accuracy: 87.50%\n",
      "Batch 2, Loss: 0.823636, Accuracy: 89.06%\n",
      "Batch 3, Loss: 0.823661, Accuracy: 90.62%\n",
      "Batch 4, Loss: 0.826383, Accuracy: 91.02%\n",
      "Batch 5, Loss: 0.889727, Accuracy: 89.69%\n",
      "Batch 6, Loss: 0.784009, Accuracy: 90.62%\n",
      "Batch 7, Loss: 0.818120, Accuracy: 91.07%\n",
      "Batch 8, Loss: 0.834101, Accuracy: 91.21%\n",
      "Batch 9, Loss: 0.821531, Accuracy: 91.32%\n",
      "Batch 10, Loss: 0.841049, Accuracy: 91.25%\n",
      "Batch 11, Loss: 0.809129, Accuracy: 91.48%\n",
      "Batch 12, Loss: 0.822190, Accuracy: 91.54%\n",
      "Batch 13, Loss: 0.917259, Accuracy: 90.75%\n",
      "Batch 14, Loss: 0.860154, Accuracy: 90.62%\n",
      "Batch 15, Loss: 0.861041, Accuracy: 90.42%\n",
      "Batch 16, Loss: 0.818932, Accuracy: 90.62%\n",
      "Batch 17, Loss: 0.854357, Accuracy: 90.44%\n",
      "Batch 18, Loss: 0.820041, Accuracy: 90.54%\n",
      "Batch 19, Loss: 0.862922, Accuracy: 90.38%\n",
      "Batch 20, Loss: 0.823416, Accuracy: 90.47%\n",
      "Batch 21, Loss: 0.797045, Accuracy: 90.70%\n",
      "Batch 22, Loss: 0.858841, Accuracy: 90.62%\n",
      "Batch 23, Loss: 0.802080, Accuracy: 90.76%\n",
      "Batch 24, Loss: 0.902488, Accuracy: 90.43%\n",
      "Batch 25, Loss: 0.871000, Accuracy: 90.31%\n",
      "Batch 26, Loss: 0.898279, Accuracy: 90.14%\n",
      "Batch 27, Loss: 0.874130, Accuracy: 89.99%\n",
      "Batch 28, Loss: 0.848870, Accuracy: 89.96%\n",
      "Batch 29, Loss: 0.856125, Accuracy: 89.92%\n",
      "Batch 30, Loss: 0.809929, Accuracy: 90.10%\n",
      "Batch 31, Loss: 0.855510, Accuracy: 90.02%\n",
      "Batch 32, Loss: 0.903703, Accuracy: 89.84%\n",
      "Batch 33, Loss: 0.813869, Accuracy: 89.91%\n",
      "Batch 34, Loss: 0.843059, Accuracy: 89.94%\n",
      "Batch 35, Loss: 0.862025, Accuracy: 89.91%\n",
      "Batch 36, Loss: 0.852798, Accuracy: 89.89%\n",
      "Batch 37, Loss: 0.860954, Accuracy: 89.86%\n",
      "Batch 38, Loss: 0.843935, Accuracy: 89.93%\n",
      "Batch 39, Loss: 0.869477, Accuracy: 89.86%\n",
      "Batch 40, Loss: 0.849008, Accuracy: 89.88%\n",
      "Batch 41, Loss: 0.812903, Accuracy: 89.98%\n",
      "Batch 42, Loss: 0.859732, Accuracy: 89.96%\n",
      "Batch 43, Loss: 0.808151, Accuracy: 90.04%\n",
      "Batch 44, Loss: 0.840801, Accuracy: 90.02%\n",
      "Batch 45, Loss: 0.900440, Accuracy: 89.93%\n",
      "Batch 46, Loss: 0.847605, Accuracy: 89.91%\n",
      "Batch 47, Loss: 0.835368, Accuracy: 89.99%\n",
      "Batch 48, Loss: 0.876293, Accuracy: 89.88%\n",
      "Batch 49, Loss: 0.848692, Accuracy: 89.86%\n",
      "Batch 50, Loss: 0.810754, Accuracy: 89.94%\n",
      "Batch 51, Loss: 0.808184, Accuracy: 90.01%\n",
      "Batch 52, Loss: 0.848924, Accuracy: 89.99%\n",
      "Batch 53, Loss: 0.803774, Accuracy: 90.09%\n",
      "Batch 54, Loss: 0.897117, Accuracy: 90.02%\n",
      "Batch 55, Loss: 0.907631, Accuracy: 89.91%\n",
      "Batch 56, Loss: 0.831207, Accuracy: 89.93%\n",
      "Batch 57, Loss: 0.825007, Accuracy: 89.97%\n",
      "Batch 58, Loss: 0.812346, Accuracy: 90.03%\n",
      "Batch 59, Loss: 0.874935, Accuracy: 89.96%\n",
      "Batch 60, Loss: 0.823811, Accuracy: 89.97%\n",
      "Batch 61, Loss: 0.868775, Accuracy: 89.93%\n",
      "Batch 62, Loss: 0.857279, Accuracy: 89.92%\n",
      "Batch 63, Loss: 0.884609, Accuracy: 89.86%\n",
      "Batch 64, Loss: 0.824134, Accuracy: 89.89%\n",
      "Batch 65, Loss: 0.889524, Accuracy: 89.81%\n",
      "Batch 66, Loss: 0.842604, Accuracy: 89.82%\n",
      "Batch 67, Loss: 0.826007, Accuracy: 89.86%\n",
      "Batch 68, Loss: 0.883306, Accuracy: 89.80%\n",
      "Batch 69, Loss: 0.817549, Accuracy: 89.83%\n",
      "Batch 70, Loss: 0.837337, Accuracy: 89.84%\n",
      "Batch 71, Loss: 0.903718, Accuracy: 89.72%\n",
      "Batch 72, Loss: 0.810202, Accuracy: 89.76%\n",
      "Batch 73, Loss: 0.844727, Accuracy: 89.75%\n",
      "Batch 74, Loss: 0.819024, Accuracy: 89.78%\n",
      "Batch 75, Loss: 0.833555, Accuracy: 89.81%\n",
      "Batch 76, Loss: 0.823327, Accuracy: 89.84%\n",
      "Batch 77, Loss: 0.903837, Accuracy: 89.77%\n",
      "Batch 78, Loss: 0.880900, Accuracy: 89.72%\n",
      "Batch 79, Loss: 0.876576, Accuracy: 89.70%\n",
      "Batch 80, Loss: 0.815579, Accuracy: 89.73%\n",
      "Batch 81, Loss: 0.836831, Accuracy: 89.74%\n",
      "Batch 82, Loss: 0.907754, Accuracy: 89.67%\n",
      "Batch 83, Loss: 0.857460, Accuracy: 89.66%\n",
      "Batch 84, Loss: 0.895725, Accuracy: 89.60%\n",
      "Batch 85, Loss: 0.933327, Accuracy: 89.49%\n",
      "Batch 86, Loss: 0.829361, Accuracy: 89.52%\n",
      "Batch 87, Loss: 0.834225, Accuracy: 89.55%\n",
      "Batch 88, Loss: 0.873813, Accuracy: 89.52%\n",
      "Batch 89, Loss: 0.865629, Accuracy: 89.50%\n",
      "Batch 90, Loss: 0.877414, Accuracy: 89.48%\n",
      "Batch 91, Loss: 0.918557, Accuracy: 89.37%\n",
      "Batch 92, Loss: 0.814424, Accuracy: 89.40%\n",
      "Batch 93, Loss: 0.874377, Accuracy: 89.36%\n",
      "Batch 94, Loss: 0.785828, Accuracy: 89.43%\n",
      "Batch 95, Loss: 0.772596, Accuracy: 89.52%\n",
      "Batch 96, Loss: 0.847062, Accuracy: 89.52%\n",
      "Batch 97, Loss: 0.836973, Accuracy: 89.53%\n",
      "Batch 98, Loss: 0.853438, Accuracy: 89.51%\n",
      "Batch 99, Loss: 0.843112, Accuracy: 89.52%\n",
      "Batch 100, Loss: 0.890887, Accuracy: 89.48%\n",
      "Batch 101, Loss: 0.911393, Accuracy: 89.42%\n",
      "Batch 102, Loss: 0.836240, Accuracy: 89.43%\n",
      "Batch 103, Loss: 0.813354, Accuracy: 89.47%\n",
      "Batch 104, Loss: 0.862412, Accuracy: 89.45%\n",
      "Batch 105, Loss: 0.834098, Accuracy: 89.48%\n",
      "Batch 106, Loss: 0.854929, Accuracy: 89.48%\n",
      "Batch 107, Loss: 0.883370, Accuracy: 89.44%\n",
      "Batch 108, Loss: 0.793907, Accuracy: 89.50%\n",
      "Batch 109, Loss: 0.862118, Accuracy: 89.46%\n",
      "Batch 110, Loss: 0.857536, Accuracy: 89.45%\n",
      "Batch 111, Loss: 0.907539, Accuracy: 89.39%\n",
      "Batch 112, Loss: 0.816131, Accuracy: 89.44%\n",
      "Batch 113, Loss: 0.918041, Accuracy: 89.38%\n",
      "Batch 114, Loss: 0.861902, Accuracy: 89.39%\n",
      "Batch 115, Loss: 0.886508, Accuracy: 89.35%\n",
      "Batch 116, Loss: 0.875939, Accuracy: 89.30%\n",
      "Batch 117, Loss: 0.840790, Accuracy: 89.32%\n",
      "Batch 118, Loss: 0.922502, Accuracy: 89.25%\n",
      "Batch 119, Loss: 0.827878, Accuracy: 89.26%\n",
      "Batch 120, Loss: 0.858814, Accuracy: 89.26%\n",
      "Batch 121, Loss: 0.849710, Accuracy: 89.26%\n",
      "Batch 122, Loss: 0.811960, Accuracy: 89.29%\n",
      "Batch 123, Loss: 0.859132, Accuracy: 89.28%\n",
      "Batch 124, Loss: 0.884384, Accuracy: 89.25%\n",
      "Batch 125, Loss: 0.841992, Accuracy: 89.26%\n",
      "Batch 126, Loss: 0.938879, Accuracy: 89.19%\n",
      "Batch 127, Loss: 0.849414, Accuracy: 89.20%\n",
      "Batch 128, Loss: 0.865317, Accuracy: 89.20%\n",
      "Batch 129, Loss: 0.858398, Accuracy: 89.20%\n",
      "Batch 130, Loss: 0.899564, Accuracy: 89.16%\n",
      "Batch 131, Loss: 0.904145, Accuracy: 89.11%\n",
      "Batch 132, Loss: 0.823396, Accuracy: 89.15%\n",
      "Batch 133, Loss: 0.833910, Accuracy: 89.17%\n",
      "Batch 134, Loss: 0.816975, Accuracy: 89.20%\n",
      "Batch 135, Loss: 0.792208, Accuracy: 89.25%\n",
      "Batch 136, Loss: 0.854130, Accuracy: 89.25%\n",
      "Batch 137, Loss: 0.971557, Accuracy: 89.15%\n",
      "Batch 138, Loss: 0.894136, Accuracy: 89.13%\n",
      "Batch 139, Loss: 0.839036, Accuracy: 89.14%\n",
      "Batch 140, Loss: 0.838213, Accuracy: 89.15%\n",
      "Batch 141, Loss: 0.875369, Accuracy: 89.14%\n",
      "Batch 142, Loss: 0.832182, Accuracy: 89.16%\n",
      "Batch 143, Loss: 0.788970, Accuracy: 89.22%\n",
      "Batch 144, Loss: 0.885600, Accuracy: 89.19%\n",
      "Batch 145, Loss: 0.912041, Accuracy: 89.15%\n",
      "Batch 146, Loss: 0.896429, Accuracy: 89.09%\n",
      "Batch 147, Loss: 0.858267, Accuracy: 89.08%\n",
      "Batch 148, Loss: 0.807518, Accuracy: 89.12%\n",
      "Batch 149, Loss: 0.871435, Accuracy: 89.10%\n",
      "Batch 150, Loss: 0.828790, Accuracy: 89.11%\n",
      "Batch 151, Loss: 0.840443, Accuracy: 89.12%\n",
      "Batch 152, Loss: 0.812096, Accuracy: 89.16%\n",
      "Batch 153, Loss: 0.871476, Accuracy: 89.14%\n",
      "Batch 154, Loss: 0.920663, Accuracy: 89.10%\n",
      "Batch 155, Loss: 0.826732, Accuracy: 89.12%\n",
      "Batch 156, Loss: 0.817581, Accuracy: 89.14%\n",
      "Batch 157, Loss: 0.862670, Accuracy: 89.12%\n",
      "Batch 158, Loss: 0.829859, Accuracy: 89.13%\n",
      "Batch 159, Loss: 0.859595, Accuracy: 89.13%\n",
      "Batch 160, Loss: 0.867717, Accuracy: 89.13%\n",
      "Batch 161, Loss: 0.848153, Accuracy: 89.13%\n",
      "Batch 162, Loss: 0.780563, Accuracy: 89.18%\n",
      "Batch 163, Loss: 0.864979, Accuracy: 89.17%\n",
      "Batch 164, Loss: 0.946043, Accuracy: 89.10%\n",
      "Batch 165, Loss: 0.841429, Accuracy: 89.11%\n",
      "Batch 166, Loss: 0.790352, Accuracy: 89.15%\n",
      "Batch 167, Loss: 0.839204, Accuracy: 89.16%\n",
      "Batch 168, Loss: 0.830133, Accuracy: 89.16%\n",
      "Batch 169, Loss: 0.919944, Accuracy: 89.12%\n",
      "Batch 170, Loss: 0.866947, Accuracy: 89.10%\n",
      "Batch 171, Loss: 0.840529, Accuracy: 89.10%\n",
      "Batch 172, Loss: 0.844377, Accuracy: 89.11%\n",
      "Batch 173, Loss: 0.854295, Accuracy: 89.12%\n",
      "Batch 174, Loss: 0.874269, Accuracy: 89.10%\n",
      "Batch 175, Loss: 0.821598, Accuracy: 89.12%\n",
      "Batch 176, Loss: 0.866590, Accuracy: 89.11%\n",
      "Batch 177, Loss: 0.827757, Accuracy: 89.12%\n",
      "Batch 178, Loss: 0.849235, Accuracy: 89.09%\n",
      "Batch 179, Loss: 0.910469, Accuracy: 89.06%\n",
      "Batch 180, Loss: 0.848776, Accuracy: 89.06%\n",
      "Batch 181, Loss: 0.831320, Accuracy: 89.08%\n",
      "Batch 182, Loss: 0.862378, Accuracy: 89.07%\n",
      "Batch 183, Loss: 0.823662, Accuracy: 89.09%\n",
      "Batch 184, Loss: 0.882021, Accuracy: 89.07%\n",
      "Batch 185, Loss: 0.846769, Accuracy: 89.08%\n",
      "Batch 186, Loss: 0.897014, Accuracy: 89.06%\n",
      "Batch 187, Loss: 0.806935, Accuracy: 89.09%\n",
      "Batch 188, Loss: 0.854239, Accuracy: 89.08%\n",
      "Batch 189, Loss: 0.875229, Accuracy: 89.07%\n",
      "Batch 190, Loss: 0.888221, Accuracy: 89.05%\n",
      "Batch 191, Loss: 0.884364, Accuracy: 89.04%\n",
      "Batch 192, Loss: 0.795819, Accuracy: 89.07%\n",
      "Batch 193, Loss: 0.809543, Accuracy: 89.09%\n",
      "Batch 194, Loss: 0.826477, Accuracy: 89.10%\n",
      "Batch 195, Loss: 0.812077, Accuracy: 89.13%\n",
      "Batch 196, Loss: 0.843410, Accuracy: 89.13%\n",
      "Batch 197, Loss: 0.801727, Accuracy: 89.15%\n",
      "Batch 198, Loss: 0.862218, Accuracy: 89.15%\n",
      "Batch 199, Loss: 0.843883, Accuracy: 89.16%\n",
      "Batch 200, Loss: 0.852649, Accuracy: 89.15%\n",
      "Batch 201, Loss: 0.858509, Accuracy: 89.15%\n",
      "Batch 202, Loss: 0.833947, Accuracy: 89.16%\n",
      "Batch 203, Loss: 0.867277, Accuracy: 89.15%\n",
      "Batch 204, Loss: 0.843468, Accuracy: 89.15%\n",
      "Batch 205, Loss: 0.803030, Accuracy: 89.18%\n",
      "Batch 206, Loss: 0.800450, Accuracy: 89.21%\n",
      "Batch 207, Loss: 0.880147, Accuracy: 89.19%\n",
      "Batch 208, Loss: 0.916694, Accuracy: 89.16%\n",
      "Batch 209, Loss: 0.877311, Accuracy: 89.15%\n",
      "Batch 210, Loss: 0.838728, Accuracy: 89.16%\n",
      "Batch 211, Loss: 0.869849, Accuracy: 89.15%\n",
      "Batch 212, Loss: 0.806632, Accuracy: 89.18%\n",
      "Batch 213, Loss: 0.848150, Accuracy: 89.19%\n",
      "Training - Epoch 117, Loss: 0.851883, Accuracy: 89.19%\n",
      "Validation Batch 1, Loss: 0.811866, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.792098, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.828835, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.823341, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.806857, Accuracy: 93.44%\n",
      "Validation Batch 6, Loss: 0.791537, Accuracy: 93.75%\n",
      "Validation Batch 7, Loss: 0.794841, Accuracy: 93.97%\n",
      "Validation Batch 8, Loss: 0.860212, Accuracy: 93.36%\n",
      "Validation Batch 9, Loss: 0.877093, Accuracy: 92.71%\n",
      "Validation Batch 10, Loss: 0.800841, Accuracy: 92.81%\n",
      "Validation Batch 11, Loss: 0.820117, Accuracy: 92.76%\n",
      "Validation Batch 12, Loss: 0.827646, Accuracy: 92.71%\n",
      "Validation Batch 13, Loss: 0.823093, Accuracy: 92.79%\n",
      "Validation Batch 14, Loss: 0.834256, Accuracy: 92.75%\n",
      "Validation Batch 15, Loss: 0.799599, Accuracy: 92.81%\n",
      "Validation Batch 16, Loss: 0.818107, Accuracy: 92.87%\n",
      "Validation Batch 17, Loss: 0.864847, Accuracy: 92.65%\n",
      "Validation Batch 18, Loss: 0.794101, Accuracy: 92.80%\n",
      "Validation Batch 19, Loss: 0.869895, Accuracy: 92.52%\n",
      "Validation Batch 20, Loss: 0.801153, Accuracy: 92.66%\n",
      "Validation Batch 21, Loss: 0.856967, Accuracy: 92.49%\n",
      "Validation Batch 22, Loss: 0.816479, Accuracy: 92.40%\n",
      "Validation Batch 23, Loss: 0.858560, Accuracy: 92.26%\n",
      "Validation Batch 24, Loss: 0.827829, Accuracy: 92.19%\n",
      "Validation Batch 25, Loss: 0.804723, Accuracy: 92.25%\n",
      "Validation Batch 26, Loss: 0.834506, Accuracy: 92.25%\n",
      "Validation Batch 27, Loss: 0.800931, Accuracy: 92.31%\n",
      "Validation - Epoch 117, Loss: 0.823716, Accuracy: 92.31%\n",
      "Patience—2\n",
      "Epoch 118\n",
      "Batch 1, Loss: 0.791201, Accuracy: 95.31%\n",
      "Batch 2, Loss: 0.810342, Accuracy: 93.75%\n",
      "Batch 3, Loss: 0.853854, Accuracy: 92.19%\n",
      "Batch 4, Loss: 0.898389, Accuracy: 89.84%\n",
      "Batch 5, Loss: 0.845130, Accuracy: 90.00%\n",
      "Batch 6, Loss: 0.861333, Accuracy: 89.84%\n",
      "Batch 7, Loss: 0.964312, Accuracy: 87.95%\n",
      "Batch 8, Loss: 0.830254, Accuracy: 88.28%\n",
      "Batch 9, Loss: 0.846352, Accuracy: 88.54%\n",
      "Batch 10, Loss: 0.836330, Accuracy: 88.91%\n",
      "Batch 11, Loss: 0.782292, Accuracy: 89.63%\n",
      "Batch 12, Loss: 0.824071, Accuracy: 89.97%\n",
      "Batch 13, Loss: 0.859431, Accuracy: 89.90%\n",
      "Batch 14, Loss: 0.845439, Accuracy: 89.96%\n",
      "Batch 15, Loss: 0.827603, Accuracy: 90.10%\n",
      "Batch 16, Loss: 0.835767, Accuracy: 90.14%\n",
      "Batch 17, Loss: 0.868269, Accuracy: 89.98%\n",
      "Batch 18, Loss: 0.811658, Accuracy: 90.19%\n",
      "Batch 19, Loss: 0.889029, Accuracy: 89.97%\n",
      "Batch 20, Loss: 0.830403, Accuracy: 90.08%\n",
      "Batch 21, Loss: 0.820162, Accuracy: 90.18%\n",
      "Batch 22, Loss: 0.873058, Accuracy: 90.13%\n",
      "Batch 23, Loss: 0.861638, Accuracy: 90.08%\n",
      "Batch 24, Loss: 0.865230, Accuracy: 89.91%\n",
      "Batch 25, Loss: 0.838497, Accuracy: 89.94%\n",
      "Batch 26, Loss: 0.797209, Accuracy: 90.14%\n",
      "Batch 27, Loss: 0.801145, Accuracy: 90.28%\n",
      "Batch 28, Loss: 0.796625, Accuracy: 90.46%\n",
      "Batch 29, Loss: 0.802017, Accuracy: 90.62%\n",
      "Batch 30, Loss: 0.815430, Accuracy: 90.73%\n",
      "Batch 31, Loss: 0.843321, Accuracy: 90.73%\n",
      "Batch 32, Loss: 0.813874, Accuracy: 90.87%\n",
      "Batch 33, Loss: 0.866184, Accuracy: 90.77%\n",
      "Batch 34, Loss: 0.910858, Accuracy: 90.49%\n",
      "Batch 35, Loss: 0.868015, Accuracy: 90.40%\n",
      "Batch 36, Loss: 0.827793, Accuracy: 90.45%\n",
      "Batch 37, Loss: 0.811059, Accuracy: 90.54%\n",
      "Batch 38, Loss: 0.818016, Accuracy: 90.62%\n",
      "Batch 39, Loss: 0.861266, Accuracy: 90.58%\n",
      "Batch 40, Loss: 0.839880, Accuracy: 90.55%\n",
      "Batch 41, Loss: 0.894546, Accuracy: 90.43%\n",
      "Batch 42, Loss: 0.850424, Accuracy: 90.40%\n",
      "Batch 43, Loss: 0.876815, Accuracy: 90.26%\n",
      "Batch 44, Loss: 0.869514, Accuracy: 90.16%\n",
      "Batch 45, Loss: 0.902252, Accuracy: 90.03%\n",
      "Batch 46, Loss: 0.796471, Accuracy: 90.15%\n",
      "Batch 47, Loss: 0.857789, Accuracy: 90.13%\n",
      "Batch 48, Loss: 0.851460, Accuracy: 90.10%\n",
      "Batch 49, Loss: 0.894818, Accuracy: 89.96%\n",
      "Batch 50, Loss: 0.851834, Accuracy: 89.97%\n",
      "Batch 51, Loss: 0.779987, Accuracy: 90.10%\n",
      "Batch 52, Loss: 0.872315, Accuracy: 90.05%\n",
      "Batch 53, Loss: 0.874696, Accuracy: 89.98%\n",
      "Batch 54, Loss: 0.870113, Accuracy: 89.93%\n",
      "Batch 55, Loss: 0.813261, Accuracy: 90.00%\n",
      "Batch 56, Loss: 0.830529, Accuracy: 90.01%\n",
      "Batch 57, Loss: 0.855612, Accuracy: 89.99%\n",
      "Batch 58, Loss: 0.819930, Accuracy: 90.03%\n",
      "Batch 59, Loss: 0.860304, Accuracy: 89.99%\n",
      "Batch 60, Loss: 0.875454, Accuracy: 89.92%\n",
      "Batch 61, Loss: 0.831834, Accuracy: 89.93%\n",
      "Batch 62, Loss: 0.840580, Accuracy: 89.94%\n",
      "Batch 63, Loss: 0.875312, Accuracy: 89.91%\n",
      "Batch 64, Loss: 0.805454, Accuracy: 89.97%\n",
      "Batch 65, Loss: 0.810453, Accuracy: 90.02%\n",
      "Batch 66, Loss: 0.845707, Accuracy: 90.03%\n",
      "Batch 67, Loss: 0.884995, Accuracy: 89.97%\n",
      "Batch 68, Loss: 0.869589, Accuracy: 89.94%\n",
      "Batch 69, Loss: 0.818022, Accuracy: 89.99%\n",
      "Batch 70, Loss: 0.869940, Accuracy: 89.93%\n",
      "Batch 71, Loss: 0.803228, Accuracy: 90.01%\n",
      "Batch 72, Loss: 0.830854, Accuracy: 90.02%\n",
      "Batch 73, Loss: 0.868431, Accuracy: 89.96%\n",
      "Batch 74, Loss: 0.825665, Accuracy: 89.99%\n",
      "Batch 75, Loss: 0.820846, Accuracy: 90.02%\n",
      "Batch 76, Loss: 0.875814, Accuracy: 89.99%\n",
      "Batch 77, Loss: 0.907666, Accuracy: 89.91%\n",
      "Batch 78, Loss: 0.781983, Accuracy: 90.00%\n",
      "Batch 79, Loss: 0.853980, Accuracy: 89.99%\n",
      "Batch 80, Loss: 0.880557, Accuracy: 89.94%\n",
      "Batch 81, Loss: 0.883282, Accuracy: 89.89%\n",
      "Batch 82, Loss: 0.895723, Accuracy: 89.84%\n",
      "Batch 83, Loss: 0.896771, Accuracy: 89.80%\n",
      "Batch 84, Loss: 0.803012, Accuracy: 89.84%\n",
      "Batch 85, Loss: 0.836316, Accuracy: 89.85%\n",
      "Batch 86, Loss: 0.850320, Accuracy: 89.86%\n",
      "Batch 87, Loss: 0.882774, Accuracy: 89.82%\n",
      "Batch 88, Loss: 0.806203, Accuracy: 89.86%\n",
      "Batch 89, Loss: 0.874169, Accuracy: 89.82%\n",
      "Batch 90, Loss: 0.864944, Accuracy: 89.81%\n",
      "Batch 91, Loss: 0.872839, Accuracy: 89.78%\n",
      "Batch 92, Loss: 0.903933, Accuracy: 89.72%\n",
      "Batch 93, Loss: 0.830339, Accuracy: 89.73%\n",
      "Batch 94, Loss: 0.863036, Accuracy: 89.73%\n",
      "Batch 95, Loss: 0.821172, Accuracy: 89.77%\n",
      "Batch 96, Loss: 0.848070, Accuracy: 89.78%\n",
      "Batch 97, Loss: 0.887735, Accuracy: 89.72%\n",
      "Batch 98, Loss: 0.842156, Accuracy: 89.72%\n",
      "Batch 99, Loss: 0.868993, Accuracy: 89.69%\n",
      "Batch 100, Loss: 0.812200, Accuracy: 89.73%\n",
      "Batch 101, Loss: 0.836637, Accuracy: 89.73%\n",
      "Batch 102, Loss: 0.822152, Accuracy: 89.75%\n",
      "Batch 103, Loss: 0.806023, Accuracy: 89.79%\n",
      "Batch 104, Loss: 0.911982, Accuracy: 89.69%\n",
      "Batch 105, Loss: 0.793658, Accuracy: 89.75%\n",
      "Batch 106, Loss: 0.811538, Accuracy: 89.78%\n",
      "Batch 107, Loss: 0.821883, Accuracy: 89.82%\n",
      "Batch 108, Loss: 0.839285, Accuracy: 89.83%\n",
      "Batch 109, Loss: 0.857144, Accuracy: 89.82%\n",
      "Batch 110, Loss: 0.896644, Accuracy: 89.76%\n",
      "Batch 111, Loss: 0.914385, Accuracy: 89.71%\n",
      "Batch 112, Loss: 0.889972, Accuracy: 89.68%\n",
      "Batch 113, Loss: 0.860963, Accuracy: 89.67%\n",
      "Batch 114, Loss: 0.889220, Accuracy: 89.62%\n",
      "Batch 115, Loss: 0.943838, Accuracy: 89.54%\n",
      "Batch 116, Loss: 0.953005, Accuracy: 89.43%\n",
      "Batch 117, Loss: 0.844977, Accuracy: 89.44%\n",
      "Batch 118, Loss: 0.845138, Accuracy: 89.45%\n",
      "Batch 119, Loss: 0.895325, Accuracy: 89.42%\n",
      "Batch 120, Loss: 0.883190, Accuracy: 89.38%\n",
      "Batch 121, Loss: 0.774754, Accuracy: 89.44%\n",
      "Batch 122, Loss: 0.885931, Accuracy: 89.42%\n",
      "Batch 123, Loss: 0.829921, Accuracy: 89.43%\n",
      "Batch 124, Loss: 0.818804, Accuracy: 89.47%\n",
      "Batch 125, Loss: 0.794334, Accuracy: 89.51%\n",
      "Batch 126, Loss: 0.805944, Accuracy: 89.55%\n",
      "Batch 127, Loss: 0.920790, Accuracy: 89.48%\n",
      "Batch 128, Loss: 0.914238, Accuracy: 89.43%\n",
      "Batch 129, Loss: 0.915759, Accuracy: 89.37%\n",
      "Batch 130, Loss: 0.909897, Accuracy: 89.31%\n",
      "Batch 131, Loss: 0.763000, Accuracy: 89.38%\n",
      "Batch 132, Loss: 0.853967, Accuracy: 89.39%\n",
      "Batch 133, Loss: 0.844596, Accuracy: 89.40%\n",
      "Batch 134, Loss: 0.872934, Accuracy: 89.39%\n",
      "Batch 135, Loss: 0.842199, Accuracy: 89.41%\n",
      "Batch 136, Loss: 0.898945, Accuracy: 89.37%\n",
      "Batch 137, Loss: 0.873105, Accuracy: 89.36%\n",
      "Batch 138, Loss: 0.900152, Accuracy: 89.32%\n",
      "Batch 139, Loss: 0.821501, Accuracy: 89.35%\n",
      "Batch 140, Loss: 0.892097, Accuracy: 89.33%\n",
      "Batch 141, Loss: 0.819979, Accuracy: 89.35%\n",
      "Batch 142, Loss: 0.927634, Accuracy: 89.27%\n",
      "Batch 143, Loss: 0.856883, Accuracy: 89.27%\n",
      "Batch 144, Loss: 0.891275, Accuracy: 89.24%\n",
      "Batch 145, Loss: 0.863273, Accuracy: 89.22%\n",
      "Batch 146, Loss: 0.829407, Accuracy: 89.23%\n",
      "Batch 147, Loss: 0.915225, Accuracy: 89.19%\n",
      "Batch 148, Loss: 0.879068, Accuracy: 89.17%\n",
      "Batch 149, Loss: 0.875602, Accuracy: 89.17%\n",
      "Batch 150, Loss: 0.842915, Accuracy: 89.18%\n",
      "Batch 151, Loss: 0.858021, Accuracy: 89.17%\n",
      "Batch 152, Loss: 0.811798, Accuracy: 89.20%\n",
      "Batch 153, Loss: 0.851880, Accuracy: 89.20%\n",
      "Batch 154, Loss: 0.836866, Accuracy: 89.19%\n",
      "Batch 155, Loss: 0.836460, Accuracy: 89.20%\n",
      "Batch 156, Loss: 0.821743, Accuracy: 89.22%\n",
      "Batch 157, Loss: 0.792290, Accuracy: 89.26%\n",
      "Batch 158, Loss: 0.852194, Accuracy: 89.27%\n",
      "Batch 159, Loss: 0.833191, Accuracy: 89.28%\n",
      "Batch 160, Loss: 0.764817, Accuracy: 89.34%\n",
      "Batch 161, Loss: 0.851842, Accuracy: 89.32%\n",
      "Batch 162, Loss: 0.839334, Accuracy: 89.33%\n",
      "Batch 163, Loss: 0.885893, Accuracy: 89.30%\n",
      "Batch 164, Loss: 0.857668, Accuracy: 89.30%\n",
      "Batch 165, Loss: 0.885540, Accuracy: 89.27%\n",
      "Batch 166, Loss: 0.815142, Accuracy: 89.30%\n",
      "Batch 167, Loss: 0.933198, Accuracy: 89.23%\n",
      "Batch 168, Loss: 0.860708, Accuracy: 89.23%\n",
      "Batch 169, Loss: 0.840968, Accuracy: 89.23%\n",
      "Batch 170, Loss: 0.887652, Accuracy: 89.21%\n",
      "Batch 171, Loss: 0.849093, Accuracy: 89.21%\n",
      "Batch 172, Loss: 0.904337, Accuracy: 89.18%\n",
      "Batch 173, Loss: 0.879820, Accuracy: 89.15%\n",
      "Batch 174, Loss: 0.822040, Accuracy: 89.18%\n",
      "Batch 175, Loss: 0.773141, Accuracy: 89.22%\n",
      "Batch 176, Loss: 0.794630, Accuracy: 89.26%\n",
      "Batch 177, Loss: 0.912030, Accuracy: 89.22%\n",
      "Batch 178, Loss: 0.818826, Accuracy: 89.24%\n",
      "Batch 179, Loss: 0.839375, Accuracy: 89.24%\n",
      "Batch 180, Loss: 0.954989, Accuracy: 89.18%\n",
      "Batch 181, Loss: 0.868625, Accuracy: 89.17%\n",
      "Batch 182, Loss: 0.783625, Accuracy: 89.22%\n",
      "Batch 183, Loss: 0.826770, Accuracy: 89.22%\n",
      "Batch 184, Loss: 0.854208, Accuracy: 89.22%\n",
      "Batch 185, Loss: 0.838384, Accuracy: 89.22%\n",
      "Batch 186, Loss: 0.818899, Accuracy: 89.24%\n",
      "Batch 187, Loss: 0.829364, Accuracy: 89.25%\n",
      "Batch 188, Loss: 0.905934, Accuracy: 89.22%\n",
      "Batch 189, Loss: 0.824035, Accuracy: 89.24%\n",
      "Batch 190, Loss: 0.834383, Accuracy: 89.25%\n",
      "Batch 191, Loss: 0.805742, Accuracy: 89.28%\n",
      "Batch 192, Loss: 0.837680, Accuracy: 89.30%\n",
      "Batch 193, Loss: 0.846460, Accuracy: 89.30%\n",
      "Batch 194, Loss: 0.833743, Accuracy: 89.30%\n",
      "Batch 195, Loss: 0.791872, Accuracy: 89.33%\n",
      "Batch 196, Loss: 0.800521, Accuracy: 89.37%\n",
      "Batch 197, Loss: 0.877237, Accuracy: 89.35%\n",
      "Batch 198, Loss: 0.876773, Accuracy: 89.35%\n",
      "Batch 199, Loss: 0.832765, Accuracy: 89.35%\n",
      "Batch 200, Loss: 0.799439, Accuracy: 89.38%\n",
      "Batch 201, Loss: 0.839335, Accuracy: 89.38%\n",
      "Batch 202, Loss: 0.857160, Accuracy: 89.37%\n",
      "Batch 203, Loss: 0.808111, Accuracy: 89.39%\n",
      "Batch 204, Loss: 0.846277, Accuracy: 89.40%\n",
      "Batch 205, Loss: 0.895254, Accuracy: 89.37%\n",
      "Batch 206, Loss: 0.853128, Accuracy: 89.37%\n",
      "Batch 207, Loss: 0.837666, Accuracy: 89.38%\n",
      "Batch 208, Loss: 0.826629, Accuracy: 89.39%\n",
      "Batch 209, Loss: 0.840501, Accuracy: 89.40%\n",
      "Batch 210, Loss: 0.835695, Accuracy: 89.40%\n",
      "Batch 211, Loss: 0.889118, Accuracy: 89.39%\n",
      "Batch 212, Loss: 0.864324, Accuracy: 89.39%\n",
      "Batch 213, Loss: 0.894926, Accuracy: 89.37%\n",
      "Training - Epoch 118, Loss: 0.850596, Accuracy: 89.37%\n",
      "Validation Batch 1, Loss: 0.823191, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.795554, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.834254, Accuracy: 93.23%\n",
      "Validation Batch 4, Loss: 0.843252, Accuracy: 92.19%\n",
      "Validation Batch 5, Loss: 0.826065, Accuracy: 92.19%\n",
      "Validation Batch 6, Loss: 0.797313, Accuracy: 92.71%\n",
      "Validation Batch 7, Loss: 0.801299, Accuracy: 93.08%\n",
      "Validation Batch 8, Loss: 0.873080, Accuracy: 92.19%\n",
      "Validation Batch 9, Loss: 0.895214, Accuracy: 91.32%\n",
      "Validation Batch 10, Loss: 0.803493, Accuracy: 91.56%\n",
      "Validation Batch 11, Loss: 0.831397, Accuracy: 91.62%\n",
      "Validation Batch 12, Loss: 0.832992, Accuracy: 91.67%\n",
      "Validation Batch 13, Loss: 0.849016, Accuracy: 91.47%\n",
      "Validation Batch 14, Loss: 0.839550, Accuracy: 91.52%\n",
      "Validation Batch 15, Loss: 0.807432, Accuracy: 91.67%\n",
      "Validation Batch 16, Loss: 0.843521, Accuracy: 91.50%\n",
      "Validation Batch 17, Loss: 0.873410, Accuracy: 91.18%\n",
      "Validation Batch 18, Loss: 0.803035, Accuracy: 91.41%\n",
      "Validation Batch 19, Loss: 0.880685, Accuracy: 91.12%\n",
      "Validation Batch 20, Loss: 0.825456, Accuracy: 91.17%\n",
      "Validation Batch 21, Loss: 0.862335, Accuracy: 91.07%\n",
      "Validation Batch 22, Loss: 0.830180, Accuracy: 91.05%\n",
      "Validation Batch 23, Loss: 0.871266, Accuracy: 90.90%\n",
      "Validation Batch 24, Loss: 0.841056, Accuracy: 90.82%\n",
      "Validation Batch 25, Loss: 0.805879, Accuracy: 90.94%\n",
      "Validation Batch 26, Loss: 0.845179, Accuracy: 90.93%\n",
      "Validation Batch 27, Loss: 0.804650, Accuracy: 91.02%\n",
      "Validation - Epoch 118, Loss: 0.834806, Accuracy: 91.02%\n",
      "Patience—3\n",
      "Epoch 119\n",
      "Batch 1, Loss: 0.863175, Accuracy: 89.06%\n",
      "Batch 2, Loss: 0.817590, Accuracy: 90.62%\n",
      "Batch 3, Loss: 0.861342, Accuracy: 90.10%\n",
      "Batch 4, Loss: 0.815974, Accuracy: 91.02%\n",
      "Batch 5, Loss: 0.867926, Accuracy: 90.31%\n",
      "Batch 6, Loss: 0.889030, Accuracy: 89.58%\n",
      "Batch 7, Loss: 0.825713, Accuracy: 89.96%\n",
      "Batch 8, Loss: 0.855762, Accuracy: 89.84%\n",
      "Batch 9, Loss: 0.845021, Accuracy: 89.76%\n",
      "Batch 10, Loss: 0.870218, Accuracy: 89.53%\n",
      "Batch 11, Loss: 0.824831, Accuracy: 89.77%\n",
      "Batch 12, Loss: 0.812575, Accuracy: 89.97%\n",
      "Batch 13, Loss: 0.828498, Accuracy: 90.14%\n",
      "Batch 14, Loss: 0.792790, Accuracy: 90.51%\n",
      "Batch 15, Loss: 0.873547, Accuracy: 90.21%\n",
      "Batch 16, Loss: 0.821527, Accuracy: 90.43%\n",
      "Batch 17, Loss: 0.884888, Accuracy: 90.07%\n",
      "Batch 18, Loss: 0.814723, Accuracy: 90.28%\n",
      "Batch 19, Loss: 0.828934, Accuracy: 90.30%\n",
      "Batch 20, Loss: 0.832440, Accuracy: 90.31%\n",
      "Batch 21, Loss: 0.869554, Accuracy: 90.18%\n",
      "Batch 22, Loss: 0.838267, Accuracy: 90.20%\n",
      "Batch 23, Loss: 0.788970, Accuracy: 90.49%\n",
      "Batch 24, Loss: 0.880367, Accuracy: 90.36%\n",
      "Batch 25, Loss: 0.872160, Accuracy: 90.25%\n",
      "Batch 26, Loss: 0.892394, Accuracy: 90.08%\n",
      "Batch 27, Loss: 0.843372, Accuracy: 90.10%\n",
      "Batch 28, Loss: 0.851296, Accuracy: 90.07%\n",
      "Batch 29, Loss: 0.819828, Accuracy: 90.14%\n",
      "Batch 30, Loss: 0.819045, Accuracy: 90.26%\n",
      "Batch 31, Loss: 0.887089, Accuracy: 90.12%\n",
      "Batch 32, Loss: 0.836464, Accuracy: 90.14%\n",
      "Batch 33, Loss: 0.813500, Accuracy: 90.25%\n",
      "Batch 34, Loss: 0.801770, Accuracy: 90.40%\n",
      "Batch 35, Loss: 0.932224, Accuracy: 90.13%\n",
      "Batch 36, Loss: 0.915799, Accuracy: 89.89%\n",
      "Batch 37, Loss: 0.849480, Accuracy: 89.86%\n",
      "Batch 38, Loss: 0.830663, Accuracy: 89.93%\n",
      "Batch 39, Loss: 0.858589, Accuracy: 89.90%\n",
      "Batch 40, Loss: 0.870117, Accuracy: 89.80%\n",
      "Batch 41, Loss: 0.844624, Accuracy: 89.86%\n",
      "Batch 42, Loss: 0.856830, Accuracy: 89.81%\n",
      "Batch 43, Loss: 0.792766, Accuracy: 89.93%\n",
      "Batch 44, Loss: 0.823637, Accuracy: 89.95%\n",
      "Batch 45, Loss: 0.884969, Accuracy: 89.86%\n",
      "Batch 46, Loss: 0.870798, Accuracy: 89.81%\n",
      "Batch 47, Loss: 0.799334, Accuracy: 89.93%\n",
      "Batch 48, Loss: 0.830155, Accuracy: 89.97%\n",
      "Batch 49, Loss: 0.800307, Accuracy: 90.05%\n",
      "Batch 50, Loss: 0.817428, Accuracy: 90.09%\n",
      "Batch 51, Loss: 0.864865, Accuracy: 90.01%\n",
      "Batch 52, Loss: 0.877031, Accuracy: 89.96%\n",
      "Batch 53, Loss: 0.798249, Accuracy: 90.06%\n",
      "Batch 54, Loss: 0.835364, Accuracy: 90.08%\n",
      "Batch 55, Loss: 0.870327, Accuracy: 90.03%\n",
      "Batch 56, Loss: 0.866556, Accuracy: 89.98%\n",
      "Batch 57, Loss: 0.892512, Accuracy: 89.91%\n",
      "Batch 58, Loss: 0.852428, Accuracy: 89.90%\n",
      "Batch 59, Loss: 0.838399, Accuracy: 89.91%\n",
      "Batch 60, Loss: 0.861836, Accuracy: 89.87%\n",
      "Batch 61, Loss: 0.860293, Accuracy: 89.83%\n",
      "Batch 62, Loss: 0.872992, Accuracy: 89.79%\n",
      "Batch 63, Loss: 0.845847, Accuracy: 89.81%\n",
      "Batch 64, Loss: 0.872743, Accuracy: 89.77%\n",
      "Batch 65, Loss: 0.853445, Accuracy: 89.76%\n",
      "Batch 66, Loss: 0.843018, Accuracy: 89.77%\n",
      "Batch 67, Loss: 0.806087, Accuracy: 89.86%\n",
      "Batch 68, Loss: 0.909435, Accuracy: 89.75%\n",
      "Batch 69, Loss: 0.882878, Accuracy: 89.67%\n",
      "Batch 70, Loss: 0.902186, Accuracy: 89.58%\n",
      "Batch 71, Loss: 0.881333, Accuracy: 89.52%\n",
      "Batch 72, Loss: 0.851362, Accuracy: 89.52%\n",
      "Batch 73, Loss: 0.891539, Accuracy: 89.47%\n",
      "Batch 74, Loss: 0.862760, Accuracy: 89.44%\n",
      "Batch 75, Loss: 0.812978, Accuracy: 89.50%\n",
      "Batch 76, Loss: 0.874677, Accuracy: 89.47%\n",
      "Batch 77, Loss: 0.854670, Accuracy: 89.47%\n",
      "Batch 78, Loss: 0.879456, Accuracy: 89.46%\n",
      "Batch 79, Loss: 0.817338, Accuracy: 89.52%\n",
      "Batch 80, Loss: 0.891869, Accuracy: 89.45%\n",
      "Batch 81, Loss: 0.884318, Accuracy: 89.41%\n",
      "Batch 82, Loss: 0.872063, Accuracy: 89.39%\n",
      "Batch 83, Loss: 0.856622, Accuracy: 89.36%\n",
      "Batch 84, Loss: 0.891722, Accuracy: 89.32%\n",
      "Batch 85, Loss: 0.822977, Accuracy: 89.36%\n",
      "Batch 86, Loss: 0.835194, Accuracy: 89.39%\n",
      "Batch 87, Loss: 0.813992, Accuracy: 89.42%\n",
      "Batch 88, Loss: 0.864688, Accuracy: 89.42%\n",
      "Batch 89, Loss: 0.885496, Accuracy: 89.36%\n",
      "Batch 90, Loss: 0.822524, Accuracy: 89.39%\n",
      "Batch 91, Loss: 0.809320, Accuracy: 89.46%\n",
      "Batch 92, Loss: 0.887391, Accuracy: 89.42%\n",
      "Batch 93, Loss: 0.866249, Accuracy: 89.42%\n",
      "Batch 94, Loss: 0.803962, Accuracy: 89.46%\n",
      "Batch 95, Loss: 0.818891, Accuracy: 89.51%\n",
      "Batch 96, Loss: 0.862827, Accuracy: 89.50%\n",
      "Batch 97, Loss: 0.806651, Accuracy: 89.55%\n",
      "Batch 98, Loss: 0.826628, Accuracy: 89.57%\n",
      "Batch 99, Loss: 0.839854, Accuracy: 89.57%\n",
      "Batch 100, Loss: 0.882029, Accuracy: 89.53%\n",
      "Batch 101, Loss: 0.848210, Accuracy: 89.53%\n",
      "Batch 102, Loss: 0.796927, Accuracy: 89.57%\n",
      "Batch 103, Loss: 0.787051, Accuracy: 89.62%\n",
      "Batch 104, Loss: 0.835741, Accuracy: 89.63%\n",
      "Batch 105, Loss: 0.874161, Accuracy: 89.58%\n",
      "Batch 106, Loss: 0.800885, Accuracy: 89.62%\n",
      "Batch 107, Loss: 0.795767, Accuracy: 89.68%\n",
      "Batch 108, Loss: 0.805949, Accuracy: 89.71%\n",
      "Batch 109, Loss: 0.815229, Accuracy: 89.75%\n",
      "Batch 110, Loss: 0.888579, Accuracy: 89.69%\n",
      "Batch 111, Loss: 0.849867, Accuracy: 89.70%\n",
      "Batch 112, Loss: 0.868298, Accuracy: 89.68%\n",
      "Batch 113, Loss: 0.853417, Accuracy: 89.66%\n",
      "Batch 114, Loss: 0.828108, Accuracy: 89.67%\n",
      "Batch 115, Loss: 0.796482, Accuracy: 89.71%\n",
      "Batch 116, Loss: 0.801796, Accuracy: 89.75%\n",
      "Batch 117, Loss: 0.819746, Accuracy: 89.76%\n",
      "Batch 118, Loss: 0.757072, Accuracy: 89.84%\n",
      "Batch 119, Loss: 0.748184, Accuracy: 89.93%\n",
      "Batch 120, Loss: 0.861118, Accuracy: 89.91%\n",
      "Batch 121, Loss: 0.895903, Accuracy: 89.86%\n",
      "Batch 122, Loss: 0.894617, Accuracy: 89.82%\n",
      "Batch 123, Loss: 0.806007, Accuracy: 89.85%\n",
      "Batch 124, Loss: 0.894485, Accuracy: 89.81%\n",
      "Batch 125, Loss: 0.890240, Accuracy: 89.76%\n",
      "Batch 126, Loss: 0.885893, Accuracy: 89.73%\n",
      "Batch 127, Loss: 0.863513, Accuracy: 89.71%\n",
      "Batch 128, Loss: 0.818085, Accuracy: 89.75%\n",
      "Batch 129, Loss: 0.793060, Accuracy: 89.79%\n",
      "Batch 130, Loss: 0.892985, Accuracy: 89.76%\n",
      "Batch 131, Loss: 0.876855, Accuracy: 89.74%\n",
      "Batch 132, Loss: 0.858195, Accuracy: 89.73%\n",
      "Batch 133, Loss: 0.847043, Accuracy: 89.73%\n",
      "Batch 134, Loss: 0.880120, Accuracy: 89.70%\n",
      "Batch 135, Loss: 0.864295, Accuracy: 89.69%\n",
      "Batch 136, Loss: 0.859920, Accuracy: 89.67%\n",
      "Batch 137, Loss: 0.824625, Accuracy: 89.69%\n",
      "Batch 138, Loss: 0.867125, Accuracy: 89.67%\n",
      "Batch 139, Loss: 0.843350, Accuracy: 89.68%\n",
      "Batch 140, Loss: 0.816984, Accuracy: 89.71%\n",
      "Batch 141, Loss: 0.812588, Accuracy: 89.74%\n",
      "Batch 142, Loss: 0.824723, Accuracy: 89.76%\n",
      "Batch 143, Loss: 0.910711, Accuracy: 89.72%\n",
      "Batch 144, Loss: 0.842886, Accuracy: 89.71%\n",
      "Batch 145, Loss: 0.832743, Accuracy: 89.72%\n",
      "Batch 146, Loss: 0.871297, Accuracy: 89.70%\n",
      "Batch 147, Loss: 0.910673, Accuracy: 89.67%\n",
      "Batch 148, Loss: 0.835761, Accuracy: 89.67%\n",
      "Batch 149, Loss: 0.826687, Accuracy: 89.68%\n",
      "Batch 150, Loss: 0.817084, Accuracy: 89.70%\n",
      "Batch 151, Loss: 0.844835, Accuracy: 89.70%\n",
      "Batch 152, Loss: 0.825176, Accuracy: 89.73%\n",
      "Batch 153, Loss: 0.844031, Accuracy: 89.74%\n",
      "Batch 154, Loss: 0.864699, Accuracy: 89.73%\n",
      "Batch 155, Loss: 0.854299, Accuracy: 89.73%\n",
      "Batch 156, Loss: 0.815345, Accuracy: 89.76%\n",
      "Batch 157, Loss: 0.884726, Accuracy: 89.76%\n",
      "Batch 158, Loss: 0.832082, Accuracy: 89.77%\n",
      "Batch 159, Loss: 0.843506, Accuracy: 89.78%\n",
      "Batch 160, Loss: 0.809986, Accuracy: 89.80%\n",
      "Batch 161, Loss: 0.855039, Accuracy: 89.80%\n",
      "Batch 162, Loss: 0.840123, Accuracy: 89.81%\n",
      "Batch 163, Loss: 0.846768, Accuracy: 89.81%\n",
      "Batch 164, Loss: 0.801513, Accuracy: 89.84%\n",
      "Batch 165, Loss: 0.862484, Accuracy: 89.85%\n",
      "Batch 166, Loss: 0.934688, Accuracy: 89.79%\n",
      "Batch 167, Loss: 0.925316, Accuracy: 89.74%\n",
      "Batch 168, Loss: 0.848651, Accuracy: 89.73%\n",
      "Batch 169, Loss: 0.834579, Accuracy: 89.74%\n",
      "Batch 170, Loss: 0.849051, Accuracy: 89.72%\n",
      "Batch 171, Loss: 0.876216, Accuracy: 89.70%\n",
      "Batch 172, Loss: 0.849354, Accuracy: 89.70%\n",
      "Batch 173, Loss: 0.826872, Accuracy: 89.71%\n",
      "Batch 174, Loss: 0.817781, Accuracy: 89.74%\n",
      "Batch 175, Loss: 0.897332, Accuracy: 89.71%\n",
      "Batch 176, Loss: 0.874405, Accuracy: 89.68%\n",
      "Batch 177, Loss: 0.841077, Accuracy: 89.69%\n",
      "Batch 178, Loss: 0.892970, Accuracy: 89.67%\n",
      "Batch 179, Loss: 0.837457, Accuracy: 89.68%\n",
      "Batch 180, Loss: 0.865160, Accuracy: 89.67%\n",
      "Batch 181, Loss: 0.882058, Accuracy: 89.65%\n",
      "Batch 182, Loss: 0.827590, Accuracy: 89.65%\n",
      "Batch 183, Loss: 0.869260, Accuracy: 89.64%\n",
      "Batch 184, Loss: 0.833152, Accuracy: 89.66%\n",
      "Batch 185, Loss: 0.824691, Accuracy: 89.67%\n",
      "Batch 186, Loss: 0.862688, Accuracy: 89.67%\n",
      "Batch 187, Loss: 0.881137, Accuracy: 89.65%\n",
      "Batch 188, Loss: 0.855424, Accuracy: 89.64%\n",
      "Batch 189, Loss: 0.877056, Accuracy: 89.62%\n",
      "Batch 190, Loss: 0.831329, Accuracy: 89.63%\n",
      "Batch 191, Loss: 0.845812, Accuracy: 89.64%\n",
      "Batch 192, Loss: 0.880825, Accuracy: 89.62%\n",
      "Batch 193, Loss: 0.844529, Accuracy: 89.62%\n",
      "Batch 194, Loss: 0.811897, Accuracy: 89.64%\n",
      "Batch 195, Loss: 0.890246, Accuracy: 89.62%\n",
      "Batch 196, Loss: 0.836419, Accuracy: 89.62%\n",
      "Batch 197, Loss: 0.839975, Accuracy: 89.63%\n",
      "Batch 198, Loss: 0.844109, Accuracy: 89.63%\n",
      "Batch 199, Loss: 0.828677, Accuracy: 89.64%\n",
      "Batch 200, Loss: 0.768242, Accuracy: 89.69%\n",
      "Batch 201, Loss: 0.840992, Accuracy: 89.69%\n",
      "Batch 202, Loss: 0.837672, Accuracy: 89.70%\n",
      "Batch 203, Loss: 0.890023, Accuracy: 89.67%\n",
      "Batch 204, Loss: 0.867237, Accuracy: 89.66%\n",
      "Batch 205, Loss: 0.813362, Accuracy: 89.69%\n",
      "Batch 206, Loss: 0.885090, Accuracy: 89.66%\n",
      "Batch 207, Loss: 0.791751, Accuracy: 89.69%\n",
      "Batch 208, Loss: 0.847261, Accuracy: 89.69%\n",
      "Batch 209, Loss: 0.883701, Accuracy: 89.68%\n",
      "Batch 210, Loss: 0.817826, Accuracy: 89.69%\n",
      "Batch 211, Loss: 0.886417, Accuracy: 89.68%\n",
      "Batch 212, Loss: 0.843765, Accuracy: 89.67%\n",
      "Batch 213, Loss: 0.843686, Accuracy: 89.68%\n",
      "Training - Epoch 119, Loss: 0.848200, Accuracy: 89.68%\n",
      "Validation Batch 1, Loss: 0.843192, Accuracy: 89.06%\n",
      "Validation Batch 2, Loss: 0.812659, Accuracy: 90.62%\n",
      "Validation Batch 3, Loss: 0.856312, Accuracy: 90.62%\n",
      "Validation Batch 4, Loss: 0.857053, Accuracy: 90.62%\n",
      "Validation Batch 5, Loss: 0.833054, Accuracy: 90.94%\n",
      "Validation Batch 6, Loss: 0.809798, Accuracy: 91.41%\n",
      "Validation Batch 7, Loss: 0.811002, Accuracy: 91.96%\n",
      "Validation Batch 8, Loss: 0.881316, Accuracy: 91.02%\n",
      "Validation Batch 9, Loss: 0.910534, Accuracy: 90.10%\n",
      "Validation Batch 10, Loss: 0.812103, Accuracy: 90.47%\n",
      "Validation Batch 11, Loss: 0.847410, Accuracy: 90.48%\n",
      "Validation Batch 12, Loss: 0.857820, Accuracy: 90.49%\n",
      "Validation Batch 13, Loss: 0.856611, Accuracy: 90.26%\n",
      "Validation Batch 14, Loss: 0.862955, Accuracy: 90.18%\n",
      "Validation Batch 15, Loss: 0.814185, Accuracy: 90.31%\n",
      "Validation Batch 16, Loss: 0.861839, Accuracy: 90.14%\n",
      "Validation Batch 17, Loss: 0.874317, Accuracy: 89.98%\n",
      "Validation Batch 18, Loss: 0.811867, Accuracy: 90.19%\n",
      "Validation Batch 19, Loss: 0.892326, Accuracy: 89.97%\n",
      "Validation Batch 20, Loss: 0.857134, Accuracy: 89.77%\n",
      "Validation Batch 21, Loss: 0.867575, Accuracy: 89.73%\n",
      "Validation Batch 22, Loss: 0.822515, Accuracy: 89.91%\n",
      "Validation Batch 23, Loss: 0.879363, Accuracy: 89.81%\n",
      "Validation Batch 24, Loss: 0.840210, Accuracy: 89.91%\n",
      "Validation Batch 25, Loss: 0.817159, Accuracy: 90.06%\n",
      "Validation Batch 26, Loss: 0.858220, Accuracy: 90.02%\n",
      "Validation Batch 27, Loss: 0.811163, Accuracy: 90.14%\n",
      "Validation - Epoch 119, Loss: 0.846655, Accuracy: 90.14%\n",
      "Patience—4\n",
      "Epoch 120\n",
      "Batch 1, Loss: 0.845808, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.851434, Accuracy: 90.62%\n",
      "Batch 3, Loss: 0.852364, Accuracy: 90.10%\n",
      "Batch 4, Loss: 0.776394, Accuracy: 91.80%\n",
      "Batch 5, Loss: 0.876075, Accuracy: 90.94%\n",
      "Batch 6, Loss: 0.861525, Accuracy: 90.62%\n",
      "Batch 7, Loss: 0.848310, Accuracy: 90.18%\n",
      "Batch 8, Loss: 0.829526, Accuracy: 90.43%\n",
      "Batch 9, Loss: 0.893295, Accuracy: 89.76%\n",
      "Batch 10, Loss: 0.826182, Accuracy: 90.00%\n",
      "Batch 11, Loss: 0.865826, Accuracy: 89.77%\n",
      "Batch 12, Loss: 0.899583, Accuracy: 89.19%\n",
      "Batch 13, Loss: 0.835055, Accuracy: 89.30%\n",
      "Batch 14, Loss: 0.847971, Accuracy: 89.29%\n",
      "Batch 15, Loss: 0.886955, Accuracy: 89.06%\n",
      "Batch 16, Loss: 0.864339, Accuracy: 89.06%\n",
      "Batch 17, Loss: 0.817287, Accuracy: 89.25%\n",
      "Batch 18, Loss: 0.927773, Accuracy: 88.72%\n",
      "Batch 19, Loss: 0.842464, Accuracy: 88.90%\n",
      "Batch 20, Loss: 0.843961, Accuracy: 88.98%\n",
      "Batch 21, Loss: 0.871084, Accuracy: 88.84%\n",
      "Batch 22, Loss: 0.875383, Accuracy: 88.71%\n",
      "Batch 23, Loss: 0.836524, Accuracy: 88.79%\n",
      "Batch 24, Loss: 0.833903, Accuracy: 88.93%\n",
      "Batch 25, Loss: 0.842324, Accuracy: 89.00%\n",
      "Batch 26, Loss: 0.872566, Accuracy: 88.94%\n",
      "Batch 27, Loss: 0.831505, Accuracy: 89.00%\n",
      "Batch 28, Loss: 0.841524, Accuracy: 89.06%\n",
      "Batch 29, Loss: 0.807447, Accuracy: 89.22%\n",
      "Batch 30, Loss: 0.891470, Accuracy: 89.11%\n",
      "Batch 31, Loss: 0.892785, Accuracy: 89.01%\n",
      "Batch 32, Loss: 0.837925, Accuracy: 89.06%\n",
      "Batch 33, Loss: 0.855431, Accuracy: 89.02%\n",
      "Batch 34, Loss: 0.833456, Accuracy: 89.06%\n",
      "Batch 35, Loss: 0.883086, Accuracy: 88.97%\n",
      "Batch 36, Loss: 0.762856, Accuracy: 89.24%\n",
      "Batch 37, Loss: 0.805108, Accuracy: 89.36%\n",
      "Batch 38, Loss: 0.862501, Accuracy: 89.31%\n",
      "Batch 39, Loss: 0.808191, Accuracy: 89.42%\n",
      "Batch 40, Loss: 0.861976, Accuracy: 89.41%\n",
      "Batch 41, Loss: 0.801601, Accuracy: 89.56%\n",
      "Batch 42, Loss: 0.871906, Accuracy: 89.51%\n",
      "Batch 43, Loss: 0.845746, Accuracy: 89.50%\n",
      "Batch 44, Loss: 0.846357, Accuracy: 89.45%\n",
      "Batch 45, Loss: 0.896736, Accuracy: 89.34%\n",
      "Batch 46, Loss: 0.868088, Accuracy: 89.30%\n",
      "Batch 47, Loss: 0.819659, Accuracy: 89.39%\n",
      "Batch 48, Loss: 0.840188, Accuracy: 89.42%\n",
      "Batch 49, Loss: 0.850566, Accuracy: 89.41%\n",
      "Batch 50, Loss: 0.910408, Accuracy: 89.28%\n",
      "Batch 51, Loss: 0.893416, Accuracy: 89.15%\n",
      "Batch 52, Loss: 0.842741, Accuracy: 89.18%\n",
      "Batch 53, Loss: 0.828879, Accuracy: 89.21%\n",
      "Batch 54, Loss: 0.830770, Accuracy: 89.24%\n",
      "Batch 55, Loss: 0.907828, Accuracy: 89.12%\n",
      "Batch 56, Loss: 0.807840, Accuracy: 89.20%\n",
      "Batch 57, Loss: 0.827997, Accuracy: 89.25%\n",
      "Batch 58, Loss: 0.825387, Accuracy: 89.30%\n",
      "Batch 59, Loss: 0.927619, Accuracy: 89.17%\n",
      "Batch 60, Loss: 0.834738, Accuracy: 89.19%\n",
      "Batch 61, Loss: 0.790845, Accuracy: 89.29%\n",
      "Batch 62, Loss: 0.843677, Accuracy: 89.34%\n",
      "Batch 63, Loss: 0.866866, Accuracy: 89.31%\n",
      "Batch 64, Loss: 0.884801, Accuracy: 89.26%\n",
      "Batch 65, Loss: 0.840760, Accuracy: 89.25%\n",
      "Batch 66, Loss: 0.797959, Accuracy: 89.35%\n",
      "Batch 67, Loss: 0.860137, Accuracy: 89.34%\n",
      "Batch 68, Loss: 0.861411, Accuracy: 89.32%\n",
      "Batch 69, Loss: 0.829739, Accuracy: 89.36%\n",
      "Batch 70, Loss: 0.822421, Accuracy: 89.42%\n",
      "Batch 71, Loss: 0.822274, Accuracy: 89.44%\n",
      "Batch 72, Loss: 0.873686, Accuracy: 89.39%\n",
      "Batch 73, Loss: 0.880806, Accuracy: 89.36%\n",
      "Batch 74, Loss: 0.817518, Accuracy: 89.40%\n",
      "Batch 75, Loss: 0.808600, Accuracy: 89.44%\n",
      "Batch 76, Loss: 0.900853, Accuracy: 89.33%\n",
      "Batch 77, Loss: 0.833994, Accuracy: 89.37%\n",
      "Batch 78, Loss: 0.802294, Accuracy: 89.44%\n",
      "Batch 79, Loss: 0.892635, Accuracy: 89.38%\n",
      "Batch 80, Loss: 0.912300, Accuracy: 89.30%\n",
      "Batch 81, Loss: 0.885444, Accuracy: 89.26%\n",
      "Batch 82, Loss: 0.848668, Accuracy: 89.25%\n",
      "Batch 83, Loss: 0.802333, Accuracy: 89.31%\n",
      "Batch 84, Loss: 0.818101, Accuracy: 89.34%\n",
      "Batch 85, Loss: 0.852339, Accuracy: 89.34%\n",
      "Batch 86, Loss: 0.901365, Accuracy: 89.28%\n",
      "Batch 87, Loss: 0.875345, Accuracy: 89.26%\n",
      "Batch 88, Loss: 0.868834, Accuracy: 89.24%\n",
      "Batch 89, Loss: 0.845075, Accuracy: 89.24%\n",
      "Batch 90, Loss: 0.841270, Accuracy: 89.25%\n",
      "Batch 91, Loss: 0.920458, Accuracy: 89.20%\n",
      "Batch 92, Loss: 0.881282, Accuracy: 89.18%\n",
      "Batch 93, Loss: 0.864121, Accuracy: 89.16%\n",
      "Batch 94, Loss: 0.873430, Accuracy: 89.15%\n",
      "Batch 95, Loss: 0.879237, Accuracy: 89.13%\n",
      "Batch 96, Loss: 0.883560, Accuracy: 89.08%\n",
      "Batch 97, Loss: 0.818561, Accuracy: 89.13%\n",
      "Batch 98, Loss: 0.820977, Accuracy: 89.17%\n",
      "Batch 99, Loss: 0.857708, Accuracy: 89.16%\n",
      "Batch 100, Loss: 0.876253, Accuracy: 89.14%\n",
      "Batch 101, Loss: 0.903603, Accuracy: 89.09%\n",
      "Batch 102, Loss: 0.812212, Accuracy: 89.12%\n",
      "Batch 103, Loss: 0.815793, Accuracy: 89.17%\n",
      "Batch 104, Loss: 0.852664, Accuracy: 89.17%\n",
      "Batch 105, Loss: 0.824610, Accuracy: 89.20%\n",
      "Batch 106, Loss: 0.867074, Accuracy: 89.20%\n",
      "Batch 107, Loss: 0.833813, Accuracy: 89.24%\n",
      "Batch 108, Loss: 0.841313, Accuracy: 89.25%\n",
      "Batch 109, Loss: 0.912049, Accuracy: 89.16%\n",
      "Batch 110, Loss: 0.914544, Accuracy: 89.11%\n",
      "Batch 111, Loss: 0.820313, Accuracy: 89.13%\n",
      "Batch 112, Loss: 0.816876, Accuracy: 89.17%\n",
      "Batch 113, Loss: 0.883982, Accuracy: 89.15%\n",
      "Batch 114, Loss: 0.884133, Accuracy: 89.12%\n",
      "Batch 115, Loss: 0.874901, Accuracy: 89.09%\n",
      "Batch 116, Loss: 0.876699, Accuracy: 89.08%\n",
      "Batch 117, Loss: 0.840896, Accuracy: 89.09%\n",
      "Batch 118, Loss: 0.787639, Accuracy: 89.14%\n",
      "Batch 119, Loss: 0.858937, Accuracy: 89.13%\n",
      "Batch 120, Loss: 0.849003, Accuracy: 89.14%\n",
      "Batch 121, Loss: 0.852853, Accuracy: 89.14%\n",
      "Batch 122, Loss: 0.813293, Accuracy: 89.16%\n",
      "Batch 123, Loss: 0.920315, Accuracy: 89.10%\n",
      "Batch 124, Loss: 0.900582, Accuracy: 89.05%\n",
      "Batch 125, Loss: 0.841033, Accuracy: 89.06%\n",
      "Batch 126, Loss: 0.865758, Accuracy: 89.05%\n",
      "Batch 127, Loss: 0.827055, Accuracy: 89.06%\n",
      "Batch 128, Loss: 0.878439, Accuracy: 89.04%\n",
      "Batch 129, Loss: 0.870194, Accuracy: 89.01%\n",
      "Batch 130, Loss: 0.840887, Accuracy: 89.03%\n",
      "Batch 131, Loss: 0.851636, Accuracy: 89.03%\n",
      "Batch 132, Loss: 0.856779, Accuracy: 89.02%\n",
      "Batch 133, Loss: 0.911929, Accuracy: 88.96%\n",
      "Batch 134, Loss: 0.890590, Accuracy: 88.93%\n",
      "Batch 135, Loss: 0.845719, Accuracy: 88.94%\n",
      "Batch 136, Loss: 0.872331, Accuracy: 88.92%\n",
      "Batch 137, Loss: 0.845521, Accuracy: 88.94%\n",
      "Batch 138, Loss: 0.886924, Accuracy: 88.90%\n",
      "Batch 139, Loss: 0.808467, Accuracy: 88.94%\n",
      "Batch 140, Loss: 0.900919, Accuracy: 88.90%\n",
      "Batch 141, Loss: 0.845556, Accuracy: 88.90%\n",
      "Batch 142, Loss: 0.853581, Accuracy: 88.89%\n",
      "Batch 143, Loss: 0.789590, Accuracy: 88.93%\n",
      "Batch 144, Loss: 0.831001, Accuracy: 88.94%\n",
      "Batch 145, Loss: 0.802021, Accuracy: 88.99%\n",
      "Batch 146, Loss: 0.827331, Accuracy: 89.01%\n",
      "Batch 147, Loss: 0.845696, Accuracy: 89.01%\n",
      "Batch 148, Loss: 0.943006, Accuracy: 88.94%\n",
      "Batch 149, Loss: 0.878222, Accuracy: 88.94%\n",
      "Batch 150, Loss: 0.846532, Accuracy: 88.95%\n",
      "Batch 151, Loss: 0.865313, Accuracy: 88.95%\n",
      "Batch 152, Loss: 0.853336, Accuracy: 88.95%\n",
      "Batch 153, Loss: 0.890206, Accuracy: 88.93%\n",
      "Batch 154, Loss: 0.846000, Accuracy: 88.93%\n",
      "Batch 155, Loss: 0.866399, Accuracy: 88.93%\n",
      "Batch 156, Loss: 0.931912, Accuracy: 88.88%\n",
      "Batch 157, Loss: 0.812614, Accuracy: 88.91%\n",
      "Batch 158, Loss: 0.820104, Accuracy: 88.93%\n",
      "Batch 159, Loss: 0.869239, Accuracy: 88.93%\n",
      "Batch 160, Loss: 0.802888, Accuracy: 88.96%\n",
      "Batch 161, Loss: 0.863432, Accuracy: 88.96%\n",
      "Batch 162, Loss: 0.814492, Accuracy: 88.98%\n",
      "Batch 163, Loss: 0.893868, Accuracy: 88.95%\n",
      "Batch 164, Loss: 0.861344, Accuracy: 88.95%\n",
      "Batch 165, Loss: 0.886362, Accuracy: 88.92%\n",
      "Batch 166, Loss: 0.833097, Accuracy: 88.93%\n",
      "Batch 167, Loss: 0.905199, Accuracy: 88.90%\n",
      "Batch 168, Loss: 0.857018, Accuracy: 88.90%\n",
      "Batch 169, Loss: 0.910474, Accuracy: 88.88%\n",
      "Batch 170, Loss: 0.858702, Accuracy: 88.87%\n",
      "Batch 171, Loss: 0.877184, Accuracy: 88.85%\n",
      "Batch 172, Loss: 0.836121, Accuracy: 88.86%\n",
      "Batch 173, Loss: 0.856984, Accuracy: 88.86%\n",
      "Batch 174, Loss: 0.796105, Accuracy: 88.89%\n",
      "Batch 175, Loss: 0.832411, Accuracy: 88.90%\n",
      "Batch 176, Loss: 0.860154, Accuracy: 88.90%\n",
      "Batch 177, Loss: 0.833065, Accuracy: 88.92%\n",
      "Batch 178, Loss: 0.854093, Accuracy: 88.92%\n",
      "Batch 179, Loss: 0.866075, Accuracy: 88.91%\n",
      "Batch 180, Loss: 0.884758, Accuracy: 88.90%\n",
      "Batch 181, Loss: 0.799949, Accuracy: 88.92%\n",
      "Batch 182, Loss: 0.842743, Accuracy: 88.93%\n",
      "Batch 183, Loss: 0.833903, Accuracy: 88.94%\n",
      "Batch 184, Loss: 0.876334, Accuracy: 88.93%\n",
      "Batch 185, Loss: 0.851421, Accuracy: 88.93%\n",
      "Batch 186, Loss: 0.830792, Accuracy: 88.94%\n",
      "Batch 187, Loss: 0.878056, Accuracy: 88.93%\n",
      "Batch 188, Loss: 0.838502, Accuracy: 88.93%\n",
      "Batch 189, Loss: 0.849987, Accuracy: 88.93%\n",
      "Batch 190, Loss: 0.866902, Accuracy: 88.92%\n",
      "Batch 191, Loss: 0.865671, Accuracy: 88.92%\n",
      "Batch 192, Loss: 0.884011, Accuracy: 88.89%\n",
      "Batch 193, Loss: 0.874299, Accuracy: 88.88%\n",
      "Batch 194, Loss: 0.851583, Accuracy: 88.89%\n",
      "Batch 195, Loss: 0.845384, Accuracy: 88.90%\n",
      "Batch 196, Loss: 0.854093, Accuracy: 88.91%\n",
      "Batch 197, Loss: 0.816095, Accuracy: 88.93%\n",
      "Batch 198, Loss: 0.850917, Accuracy: 88.93%\n",
      "Batch 199, Loss: 0.856565, Accuracy: 88.93%\n",
      "Batch 200, Loss: 0.838866, Accuracy: 88.95%\n",
      "Batch 201, Loss: 0.826818, Accuracy: 88.96%\n",
      "Batch 202, Loss: 0.869697, Accuracy: 88.95%\n",
      "Batch 203, Loss: 0.841447, Accuracy: 88.95%\n",
      "Batch 204, Loss: 0.880768, Accuracy: 88.95%\n",
      "Batch 205, Loss: 0.808429, Accuracy: 88.97%\n",
      "Batch 206, Loss: 0.872445, Accuracy: 88.96%\n",
      "Batch 207, Loss: 0.830393, Accuracy: 88.97%\n",
      "Batch 208, Loss: 0.828134, Accuracy: 88.98%\n",
      "Batch 209, Loss: 0.851500, Accuracy: 88.98%\n",
      "Batch 210, Loss: 0.870627, Accuracy: 88.97%\n",
      "Batch 211, Loss: 0.898152, Accuracy: 88.94%\n",
      "Batch 212, Loss: 0.848677, Accuracy: 88.95%\n",
      "Batch 213, Loss: 0.864834, Accuracy: 88.95%\n",
      "Training - Epoch 120, Loss: 0.853931, Accuracy: 88.95%\n",
      "Validation Batch 1, Loss: 0.815892, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.797776, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.824597, Accuracy: 94.27%\n",
      "Validation Batch 4, Loss: 0.836022, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.816420, Accuracy: 93.44%\n",
      "Validation Batch 6, Loss: 0.795850, Accuracy: 93.49%\n",
      "Validation Batch 7, Loss: 0.795436, Accuracy: 93.75%\n",
      "Validation Batch 8, Loss: 0.860548, Accuracy: 92.97%\n",
      "Validation Batch 9, Loss: 0.882238, Accuracy: 92.19%\n",
      "Validation Batch 10, Loss: 0.795450, Accuracy: 92.50%\n",
      "Validation Batch 11, Loss: 0.823599, Accuracy: 92.47%\n",
      "Validation Batch 12, Loss: 0.828715, Accuracy: 92.45%\n",
      "Validation Batch 13, Loss: 0.836981, Accuracy: 92.31%\n",
      "Validation Batch 14, Loss: 0.841076, Accuracy: 92.19%\n",
      "Validation Batch 15, Loss: 0.797168, Accuracy: 92.40%\n",
      "Validation Batch 16, Loss: 0.825571, Accuracy: 92.38%\n",
      "Validation Batch 17, Loss: 0.870684, Accuracy: 92.10%\n",
      "Validation Batch 18, Loss: 0.796511, Accuracy: 92.27%\n",
      "Validation Batch 19, Loss: 0.864986, Accuracy: 92.11%\n",
      "Validation Batch 20, Loss: 0.820430, Accuracy: 92.03%\n",
      "Validation Batch 21, Loss: 0.857904, Accuracy: 91.89%\n",
      "Validation Batch 22, Loss: 0.814477, Accuracy: 91.97%\n",
      "Validation Batch 23, Loss: 0.861673, Accuracy: 91.78%\n",
      "Validation Batch 24, Loss: 0.832199, Accuracy: 91.67%\n",
      "Validation Batch 25, Loss: 0.799147, Accuracy: 91.81%\n",
      "Validation Batch 26, Loss: 0.839307, Accuracy: 91.77%\n",
      "Validation Batch 27, Loss: 0.797775, Accuracy: 91.84%\n",
      "Validation - Epoch 120, Loss: 0.826979, Accuracy: 91.84%\n",
      "Patience—5\n",
      "Epoch 121\n",
      "Batch 1, Loss: 0.826419, Accuracy: 92.19%\n",
      "Batch 2, Loss: 0.866060, Accuracy: 90.62%\n",
      "Batch 3, Loss: 0.943676, Accuracy: 87.50%\n",
      "Batch 4, Loss: 0.840646, Accuracy: 88.28%\n",
      "Batch 5, Loss: 0.896733, Accuracy: 87.50%\n",
      "Batch 6, Loss: 0.855448, Accuracy: 87.76%\n",
      "Batch 7, Loss: 0.819996, Accuracy: 88.39%\n",
      "Batch 8, Loss: 0.837870, Accuracy: 88.67%\n",
      "Batch 9, Loss: 0.855625, Accuracy: 88.72%\n",
      "Batch 10, Loss: 0.754531, Accuracy: 89.84%\n",
      "Batch 11, Loss: 0.859948, Accuracy: 89.77%\n",
      "Batch 12, Loss: 0.817078, Accuracy: 89.97%\n",
      "Batch 13, Loss: 0.868363, Accuracy: 89.78%\n",
      "Batch 14, Loss: 0.854160, Accuracy: 89.62%\n",
      "Batch 15, Loss: 0.922200, Accuracy: 89.06%\n",
      "Batch 16, Loss: 0.881851, Accuracy: 88.87%\n",
      "Batch 17, Loss: 0.810324, Accuracy: 89.25%\n",
      "Batch 18, Loss: 0.840521, Accuracy: 89.32%\n",
      "Batch 19, Loss: 0.817478, Accuracy: 89.56%\n",
      "Batch 20, Loss: 0.838636, Accuracy: 89.69%\n",
      "Batch 21, Loss: 0.850178, Accuracy: 89.73%\n",
      "Batch 22, Loss: 0.916173, Accuracy: 89.42%\n",
      "Batch 23, Loss: 0.837698, Accuracy: 89.40%\n",
      "Batch 24, Loss: 0.842455, Accuracy: 89.45%\n",
      "Batch 25, Loss: 0.854575, Accuracy: 89.44%\n",
      "Batch 26, Loss: 0.926112, Accuracy: 89.06%\n",
      "Batch 27, Loss: 0.825172, Accuracy: 89.24%\n",
      "Batch 28, Loss: 0.849228, Accuracy: 89.23%\n",
      "Batch 29, Loss: 0.871064, Accuracy: 89.17%\n",
      "Batch 30, Loss: 0.851531, Accuracy: 89.22%\n",
      "Batch 31, Loss: 0.878412, Accuracy: 89.06%\n",
      "Batch 32, Loss: 0.816912, Accuracy: 89.21%\n",
      "Batch 33, Loss: 0.870961, Accuracy: 89.11%\n",
      "Batch 34, Loss: 0.844957, Accuracy: 89.11%\n",
      "Batch 35, Loss: 0.862545, Accuracy: 89.06%\n",
      "Batch 36, Loss: 0.831982, Accuracy: 89.15%\n",
      "Batch 37, Loss: 0.832461, Accuracy: 89.19%\n",
      "Batch 38, Loss: 0.860936, Accuracy: 89.14%\n",
      "Batch 39, Loss: 0.892934, Accuracy: 89.06%\n",
      "Batch 40, Loss: 0.848600, Accuracy: 89.06%\n",
      "Batch 41, Loss: 0.908120, Accuracy: 88.95%\n",
      "Batch 42, Loss: 0.842117, Accuracy: 88.99%\n",
      "Batch 43, Loss: 0.943883, Accuracy: 88.77%\n",
      "Batch 44, Loss: 0.838251, Accuracy: 88.81%\n",
      "Batch 45, Loss: 0.855008, Accuracy: 88.82%\n",
      "Batch 46, Loss: 0.867885, Accuracy: 88.76%\n",
      "Batch 47, Loss: 0.841748, Accuracy: 88.80%\n",
      "Batch 48, Loss: 0.893711, Accuracy: 88.74%\n",
      "Batch 49, Loss: 0.844933, Accuracy: 88.78%\n",
      "Batch 50, Loss: 0.829187, Accuracy: 88.81%\n",
      "Batch 51, Loss: 0.950147, Accuracy: 88.60%\n",
      "Batch 52, Loss: 0.849342, Accuracy: 88.61%\n",
      "Batch 53, Loss: 0.856910, Accuracy: 88.62%\n",
      "Batch 54, Loss: 0.888845, Accuracy: 88.54%\n",
      "Batch 55, Loss: 0.839826, Accuracy: 88.58%\n",
      "Batch 56, Loss: 0.861504, Accuracy: 88.62%\n",
      "Batch 57, Loss: 0.864445, Accuracy: 88.60%\n",
      "Batch 58, Loss: 0.866452, Accuracy: 88.60%\n",
      "Batch 59, Loss: 0.823120, Accuracy: 88.67%\n",
      "Batch 60, Loss: 0.864069, Accuracy: 88.65%\n",
      "Batch 61, Loss: 0.805335, Accuracy: 88.73%\n",
      "Batch 62, Loss: 0.818046, Accuracy: 88.79%\n",
      "Batch 63, Loss: 0.848356, Accuracy: 88.81%\n",
      "Batch 64, Loss: 0.861529, Accuracy: 88.79%\n",
      "Batch 65, Loss: 0.871271, Accuracy: 88.75%\n",
      "Batch 66, Loss: 0.863249, Accuracy: 88.73%\n",
      "Batch 67, Loss: 0.937397, Accuracy: 88.60%\n",
      "Batch 68, Loss: 0.822138, Accuracy: 88.65%\n",
      "Batch 69, Loss: 0.798956, Accuracy: 88.75%\n",
      "Batch 70, Loss: 0.761005, Accuracy: 88.88%\n",
      "Batch 71, Loss: 0.818756, Accuracy: 88.93%\n",
      "Batch 72, Loss: 0.858995, Accuracy: 88.91%\n",
      "Batch 73, Loss: 0.804574, Accuracy: 89.00%\n",
      "Batch 74, Loss: 0.870421, Accuracy: 88.98%\n",
      "Batch 75, Loss: 0.870176, Accuracy: 88.94%\n",
      "Batch 76, Loss: 0.875704, Accuracy: 88.90%\n",
      "Batch 77, Loss: 0.861741, Accuracy: 88.90%\n",
      "Batch 78, Loss: 0.910930, Accuracy: 88.84%\n",
      "Batch 79, Loss: 0.920763, Accuracy: 88.75%\n",
      "Batch 80, Loss: 0.825087, Accuracy: 88.79%\n",
      "Batch 81, Loss: 0.860613, Accuracy: 88.77%\n",
      "Batch 82, Loss: 0.834795, Accuracy: 88.80%\n",
      "Batch 83, Loss: 0.823208, Accuracy: 88.86%\n",
      "Batch 84, Loss: 0.869586, Accuracy: 88.84%\n",
      "Batch 85, Loss: 0.805344, Accuracy: 88.92%\n",
      "Batch 86, Loss: 0.817298, Accuracy: 88.95%\n",
      "Batch 87, Loss: 0.818862, Accuracy: 88.99%\n",
      "Batch 88, Loss: 0.879632, Accuracy: 88.96%\n",
      "Batch 89, Loss: 0.795169, Accuracy: 89.03%\n",
      "Batch 90, Loss: 0.828801, Accuracy: 89.06%\n",
      "Batch 91, Loss: 0.857132, Accuracy: 89.06%\n",
      "Batch 92, Loss: 0.864264, Accuracy: 89.05%\n",
      "Batch 93, Loss: 0.831357, Accuracy: 89.06%\n",
      "Batch 94, Loss: 0.890571, Accuracy: 89.03%\n",
      "Batch 95, Loss: 0.862994, Accuracy: 89.03%\n",
      "Batch 96, Loss: 0.901989, Accuracy: 88.96%\n",
      "Batch 97, Loss: 0.880097, Accuracy: 88.92%\n",
      "Batch 98, Loss: 0.816824, Accuracy: 88.97%\n",
      "Batch 99, Loss: 0.847395, Accuracy: 88.95%\n",
      "Batch 100, Loss: 0.825732, Accuracy: 88.98%\n",
      "Batch 101, Loss: 0.849998, Accuracy: 89.00%\n",
      "Batch 102, Loss: 0.837215, Accuracy: 89.02%\n",
      "Batch 103, Loss: 0.860476, Accuracy: 89.02%\n",
      "Batch 104, Loss: 0.877570, Accuracy: 89.00%\n",
      "Batch 105, Loss: 0.854100, Accuracy: 89.02%\n",
      "Batch 106, Loss: 0.835373, Accuracy: 89.02%\n",
      "Batch 107, Loss: 0.946357, Accuracy: 88.93%\n",
      "Batch 108, Loss: 0.826029, Accuracy: 88.95%\n",
      "Batch 109, Loss: 0.898454, Accuracy: 88.90%\n",
      "Batch 110, Loss: 0.851704, Accuracy: 88.93%\n",
      "Batch 111, Loss: 0.850092, Accuracy: 88.92%\n",
      "Batch 112, Loss: 0.811118, Accuracy: 88.95%\n",
      "Batch 113, Loss: 0.887735, Accuracy: 88.91%\n",
      "Batch 114, Loss: 0.855897, Accuracy: 88.90%\n",
      "Batch 115, Loss: 0.880614, Accuracy: 88.87%\n",
      "Batch 116, Loss: 0.870735, Accuracy: 88.86%\n",
      "Batch 117, Loss: 0.797096, Accuracy: 88.92%\n",
      "Batch 118, Loss: 0.826829, Accuracy: 88.94%\n",
      "Batch 119, Loss: 0.829425, Accuracy: 88.96%\n",
      "Batch 120, Loss: 0.853072, Accuracy: 88.96%\n",
      "Batch 121, Loss: 0.883329, Accuracy: 88.92%\n",
      "Batch 122, Loss: 0.844276, Accuracy: 88.92%\n",
      "Batch 123, Loss: 0.853027, Accuracy: 88.92%\n",
      "Batch 124, Loss: 0.790792, Accuracy: 88.97%\n",
      "Batch 125, Loss: 0.778676, Accuracy: 89.04%\n",
      "Batch 126, Loss: 0.885933, Accuracy: 89.01%\n",
      "Batch 127, Loss: 0.833871, Accuracy: 89.03%\n",
      "Batch 128, Loss: 0.857190, Accuracy: 89.03%\n",
      "Batch 129, Loss: 0.871874, Accuracy: 89.01%\n",
      "Batch 130, Loss: 0.824201, Accuracy: 89.04%\n",
      "Batch 131, Loss: 0.823497, Accuracy: 89.06%\n",
      "Batch 132, Loss: 0.863099, Accuracy: 89.06%\n",
      "Batch 133, Loss: 0.898318, Accuracy: 89.03%\n",
      "Batch 134, Loss: 0.921613, Accuracy: 88.98%\n",
      "Batch 135, Loss: 0.866859, Accuracy: 88.97%\n",
      "Batch 136, Loss: 0.806720, Accuracy: 89.02%\n",
      "Batch 137, Loss: 0.878333, Accuracy: 89.01%\n",
      "Batch 138, Loss: 0.896157, Accuracy: 88.97%\n",
      "Batch 139, Loss: 0.866742, Accuracy: 88.96%\n",
      "Batch 140, Loss: 0.846459, Accuracy: 88.96%\n",
      "Batch 141, Loss: 0.835062, Accuracy: 88.96%\n",
      "Batch 142, Loss: 0.848107, Accuracy: 88.97%\n",
      "Batch 143, Loss: 0.908577, Accuracy: 88.93%\n",
      "Batch 144, Loss: 0.863987, Accuracy: 88.93%\n",
      "Batch 145, Loss: 0.810293, Accuracy: 88.97%\n",
      "Batch 146, Loss: 0.851309, Accuracy: 88.97%\n",
      "Batch 147, Loss: 0.815101, Accuracy: 89.00%\n",
      "Batch 148, Loss: 0.833819, Accuracy: 89.01%\n",
      "Batch 149, Loss: 0.835222, Accuracy: 89.04%\n",
      "Batch 150, Loss: 0.830980, Accuracy: 89.05%\n",
      "Batch 151, Loss: 0.830101, Accuracy: 89.07%\n",
      "Batch 152, Loss: 0.824442, Accuracy: 89.09%\n",
      "Batch 153, Loss: 0.846199, Accuracy: 89.08%\n",
      "Batch 154, Loss: 0.863949, Accuracy: 89.08%\n",
      "Batch 155, Loss: 0.933213, Accuracy: 89.03%\n",
      "Batch 156, Loss: 0.838177, Accuracy: 89.04%\n",
      "Batch 157, Loss: 0.840917, Accuracy: 89.05%\n",
      "Batch 158, Loss: 0.921036, Accuracy: 89.02%\n",
      "Batch 159, Loss: 0.822838, Accuracy: 89.03%\n",
      "Batch 160, Loss: 0.785745, Accuracy: 89.08%\n",
      "Batch 161, Loss: 0.800098, Accuracy: 89.12%\n",
      "Batch 162, Loss: 0.839181, Accuracy: 89.13%\n",
      "Batch 163, Loss: 0.779705, Accuracy: 89.18%\n",
      "Batch 164, Loss: 0.890370, Accuracy: 89.16%\n",
      "Batch 165, Loss: 0.806818, Accuracy: 89.19%\n",
      "Batch 166, Loss: 0.843458, Accuracy: 89.18%\n",
      "Batch 167, Loss: 0.821805, Accuracy: 89.20%\n",
      "Batch 168, Loss: 0.876124, Accuracy: 89.18%\n",
      "Batch 169, Loss: 0.851777, Accuracy: 89.18%\n",
      "Batch 170, Loss: 0.918051, Accuracy: 89.15%\n",
      "Batch 171, Loss: 0.878678, Accuracy: 89.14%\n",
      "Batch 172, Loss: 0.852406, Accuracy: 89.14%\n",
      "Batch 173, Loss: 0.840857, Accuracy: 89.15%\n",
      "Batch 174, Loss: 0.822774, Accuracy: 89.17%\n",
      "Batch 175, Loss: 0.852053, Accuracy: 89.17%\n",
      "Batch 176, Loss: 0.790151, Accuracy: 89.21%\n",
      "Batch 177, Loss: 0.869891, Accuracy: 89.19%\n",
      "Batch 178, Loss: 0.899675, Accuracy: 89.18%\n",
      "Batch 179, Loss: 0.867873, Accuracy: 89.17%\n",
      "Batch 180, Loss: 0.805850, Accuracy: 89.19%\n",
      "Batch 181, Loss: 0.845369, Accuracy: 89.21%\n",
      "Batch 182, Loss: 0.859853, Accuracy: 89.21%\n",
      "Batch 183, Loss: 0.871408, Accuracy: 89.21%\n",
      "Batch 184, Loss: 0.832001, Accuracy: 89.21%\n",
      "Batch 185, Loss: 0.844704, Accuracy: 89.21%\n",
      "Batch 186, Loss: 0.833861, Accuracy: 89.21%\n",
      "Batch 187, Loss: 0.818994, Accuracy: 89.23%\n",
      "Batch 188, Loss: 0.805225, Accuracy: 89.25%\n",
      "Batch 189, Loss: 0.824996, Accuracy: 89.28%\n",
      "Batch 190, Loss: 0.865401, Accuracy: 89.26%\n",
      "Batch 191, Loss: 0.849580, Accuracy: 89.26%\n",
      "Batch 192, Loss: 0.861128, Accuracy: 89.26%\n",
      "Batch 193, Loss: 0.895957, Accuracy: 89.24%\n",
      "Batch 194, Loss: 0.836121, Accuracy: 89.25%\n",
      "Batch 195, Loss: 0.882526, Accuracy: 89.23%\n",
      "Batch 196, Loss: 0.852648, Accuracy: 89.23%\n",
      "Batch 197, Loss: 0.872934, Accuracy: 89.22%\n",
      "Batch 198, Loss: 0.813276, Accuracy: 89.24%\n",
      "Batch 199, Loss: 0.844873, Accuracy: 89.25%\n",
      "Batch 200, Loss: 0.765822, Accuracy: 89.30%\n",
      "Batch 201, Loss: 0.816791, Accuracy: 89.32%\n",
      "Batch 202, Loss: 0.839563, Accuracy: 89.33%\n",
      "Batch 203, Loss: 0.826293, Accuracy: 89.34%\n",
      "Batch 204, Loss: 0.885301, Accuracy: 89.32%\n",
      "Batch 205, Loss: 0.791592, Accuracy: 89.35%\n",
      "Batch 206, Loss: 0.831067, Accuracy: 89.37%\n",
      "Batch 207, Loss: 0.834562, Accuracy: 89.37%\n",
      "Batch 208, Loss: 0.846424, Accuracy: 89.37%\n",
      "Batch 209, Loss: 0.835973, Accuracy: 89.38%\n",
      "Batch 210, Loss: 0.818991, Accuracy: 89.40%\n",
      "Batch 211, Loss: 0.860519, Accuracy: 89.40%\n",
      "Batch 212, Loss: 0.827078, Accuracy: 89.41%\n",
      "Batch 213, Loss: 0.826648, Accuracy: 89.42%\n",
      "Training - Epoch 121, Loss: 0.850578, Accuracy: 89.42%\n",
      "Validation Batch 1, Loss: 0.815003, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.785346, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.822928, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.836054, Accuracy: 92.97%\n",
      "Validation Batch 5, Loss: 0.818427, Accuracy: 92.81%\n",
      "Validation Batch 6, Loss: 0.789277, Accuracy: 93.49%\n",
      "Validation Batch 7, Loss: 0.793882, Accuracy: 93.75%\n",
      "Validation Batch 8, Loss: 0.861407, Accuracy: 93.16%\n",
      "Validation Batch 9, Loss: 0.874100, Accuracy: 92.71%\n",
      "Validation Batch 10, Loss: 0.787241, Accuracy: 93.12%\n",
      "Validation Batch 11, Loss: 0.823515, Accuracy: 93.04%\n",
      "Validation Batch 12, Loss: 0.827190, Accuracy: 92.97%\n",
      "Validation Batch 13, Loss: 0.827866, Accuracy: 92.91%\n",
      "Validation Batch 14, Loss: 0.836383, Accuracy: 92.75%\n",
      "Validation Batch 15, Loss: 0.795835, Accuracy: 92.92%\n",
      "Validation Batch 16, Loss: 0.824642, Accuracy: 92.87%\n",
      "Validation Batch 17, Loss: 0.862896, Accuracy: 92.56%\n",
      "Validation Batch 18, Loss: 0.791941, Accuracy: 92.71%\n",
      "Validation Batch 19, Loss: 0.860616, Accuracy: 92.52%\n",
      "Validation Batch 20, Loss: 0.820789, Accuracy: 92.50%\n",
      "Validation Batch 21, Loss: 0.857694, Accuracy: 92.34%\n",
      "Validation Batch 22, Loss: 0.799013, Accuracy: 92.54%\n",
      "Validation Batch 23, Loss: 0.856940, Accuracy: 92.39%\n",
      "Validation Batch 24, Loss: 0.821261, Accuracy: 92.38%\n",
      "Validation Batch 25, Loss: 0.801853, Accuracy: 92.44%\n",
      "Validation Batch 26, Loss: 0.831471, Accuracy: 92.43%\n",
      "Validation Batch 27, Loss: 0.794026, Accuracy: 92.48%\n",
      "Validation - Epoch 121, Loss: 0.822874, Accuracy: 92.48%\n",
      "Patience—6\n",
      "Epoch 122\n",
      "Batch 1, Loss: 0.816106, Accuracy: 93.75%\n",
      "Batch 2, Loss: 0.853304, Accuracy: 90.62%\n",
      "Batch 3, Loss: 0.843371, Accuracy: 90.10%\n",
      "Batch 4, Loss: 0.886077, Accuracy: 89.06%\n",
      "Batch 5, Loss: 0.875914, Accuracy: 88.75%\n",
      "Batch 6, Loss: 0.781147, Accuracy: 90.10%\n",
      "Batch 7, Loss: 0.938579, Accuracy: 88.84%\n",
      "Batch 8, Loss: 0.849211, Accuracy: 88.87%\n",
      "Batch 9, Loss: 0.926040, Accuracy: 88.19%\n",
      "Batch 10, Loss: 0.923217, Accuracy: 87.50%\n",
      "Batch 11, Loss: 0.845439, Accuracy: 87.78%\n",
      "Batch 12, Loss: 0.850409, Accuracy: 87.76%\n",
      "Batch 13, Loss: 0.816811, Accuracy: 88.10%\n",
      "Batch 14, Loss: 0.872139, Accuracy: 88.17%\n",
      "Batch 15, Loss: 0.804939, Accuracy: 88.54%\n",
      "Batch 16, Loss: 0.811494, Accuracy: 88.96%\n",
      "Batch 17, Loss: 0.857501, Accuracy: 88.97%\n",
      "Batch 18, Loss: 0.895221, Accuracy: 88.80%\n",
      "Batch 19, Loss: 0.822385, Accuracy: 88.98%\n",
      "Batch 20, Loss: 0.900045, Accuracy: 88.75%\n",
      "Batch 21, Loss: 0.848712, Accuracy: 88.84%\n",
      "Batch 22, Loss: 0.827786, Accuracy: 88.99%\n",
      "Batch 23, Loss: 0.820375, Accuracy: 89.13%\n",
      "Batch 24, Loss: 0.792743, Accuracy: 89.45%\n",
      "Batch 25, Loss: 0.896779, Accuracy: 89.25%\n",
      "Batch 26, Loss: 0.815639, Accuracy: 89.42%\n",
      "Batch 27, Loss: 0.877695, Accuracy: 89.29%\n",
      "Batch 28, Loss: 0.845563, Accuracy: 89.34%\n",
      "Batch 29, Loss: 0.885623, Accuracy: 89.22%\n",
      "Batch 30, Loss: 0.849797, Accuracy: 89.17%\n",
      "Batch 31, Loss: 0.875894, Accuracy: 89.06%\n",
      "Batch 32, Loss: 0.810068, Accuracy: 89.21%\n",
      "Batch 33, Loss: 0.896349, Accuracy: 89.11%\n",
      "Batch 34, Loss: 0.819642, Accuracy: 89.20%\n",
      "Batch 35, Loss: 0.843583, Accuracy: 89.20%\n",
      "Batch 36, Loss: 0.941286, Accuracy: 88.93%\n",
      "Batch 37, Loss: 0.852350, Accuracy: 88.89%\n",
      "Batch 38, Loss: 0.810302, Accuracy: 89.06%\n",
      "Batch 39, Loss: 0.825332, Accuracy: 89.14%\n",
      "Batch 40, Loss: 0.890908, Accuracy: 89.02%\n",
      "Batch 41, Loss: 0.914345, Accuracy: 88.87%\n",
      "Batch 42, Loss: 0.826318, Accuracy: 88.95%\n",
      "Batch 43, Loss: 0.864631, Accuracy: 88.92%\n",
      "Batch 44, Loss: 0.799367, Accuracy: 89.06%\n",
      "Batch 45, Loss: 0.790563, Accuracy: 89.20%\n",
      "Batch 46, Loss: 0.903790, Accuracy: 89.06%\n",
      "Batch 47, Loss: 0.785773, Accuracy: 89.20%\n",
      "Batch 48, Loss: 0.859255, Accuracy: 89.19%\n",
      "Batch 49, Loss: 0.830097, Accuracy: 89.22%\n",
      "Batch 50, Loss: 0.841123, Accuracy: 89.25%\n",
      "Batch 51, Loss: 0.871483, Accuracy: 89.22%\n",
      "Batch 52, Loss: 0.882583, Accuracy: 89.15%\n",
      "Batch 53, Loss: 0.896413, Accuracy: 89.06%\n",
      "Batch 54, Loss: 0.862086, Accuracy: 89.03%\n",
      "Batch 55, Loss: 0.813262, Accuracy: 89.12%\n",
      "Batch 56, Loss: 0.849939, Accuracy: 89.12%\n",
      "Batch 57, Loss: 0.831804, Accuracy: 89.14%\n",
      "Batch 58, Loss: 0.840844, Accuracy: 89.20%\n",
      "Batch 59, Loss: 0.809807, Accuracy: 89.27%\n",
      "Batch 60, Loss: 0.875315, Accuracy: 89.24%\n",
      "Batch 61, Loss: 0.838429, Accuracy: 89.27%\n",
      "Batch 62, Loss: 0.818650, Accuracy: 89.31%\n",
      "Batch 63, Loss: 0.851821, Accuracy: 89.31%\n",
      "Batch 64, Loss: 0.851561, Accuracy: 89.31%\n",
      "Batch 65, Loss: 0.820511, Accuracy: 89.35%\n",
      "Batch 66, Loss: 0.830713, Accuracy: 89.39%\n",
      "Batch 67, Loss: 0.843262, Accuracy: 89.41%\n",
      "Batch 68, Loss: 0.801783, Accuracy: 89.48%\n",
      "Batch 69, Loss: 0.875748, Accuracy: 89.45%\n",
      "Batch 70, Loss: 0.843325, Accuracy: 89.44%\n",
      "Batch 71, Loss: 0.894969, Accuracy: 89.35%\n",
      "Batch 72, Loss: 0.821643, Accuracy: 89.41%\n",
      "Batch 73, Loss: 0.899274, Accuracy: 89.34%\n",
      "Batch 74, Loss: 0.841710, Accuracy: 89.36%\n",
      "Batch 75, Loss: 0.802323, Accuracy: 89.44%\n",
      "Batch 76, Loss: 0.879534, Accuracy: 89.39%\n",
      "Batch 77, Loss: 0.839269, Accuracy: 89.41%\n",
      "Batch 78, Loss: 0.885471, Accuracy: 89.34%\n",
      "Batch 79, Loss: 0.822648, Accuracy: 89.40%\n",
      "Batch 80, Loss: 0.809404, Accuracy: 89.45%\n",
      "Batch 81, Loss: 0.842024, Accuracy: 89.47%\n",
      "Batch 82, Loss: 0.902863, Accuracy: 89.41%\n",
      "Batch 83, Loss: 0.821221, Accuracy: 89.44%\n",
      "Batch 84, Loss: 0.846714, Accuracy: 89.43%\n",
      "Batch 85, Loss: 0.824130, Accuracy: 89.47%\n",
      "Batch 86, Loss: 0.872186, Accuracy: 89.43%\n",
      "Batch 87, Loss: 0.775604, Accuracy: 89.51%\n",
      "Batch 88, Loss: 0.789288, Accuracy: 89.58%\n",
      "Batch 89, Loss: 0.854327, Accuracy: 89.57%\n",
      "Batch 90, Loss: 0.913981, Accuracy: 89.50%\n",
      "Batch 91, Loss: 0.945934, Accuracy: 89.39%\n",
      "Batch 92, Loss: 0.906095, Accuracy: 89.32%\n",
      "Batch 93, Loss: 0.867135, Accuracy: 89.30%\n",
      "Batch 94, Loss: 0.848929, Accuracy: 89.31%\n",
      "Batch 95, Loss: 0.803130, Accuracy: 89.36%\n",
      "Batch 96, Loss: 0.806806, Accuracy: 89.40%\n",
      "Batch 97, Loss: 0.905782, Accuracy: 89.35%\n",
      "Batch 98, Loss: 0.868448, Accuracy: 89.35%\n",
      "Batch 99, Loss: 0.826025, Accuracy: 89.36%\n",
      "Batch 100, Loss: 0.828766, Accuracy: 89.38%\n",
      "Batch 101, Loss: 0.819922, Accuracy: 89.40%\n",
      "Batch 102, Loss: 0.850812, Accuracy: 89.41%\n",
      "Batch 103, Loss: 0.879722, Accuracy: 89.38%\n",
      "Batch 104, Loss: 0.817316, Accuracy: 89.42%\n",
      "Batch 105, Loss: 0.807085, Accuracy: 89.46%\n",
      "Batch 106, Loss: 0.810917, Accuracy: 89.50%\n",
      "Batch 107, Loss: 0.853847, Accuracy: 89.50%\n",
      "Batch 108, Loss: 0.765295, Accuracy: 89.58%\n",
      "Batch 109, Loss: 0.882258, Accuracy: 89.55%\n",
      "Batch 110, Loss: 0.886267, Accuracy: 89.52%\n",
      "Batch 111, Loss: 0.828236, Accuracy: 89.51%\n",
      "Batch 112, Loss: 0.788631, Accuracy: 89.55%\n",
      "Batch 113, Loss: 0.869684, Accuracy: 89.52%\n",
      "Batch 114, Loss: 0.832331, Accuracy: 89.54%\n",
      "Batch 115, Loss: 0.821327, Accuracy: 89.58%\n",
      "Batch 116, Loss: 0.916245, Accuracy: 89.52%\n",
      "Batch 117, Loss: 0.855167, Accuracy: 89.52%\n",
      "Batch 118, Loss: 0.866695, Accuracy: 89.51%\n",
      "Batch 119, Loss: 0.844738, Accuracy: 89.54%\n",
      "Batch 120, Loss: 0.836014, Accuracy: 89.54%\n",
      "Batch 121, Loss: 0.826461, Accuracy: 89.57%\n",
      "Batch 122, Loss: 0.813219, Accuracy: 89.60%\n",
      "Batch 123, Loss: 0.873833, Accuracy: 89.58%\n",
      "Batch 124, Loss: 0.867453, Accuracy: 89.58%\n",
      "Batch 125, Loss: 0.855519, Accuracy: 89.58%\n",
      "Batch 126, Loss: 0.817963, Accuracy: 89.60%\n",
      "Batch 127, Loss: 0.849970, Accuracy: 89.59%\n",
      "Batch 128, Loss: 0.875585, Accuracy: 89.55%\n",
      "Batch 129, Loss: 0.839671, Accuracy: 89.56%\n",
      "Batch 130, Loss: 0.866407, Accuracy: 89.56%\n",
      "Batch 131, Loss: 0.897172, Accuracy: 89.52%\n",
      "Batch 132, Loss: 0.808151, Accuracy: 89.55%\n",
      "Batch 133, Loss: 0.824727, Accuracy: 89.57%\n",
      "Batch 134, Loss: 0.947069, Accuracy: 89.51%\n",
      "Batch 135, Loss: 0.849292, Accuracy: 89.51%\n",
      "Batch 136, Loss: 0.789414, Accuracy: 89.56%\n",
      "Batch 137, Loss: 0.856080, Accuracy: 89.55%\n",
      "Batch 138, Loss: 0.833334, Accuracy: 89.56%\n",
      "Batch 139, Loss: 0.820588, Accuracy: 89.58%\n",
      "Batch 140, Loss: 0.820136, Accuracy: 89.61%\n",
      "Batch 141, Loss: 0.883152, Accuracy: 89.58%\n",
      "Batch 142, Loss: 0.831366, Accuracy: 89.59%\n",
      "Batch 143, Loss: 0.851039, Accuracy: 89.59%\n",
      "Batch 144, Loss: 0.796836, Accuracy: 89.63%\n",
      "Batch 145, Loss: 0.884546, Accuracy: 89.60%\n",
      "Batch 146, Loss: 0.833699, Accuracy: 89.61%\n",
      "Batch 147, Loss: 0.858159, Accuracy: 89.60%\n",
      "Batch 148, Loss: 0.778129, Accuracy: 89.65%\n",
      "Batch 149, Loss: 0.841563, Accuracy: 89.65%\n",
      "Batch 150, Loss: 0.833072, Accuracy: 89.65%\n",
      "Batch 151, Loss: 0.806380, Accuracy: 89.68%\n",
      "Batch 152, Loss: 0.823969, Accuracy: 89.70%\n",
      "Batch 153, Loss: 0.837665, Accuracy: 89.71%\n",
      "Batch 154, Loss: 0.850626, Accuracy: 89.70%\n",
      "Batch 155, Loss: 0.823481, Accuracy: 89.73%\n",
      "Batch 156, Loss: 0.828437, Accuracy: 89.73%\n",
      "Batch 157, Loss: 0.882493, Accuracy: 89.70%\n",
      "Batch 158, Loss: 0.818422, Accuracy: 89.72%\n",
      "Batch 159, Loss: 0.794350, Accuracy: 89.75%\n",
      "Batch 160, Loss: 0.792029, Accuracy: 89.79%\n",
      "Batch 161, Loss: 0.813933, Accuracy: 89.81%\n",
      "Batch 162, Loss: 0.851016, Accuracy: 89.80%\n",
      "Batch 163, Loss: 0.887531, Accuracy: 89.77%\n",
      "Batch 164, Loss: 0.851408, Accuracy: 89.77%\n",
      "Batch 165, Loss: 0.825075, Accuracy: 89.78%\n",
      "Batch 166, Loss: 0.859358, Accuracy: 89.77%\n",
      "Batch 167, Loss: 0.833764, Accuracy: 89.78%\n",
      "Batch 168, Loss: 0.846145, Accuracy: 89.79%\n",
      "Batch 169, Loss: 0.790976, Accuracy: 89.82%\n",
      "Batch 170, Loss: 0.842010, Accuracy: 89.82%\n",
      "Batch 171, Loss: 0.911956, Accuracy: 89.78%\n",
      "Batch 172, Loss: 0.890470, Accuracy: 89.75%\n",
      "Batch 173, Loss: 0.901309, Accuracy: 89.71%\n",
      "Batch 174, Loss: 0.822775, Accuracy: 89.73%\n",
      "Batch 175, Loss: 0.863372, Accuracy: 89.71%\n",
      "Batch 176, Loss: 0.815189, Accuracy: 89.72%\n",
      "Batch 177, Loss: 0.855073, Accuracy: 89.72%\n",
      "Batch 178, Loss: 0.881961, Accuracy: 89.70%\n",
      "Batch 179, Loss: 0.909808, Accuracy: 89.66%\n",
      "Batch 180, Loss: 0.831720, Accuracy: 89.67%\n",
      "Batch 181, Loss: 0.927564, Accuracy: 89.61%\n",
      "Batch 182, Loss: 0.867043, Accuracy: 89.59%\n",
      "Batch 183, Loss: 0.867211, Accuracy: 89.58%\n",
      "Batch 184, Loss: 0.860887, Accuracy: 89.57%\n",
      "Batch 185, Loss: 0.846759, Accuracy: 89.57%\n",
      "Batch 186, Loss: 0.823987, Accuracy: 89.58%\n",
      "Batch 187, Loss: 0.816707, Accuracy: 89.61%\n",
      "Batch 188, Loss: 0.834054, Accuracy: 89.62%\n",
      "Batch 189, Loss: 0.820073, Accuracy: 89.63%\n",
      "Batch 190, Loss: 0.907006, Accuracy: 89.60%\n",
      "Batch 191, Loss: 0.791540, Accuracy: 89.63%\n",
      "Batch 192, Loss: 0.868088, Accuracy: 89.61%\n",
      "Batch 193, Loss: 0.827986, Accuracy: 89.62%\n",
      "Batch 194, Loss: 0.820182, Accuracy: 89.63%\n",
      "Batch 195, Loss: 0.894528, Accuracy: 89.60%\n",
      "Batch 196, Loss: 0.858501, Accuracy: 89.60%\n",
      "Batch 197, Loss: 0.824513, Accuracy: 89.62%\n",
      "Batch 198, Loss: 0.834022, Accuracy: 89.63%\n",
      "Batch 199, Loss: 0.920852, Accuracy: 89.59%\n",
      "Batch 200, Loss: 0.829636, Accuracy: 89.60%\n",
      "Batch 201, Loss: 0.844872, Accuracy: 89.61%\n",
      "Batch 202, Loss: 0.880539, Accuracy: 89.60%\n",
      "Batch 203, Loss: 0.864600, Accuracy: 89.59%\n",
      "Batch 204, Loss: 0.831937, Accuracy: 89.60%\n",
      "Batch 205, Loss: 0.902192, Accuracy: 89.57%\n",
      "Batch 206, Loss: 0.872875, Accuracy: 89.56%\n",
      "Batch 207, Loss: 0.867717, Accuracy: 89.56%\n",
      "Batch 208, Loss: 0.857981, Accuracy: 89.56%\n",
      "Batch 209, Loss: 0.942433, Accuracy: 89.52%\n",
      "Batch 210, Loss: 0.855692, Accuracy: 89.52%\n",
      "Batch 211, Loss: 0.828536, Accuracy: 89.53%\n",
      "Batch 212, Loss: 0.831942, Accuracy: 89.53%\n",
      "Batch 213, Loss: 0.834653, Accuracy: 89.54%\n",
      "Training - Epoch 122, Loss: 0.848929, Accuracy: 89.54%\n",
      "Validation Batch 1, Loss: 0.813832, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.791433, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.824931, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.829403, Accuracy: 92.97%\n",
      "Validation Batch 5, Loss: 0.810298, Accuracy: 93.44%\n",
      "Validation Batch 6, Loss: 0.788760, Accuracy: 93.75%\n",
      "Validation Batch 7, Loss: 0.792906, Accuracy: 93.97%\n",
      "Validation Batch 8, Loss: 0.850849, Accuracy: 93.55%\n",
      "Validation Batch 9, Loss: 0.877489, Accuracy: 92.88%\n",
      "Validation Batch 10, Loss: 0.794894, Accuracy: 93.12%\n",
      "Validation Batch 11, Loss: 0.822394, Accuracy: 93.04%\n",
      "Validation Batch 12, Loss: 0.827594, Accuracy: 92.97%\n",
      "Validation Batch 13, Loss: 0.829662, Accuracy: 92.91%\n",
      "Validation Batch 14, Loss: 0.833891, Accuracy: 92.86%\n",
      "Validation Batch 15, Loss: 0.794744, Accuracy: 93.02%\n",
      "Validation Batch 16, Loss: 0.823635, Accuracy: 92.97%\n",
      "Validation Batch 17, Loss: 0.866213, Accuracy: 92.74%\n",
      "Validation Batch 18, Loss: 0.793970, Accuracy: 92.88%\n",
      "Validation Batch 19, Loss: 0.867747, Accuracy: 92.60%\n",
      "Validation Batch 20, Loss: 0.802834, Accuracy: 92.81%\n",
      "Validation Batch 21, Loss: 0.855501, Accuracy: 92.63%\n",
      "Validation Batch 22, Loss: 0.816774, Accuracy: 92.61%\n",
      "Validation Batch 23, Loss: 0.863083, Accuracy: 92.39%\n",
      "Validation Batch 24, Loss: 0.825549, Accuracy: 92.38%\n",
      "Validation Batch 25, Loss: 0.804722, Accuracy: 92.38%\n",
      "Validation Batch 26, Loss: 0.832788, Accuracy: 92.37%\n",
      "Validation Batch 27, Loss: 0.798816, Accuracy: 92.43%\n",
      "Validation - Epoch 122, Loss: 0.823508, Accuracy: 92.43%\n",
      "Patience—7\n",
      "Epoch 123\n",
      "Batch 1, Loss: 0.808724, Accuracy: 93.75%\n",
      "Batch 2, Loss: 0.921876, Accuracy: 88.28%\n",
      "Batch 3, Loss: 0.858732, Accuracy: 88.02%\n",
      "Batch 4, Loss: 0.846220, Accuracy: 88.67%\n",
      "Batch 5, Loss: 0.840688, Accuracy: 89.06%\n",
      "Batch 6, Loss: 0.806063, Accuracy: 89.58%\n",
      "Batch 7, Loss: 0.917317, Accuracy: 88.39%\n",
      "Batch 8, Loss: 0.839998, Accuracy: 88.67%\n",
      "Batch 9, Loss: 0.846623, Accuracy: 88.89%\n",
      "Batch 10, Loss: 0.886374, Accuracy: 88.59%\n",
      "Batch 11, Loss: 0.883677, Accuracy: 88.35%\n",
      "Batch 12, Loss: 0.820472, Accuracy: 88.67%\n",
      "Batch 13, Loss: 0.898624, Accuracy: 88.34%\n",
      "Batch 14, Loss: 0.844126, Accuracy: 88.28%\n",
      "Batch 15, Loss: 0.818712, Accuracy: 88.65%\n",
      "Batch 16, Loss: 0.812550, Accuracy: 88.96%\n",
      "Batch 17, Loss: 0.850631, Accuracy: 88.97%\n",
      "Batch 18, Loss: 0.852889, Accuracy: 89.06%\n",
      "Batch 19, Loss: 0.825089, Accuracy: 89.23%\n",
      "Batch 20, Loss: 0.893742, Accuracy: 88.98%\n",
      "Batch 21, Loss: 0.887448, Accuracy: 88.84%\n",
      "Batch 22, Loss: 0.843455, Accuracy: 88.92%\n",
      "Batch 23, Loss: 0.811638, Accuracy: 89.06%\n",
      "Batch 24, Loss: 0.807120, Accuracy: 89.26%\n",
      "Batch 25, Loss: 0.825863, Accuracy: 89.38%\n",
      "Batch 26, Loss: 0.760469, Accuracy: 89.72%\n",
      "Batch 27, Loss: 0.845487, Accuracy: 89.70%\n",
      "Batch 28, Loss: 0.846480, Accuracy: 89.73%\n",
      "Batch 29, Loss: 0.849471, Accuracy: 89.71%\n",
      "Batch 30, Loss: 0.885853, Accuracy: 89.58%\n",
      "Batch 31, Loss: 0.792258, Accuracy: 89.82%\n",
      "Batch 32, Loss: 0.828637, Accuracy: 89.84%\n",
      "Batch 33, Loss: 0.809891, Accuracy: 89.96%\n",
      "Batch 34, Loss: 0.800550, Accuracy: 90.07%\n",
      "Batch 35, Loss: 0.844551, Accuracy: 90.09%\n",
      "Batch 36, Loss: 0.832538, Accuracy: 90.10%\n",
      "Batch 37, Loss: 0.807019, Accuracy: 90.16%\n",
      "Batch 38, Loss: 0.794699, Accuracy: 90.30%\n",
      "Batch 39, Loss: 0.837360, Accuracy: 90.30%\n",
      "Batch 40, Loss: 0.872186, Accuracy: 90.20%\n",
      "Batch 41, Loss: 0.840654, Accuracy: 90.17%\n",
      "Batch 42, Loss: 0.932803, Accuracy: 89.92%\n",
      "Batch 43, Loss: 0.870445, Accuracy: 89.86%\n",
      "Batch 44, Loss: 0.821258, Accuracy: 89.91%\n",
      "Batch 45, Loss: 0.834775, Accuracy: 89.93%\n",
      "Batch 46, Loss: 0.850463, Accuracy: 89.88%\n",
      "Batch 47, Loss: 0.959017, Accuracy: 89.63%\n",
      "Batch 48, Loss: 0.824638, Accuracy: 89.68%\n",
      "Batch 49, Loss: 0.794957, Accuracy: 89.80%\n",
      "Batch 50, Loss: 0.836398, Accuracy: 89.81%\n",
      "Batch 51, Loss: 0.805363, Accuracy: 89.89%\n",
      "Batch 52, Loss: 0.847616, Accuracy: 89.90%\n",
      "Batch 53, Loss: 0.880596, Accuracy: 89.83%\n",
      "Batch 54, Loss: 0.878301, Accuracy: 89.76%\n",
      "Batch 55, Loss: 0.817004, Accuracy: 89.80%\n",
      "Batch 56, Loss: 0.811781, Accuracy: 89.87%\n",
      "Batch 57, Loss: 0.898437, Accuracy: 89.80%\n",
      "Batch 58, Loss: 0.875460, Accuracy: 89.76%\n",
      "Batch 59, Loss: 0.891057, Accuracy: 89.70%\n",
      "Batch 60, Loss: 0.858738, Accuracy: 89.69%\n",
      "Batch 61, Loss: 0.806567, Accuracy: 89.70%\n",
      "Batch 62, Loss: 0.871113, Accuracy: 89.69%\n",
      "Batch 63, Loss: 0.888579, Accuracy: 89.63%\n",
      "Batch 64, Loss: 0.809273, Accuracy: 89.70%\n",
      "Batch 65, Loss: 0.830167, Accuracy: 89.74%\n",
      "Batch 66, Loss: 0.901015, Accuracy: 89.68%\n",
      "Batch 67, Loss: 0.888898, Accuracy: 89.62%\n",
      "Batch 68, Loss: 0.791227, Accuracy: 89.73%\n",
      "Batch 69, Loss: 0.845909, Accuracy: 89.72%\n",
      "Batch 70, Loss: 0.826665, Accuracy: 89.75%\n",
      "Batch 71, Loss: 0.863482, Accuracy: 89.74%\n",
      "Batch 72, Loss: 0.850498, Accuracy: 89.74%\n",
      "Batch 73, Loss: 0.787782, Accuracy: 89.83%\n",
      "Batch 74, Loss: 0.823205, Accuracy: 89.86%\n",
      "Batch 75, Loss: 0.866534, Accuracy: 89.83%\n",
      "Batch 76, Loss: 0.825431, Accuracy: 89.86%\n",
      "Batch 77, Loss: 0.851657, Accuracy: 89.87%\n",
      "Batch 78, Loss: 0.821638, Accuracy: 89.92%\n",
      "Batch 79, Loss: 0.832014, Accuracy: 89.95%\n",
      "Batch 80, Loss: 0.843728, Accuracy: 89.96%\n",
      "Batch 81, Loss: 0.857991, Accuracy: 89.95%\n",
      "Batch 82, Loss: 0.844976, Accuracy: 89.96%\n",
      "Batch 83, Loss: 0.833040, Accuracy: 89.97%\n",
      "Batch 84, Loss: 0.852555, Accuracy: 89.94%\n",
      "Batch 85, Loss: 0.874186, Accuracy: 89.89%\n",
      "Batch 86, Loss: 0.835010, Accuracy: 89.88%\n",
      "Batch 87, Loss: 0.914905, Accuracy: 89.82%\n",
      "Batch 88, Loss: 0.856902, Accuracy: 89.83%\n",
      "Batch 89, Loss: 0.870114, Accuracy: 89.78%\n",
      "Batch 90, Loss: 0.869551, Accuracy: 89.76%\n",
      "Batch 91, Loss: 0.915350, Accuracy: 89.65%\n",
      "Batch 92, Loss: 0.838257, Accuracy: 89.64%\n",
      "Batch 93, Loss: 0.877067, Accuracy: 89.60%\n",
      "Batch 94, Loss: 0.809072, Accuracy: 89.64%\n",
      "Batch 95, Loss: 0.844878, Accuracy: 89.65%\n",
      "Batch 96, Loss: 0.860339, Accuracy: 89.63%\n",
      "Batch 97, Loss: 0.856869, Accuracy: 89.61%\n",
      "Batch 98, Loss: 0.885967, Accuracy: 89.57%\n",
      "Batch 99, Loss: 0.855674, Accuracy: 89.57%\n",
      "Batch 100, Loss: 0.953845, Accuracy: 89.45%\n",
      "Batch 101, Loss: 0.832376, Accuracy: 89.46%\n",
      "Batch 102, Loss: 0.835222, Accuracy: 89.48%\n",
      "Batch 103, Loss: 0.830185, Accuracy: 89.49%\n",
      "Batch 104, Loss: 0.782225, Accuracy: 89.54%\n",
      "Batch 105, Loss: 0.825703, Accuracy: 89.58%\n",
      "Batch 106, Loss: 0.843970, Accuracy: 89.58%\n",
      "Batch 107, Loss: 0.869318, Accuracy: 89.56%\n",
      "Batch 108, Loss: 0.880609, Accuracy: 89.53%\n",
      "Batch 109, Loss: 0.816844, Accuracy: 89.55%\n",
      "Batch 110, Loss: 0.891086, Accuracy: 89.52%\n",
      "Batch 111, Loss: 0.917956, Accuracy: 89.46%\n",
      "Batch 112, Loss: 0.850872, Accuracy: 89.47%\n",
      "Batch 113, Loss: 0.830746, Accuracy: 89.49%\n",
      "Batch 114, Loss: 0.819022, Accuracy: 89.51%\n",
      "Batch 115, Loss: 0.835072, Accuracy: 89.51%\n",
      "Batch 116, Loss: 0.823710, Accuracy: 89.53%\n",
      "Batch 117, Loss: 0.864554, Accuracy: 89.52%\n",
      "Batch 118, Loss: 0.885419, Accuracy: 89.47%\n",
      "Batch 119, Loss: 0.825476, Accuracy: 89.50%\n",
      "Batch 120, Loss: 0.888435, Accuracy: 89.45%\n",
      "Batch 121, Loss: 0.818828, Accuracy: 89.48%\n",
      "Batch 122, Loss: 0.815660, Accuracy: 89.50%\n",
      "Batch 123, Loss: 0.865932, Accuracy: 89.47%\n",
      "Batch 124, Loss: 0.905621, Accuracy: 89.42%\n",
      "Batch 125, Loss: 0.798803, Accuracy: 89.46%\n",
      "Batch 126, Loss: 0.845497, Accuracy: 89.46%\n",
      "Batch 127, Loss: 0.873091, Accuracy: 89.44%\n",
      "Batch 128, Loss: 0.782712, Accuracy: 89.50%\n",
      "Batch 129, Loss: 0.828638, Accuracy: 89.52%\n",
      "Batch 130, Loss: 0.818777, Accuracy: 89.56%\n",
      "Batch 131, Loss: 0.914118, Accuracy: 89.50%\n",
      "Batch 132, Loss: 0.831479, Accuracy: 89.52%\n",
      "Batch 133, Loss: 0.860940, Accuracy: 89.52%\n",
      "Batch 134, Loss: 0.846310, Accuracy: 89.52%\n",
      "Batch 135, Loss: 0.801151, Accuracy: 89.55%\n",
      "Batch 136, Loss: 0.899199, Accuracy: 89.51%\n",
      "Batch 137, Loss: 0.846593, Accuracy: 89.52%\n",
      "Batch 138, Loss: 0.850360, Accuracy: 89.52%\n",
      "Batch 139, Loss: 0.816909, Accuracy: 89.55%\n",
      "Batch 140, Loss: 0.868775, Accuracy: 89.54%\n",
      "Batch 141, Loss: 0.852401, Accuracy: 89.54%\n",
      "Batch 142, Loss: 0.884306, Accuracy: 89.51%\n",
      "Batch 143, Loss: 0.784515, Accuracy: 89.57%\n",
      "Batch 144, Loss: 0.901424, Accuracy: 89.52%\n",
      "Batch 145, Loss: 0.889410, Accuracy: 89.48%\n",
      "Batch 146, Loss: 0.856576, Accuracy: 89.48%\n",
      "Batch 147, Loss: 0.783649, Accuracy: 89.52%\n",
      "Batch 148, Loss: 0.878014, Accuracy: 89.50%\n",
      "Batch 149, Loss: 0.876456, Accuracy: 89.47%\n",
      "Batch 150, Loss: 0.841952, Accuracy: 89.49%\n",
      "Batch 151, Loss: 0.888862, Accuracy: 89.46%\n",
      "Batch 152, Loss: 0.874282, Accuracy: 89.44%\n",
      "Batch 153, Loss: 0.868127, Accuracy: 89.43%\n",
      "Batch 154, Loss: 0.785259, Accuracy: 89.48%\n",
      "Batch 155, Loss: 0.889560, Accuracy: 89.46%\n",
      "Batch 156, Loss: 0.847472, Accuracy: 89.46%\n",
      "Batch 157, Loss: 0.910149, Accuracy: 89.43%\n",
      "Batch 158, Loss: 0.829702, Accuracy: 89.44%\n",
      "Batch 159, Loss: 0.878284, Accuracy: 89.43%\n",
      "Batch 160, Loss: 0.890588, Accuracy: 89.39%\n",
      "Batch 161, Loss: 0.833696, Accuracy: 89.40%\n",
      "Batch 162, Loss: 0.792476, Accuracy: 89.44%\n",
      "Batch 163, Loss: 0.823817, Accuracy: 89.46%\n",
      "Batch 164, Loss: 0.859967, Accuracy: 89.45%\n",
      "Batch 165, Loss: 0.882083, Accuracy: 89.43%\n",
      "Batch 166, Loss: 0.844832, Accuracy: 89.44%\n",
      "Batch 167, Loss: 0.886870, Accuracy: 89.42%\n",
      "Batch 168, Loss: 0.825265, Accuracy: 89.44%\n",
      "Batch 169, Loss: 0.828344, Accuracy: 89.47%\n",
      "Batch 170, Loss: 0.845696, Accuracy: 89.48%\n",
      "Batch 171, Loss: 0.799560, Accuracy: 89.52%\n",
      "Batch 172, Loss: 0.871466, Accuracy: 89.51%\n",
      "Batch 173, Loss: 0.872158, Accuracy: 89.49%\n",
      "Batch 174, Loss: 0.807560, Accuracy: 89.51%\n",
      "Batch 175, Loss: 0.858718, Accuracy: 89.51%\n",
      "Batch 176, Loss: 0.835359, Accuracy: 89.52%\n",
      "Batch 177, Loss: 0.949245, Accuracy: 89.46%\n",
      "Batch 178, Loss: 0.878729, Accuracy: 89.45%\n",
      "Batch 179, Loss: 0.863873, Accuracy: 89.44%\n",
      "Batch 180, Loss: 0.796733, Accuracy: 89.47%\n",
      "Batch 181, Loss: 0.838946, Accuracy: 89.48%\n",
      "Batch 182, Loss: 0.815448, Accuracy: 89.50%\n",
      "Batch 183, Loss: 0.840244, Accuracy: 89.51%\n",
      "Batch 184, Loss: 0.872519, Accuracy: 89.49%\n",
      "Batch 185, Loss: 0.840479, Accuracy: 89.49%\n",
      "Batch 186, Loss: 0.824888, Accuracy: 89.52%\n",
      "Batch 187, Loss: 0.849717, Accuracy: 89.52%\n",
      "Batch 188, Loss: 0.841247, Accuracy: 89.52%\n",
      "Batch 189, Loss: 0.830183, Accuracy: 89.53%\n",
      "Batch 190, Loss: 0.832777, Accuracy: 89.55%\n",
      "Batch 191, Loss: 0.852253, Accuracy: 89.55%\n",
      "Batch 192, Loss: 0.876331, Accuracy: 89.53%\n",
      "Batch 193, Loss: 0.917067, Accuracy: 89.50%\n",
      "Batch 194, Loss: 0.829582, Accuracy: 89.51%\n",
      "Batch 195, Loss: 0.846542, Accuracy: 89.52%\n",
      "Batch 196, Loss: 0.878896, Accuracy: 89.51%\n",
      "Batch 197, Loss: 0.899024, Accuracy: 89.48%\n",
      "Batch 198, Loss: 0.899872, Accuracy: 89.46%\n",
      "Batch 199, Loss: 0.872779, Accuracy: 89.45%\n",
      "Batch 200, Loss: 0.845139, Accuracy: 89.45%\n",
      "Batch 201, Loss: 0.863914, Accuracy: 89.44%\n",
      "Batch 202, Loss: 0.882235, Accuracy: 89.42%\n",
      "Batch 203, Loss: 0.782569, Accuracy: 89.46%\n",
      "Batch 204, Loss: 0.830121, Accuracy: 89.47%\n",
      "Batch 205, Loss: 0.837286, Accuracy: 89.47%\n",
      "Batch 206, Loss: 0.845170, Accuracy: 89.48%\n",
      "Batch 207, Loss: 0.899882, Accuracy: 89.46%\n",
      "Batch 208, Loss: 0.901444, Accuracy: 89.44%\n",
      "Batch 209, Loss: 0.904652, Accuracy: 89.41%\n",
      "Batch 210, Loss: 0.832829, Accuracy: 89.42%\n",
      "Batch 211, Loss: 0.851108, Accuracy: 89.41%\n",
      "Batch 212, Loss: 0.913085, Accuracy: 89.37%\n",
      "Batch 213, Loss: 0.829365, Accuracy: 89.38%\n",
      "Training - Epoch 123, Loss: 0.850709, Accuracy: 89.38%\n",
      "Validation Batch 1, Loss: 0.820727, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.790286, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.823359, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.843313, Accuracy: 92.97%\n",
      "Validation Batch 5, Loss: 0.821573, Accuracy: 92.81%\n",
      "Validation Batch 6, Loss: 0.794654, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.796811, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.859494, Accuracy: 92.77%\n",
      "Validation Batch 9, Loss: 0.887971, Accuracy: 91.84%\n",
      "Validation Batch 10, Loss: 0.791663, Accuracy: 92.34%\n",
      "Validation Batch 11, Loss: 0.828676, Accuracy: 92.33%\n",
      "Validation Batch 12, Loss: 0.832503, Accuracy: 92.32%\n",
      "Validation Batch 13, Loss: 0.835816, Accuracy: 92.19%\n",
      "Validation Batch 14, Loss: 0.843666, Accuracy: 92.08%\n",
      "Validation Batch 15, Loss: 0.791302, Accuracy: 92.40%\n",
      "Validation Batch 16, Loss: 0.830607, Accuracy: 92.29%\n",
      "Validation Batch 17, Loss: 0.862879, Accuracy: 92.00%\n",
      "Validation Batch 18, Loss: 0.801566, Accuracy: 92.19%\n",
      "Validation Batch 19, Loss: 0.858352, Accuracy: 92.02%\n",
      "Validation Batch 20, Loss: 0.818666, Accuracy: 92.11%\n",
      "Validation Batch 21, Loss: 0.858890, Accuracy: 91.96%\n",
      "Validation Batch 22, Loss: 0.797289, Accuracy: 92.19%\n",
      "Validation Batch 23, Loss: 0.858702, Accuracy: 92.05%\n",
      "Validation Batch 24, Loss: 0.824171, Accuracy: 92.06%\n",
      "Validation Batch 25, Loss: 0.802324, Accuracy: 92.19%\n",
      "Validation Batch 26, Loss: 0.837996, Accuracy: 92.19%\n",
      "Validation Batch 27, Loss: 0.789547, Accuracy: 92.25%\n",
      "Validation - Epoch 123, Loss: 0.826030, Accuracy: 92.25%\n",
      "Patience—8\n",
      "Epoch 124\n",
      "Batch 1, Loss: 0.868573, Accuracy: 87.50%\n",
      "Batch 2, Loss: 0.824338, Accuracy: 90.62%\n",
      "Batch 3, Loss: 0.839889, Accuracy: 90.62%\n",
      "Batch 4, Loss: 0.875386, Accuracy: 89.84%\n",
      "Batch 5, Loss: 0.854090, Accuracy: 89.38%\n",
      "Batch 6, Loss: 0.904390, Accuracy: 88.28%\n",
      "Batch 7, Loss: 0.824651, Accuracy: 88.84%\n",
      "Batch 8, Loss: 0.835457, Accuracy: 89.26%\n",
      "Batch 9, Loss: 0.799664, Accuracy: 89.93%\n",
      "Batch 10, Loss: 0.813550, Accuracy: 90.16%\n",
      "Batch 11, Loss: 0.817768, Accuracy: 90.48%\n",
      "Batch 12, Loss: 0.775075, Accuracy: 91.02%\n",
      "Batch 13, Loss: 0.813506, Accuracy: 91.23%\n",
      "Batch 14, Loss: 0.820845, Accuracy: 91.41%\n",
      "Batch 15, Loss: 0.843406, Accuracy: 91.35%\n",
      "Batch 16, Loss: 0.881869, Accuracy: 91.11%\n",
      "Batch 17, Loss: 0.821237, Accuracy: 91.27%\n",
      "Batch 18, Loss: 0.837044, Accuracy: 91.23%\n",
      "Batch 19, Loss: 0.864658, Accuracy: 91.04%\n",
      "Batch 20, Loss: 0.835804, Accuracy: 91.02%\n",
      "Batch 21, Loss: 0.822609, Accuracy: 91.07%\n",
      "Batch 22, Loss: 0.856637, Accuracy: 90.91%\n",
      "Batch 23, Loss: 0.907486, Accuracy: 90.56%\n",
      "Batch 24, Loss: 0.876133, Accuracy: 90.36%\n",
      "Batch 25, Loss: 0.874668, Accuracy: 90.25%\n",
      "Batch 26, Loss: 0.823367, Accuracy: 90.32%\n",
      "Batch 27, Loss: 0.837130, Accuracy: 90.28%\n",
      "Batch 28, Loss: 0.836140, Accuracy: 90.29%\n",
      "Batch 29, Loss: 0.860120, Accuracy: 90.19%\n",
      "Batch 30, Loss: 0.847385, Accuracy: 90.26%\n",
      "Batch 31, Loss: 0.819428, Accuracy: 90.32%\n",
      "Batch 32, Loss: 0.828847, Accuracy: 90.38%\n",
      "Batch 33, Loss: 0.852598, Accuracy: 90.34%\n",
      "Batch 34, Loss: 0.875686, Accuracy: 90.21%\n",
      "Batch 35, Loss: 0.864193, Accuracy: 90.13%\n",
      "Batch 36, Loss: 0.808550, Accuracy: 90.23%\n",
      "Batch 37, Loss: 0.860873, Accuracy: 90.16%\n",
      "Batch 38, Loss: 0.828902, Accuracy: 90.21%\n",
      "Batch 39, Loss: 0.839089, Accuracy: 90.22%\n",
      "Batch 40, Loss: 0.843182, Accuracy: 90.20%\n",
      "Batch 41, Loss: 0.875154, Accuracy: 90.09%\n",
      "Batch 42, Loss: 0.929231, Accuracy: 89.88%\n",
      "Batch 43, Loss: 0.814557, Accuracy: 89.97%\n",
      "Batch 44, Loss: 0.780495, Accuracy: 90.09%\n",
      "Batch 45, Loss: 0.909084, Accuracy: 89.93%\n",
      "Batch 46, Loss: 0.856256, Accuracy: 89.88%\n",
      "Batch 47, Loss: 0.806339, Accuracy: 89.96%\n",
      "Batch 48, Loss: 0.953887, Accuracy: 89.75%\n",
      "Batch 49, Loss: 0.889392, Accuracy: 89.67%\n",
      "Batch 50, Loss: 0.839101, Accuracy: 89.66%\n",
      "Batch 51, Loss: 0.819426, Accuracy: 89.71%\n",
      "Batch 52, Loss: 0.817928, Accuracy: 89.78%\n",
      "Batch 53, Loss: 0.809014, Accuracy: 89.86%\n",
      "Batch 54, Loss: 0.880130, Accuracy: 89.81%\n",
      "Batch 55, Loss: 0.854729, Accuracy: 89.80%\n",
      "Batch 56, Loss: 0.833156, Accuracy: 89.84%\n",
      "Batch 57, Loss: 0.862750, Accuracy: 89.80%\n",
      "Batch 58, Loss: 0.815699, Accuracy: 89.87%\n",
      "Batch 59, Loss: 0.817031, Accuracy: 89.91%\n",
      "Batch 60, Loss: 0.898533, Accuracy: 89.82%\n",
      "Batch 61, Loss: 0.813642, Accuracy: 89.88%\n",
      "Batch 62, Loss: 0.872482, Accuracy: 89.84%\n",
      "Batch 63, Loss: 0.797602, Accuracy: 89.93%\n",
      "Batch 64, Loss: 0.831154, Accuracy: 89.94%\n",
      "Batch 65, Loss: 0.867927, Accuracy: 89.90%\n",
      "Batch 66, Loss: 0.842194, Accuracy: 89.89%\n",
      "Batch 67, Loss: 0.812217, Accuracy: 89.93%\n",
      "Batch 68, Loss: 0.911279, Accuracy: 89.82%\n",
      "Batch 69, Loss: 0.803730, Accuracy: 89.88%\n",
      "Batch 70, Loss: 0.844773, Accuracy: 89.87%\n",
      "Batch 71, Loss: 0.799684, Accuracy: 89.94%\n",
      "Batch 72, Loss: 0.825945, Accuracy: 89.97%\n",
      "Batch 73, Loss: 0.893249, Accuracy: 89.90%\n",
      "Batch 74, Loss: 0.848611, Accuracy: 89.86%\n",
      "Batch 75, Loss: 0.882379, Accuracy: 89.79%\n",
      "Batch 76, Loss: 0.786838, Accuracy: 89.88%\n",
      "Batch 77, Loss: 0.827942, Accuracy: 89.89%\n",
      "Batch 78, Loss: 0.876831, Accuracy: 89.84%\n",
      "Batch 79, Loss: 0.861421, Accuracy: 89.79%\n",
      "Batch 80, Loss: 0.898416, Accuracy: 89.73%\n",
      "Batch 81, Loss: 0.845881, Accuracy: 89.74%\n",
      "Batch 82, Loss: 0.853429, Accuracy: 89.71%\n",
      "Batch 83, Loss: 0.926976, Accuracy: 89.61%\n",
      "Batch 84, Loss: 0.841753, Accuracy: 89.62%\n",
      "Batch 85, Loss: 0.784690, Accuracy: 89.69%\n",
      "Batch 86, Loss: 0.898257, Accuracy: 89.63%\n",
      "Batch 87, Loss: 0.845708, Accuracy: 89.64%\n",
      "Batch 88, Loss: 0.905014, Accuracy: 89.56%\n",
      "Batch 89, Loss: 0.827371, Accuracy: 89.59%\n",
      "Batch 90, Loss: 0.805780, Accuracy: 89.65%\n",
      "Batch 91, Loss: 0.860679, Accuracy: 89.65%\n",
      "Batch 92, Loss: 0.944622, Accuracy: 89.54%\n",
      "Batch 93, Loss: 0.819611, Accuracy: 89.57%\n",
      "Batch 94, Loss: 0.806278, Accuracy: 89.61%\n",
      "Batch 95, Loss: 0.917518, Accuracy: 89.54%\n",
      "Batch 96, Loss: 0.840646, Accuracy: 89.57%\n",
      "Batch 97, Loss: 0.908223, Accuracy: 89.50%\n",
      "Batch 98, Loss: 0.857188, Accuracy: 89.49%\n",
      "Batch 99, Loss: 0.859801, Accuracy: 89.47%\n",
      "Batch 100, Loss: 0.817684, Accuracy: 89.50%\n",
      "Batch 101, Loss: 0.835362, Accuracy: 89.51%\n",
      "Batch 102, Loss: 0.848997, Accuracy: 89.49%\n",
      "Batch 103, Loss: 0.886151, Accuracy: 89.47%\n",
      "Batch 104, Loss: 0.906767, Accuracy: 89.42%\n",
      "Batch 105, Loss: 0.840960, Accuracy: 89.43%\n",
      "Batch 106, Loss: 0.896570, Accuracy: 89.39%\n",
      "Batch 107, Loss: 0.830359, Accuracy: 89.41%\n",
      "Batch 108, Loss: 0.841027, Accuracy: 89.42%\n",
      "Batch 109, Loss: 0.907492, Accuracy: 89.36%\n",
      "Batch 110, Loss: 0.799998, Accuracy: 89.42%\n",
      "Batch 111, Loss: 0.843302, Accuracy: 89.43%\n",
      "Batch 112, Loss: 0.877329, Accuracy: 89.40%\n",
      "Batch 113, Loss: 0.871417, Accuracy: 89.37%\n",
      "Batch 114, Loss: 0.830067, Accuracy: 89.38%\n",
      "Batch 115, Loss: 0.900960, Accuracy: 89.33%\n",
      "Batch 116, Loss: 0.838560, Accuracy: 89.36%\n",
      "Batch 117, Loss: 0.870462, Accuracy: 89.33%\n",
      "Batch 118, Loss: 0.856031, Accuracy: 89.33%\n",
      "Batch 119, Loss: 0.838409, Accuracy: 89.35%\n",
      "Batch 120, Loss: 0.834707, Accuracy: 89.36%\n",
      "Batch 121, Loss: 0.882595, Accuracy: 89.33%\n",
      "Batch 122, Loss: 0.851289, Accuracy: 89.34%\n",
      "Batch 123, Loss: 0.823236, Accuracy: 89.37%\n",
      "Batch 124, Loss: 0.868347, Accuracy: 89.35%\n",
      "Batch 125, Loss: 0.844149, Accuracy: 89.35%\n",
      "Batch 126, Loss: 0.852864, Accuracy: 89.35%\n",
      "Batch 127, Loss: 0.860655, Accuracy: 89.35%\n",
      "Batch 128, Loss: 0.831423, Accuracy: 89.36%\n",
      "Batch 129, Loss: 0.868509, Accuracy: 89.34%\n",
      "Batch 130, Loss: 0.814568, Accuracy: 89.38%\n",
      "Batch 131, Loss: 0.871989, Accuracy: 89.35%\n",
      "Batch 132, Loss: 0.841237, Accuracy: 89.36%\n",
      "Batch 133, Loss: 0.844858, Accuracy: 89.36%\n",
      "Batch 134, Loss: 0.800188, Accuracy: 89.40%\n",
      "Batch 135, Loss: 0.806386, Accuracy: 89.43%\n",
      "Batch 136, Loss: 0.847757, Accuracy: 89.43%\n",
      "Batch 137, Loss: 0.866121, Accuracy: 89.40%\n",
      "Batch 138, Loss: 0.813913, Accuracy: 89.42%\n",
      "Batch 139, Loss: 0.858056, Accuracy: 89.42%\n",
      "Batch 140, Loss: 0.862341, Accuracy: 89.41%\n",
      "Batch 141, Loss: 0.839710, Accuracy: 89.42%\n",
      "Batch 142, Loss: 0.881460, Accuracy: 89.40%\n",
      "Batch 143, Loss: 0.859328, Accuracy: 89.39%\n",
      "Batch 144, Loss: 0.869806, Accuracy: 89.37%\n",
      "Batch 145, Loss: 0.914772, Accuracy: 89.32%\n",
      "Batch 146, Loss: 0.788142, Accuracy: 89.36%\n",
      "Batch 147, Loss: 0.785983, Accuracy: 89.40%\n",
      "Batch 148, Loss: 0.838688, Accuracy: 89.40%\n",
      "Batch 149, Loss: 0.886564, Accuracy: 89.38%\n",
      "Batch 150, Loss: 0.831559, Accuracy: 89.40%\n",
      "Batch 151, Loss: 0.847313, Accuracy: 89.41%\n",
      "Batch 152, Loss: 0.857216, Accuracy: 89.40%\n",
      "Batch 153, Loss: 0.934267, Accuracy: 89.34%\n",
      "Batch 154, Loss: 0.869128, Accuracy: 89.33%\n",
      "Batch 155, Loss: 0.860359, Accuracy: 89.32%\n",
      "Batch 156, Loss: 0.853720, Accuracy: 89.32%\n",
      "Batch 157, Loss: 0.872104, Accuracy: 89.30%\n",
      "Batch 158, Loss: 0.881405, Accuracy: 89.28%\n",
      "Batch 159, Loss: 0.881812, Accuracy: 89.26%\n",
      "Batch 160, Loss: 0.821543, Accuracy: 89.27%\n",
      "Batch 161, Loss: 0.872719, Accuracy: 89.25%\n",
      "Batch 162, Loss: 0.820867, Accuracy: 89.27%\n",
      "Batch 163, Loss: 0.862878, Accuracy: 89.26%\n",
      "Batch 164, Loss: 0.872027, Accuracy: 89.24%\n",
      "Batch 165, Loss: 0.876323, Accuracy: 89.23%\n",
      "Batch 166, Loss: 0.877253, Accuracy: 89.22%\n",
      "Batch 167, Loss: 0.890386, Accuracy: 89.20%\n",
      "Batch 168, Loss: 0.881898, Accuracy: 89.18%\n",
      "Batch 169, Loss: 0.820738, Accuracy: 89.20%\n",
      "Batch 170, Loss: 0.850347, Accuracy: 89.20%\n",
      "Batch 171, Loss: 0.912614, Accuracy: 89.16%\n",
      "Batch 172, Loss: 0.836200, Accuracy: 89.17%\n",
      "Batch 173, Loss: 0.885670, Accuracy: 89.15%\n",
      "Batch 174, Loss: 0.838522, Accuracy: 89.15%\n",
      "Batch 175, Loss: 0.840087, Accuracy: 89.16%\n",
      "Batch 176, Loss: 0.848599, Accuracy: 89.17%\n",
      "Batch 177, Loss: 0.860693, Accuracy: 89.17%\n",
      "Batch 178, Loss: 0.834822, Accuracy: 89.19%\n",
      "Batch 179, Loss: 0.933853, Accuracy: 89.14%\n",
      "Batch 180, Loss: 0.875492, Accuracy: 89.11%\n",
      "Batch 181, Loss: 0.846273, Accuracy: 89.11%\n",
      "Batch 182, Loss: 0.802972, Accuracy: 89.14%\n",
      "Batch 183, Loss: 0.816288, Accuracy: 89.16%\n",
      "Batch 184, Loss: 0.908171, Accuracy: 89.12%\n",
      "Batch 185, Loss: 0.823058, Accuracy: 89.15%\n",
      "Batch 186, Loss: 0.825781, Accuracy: 89.16%\n",
      "Batch 187, Loss: 0.823026, Accuracy: 89.18%\n",
      "Batch 188, Loss: 0.962381, Accuracy: 89.12%\n",
      "Batch 189, Loss: 0.856740, Accuracy: 89.12%\n",
      "Batch 190, Loss: 0.912442, Accuracy: 89.09%\n",
      "Batch 191, Loss: 0.950206, Accuracy: 89.04%\n",
      "Batch 192, Loss: 0.854944, Accuracy: 89.04%\n",
      "Batch 193, Loss: 0.848001, Accuracy: 89.05%\n",
      "Batch 194, Loss: 0.878998, Accuracy: 89.03%\n",
      "Batch 195, Loss: 0.845198, Accuracy: 89.03%\n",
      "Batch 196, Loss: 0.796591, Accuracy: 89.06%\n",
      "Batch 197, Loss: 0.818025, Accuracy: 89.07%\n",
      "Batch 198, Loss: 0.864628, Accuracy: 89.07%\n",
      "Batch 199, Loss: 0.860267, Accuracy: 89.06%\n",
      "Batch 200, Loss: 0.798239, Accuracy: 89.09%\n",
      "Batch 201, Loss: 0.826998, Accuracy: 89.10%\n",
      "Batch 202, Loss: 0.853023, Accuracy: 89.10%\n",
      "Batch 203, Loss: 0.878951, Accuracy: 89.08%\n",
      "Batch 204, Loss: 0.843679, Accuracy: 89.08%\n",
      "Batch 205, Loss: 0.842904, Accuracy: 89.09%\n",
      "Batch 206, Loss: 0.893549, Accuracy: 89.06%\n",
      "Batch 207, Loss: 0.852889, Accuracy: 89.06%\n",
      "Batch 208, Loss: 0.869060, Accuracy: 89.05%\n",
      "Batch 209, Loss: 0.827623, Accuracy: 89.06%\n",
      "Batch 210, Loss: 0.879890, Accuracy: 89.05%\n",
      "Batch 211, Loss: 0.813036, Accuracy: 89.07%\n",
      "Batch 212, Loss: 0.842019, Accuracy: 89.08%\n",
      "Batch 213, Loss: 0.868763, Accuracy: 89.08%\n",
      "Training - Epoch 124, Loss: 0.852387, Accuracy: 89.08%\n",
      "Validation Batch 1, Loss: 0.827518, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.815417, Accuracy: 93.75%\n",
      "Validation Batch 3, Loss: 0.838983, Accuracy: 92.19%\n",
      "Validation Batch 4, Loss: 0.850145, Accuracy: 91.41%\n",
      "Validation Batch 5, Loss: 0.831414, Accuracy: 91.56%\n",
      "Validation Batch 6, Loss: 0.800952, Accuracy: 91.93%\n",
      "Validation Batch 7, Loss: 0.803153, Accuracy: 92.41%\n",
      "Validation Batch 8, Loss: 0.871065, Accuracy: 91.99%\n",
      "Validation Batch 9, Loss: 0.896075, Accuracy: 90.97%\n",
      "Validation Batch 10, Loss: 0.805224, Accuracy: 91.25%\n",
      "Validation Batch 11, Loss: 0.833328, Accuracy: 91.34%\n",
      "Validation Batch 12, Loss: 0.837267, Accuracy: 91.41%\n",
      "Validation Batch 13, Loss: 0.852491, Accuracy: 91.23%\n",
      "Validation Batch 14, Loss: 0.849510, Accuracy: 91.18%\n",
      "Validation Batch 15, Loss: 0.790913, Accuracy: 91.46%\n",
      "Validation Batch 16, Loss: 0.839103, Accuracy: 91.41%\n",
      "Validation Batch 17, Loss: 0.866947, Accuracy: 91.27%\n",
      "Validation Batch 18, Loss: 0.815908, Accuracy: 91.41%\n",
      "Validation Batch 19, Loss: 0.869479, Accuracy: 91.28%\n",
      "Validation Batch 20, Loss: 0.826956, Accuracy: 91.33%\n",
      "Validation Batch 21, Loss: 0.860854, Accuracy: 91.22%\n",
      "Validation Batch 22, Loss: 0.824571, Accuracy: 91.26%\n",
      "Validation Batch 23, Loss: 0.876287, Accuracy: 91.10%\n",
      "Validation Batch 24, Loss: 0.843707, Accuracy: 91.02%\n",
      "Validation Batch 25, Loss: 0.814569, Accuracy: 91.06%\n",
      "Validation Batch 26, Loss: 0.849883, Accuracy: 90.99%\n",
      "Validation Batch 27, Loss: 0.805391, Accuracy: 91.07%\n",
      "Validation - Epoch 124, Loss: 0.836930, Accuracy: 91.07%\n",
      "Patience—9\n",
      "Epoch 125\n",
      "Batch 1, Loss: 0.863236, Accuracy: 87.50%\n",
      "Batch 2, Loss: 0.791884, Accuracy: 91.41%\n",
      "Batch 3, Loss: 0.818759, Accuracy: 91.67%\n",
      "Batch 4, Loss: 0.841237, Accuracy: 91.41%\n",
      "Batch 5, Loss: 0.901991, Accuracy: 89.69%\n",
      "Batch 6, Loss: 0.932734, Accuracy: 88.28%\n",
      "Batch 7, Loss: 0.833442, Accuracy: 88.62%\n",
      "Batch 8, Loss: 0.827966, Accuracy: 88.87%\n",
      "Batch 9, Loss: 0.808072, Accuracy: 89.41%\n",
      "Batch 10, Loss: 0.800411, Accuracy: 90.00%\n",
      "Batch 11, Loss: 0.799159, Accuracy: 90.34%\n",
      "Batch 12, Loss: 0.864958, Accuracy: 90.10%\n",
      "Batch 13, Loss: 0.840738, Accuracy: 90.02%\n",
      "Batch 14, Loss: 0.833267, Accuracy: 90.07%\n",
      "Batch 15, Loss: 0.854957, Accuracy: 90.10%\n",
      "Batch 16, Loss: 0.827701, Accuracy: 90.14%\n",
      "Batch 17, Loss: 0.859402, Accuracy: 90.07%\n",
      "Batch 18, Loss: 0.847476, Accuracy: 90.02%\n",
      "Batch 19, Loss: 0.813225, Accuracy: 90.13%\n",
      "Batch 20, Loss: 0.793970, Accuracy: 90.39%\n",
      "Batch 21, Loss: 0.851792, Accuracy: 90.40%\n",
      "Batch 22, Loss: 0.837189, Accuracy: 90.48%\n",
      "Batch 23, Loss: 0.872401, Accuracy: 90.29%\n",
      "Batch 24, Loss: 0.861379, Accuracy: 90.17%\n",
      "Batch 25, Loss: 0.828041, Accuracy: 90.25%\n",
      "Batch 26, Loss: 0.853171, Accuracy: 90.20%\n",
      "Batch 27, Loss: 0.798089, Accuracy: 90.39%\n",
      "Batch 28, Loss: 0.905449, Accuracy: 90.12%\n",
      "Batch 29, Loss: 0.962131, Accuracy: 89.71%\n",
      "Batch 30, Loss: 0.767974, Accuracy: 90.00%\n",
      "Batch 31, Loss: 0.873008, Accuracy: 89.87%\n",
      "Batch 32, Loss: 0.880940, Accuracy: 89.75%\n",
      "Batch 33, Loss: 0.823206, Accuracy: 89.82%\n",
      "Batch 34, Loss: 0.828422, Accuracy: 89.94%\n",
      "Batch 35, Loss: 0.851875, Accuracy: 89.91%\n",
      "Batch 36, Loss: 0.856568, Accuracy: 89.89%\n",
      "Batch 37, Loss: 0.837748, Accuracy: 89.91%\n",
      "Batch 38, Loss: 0.797031, Accuracy: 90.05%\n",
      "Batch 39, Loss: 0.795578, Accuracy: 90.18%\n",
      "Batch 40, Loss: 0.866582, Accuracy: 90.12%\n",
      "Batch 41, Loss: 0.868589, Accuracy: 90.02%\n",
      "Batch 42, Loss: 0.834443, Accuracy: 90.03%\n",
      "Batch 43, Loss: 0.847504, Accuracy: 89.97%\n",
      "Batch 44, Loss: 0.810287, Accuracy: 90.06%\n",
      "Batch 45, Loss: 0.804572, Accuracy: 90.14%\n",
      "Batch 46, Loss: 0.883572, Accuracy: 90.01%\n",
      "Batch 47, Loss: 0.885849, Accuracy: 89.93%\n",
      "Batch 48, Loss: 0.841915, Accuracy: 89.91%\n",
      "Batch 49, Loss: 0.818269, Accuracy: 89.99%\n",
      "Batch 50, Loss: 0.823816, Accuracy: 90.03%\n",
      "Batch 51, Loss: 0.936470, Accuracy: 89.86%\n",
      "Batch 52, Loss: 0.866824, Accuracy: 89.81%\n",
      "Batch 53, Loss: 0.850738, Accuracy: 89.83%\n",
      "Batch 54, Loss: 0.799384, Accuracy: 89.90%\n",
      "Batch 55, Loss: 0.869924, Accuracy: 89.86%\n",
      "Batch 56, Loss: 0.881483, Accuracy: 89.79%\n",
      "Batch 57, Loss: 0.839469, Accuracy: 89.80%\n",
      "Batch 58, Loss: 0.851715, Accuracy: 89.82%\n",
      "Batch 59, Loss: 0.813605, Accuracy: 89.88%\n",
      "Batch 60, Loss: 0.914414, Accuracy: 89.77%\n",
      "Batch 61, Loss: 0.773840, Accuracy: 89.88%\n",
      "Batch 62, Loss: 0.897811, Accuracy: 89.77%\n",
      "Batch 63, Loss: 0.850199, Accuracy: 89.76%\n",
      "Batch 64, Loss: 0.882390, Accuracy: 89.70%\n",
      "Batch 65, Loss: 0.841033, Accuracy: 89.71%\n",
      "Batch 66, Loss: 0.795272, Accuracy: 89.77%\n",
      "Batch 67, Loss: 0.873253, Accuracy: 89.74%\n",
      "Batch 68, Loss: 0.868878, Accuracy: 89.71%\n",
      "Batch 69, Loss: 0.838915, Accuracy: 89.72%\n",
      "Batch 70, Loss: 0.901046, Accuracy: 89.64%\n",
      "Batch 71, Loss: 0.887391, Accuracy: 89.59%\n",
      "Batch 72, Loss: 0.822900, Accuracy: 89.63%\n",
      "Batch 73, Loss: 0.774700, Accuracy: 89.73%\n",
      "Batch 74, Loss: 0.811210, Accuracy: 89.78%\n",
      "Batch 75, Loss: 0.873964, Accuracy: 89.75%\n",
      "Batch 76, Loss: 0.807698, Accuracy: 89.80%\n",
      "Batch 77, Loss: 0.795142, Accuracy: 89.89%\n",
      "Batch 78, Loss: 0.859852, Accuracy: 89.88%\n",
      "Batch 79, Loss: 0.864891, Accuracy: 89.85%\n",
      "Batch 80, Loss: 0.865794, Accuracy: 89.82%\n",
      "Batch 81, Loss: 0.866825, Accuracy: 89.81%\n",
      "Batch 82, Loss: 0.879981, Accuracy: 89.79%\n",
      "Batch 83, Loss: 0.817691, Accuracy: 89.83%\n",
      "Batch 84, Loss: 0.833349, Accuracy: 89.84%\n",
      "Batch 85, Loss: 0.879843, Accuracy: 89.80%\n",
      "Batch 86, Loss: 0.857567, Accuracy: 89.79%\n",
      "Batch 87, Loss: 0.898839, Accuracy: 89.71%\n",
      "Batch 88, Loss: 0.860331, Accuracy: 89.68%\n",
      "Batch 89, Loss: 0.829896, Accuracy: 89.69%\n",
      "Batch 90, Loss: 0.910885, Accuracy: 89.60%\n",
      "Batch 91, Loss: 0.886290, Accuracy: 89.56%\n",
      "Batch 92, Loss: 0.830209, Accuracy: 89.59%\n",
      "Batch 93, Loss: 0.832825, Accuracy: 89.62%\n",
      "Batch 94, Loss: 0.810429, Accuracy: 89.66%\n",
      "Batch 95, Loss: 0.839279, Accuracy: 89.69%\n",
      "Batch 96, Loss: 0.830414, Accuracy: 89.70%\n",
      "Batch 97, Loss: 0.876438, Accuracy: 89.67%\n",
      "Batch 98, Loss: 0.877031, Accuracy: 89.64%\n",
      "Batch 99, Loss: 0.822893, Accuracy: 89.66%\n",
      "Batch 100, Loss: 0.907423, Accuracy: 89.61%\n",
      "Batch 101, Loss: 0.847490, Accuracy: 89.62%\n",
      "Batch 102, Loss: 0.809010, Accuracy: 89.66%\n",
      "Batch 103, Loss: 0.898140, Accuracy: 89.59%\n",
      "Batch 104, Loss: 0.824066, Accuracy: 89.63%\n",
      "Batch 105, Loss: 0.817597, Accuracy: 89.66%\n",
      "Batch 106, Loss: 0.855010, Accuracy: 89.65%\n",
      "Batch 107, Loss: 0.904036, Accuracy: 89.60%\n",
      "Batch 108, Loss: 0.821663, Accuracy: 89.63%\n",
      "Batch 109, Loss: 0.887494, Accuracy: 89.58%\n",
      "Batch 110, Loss: 0.835365, Accuracy: 89.59%\n",
      "Batch 111, Loss: 0.870755, Accuracy: 89.57%\n",
      "Batch 112, Loss: 0.918363, Accuracy: 89.49%\n",
      "Batch 113, Loss: 0.838071, Accuracy: 89.50%\n",
      "Batch 114, Loss: 0.825903, Accuracy: 89.53%\n",
      "Batch 115, Loss: 0.824885, Accuracy: 89.55%\n",
      "Batch 116, Loss: 0.943465, Accuracy: 89.44%\n",
      "Batch 117, Loss: 0.862485, Accuracy: 89.42%\n",
      "Batch 118, Loss: 0.790938, Accuracy: 89.47%\n",
      "Batch 119, Loss: 0.849822, Accuracy: 89.47%\n",
      "Batch 120, Loss: 0.850384, Accuracy: 89.47%\n",
      "Batch 121, Loss: 0.867804, Accuracy: 89.44%\n",
      "Batch 122, Loss: 0.931487, Accuracy: 89.37%\n",
      "Batch 123, Loss: 0.819732, Accuracy: 89.41%\n",
      "Batch 124, Loss: 0.874614, Accuracy: 89.36%\n",
      "Batch 125, Loss: 0.914566, Accuracy: 89.31%\n",
      "Batch 126, Loss: 0.894369, Accuracy: 89.27%\n",
      "Batch 127, Loss: 0.848100, Accuracy: 89.27%\n",
      "Batch 128, Loss: 0.799188, Accuracy: 89.32%\n",
      "Batch 129, Loss: 0.789719, Accuracy: 89.38%\n",
      "Batch 130, Loss: 0.866858, Accuracy: 89.36%\n",
      "Batch 131, Loss: 0.807984, Accuracy: 89.40%\n",
      "Batch 132, Loss: 0.856101, Accuracy: 89.41%\n",
      "Batch 133, Loss: 0.834388, Accuracy: 89.41%\n",
      "Batch 134, Loss: 0.820563, Accuracy: 89.45%\n",
      "Batch 135, Loss: 0.875327, Accuracy: 89.42%\n",
      "Batch 136, Loss: 0.857333, Accuracy: 89.42%\n",
      "Batch 137, Loss: 0.833495, Accuracy: 89.43%\n",
      "Batch 138, Loss: 0.888367, Accuracy: 89.40%\n",
      "Batch 139, Loss: 0.813555, Accuracy: 89.43%\n",
      "Batch 140, Loss: 0.863017, Accuracy: 89.44%\n",
      "Batch 141, Loss: 0.900449, Accuracy: 89.41%\n",
      "Batch 142, Loss: 0.861061, Accuracy: 89.40%\n",
      "Batch 143, Loss: 0.858539, Accuracy: 89.39%\n",
      "Batch 144, Loss: 0.809762, Accuracy: 89.42%\n",
      "Batch 145, Loss: 0.858726, Accuracy: 89.41%\n",
      "Batch 146, Loss: 0.821214, Accuracy: 89.43%\n",
      "Batch 147, Loss: 0.818834, Accuracy: 89.46%\n",
      "Batch 148, Loss: 0.777682, Accuracy: 89.52%\n",
      "Batch 149, Loss: 0.800555, Accuracy: 89.56%\n",
      "Batch 150, Loss: 0.838633, Accuracy: 89.56%\n",
      "Batch 151, Loss: 0.866403, Accuracy: 89.55%\n",
      "Batch 152, Loss: 0.768977, Accuracy: 89.61%\n",
      "Batch 153, Loss: 0.791517, Accuracy: 89.64%\n",
      "Batch 154, Loss: 0.903261, Accuracy: 89.62%\n",
      "Batch 155, Loss: 0.852788, Accuracy: 89.62%\n",
      "Batch 156, Loss: 0.844517, Accuracy: 89.61%\n",
      "Batch 157, Loss: 0.824071, Accuracy: 89.64%\n",
      "Batch 158, Loss: 0.851094, Accuracy: 89.64%\n",
      "Batch 159, Loss: 0.888703, Accuracy: 89.60%\n",
      "Batch 160, Loss: 0.793188, Accuracy: 89.64%\n",
      "Batch 161, Loss: 0.831963, Accuracy: 89.64%\n",
      "Batch 162, Loss: 0.841221, Accuracy: 89.65%\n",
      "Batch 163, Loss: 0.888016, Accuracy: 89.63%\n",
      "Batch 164, Loss: 0.883792, Accuracy: 89.61%\n",
      "Batch 165, Loss: 0.859485, Accuracy: 89.59%\n",
      "Batch 166, Loss: 0.909376, Accuracy: 89.54%\n",
      "Batch 167, Loss: 0.874487, Accuracy: 89.53%\n",
      "Batch 168, Loss: 0.830661, Accuracy: 89.56%\n",
      "Batch 169, Loss: 0.877114, Accuracy: 89.52%\n",
      "Batch 170, Loss: 0.768357, Accuracy: 89.58%\n",
      "Batch 171, Loss: 0.842045, Accuracy: 89.57%\n",
      "Batch 172, Loss: 0.831495, Accuracy: 89.59%\n",
      "Batch 173, Loss: 0.849437, Accuracy: 89.58%\n",
      "Batch 174, Loss: 0.810623, Accuracy: 89.60%\n",
      "Batch 175, Loss: 0.877297, Accuracy: 89.59%\n",
      "Batch 176, Loss: 0.793698, Accuracy: 89.61%\n",
      "Batch 177, Loss: 0.856834, Accuracy: 89.61%\n",
      "Batch 178, Loss: 0.921550, Accuracy: 89.56%\n",
      "Batch 179, Loss: 0.868551, Accuracy: 89.55%\n",
      "Batch 180, Loss: 0.842104, Accuracy: 89.56%\n",
      "Batch 181, Loss: 0.819502, Accuracy: 89.58%\n",
      "Batch 182, Loss: 0.860344, Accuracy: 89.58%\n",
      "Batch 183, Loss: 0.853369, Accuracy: 89.57%\n",
      "Batch 184, Loss: 0.824898, Accuracy: 89.59%\n",
      "Batch 185, Loss: 0.949768, Accuracy: 89.54%\n",
      "Batch 186, Loss: 0.903412, Accuracy: 89.50%\n",
      "Batch 187, Loss: 0.795607, Accuracy: 89.54%\n",
      "Batch 188, Loss: 0.810993, Accuracy: 89.56%\n",
      "Batch 189, Loss: 0.838322, Accuracy: 89.58%\n",
      "Batch 190, Loss: 0.870956, Accuracy: 89.57%\n",
      "Batch 191, Loss: 0.815534, Accuracy: 89.59%\n",
      "Batch 192, Loss: 0.891185, Accuracy: 89.56%\n",
      "Batch 193, Loss: 0.780768, Accuracy: 89.60%\n",
      "Batch 194, Loss: 0.815097, Accuracy: 89.61%\n",
      "Batch 195, Loss: 0.849320, Accuracy: 89.61%\n",
      "Batch 196, Loss: 0.873130, Accuracy: 89.60%\n",
      "Batch 197, Loss: 0.852479, Accuracy: 89.60%\n",
      "Batch 198, Loss: 0.889870, Accuracy: 89.58%\n",
      "Batch 199, Loss: 0.789915, Accuracy: 89.60%\n",
      "Batch 200, Loss: 0.901773, Accuracy: 89.58%\n",
      "Batch 201, Loss: 0.875188, Accuracy: 89.56%\n",
      "Batch 202, Loss: 0.853191, Accuracy: 89.57%\n",
      "Batch 203, Loss: 0.846105, Accuracy: 89.56%\n",
      "Batch 204, Loss: 0.843634, Accuracy: 89.57%\n",
      "Batch 205, Loss: 0.840458, Accuracy: 89.57%\n",
      "Batch 206, Loss: 0.799175, Accuracy: 89.59%\n",
      "Batch 207, Loss: 0.854579, Accuracy: 89.58%\n",
      "Batch 208, Loss: 0.813760, Accuracy: 89.60%\n",
      "Batch 209, Loss: 0.823727, Accuracy: 89.61%\n",
      "Batch 210, Loss: 0.846458, Accuracy: 89.61%\n",
      "Batch 211, Loss: 0.817096, Accuracy: 89.63%\n",
      "Batch 212, Loss: 0.835087, Accuracy: 89.63%\n",
      "Batch 213, Loss: 0.846765, Accuracy: 89.63%\n",
      "Training - Epoch 125, Loss: 0.847801, Accuracy: 89.63%\n",
      "Validation Batch 1, Loss: 0.826143, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.796470, Accuracy: 93.75%\n",
      "Validation Batch 3, Loss: 0.826506, Accuracy: 92.71%\n",
      "Validation Batch 4, Loss: 0.844729, Accuracy: 91.80%\n",
      "Validation Batch 5, Loss: 0.827697, Accuracy: 91.88%\n",
      "Validation Batch 6, Loss: 0.799618, Accuracy: 92.19%\n",
      "Validation Batch 7, Loss: 0.797200, Accuracy: 92.63%\n",
      "Validation Batch 8, Loss: 0.862547, Accuracy: 92.19%\n",
      "Validation Batch 9, Loss: 0.894131, Accuracy: 91.32%\n",
      "Validation Batch 10, Loss: 0.801228, Accuracy: 91.72%\n",
      "Validation Batch 11, Loss: 0.830076, Accuracy: 91.76%\n",
      "Validation Batch 12, Loss: 0.836376, Accuracy: 91.80%\n",
      "Validation Batch 13, Loss: 0.839984, Accuracy: 91.71%\n",
      "Validation Batch 14, Loss: 0.840294, Accuracy: 91.74%\n",
      "Validation Batch 15, Loss: 0.794063, Accuracy: 91.88%\n",
      "Validation Batch 16, Loss: 0.839724, Accuracy: 91.80%\n",
      "Validation Batch 17, Loss: 0.859033, Accuracy: 91.64%\n",
      "Validation Batch 18, Loss: 0.805367, Accuracy: 91.75%\n",
      "Validation Batch 19, Loss: 0.858216, Accuracy: 91.69%\n",
      "Validation Batch 20, Loss: 0.827003, Accuracy: 91.80%\n",
      "Validation Batch 21, Loss: 0.858999, Accuracy: 91.67%\n",
      "Validation Batch 22, Loss: 0.812004, Accuracy: 91.76%\n",
      "Validation Batch 23, Loss: 0.859871, Accuracy: 91.64%\n",
      "Validation Batch 24, Loss: 0.836715, Accuracy: 91.60%\n",
      "Validation Batch 25, Loss: 0.805161, Accuracy: 91.62%\n",
      "Validation Batch 26, Loss: 0.841912, Accuracy: 91.59%\n",
      "Validation Batch 27, Loss: 0.802034, Accuracy: 91.66%\n",
      "Validation - Epoch 125, Loss: 0.830485, Accuracy: 91.66%\n",
      "Patience—10\n",
      "Epoch 126\n",
      "Batch 1, Loss: 0.794683, Accuracy: 95.31%\n",
      "Batch 2, Loss: 0.823245, Accuracy: 93.75%\n",
      "Batch 3, Loss: 0.825006, Accuracy: 93.23%\n",
      "Batch 4, Loss: 0.860705, Accuracy: 91.80%\n",
      "Batch 5, Loss: 0.835789, Accuracy: 91.88%\n",
      "Batch 6, Loss: 0.823362, Accuracy: 91.93%\n",
      "Batch 7, Loss: 0.859331, Accuracy: 91.52%\n",
      "Batch 8, Loss: 0.862423, Accuracy: 91.02%\n",
      "Batch 9, Loss: 0.864503, Accuracy: 90.80%\n",
      "Batch 10, Loss: 0.838292, Accuracy: 90.78%\n",
      "Batch 11, Loss: 0.813160, Accuracy: 91.05%\n",
      "Batch 12, Loss: 0.822150, Accuracy: 91.28%\n",
      "Batch 13, Loss: 0.818328, Accuracy: 91.23%\n",
      "Batch 14, Loss: 0.820957, Accuracy: 91.29%\n",
      "Batch 15, Loss: 0.788820, Accuracy: 91.67%\n",
      "Batch 16, Loss: 0.819178, Accuracy: 91.70%\n",
      "Batch 17, Loss: 0.892279, Accuracy: 91.27%\n",
      "Batch 18, Loss: 0.868159, Accuracy: 91.06%\n",
      "Batch 19, Loss: 0.786049, Accuracy: 91.28%\n",
      "Batch 20, Loss: 0.939137, Accuracy: 90.78%\n",
      "Batch 21, Loss: 0.904307, Accuracy: 90.48%\n",
      "Batch 22, Loss: 0.781550, Accuracy: 90.77%\n",
      "Batch 23, Loss: 0.907062, Accuracy: 90.42%\n",
      "Batch 24, Loss: 0.849693, Accuracy: 90.43%\n",
      "Batch 25, Loss: 0.922448, Accuracy: 90.06%\n",
      "Batch 26, Loss: 0.877722, Accuracy: 89.90%\n",
      "Batch 27, Loss: 0.894200, Accuracy: 89.70%\n",
      "Batch 28, Loss: 0.854986, Accuracy: 89.68%\n",
      "Batch 29, Loss: 0.812853, Accuracy: 89.82%\n",
      "Batch 30, Loss: 0.860277, Accuracy: 89.74%\n",
      "Batch 31, Loss: 0.874069, Accuracy: 89.62%\n",
      "Batch 32, Loss: 0.872509, Accuracy: 89.55%\n",
      "Batch 33, Loss: 0.813517, Accuracy: 89.63%\n",
      "Batch 34, Loss: 0.893241, Accuracy: 89.52%\n",
      "Batch 35, Loss: 0.826754, Accuracy: 89.60%\n",
      "Batch 36, Loss: 0.909123, Accuracy: 89.41%\n",
      "Batch 37, Loss: 0.846219, Accuracy: 89.44%\n",
      "Batch 38, Loss: 0.895560, Accuracy: 89.31%\n",
      "Batch 39, Loss: 0.875707, Accuracy: 89.26%\n",
      "Batch 40, Loss: 0.842789, Accuracy: 89.30%\n",
      "Batch 41, Loss: 0.845008, Accuracy: 89.33%\n",
      "Batch 42, Loss: 0.904662, Accuracy: 89.17%\n",
      "Batch 43, Loss: 0.877455, Accuracy: 89.14%\n",
      "Batch 44, Loss: 0.812359, Accuracy: 89.20%\n",
      "Batch 45, Loss: 0.818748, Accuracy: 89.31%\n",
      "Batch 46, Loss: 0.859023, Accuracy: 89.27%\n",
      "Batch 47, Loss: 0.849018, Accuracy: 89.30%\n",
      "Batch 48, Loss: 0.852942, Accuracy: 89.29%\n",
      "Batch 49, Loss: 0.810760, Accuracy: 89.38%\n",
      "Batch 50, Loss: 0.827133, Accuracy: 89.41%\n",
      "Batch 51, Loss: 0.871685, Accuracy: 89.34%\n",
      "Batch 52, Loss: 0.856974, Accuracy: 89.33%\n",
      "Batch 53, Loss: 0.824404, Accuracy: 89.45%\n",
      "Batch 54, Loss: 0.881906, Accuracy: 89.41%\n",
      "Batch 55, Loss: 0.811371, Accuracy: 89.49%\n",
      "Batch 56, Loss: 0.807620, Accuracy: 89.59%\n",
      "Batch 57, Loss: 0.827017, Accuracy: 89.61%\n",
      "Batch 58, Loss: 0.865394, Accuracy: 89.60%\n",
      "Batch 59, Loss: 0.832778, Accuracy: 89.62%\n",
      "Batch 60, Loss: 0.833357, Accuracy: 89.64%\n",
      "Batch 61, Loss: 0.789370, Accuracy: 89.73%\n",
      "Batch 62, Loss: 0.827519, Accuracy: 89.74%\n",
      "Batch 63, Loss: 0.827209, Accuracy: 89.76%\n",
      "Batch 64, Loss: 0.867397, Accuracy: 89.70%\n",
      "Batch 65, Loss: 0.838432, Accuracy: 89.71%\n",
      "Batch 66, Loss: 0.865711, Accuracy: 89.65%\n",
      "Batch 67, Loss: 0.785630, Accuracy: 89.76%\n",
      "Batch 68, Loss: 0.798829, Accuracy: 89.84%\n",
      "Batch 69, Loss: 0.850090, Accuracy: 89.86%\n",
      "Batch 70, Loss: 0.812647, Accuracy: 89.91%\n",
      "Batch 71, Loss: 0.808664, Accuracy: 89.94%\n",
      "Batch 72, Loss: 0.876707, Accuracy: 89.91%\n",
      "Batch 73, Loss: 0.878899, Accuracy: 89.85%\n",
      "Batch 74, Loss: 0.773764, Accuracy: 89.97%\n",
      "Batch 75, Loss: 0.885239, Accuracy: 89.92%\n",
      "Batch 76, Loss: 0.890638, Accuracy: 89.84%\n",
      "Batch 77, Loss: 0.848278, Accuracy: 89.83%\n",
      "Batch 78, Loss: 0.822506, Accuracy: 89.86%\n",
      "Batch 79, Loss: 0.892196, Accuracy: 89.79%\n",
      "Batch 80, Loss: 0.892933, Accuracy: 89.71%\n",
      "Batch 81, Loss: 0.851735, Accuracy: 89.72%\n",
      "Batch 82, Loss: 0.825801, Accuracy: 89.77%\n",
      "Batch 83, Loss: 0.848943, Accuracy: 89.76%\n",
      "Batch 84, Loss: 0.841490, Accuracy: 89.75%\n",
      "Batch 85, Loss: 0.829676, Accuracy: 89.78%\n",
      "Batch 86, Loss: 0.853549, Accuracy: 89.77%\n",
      "Batch 87, Loss: 0.816315, Accuracy: 89.80%\n",
      "Batch 88, Loss: 0.871800, Accuracy: 89.77%\n",
      "Batch 89, Loss: 0.861780, Accuracy: 89.75%\n",
      "Batch 90, Loss: 0.847096, Accuracy: 89.76%\n",
      "Batch 91, Loss: 0.797892, Accuracy: 89.80%\n",
      "Batch 92, Loss: 0.907019, Accuracy: 89.76%\n",
      "Batch 93, Loss: 0.807155, Accuracy: 89.80%\n",
      "Batch 94, Loss: 0.801000, Accuracy: 89.84%\n",
      "Batch 95, Loss: 0.820114, Accuracy: 89.88%\n",
      "Batch 96, Loss: 0.850727, Accuracy: 89.86%\n",
      "Batch 97, Loss: 0.917052, Accuracy: 89.79%\n",
      "Batch 98, Loss: 0.865276, Accuracy: 89.76%\n",
      "Batch 99, Loss: 0.915532, Accuracy: 89.68%\n",
      "Batch 100, Loss: 0.814470, Accuracy: 89.72%\n",
      "Batch 101, Loss: 0.773416, Accuracy: 89.81%\n",
      "Batch 102, Loss: 0.875643, Accuracy: 89.78%\n",
      "Batch 103, Loss: 0.847634, Accuracy: 89.78%\n",
      "Batch 104, Loss: 0.865067, Accuracy: 89.74%\n",
      "Batch 105, Loss: 0.852145, Accuracy: 89.73%\n",
      "Batch 106, Loss: 0.806454, Accuracy: 89.77%\n",
      "Batch 107, Loss: 0.848801, Accuracy: 89.78%\n",
      "Batch 108, Loss: 0.857420, Accuracy: 89.77%\n",
      "Batch 109, Loss: 0.852898, Accuracy: 89.78%\n",
      "Batch 110, Loss: 0.839244, Accuracy: 89.77%\n",
      "Batch 111, Loss: 0.819672, Accuracy: 89.81%\n",
      "Batch 112, Loss: 0.836635, Accuracy: 89.82%\n",
      "Batch 113, Loss: 0.919452, Accuracy: 89.75%\n",
      "Batch 114, Loss: 0.883500, Accuracy: 89.71%\n",
      "Batch 115, Loss: 0.833345, Accuracy: 89.73%\n",
      "Batch 116, Loss: 0.836480, Accuracy: 89.74%\n",
      "Batch 117, Loss: 0.843428, Accuracy: 89.74%\n",
      "Batch 118, Loss: 0.908832, Accuracy: 89.68%\n",
      "Batch 119, Loss: 0.854600, Accuracy: 89.68%\n",
      "Batch 120, Loss: 0.807946, Accuracy: 89.71%\n",
      "Batch 121, Loss: 0.872552, Accuracy: 89.71%\n",
      "Batch 122, Loss: 0.873100, Accuracy: 89.68%\n",
      "Batch 123, Loss: 0.805579, Accuracy: 89.72%\n",
      "Batch 124, Loss: 0.851903, Accuracy: 89.72%\n",
      "Batch 125, Loss: 0.861519, Accuracy: 89.71%\n",
      "Batch 126, Loss: 0.832578, Accuracy: 89.72%\n",
      "Batch 127, Loss: 0.813573, Accuracy: 89.74%\n",
      "Batch 128, Loss: 0.843820, Accuracy: 89.73%\n",
      "Batch 129, Loss: 0.838568, Accuracy: 89.74%\n",
      "Batch 130, Loss: 0.855713, Accuracy: 89.74%\n",
      "Batch 131, Loss: 0.861820, Accuracy: 89.72%\n",
      "Batch 132, Loss: 0.824191, Accuracy: 89.75%\n",
      "Batch 133, Loss: 0.831560, Accuracy: 89.77%\n",
      "Batch 134, Loss: 0.841844, Accuracy: 89.77%\n",
      "Batch 135, Loss: 0.829232, Accuracy: 89.79%\n",
      "Batch 136, Loss: 0.766193, Accuracy: 89.86%\n",
      "Batch 137, Loss: 0.835781, Accuracy: 89.87%\n",
      "Batch 138, Loss: 0.884808, Accuracy: 89.84%\n",
      "Batch 139, Loss: 0.842162, Accuracy: 89.84%\n",
      "Batch 140, Loss: 0.835885, Accuracy: 89.84%\n",
      "Batch 141, Loss: 0.845789, Accuracy: 89.85%\n",
      "Batch 142, Loss: 0.813686, Accuracy: 89.88%\n",
      "Batch 143, Loss: 0.910517, Accuracy: 89.83%\n",
      "Batch 144, Loss: 0.796187, Accuracy: 89.87%\n",
      "Batch 145, Loss: 0.861760, Accuracy: 89.86%\n",
      "Batch 146, Loss: 0.842107, Accuracy: 89.85%\n",
      "Batch 147, Loss: 0.877602, Accuracy: 89.83%\n",
      "Batch 148, Loss: 0.799087, Accuracy: 89.86%\n",
      "Batch 149, Loss: 0.830074, Accuracy: 89.87%\n",
      "Batch 150, Loss: 0.897608, Accuracy: 89.83%\n",
      "Batch 151, Loss: 0.907428, Accuracy: 89.79%\n",
      "Batch 152, Loss: 0.870376, Accuracy: 89.77%\n",
      "Batch 153, Loss: 0.865589, Accuracy: 89.77%\n",
      "Batch 154, Loss: 0.870104, Accuracy: 89.75%\n",
      "Batch 155, Loss: 0.837211, Accuracy: 89.76%\n",
      "Batch 156, Loss: 0.813261, Accuracy: 89.78%\n",
      "Batch 157, Loss: 0.847507, Accuracy: 89.79%\n",
      "Batch 158, Loss: 0.884772, Accuracy: 89.77%\n",
      "Batch 159, Loss: 0.813811, Accuracy: 89.78%\n",
      "Batch 160, Loss: 0.769377, Accuracy: 89.83%\n",
      "Batch 161, Loss: 0.949567, Accuracy: 89.77%\n",
      "Batch 162, Loss: 0.887397, Accuracy: 89.75%\n",
      "Batch 163, Loss: 0.917558, Accuracy: 89.70%\n",
      "Batch 164, Loss: 0.850641, Accuracy: 89.71%\n",
      "Batch 165, Loss: 0.889354, Accuracy: 89.68%\n",
      "Batch 166, Loss: 0.843589, Accuracy: 89.68%\n",
      "Batch 167, Loss: 0.839646, Accuracy: 89.69%\n",
      "Batch 168, Loss: 0.834889, Accuracy: 89.70%\n",
      "Batch 169, Loss: 0.907745, Accuracy: 89.66%\n",
      "Batch 170, Loss: 0.839586, Accuracy: 89.67%\n",
      "Batch 171, Loss: 0.840596, Accuracy: 89.67%\n",
      "Batch 172, Loss: 0.776380, Accuracy: 89.73%\n",
      "Batch 173, Loss: 0.860091, Accuracy: 89.71%\n",
      "Batch 174, Loss: 0.881285, Accuracy: 89.69%\n",
      "Batch 175, Loss: 0.821643, Accuracy: 89.70%\n",
      "Batch 176, Loss: 0.921774, Accuracy: 89.66%\n",
      "Batch 177, Loss: 0.886674, Accuracy: 89.64%\n",
      "Batch 178, Loss: 0.845669, Accuracy: 89.63%\n",
      "Batch 179, Loss: 0.835143, Accuracy: 89.65%\n",
      "Batch 180, Loss: 0.805660, Accuracy: 89.67%\n",
      "Batch 181, Loss: 0.871605, Accuracy: 89.67%\n",
      "Batch 182, Loss: 0.937848, Accuracy: 89.61%\n",
      "Batch 183, Loss: 0.834323, Accuracy: 89.62%\n",
      "Batch 184, Loss: 0.835531, Accuracy: 89.62%\n",
      "Batch 185, Loss: 0.872249, Accuracy: 89.60%\n",
      "Batch 186, Loss: 0.866229, Accuracy: 89.59%\n",
      "Batch 187, Loss: 0.824773, Accuracy: 89.61%\n",
      "Batch 188, Loss: 0.821624, Accuracy: 89.62%\n",
      "Batch 189, Loss: 0.851306, Accuracy: 89.63%\n",
      "Batch 190, Loss: 0.840936, Accuracy: 89.64%\n",
      "Batch 191, Loss: 0.859005, Accuracy: 89.64%\n",
      "Batch 192, Loss: 0.904034, Accuracy: 89.60%\n",
      "Batch 193, Loss: 0.866252, Accuracy: 89.59%\n",
      "Batch 194, Loss: 0.810088, Accuracy: 89.61%\n",
      "Batch 195, Loss: 0.869176, Accuracy: 89.60%\n",
      "Batch 196, Loss: 0.845785, Accuracy: 89.60%\n",
      "Batch 197, Loss: 0.856604, Accuracy: 89.59%\n",
      "Batch 198, Loss: 0.849028, Accuracy: 89.59%\n",
      "Batch 199, Loss: 0.875130, Accuracy: 89.58%\n",
      "Batch 200, Loss: 0.803889, Accuracy: 89.61%\n",
      "Batch 201, Loss: 0.904551, Accuracy: 89.58%\n",
      "Batch 202, Loss: 0.849736, Accuracy: 89.59%\n",
      "Batch 203, Loss: 0.763544, Accuracy: 89.63%\n",
      "Batch 204, Loss: 0.824229, Accuracy: 89.64%\n",
      "Batch 205, Loss: 0.880185, Accuracy: 89.63%\n",
      "Batch 206, Loss: 0.843281, Accuracy: 89.63%\n",
      "Batch 207, Loss: 0.790131, Accuracy: 89.67%\n",
      "Batch 208, Loss: 0.841022, Accuracy: 89.67%\n",
      "Batch 209, Loss: 0.916785, Accuracy: 89.65%\n",
      "Batch 210, Loss: 0.769334, Accuracy: 89.69%\n",
      "Batch 211, Loss: 0.904467, Accuracy: 89.65%\n",
      "Batch 212, Loss: 0.860961, Accuracy: 89.65%\n",
      "Batch 213, Loss: 0.843820, Accuracy: 89.66%\n",
      "Training - Epoch 126, Loss: 0.848629, Accuracy: 89.66%\n",
      "Validation Batch 1, Loss: 0.817029, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.791555, Accuracy: 93.75%\n",
      "Validation Batch 3, Loss: 0.817736, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.833217, Accuracy: 92.97%\n",
      "Validation Batch 5, Loss: 0.815696, Accuracy: 92.81%\n",
      "Validation Batch 6, Loss: 0.793644, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.793477, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.857056, Accuracy: 92.97%\n",
      "Validation Batch 9, Loss: 0.885740, Accuracy: 92.01%\n",
      "Validation Batch 10, Loss: 0.794696, Accuracy: 92.34%\n",
      "Validation Batch 11, Loss: 0.822699, Accuracy: 92.33%\n",
      "Validation Batch 12, Loss: 0.829782, Accuracy: 92.32%\n",
      "Validation Batch 13, Loss: 0.833479, Accuracy: 92.31%\n",
      "Validation Batch 14, Loss: 0.832576, Accuracy: 92.30%\n",
      "Validation Batch 15, Loss: 0.790184, Accuracy: 92.50%\n",
      "Validation Batch 16, Loss: 0.828956, Accuracy: 92.48%\n",
      "Validation Batch 17, Loss: 0.865285, Accuracy: 92.28%\n",
      "Validation Batch 18, Loss: 0.795512, Accuracy: 92.45%\n",
      "Validation Batch 19, Loss: 0.859021, Accuracy: 92.27%\n",
      "Validation Batch 20, Loss: 0.810201, Accuracy: 92.42%\n",
      "Validation Batch 21, Loss: 0.856159, Accuracy: 92.26%\n",
      "Validation Batch 22, Loss: 0.805516, Accuracy: 92.40%\n",
      "Validation Batch 23, Loss: 0.855160, Accuracy: 92.26%\n",
      "Validation Batch 24, Loss: 0.826897, Accuracy: 92.25%\n",
      "Validation Batch 25, Loss: 0.799158, Accuracy: 92.38%\n",
      "Validation Batch 26, Loss: 0.835856, Accuracy: 92.37%\n",
      "Validation Batch 27, Loss: 0.795550, Accuracy: 92.43%\n",
      "Validation - Epoch 126, Loss: 0.823772, Accuracy: 92.43%\n",
      "Patience—11\n",
      "Epoch 127\n",
      "Batch 1, Loss: 0.886504, Accuracy: 85.94%\n",
      "Batch 2, Loss: 0.848001, Accuracy: 88.28%\n",
      "Batch 3, Loss: 0.850703, Accuracy: 88.54%\n",
      "Batch 4, Loss: 0.853940, Accuracy: 88.67%\n",
      "Batch 5, Loss: 0.843975, Accuracy: 88.75%\n",
      "Batch 6, Loss: 0.848909, Accuracy: 89.06%\n",
      "Batch 7, Loss: 0.840320, Accuracy: 89.29%\n",
      "Batch 8, Loss: 0.844634, Accuracy: 89.26%\n",
      "Batch 9, Loss: 0.795366, Accuracy: 89.93%\n",
      "Batch 10, Loss: 0.888719, Accuracy: 89.38%\n",
      "Batch 11, Loss: 0.862409, Accuracy: 89.06%\n",
      "Batch 12, Loss: 0.831070, Accuracy: 89.32%\n",
      "Batch 13, Loss: 0.818392, Accuracy: 89.66%\n",
      "Batch 14, Loss: 0.850459, Accuracy: 89.51%\n",
      "Batch 15, Loss: 0.865524, Accuracy: 89.38%\n",
      "Batch 16, Loss: 0.830432, Accuracy: 89.55%\n",
      "Batch 17, Loss: 0.859018, Accuracy: 89.43%\n",
      "Batch 18, Loss: 0.873925, Accuracy: 89.24%\n",
      "Batch 19, Loss: 0.820030, Accuracy: 89.47%\n",
      "Batch 20, Loss: 0.826675, Accuracy: 89.61%\n",
      "Batch 21, Loss: 0.801819, Accuracy: 89.81%\n",
      "Batch 22, Loss: 0.882521, Accuracy: 89.70%\n",
      "Batch 23, Loss: 0.803985, Accuracy: 89.88%\n",
      "Batch 24, Loss: 0.882007, Accuracy: 89.71%\n",
      "Batch 25, Loss: 0.840612, Accuracy: 89.75%\n",
      "Batch 26, Loss: 0.813484, Accuracy: 89.90%\n",
      "Batch 27, Loss: 0.802640, Accuracy: 90.05%\n",
      "Batch 28, Loss: 0.869941, Accuracy: 89.96%\n",
      "Batch 29, Loss: 0.876049, Accuracy: 89.82%\n",
      "Batch 30, Loss: 0.889054, Accuracy: 89.69%\n",
      "Batch 31, Loss: 0.905874, Accuracy: 89.47%\n",
      "Batch 32, Loss: 0.826200, Accuracy: 89.50%\n",
      "Batch 33, Loss: 0.871417, Accuracy: 89.39%\n",
      "Batch 34, Loss: 0.832019, Accuracy: 89.52%\n",
      "Batch 35, Loss: 0.795491, Accuracy: 89.69%\n",
      "Batch 36, Loss: 0.931720, Accuracy: 89.50%\n",
      "Batch 37, Loss: 0.896296, Accuracy: 89.36%\n",
      "Batch 38, Loss: 0.831014, Accuracy: 89.39%\n",
      "Batch 39, Loss: 0.781077, Accuracy: 89.58%\n",
      "Batch 40, Loss: 0.874635, Accuracy: 89.53%\n",
      "Batch 41, Loss: 0.883976, Accuracy: 89.44%\n",
      "Batch 42, Loss: 0.787286, Accuracy: 89.62%\n",
      "Batch 43, Loss: 0.945010, Accuracy: 89.39%\n",
      "Batch 44, Loss: 0.843171, Accuracy: 89.42%\n",
      "Batch 45, Loss: 0.823757, Accuracy: 89.48%\n",
      "Batch 46, Loss: 0.820296, Accuracy: 89.54%\n",
      "Batch 47, Loss: 0.876644, Accuracy: 89.46%\n",
      "Batch 48, Loss: 0.846402, Accuracy: 89.42%\n",
      "Batch 49, Loss: 0.823647, Accuracy: 89.48%\n",
      "Batch 50, Loss: 0.848817, Accuracy: 89.47%\n",
      "Batch 51, Loss: 0.831964, Accuracy: 89.49%\n",
      "Batch 52, Loss: 0.849036, Accuracy: 89.48%\n",
      "Batch 53, Loss: 0.838104, Accuracy: 89.50%\n",
      "Batch 54, Loss: 0.864040, Accuracy: 89.50%\n",
      "Batch 55, Loss: 0.815370, Accuracy: 89.57%\n",
      "Batch 56, Loss: 0.822421, Accuracy: 89.62%\n",
      "Batch 57, Loss: 0.816918, Accuracy: 89.67%\n",
      "Batch 58, Loss: 0.869614, Accuracy: 89.63%\n",
      "Batch 59, Loss: 0.809466, Accuracy: 89.70%\n",
      "Batch 60, Loss: 0.861361, Accuracy: 89.66%\n",
      "Batch 61, Loss: 0.812466, Accuracy: 89.70%\n",
      "Batch 62, Loss: 0.860954, Accuracy: 89.67%\n",
      "Batch 63, Loss: 0.824167, Accuracy: 89.73%\n",
      "Batch 64, Loss: 0.797363, Accuracy: 89.82%\n",
      "Batch 65, Loss: 0.838057, Accuracy: 89.83%\n",
      "Batch 66, Loss: 0.851917, Accuracy: 89.80%\n",
      "Batch 67, Loss: 0.923688, Accuracy: 89.69%\n",
      "Batch 68, Loss: 0.871760, Accuracy: 89.66%\n",
      "Batch 69, Loss: 0.827136, Accuracy: 89.70%\n",
      "Batch 70, Loss: 0.901343, Accuracy: 89.62%\n",
      "Batch 71, Loss: 0.898243, Accuracy: 89.52%\n",
      "Batch 72, Loss: 0.810931, Accuracy: 89.61%\n",
      "Batch 73, Loss: 0.895310, Accuracy: 89.55%\n",
      "Batch 74, Loss: 0.802827, Accuracy: 89.61%\n",
      "Batch 75, Loss: 0.834899, Accuracy: 89.65%\n",
      "Batch 76, Loss: 0.822329, Accuracy: 89.68%\n",
      "Batch 77, Loss: 0.897677, Accuracy: 89.61%\n",
      "Batch 78, Loss: 0.772425, Accuracy: 89.70%\n",
      "Batch 79, Loss: 0.830941, Accuracy: 89.72%\n",
      "Batch 80, Loss: 0.809664, Accuracy: 89.77%\n",
      "Batch 81, Loss: 0.844179, Accuracy: 89.78%\n",
      "Batch 82, Loss: 0.861105, Accuracy: 89.75%\n",
      "Batch 83, Loss: 0.828309, Accuracy: 89.78%\n",
      "Batch 84, Loss: 0.832450, Accuracy: 89.79%\n",
      "Batch 85, Loss: 0.826223, Accuracy: 89.82%\n",
      "Batch 86, Loss: 0.796711, Accuracy: 89.86%\n",
      "Batch 87, Loss: 0.896466, Accuracy: 89.80%\n",
      "Batch 88, Loss: 0.854894, Accuracy: 89.81%\n",
      "Batch 89, Loss: 0.868029, Accuracy: 89.80%\n",
      "Batch 90, Loss: 0.850281, Accuracy: 89.79%\n",
      "Batch 91, Loss: 0.805745, Accuracy: 89.84%\n",
      "Batch 92, Loss: 0.844526, Accuracy: 89.84%\n",
      "Batch 93, Loss: 0.875822, Accuracy: 89.80%\n",
      "Batch 94, Loss: 0.833238, Accuracy: 89.81%\n",
      "Batch 95, Loss: 0.784311, Accuracy: 89.87%\n",
      "Batch 96, Loss: 0.887057, Accuracy: 89.83%\n",
      "Batch 97, Loss: 0.829477, Accuracy: 89.85%\n",
      "Batch 98, Loss: 0.858040, Accuracy: 89.84%\n",
      "Batch 99, Loss: 0.842894, Accuracy: 89.85%\n",
      "Batch 100, Loss: 0.845791, Accuracy: 89.86%\n",
      "Batch 101, Loss: 0.854518, Accuracy: 89.84%\n",
      "Batch 102, Loss: 0.827913, Accuracy: 89.86%\n",
      "Batch 103, Loss: 0.815640, Accuracy: 89.90%\n",
      "Batch 104, Loss: 0.831503, Accuracy: 89.92%\n",
      "Batch 105, Loss: 0.823819, Accuracy: 89.94%\n",
      "Batch 106, Loss: 0.840759, Accuracy: 89.93%\n",
      "Batch 107, Loss: 0.827636, Accuracy: 89.95%\n",
      "Batch 108, Loss: 0.911411, Accuracy: 89.89%\n",
      "Batch 109, Loss: 0.788332, Accuracy: 89.94%\n",
      "Batch 110, Loss: 0.828426, Accuracy: 89.96%\n",
      "Batch 111, Loss: 0.844254, Accuracy: 89.94%\n",
      "Batch 112, Loss: 0.895589, Accuracy: 89.90%\n",
      "Batch 113, Loss: 0.875072, Accuracy: 89.86%\n",
      "Batch 114, Loss: 0.901760, Accuracy: 89.80%\n",
      "Batch 115, Loss: 0.822285, Accuracy: 89.84%\n",
      "Batch 116, Loss: 0.894975, Accuracy: 89.79%\n",
      "Batch 117, Loss: 0.867265, Accuracy: 89.77%\n",
      "Batch 118, Loss: 0.970001, Accuracy: 89.66%\n",
      "Batch 119, Loss: 0.892003, Accuracy: 89.61%\n",
      "Batch 120, Loss: 0.867418, Accuracy: 89.60%\n",
      "Batch 121, Loss: 0.821102, Accuracy: 89.63%\n",
      "Batch 122, Loss: 0.881001, Accuracy: 89.60%\n",
      "Batch 123, Loss: 0.820874, Accuracy: 89.62%\n",
      "Batch 124, Loss: 0.879462, Accuracy: 89.60%\n",
      "Batch 125, Loss: 0.819241, Accuracy: 89.62%\n",
      "Batch 126, Loss: 0.864280, Accuracy: 89.61%\n",
      "Batch 127, Loss: 0.839883, Accuracy: 89.62%\n",
      "Batch 128, Loss: 0.840525, Accuracy: 89.62%\n",
      "Batch 129, Loss: 0.832383, Accuracy: 89.63%\n",
      "Batch 130, Loss: 0.835103, Accuracy: 89.65%\n",
      "Batch 131, Loss: 0.854182, Accuracy: 89.65%\n",
      "Batch 132, Loss: 0.833635, Accuracy: 89.65%\n",
      "Batch 133, Loss: 0.828514, Accuracy: 89.67%\n",
      "Batch 134, Loss: 0.909368, Accuracy: 89.63%\n",
      "Batch 135, Loss: 0.883706, Accuracy: 89.61%\n",
      "Batch 136, Loss: 0.849412, Accuracy: 89.60%\n",
      "Batch 137, Loss: 0.823167, Accuracy: 89.61%\n",
      "Batch 138, Loss: 0.833933, Accuracy: 89.62%\n",
      "Batch 139, Loss: 0.870123, Accuracy: 89.60%\n",
      "Batch 140, Loss: 0.933826, Accuracy: 89.54%\n",
      "Batch 141, Loss: 0.854702, Accuracy: 89.54%\n",
      "Batch 142, Loss: 0.895673, Accuracy: 89.50%\n",
      "Batch 143, Loss: 0.877084, Accuracy: 89.49%\n",
      "Batch 144, Loss: 0.811558, Accuracy: 89.53%\n",
      "Batch 145, Loss: 0.836360, Accuracy: 89.54%\n",
      "Batch 146, Loss: 0.871675, Accuracy: 89.52%\n",
      "Batch 147, Loss: 0.847610, Accuracy: 89.53%\n",
      "Batch 148, Loss: 0.850401, Accuracy: 89.52%\n",
      "Batch 149, Loss: 0.821635, Accuracy: 89.54%\n",
      "Batch 150, Loss: 0.862141, Accuracy: 89.53%\n",
      "Batch 151, Loss: 0.897683, Accuracy: 89.49%\n",
      "Batch 152, Loss: 0.903669, Accuracy: 89.45%\n",
      "Batch 153, Loss: 0.844973, Accuracy: 89.46%\n",
      "Batch 154, Loss: 0.886669, Accuracy: 89.44%\n",
      "Batch 155, Loss: 0.828911, Accuracy: 89.45%\n",
      "Batch 156, Loss: 0.840774, Accuracy: 89.45%\n",
      "Batch 157, Loss: 0.817012, Accuracy: 89.47%\n",
      "Batch 158, Loss: 0.827946, Accuracy: 89.49%\n",
      "Batch 159, Loss: 0.853101, Accuracy: 89.49%\n",
      "Batch 160, Loss: 0.861883, Accuracy: 89.47%\n",
      "Batch 161, Loss: 0.885090, Accuracy: 89.45%\n",
      "Batch 162, Loss: 0.822106, Accuracy: 89.47%\n",
      "Batch 163, Loss: 0.850218, Accuracy: 89.47%\n",
      "Batch 164, Loss: 0.794796, Accuracy: 89.50%\n",
      "Batch 165, Loss: 0.821964, Accuracy: 89.53%\n",
      "Batch 166, Loss: 0.805278, Accuracy: 89.56%\n",
      "Batch 167, Loss: 0.810859, Accuracy: 89.58%\n",
      "Batch 168, Loss: 0.892920, Accuracy: 89.55%\n",
      "Batch 169, Loss: 0.864712, Accuracy: 89.52%\n",
      "Batch 170, Loss: 0.847265, Accuracy: 89.53%\n",
      "Batch 171, Loss: 0.792355, Accuracy: 89.57%\n",
      "Batch 172, Loss: 0.830985, Accuracy: 89.57%\n",
      "Batch 173, Loss: 0.822154, Accuracy: 89.58%\n",
      "Batch 174, Loss: 0.843088, Accuracy: 89.57%\n",
      "Batch 175, Loss: 0.823366, Accuracy: 89.59%\n",
      "Batch 176, Loss: 0.873248, Accuracy: 89.58%\n",
      "Batch 177, Loss: 0.907992, Accuracy: 89.55%\n",
      "Batch 178, Loss: 0.851883, Accuracy: 89.55%\n",
      "Batch 179, Loss: 0.801827, Accuracy: 89.58%\n",
      "Batch 180, Loss: 0.842671, Accuracy: 89.58%\n",
      "Batch 181, Loss: 0.816574, Accuracy: 89.61%\n",
      "Batch 182, Loss: 0.876265, Accuracy: 89.59%\n",
      "Batch 183, Loss: 0.832061, Accuracy: 89.60%\n",
      "Batch 184, Loss: 0.880147, Accuracy: 89.59%\n",
      "Batch 185, Loss: 0.837384, Accuracy: 89.60%\n",
      "Batch 186, Loss: 0.846975, Accuracy: 89.60%\n",
      "Batch 187, Loss: 0.800968, Accuracy: 89.62%\n",
      "Batch 188, Loss: 0.866143, Accuracy: 89.61%\n",
      "Batch 189, Loss: 0.891892, Accuracy: 89.58%\n",
      "Batch 190, Loss: 0.811851, Accuracy: 89.61%\n",
      "Batch 191, Loss: 0.831685, Accuracy: 89.62%\n",
      "Batch 192, Loss: 0.788278, Accuracy: 89.66%\n",
      "Batch 193, Loss: 0.858349, Accuracy: 89.65%\n",
      "Batch 194, Loss: 0.861335, Accuracy: 89.64%\n",
      "Batch 195, Loss: 0.826351, Accuracy: 89.66%\n",
      "Batch 196, Loss: 0.830017, Accuracy: 89.66%\n",
      "Batch 197, Loss: 0.823555, Accuracy: 89.67%\n",
      "Batch 198, Loss: 0.910433, Accuracy: 89.64%\n",
      "Batch 199, Loss: 0.927188, Accuracy: 89.60%\n",
      "Batch 200, Loss: 0.825693, Accuracy: 89.61%\n",
      "Batch 201, Loss: 0.831202, Accuracy: 89.62%\n",
      "Batch 202, Loss: 0.806939, Accuracy: 89.64%\n",
      "Batch 203, Loss: 0.846985, Accuracy: 89.63%\n",
      "Batch 204, Loss: 0.932230, Accuracy: 89.58%\n",
      "Batch 205, Loss: 0.796596, Accuracy: 89.61%\n",
      "Batch 206, Loss: 0.792011, Accuracy: 89.65%\n",
      "Batch 207, Loss: 0.858183, Accuracy: 89.64%\n",
      "Batch 208, Loss: 0.845070, Accuracy: 89.64%\n",
      "Batch 209, Loss: 0.855642, Accuracy: 89.64%\n",
      "Batch 210, Loss: 0.863748, Accuracy: 89.63%\n",
      "Batch 211, Loss: 0.819866, Accuracy: 89.64%\n",
      "Batch 212, Loss: 0.903583, Accuracy: 89.62%\n",
      "Batch 213, Loss: 0.845792, Accuracy: 89.62%\n",
      "Training - Epoch 127, Loss: 0.848006, Accuracy: 89.62%\n",
      "Validation Batch 1, Loss: 0.815563, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.782804, Accuracy: 95.31%\n",
      "Validation Batch 3, Loss: 0.807420, Accuracy: 94.79%\n",
      "Validation Batch 4, Loss: 0.825459, Accuracy: 93.75%\n",
      "Validation Batch 5, Loss: 0.805820, Accuracy: 93.75%\n",
      "Validation Batch 6, Loss: 0.785380, Accuracy: 94.27%\n",
      "Validation Batch 7, Loss: 0.792794, Accuracy: 94.42%\n",
      "Validation Batch 8, Loss: 0.847921, Accuracy: 93.75%\n",
      "Validation Batch 9, Loss: 0.874896, Accuracy: 93.06%\n",
      "Validation Batch 10, Loss: 0.793909, Accuracy: 93.28%\n",
      "Validation Batch 11, Loss: 0.815201, Accuracy: 93.32%\n",
      "Validation Batch 12, Loss: 0.825000, Accuracy: 93.23%\n",
      "Validation Batch 13, Loss: 0.826608, Accuracy: 93.15%\n",
      "Validation Batch 14, Loss: 0.823296, Accuracy: 93.19%\n",
      "Validation Batch 15, Loss: 0.796103, Accuracy: 93.33%\n",
      "Validation Batch 16, Loss: 0.815265, Accuracy: 93.36%\n",
      "Validation Batch 17, Loss: 0.856445, Accuracy: 93.11%\n",
      "Validation Batch 18, Loss: 0.797095, Accuracy: 93.14%\n",
      "Validation Batch 19, Loss: 0.852819, Accuracy: 92.93%\n",
      "Validation Batch 20, Loss: 0.793873, Accuracy: 93.12%\n",
      "Validation Batch 21, Loss: 0.857774, Accuracy: 92.93%\n",
      "Validation Batch 22, Loss: 0.800539, Accuracy: 93.04%\n",
      "Validation Batch 23, Loss: 0.851485, Accuracy: 92.87%\n",
      "Validation Batch 24, Loss: 0.817510, Accuracy: 92.90%\n",
      "Validation Batch 25, Loss: 0.793306, Accuracy: 93.06%\n",
      "Validation Batch 26, Loss: 0.828035, Accuracy: 93.03%\n",
      "Validation Batch 27, Loss: 0.792842, Accuracy: 93.07%\n",
      "Validation - Epoch 127, Loss: 0.817598, Accuracy: 93.07%\n",
      "Patience—0\n",
      "Epoch 128\n",
      "Batch 1, Loss: 0.805170, Accuracy: 93.75%\n",
      "Batch 2, Loss: 0.844293, Accuracy: 91.41%\n",
      "Batch 3, Loss: 0.809260, Accuracy: 91.67%\n",
      "Batch 4, Loss: 0.877169, Accuracy: 90.62%\n",
      "Batch 5, Loss: 0.864856, Accuracy: 89.69%\n",
      "Batch 6, Loss: 0.875566, Accuracy: 89.06%\n",
      "Batch 7, Loss: 0.811272, Accuracy: 89.73%\n",
      "Batch 8, Loss: 0.804577, Accuracy: 90.43%\n",
      "Batch 9, Loss: 0.875370, Accuracy: 90.10%\n",
      "Batch 10, Loss: 0.840854, Accuracy: 90.16%\n",
      "Batch 11, Loss: 0.843489, Accuracy: 90.20%\n",
      "Batch 12, Loss: 0.861020, Accuracy: 89.97%\n",
      "Batch 13, Loss: 0.829661, Accuracy: 90.14%\n",
      "Batch 14, Loss: 0.833112, Accuracy: 90.29%\n",
      "Batch 15, Loss: 0.885112, Accuracy: 90.00%\n",
      "Batch 16, Loss: 0.820314, Accuracy: 90.14%\n",
      "Batch 17, Loss: 0.826725, Accuracy: 90.26%\n",
      "Batch 18, Loss: 0.851843, Accuracy: 90.19%\n",
      "Batch 19, Loss: 0.832552, Accuracy: 90.30%\n",
      "Batch 20, Loss: 0.827927, Accuracy: 90.39%\n",
      "Batch 21, Loss: 0.814808, Accuracy: 90.55%\n",
      "Batch 22, Loss: 0.852764, Accuracy: 90.55%\n",
      "Batch 23, Loss: 0.898431, Accuracy: 90.29%\n",
      "Batch 24, Loss: 0.951915, Accuracy: 89.78%\n",
      "Batch 25, Loss: 0.885405, Accuracy: 89.62%\n",
      "Batch 26, Loss: 0.863685, Accuracy: 89.54%\n",
      "Batch 27, Loss: 0.909276, Accuracy: 89.29%\n",
      "Batch 28, Loss: 0.869765, Accuracy: 89.23%\n",
      "Batch 29, Loss: 0.802026, Accuracy: 89.39%\n",
      "Batch 30, Loss: 0.856758, Accuracy: 89.32%\n",
      "Batch 31, Loss: 0.864119, Accuracy: 89.21%\n",
      "Batch 32, Loss: 0.892191, Accuracy: 89.01%\n",
      "Batch 33, Loss: 0.916349, Accuracy: 88.83%\n",
      "Batch 34, Loss: 0.780753, Accuracy: 89.06%\n",
      "Batch 35, Loss: 0.845246, Accuracy: 89.11%\n",
      "Batch 36, Loss: 0.869464, Accuracy: 89.11%\n",
      "Batch 37, Loss: 0.807478, Accuracy: 89.23%\n",
      "Batch 38, Loss: 0.917919, Accuracy: 89.06%\n",
      "Batch 39, Loss: 0.898135, Accuracy: 88.98%\n",
      "Batch 40, Loss: 0.838044, Accuracy: 88.98%\n",
      "Batch 41, Loss: 0.843670, Accuracy: 89.02%\n",
      "Batch 42, Loss: 0.863283, Accuracy: 89.03%\n",
      "Batch 43, Loss: 0.855344, Accuracy: 88.99%\n",
      "Batch 44, Loss: 0.905975, Accuracy: 88.85%\n",
      "Batch 45, Loss: 0.832782, Accuracy: 88.89%\n",
      "Batch 46, Loss: 0.875011, Accuracy: 88.86%\n",
      "Batch 47, Loss: 0.865778, Accuracy: 88.86%\n",
      "Batch 48, Loss: 0.877558, Accuracy: 88.80%\n",
      "Batch 49, Loss: 0.832765, Accuracy: 88.84%\n",
      "Batch 50, Loss: 0.793885, Accuracy: 88.97%\n",
      "Batch 51, Loss: 0.832939, Accuracy: 89.06%\n",
      "Batch 52, Loss: 0.805066, Accuracy: 89.18%\n",
      "Batch 53, Loss: 0.855218, Accuracy: 89.15%\n",
      "Batch 54, Loss: 0.835123, Accuracy: 89.24%\n",
      "Batch 55, Loss: 0.827844, Accuracy: 89.29%\n",
      "Batch 56, Loss: 0.859156, Accuracy: 89.29%\n",
      "Batch 57, Loss: 0.848314, Accuracy: 89.28%\n",
      "Batch 58, Loss: 0.836573, Accuracy: 89.30%\n",
      "Batch 59, Loss: 0.761002, Accuracy: 89.46%\n",
      "Batch 60, Loss: 0.806103, Accuracy: 89.53%\n",
      "Batch 61, Loss: 0.847295, Accuracy: 89.55%\n",
      "Batch 62, Loss: 0.928415, Accuracy: 89.42%\n",
      "Batch 63, Loss: 0.857800, Accuracy: 89.38%\n",
      "Batch 64, Loss: 0.827930, Accuracy: 89.43%\n",
      "Batch 65, Loss: 0.832045, Accuracy: 89.45%\n",
      "Batch 66, Loss: 0.820654, Accuracy: 89.49%\n",
      "Batch 67, Loss: 0.849717, Accuracy: 89.48%\n",
      "Batch 68, Loss: 0.882770, Accuracy: 89.41%\n",
      "Batch 69, Loss: 0.880532, Accuracy: 89.38%\n",
      "Batch 70, Loss: 0.870134, Accuracy: 89.35%\n",
      "Batch 71, Loss: 0.820051, Accuracy: 89.39%\n",
      "Batch 72, Loss: 0.855962, Accuracy: 89.39%\n",
      "Batch 73, Loss: 0.808711, Accuracy: 89.45%\n",
      "Batch 74, Loss: 0.805151, Accuracy: 89.53%\n",
      "Batch 75, Loss: 0.863902, Accuracy: 89.50%\n",
      "Batch 76, Loss: 0.834074, Accuracy: 89.51%\n",
      "Batch 77, Loss: 0.799232, Accuracy: 89.57%\n",
      "Batch 78, Loss: 0.914805, Accuracy: 89.48%\n",
      "Batch 79, Loss: 0.874670, Accuracy: 89.44%\n",
      "Batch 80, Loss: 0.839810, Accuracy: 89.45%\n",
      "Batch 81, Loss: 0.879800, Accuracy: 89.41%\n",
      "Batch 82, Loss: 0.890578, Accuracy: 89.37%\n",
      "Batch 83, Loss: 0.878179, Accuracy: 89.34%\n",
      "Batch 84, Loss: 0.855895, Accuracy: 89.32%\n",
      "Batch 85, Loss: 0.867722, Accuracy: 89.30%\n",
      "Batch 86, Loss: 0.897784, Accuracy: 89.24%\n",
      "Batch 87, Loss: 0.798827, Accuracy: 89.31%\n",
      "Batch 88, Loss: 0.836649, Accuracy: 89.33%\n",
      "Batch 89, Loss: 0.820489, Accuracy: 89.34%\n",
      "Batch 90, Loss: 0.885026, Accuracy: 89.29%\n",
      "Batch 91, Loss: 0.832910, Accuracy: 89.30%\n",
      "Batch 92, Loss: 0.859027, Accuracy: 89.30%\n",
      "Batch 93, Loss: 0.897892, Accuracy: 89.25%\n",
      "Batch 94, Loss: 0.886217, Accuracy: 89.21%\n",
      "Batch 95, Loss: 0.808447, Accuracy: 89.26%\n",
      "Batch 96, Loss: 0.834019, Accuracy: 89.29%\n",
      "Batch 97, Loss: 0.858151, Accuracy: 89.29%\n",
      "Batch 98, Loss: 0.869972, Accuracy: 89.27%\n",
      "Batch 99, Loss: 0.879039, Accuracy: 89.24%\n",
      "Batch 100, Loss: 0.777127, Accuracy: 89.31%\n",
      "Batch 101, Loss: 0.854087, Accuracy: 89.33%\n",
      "Batch 102, Loss: 0.820213, Accuracy: 89.35%\n",
      "Batch 103, Loss: 0.814128, Accuracy: 89.38%\n",
      "Batch 104, Loss: 0.818372, Accuracy: 89.41%\n",
      "Batch 105, Loss: 0.866592, Accuracy: 89.39%\n",
      "Batch 106, Loss: 0.842452, Accuracy: 89.40%\n",
      "Batch 107, Loss: 0.858896, Accuracy: 89.38%\n",
      "Batch 108, Loss: 0.860170, Accuracy: 89.37%\n",
      "Batch 109, Loss: 0.864393, Accuracy: 89.36%\n",
      "Batch 110, Loss: 0.829273, Accuracy: 89.39%\n",
      "Batch 111, Loss: 0.823002, Accuracy: 89.43%\n",
      "Batch 112, Loss: 0.928144, Accuracy: 89.36%\n",
      "Batch 113, Loss: 0.831549, Accuracy: 89.38%\n",
      "Batch 114, Loss: 0.819148, Accuracy: 89.41%\n",
      "Batch 115, Loss: 0.823346, Accuracy: 89.44%\n",
      "Batch 116, Loss: 0.846534, Accuracy: 89.44%\n",
      "Batch 117, Loss: 0.813237, Accuracy: 89.48%\n",
      "Batch 118, Loss: 0.819482, Accuracy: 89.50%\n",
      "Batch 119, Loss: 0.837481, Accuracy: 89.50%\n",
      "Batch 120, Loss: 0.798127, Accuracy: 89.54%\n",
      "Batch 121, Loss: 0.839035, Accuracy: 89.55%\n",
      "Batch 122, Loss: 0.881777, Accuracy: 89.52%\n",
      "Batch 123, Loss: 0.818511, Accuracy: 89.55%\n",
      "Batch 124, Loss: 0.907925, Accuracy: 89.50%\n",
      "Batch 125, Loss: 0.813136, Accuracy: 89.54%\n",
      "Batch 126, Loss: 0.850440, Accuracy: 89.55%\n",
      "Batch 127, Loss: 0.827735, Accuracy: 89.55%\n",
      "Batch 128, Loss: 0.861222, Accuracy: 89.55%\n",
      "Batch 129, Loss: 0.851761, Accuracy: 89.55%\n",
      "Batch 130, Loss: 0.811973, Accuracy: 89.59%\n",
      "Batch 131, Loss: 0.863132, Accuracy: 89.59%\n",
      "Batch 132, Loss: 0.828198, Accuracy: 89.60%\n",
      "Batch 133, Loss: 0.834085, Accuracy: 89.61%\n",
      "Batch 134, Loss: 0.899880, Accuracy: 89.58%\n",
      "Batch 135, Loss: 0.842238, Accuracy: 89.57%\n",
      "Batch 136, Loss: 0.838443, Accuracy: 89.58%\n",
      "Batch 137, Loss: 0.861065, Accuracy: 89.58%\n",
      "Batch 138, Loss: 0.828264, Accuracy: 89.58%\n",
      "Batch 139, Loss: 0.860681, Accuracy: 89.58%\n",
      "Batch 140, Loss: 0.868641, Accuracy: 89.58%\n",
      "Batch 141, Loss: 0.870203, Accuracy: 89.55%\n",
      "Batch 142, Loss: 0.853433, Accuracy: 89.55%\n",
      "Batch 143, Loss: 0.862407, Accuracy: 89.52%\n",
      "Batch 144, Loss: 0.857066, Accuracy: 89.51%\n",
      "Batch 145, Loss: 0.890879, Accuracy: 89.47%\n",
      "Batch 146, Loss: 0.914184, Accuracy: 89.43%\n",
      "Batch 147, Loss: 0.829537, Accuracy: 89.43%\n",
      "Batch 148, Loss: 0.878454, Accuracy: 89.41%\n",
      "Batch 149, Loss: 0.832398, Accuracy: 89.42%\n",
      "Batch 150, Loss: 0.847003, Accuracy: 89.43%\n",
      "Batch 151, Loss: 0.874259, Accuracy: 89.41%\n",
      "Batch 152, Loss: 0.818649, Accuracy: 89.42%\n",
      "Batch 153, Loss: 0.845775, Accuracy: 89.42%\n",
      "Batch 154, Loss: 0.818000, Accuracy: 89.44%\n",
      "Batch 155, Loss: 0.806343, Accuracy: 89.47%\n",
      "Batch 156, Loss: 0.858443, Accuracy: 89.45%\n",
      "Batch 157, Loss: 0.883617, Accuracy: 89.43%\n",
      "Batch 158, Loss: 0.857323, Accuracy: 89.42%\n",
      "Batch 159, Loss: 0.846391, Accuracy: 89.42%\n",
      "Batch 160, Loss: 0.824492, Accuracy: 89.43%\n",
      "Batch 161, Loss: 0.855537, Accuracy: 89.42%\n",
      "Batch 162, Loss: 0.853350, Accuracy: 89.42%\n",
      "Batch 163, Loss: 0.898955, Accuracy: 89.38%\n",
      "Batch 164, Loss: 0.843555, Accuracy: 89.39%\n",
      "Batch 165, Loss: 0.871250, Accuracy: 89.38%\n",
      "Batch 166, Loss: 0.798167, Accuracy: 89.40%\n",
      "Batch 167, Loss: 0.831927, Accuracy: 89.42%\n",
      "Batch 168, Loss: 0.819512, Accuracy: 89.43%\n",
      "Batch 169, Loss: 0.864453, Accuracy: 89.43%\n",
      "Batch 170, Loss: 0.885202, Accuracy: 89.41%\n",
      "Batch 171, Loss: 0.829738, Accuracy: 89.42%\n",
      "Batch 172, Loss: 0.849200, Accuracy: 89.42%\n",
      "Batch 173, Loss: 0.813699, Accuracy: 89.44%\n",
      "Batch 174, Loss: 0.854320, Accuracy: 89.44%\n",
      "Batch 175, Loss: 0.854257, Accuracy: 89.44%\n",
      "Batch 176, Loss: 0.836766, Accuracy: 89.45%\n",
      "Batch 177, Loss: 0.845238, Accuracy: 89.46%\n",
      "Batch 178, Loss: 0.860074, Accuracy: 89.46%\n",
      "Batch 179, Loss: 0.826191, Accuracy: 89.48%\n",
      "Batch 180, Loss: 0.854717, Accuracy: 89.48%\n",
      "Batch 181, Loss: 0.791579, Accuracy: 89.51%\n",
      "Batch 182, Loss: 0.839260, Accuracy: 89.53%\n",
      "Batch 183, Loss: 0.828445, Accuracy: 89.53%\n",
      "Batch 184, Loss: 0.839185, Accuracy: 89.54%\n",
      "Batch 185, Loss: 0.864576, Accuracy: 89.53%\n",
      "Batch 186, Loss: 0.866687, Accuracy: 89.52%\n",
      "Batch 187, Loss: 0.882801, Accuracy: 89.49%\n",
      "Batch 188, Loss: 0.878594, Accuracy: 89.47%\n",
      "Batch 189, Loss: 0.845494, Accuracy: 89.47%\n",
      "Batch 190, Loss: 0.799850, Accuracy: 89.50%\n",
      "Batch 191, Loss: 0.870178, Accuracy: 89.49%\n",
      "Batch 192, Loss: 0.842725, Accuracy: 89.49%\n",
      "Batch 193, Loss: 0.834776, Accuracy: 89.48%\n",
      "Batch 194, Loss: 0.768033, Accuracy: 89.53%\n",
      "Batch 195, Loss: 0.837591, Accuracy: 89.54%\n",
      "Batch 196, Loss: 0.848598, Accuracy: 89.53%\n",
      "Batch 197, Loss: 0.864518, Accuracy: 89.52%\n",
      "Batch 198, Loss: 0.832811, Accuracy: 89.54%\n",
      "Batch 199, Loss: 0.820965, Accuracy: 89.56%\n",
      "Batch 200, Loss: 0.818905, Accuracy: 89.57%\n",
      "Batch 201, Loss: 0.873282, Accuracy: 89.56%\n",
      "Batch 202, Loss: 0.867223, Accuracy: 89.55%\n",
      "Batch 203, Loss: 0.809797, Accuracy: 89.57%\n",
      "Batch 204, Loss: 0.837376, Accuracy: 89.57%\n",
      "Batch 205, Loss: 0.841637, Accuracy: 89.57%\n",
      "Batch 206, Loss: 0.895235, Accuracy: 89.55%\n",
      "Batch 207, Loss: 0.873750, Accuracy: 89.53%\n",
      "Batch 208, Loss: 0.848544, Accuracy: 89.54%\n",
      "Batch 209, Loss: 0.810834, Accuracy: 89.56%\n",
      "Batch 210, Loss: 0.837247, Accuracy: 89.57%\n",
      "Batch 211, Loss: 0.870087, Accuracy: 89.56%\n",
      "Batch 212, Loss: 0.879492, Accuracy: 89.54%\n",
      "Batch 213, Loss: 0.840429, Accuracy: 89.55%\n",
      "Training - Epoch 128, Loss: 0.848487, Accuracy: 89.55%\n",
      "Validation Batch 1, Loss: 0.815045, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.788526, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.810600, Accuracy: 94.27%\n",
      "Validation Batch 4, Loss: 0.829926, Accuracy: 93.75%\n",
      "Validation Batch 5, Loss: 0.808668, Accuracy: 93.75%\n",
      "Validation Batch 6, Loss: 0.789498, Accuracy: 94.01%\n",
      "Validation Batch 7, Loss: 0.792926, Accuracy: 94.20%\n",
      "Validation Batch 8, Loss: 0.850783, Accuracy: 93.75%\n",
      "Validation Batch 9, Loss: 0.881243, Accuracy: 92.88%\n",
      "Validation Batch 10, Loss: 0.791794, Accuracy: 93.12%\n",
      "Validation Batch 11, Loss: 0.825469, Accuracy: 93.04%\n",
      "Validation Batch 12, Loss: 0.826892, Accuracy: 92.97%\n",
      "Validation Batch 13, Loss: 0.834566, Accuracy: 92.79%\n",
      "Validation Batch 14, Loss: 0.831478, Accuracy: 92.75%\n",
      "Validation Batch 15, Loss: 0.790578, Accuracy: 92.92%\n",
      "Validation Batch 16, Loss: 0.826529, Accuracy: 92.87%\n",
      "Validation Batch 17, Loss: 0.862110, Accuracy: 92.65%\n",
      "Validation Batch 18, Loss: 0.792413, Accuracy: 92.80%\n",
      "Validation Batch 19, Loss: 0.848489, Accuracy: 92.68%\n",
      "Validation Batch 20, Loss: 0.807763, Accuracy: 92.81%\n",
      "Validation Batch 21, Loss: 0.854321, Accuracy: 92.63%\n",
      "Validation Batch 22, Loss: 0.805822, Accuracy: 92.68%\n",
      "Validation Batch 23, Loss: 0.856309, Accuracy: 92.53%\n",
      "Validation Batch 24, Loss: 0.835094, Accuracy: 92.38%\n",
      "Validation Batch 25, Loss: 0.794087, Accuracy: 92.50%\n",
      "Validation Batch 26, Loss: 0.834597, Accuracy: 92.49%\n",
      "Validation Batch 27, Loss: 0.797864, Accuracy: 92.54%\n",
      "Validation - Epoch 128, Loss: 0.821607, Accuracy: 92.54%\n",
      "Patience—1\n",
      "Epoch 129\n",
      "Batch 1, Loss: 0.803100, Accuracy: 95.31%\n",
      "Batch 2, Loss: 0.841928, Accuracy: 92.97%\n",
      "Batch 3, Loss: 0.877732, Accuracy: 90.62%\n",
      "Batch 4, Loss: 0.870773, Accuracy: 89.84%\n",
      "Batch 5, Loss: 0.855602, Accuracy: 89.69%\n",
      "Batch 6, Loss: 0.867178, Accuracy: 89.32%\n",
      "Batch 7, Loss: 0.827129, Accuracy: 89.73%\n",
      "Batch 8, Loss: 0.830808, Accuracy: 90.04%\n",
      "Batch 9, Loss: 0.818551, Accuracy: 90.28%\n",
      "Batch 10, Loss: 0.836729, Accuracy: 90.47%\n",
      "Batch 11, Loss: 0.822323, Accuracy: 90.62%\n",
      "Batch 12, Loss: 0.778114, Accuracy: 91.15%\n",
      "Batch 13, Loss: 0.841084, Accuracy: 90.99%\n",
      "Batch 14, Loss: 0.786375, Accuracy: 91.41%\n",
      "Batch 15, Loss: 0.852910, Accuracy: 91.25%\n",
      "Batch 16, Loss: 0.796506, Accuracy: 91.50%\n",
      "Batch 17, Loss: 0.836953, Accuracy: 91.45%\n",
      "Batch 18, Loss: 0.872569, Accuracy: 91.23%\n",
      "Batch 19, Loss: 0.818828, Accuracy: 91.28%\n",
      "Batch 20, Loss: 0.842619, Accuracy: 91.25%\n",
      "Batch 21, Loss: 0.790911, Accuracy: 91.44%\n",
      "Batch 22, Loss: 0.853838, Accuracy: 91.26%\n",
      "Batch 23, Loss: 0.807927, Accuracy: 91.37%\n",
      "Batch 24, Loss: 0.846696, Accuracy: 91.28%\n",
      "Batch 25, Loss: 0.860979, Accuracy: 91.12%\n",
      "Batch 26, Loss: 0.841559, Accuracy: 91.11%\n",
      "Batch 27, Loss: 0.794322, Accuracy: 91.26%\n",
      "Batch 28, Loss: 0.837008, Accuracy: 91.24%\n",
      "Batch 29, Loss: 0.894051, Accuracy: 90.95%\n",
      "Batch 30, Loss: 0.840499, Accuracy: 90.94%\n",
      "Batch 31, Loss: 0.832720, Accuracy: 90.93%\n",
      "Batch 32, Loss: 0.896650, Accuracy: 90.72%\n",
      "Batch 33, Loss: 0.901642, Accuracy: 90.53%\n",
      "Batch 34, Loss: 0.864662, Accuracy: 90.49%\n",
      "Batch 35, Loss: 0.803613, Accuracy: 90.62%\n",
      "Batch 36, Loss: 0.812713, Accuracy: 90.71%\n",
      "Batch 37, Loss: 0.824850, Accuracy: 90.75%\n",
      "Batch 38, Loss: 0.851354, Accuracy: 90.75%\n",
      "Batch 39, Loss: 0.901147, Accuracy: 90.58%\n",
      "Batch 40, Loss: 0.829581, Accuracy: 90.59%\n",
      "Batch 41, Loss: 0.889965, Accuracy: 90.43%\n",
      "Batch 42, Loss: 0.823370, Accuracy: 90.51%\n",
      "Batch 43, Loss: 0.833661, Accuracy: 90.48%\n",
      "Batch 44, Loss: 0.766396, Accuracy: 90.66%\n",
      "Batch 45, Loss: 0.844616, Accuracy: 90.66%\n",
      "Batch 46, Loss: 0.870788, Accuracy: 90.62%\n",
      "Batch 47, Loss: 0.834244, Accuracy: 90.62%\n",
      "Batch 48, Loss: 0.807820, Accuracy: 90.69%\n",
      "Batch 49, Loss: 0.836295, Accuracy: 90.69%\n",
      "Batch 50, Loss: 0.821576, Accuracy: 90.75%\n",
      "Batch 51, Loss: 0.824219, Accuracy: 90.81%\n",
      "Batch 52, Loss: 0.835216, Accuracy: 90.81%\n",
      "Batch 53, Loss: 0.785298, Accuracy: 90.92%\n",
      "Batch 54, Loss: 0.821757, Accuracy: 90.94%\n",
      "Batch 55, Loss: 0.833774, Accuracy: 90.91%\n",
      "Batch 56, Loss: 0.909503, Accuracy: 90.82%\n",
      "Batch 57, Loss: 0.805315, Accuracy: 90.87%\n",
      "Batch 58, Loss: 0.855539, Accuracy: 90.84%\n",
      "Batch 59, Loss: 0.828918, Accuracy: 90.86%\n",
      "Batch 60, Loss: 0.803856, Accuracy: 90.91%\n",
      "Batch 61, Loss: 0.870818, Accuracy: 90.86%\n",
      "Batch 62, Loss: 0.814109, Accuracy: 90.90%\n",
      "Batch 63, Loss: 0.878339, Accuracy: 90.85%\n",
      "Batch 64, Loss: 0.862664, Accuracy: 90.84%\n",
      "Batch 65, Loss: 0.837647, Accuracy: 90.84%\n",
      "Batch 66, Loss: 0.831345, Accuracy: 90.86%\n",
      "Batch 67, Loss: 0.841277, Accuracy: 90.88%\n",
      "Batch 68, Loss: 0.872070, Accuracy: 90.81%\n",
      "Batch 69, Loss: 0.910091, Accuracy: 90.72%\n",
      "Batch 70, Loss: 0.865469, Accuracy: 90.65%\n",
      "Batch 71, Loss: 0.863550, Accuracy: 90.62%\n",
      "Batch 72, Loss: 0.909152, Accuracy: 90.54%\n",
      "Batch 73, Loss: 0.843182, Accuracy: 90.54%\n",
      "Batch 74, Loss: 0.819577, Accuracy: 90.58%\n",
      "Batch 75, Loss: 0.792075, Accuracy: 90.67%\n",
      "Batch 76, Loss: 0.912073, Accuracy: 90.58%\n",
      "Batch 77, Loss: 0.903011, Accuracy: 90.50%\n",
      "Batch 78, Loss: 0.869769, Accuracy: 90.44%\n",
      "Batch 79, Loss: 0.860536, Accuracy: 90.43%\n",
      "Batch 80, Loss: 0.839323, Accuracy: 90.43%\n",
      "Batch 81, Loss: 0.873589, Accuracy: 90.39%\n",
      "Batch 82, Loss: 0.844332, Accuracy: 90.40%\n",
      "Batch 83, Loss: 0.828674, Accuracy: 90.42%\n",
      "Batch 84, Loss: 0.806471, Accuracy: 90.46%\n",
      "Batch 85, Loss: 0.889831, Accuracy: 90.40%\n",
      "Batch 86, Loss: 0.840713, Accuracy: 90.41%\n",
      "Batch 87, Loss: 0.859923, Accuracy: 90.39%\n",
      "Batch 88, Loss: 0.852338, Accuracy: 90.39%\n",
      "Batch 89, Loss: 0.806283, Accuracy: 90.43%\n",
      "Batch 90, Loss: 0.825534, Accuracy: 90.45%\n",
      "Batch 91, Loss: 0.832301, Accuracy: 90.45%\n",
      "Batch 92, Loss: 0.842742, Accuracy: 90.44%\n",
      "Batch 93, Loss: 0.805256, Accuracy: 90.49%\n",
      "Batch 94, Loss: 0.898531, Accuracy: 90.43%\n",
      "Batch 95, Loss: 0.866401, Accuracy: 90.41%\n",
      "Batch 96, Loss: 0.867204, Accuracy: 90.38%\n",
      "Batch 97, Loss: 0.910493, Accuracy: 90.30%\n",
      "Batch 98, Loss: 0.828969, Accuracy: 90.31%\n",
      "Batch 99, Loss: 0.835272, Accuracy: 90.33%\n",
      "Batch 100, Loss: 0.843715, Accuracy: 90.31%\n",
      "Batch 101, Loss: 0.873181, Accuracy: 90.27%\n",
      "Batch 102, Loss: 0.845744, Accuracy: 90.26%\n",
      "Batch 103, Loss: 0.839898, Accuracy: 90.26%\n",
      "Batch 104, Loss: 0.897591, Accuracy: 90.20%\n",
      "Batch 105, Loss: 0.858977, Accuracy: 90.19%\n",
      "Batch 106, Loss: 0.804066, Accuracy: 90.23%\n",
      "Batch 107, Loss: 0.874867, Accuracy: 90.20%\n",
      "Batch 108, Loss: 0.799409, Accuracy: 90.23%\n",
      "Batch 109, Loss: 0.854871, Accuracy: 90.22%\n",
      "Batch 110, Loss: 0.774288, Accuracy: 90.28%\n",
      "Batch 111, Loss: 0.869339, Accuracy: 90.27%\n",
      "Batch 112, Loss: 0.808400, Accuracy: 90.30%\n",
      "Batch 113, Loss: 0.817626, Accuracy: 90.32%\n",
      "Batch 114, Loss: 0.814080, Accuracy: 90.34%\n",
      "Batch 115, Loss: 0.867127, Accuracy: 90.33%\n",
      "Batch 116, Loss: 0.866073, Accuracy: 90.29%\n",
      "Batch 117, Loss: 0.840072, Accuracy: 90.29%\n",
      "Batch 118, Loss: 0.814836, Accuracy: 90.31%\n",
      "Batch 119, Loss: 0.800056, Accuracy: 90.35%\n",
      "Batch 120, Loss: 0.802294, Accuracy: 90.39%\n",
      "Batch 121, Loss: 0.880904, Accuracy: 90.37%\n",
      "Batch 122, Loss: 0.878759, Accuracy: 90.33%\n",
      "Batch 123, Loss: 0.882696, Accuracy: 90.28%\n",
      "Batch 124, Loss: 0.854751, Accuracy: 90.28%\n",
      "Batch 125, Loss: 0.913312, Accuracy: 90.21%\n",
      "Batch 126, Loss: 0.823993, Accuracy: 90.23%\n",
      "Batch 127, Loss: 0.807144, Accuracy: 90.26%\n",
      "Batch 128, Loss: 0.787381, Accuracy: 90.28%\n",
      "Batch 129, Loss: 0.881596, Accuracy: 90.26%\n",
      "Batch 130, Loss: 0.815709, Accuracy: 90.28%\n",
      "Batch 131, Loss: 0.840168, Accuracy: 90.28%\n",
      "Batch 132, Loss: 0.791429, Accuracy: 90.32%\n",
      "Batch 133, Loss: 0.844175, Accuracy: 90.31%\n",
      "Batch 134, Loss: 0.871144, Accuracy: 90.29%\n",
      "Batch 135, Loss: 0.826697, Accuracy: 90.29%\n",
      "Batch 136, Loss: 0.800427, Accuracy: 90.31%\n",
      "Batch 137, Loss: 0.790351, Accuracy: 90.35%\n",
      "Batch 138, Loss: 0.862446, Accuracy: 90.33%\n",
      "Batch 139, Loss: 0.809201, Accuracy: 90.36%\n",
      "Batch 140, Loss: 0.863107, Accuracy: 90.33%\n",
      "Batch 141, Loss: 0.884096, Accuracy: 90.30%\n",
      "Batch 142, Loss: 0.851887, Accuracy: 90.31%\n",
      "Batch 143, Loss: 0.878372, Accuracy: 90.28%\n",
      "Batch 144, Loss: 0.857861, Accuracy: 90.27%\n",
      "Batch 145, Loss: 0.855103, Accuracy: 90.26%\n",
      "Batch 146, Loss: 0.834977, Accuracy: 90.26%\n",
      "Batch 147, Loss: 0.846468, Accuracy: 90.25%\n",
      "Batch 148, Loss: 0.835318, Accuracy: 90.26%\n",
      "Batch 149, Loss: 0.872087, Accuracy: 90.23%\n",
      "Batch 150, Loss: 0.849336, Accuracy: 90.22%\n",
      "Batch 151, Loss: 0.826820, Accuracy: 90.22%\n",
      "Batch 152, Loss: 0.940028, Accuracy: 90.15%\n",
      "Batch 153, Loss: 0.837468, Accuracy: 90.16%\n",
      "Batch 154, Loss: 0.797965, Accuracy: 90.19%\n",
      "Batch 155, Loss: 0.832406, Accuracy: 90.19%\n",
      "Batch 156, Loss: 0.890713, Accuracy: 90.17%\n",
      "Batch 157, Loss: 0.871677, Accuracy: 90.16%\n",
      "Batch 158, Loss: 0.897198, Accuracy: 90.12%\n",
      "Batch 159, Loss: 0.840796, Accuracy: 90.11%\n",
      "Batch 160, Loss: 0.822173, Accuracy: 90.13%\n",
      "Batch 161, Loss: 0.817952, Accuracy: 90.14%\n",
      "Batch 162, Loss: 0.887163, Accuracy: 90.11%\n",
      "Batch 163, Loss: 0.870913, Accuracy: 90.10%\n",
      "Batch 164, Loss: 0.868721, Accuracy: 90.08%\n",
      "Batch 165, Loss: 0.841037, Accuracy: 90.09%\n",
      "Batch 166, Loss: 0.803652, Accuracy: 90.11%\n",
      "Batch 167, Loss: 0.812831, Accuracy: 90.14%\n",
      "Batch 168, Loss: 0.838417, Accuracy: 90.14%\n",
      "Batch 169, Loss: 0.855962, Accuracy: 90.14%\n",
      "Batch 170, Loss: 0.873726, Accuracy: 90.12%\n",
      "Batch 171, Loss: 0.792923, Accuracy: 90.15%\n",
      "Batch 172, Loss: 0.896181, Accuracy: 90.13%\n",
      "Batch 173, Loss: 0.818729, Accuracy: 90.16%\n",
      "Batch 174, Loss: 0.899545, Accuracy: 90.12%\n",
      "Batch 175, Loss: 0.818002, Accuracy: 90.14%\n",
      "Batch 176, Loss: 0.867093, Accuracy: 90.14%\n",
      "Batch 177, Loss: 0.857823, Accuracy: 90.14%\n",
      "Batch 178, Loss: 0.829326, Accuracy: 90.15%\n",
      "Batch 179, Loss: 0.868670, Accuracy: 90.13%\n",
      "Batch 180, Loss: 0.800347, Accuracy: 90.15%\n",
      "Batch 181, Loss: 0.839629, Accuracy: 90.15%\n",
      "Batch 182, Loss: 0.812473, Accuracy: 90.17%\n",
      "Batch 183, Loss: 0.841857, Accuracy: 90.16%\n",
      "Batch 184, Loss: 0.885942, Accuracy: 90.14%\n",
      "Batch 185, Loss: 0.820644, Accuracy: 90.16%\n",
      "Batch 186, Loss: 0.865921, Accuracy: 90.14%\n",
      "Batch 187, Loss: 0.835275, Accuracy: 90.14%\n",
      "Batch 188, Loss: 0.814710, Accuracy: 90.15%\n",
      "Batch 189, Loss: 0.803850, Accuracy: 90.18%\n",
      "Batch 190, Loss: 0.879964, Accuracy: 90.16%\n",
      "Batch 191, Loss: 0.867645, Accuracy: 90.16%\n",
      "Batch 192, Loss: 0.808080, Accuracy: 90.17%\n",
      "Batch 193, Loss: 0.865443, Accuracy: 90.16%\n",
      "Batch 194, Loss: 0.811030, Accuracy: 90.17%\n",
      "Batch 195, Loss: 0.835497, Accuracy: 90.18%\n",
      "Batch 196, Loss: 0.843964, Accuracy: 90.19%\n",
      "Batch 197, Loss: 0.852088, Accuracy: 90.19%\n",
      "Batch 198, Loss: 0.858900, Accuracy: 90.18%\n",
      "Batch 199, Loss: 0.800129, Accuracy: 90.21%\n",
      "Batch 200, Loss: 0.875955, Accuracy: 90.20%\n",
      "Batch 201, Loss: 0.934072, Accuracy: 90.14%\n",
      "Batch 202, Loss: 0.914587, Accuracy: 90.11%\n",
      "Batch 203, Loss: 0.836811, Accuracy: 90.11%\n",
      "Batch 204, Loss: 0.929107, Accuracy: 90.07%\n",
      "Batch 205, Loss: 0.894260, Accuracy: 90.04%\n",
      "Batch 206, Loss: 0.794426, Accuracy: 90.06%\n",
      "Batch 207, Loss: 0.870212, Accuracy: 90.06%\n",
      "Batch 208, Loss: 0.865403, Accuracy: 90.05%\n",
      "Batch 209, Loss: 0.840594, Accuracy: 90.06%\n",
      "Batch 210, Loss: 0.814403, Accuracy: 90.07%\n",
      "Batch 211, Loss: 0.858914, Accuracy: 90.08%\n",
      "Batch 212, Loss: 0.867877, Accuracy: 90.06%\n",
      "Batch 213, Loss: 0.852902, Accuracy: 90.06%\n",
      "Training - Epoch 129, Loss: 0.845229, Accuracy: 90.06%\n",
      "Validation Batch 1, Loss: 0.812806, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.788115, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.803018, Accuracy: 94.27%\n",
      "Validation Batch 4, Loss: 0.829085, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.804317, Accuracy: 93.75%\n",
      "Validation Batch 6, Loss: 0.788314, Accuracy: 94.01%\n",
      "Validation Batch 7, Loss: 0.792316, Accuracy: 94.20%\n",
      "Validation Batch 8, Loss: 0.850105, Accuracy: 93.55%\n",
      "Validation Batch 9, Loss: 0.875532, Accuracy: 92.71%\n",
      "Validation Batch 10, Loss: 0.786560, Accuracy: 93.12%\n",
      "Validation Batch 11, Loss: 0.819235, Accuracy: 93.04%\n",
      "Validation Batch 12, Loss: 0.824600, Accuracy: 92.97%\n",
      "Validation Batch 13, Loss: 0.824041, Accuracy: 93.03%\n",
      "Validation Batch 14, Loss: 0.824217, Accuracy: 93.08%\n",
      "Validation Batch 15, Loss: 0.791111, Accuracy: 93.23%\n",
      "Validation Batch 16, Loss: 0.816966, Accuracy: 93.26%\n",
      "Validation Batch 17, Loss: 0.862208, Accuracy: 93.01%\n",
      "Validation Batch 18, Loss: 0.790556, Accuracy: 93.14%\n",
      "Validation Batch 19, Loss: 0.842272, Accuracy: 93.09%\n",
      "Validation Batch 20, Loss: 0.797795, Accuracy: 93.28%\n",
      "Validation Batch 21, Loss: 0.853303, Accuracy: 93.08%\n",
      "Validation Batch 22, Loss: 0.796181, Accuracy: 93.18%\n",
      "Validation Batch 23, Loss: 0.852365, Accuracy: 93.00%\n",
      "Validation Batch 24, Loss: 0.819973, Accuracy: 92.97%\n",
      "Validation Batch 25, Loss: 0.787872, Accuracy: 93.12%\n",
      "Validation Batch 26, Loss: 0.829324, Accuracy: 93.09%\n",
      "Validation Batch 27, Loss: 0.790768, Accuracy: 93.13%\n",
      "Validation - Epoch 129, Loss: 0.816776, Accuracy: 93.13%\n",
      "Patience—0\n",
      "Epoch 130\n",
      "Batch 1, Loss: 0.820943, Accuracy: 93.75%\n",
      "Batch 2, Loss: 0.828612, Accuracy: 92.97%\n",
      "Batch 3, Loss: 0.862679, Accuracy: 91.67%\n",
      "Batch 4, Loss: 0.794567, Accuracy: 92.58%\n",
      "Batch 5, Loss: 0.860736, Accuracy: 91.56%\n",
      "Batch 6, Loss: 0.844463, Accuracy: 91.41%\n",
      "Batch 7, Loss: 0.858248, Accuracy: 90.85%\n",
      "Batch 8, Loss: 0.809938, Accuracy: 91.02%\n",
      "Batch 9, Loss: 0.837722, Accuracy: 90.97%\n",
      "Batch 10, Loss: 0.899820, Accuracy: 90.31%\n",
      "Batch 11, Loss: 0.814443, Accuracy: 90.48%\n",
      "Batch 12, Loss: 0.847460, Accuracy: 90.36%\n",
      "Batch 13, Loss: 0.829688, Accuracy: 90.38%\n",
      "Batch 14, Loss: 0.819730, Accuracy: 90.51%\n",
      "Batch 15, Loss: 0.847833, Accuracy: 90.42%\n",
      "Batch 16, Loss: 0.832503, Accuracy: 90.43%\n",
      "Batch 17, Loss: 0.826994, Accuracy: 90.53%\n",
      "Batch 18, Loss: 0.829987, Accuracy: 90.54%\n",
      "Batch 19, Loss: 0.866092, Accuracy: 90.46%\n",
      "Batch 20, Loss: 0.918334, Accuracy: 89.92%\n",
      "Batch 21, Loss: 0.856574, Accuracy: 89.88%\n",
      "Batch 22, Loss: 0.798671, Accuracy: 90.06%\n",
      "Batch 23, Loss: 0.839460, Accuracy: 90.01%\n",
      "Batch 24, Loss: 0.840710, Accuracy: 90.04%\n",
      "Batch 25, Loss: 0.892824, Accuracy: 89.88%\n",
      "Batch 26, Loss: 0.849123, Accuracy: 89.78%\n",
      "Batch 27, Loss: 0.790008, Accuracy: 89.99%\n",
      "Batch 28, Loss: 0.827560, Accuracy: 90.07%\n",
      "Batch 29, Loss: 0.872539, Accuracy: 89.98%\n",
      "Batch 30, Loss: 0.832001, Accuracy: 90.00%\n",
      "Batch 31, Loss: 0.887779, Accuracy: 89.77%\n",
      "Batch 32, Loss: 0.791444, Accuracy: 89.94%\n",
      "Batch 33, Loss: 0.823208, Accuracy: 90.06%\n",
      "Batch 34, Loss: 0.837930, Accuracy: 90.07%\n",
      "Batch 35, Loss: 0.811451, Accuracy: 90.18%\n",
      "Batch 36, Loss: 0.850872, Accuracy: 90.15%\n",
      "Batch 37, Loss: 0.895964, Accuracy: 89.99%\n",
      "Batch 38, Loss: 0.840794, Accuracy: 90.01%\n",
      "Batch 39, Loss: 0.826184, Accuracy: 90.06%\n",
      "Batch 40, Loss: 0.814530, Accuracy: 90.16%\n",
      "Batch 41, Loss: 0.898132, Accuracy: 90.02%\n",
      "Batch 42, Loss: 0.922236, Accuracy: 89.81%\n",
      "Batch 43, Loss: 0.793785, Accuracy: 89.90%\n",
      "Batch 44, Loss: 0.803837, Accuracy: 89.99%\n",
      "Batch 45, Loss: 0.832820, Accuracy: 90.00%\n",
      "Batch 46, Loss: 0.972171, Accuracy: 89.71%\n",
      "Batch 47, Loss: 0.957772, Accuracy: 89.49%\n",
      "Batch 48, Loss: 0.814005, Accuracy: 89.58%\n",
      "Batch 49, Loss: 0.812543, Accuracy: 89.64%\n",
      "Batch 50, Loss: 0.873139, Accuracy: 89.56%\n",
      "Batch 51, Loss: 0.807820, Accuracy: 89.64%\n",
      "Batch 52, Loss: 0.915110, Accuracy: 89.51%\n",
      "Batch 53, Loss: 0.821599, Accuracy: 89.53%\n",
      "Batch 54, Loss: 0.892505, Accuracy: 89.47%\n",
      "Batch 55, Loss: 0.844635, Accuracy: 89.52%\n",
      "Batch 56, Loss: 0.827266, Accuracy: 89.56%\n",
      "Batch 57, Loss: 0.825710, Accuracy: 89.64%\n",
      "Batch 58, Loss: 0.792080, Accuracy: 89.76%\n",
      "Batch 59, Loss: 0.826122, Accuracy: 89.78%\n",
      "Batch 60, Loss: 0.845065, Accuracy: 89.77%\n",
      "Batch 61, Loss: 0.822005, Accuracy: 89.81%\n",
      "Batch 62, Loss: 0.845504, Accuracy: 89.79%\n",
      "Batch 63, Loss: 0.808902, Accuracy: 89.88%\n",
      "Batch 64, Loss: 0.892402, Accuracy: 89.79%\n",
      "Batch 65, Loss: 0.847554, Accuracy: 89.81%\n",
      "Batch 66, Loss: 0.816038, Accuracy: 89.87%\n",
      "Batch 67, Loss: 0.901510, Accuracy: 89.79%\n",
      "Batch 68, Loss: 0.858465, Accuracy: 89.77%\n",
      "Batch 69, Loss: 0.845008, Accuracy: 89.79%\n",
      "Batch 70, Loss: 0.811813, Accuracy: 89.87%\n",
      "Batch 71, Loss: 0.898678, Accuracy: 89.79%\n",
      "Batch 72, Loss: 0.893437, Accuracy: 89.74%\n",
      "Batch 73, Loss: 0.822233, Accuracy: 89.77%\n",
      "Batch 74, Loss: 0.858440, Accuracy: 89.74%\n",
      "Batch 75, Loss: 0.821018, Accuracy: 89.77%\n",
      "Batch 76, Loss: 0.815938, Accuracy: 89.82%\n",
      "Batch 77, Loss: 0.839648, Accuracy: 89.83%\n",
      "Batch 78, Loss: 0.808268, Accuracy: 89.88%\n",
      "Batch 79, Loss: 0.844056, Accuracy: 89.91%\n",
      "Batch 80, Loss: 0.815855, Accuracy: 89.98%\n",
      "Batch 81, Loss: 0.798780, Accuracy: 90.05%\n",
      "Batch 82, Loss: 0.850694, Accuracy: 90.03%\n",
      "Batch 83, Loss: 0.898011, Accuracy: 89.97%\n",
      "Batch 84, Loss: 0.827040, Accuracy: 89.99%\n",
      "Batch 85, Loss: 0.849003, Accuracy: 89.98%\n",
      "Batch 86, Loss: 0.809154, Accuracy: 90.01%\n",
      "Batch 87, Loss: 0.861028, Accuracy: 90.00%\n",
      "Batch 88, Loss: 0.901852, Accuracy: 89.93%\n",
      "Batch 89, Loss: 0.856955, Accuracy: 89.92%\n",
      "Batch 90, Loss: 0.823054, Accuracy: 89.97%\n",
      "Batch 91, Loss: 0.838167, Accuracy: 89.97%\n",
      "Batch 92, Loss: 0.817130, Accuracy: 90.01%\n",
      "Batch 93, Loss: 0.827589, Accuracy: 90.04%\n",
      "Batch 94, Loss: 0.842192, Accuracy: 90.04%\n",
      "Batch 95, Loss: 0.788019, Accuracy: 90.10%\n",
      "Batch 96, Loss: 0.882198, Accuracy: 90.06%\n",
      "Batch 97, Loss: 0.845072, Accuracy: 90.05%\n",
      "Batch 98, Loss: 0.796379, Accuracy: 90.10%\n",
      "Batch 99, Loss: 0.874529, Accuracy: 90.07%\n",
      "Batch 100, Loss: 0.843534, Accuracy: 90.06%\n",
      "Batch 101, Loss: 0.807625, Accuracy: 90.08%\n",
      "Batch 102, Loss: 0.818465, Accuracy: 90.10%\n",
      "Batch 103, Loss: 0.906630, Accuracy: 90.02%\n",
      "Batch 104, Loss: 0.816037, Accuracy: 90.04%\n",
      "Batch 105, Loss: 0.865360, Accuracy: 90.01%\n",
      "Batch 106, Loss: 0.885131, Accuracy: 89.99%\n",
      "Batch 107, Loss: 0.861130, Accuracy: 89.98%\n",
      "Batch 108, Loss: 0.863108, Accuracy: 89.95%\n",
      "Batch 109, Loss: 0.894103, Accuracy: 89.88%\n",
      "Batch 110, Loss: 0.841550, Accuracy: 89.89%\n",
      "Batch 111, Loss: 0.794508, Accuracy: 89.94%\n",
      "Batch 112, Loss: 0.897273, Accuracy: 89.89%\n",
      "Batch 113, Loss: 0.825323, Accuracy: 89.89%\n",
      "Batch 114, Loss: 0.830536, Accuracy: 89.90%\n",
      "Batch 115, Loss: 0.905894, Accuracy: 89.85%\n",
      "Batch 116, Loss: 0.854201, Accuracy: 89.86%\n",
      "Batch 117, Loss: 0.858499, Accuracy: 89.86%\n",
      "Batch 118, Loss: 0.863545, Accuracy: 89.86%\n",
      "Batch 119, Loss: 0.809296, Accuracy: 89.89%\n",
      "Batch 120, Loss: 0.797267, Accuracy: 89.93%\n",
      "Batch 121, Loss: 0.860059, Accuracy: 89.93%\n",
      "Batch 122, Loss: 0.819661, Accuracy: 89.95%\n",
      "Batch 123, Loss: 0.866799, Accuracy: 89.95%\n",
      "Batch 124, Loss: 0.866339, Accuracy: 89.93%\n",
      "Batch 125, Loss: 0.912236, Accuracy: 89.86%\n",
      "Batch 126, Loss: 0.937579, Accuracy: 89.78%\n",
      "Batch 127, Loss: 0.825635, Accuracy: 89.80%\n",
      "Batch 128, Loss: 0.835489, Accuracy: 89.81%\n",
      "Batch 129, Loss: 0.886949, Accuracy: 89.78%\n",
      "Batch 130, Loss: 0.835395, Accuracy: 89.78%\n",
      "Batch 131, Loss: 0.869687, Accuracy: 89.77%\n",
      "Batch 132, Loss: 0.811098, Accuracy: 89.80%\n",
      "Batch 133, Loss: 0.863772, Accuracy: 89.78%\n",
      "Batch 134, Loss: 0.845169, Accuracy: 89.79%\n",
      "Batch 135, Loss: 0.871864, Accuracy: 89.77%\n",
      "Batch 136, Loss: 0.806200, Accuracy: 89.80%\n",
      "Batch 137, Loss: 0.818348, Accuracy: 89.83%\n",
      "Batch 138, Loss: 0.843497, Accuracy: 89.83%\n",
      "Batch 139, Loss: 0.810179, Accuracy: 89.86%\n",
      "Batch 140, Loss: 0.862129, Accuracy: 89.85%\n",
      "Batch 141, Loss: 0.853872, Accuracy: 89.85%\n",
      "Batch 142, Loss: 0.841757, Accuracy: 89.84%\n",
      "Batch 143, Loss: 0.829243, Accuracy: 89.86%\n",
      "Batch 144, Loss: 0.883440, Accuracy: 89.83%\n",
      "Batch 145, Loss: 0.866045, Accuracy: 89.82%\n",
      "Batch 146, Loss: 0.851009, Accuracy: 89.82%\n",
      "Batch 147, Loss: 0.828647, Accuracy: 89.85%\n",
      "Batch 148, Loss: 0.824807, Accuracy: 89.86%\n",
      "Batch 149, Loss: 0.846557, Accuracy: 89.87%\n",
      "Batch 150, Loss: 0.838761, Accuracy: 89.88%\n",
      "Batch 151, Loss: 0.822694, Accuracy: 89.90%\n",
      "Batch 152, Loss: 0.866127, Accuracy: 89.88%\n",
      "Batch 153, Loss: 0.859084, Accuracy: 89.87%\n",
      "Batch 154, Loss: 0.880792, Accuracy: 89.84%\n",
      "Batch 155, Loss: 0.836104, Accuracy: 89.85%\n",
      "Batch 156, Loss: 0.881529, Accuracy: 89.82%\n",
      "Batch 157, Loss: 0.875242, Accuracy: 89.80%\n",
      "Batch 158, Loss: 0.877670, Accuracy: 89.78%\n",
      "Batch 159, Loss: 0.857431, Accuracy: 89.77%\n",
      "Batch 160, Loss: 0.863024, Accuracy: 89.77%\n",
      "Batch 161, Loss: 0.829386, Accuracy: 89.77%\n",
      "Batch 162, Loss: 0.852159, Accuracy: 89.77%\n",
      "Batch 163, Loss: 0.870023, Accuracy: 89.75%\n",
      "Batch 164, Loss: 0.808404, Accuracy: 89.78%\n",
      "Batch 165, Loss: 0.897821, Accuracy: 89.73%\n",
      "Batch 166, Loss: 0.856241, Accuracy: 89.72%\n",
      "Batch 167, Loss: 0.862879, Accuracy: 89.71%\n",
      "Batch 168, Loss: 0.849747, Accuracy: 89.71%\n",
      "Batch 169, Loss: 0.809447, Accuracy: 89.74%\n",
      "Batch 170, Loss: 0.842992, Accuracy: 89.74%\n",
      "Batch 171, Loss: 0.858864, Accuracy: 89.74%\n",
      "Batch 172, Loss: 0.847102, Accuracy: 89.73%\n",
      "Batch 173, Loss: 0.826250, Accuracy: 89.73%\n",
      "Batch 174, Loss: 0.882285, Accuracy: 89.71%\n",
      "Batch 175, Loss: 0.875358, Accuracy: 89.70%\n",
      "Batch 176, Loss: 0.842463, Accuracy: 89.71%\n",
      "Batch 177, Loss: 0.820154, Accuracy: 89.72%\n",
      "Batch 178, Loss: 0.817398, Accuracy: 89.75%\n",
      "Batch 179, Loss: 0.866862, Accuracy: 89.73%\n",
      "Batch 180, Loss: 0.845691, Accuracy: 89.73%\n",
      "Batch 181, Loss: 0.823133, Accuracy: 89.74%\n",
      "Batch 182, Loss: 0.819057, Accuracy: 89.75%\n",
      "Batch 183, Loss: 0.778254, Accuracy: 89.79%\n",
      "Batch 184, Loss: 0.872959, Accuracy: 89.78%\n",
      "Batch 185, Loss: 0.793641, Accuracy: 89.81%\n",
      "Batch 186, Loss: 0.837982, Accuracy: 89.82%\n",
      "Batch 187, Loss: 0.893444, Accuracy: 89.80%\n",
      "Batch 188, Loss: 0.822233, Accuracy: 89.81%\n",
      "Batch 189, Loss: 0.843231, Accuracy: 89.81%\n",
      "Batch 190, Loss: 0.839021, Accuracy: 89.82%\n",
      "Batch 191, Loss: 0.832363, Accuracy: 89.82%\n",
      "Batch 192, Loss: 0.829426, Accuracy: 89.84%\n",
      "Batch 193, Loss: 0.837517, Accuracy: 89.85%\n",
      "Batch 194, Loss: 0.839339, Accuracy: 89.84%\n",
      "Batch 195, Loss: 0.794794, Accuracy: 89.87%\n",
      "Batch 196, Loss: 0.801350, Accuracy: 89.90%\n",
      "Batch 197, Loss: 0.880006, Accuracy: 89.88%\n",
      "Batch 198, Loss: 0.838025, Accuracy: 89.88%\n",
      "Batch 199, Loss: 0.840675, Accuracy: 89.89%\n",
      "Batch 200, Loss: 0.826613, Accuracy: 89.90%\n",
      "Batch 201, Loss: 0.804548, Accuracy: 89.92%\n",
      "Batch 202, Loss: 0.883532, Accuracy: 89.90%\n",
      "Batch 203, Loss: 0.859720, Accuracy: 89.89%\n",
      "Batch 204, Loss: 0.872850, Accuracy: 89.88%\n",
      "Batch 205, Loss: 0.852996, Accuracy: 89.88%\n",
      "Batch 206, Loss: 0.842355, Accuracy: 89.88%\n",
      "Batch 207, Loss: 0.894457, Accuracy: 89.85%\n",
      "Batch 208, Loss: 0.837406, Accuracy: 89.85%\n",
      "Batch 209, Loss: 0.832341, Accuracy: 89.85%\n",
      "Batch 210, Loss: 0.795836, Accuracy: 89.88%\n",
      "Batch 211, Loss: 0.810598, Accuracy: 89.89%\n",
      "Batch 212, Loss: 0.843417, Accuracy: 89.89%\n",
      "Batch 213, Loss: 0.842462, Accuracy: 89.89%\n",
      "Training - Epoch 130, Loss: 0.845589, Accuracy: 89.89%\n",
      "Validation Batch 1, Loss: 0.810378, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.782190, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.800577, Accuracy: 94.79%\n",
      "Validation Batch 4, Loss: 0.821334, Accuracy: 94.14%\n",
      "Validation Batch 5, Loss: 0.804130, Accuracy: 94.38%\n",
      "Validation Batch 6, Loss: 0.782715, Accuracy: 94.79%\n",
      "Validation Batch 7, Loss: 0.791482, Accuracy: 94.87%\n",
      "Validation Batch 8, Loss: 0.848330, Accuracy: 94.14%\n",
      "Validation Batch 9, Loss: 0.868748, Accuracy: 93.40%\n",
      "Validation Batch 10, Loss: 0.782273, Accuracy: 93.75%\n",
      "Validation Batch 11, Loss: 0.818600, Accuracy: 93.75%\n",
      "Validation Batch 12, Loss: 0.824066, Accuracy: 93.62%\n",
      "Validation Batch 13, Loss: 0.814897, Accuracy: 93.63%\n",
      "Validation Batch 14, Loss: 0.824314, Accuracy: 93.64%\n",
      "Validation Batch 15, Loss: 0.792545, Accuracy: 93.75%\n",
      "Validation Batch 16, Loss: 0.814519, Accuracy: 93.75%\n",
      "Validation Batch 17, Loss: 0.852643, Accuracy: 93.47%\n",
      "Validation Batch 18, Loss: 0.787480, Accuracy: 93.58%\n",
      "Validation Batch 19, Loss: 0.835901, Accuracy: 93.50%\n",
      "Validation Batch 20, Loss: 0.791942, Accuracy: 93.67%\n",
      "Validation Batch 21, Loss: 0.853003, Accuracy: 93.45%\n",
      "Validation Batch 22, Loss: 0.787492, Accuracy: 93.61%\n",
      "Validation Batch 23, Loss: 0.848559, Accuracy: 93.41%\n",
      "Validation Batch 24, Loss: 0.813150, Accuracy: 93.36%\n",
      "Validation Batch 25, Loss: 0.786574, Accuracy: 93.50%\n",
      "Validation Batch 26, Loss: 0.826596, Accuracy: 93.45%\n",
      "Validation Batch 27, Loss: 0.784745, Accuracy: 93.48%\n",
      "Validation - Epoch 130, Loss: 0.812933, Accuracy: 93.48%\n",
      "Patience—0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 130  \n",
    "patience = 20  \n",
    "best_val_loss = float('inf')  \n",
    "best_model = None  \n",
    "\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "train_accuracy_list = list()\n",
    "val_accuracy_list = list()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)  \n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(images)  \n",
    "        loss = criterion(outputs, labels)  \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Batch {batch_idx+1}, Loss: {loss.item():.6f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Training - Epoch {epoch+1}, Loss: {train_loss:.6f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()  \n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for batch_idx, (images, labels) in enumerate(val_loader):\n",
    "            images, labels = images.to(device), labels.to(device)  \n",
    "\n",
    "            outputs = model(images) \n",
    "            loss = criterion(outputs, labels) \n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            print(f\"Validation Batch {batch_idx+1}, Loss: {loss.item():.6f}, Accuracy: {100 * val_correct / val_total:.2f}%\")\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    print(f\"Validation - Epoch {epoch+1}, Loss: {val_loss:.6f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accuracy_list.append(val_accuracy)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    print(f'Patience—{patience_counter}')\n",
    "\n",
    "    # Check for early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "model = best_model\n",
    "torch.save(model.state_dict(), f'{experiment}-{model_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_accuracy_list, label='Train Accuracy')\n",
    "\n",
    "# Plotting the second graph\n",
    "plt.plot(val_accuracy_list, label='Val Accuracy')\n",
    "\n",
    "# Adding labels and a legend\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Train vs Val Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_loss_list, label='Train Loss')\n",
    "\n",
    "# Plotting the second graph\n",
    "plt.plot(val_loss_list, label='Val Loss')\n",
    "\n",
    "# Adding labels and a legend\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train vs Val Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch 1, Loss: 0.847946, Accuracy: 89.06%\n",
      "Test Batch 2, Loss: 0.826492, Accuracy: 89.84%\n",
      "Test Batch 3, Loss: 0.830174, Accuracy: 91.15%\n",
      "Test Batch 4, Loss: 0.807507, Accuracy: 91.80%\n",
      "Test Batch 5, Loss: 0.791317, Accuracy: 92.50%\n",
      "Test Batch 6, Loss: 0.835469, Accuracy: 92.45%\n",
      "Test Batch 7, Loss: 0.807470, Accuracy: 92.41%\n",
      "Test Batch 8, Loss: 0.831125, Accuracy: 92.38%\n",
      "Test Batch 9, Loss: 0.814786, Accuracy: 92.36%\n",
      "Test Batch 10, Loss: 0.857463, Accuracy: 92.03%\n",
      "Test Batch 11, Loss: 0.886698, Accuracy: 91.34%\n",
      "Test Batch 12, Loss: 0.773976, Accuracy: 91.93%\n",
      "Test Batch 13, Loss: 0.845479, Accuracy: 91.83%\n",
      "Test Batch 14, Loss: 0.877339, Accuracy: 91.52%\n",
      "Test Batch 15, Loss: 0.903838, Accuracy: 91.04%\n",
      "Test Batch 16, Loss: 0.839689, Accuracy: 90.92%\n",
      "Test Batch 17, Loss: 0.827635, Accuracy: 91.08%\n",
      "Test Batch 18, Loss: 0.898265, Accuracy: 90.62%\n",
      "Test Batch 19, Loss: 0.836514, Accuracy: 90.71%\n",
      "Test Batch 20, Loss: 0.827211, Accuracy: 90.86%\n",
      "Test Batch 21, Loss: 0.800165, Accuracy: 91.07%\n",
      "Test Batch 22, Loss: 0.905980, Accuracy: 90.77%\n",
      "Test Batch 23, Loss: 0.871873, Accuracy: 90.62%\n",
      "Test Batch 24, Loss: 0.794074, Accuracy: 90.89%\n",
      "Test Batch 25, Loss: 0.843469, Accuracy: 90.88%\n",
      "Test Batch 26, Loss: 0.815138, Accuracy: 90.99%\n",
      "Test Batch 27, Loss: 0.897896, Accuracy: 90.73%\n",
      "Test Set - Loss: 0.840555, Accuracy: 90.73%\n",
      "Confusion Matrix:\n",
      "[[ 95   4  19   0]\n",
      " [  2 378  35  24]\n",
      " [  1  11 563   4]\n",
      " [  4  12  42 511]]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        print(f\"Test Batch {batch_idx+1}, Loss: {loss.item():.6f}, Accuracy: {100 * test_correct / test_total:.2f}%\")\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"Test Set - Loss: {test_loss:.6f}, Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "# Calculate percentages for each element\n",
    "cm_percentage = (cm / np.sum(cm)) * 100\n",
    "\n",
    "# Plot confusion matrix with count and percentages\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=['MILD', 'MODERATE', 'NORMAL', 'SEVERE'],\n",
    "            yticklabels=['MILD', 'MODERATE', 'NORMAL', 'SEVERE'],\n",
    "            ax=ax)\n",
    "\n",
    "# Annotate each box with count and percentage\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm[0])):\n",
    "        ax.text(j + 0.5, i + 0.5, f'\\n\\n{cm_percentage[i, j]:.2f}%',\n",
    "                ha='center', va='center', color='black')\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Epoch 1\n",
      "Batch 1, Loss: 0.841388, Accuracy: 89.06%\n",
      "Batch 2, Loss: 0.812646, Accuracy: 91.41%\n",
      "Batch 3, Loss: 0.808003, Accuracy: 92.19%\n",
      "Batch 4, Loss: 0.788760, Accuracy: 93.36%\n",
      "Batch 5, Loss: 0.817542, Accuracy: 93.12%\n",
      "Batch 6, Loss: 0.819458, Accuracy: 92.97%\n",
      "Batch 7, Loss: 0.780254, Accuracy: 93.53%\n",
      "Batch 8, Loss: 0.791852, Accuracy: 93.75%\n",
      "Batch 9, Loss: 0.805788, Accuracy: 93.75%\n",
      "Batch 10, Loss: 0.824610, Accuracy: 93.59%\n",
      "Batch 11, Loss: 0.925936, Accuracy: 92.61%\n",
      "Batch 12, Loss: 0.857345, Accuracy: 92.32%\n",
      "Batch 13, Loss: 0.851434, Accuracy: 92.19%\n",
      "Batch 14, Loss: 0.852566, Accuracy: 91.85%\n",
      "Batch 15, Loss: 0.857027, Accuracy: 91.67%\n",
      "Batch 16, Loss: 0.813333, Accuracy: 91.80%\n",
      "Batch 17, Loss: 0.856240, Accuracy: 91.64%\n",
      "Batch 18, Loss: 0.825944, Accuracy: 91.67%\n",
      "Batch 19, Loss: 0.838109, Accuracy: 91.61%\n",
      "Batch 20, Loss: 0.906427, Accuracy: 91.17%\n",
      "Batch 21, Loss: 0.861461, Accuracy: 91.07%\n",
      "Batch 22, Loss: 0.852954, Accuracy: 90.98%\n",
      "Batch 23, Loss: 0.854358, Accuracy: 90.96%\n",
      "Batch 24, Loss: 0.844977, Accuracy: 90.89%\n",
      "Batch 25, Loss: 0.887713, Accuracy: 90.62%\n",
      "Batch 26, Loss: 0.795232, Accuracy: 90.75%\n",
      "Batch 27, Loss: 0.796741, Accuracy: 90.91%\n",
      "Batch 28, Loss: 0.895544, Accuracy: 90.62%\n",
      "Batch 29, Loss: 0.813718, Accuracy: 90.73%\n",
      "Batch 30, Loss: 0.857729, Accuracy: 90.73%\n",
      "Batch 31, Loss: 0.825143, Accuracy: 90.78%\n",
      "Batch 32, Loss: 0.823412, Accuracy: 90.77%\n",
      "Batch 33, Loss: 0.829299, Accuracy: 90.77%\n",
      "Batch 34, Loss: 0.867982, Accuracy: 90.67%\n",
      "Batch 35, Loss: 0.828285, Accuracy: 90.71%\n",
      "Batch 36, Loss: 0.806995, Accuracy: 90.80%\n",
      "Batch 37, Loss: 0.825751, Accuracy: 90.79%\n",
      "Batch 38, Loss: 0.852929, Accuracy: 90.71%\n",
      "Batch 39, Loss: 0.837775, Accuracy: 90.71%\n",
      "Batch 40, Loss: 0.851162, Accuracy: 90.70%\n",
      "Batch 41, Loss: 0.876109, Accuracy: 90.62%\n",
      "Batch 42, Loss: 0.832687, Accuracy: 90.66%\n",
      "Batch 43, Loss: 0.835923, Accuracy: 90.66%\n",
      "Batch 44, Loss: 0.908452, Accuracy: 90.45%\n",
      "Batch 45, Loss: 0.859304, Accuracy: 90.45%\n",
      "Batch 46, Loss: 0.826511, Accuracy: 90.49%\n",
      "Batch 47, Loss: 0.873966, Accuracy: 90.43%\n",
      "Batch 48, Loss: 0.838830, Accuracy: 90.43%\n",
      "Batch 49, Loss: 0.819528, Accuracy: 90.47%\n",
      "Batch 50, Loss: 0.891327, Accuracy: 90.34%\n",
      "Batch 51, Loss: 0.818656, Accuracy: 90.41%\n",
      "Batch 52, Loss: 0.833664, Accuracy: 90.41%\n",
      "Batch 53, Loss: 0.888001, Accuracy: 90.33%\n",
      "Batch 54, Loss: 0.803130, Accuracy: 90.39%\n",
      "Batch 55, Loss: 0.859869, Accuracy: 90.37%\n",
      "Batch 56, Loss: 0.821713, Accuracy: 90.40%\n",
      "Batch 57, Loss: 0.824179, Accuracy: 90.43%\n",
      "Batch 58, Loss: 0.872362, Accuracy: 90.36%\n",
      "Batch 59, Loss: 0.823727, Accuracy: 90.39%\n",
      "Batch 60, Loss: 0.791427, Accuracy: 90.47%\n",
      "Batch 61, Loss: 0.802178, Accuracy: 90.55%\n",
      "Batch 62, Loss: 0.801441, Accuracy: 90.60%\n",
      "Batch 63, Loss: 0.865187, Accuracy: 90.53%\n",
      "Batch 64, Loss: 0.848893, Accuracy: 90.53%\n",
      "Batch 65, Loss: 0.869908, Accuracy: 90.50%\n",
      "Batch 66, Loss: 0.875094, Accuracy: 90.46%\n",
      "Batch 67, Loss: 0.899606, Accuracy: 90.37%\n",
      "Batch 68, Loss: 0.860738, Accuracy: 90.35%\n",
      "Batch 69, Loss: 0.861457, Accuracy: 90.31%\n",
      "Batch 70, Loss: 0.815560, Accuracy: 90.36%\n",
      "Batch 71, Loss: 0.779392, Accuracy: 90.45%\n",
      "Batch 72, Loss: 0.901596, Accuracy: 90.36%\n",
      "Batch 73, Loss: 0.899290, Accuracy: 90.28%\n",
      "Batch 74, Loss: 0.863118, Accuracy: 90.27%\n",
      "Batch 75, Loss: 0.813501, Accuracy: 90.31%\n",
      "Batch 76, Loss: 0.865074, Accuracy: 90.25%\n",
      "Batch 77, Loss: 0.795341, Accuracy: 90.32%\n",
      "Batch 78, Loss: 0.852687, Accuracy: 90.30%\n",
      "Batch 79, Loss: 0.871819, Accuracy: 90.27%\n",
      "Batch 80, Loss: 0.846156, Accuracy: 90.27%\n",
      "Batch 81, Loss: 0.873481, Accuracy: 90.22%\n",
      "Batch 82, Loss: 0.859502, Accuracy: 90.21%\n",
      "Batch 83, Loss: 0.852398, Accuracy: 90.19%\n",
      "Batch 84, Loss: 0.812291, Accuracy: 90.22%\n",
      "Batch 85, Loss: 0.878900, Accuracy: 90.17%\n",
      "Batch 86, Loss: 0.879964, Accuracy: 90.12%\n",
      "Batch 87, Loss: 0.861474, Accuracy: 90.09%\n",
      "Batch 88, Loss: 0.824671, Accuracy: 90.11%\n",
      "Batch 89, Loss: 0.808763, Accuracy: 90.15%\n",
      "Batch 90, Loss: 0.879261, Accuracy: 90.12%\n",
      "Batch 91, Loss: 0.852397, Accuracy: 90.11%\n",
      "Batch 92, Loss: 0.832355, Accuracy: 90.12%\n",
      "Batch 93, Loss: 0.891036, Accuracy: 90.05%\n",
      "Batch 94, Loss: 0.892932, Accuracy: 89.99%\n",
      "Batch 95, Loss: 0.810236, Accuracy: 90.03%\n",
      "Batch 96, Loss: 0.855740, Accuracy: 90.02%\n",
      "Batch 97, Loss: 0.856260, Accuracy: 90.00%\n",
      "Batch 98, Loss: 0.833533, Accuracy: 90.02%\n",
      "Batch 99, Loss: 0.859229, Accuracy: 90.01%\n",
      "Batch 100, Loss: 0.807888, Accuracy: 90.05%\n",
      "Batch 101, Loss: 0.857786, Accuracy: 90.02%\n",
      "Batch 102, Loss: 0.883002, Accuracy: 90.00%\n",
      "Batch 103, Loss: 0.863636, Accuracy: 89.97%\n",
      "Batch 104, Loss: 0.856239, Accuracy: 89.96%\n",
      "Batch 105, Loss: 0.807520, Accuracy: 90.00%\n",
      "Batch 106, Loss: 0.822960, Accuracy: 90.02%\n",
      "Batch 107, Loss: 0.873602, Accuracy: 90.00%\n",
      "Batch 108, Loss: 0.821626, Accuracy: 90.02%\n",
      "Batch 109, Loss: 0.846790, Accuracy: 90.01%\n",
      "Batch 110, Loss: 0.874291, Accuracy: 90.00%\n",
      "Batch 111, Loss: 0.816961, Accuracy: 90.02%\n",
      "Batch 112, Loss: 0.817613, Accuracy: 90.04%\n",
      "Batch 113, Loss: 0.774434, Accuracy: 90.10%\n",
      "Batch 114, Loss: 0.794280, Accuracy: 90.15%\n",
      "Batch 115, Loss: 0.838712, Accuracy: 90.15%\n",
      "Batch 116, Loss: 0.817848, Accuracy: 90.17%\n",
      "Batch 117, Loss: 0.786518, Accuracy: 90.22%\n",
      "Batch 118, Loss: 0.808340, Accuracy: 90.25%\n",
      "Batch 119, Loss: 0.803856, Accuracy: 90.30%\n",
      "Batch 120, Loss: 0.870686, Accuracy: 90.26%\n",
      "Batch 121, Loss: 0.841638, Accuracy: 90.26%\n",
      "Batch 122, Loss: 0.858093, Accuracy: 90.24%\n",
      "Batch 123, Loss: 0.835936, Accuracy: 90.24%\n",
      "Batch 124, Loss: 0.792232, Accuracy: 90.28%\n",
      "Batch 125, Loss: 0.841750, Accuracy: 90.29%\n",
      "Batch 126, Loss: 0.803672, Accuracy: 90.33%\n",
      "Batch 127, Loss: 0.857245, Accuracy: 90.31%\n",
      "Batch 128, Loss: 0.779062, Accuracy: 90.36%\n",
      "Batch 129, Loss: 0.842186, Accuracy: 90.36%\n",
      "Batch 130, Loss: 0.813329, Accuracy: 90.38%\n",
      "Batch 131, Loss: 0.910884, Accuracy: 90.33%\n",
      "Batch 132, Loss: 0.841326, Accuracy: 90.33%\n",
      "Batch 133, Loss: 0.902185, Accuracy: 90.27%\n",
      "Batch 134, Loss: 0.867681, Accuracy: 90.24%\n",
      "Batch 135, Loss: 0.866813, Accuracy: 90.22%\n",
      "Batch 136, Loss: 0.855270, Accuracy: 90.21%\n",
      "Batch 137, Loss: 0.819685, Accuracy: 90.24%\n",
      "Batch 138, Loss: 0.869274, Accuracy: 90.22%\n",
      "Batch 139, Loss: 0.832365, Accuracy: 90.22%\n",
      "Batch 140, Loss: 0.810140, Accuracy: 90.25%\n",
      "Batch 141, Loss: 0.826560, Accuracy: 90.26%\n",
      "Batch 142, Loss: 0.842857, Accuracy: 90.25%\n",
      "Batch 143, Loss: 0.840587, Accuracy: 90.25%\n",
      "Batch 144, Loss: 0.860625, Accuracy: 90.25%\n",
      "Batch 145, Loss: 0.811972, Accuracy: 90.27%\n",
      "Batch 146, Loss: 0.898231, Accuracy: 90.22%\n",
      "Batch 147, Loss: 0.876406, Accuracy: 90.20%\n",
      "Batch 148, Loss: 0.839433, Accuracy: 90.20%\n",
      "Batch 149, Loss: 0.827531, Accuracy: 90.22%\n",
      "Batch 150, Loss: 0.873424, Accuracy: 90.20%\n",
      "Batch 151, Loss: 0.823853, Accuracy: 90.21%\n",
      "Batch 152, Loss: 0.875009, Accuracy: 90.18%\n",
      "Batch 153, Loss: 0.816772, Accuracy: 90.20%\n",
      "Batch 154, Loss: 0.868905, Accuracy: 90.18%\n",
      "Batch 155, Loss: 0.837490, Accuracy: 90.18%\n",
      "Batch 156, Loss: 0.875006, Accuracy: 90.15%\n",
      "Batch 157, Loss: 0.859515, Accuracy: 90.15%\n",
      "Batch 158, Loss: 0.856185, Accuracy: 90.15%\n",
      "Batch 159, Loss: 0.845286, Accuracy: 90.15%\n",
      "Batch 160, Loss: 0.864839, Accuracy: 90.15%\n",
      "Batch 161, Loss: 0.827013, Accuracy: 90.16%\n",
      "Batch 162, Loss: 0.855518, Accuracy: 90.15%\n",
      "Batch 163, Loss: 0.838472, Accuracy: 90.16%\n",
      "Batch 164, Loss: 0.865986, Accuracy: 90.13%\n",
      "Batch 165, Loss: 0.855826, Accuracy: 90.12%\n",
      "Batch 166, Loss: 0.787355, Accuracy: 90.15%\n",
      "Batch 167, Loss: 0.791187, Accuracy: 90.19%\n",
      "Batch 168, Loss: 0.866257, Accuracy: 90.17%\n",
      "Batch 169, Loss: 0.879206, Accuracy: 90.15%\n",
      "Batch 170, Loss: 0.842621, Accuracy: 90.16%\n",
      "Batch 171, Loss: 0.830881, Accuracy: 90.16%\n",
      "Batch 172, Loss: 0.812317, Accuracy: 90.18%\n",
      "Batch 173, Loss: 0.846241, Accuracy: 90.17%\n",
      "Batch 174, Loss: 0.826326, Accuracy: 90.18%\n",
      "Batch 175, Loss: 0.800280, Accuracy: 90.21%\n",
      "Batch 176, Loss: 0.836414, Accuracy: 90.22%\n",
      "Batch 177, Loss: 0.846300, Accuracy: 90.21%\n",
      "Batch 178, Loss: 0.821674, Accuracy: 90.21%\n",
      "Batch 179, Loss: 0.933290, Accuracy: 90.16%\n",
      "Batch 180, Loss: 0.853019, Accuracy: 90.16%\n",
      "Batch 181, Loss: 0.829418, Accuracy: 90.18%\n",
      "Batch 182, Loss: 0.858177, Accuracy: 90.17%\n",
      "Batch 183, Loss: 0.843551, Accuracy: 90.18%\n",
      "Batch 184, Loss: 0.785786, Accuracy: 90.22%\n",
      "Batch 185, Loss: 0.883147, Accuracy: 90.19%\n",
      "Batch 186, Loss: 0.834495, Accuracy: 90.20%\n",
      "Batch 187, Loss: 0.856389, Accuracy: 90.20%\n",
      "Batch 188, Loss: 0.869885, Accuracy: 90.18%\n",
      "Batch 189, Loss: 0.868723, Accuracy: 90.17%\n",
      "Batch 190, Loss: 0.857125, Accuracy: 90.16%\n",
      "Batch 191, Loss: 0.861062, Accuracy: 90.15%\n",
      "Batch 192, Loss: 0.871125, Accuracy: 90.13%\n",
      "Batch 193, Loss: 0.833859, Accuracy: 90.13%\n",
      "Batch 194, Loss: 0.878739, Accuracy: 90.10%\n",
      "Batch 195, Loss: 0.834793, Accuracy: 90.11%\n",
      "Batch 196, Loss: 0.846477, Accuracy: 90.11%\n",
      "Batch 197, Loss: 0.853580, Accuracy: 90.11%\n",
      "Batch 198, Loss: 0.842113, Accuracy: 90.11%\n",
      "Batch 199, Loss: 0.864393, Accuracy: 90.11%\n",
      "Batch 200, Loss: 0.829067, Accuracy: 90.12%\n",
      "Batch 201, Loss: 0.875904, Accuracy: 90.10%\n",
      "Batch 202, Loss: 0.865286, Accuracy: 90.09%\n",
      "Batch 203, Loss: 0.825512, Accuracy: 90.10%\n",
      "Batch 204, Loss: 0.862406, Accuracy: 90.10%\n",
      "Batch 205, Loss: 0.890465, Accuracy: 90.08%\n",
      "Batch 206, Loss: 0.802665, Accuracy: 90.09%\n",
      "Batch 207, Loss: 0.782046, Accuracy: 90.13%\n",
      "Batch 208, Loss: 0.850428, Accuracy: 90.13%\n",
      "Batch 209, Loss: 0.853792, Accuracy: 90.12%\n",
      "Batch 210, Loss: 0.936576, Accuracy: 90.08%\n",
      "Batch 211, Loss: 0.810587, Accuracy: 90.10%\n",
      "Batch 212, Loss: 0.817086, Accuracy: 90.12%\n",
      "Batch 213, Loss: 0.847881, Accuracy: 90.12%\n",
      "Training - Epoch 1, Loss: 0.843804, Accuracy: 90.12%\n",
      "Validation Batch 1, Loss: 0.819970, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.791887, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.816243, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.835275, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.818784, Accuracy: 93.12%\n",
      "Validation Batch 6, Loss: 0.793411, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.794676, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.856021, Accuracy: 93.16%\n",
      "Validation Batch 9, Loss: 0.889367, Accuracy: 92.19%\n",
      "Validation Batch 10, Loss: 0.787641, Accuracy: 92.66%\n",
      "Validation Batch 11, Loss: 0.827405, Accuracy: 92.61%\n",
      "Validation Batch 12, Loss: 0.827225, Accuracy: 92.58%\n",
      "Validation Batch 13, Loss: 0.832732, Accuracy: 92.43%\n",
      "Validation Batch 14, Loss: 0.834028, Accuracy: 92.41%\n",
      "Validation Batch 15, Loss: 0.798826, Accuracy: 92.50%\n",
      "Validation Batch 16, Loss: 0.838624, Accuracy: 92.38%\n",
      "Validation Batch 17, Loss: 0.856990, Accuracy: 92.19%\n",
      "Validation Batch 18, Loss: 0.797590, Accuracy: 92.36%\n",
      "Validation Batch 19, Loss: 0.845698, Accuracy: 92.27%\n",
      "Validation Batch 20, Loss: 0.816843, Accuracy: 92.42%\n",
      "Validation Batch 21, Loss: 0.858803, Accuracy: 92.26%\n",
      "Validation Batch 22, Loss: 0.794836, Accuracy: 92.47%\n",
      "Validation Batch 23, Loss: 0.859233, Accuracy: 92.32%\n",
      "Validation Batch 24, Loss: 0.827753, Accuracy: 92.32%\n",
      "Validation Batch 25, Loss: 0.796313, Accuracy: 92.44%\n",
      "Validation Batch 26, Loss: 0.836339, Accuracy: 92.37%\n",
      "Validation Batch 27, Loss: 0.792749, Accuracy: 92.43%\n",
      "Validation - Epoch 1, Loss: 0.823899, Accuracy: 92.43%\n",
      "Patience—1\n",
      "Epoch 2\n",
      "Batch 1, Loss: 0.844201, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.869122, Accuracy: 89.06%\n",
      "Batch 3, Loss: 0.871739, Accuracy: 88.54%\n",
      "Batch 4, Loss: 0.845257, Accuracy: 88.67%\n",
      "Batch 5, Loss: 0.779464, Accuracy: 90.31%\n",
      "Batch 6, Loss: 0.845002, Accuracy: 89.84%\n",
      "Batch 7, Loss: 0.864114, Accuracy: 89.73%\n",
      "Batch 8, Loss: 0.833490, Accuracy: 89.84%\n",
      "Batch 9, Loss: 0.813491, Accuracy: 90.10%\n",
      "Batch 10, Loss: 0.849560, Accuracy: 90.00%\n",
      "Batch 11, Loss: 0.870210, Accuracy: 89.77%\n",
      "Batch 12, Loss: 0.765605, Accuracy: 90.62%\n",
      "Batch 13, Loss: 0.824405, Accuracy: 90.75%\n",
      "Batch 14, Loss: 0.778663, Accuracy: 91.18%\n",
      "Batch 15, Loss: 0.827230, Accuracy: 91.25%\n",
      "Batch 16, Loss: 0.815864, Accuracy: 91.41%\n",
      "Batch 17, Loss: 0.818217, Accuracy: 91.45%\n",
      "Batch 18, Loss: 0.852225, Accuracy: 91.32%\n",
      "Batch 19, Loss: 0.813615, Accuracy: 91.37%\n",
      "Batch 20, Loss: 0.794888, Accuracy: 91.56%\n",
      "Batch 21, Loss: 0.822004, Accuracy: 91.59%\n",
      "Batch 22, Loss: 0.814057, Accuracy: 91.69%\n",
      "Batch 23, Loss: 0.948962, Accuracy: 91.17%\n",
      "Batch 24, Loss: 0.858449, Accuracy: 91.08%\n",
      "Batch 25, Loss: 0.811269, Accuracy: 91.12%\n",
      "Batch 26, Loss: 0.816265, Accuracy: 91.23%\n",
      "Batch 27, Loss: 0.813479, Accuracy: 91.26%\n",
      "Batch 28, Loss: 0.806008, Accuracy: 91.35%\n",
      "Batch 29, Loss: 0.854144, Accuracy: 91.22%\n",
      "Batch 30, Loss: 0.926192, Accuracy: 90.94%\n",
      "Batch 31, Loss: 0.861533, Accuracy: 90.88%\n",
      "Batch 32, Loss: 0.866265, Accuracy: 90.82%\n",
      "Batch 33, Loss: 0.846451, Accuracy: 90.81%\n",
      "Batch 34, Loss: 0.864429, Accuracy: 90.72%\n",
      "Batch 35, Loss: 0.908159, Accuracy: 90.54%\n",
      "Batch 36, Loss: 0.830562, Accuracy: 90.58%\n",
      "Batch 37, Loss: 0.828921, Accuracy: 90.62%\n",
      "Batch 38, Loss: 0.812467, Accuracy: 90.67%\n",
      "Batch 39, Loss: 0.888107, Accuracy: 90.54%\n",
      "Batch 40, Loss: 0.855151, Accuracy: 90.47%\n",
      "Batch 41, Loss: 0.807510, Accuracy: 90.55%\n",
      "Batch 42, Loss: 0.799169, Accuracy: 90.70%\n",
      "Batch 43, Loss: 0.805134, Accuracy: 90.77%\n",
      "Batch 44, Loss: 0.928066, Accuracy: 90.59%\n",
      "Batch 45, Loss: 0.833431, Accuracy: 90.62%\n",
      "Batch 46, Loss: 0.798866, Accuracy: 90.73%\n",
      "Batch 47, Loss: 0.872424, Accuracy: 90.62%\n",
      "Batch 48, Loss: 0.842318, Accuracy: 90.62%\n",
      "Batch 49, Loss: 0.826695, Accuracy: 90.66%\n",
      "Batch 50, Loss: 0.814558, Accuracy: 90.75%\n",
      "Batch 51, Loss: 0.871064, Accuracy: 90.69%\n",
      "Batch 52, Loss: 0.837585, Accuracy: 90.72%\n",
      "Batch 53, Loss: 0.829066, Accuracy: 90.68%\n",
      "Batch 54, Loss: 0.859839, Accuracy: 90.62%\n",
      "Batch 55, Loss: 0.811725, Accuracy: 90.65%\n",
      "Batch 56, Loss: 0.912995, Accuracy: 90.54%\n",
      "Batch 57, Loss: 0.827926, Accuracy: 90.60%\n",
      "Batch 58, Loss: 0.892873, Accuracy: 90.49%\n",
      "Batch 59, Loss: 0.894264, Accuracy: 90.39%\n",
      "Batch 60, Loss: 0.844262, Accuracy: 90.39%\n",
      "Batch 61, Loss: 0.901228, Accuracy: 90.29%\n",
      "Batch 62, Loss: 0.838065, Accuracy: 90.32%\n",
      "Batch 63, Loss: 0.885065, Accuracy: 90.25%\n",
      "Batch 64, Loss: 0.856523, Accuracy: 90.23%\n",
      "Batch 65, Loss: 0.862360, Accuracy: 90.22%\n",
      "Batch 66, Loss: 0.826747, Accuracy: 90.27%\n",
      "Batch 67, Loss: 0.856460, Accuracy: 90.25%\n",
      "Batch 68, Loss: 0.793694, Accuracy: 90.30%\n",
      "Batch 69, Loss: 0.826242, Accuracy: 90.31%\n",
      "Batch 70, Loss: 0.858807, Accuracy: 90.29%\n",
      "Batch 71, Loss: 0.859462, Accuracy: 90.25%\n",
      "Batch 72, Loss: 0.850269, Accuracy: 90.26%\n",
      "Batch 73, Loss: 0.923023, Accuracy: 90.15%\n",
      "Batch 74, Loss: 0.864995, Accuracy: 90.12%\n",
      "Batch 75, Loss: 0.831389, Accuracy: 90.15%\n",
      "Batch 76, Loss: 0.846809, Accuracy: 90.13%\n",
      "Batch 77, Loss: 0.811158, Accuracy: 90.18%\n",
      "Batch 78, Loss: 0.842289, Accuracy: 90.20%\n",
      "Batch 79, Loss: 0.833781, Accuracy: 90.23%\n",
      "Batch 80, Loss: 0.889874, Accuracy: 90.18%\n",
      "Batch 81, Loss: 0.856749, Accuracy: 90.18%\n",
      "Batch 82, Loss: 0.868192, Accuracy: 90.15%\n",
      "Batch 83, Loss: 0.857904, Accuracy: 90.14%\n",
      "Batch 84, Loss: 0.860708, Accuracy: 90.12%\n",
      "Batch 85, Loss: 0.814609, Accuracy: 90.17%\n",
      "Batch 86, Loss: 0.846426, Accuracy: 90.15%\n",
      "Batch 87, Loss: 0.837220, Accuracy: 90.18%\n",
      "Batch 88, Loss: 0.841659, Accuracy: 90.18%\n",
      "Batch 89, Loss: 0.867266, Accuracy: 90.13%\n",
      "Batch 90, Loss: 0.860286, Accuracy: 90.12%\n",
      "Batch 91, Loss: 0.814818, Accuracy: 90.16%\n",
      "Batch 92, Loss: 0.871943, Accuracy: 90.13%\n",
      "Batch 93, Loss: 0.826242, Accuracy: 90.15%\n",
      "Batch 94, Loss: 0.841202, Accuracy: 90.16%\n",
      "Batch 95, Loss: 0.818528, Accuracy: 90.18%\n",
      "Batch 96, Loss: 0.802355, Accuracy: 90.23%\n",
      "Batch 97, Loss: 0.866663, Accuracy: 90.21%\n",
      "Batch 98, Loss: 0.840683, Accuracy: 90.21%\n",
      "Batch 99, Loss: 0.864631, Accuracy: 90.18%\n",
      "Batch 100, Loss: 0.856738, Accuracy: 90.17%\n",
      "Batch 101, Loss: 0.817033, Accuracy: 90.19%\n",
      "Batch 102, Loss: 0.801070, Accuracy: 90.23%\n",
      "Batch 103, Loss: 0.823032, Accuracy: 90.25%\n",
      "Batch 104, Loss: 0.878184, Accuracy: 90.20%\n",
      "Batch 105, Loss: 0.795160, Accuracy: 90.24%\n",
      "Batch 106, Loss: 0.818942, Accuracy: 90.27%\n",
      "Batch 107, Loss: 0.825541, Accuracy: 90.30%\n",
      "Batch 108, Loss: 0.968747, Accuracy: 90.18%\n",
      "Batch 109, Loss: 0.890443, Accuracy: 90.14%\n",
      "Batch 110, Loss: 0.845761, Accuracy: 90.13%\n",
      "Batch 111, Loss: 0.814384, Accuracy: 90.16%\n",
      "Batch 112, Loss: 0.852303, Accuracy: 90.15%\n",
      "Batch 113, Loss: 0.908736, Accuracy: 90.09%\n",
      "Batch 114, Loss: 0.837500, Accuracy: 90.09%\n",
      "Batch 115, Loss: 0.829876, Accuracy: 90.11%\n",
      "Batch 116, Loss: 0.833415, Accuracy: 90.13%\n",
      "Batch 117, Loss: 0.832935, Accuracy: 90.13%\n",
      "Batch 118, Loss: 0.834904, Accuracy: 90.14%\n",
      "Batch 119, Loss: 0.802148, Accuracy: 90.18%\n",
      "Batch 120, Loss: 0.751873, Accuracy: 90.26%\n",
      "Batch 121, Loss: 0.806325, Accuracy: 90.29%\n",
      "Batch 122, Loss: 0.889620, Accuracy: 90.24%\n",
      "Batch 123, Loss: 0.774845, Accuracy: 90.31%\n",
      "Batch 124, Loss: 0.839833, Accuracy: 90.30%\n",
      "Batch 125, Loss: 0.777498, Accuracy: 90.36%\n",
      "Batch 126, Loss: 0.861138, Accuracy: 90.35%\n",
      "Batch 127, Loss: 0.881506, Accuracy: 90.33%\n",
      "Batch 128, Loss: 0.811408, Accuracy: 90.36%\n",
      "Batch 129, Loss: 0.857037, Accuracy: 90.35%\n",
      "Batch 130, Loss: 0.790888, Accuracy: 90.38%\n",
      "Batch 131, Loss: 0.854128, Accuracy: 90.37%\n",
      "Batch 132, Loss: 0.861845, Accuracy: 90.36%\n",
      "Batch 133, Loss: 0.799298, Accuracy: 90.39%\n",
      "Batch 134, Loss: 0.842714, Accuracy: 90.39%\n",
      "Batch 135, Loss: 0.969294, Accuracy: 90.28%\n",
      "Batch 136, Loss: 0.856127, Accuracy: 90.26%\n",
      "Batch 137, Loss: 0.800467, Accuracy: 90.31%\n",
      "Batch 138, Loss: 0.828785, Accuracy: 90.32%\n",
      "Batch 139, Loss: 0.824868, Accuracy: 90.33%\n",
      "Batch 140, Loss: 0.828206, Accuracy: 90.33%\n",
      "Batch 141, Loss: 0.830698, Accuracy: 90.34%\n",
      "Batch 142, Loss: 0.803143, Accuracy: 90.36%\n",
      "Batch 143, Loss: 0.929839, Accuracy: 90.29%\n",
      "Batch 144, Loss: 0.910087, Accuracy: 90.23%\n",
      "Batch 145, Loss: 0.846978, Accuracy: 90.22%\n",
      "Batch 146, Loss: 0.900345, Accuracy: 90.16%\n",
      "Batch 147, Loss: 0.826699, Accuracy: 90.18%\n",
      "Batch 148, Loss: 0.872656, Accuracy: 90.16%\n",
      "Batch 149, Loss: 0.894993, Accuracy: 90.12%\n",
      "Batch 150, Loss: 0.870178, Accuracy: 90.10%\n",
      "Batch 151, Loss: 0.791861, Accuracy: 90.15%\n",
      "Batch 152, Loss: 0.891041, Accuracy: 90.11%\n",
      "Batch 153, Loss: 0.876761, Accuracy: 90.08%\n",
      "Batch 154, Loss: 0.850304, Accuracy: 90.08%\n",
      "Batch 155, Loss: 0.825005, Accuracy: 90.09%\n",
      "Batch 156, Loss: 0.885202, Accuracy: 90.05%\n",
      "Batch 157, Loss: 0.856395, Accuracy: 90.05%\n",
      "Batch 158, Loss: 0.818783, Accuracy: 90.06%\n",
      "Batch 159, Loss: 0.818211, Accuracy: 90.08%\n",
      "Batch 160, Loss: 0.828936, Accuracy: 90.10%\n",
      "Batch 161, Loss: 0.844866, Accuracy: 90.10%\n",
      "Batch 162, Loss: 0.860344, Accuracy: 90.09%\n",
      "Batch 163, Loss: 0.805619, Accuracy: 90.12%\n",
      "Batch 164, Loss: 0.835926, Accuracy: 90.13%\n",
      "Batch 165, Loss: 0.800401, Accuracy: 90.15%\n",
      "Batch 166, Loss: 0.818866, Accuracy: 90.17%\n",
      "Batch 167, Loss: 0.881120, Accuracy: 90.16%\n",
      "Batch 168, Loss: 0.836252, Accuracy: 90.16%\n",
      "Batch 169, Loss: 0.792100, Accuracy: 90.19%\n",
      "Batch 170, Loss: 0.844761, Accuracy: 90.19%\n",
      "Batch 171, Loss: 0.814870, Accuracy: 90.22%\n",
      "Batch 172, Loss: 0.829002, Accuracy: 90.23%\n",
      "Batch 173, Loss: 0.843614, Accuracy: 90.21%\n",
      "Batch 174, Loss: 0.832708, Accuracy: 90.22%\n",
      "Batch 175, Loss: 0.865511, Accuracy: 90.21%\n",
      "Batch 176, Loss: 0.874281, Accuracy: 90.19%\n",
      "Batch 177, Loss: 0.865100, Accuracy: 90.17%\n",
      "Batch 178, Loss: 0.821347, Accuracy: 90.19%\n",
      "Batch 179, Loss: 0.844189, Accuracy: 90.19%\n",
      "Batch 180, Loss: 0.808962, Accuracy: 90.21%\n",
      "Batch 181, Loss: 0.877385, Accuracy: 90.18%\n",
      "Batch 182, Loss: 0.853994, Accuracy: 90.19%\n",
      "Batch 183, Loss: 0.890322, Accuracy: 90.15%\n",
      "Batch 184, Loss: 0.856895, Accuracy: 90.14%\n",
      "Batch 185, Loss: 0.874835, Accuracy: 90.12%\n",
      "Batch 186, Loss: 0.859749, Accuracy: 90.10%\n",
      "Batch 187, Loss: 0.872379, Accuracy: 90.08%\n",
      "Batch 188, Loss: 0.834128, Accuracy: 90.09%\n",
      "Batch 189, Loss: 0.860799, Accuracy: 90.08%\n",
      "Batch 190, Loss: 0.856826, Accuracy: 90.07%\n",
      "Batch 191, Loss: 0.910533, Accuracy: 90.05%\n",
      "Batch 192, Loss: 0.846075, Accuracy: 90.06%\n",
      "Batch 193, Loss: 0.800841, Accuracy: 90.08%\n",
      "Batch 194, Loss: 0.866787, Accuracy: 90.06%\n",
      "Batch 195, Loss: 0.780801, Accuracy: 90.10%\n",
      "Batch 196, Loss: 0.772082, Accuracy: 90.14%\n",
      "Batch 197, Loss: 0.858727, Accuracy: 90.13%\n",
      "Batch 198, Loss: 0.874635, Accuracy: 90.11%\n",
      "Batch 199, Loss: 0.862531, Accuracy: 90.10%\n",
      "Batch 200, Loss: 0.877858, Accuracy: 90.08%\n",
      "Batch 201, Loss: 0.812338, Accuracy: 90.09%\n",
      "Batch 202, Loss: 0.827241, Accuracy: 90.10%\n",
      "Batch 203, Loss: 0.934184, Accuracy: 90.05%\n",
      "Batch 204, Loss: 0.808543, Accuracy: 90.07%\n",
      "Batch 205, Loss: 0.887675, Accuracy: 90.03%\n",
      "Batch 206, Loss: 0.846436, Accuracy: 90.03%\n",
      "Batch 207, Loss: 0.879725, Accuracy: 90.01%\n",
      "Batch 208, Loss: 0.862777, Accuracy: 90.01%\n",
      "Batch 209, Loss: 0.851660, Accuracy: 90.00%\n",
      "Batch 210, Loss: 0.892988, Accuracy: 89.99%\n",
      "Batch 211, Loss: 0.906412, Accuracy: 89.96%\n",
      "Batch 212, Loss: 0.847391, Accuracy: 89.95%\n",
      "Batch 213, Loss: 0.872049, Accuracy: 89.93%\n",
      "Training - Epoch 2, Loss: 0.845894, Accuracy: 89.93%\n",
      "Validation Batch 1, Loss: 0.819936, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.792631, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.815881, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.834871, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.817536, Accuracy: 93.12%\n",
      "Validation Batch 6, Loss: 0.792821, Accuracy: 93.49%\n",
      "Validation Batch 7, Loss: 0.794828, Accuracy: 93.75%\n",
      "Validation Batch 8, Loss: 0.855402, Accuracy: 93.36%\n",
      "Validation Batch 9, Loss: 0.887430, Accuracy: 92.36%\n",
      "Validation Batch 10, Loss: 0.787499, Accuracy: 92.81%\n",
      "Validation Batch 11, Loss: 0.826451, Accuracy: 92.76%\n",
      "Validation Batch 12, Loss: 0.827034, Accuracy: 92.71%\n",
      "Validation Batch 13, Loss: 0.833927, Accuracy: 92.55%\n",
      "Validation Batch 14, Loss: 0.833644, Accuracy: 92.52%\n",
      "Validation Batch 15, Loss: 0.797806, Accuracy: 92.60%\n",
      "Validation Batch 16, Loss: 0.838009, Accuracy: 92.48%\n",
      "Validation Batch 17, Loss: 0.855721, Accuracy: 92.28%\n",
      "Validation Batch 18, Loss: 0.797904, Accuracy: 92.45%\n",
      "Validation Batch 19, Loss: 0.846624, Accuracy: 92.35%\n",
      "Validation Batch 20, Loss: 0.816272, Accuracy: 92.50%\n",
      "Validation Batch 21, Loss: 0.859309, Accuracy: 92.34%\n",
      "Validation Batch 22, Loss: 0.794761, Accuracy: 92.54%\n",
      "Validation Batch 23, Loss: 0.859803, Accuracy: 92.39%\n",
      "Validation Batch 24, Loss: 0.827492, Accuracy: 92.38%\n",
      "Validation Batch 25, Loss: 0.795650, Accuracy: 92.50%\n",
      "Validation Batch 26, Loss: 0.836403, Accuracy: 92.43%\n",
      "Validation Batch 27, Loss: 0.792632, Accuracy: 92.48%\n",
      "Validation - Epoch 2, Loss: 0.823640, Accuracy: 92.48%\n",
      "Patience—2\n",
      "Epoch 3\n",
      "Batch 1, Loss: 0.910708, Accuracy: 84.38%\n",
      "Batch 2, Loss: 0.841194, Accuracy: 87.50%\n",
      "Batch 3, Loss: 0.848703, Accuracy: 88.02%\n",
      "Batch 4, Loss: 0.822210, Accuracy: 89.06%\n",
      "Batch 5, Loss: 0.862675, Accuracy: 89.06%\n",
      "Batch 6, Loss: 0.890121, Accuracy: 88.54%\n",
      "Batch 7, Loss: 0.848978, Accuracy: 88.39%\n",
      "Batch 8, Loss: 0.841168, Accuracy: 88.67%\n",
      "Batch 9, Loss: 0.852073, Accuracy: 88.54%\n",
      "Batch 10, Loss: 0.783550, Accuracy: 89.38%\n",
      "Batch 11, Loss: 0.872324, Accuracy: 89.06%\n",
      "Batch 12, Loss: 0.783775, Accuracy: 89.71%\n",
      "Batch 13, Loss: 0.836919, Accuracy: 89.90%\n",
      "Batch 14, Loss: 0.861959, Accuracy: 89.84%\n",
      "Batch 15, Loss: 0.871240, Accuracy: 89.69%\n",
      "Batch 16, Loss: 0.802426, Accuracy: 90.04%\n",
      "Batch 17, Loss: 0.833811, Accuracy: 89.98%\n",
      "Batch 18, Loss: 0.886880, Accuracy: 89.76%\n",
      "Batch 19, Loss: 0.881064, Accuracy: 89.56%\n",
      "Batch 20, Loss: 0.813752, Accuracy: 89.69%\n",
      "Batch 21, Loss: 0.870191, Accuracy: 89.51%\n",
      "Batch 22, Loss: 0.854251, Accuracy: 89.56%\n",
      "Batch 23, Loss: 0.830914, Accuracy: 89.61%\n",
      "Batch 24, Loss: 0.827077, Accuracy: 89.65%\n",
      "Batch 25, Loss: 0.838219, Accuracy: 89.69%\n",
      "Batch 26, Loss: 0.817999, Accuracy: 89.84%\n",
      "Batch 27, Loss: 0.831286, Accuracy: 89.99%\n",
      "Batch 28, Loss: 0.835824, Accuracy: 89.96%\n",
      "Batch 29, Loss: 0.824435, Accuracy: 90.03%\n",
      "Batch 30, Loss: 0.792096, Accuracy: 90.21%\n",
      "Batch 31, Loss: 0.854984, Accuracy: 90.22%\n",
      "Batch 32, Loss: 0.823671, Accuracy: 90.23%\n",
      "Batch 33, Loss: 0.837636, Accuracy: 90.29%\n",
      "Batch 34, Loss: 0.813928, Accuracy: 90.40%\n",
      "Batch 35, Loss: 0.815366, Accuracy: 90.49%\n",
      "Batch 36, Loss: 0.796340, Accuracy: 90.58%\n",
      "Batch 37, Loss: 0.823289, Accuracy: 90.67%\n",
      "Batch 38, Loss: 0.899700, Accuracy: 90.50%\n",
      "Batch 39, Loss: 0.859682, Accuracy: 90.42%\n",
      "Batch 40, Loss: 0.887092, Accuracy: 90.27%\n",
      "Batch 41, Loss: 0.853726, Accuracy: 90.21%\n",
      "Batch 42, Loss: 0.855143, Accuracy: 90.18%\n",
      "Batch 43, Loss: 0.873046, Accuracy: 90.15%\n",
      "Batch 44, Loss: 0.812500, Accuracy: 90.20%\n",
      "Batch 45, Loss: 0.936153, Accuracy: 89.97%\n",
      "Batch 46, Loss: 0.800662, Accuracy: 90.05%\n",
      "Batch 47, Loss: 0.893875, Accuracy: 89.96%\n",
      "Batch 48, Loss: 0.842903, Accuracy: 89.97%\n",
      "Batch 49, Loss: 0.827573, Accuracy: 90.02%\n",
      "Batch 50, Loss: 0.877695, Accuracy: 89.97%\n",
      "Batch 51, Loss: 0.849404, Accuracy: 89.95%\n",
      "Batch 52, Loss: 0.800214, Accuracy: 89.99%\n",
      "Batch 53, Loss: 0.833355, Accuracy: 90.04%\n",
      "Batch 54, Loss: 0.843375, Accuracy: 90.02%\n",
      "Batch 55, Loss: 0.867366, Accuracy: 89.94%\n",
      "Batch 56, Loss: 0.813796, Accuracy: 89.98%\n",
      "Batch 57, Loss: 0.878257, Accuracy: 89.88%\n",
      "Batch 58, Loss: 0.855241, Accuracy: 89.87%\n",
      "Batch 59, Loss: 0.839670, Accuracy: 89.88%\n",
      "Batch 60, Loss: 0.818895, Accuracy: 89.95%\n",
      "Batch 61, Loss: 0.826950, Accuracy: 89.96%\n",
      "Batch 62, Loss: 0.813803, Accuracy: 89.99%\n",
      "Batch 63, Loss: 0.859555, Accuracy: 89.98%\n",
      "Batch 64, Loss: 0.878079, Accuracy: 89.92%\n",
      "Batch 65, Loss: 0.862978, Accuracy: 89.88%\n",
      "Batch 66, Loss: 0.921749, Accuracy: 89.77%\n",
      "Batch 67, Loss: 0.813525, Accuracy: 89.83%\n",
      "Batch 68, Loss: 0.864589, Accuracy: 89.80%\n",
      "Batch 69, Loss: 0.864168, Accuracy: 89.76%\n",
      "Batch 70, Loss: 0.851879, Accuracy: 89.75%\n",
      "Batch 71, Loss: 0.815313, Accuracy: 89.81%\n",
      "Batch 72, Loss: 0.849266, Accuracy: 89.80%\n",
      "Batch 73, Loss: 0.884983, Accuracy: 89.73%\n",
      "Batch 74, Loss: 0.802411, Accuracy: 89.78%\n",
      "Batch 75, Loss: 0.864870, Accuracy: 89.75%\n",
      "Batch 76, Loss: 0.907678, Accuracy: 89.68%\n",
      "Batch 77, Loss: 0.797509, Accuracy: 89.75%\n",
      "Batch 78, Loss: 0.809652, Accuracy: 89.80%\n",
      "Batch 79, Loss: 0.821908, Accuracy: 89.83%\n",
      "Batch 80, Loss: 0.877102, Accuracy: 89.79%\n",
      "Batch 81, Loss: 0.857828, Accuracy: 89.78%\n",
      "Batch 82, Loss: 0.889010, Accuracy: 89.69%\n",
      "Batch 83, Loss: 0.821938, Accuracy: 89.72%\n",
      "Batch 84, Loss: 0.817742, Accuracy: 89.77%\n",
      "Batch 85, Loss: 0.906315, Accuracy: 89.69%\n",
      "Batch 86, Loss: 0.854691, Accuracy: 89.68%\n",
      "Batch 87, Loss: 0.830236, Accuracy: 89.71%\n",
      "Batch 88, Loss: 0.844843, Accuracy: 89.72%\n",
      "Batch 89, Loss: 0.835234, Accuracy: 89.75%\n",
      "Batch 90, Loss: 0.900840, Accuracy: 89.67%\n",
      "Batch 91, Loss: 0.797112, Accuracy: 89.71%\n",
      "Batch 92, Loss: 0.823131, Accuracy: 89.74%\n",
      "Batch 93, Loss: 0.858532, Accuracy: 89.72%\n",
      "Batch 94, Loss: 0.834973, Accuracy: 89.74%\n",
      "Batch 95, Loss: 0.869123, Accuracy: 89.72%\n",
      "Batch 96, Loss: 0.894592, Accuracy: 89.66%\n",
      "Batch 97, Loss: 0.803910, Accuracy: 89.71%\n",
      "Batch 98, Loss: 0.808231, Accuracy: 89.75%\n",
      "Batch 99, Loss: 0.863398, Accuracy: 89.74%\n",
      "Batch 100, Loss: 0.812998, Accuracy: 89.77%\n",
      "Batch 101, Loss: 0.819052, Accuracy: 89.79%\n",
      "Batch 102, Loss: 0.842675, Accuracy: 89.78%\n",
      "Batch 103, Loss: 0.872423, Accuracy: 89.76%\n",
      "Batch 104, Loss: 0.808541, Accuracy: 89.80%\n",
      "Batch 105, Loss: 0.861585, Accuracy: 89.79%\n",
      "Batch 106, Loss: 0.852256, Accuracy: 89.78%\n",
      "Batch 107, Loss: 0.796855, Accuracy: 89.84%\n",
      "Batch 108, Loss: 0.859965, Accuracy: 89.81%\n",
      "Batch 109, Loss: 0.824638, Accuracy: 89.84%\n",
      "Batch 110, Loss: 0.852039, Accuracy: 89.84%\n",
      "Batch 111, Loss: 0.830875, Accuracy: 89.85%\n",
      "Batch 112, Loss: 0.833933, Accuracy: 89.86%\n",
      "Batch 113, Loss: 0.827694, Accuracy: 89.86%\n",
      "Batch 114, Loss: 0.869710, Accuracy: 89.84%\n",
      "Batch 115, Loss: 0.802074, Accuracy: 89.89%\n",
      "Batch 116, Loss: 0.885681, Accuracy: 89.84%\n",
      "Batch 117, Loss: 0.822039, Accuracy: 89.85%\n",
      "Batch 118, Loss: 0.785081, Accuracy: 89.91%\n",
      "Batch 119, Loss: 0.865233, Accuracy: 89.90%\n",
      "Batch 120, Loss: 0.903233, Accuracy: 89.84%\n",
      "Batch 121, Loss: 0.888541, Accuracy: 89.80%\n",
      "Batch 122, Loss: 0.987060, Accuracy: 89.68%\n",
      "Batch 123, Loss: 0.852347, Accuracy: 89.67%\n",
      "Batch 124, Loss: 0.828375, Accuracy: 89.68%\n",
      "Batch 125, Loss: 0.828609, Accuracy: 89.70%\n",
      "Batch 126, Loss: 0.868136, Accuracy: 89.68%\n",
      "Batch 127, Loss: 0.874277, Accuracy: 89.67%\n",
      "Batch 128, Loss: 0.833199, Accuracy: 89.67%\n",
      "Batch 129, Loss: 0.867995, Accuracy: 89.67%\n",
      "Batch 130, Loss: 0.818283, Accuracy: 89.70%\n",
      "Batch 131, Loss: 0.854757, Accuracy: 89.69%\n",
      "Batch 132, Loss: 0.833302, Accuracy: 89.70%\n",
      "Batch 133, Loss: 0.844436, Accuracy: 89.70%\n",
      "Batch 134, Loss: 0.824109, Accuracy: 89.72%\n",
      "Batch 135, Loss: 0.798503, Accuracy: 89.76%\n",
      "Batch 136, Loss: 0.801639, Accuracy: 89.80%\n",
      "Batch 137, Loss: 0.878172, Accuracy: 89.77%\n",
      "Batch 138, Loss: 0.828778, Accuracy: 89.78%\n",
      "Batch 139, Loss: 0.838750, Accuracy: 89.77%\n",
      "Batch 140, Loss: 0.863898, Accuracy: 89.75%\n",
      "Batch 141, Loss: 0.854214, Accuracy: 89.75%\n",
      "Batch 142, Loss: 0.861805, Accuracy: 89.73%\n",
      "Batch 143, Loss: 0.823478, Accuracy: 89.75%\n",
      "Batch 144, Loss: 0.846510, Accuracy: 89.76%\n",
      "Batch 145, Loss: 0.891772, Accuracy: 89.72%\n",
      "Batch 146, Loss: 0.799735, Accuracy: 89.74%\n",
      "Batch 147, Loss: 0.796411, Accuracy: 89.77%\n",
      "Batch 148, Loss: 0.774361, Accuracy: 89.83%\n",
      "Batch 149, Loss: 0.814607, Accuracy: 89.85%\n",
      "Batch 150, Loss: 0.811288, Accuracy: 89.90%\n",
      "Batch 151, Loss: 0.899805, Accuracy: 89.85%\n",
      "Batch 152, Loss: 0.812569, Accuracy: 89.87%\n",
      "Batch 153, Loss: 0.833352, Accuracy: 89.88%\n",
      "Batch 154, Loss: 0.797367, Accuracy: 89.91%\n",
      "Batch 155, Loss: 0.810170, Accuracy: 89.93%\n",
      "Batch 156, Loss: 0.837008, Accuracy: 89.93%\n",
      "Batch 157, Loss: 0.867737, Accuracy: 89.91%\n",
      "Batch 158, Loss: 0.840651, Accuracy: 89.91%\n",
      "Batch 159, Loss: 0.796065, Accuracy: 89.95%\n",
      "Batch 160, Loss: 0.856110, Accuracy: 89.94%\n",
      "Batch 161, Loss: 0.802250, Accuracy: 89.97%\n",
      "Batch 162, Loss: 0.817273, Accuracy: 89.99%\n",
      "Batch 163, Loss: 0.823089, Accuracy: 90.00%\n",
      "Batch 164, Loss: 0.782335, Accuracy: 90.04%\n",
      "Batch 165, Loss: 0.878316, Accuracy: 90.02%\n",
      "Batch 166, Loss: 0.857135, Accuracy: 90.01%\n",
      "Batch 167, Loss: 0.840203, Accuracy: 90.02%\n",
      "Batch 168, Loss: 0.885794, Accuracy: 89.99%\n",
      "Batch 169, Loss: 0.867956, Accuracy: 89.98%\n",
      "Batch 170, Loss: 0.801460, Accuracy: 90.00%\n",
      "Batch 171, Loss: 0.908757, Accuracy: 89.96%\n",
      "Batch 172, Loss: 0.843732, Accuracy: 89.96%\n",
      "Batch 173, Loss: 0.875821, Accuracy: 89.95%\n",
      "Batch 174, Loss: 0.879769, Accuracy: 89.92%\n",
      "Batch 175, Loss: 0.809197, Accuracy: 89.95%\n",
      "Batch 176, Loss: 0.864864, Accuracy: 89.93%\n",
      "Batch 177, Loss: 0.828203, Accuracy: 89.95%\n",
      "Batch 178, Loss: 0.827004, Accuracy: 89.96%\n",
      "Batch 179, Loss: 0.907620, Accuracy: 89.92%\n",
      "Batch 180, Loss: 0.864793, Accuracy: 89.90%\n",
      "Batch 181, Loss: 0.954238, Accuracy: 89.84%\n",
      "Batch 182, Loss: 0.860111, Accuracy: 89.83%\n",
      "Batch 183, Loss: 0.880353, Accuracy: 89.81%\n",
      "Batch 184, Loss: 0.890206, Accuracy: 89.78%\n",
      "Batch 185, Loss: 0.808416, Accuracy: 89.81%\n",
      "Batch 186, Loss: 0.887813, Accuracy: 89.79%\n",
      "Batch 187, Loss: 0.893128, Accuracy: 89.76%\n",
      "Batch 188, Loss: 0.819903, Accuracy: 89.78%\n",
      "Batch 189, Loss: 0.812516, Accuracy: 89.81%\n",
      "Batch 190, Loss: 0.886548, Accuracy: 89.79%\n",
      "Batch 191, Loss: 0.910737, Accuracy: 89.75%\n",
      "Batch 192, Loss: 0.806997, Accuracy: 89.76%\n",
      "Batch 193, Loss: 0.882629, Accuracy: 89.75%\n",
      "Batch 194, Loss: 0.863412, Accuracy: 89.75%\n",
      "Batch 195, Loss: 0.785769, Accuracy: 89.78%\n",
      "Batch 196, Loss: 0.879768, Accuracy: 89.76%\n",
      "Batch 197, Loss: 0.895593, Accuracy: 89.74%\n",
      "Batch 198, Loss: 0.810435, Accuracy: 89.76%\n",
      "Batch 199, Loss: 0.865324, Accuracy: 89.75%\n",
      "Batch 200, Loss: 0.845149, Accuracy: 89.74%\n",
      "Batch 201, Loss: 0.863254, Accuracy: 89.74%\n",
      "Batch 202, Loss: 0.916391, Accuracy: 89.70%\n",
      "Batch 203, Loss: 0.772224, Accuracy: 89.75%\n",
      "Batch 204, Loss: 0.817639, Accuracy: 89.75%\n",
      "Batch 205, Loss: 0.835369, Accuracy: 89.76%\n",
      "Batch 206, Loss: 0.824687, Accuracy: 89.78%\n",
      "Batch 207, Loss: 0.843589, Accuracy: 89.78%\n",
      "Batch 208, Loss: 0.802106, Accuracy: 89.81%\n",
      "Batch 209, Loss: 0.799830, Accuracy: 89.83%\n",
      "Batch 210, Loss: 0.811872, Accuracy: 89.85%\n",
      "Batch 211, Loss: 0.933984, Accuracy: 89.81%\n",
      "Batch 212, Loss: 0.857199, Accuracy: 89.81%\n",
      "Batch 213, Loss: 0.810408, Accuracy: 89.82%\n",
      "Training - Epoch 3, Loss: 0.845596, Accuracy: 89.82%\n",
      "Validation Batch 1, Loss: 0.819768, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.791607, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.816229, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.835360, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.817663, Accuracy: 93.12%\n",
      "Validation Batch 6, Loss: 0.792990, Accuracy: 93.49%\n",
      "Validation Batch 7, Loss: 0.794858, Accuracy: 93.75%\n",
      "Validation Batch 8, Loss: 0.855873, Accuracy: 93.36%\n",
      "Validation Batch 9, Loss: 0.888274, Accuracy: 92.36%\n",
      "Validation Batch 10, Loss: 0.787615, Accuracy: 92.81%\n",
      "Validation Batch 11, Loss: 0.827012, Accuracy: 92.76%\n",
      "Validation Batch 12, Loss: 0.827040, Accuracy: 92.71%\n",
      "Validation Batch 13, Loss: 0.833209, Accuracy: 92.55%\n",
      "Validation Batch 14, Loss: 0.833854, Accuracy: 92.52%\n",
      "Validation Batch 15, Loss: 0.798645, Accuracy: 92.60%\n",
      "Validation Batch 16, Loss: 0.838044, Accuracy: 92.48%\n",
      "Validation Batch 17, Loss: 0.856606, Accuracy: 92.28%\n",
      "Validation Batch 18, Loss: 0.797703, Accuracy: 92.45%\n",
      "Validation Batch 19, Loss: 0.845887, Accuracy: 92.35%\n",
      "Validation Batch 20, Loss: 0.816954, Accuracy: 92.42%\n",
      "Validation Batch 21, Loss: 0.858836, Accuracy: 92.26%\n",
      "Validation Batch 22, Loss: 0.795092, Accuracy: 92.47%\n",
      "Validation Batch 23, Loss: 0.859601, Accuracy: 92.32%\n",
      "Validation Batch 24, Loss: 0.828326, Accuracy: 92.32%\n",
      "Validation Batch 25, Loss: 0.795907, Accuracy: 92.44%\n",
      "Validation Batch 26, Loss: 0.836291, Accuracy: 92.37%\n",
      "Validation Batch 27, Loss: 0.792556, Accuracy: 92.43%\n",
      "Validation - Epoch 3, Loss: 0.823770, Accuracy: 92.43%\n",
      "Patience—3\n",
      "Epoch 4\n",
      "Batch 1, Loss: 0.842669, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.856055, Accuracy: 89.06%\n",
      "Batch 3, Loss: 0.836180, Accuracy: 89.58%\n",
      "Batch 4, Loss: 0.851598, Accuracy: 89.45%\n",
      "Batch 5, Loss: 0.840995, Accuracy: 89.69%\n",
      "Batch 6, Loss: 0.795150, Accuracy: 90.62%\n",
      "Batch 7, Loss: 0.806554, Accuracy: 91.07%\n",
      "Batch 8, Loss: 0.826145, Accuracy: 91.21%\n",
      "Batch 9, Loss: 0.865557, Accuracy: 90.80%\n",
      "Batch 10, Loss: 0.841592, Accuracy: 90.78%\n",
      "Batch 11, Loss: 0.820241, Accuracy: 91.05%\n",
      "Batch 12, Loss: 0.814124, Accuracy: 91.15%\n",
      "Batch 13, Loss: 0.908352, Accuracy: 90.50%\n",
      "Batch 14, Loss: 0.842008, Accuracy: 90.62%\n",
      "Batch 15, Loss: 0.890826, Accuracy: 90.31%\n",
      "Batch 16, Loss: 0.833526, Accuracy: 90.33%\n",
      "Batch 17, Loss: 0.941026, Accuracy: 89.71%\n",
      "Batch 18, Loss: 0.830263, Accuracy: 89.76%\n",
      "Batch 19, Loss: 0.818789, Accuracy: 89.97%\n",
      "Batch 20, Loss: 0.816944, Accuracy: 90.16%\n",
      "Batch 21, Loss: 0.823744, Accuracy: 90.25%\n",
      "Batch 22, Loss: 0.860855, Accuracy: 90.13%\n",
      "Batch 23, Loss: 0.806785, Accuracy: 90.29%\n",
      "Batch 24, Loss: 0.823395, Accuracy: 90.36%\n",
      "Batch 25, Loss: 0.852645, Accuracy: 90.31%\n",
      "Batch 26, Loss: 0.860389, Accuracy: 90.20%\n",
      "Batch 27, Loss: 0.923099, Accuracy: 89.87%\n",
      "Batch 28, Loss: 0.864371, Accuracy: 89.79%\n",
      "Batch 29, Loss: 0.858052, Accuracy: 89.76%\n",
      "Batch 30, Loss: 0.822243, Accuracy: 89.84%\n",
      "Batch 31, Loss: 0.829505, Accuracy: 89.92%\n",
      "Batch 32, Loss: 0.876685, Accuracy: 89.84%\n",
      "Batch 33, Loss: 0.916904, Accuracy: 89.63%\n",
      "Batch 34, Loss: 0.831272, Accuracy: 89.66%\n",
      "Batch 35, Loss: 0.825401, Accuracy: 89.73%\n",
      "Batch 36, Loss: 0.796302, Accuracy: 89.89%\n",
      "Batch 37, Loss: 0.903726, Accuracy: 89.74%\n",
      "Batch 38, Loss: 0.861670, Accuracy: 89.72%\n",
      "Batch 39, Loss: 0.831528, Accuracy: 89.74%\n",
      "Batch 40, Loss: 0.815092, Accuracy: 89.84%\n",
      "Batch 41, Loss: 0.832499, Accuracy: 89.86%\n",
      "Batch 42, Loss: 0.792904, Accuracy: 89.99%\n",
      "Batch 43, Loss: 0.816402, Accuracy: 90.08%\n",
      "Batch 44, Loss: 0.838031, Accuracy: 90.06%\n",
      "Batch 45, Loss: 0.875683, Accuracy: 89.97%\n",
      "Batch 46, Loss: 0.860726, Accuracy: 89.91%\n",
      "Batch 47, Loss: 0.868453, Accuracy: 89.89%\n",
      "Batch 48, Loss: 0.874101, Accuracy: 89.84%\n",
      "Batch 49, Loss: 0.853760, Accuracy: 89.83%\n",
      "Batch 50, Loss: 0.841101, Accuracy: 89.84%\n",
      "Batch 51, Loss: 0.841640, Accuracy: 89.83%\n",
      "Batch 52, Loss: 0.813606, Accuracy: 89.87%\n",
      "Batch 53, Loss: 0.797779, Accuracy: 89.98%\n",
      "Batch 54, Loss: 0.849908, Accuracy: 89.96%\n",
      "Batch 55, Loss: 0.900769, Accuracy: 89.86%\n",
      "Batch 56, Loss: 0.878044, Accuracy: 89.76%\n",
      "Batch 57, Loss: 0.843645, Accuracy: 89.75%\n",
      "Batch 58, Loss: 0.834131, Accuracy: 89.79%\n",
      "Batch 59, Loss: 0.879534, Accuracy: 89.75%\n",
      "Batch 60, Loss: 0.789183, Accuracy: 89.87%\n",
      "Batch 61, Loss: 0.826358, Accuracy: 89.91%\n",
      "Batch 62, Loss: 0.804444, Accuracy: 89.97%\n",
      "Batch 63, Loss: 0.876217, Accuracy: 89.91%\n",
      "Batch 64, Loss: 0.837371, Accuracy: 89.94%\n",
      "Batch 65, Loss: 0.872953, Accuracy: 89.90%\n",
      "Batch 66, Loss: 0.857239, Accuracy: 89.87%\n",
      "Batch 67, Loss: 0.842104, Accuracy: 89.88%\n",
      "Batch 68, Loss: 0.858547, Accuracy: 89.87%\n",
      "Batch 69, Loss: 0.866027, Accuracy: 89.83%\n",
      "Batch 70, Loss: 0.793068, Accuracy: 89.91%\n",
      "Batch 71, Loss: 0.816075, Accuracy: 89.96%\n",
      "Batch 72, Loss: 0.842081, Accuracy: 89.97%\n",
      "Batch 73, Loss: 0.817996, Accuracy: 90.03%\n",
      "Batch 74, Loss: 0.825536, Accuracy: 90.05%\n",
      "Batch 75, Loss: 0.843521, Accuracy: 90.04%\n",
      "Batch 76, Loss: 0.803301, Accuracy: 90.09%\n",
      "Batch 77, Loss: 0.889175, Accuracy: 90.04%\n",
      "Batch 78, Loss: 0.819392, Accuracy: 90.06%\n",
      "Batch 79, Loss: 0.807154, Accuracy: 90.13%\n",
      "Batch 80, Loss: 0.813784, Accuracy: 90.18%\n",
      "Batch 81, Loss: 0.895842, Accuracy: 90.12%\n",
      "Batch 82, Loss: 0.799733, Accuracy: 90.19%\n",
      "Batch 83, Loss: 0.865865, Accuracy: 90.17%\n",
      "Batch 84, Loss: 0.868817, Accuracy: 90.14%\n",
      "Batch 85, Loss: 0.872791, Accuracy: 90.09%\n",
      "Batch 86, Loss: 0.845658, Accuracy: 90.10%\n",
      "Batch 87, Loss: 0.800364, Accuracy: 90.16%\n",
      "Batch 88, Loss: 0.829241, Accuracy: 90.18%\n",
      "Batch 89, Loss: 0.812665, Accuracy: 90.22%\n",
      "Batch 90, Loss: 0.779336, Accuracy: 90.30%\n",
      "Batch 91, Loss: 0.808329, Accuracy: 90.33%\n",
      "Batch 92, Loss: 0.867925, Accuracy: 90.30%\n",
      "Batch 93, Loss: 0.814938, Accuracy: 90.34%\n",
      "Batch 94, Loss: 0.852814, Accuracy: 90.31%\n",
      "Batch 95, Loss: 0.835648, Accuracy: 90.31%\n",
      "Batch 96, Loss: 0.793391, Accuracy: 90.36%\n",
      "Batch 97, Loss: 0.873039, Accuracy: 90.35%\n",
      "Batch 98, Loss: 0.832460, Accuracy: 90.35%\n",
      "Batch 99, Loss: 0.803425, Accuracy: 90.39%\n",
      "Batch 100, Loss: 0.876238, Accuracy: 90.34%\n",
      "Batch 101, Loss: 0.838456, Accuracy: 90.35%\n",
      "Batch 102, Loss: 0.857871, Accuracy: 90.33%\n",
      "Batch 103, Loss: 0.862868, Accuracy: 90.31%\n",
      "Batch 104, Loss: 0.900481, Accuracy: 90.23%\n",
      "Batch 105, Loss: 0.792157, Accuracy: 90.30%\n",
      "Batch 106, Loss: 0.827168, Accuracy: 90.30%\n",
      "Batch 107, Loss: 0.888807, Accuracy: 90.27%\n",
      "Batch 108, Loss: 0.822708, Accuracy: 90.29%\n",
      "Batch 109, Loss: 0.843145, Accuracy: 90.30%\n",
      "Batch 110, Loss: 0.869221, Accuracy: 90.27%\n",
      "Batch 111, Loss: 0.840113, Accuracy: 90.27%\n",
      "Batch 112, Loss: 0.848236, Accuracy: 90.26%\n",
      "Batch 113, Loss: 0.859313, Accuracy: 90.25%\n",
      "Batch 114, Loss: 0.884134, Accuracy: 90.23%\n",
      "Batch 115, Loss: 0.853254, Accuracy: 90.23%\n",
      "Batch 116, Loss: 0.906294, Accuracy: 90.18%\n",
      "Batch 117, Loss: 0.831401, Accuracy: 90.20%\n",
      "Batch 118, Loss: 0.826695, Accuracy: 90.21%\n",
      "Batch 119, Loss: 0.868952, Accuracy: 90.19%\n",
      "Batch 120, Loss: 0.855953, Accuracy: 90.18%\n",
      "Batch 121, Loss: 0.786777, Accuracy: 90.24%\n",
      "Batch 122, Loss: 0.900739, Accuracy: 90.19%\n",
      "Batch 123, Loss: 0.933148, Accuracy: 90.10%\n",
      "Batch 124, Loss: 0.787285, Accuracy: 90.15%\n",
      "Batch 125, Loss: 0.822297, Accuracy: 90.16%\n",
      "Batch 126, Loss: 0.756909, Accuracy: 90.24%\n",
      "Batch 127, Loss: 0.879460, Accuracy: 90.21%\n",
      "Batch 128, Loss: 0.855414, Accuracy: 90.20%\n",
      "Batch 129, Loss: 0.827586, Accuracy: 90.20%\n",
      "Batch 130, Loss: 0.828257, Accuracy: 90.22%\n",
      "Batch 131, Loss: 0.862534, Accuracy: 90.20%\n",
      "Batch 132, Loss: 0.840084, Accuracy: 90.19%\n",
      "Batch 133, Loss: 0.851623, Accuracy: 90.19%\n",
      "Batch 134, Loss: 0.776805, Accuracy: 90.24%\n",
      "Batch 135, Loss: 0.865677, Accuracy: 90.22%\n",
      "Batch 136, Loss: 0.906983, Accuracy: 90.18%\n",
      "Batch 137, Loss: 0.806097, Accuracy: 90.20%\n",
      "Batch 138, Loss: 0.752148, Accuracy: 90.27%\n",
      "Batch 139, Loss: 0.855522, Accuracy: 90.25%\n",
      "Batch 140, Loss: 0.780730, Accuracy: 90.30%\n",
      "Batch 141, Loss: 0.776004, Accuracy: 90.36%\n",
      "Batch 142, Loss: 0.914229, Accuracy: 90.31%\n",
      "Batch 143, Loss: 0.854226, Accuracy: 90.30%\n",
      "Batch 144, Loss: 0.837242, Accuracy: 90.30%\n",
      "Batch 145, Loss: 0.869430, Accuracy: 90.27%\n",
      "Batch 146, Loss: 0.834749, Accuracy: 90.27%\n",
      "Batch 147, Loss: 0.824623, Accuracy: 90.30%\n",
      "Batch 148, Loss: 0.841638, Accuracy: 90.30%\n",
      "Batch 149, Loss: 0.929761, Accuracy: 90.23%\n",
      "Batch 150, Loss: 0.880516, Accuracy: 90.20%\n",
      "Batch 151, Loss: 0.822437, Accuracy: 90.21%\n",
      "Batch 152, Loss: 0.848027, Accuracy: 90.20%\n",
      "Batch 153, Loss: 0.894142, Accuracy: 90.16%\n",
      "Batch 154, Loss: 0.856594, Accuracy: 90.15%\n",
      "Batch 155, Loss: 0.879506, Accuracy: 90.12%\n",
      "Batch 156, Loss: 0.845899, Accuracy: 90.12%\n",
      "Batch 157, Loss: 0.812584, Accuracy: 90.15%\n",
      "Batch 158, Loss: 0.878757, Accuracy: 90.13%\n",
      "Batch 159, Loss: 0.825736, Accuracy: 90.13%\n",
      "Batch 160, Loss: 0.928011, Accuracy: 90.09%\n",
      "Batch 161, Loss: 0.877426, Accuracy: 90.07%\n",
      "Batch 162, Loss: 0.818036, Accuracy: 90.08%\n",
      "Batch 163, Loss: 0.814404, Accuracy: 90.10%\n",
      "Batch 164, Loss: 0.823338, Accuracy: 90.12%\n",
      "Batch 165, Loss: 0.803588, Accuracy: 90.14%\n",
      "Batch 166, Loss: 0.865526, Accuracy: 90.13%\n",
      "Batch 167, Loss: 0.816802, Accuracy: 90.15%\n",
      "Batch 168, Loss: 0.888388, Accuracy: 90.11%\n",
      "Batch 169, Loss: 0.897499, Accuracy: 90.08%\n",
      "Batch 170, Loss: 0.867660, Accuracy: 90.06%\n",
      "Batch 171, Loss: 0.869220, Accuracy: 90.05%\n",
      "Batch 172, Loss: 0.880712, Accuracy: 90.03%\n",
      "Batch 173, Loss: 0.835638, Accuracy: 90.03%\n",
      "Batch 174, Loss: 0.864787, Accuracy: 90.02%\n",
      "Batch 175, Loss: 0.867264, Accuracy: 90.00%\n",
      "Batch 176, Loss: 0.841953, Accuracy: 90.00%\n",
      "Batch 177, Loss: 0.857774, Accuracy: 90.00%\n",
      "Batch 178, Loss: 0.860597, Accuracy: 89.98%\n",
      "Batch 179, Loss: 0.921291, Accuracy: 89.94%\n",
      "Batch 180, Loss: 0.936038, Accuracy: 89.90%\n",
      "Batch 181, Loss: 0.790324, Accuracy: 89.93%\n",
      "Batch 182, Loss: 0.868818, Accuracy: 89.91%\n",
      "Batch 183, Loss: 0.889214, Accuracy: 89.88%\n",
      "Batch 184, Loss: 0.877459, Accuracy: 89.87%\n",
      "Batch 185, Loss: 0.828481, Accuracy: 89.87%\n",
      "Batch 186, Loss: 0.857505, Accuracy: 89.87%\n",
      "Batch 187, Loss: 0.865978, Accuracy: 89.86%\n",
      "Batch 188, Loss: 0.826704, Accuracy: 89.86%\n",
      "Batch 189, Loss: 0.825388, Accuracy: 89.87%\n",
      "Batch 190, Loss: 0.813058, Accuracy: 89.89%\n",
      "Batch 191, Loss: 0.860002, Accuracy: 89.88%\n",
      "Batch 192, Loss: 0.828800, Accuracy: 89.89%\n",
      "Batch 193, Loss: 0.911626, Accuracy: 89.86%\n",
      "Batch 194, Loss: 0.825980, Accuracy: 89.87%\n",
      "Batch 195, Loss: 0.795511, Accuracy: 89.90%\n",
      "Batch 196, Loss: 0.849057, Accuracy: 89.89%\n",
      "Batch 197, Loss: 0.800565, Accuracy: 89.92%\n",
      "Batch 198, Loss: 0.788946, Accuracy: 89.95%\n",
      "Batch 199, Loss: 0.873608, Accuracy: 89.94%\n",
      "Batch 200, Loss: 0.870239, Accuracy: 89.94%\n",
      "Batch 201, Loss: 0.869288, Accuracy: 89.93%\n",
      "Batch 202, Loss: 0.808033, Accuracy: 89.95%\n",
      "Batch 203, Loss: 0.877240, Accuracy: 89.93%\n",
      "Batch 204, Loss: 0.893999, Accuracy: 89.91%\n",
      "Batch 205, Loss: 0.793159, Accuracy: 89.93%\n",
      "Batch 206, Loss: 0.846580, Accuracy: 89.93%\n",
      "Batch 207, Loss: 0.886298, Accuracy: 89.90%\n",
      "Batch 208, Loss: 0.835106, Accuracy: 89.90%\n",
      "Batch 209, Loss: 0.812161, Accuracy: 89.91%\n",
      "Batch 210, Loss: 0.836514, Accuracy: 89.92%\n",
      "Batch 211, Loss: 0.870622, Accuracy: 89.91%\n",
      "Batch 212, Loss: 0.813323, Accuracy: 89.92%\n",
      "Batch 213, Loss: 0.803151, Accuracy: 89.95%\n",
      "Training - Epoch 4, Loss: 0.845508, Accuracy: 89.95%\n",
      "Validation Batch 1, Loss: 0.820359, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.792337, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.816883, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.835465, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.818684, Accuracy: 93.12%\n",
      "Validation Batch 6, Loss: 0.793314, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.794928, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.856929, Accuracy: 93.16%\n",
      "Validation Batch 9, Loss: 0.889062, Accuracy: 92.19%\n",
      "Validation Batch 10, Loss: 0.788031, Accuracy: 92.66%\n",
      "Validation Batch 11, Loss: 0.827776, Accuracy: 92.61%\n",
      "Validation Batch 12, Loss: 0.827390, Accuracy: 92.58%\n",
      "Validation Batch 13, Loss: 0.833726, Accuracy: 92.43%\n",
      "Validation Batch 14, Loss: 0.834139, Accuracy: 92.41%\n",
      "Validation Batch 15, Loss: 0.798822, Accuracy: 92.50%\n",
      "Validation Batch 16, Loss: 0.838795, Accuracy: 92.38%\n",
      "Validation Batch 17, Loss: 0.856720, Accuracy: 92.19%\n",
      "Validation Batch 18, Loss: 0.797805, Accuracy: 92.36%\n",
      "Validation Batch 19, Loss: 0.846423, Accuracy: 92.27%\n",
      "Validation Batch 20, Loss: 0.818192, Accuracy: 92.42%\n",
      "Validation Batch 21, Loss: 0.858899, Accuracy: 92.26%\n",
      "Validation Batch 22, Loss: 0.795641, Accuracy: 92.47%\n",
      "Validation Batch 23, Loss: 0.859831, Accuracy: 92.32%\n",
      "Validation Batch 24, Loss: 0.828678, Accuracy: 92.25%\n",
      "Validation Batch 25, Loss: 0.796634, Accuracy: 92.38%\n",
      "Validation Batch 26, Loss: 0.836695, Accuracy: 92.31%\n",
      "Validation Batch 27, Loss: 0.793747, Accuracy: 92.37%\n",
      "Validation - Epoch 4, Loss: 0.824293, Accuracy: 92.37%\n",
      "Patience—4\n",
      "Epoch 5\n",
      "Batch 1, Loss: 0.822452, Accuracy: 92.19%\n",
      "Batch 2, Loss: 0.839469, Accuracy: 90.62%\n",
      "Batch 3, Loss: 0.885441, Accuracy: 88.02%\n",
      "Batch 4, Loss: 0.799739, Accuracy: 89.84%\n",
      "Batch 5, Loss: 0.845274, Accuracy: 89.69%\n",
      "Batch 6, Loss: 0.815427, Accuracy: 90.36%\n",
      "Batch 7, Loss: 0.783035, Accuracy: 91.29%\n",
      "Batch 8, Loss: 0.870682, Accuracy: 90.62%\n",
      "Batch 9, Loss: 0.799836, Accuracy: 90.97%\n",
      "Batch 10, Loss: 0.850750, Accuracy: 90.78%\n",
      "Batch 11, Loss: 0.835895, Accuracy: 90.77%\n",
      "Batch 12, Loss: 0.833531, Accuracy: 90.89%\n",
      "Batch 13, Loss: 0.836842, Accuracy: 90.99%\n",
      "Batch 14, Loss: 0.840186, Accuracy: 90.96%\n",
      "Batch 15, Loss: 0.836896, Accuracy: 90.83%\n",
      "Batch 16, Loss: 0.857489, Accuracy: 90.72%\n",
      "Batch 17, Loss: 0.846995, Accuracy: 90.62%\n",
      "Batch 18, Loss: 0.883288, Accuracy: 90.36%\n",
      "Batch 19, Loss: 0.795897, Accuracy: 90.71%\n",
      "Batch 20, Loss: 0.848681, Accuracy: 90.55%\n",
      "Batch 21, Loss: 0.870480, Accuracy: 90.40%\n",
      "Batch 22, Loss: 0.929923, Accuracy: 89.99%\n",
      "Batch 23, Loss: 0.889261, Accuracy: 89.74%\n",
      "Batch 24, Loss: 0.806974, Accuracy: 89.91%\n",
      "Batch 25, Loss: 0.811398, Accuracy: 90.06%\n",
      "Batch 26, Loss: 0.887050, Accuracy: 89.90%\n",
      "Batch 27, Loss: 0.873658, Accuracy: 89.81%\n",
      "Batch 28, Loss: 0.884052, Accuracy: 89.73%\n",
      "Batch 29, Loss: 0.896597, Accuracy: 89.55%\n",
      "Batch 30, Loss: 0.850627, Accuracy: 89.58%\n",
      "Batch 31, Loss: 0.812385, Accuracy: 89.72%\n",
      "Batch 32, Loss: 0.889699, Accuracy: 89.55%\n",
      "Batch 33, Loss: 0.834000, Accuracy: 89.58%\n",
      "Batch 34, Loss: 0.900557, Accuracy: 89.48%\n",
      "Batch 35, Loss: 0.777872, Accuracy: 89.73%\n",
      "Batch 36, Loss: 0.858005, Accuracy: 89.67%\n",
      "Batch 37, Loss: 0.826601, Accuracy: 89.74%\n",
      "Batch 38, Loss: 0.903149, Accuracy: 89.60%\n",
      "Batch 39, Loss: 0.868542, Accuracy: 89.50%\n",
      "Batch 40, Loss: 0.871797, Accuracy: 89.45%\n",
      "Batch 41, Loss: 0.858800, Accuracy: 89.41%\n",
      "Batch 42, Loss: 0.809700, Accuracy: 89.47%\n",
      "Batch 43, Loss: 0.891711, Accuracy: 89.35%\n",
      "Batch 44, Loss: 0.802028, Accuracy: 89.49%\n",
      "Batch 45, Loss: 0.850819, Accuracy: 89.51%\n",
      "Batch 46, Loss: 0.829666, Accuracy: 89.54%\n",
      "Batch 47, Loss: 0.870207, Accuracy: 89.53%\n",
      "Batch 48, Loss: 0.820704, Accuracy: 89.58%\n",
      "Batch 49, Loss: 0.820899, Accuracy: 89.64%\n",
      "Batch 50, Loss: 0.831545, Accuracy: 89.69%\n",
      "Batch 51, Loss: 0.825684, Accuracy: 89.74%\n",
      "Batch 52, Loss: 0.868714, Accuracy: 89.69%\n",
      "Batch 53, Loss: 0.844039, Accuracy: 89.71%\n",
      "Batch 54, Loss: 0.881750, Accuracy: 89.64%\n",
      "Batch 55, Loss: 0.841440, Accuracy: 89.66%\n",
      "Batch 56, Loss: 0.863701, Accuracy: 89.65%\n",
      "Batch 57, Loss: 0.801584, Accuracy: 89.75%\n",
      "Batch 58, Loss: 0.868466, Accuracy: 89.71%\n",
      "Batch 59, Loss: 0.846733, Accuracy: 89.70%\n",
      "Batch 60, Loss: 0.842158, Accuracy: 89.71%\n",
      "Batch 61, Loss: 0.814732, Accuracy: 89.78%\n",
      "Batch 62, Loss: 0.908777, Accuracy: 89.69%\n",
      "Batch 63, Loss: 0.839169, Accuracy: 89.73%\n",
      "Batch 64, Loss: 0.847118, Accuracy: 89.72%\n",
      "Batch 65, Loss: 0.843332, Accuracy: 89.71%\n",
      "Batch 66, Loss: 0.827608, Accuracy: 89.75%\n",
      "Batch 67, Loss: 0.772565, Accuracy: 89.88%\n",
      "Batch 68, Loss: 0.819873, Accuracy: 89.91%\n",
      "Batch 69, Loss: 0.851181, Accuracy: 89.92%\n",
      "Batch 70, Loss: 0.845257, Accuracy: 89.93%\n",
      "Batch 71, Loss: 0.866051, Accuracy: 89.92%\n",
      "Batch 72, Loss: 0.904028, Accuracy: 89.82%\n",
      "Batch 73, Loss: 0.845226, Accuracy: 89.81%\n",
      "Batch 74, Loss: 0.872999, Accuracy: 89.78%\n",
      "Batch 75, Loss: 0.802122, Accuracy: 89.83%\n",
      "Batch 76, Loss: 0.853652, Accuracy: 89.84%\n",
      "Batch 77, Loss: 0.776129, Accuracy: 89.96%\n",
      "Batch 78, Loss: 0.853170, Accuracy: 89.94%\n",
      "Batch 79, Loss: 0.817890, Accuracy: 89.97%\n",
      "Batch 80, Loss: 0.831354, Accuracy: 90.00%\n",
      "Batch 81, Loss: 0.808305, Accuracy: 90.05%\n",
      "Batch 82, Loss: 0.839832, Accuracy: 90.05%\n",
      "Batch 83, Loss: 0.882093, Accuracy: 90.00%\n",
      "Batch 84, Loss: 0.836061, Accuracy: 90.03%\n",
      "Batch 85, Loss: 0.861143, Accuracy: 90.02%\n",
      "Batch 86, Loss: 0.820125, Accuracy: 90.06%\n",
      "Batch 87, Loss: 0.872732, Accuracy: 90.03%\n",
      "Batch 88, Loss: 0.888347, Accuracy: 89.99%\n",
      "Batch 89, Loss: 0.837773, Accuracy: 89.99%\n",
      "Batch 90, Loss: 0.844727, Accuracy: 90.02%\n",
      "Batch 91, Loss: 0.913319, Accuracy: 89.94%\n",
      "Batch 92, Loss: 0.863333, Accuracy: 89.93%\n",
      "Batch 93, Loss: 0.873318, Accuracy: 89.90%\n",
      "Batch 94, Loss: 0.863854, Accuracy: 89.88%\n",
      "Batch 95, Loss: 0.822425, Accuracy: 89.90%\n",
      "Batch 96, Loss: 0.896624, Accuracy: 89.84%\n",
      "Batch 97, Loss: 0.846644, Accuracy: 89.84%\n",
      "Batch 98, Loss: 0.826067, Accuracy: 89.84%\n",
      "Batch 99, Loss: 0.843369, Accuracy: 89.87%\n",
      "Batch 100, Loss: 0.870225, Accuracy: 89.84%\n",
      "Batch 101, Loss: 0.808799, Accuracy: 89.90%\n",
      "Batch 102, Loss: 0.817792, Accuracy: 89.94%\n",
      "Batch 103, Loss: 0.834168, Accuracy: 89.97%\n",
      "Batch 104, Loss: 0.871266, Accuracy: 89.95%\n",
      "Batch 105, Loss: 0.793955, Accuracy: 90.00%\n",
      "Batch 106, Loss: 0.836027, Accuracy: 90.01%\n",
      "Batch 107, Loss: 0.836623, Accuracy: 90.01%\n",
      "Batch 108, Loss: 0.810858, Accuracy: 90.05%\n",
      "Batch 109, Loss: 0.899864, Accuracy: 89.98%\n",
      "Batch 110, Loss: 0.810233, Accuracy: 90.00%\n",
      "Batch 111, Loss: 0.841871, Accuracy: 90.01%\n",
      "Batch 112, Loss: 0.843618, Accuracy: 89.98%\n",
      "Batch 113, Loss: 0.822493, Accuracy: 90.00%\n",
      "Batch 114, Loss: 0.863110, Accuracy: 89.98%\n",
      "Batch 115, Loss: 0.817080, Accuracy: 90.00%\n",
      "Batch 116, Loss: 0.862803, Accuracy: 89.99%\n",
      "Batch 117, Loss: 0.847152, Accuracy: 89.98%\n",
      "Batch 118, Loss: 0.771307, Accuracy: 90.06%\n",
      "Batch 119, Loss: 0.811831, Accuracy: 90.09%\n",
      "Batch 120, Loss: 0.859301, Accuracy: 90.09%\n",
      "Batch 121, Loss: 0.801038, Accuracy: 90.13%\n",
      "Batch 122, Loss: 0.801461, Accuracy: 90.18%\n",
      "Batch 123, Loss: 0.796580, Accuracy: 90.22%\n",
      "Batch 124, Loss: 0.910177, Accuracy: 90.16%\n",
      "Batch 125, Loss: 0.870530, Accuracy: 90.15%\n",
      "Batch 126, Loss: 0.838487, Accuracy: 90.14%\n",
      "Batch 127, Loss: 0.886886, Accuracy: 90.11%\n",
      "Batch 128, Loss: 0.847485, Accuracy: 90.09%\n",
      "Batch 129, Loss: 0.841048, Accuracy: 90.09%\n",
      "Batch 130, Loss: 0.810858, Accuracy: 90.11%\n",
      "Batch 131, Loss: 0.878012, Accuracy: 90.08%\n",
      "Batch 132, Loss: 0.834546, Accuracy: 90.08%\n",
      "Batch 133, Loss: 0.811877, Accuracy: 90.10%\n",
      "Batch 134, Loss: 0.777300, Accuracy: 90.15%\n",
      "Batch 135, Loss: 0.928721, Accuracy: 90.08%\n",
      "Batch 136, Loss: 0.906587, Accuracy: 90.04%\n",
      "Batch 137, Loss: 0.861721, Accuracy: 90.02%\n",
      "Batch 138, Loss: 0.883795, Accuracy: 89.99%\n",
      "Batch 139, Loss: 0.906107, Accuracy: 89.94%\n",
      "Batch 140, Loss: 0.795800, Accuracy: 89.98%\n",
      "Batch 141, Loss: 0.903189, Accuracy: 89.94%\n",
      "Batch 142, Loss: 0.833235, Accuracy: 89.94%\n",
      "Batch 143, Loss: 0.859074, Accuracy: 89.93%\n",
      "Batch 144, Loss: 0.829741, Accuracy: 89.93%\n",
      "Batch 145, Loss: 0.852963, Accuracy: 89.92%\n",
      "Batch 146, Loss: 0.784933, Accuracy: 89.96%\n",
      "Batch 147, Loss: 0.850460, Accuracy: 89.97%\n",
      "Batch 148, Loss: 0.845727, Accuracy: 89.97%\n",
      "Batch 149, Loss: 0.847584, Accuracy: 89.95%\n",
      "Batch 150, Loss: 0.855779, Accuracy: 89.95%\n",
      "Batch 151, Loss: 0.795741, Accuracy: 89.99%\n",
      "Batch 152, Loss: 0.822507, Accuracy: 90.01%\n",
      "Batch 153, Loss: 0.751326, Accuracy: 90.07%\n",
      "Batch 154, Loss: 0.781988, Accuracy: 90.11%\n",
      "Batch 155, Loss: 0.849587, Accuracy: 90.10%\n",
      "Batch 156, Loss: 0.912539, Accuracy: 90.06%\n",
      "Batch 157, Loss: 0.806752, Accuracy: 90.10%\n",
      "Batch 158, Loss: 0.787590, Accuracy: 90.13%\n",
      "Batch 159, Loss: 0.827120, Accuracy: 90.13%\n",
      "Batch 160, Loss: 0.850802, Accuracy: 90.14%\n",
      "Batch 161, Loss: 0.809281, Accuracy: 90.16%\n",
      "Batch 162, Loss: 0.822676, Accuracy: 90.17%\n",
      "Batch 163, Loss: 0.864484, Accuracy: 90.16%\n",
      "Batch 164, Loss: 0.882696, Accuracy: 90.13%\n",
      "Batch 165, Loss: 0.863074, Accuracy: 90.11%\n",
      "Batch 166, Loss: 0.861732, Accuracy: 90.09%\n",
      "Batch 167, Loss: 0.884672, Accuracy: 90.05%\n",
      "Batch 168, Loss: 0.860828, Accuracy: 90.03%\n",
      "Batch 169, Loss: 0.934658, Accuracy: 89.98%\n",
      "Batch 170, Loss: 0.848543, Accuracy: 89.98%\n",
      "Batch 171, Loss: 0.790301, Accuracy: 90.02%\n",
      "Batch 172, Loss: 0.852049, Accuracy: 90.02%\n",
      "Batch 173, Loss: 0.833749, Accuracy: 90.02%\n",
      "Batch 174, Loss: 0.891097, Accuracy: 89.99%\n",
      "Batch 175, Loss: 0.921929, Accuracy: 89.94%\n",
      "Batch 176, Loss: 0.852930, Accuracy: 89.93%\n",
      "Batch 177, Loss: 0.837498, Accuracy: 89.94%\n",
      "Batch 178, Loss: 0.827734, Accuracy: 89.95%\n",
      "Batch 179, Loss: 0.897971, Accuracy: 89.93%\n",
      "Batch 180, Loss: 0.865986, Accuracy: 89.92%\n",
      "Batch 181, Loss: 0.878215, Accuracy: 89.90%\n",
      "Batch 182, Loss: 0.839422, Accuracy: 89.90%\n",
      "Batch 183, Loss: 0.887321, Accuracy: 89.88%\n",
      "Batch 184, Loss: 0.881838, Accuracy: 89.86%\n",
      "Batch 185, Loss: 0.809044, Accuracy: 89.88%\n",
      "Batch 186, Loss: 0.798644, Accuracy: 89.90%\n",
      "Batch 187, Loss: 0.873409, Accuracy: 89.89%\n",
      "Batch 188, Loss: 0.854716, Accuracy: 89.89%\n",
      "Batch 189, Loss: 0.820513, Accuracy: 89.91%\n",
      "Batch 190, Loss: 0.916774, Accuracy: 89.87%\n",
      "Batch 191, Loss: 0.818636, Accuracy: 89.88%\n",
      "Batch 192, Loss: 0.860651, Accuracy: 89.88%\n",
      "Batch 193, Loss: 0.939750, Accuracy: 89.82%\n",
      "Batch 194, Loss: 0.885285, Accuracy: 89.80%\n",
      "Batch 195, Loss: 0.864609, Accuracy: 89.79%\n",
      "Batch 196, Loss: 0.819575, Accuracy: 89.80%\n",
      "Batch 197, Loss: 0.884071, Accuracy: 89.78%\n",
      "Batch 198, Loss: 0.800661, Accuracy: 89.81%\n",
      "Batch 199, Loss: 0.846080, Accuracy: 89.82%\n",
      "Batch 200, Loss: 0.848598, Accuracy: 89.80%\n",
      "Batch 201, Loss: 0.863308, Accuracy: 89.80%\n",
      "Batch 202, Loss: 0.820349, Accuracy: 89.81%\n",
      "Batch 203, Loss: 0.823114, Accuracy: 89.82%\n",
      "Batch 204, Loss: 0.834605, Accuracy: 89.83%\n",
      "Batch 205, Loss: 0.820658, Accuracy: 89.85%\n",
      "Batch 206, Loss: 0.798837, Accuracy: 89.87%\n",
      "Batch 207, Loss: 0.892132, Accuracy: 89.86%\n",
      "Batch 208, Loss: 0.834443, Accuracy: 89.86%\n",
      "Batch 209, Loss: 0.833668, Accuracy: 89.87%\n",
      "Batch 210, Loss: 0.948562, Accuracy: 89.81%\n",
      "Batch 211, Loss: 0.875366, Accuracy: 89.79%\n",
      "Batch 212, Loss: 0.835205, Accuracy: 89.80%\n",
      "Batch 213, Loss: 0.819375, Accuracy: 89.82%\n",
      "Training - Epoch 5, Loss: 0.846819, Accuracy: 89.82%\n",
      "Validation Batch 1, Loss: 0.820144, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.791883, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.816742, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.835280, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.818787, Accuracy: 93.12%\n",
      "Validation Batch 6, Loss: 0.793252, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.794801, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.856644, Accuracy: 93.16%\n",
      "Validation Batch 9, Loss: 0.888921, Accuracy: 92.19%\n",
      "Validation Batch 10, Loss: 0.787953, Accuracy: 92.66%\n",
      "Validation Batch 11, Loss: 0.827628, Accuracy: 92.61%\n",
      "Validation Batch 12, Loss: 0.827081, Accuracy: 92.58%\n",
      "Validation Batch 13, Loss: 0.833389, Accuracy: 92.43%\n",
      "Validation Batch 14, Loss: 0.834064, Accuracy: 92.41%\n",
      "Validation Batch 15, Loss: 0.798628, Accuracy: 92.50%\n",
      "Validation Batch 16, Loss: 0.838311, Accuracy: 92.38%\n",
      "Validation Batch 17, Loss: 0.856860, Accuracy: 92.19%\n",
      "Validation Batch 18, Loss: 0.797740, Accuracy: 92.36%\n",
      "Validation Batch 19, Loss: 0.845770, Accuracy: 92.27%\n",
      "Validation Batch 20, Loss: 0.818038, Accuracy: 92.42%\n",
      "Validation Batch 21, Loss: 0.858776, Accuracy: 92.26%\n",
      "Validation Batch 22, Loss: 0.795317, Accuracy: 92.47%\n",
      "Validation Batch 23, Loss: 0.859653, Accuracy: 92.32%\n",
      "Validation Batch 24, Loss: 0.828803, Accuracy: 92.25%\n",
      "Validation Batch 25, Loss: 0.796678, Accuracy: 92.38%\n",
      "Validation Batch 26, Loss: 0.836154, Accuracy: 92.31%\n",
      "Validation Batch 27, Loss: 0.793485, Accuracy: 92.37%\n",
      "Validation - Epoch 5, Loss: 0.824103, Accuracy: 92.37%\n",
      "Patience—5\n",
      "Epoch 6\n",
      "Batch 1, Loss: 0.829069, Accuracy: 92.19%\n",
      "Batch 2, Loss: 0.808774, Accuracy: 92.97%\n",
      "Batch 3, Loss: 0.849660, Accuracy: 91.67%\n",
      "Batch 4, Loss: 0.882448, Accuracy: 90.23%\n",
      "Batch 5, Loss: 0.891936, Accuracy: 89.06%\n",
      "Batch 6, Loss: 0.876653, Accuracy: 88.80%\n",
      "Batch 7, Loss: 0.813497, Accuracy: 89.51%\n",
      "Batch 8, Loss: 0.854851, Accuracy: 89.45%\n",
      "Batch 9, Loss: 0.850512, Accuracy: 89.58%\n",
      "Batch 10, Loss: 0.868420, Accuracy: 89.38%\n",
      "Batch 11, Loss: 0.852427, Accuracy: 89.35%\n",
      "Batch 12, Loss: 0.850820, Accuracy: 89.32%\n",
      "Batch 13, Loss: 0.807594, Accuracy: 89.78%\n",
      "Batch 14, Loss: 0.787191, Accuracy: 90.18%\n",
      "Batch 15, Loss: 0.815518, Accuracy: 90.21%\n",
      "Batch 16, Loss: 0.828438, Accuracy: 90.33%\n",
      "Batch 17, Loss: 0.886377, Accuracy: 90.07%\n",
      "Batch 18, Loss: 0.834802, Accuracy: 90.10%\n",
      "Batch 19, Loss: 0.789553, Accuracy: 90.38%\n",
      "Batch 20, Loss: 0.826942, Accuracy: 90.47%\n",
      "Batch 21, Loss: 0.832950, Accuracy: 90.55%\n",
      "Batch 22, Loss: 0.859375, Accuracy: 90.48%\n",
      "Batch 23, Loss: 0.911241, Accuracy: 90.08%\n",
      "Batch 24, Loss: 0.866470, Accuracy: 89.97%\n",
      "Batch 25, Loss: 0.929150, Accuracy: 89.56%\n",
      "Batch 26, Loss: 0.875868, Accuracy: 89.42%\n",
      "Batch 27, Loss: 0.889244, Accuracy: 89.24%\n",
      "Batch 28, Loss: 0.806214, Accuracy: 89.40%\n",
      "Batch 29, Loss: 0.861288, Accuracy: 89.28%\n",
      "Batch 30, Loss: 0.764809, Accuracy: 89.58%\n",
      "Batch 31, Loss: 0.812205, Accuracy: 89.72%\n",
      "Batch 32, Loss: 0.868576, Accuracy: 89.65%\n",
      "Batch 33, Loss: 0.816106, Accuracy: 89.77%\n",
      "Batch 34, Loss: 0.877539, Accuracy: 89.71%\n",
      "Batch 35, Loss: 0.886105, Accuracy: 89.55%\n",
      "Batch 36, Loss: 0.815570, Accuracy: 89.63%\n",
      "Batch 37, Loss: 0.842223, Accuracy: 89.61%\n",
      "Batch 38, Loss: 0.804538, Accuracy: 89.72%\n",
      "Batch 39, Loss: 0.817816, Accuracy: 89.78%\n",
      "Batch 40, Loss: 0.840176, Accuracy: 89.77%\n",
      "Batch 41, Loss: 0.829651, Accuracy: 89.79%\n",
      "Batch 42, Loss: 0.890093, Accuracy: 89.66%\n",
      "Batch 43, Loss: 0.888699, Accuracy: 89.50%\n",
      "Batch 44, Loss: 0.904030, Accuracy: 89.38%\n",
      "Batch 45, Loss: 0.842195, Accuracy: 89.41%\n",
      "Batch 46, Loss: 0.827928, Accuracy: 89.47%\n",
      "Batch 47, Loss: 0.827325, Accuracy: 89.53%\n",
      "Batch 48, Loss: 0.790221, Accuracy: 89.65%\n",
      "Batch 49, Loss: 0.854245, Accuracy: 89.64%\n",
      "Batch 50, Loss: 0.840248, Accuracy: 89.66%\n",
      "Batch 51, Loss: 0.872380, Accuracy: 89.58%\n",
      "Batch 52, Loss: 0.847112, Accuracy: 89.60%\n",
      "Batch 53, Loss: 0.866164, Accuracy: 89.56%\n",
      "Batch 54, Loss: 0.868586, Accuracy: 89.55%\n",
      "Batch 55, Loss: 0.841425, Accuracy: 89.57%\n",
      "Batch 56, Loss: 0.825325, Accuracy: 89.62%\n",
      "Batch 57, Loss: 0.831226, Accuracy: 89.67%\n",
      "Batch 58, Loss: 0.854227, Accuracy: 89.66%\n",
      "Batch 59, Loss: 0.857653, Accuracy: 89.65%\n",
      "Batch 60, Loss: 0.860488, Accuracy: 89.61%\n",
      "Batch 61, Loss: 0.844690, Accuracy: 89.63%\n",
      "Batch 62, Loss: 0.825154, Accuracy: 89.67%\n",
      "Batch 63, Loss: 0.797039, Accuracy: 89.76%\n",
      "Batch 64, Loss: 0.807248, Accuracy: 89.82%\n",
      "Batch 65, Loss: 0.778501, Accuracy: 89.93%\n",
      "Batch 66, Loss: 0.934861, Accuracy: 89.77%\n",
      "Batch 67, Loss: 0.853886, Accuracy: 89.74%\n",
      "Batch 68, Loss: 0.807069, Accuracy: 89.80%\n",
      "Batch 69, Loss: 0.840746, Accuracy: 89.81%\n",
      "Batch 70, Loss: 0.804834, Accuracy: 89.87%\n",
      "Batch 71, Loss: 0.849247, Accuracy: 89.85%\n",
      "Batch 72, Loss: 0.858215, Accuracy: 89.82%\n",
      "Batch 73, Loss: 0.803308, Accuracy: 89.90%\n",
      "Batch 74, Loss: 0.857840, Accuracy: 89.89%\n",
      "Batch 75, Loss: 0.827024, Accuracy: 89.92%\n",
      "Batch 76, Loss: 0.868536, Accuracy: 89.88%\n",
      "Batch 77, Loss: 0.810389, Accuracy: 89.91%\n",
      "Batch 78, Loss: 0.841221, Accuracy: 89.92%\n",
      "Batch 79, Loss: 0.815246, Accuracy: 89.97%\n",
      "Batch 80, Loss: 0.850807, Accuracy: 89.96%\n",
      "Batch 81, Loss: 0.803856, Accuracy: 90.03%\n",
      "Batch 82, Loss: 0.840698, Accuracy: 90.02%\n",
      "Batch 83, Loss: 0.873316, Accuracy: 89.98%\n",
      "Batch 84, Loss: 0.848598, Accuracy: 89.99%\n",
      "Batch 85, Loss: 0.836235, Accuracy: 90.02%\n",
      "Batch 86, Loss: 0.894680, Accuracy: 89.95%\n",
      "Batch 87, Loss: 0.822426, Accuracy: 89.98%\n",
      "Batch 88, Loss: 0.871648, Accuracy: 89.95%\n",
      "Batch 89, Loss: 0.786706, Accuracy: 90.01%\n",
      "Batch 90, Loss: 0.854170, Accuracy: 90.00%\n",
      "Batch 91, Loss: 0.767756, Accuracy: 90.09%\n",
      "Batch 92, Loss: 0.881235, Accuracy: 90.05%\n",
      "Batch 93, Loss: 0.854603, Accuracy: 90.04%\n",
      "Batch 94, Loss: 0.811346, Accuracy: 90.08%\n",
      "Batch 95, Loss: 0.803768, Accuracy: 90.12%\n",
      "Batch 96, Loss: 0.806145, Accuracy: 90.15%\n",
      "Batch 97, Loss: 0.862226, Accuracy: 90.13%\n",
      "Batch 98, Loss: 0.747153, Accuracy: 90.23%\n",
      "Batch 99, Loss: 0.852780, Accuracy: 90.23%\n",
      "Batch 100, Loss: 0.870250, Accuracy: 90.20%\n",
      "Batch 101, Loss: 0.848418, Accuracy: 90.19%\n",
      "Batch 102, Loss: 0.845412, Accuracy: 90.18%\n",
      "Batch 103, Loss: 0.916627, Accuracy: 90.11%\n",
      "Batch 104, Loss: 0.847717, Accuracy: 90.11%\n",
      "Batch 105, Loss: 0.855864, Accuracy: 90.12%\n",
      "Batch 106, Loss: 0.831810, Accuracy: 90.14%\n",
      "Batch 107, Loss: 0.873597, Accuracy: 90.11%\n",
      "Batch 108, Loss: 0.823352, Accuracy: 90.13%\n",
      "Batch 109, Loss: 0.824680, Accuracy: 90.14%\n",
      "Batch 110, Loss: 0.811726, Accuracy: 90.18%\n",
      "Batch 111, Loss: 0.871959, Accuracy: 90.16%\n",
      "Batch 112, Loss: 0.831595, Accuracy: 90.18%\n",
      "Batch 113, Loss: 0.874568, Accuracy: 90.15%\n",
      "Batch 114, Loss: 0.847677, Accuracy: 90.15%\n",
      "Batch 115, Loss: 0.789917, Accuracy: 90.20%\n",
      "Batch 116, Loss: 0.848644, Accuracy: 90.21%\n",
      "Batch 117, Loss: 0.874355, Accuracy: 90.18%\n",
      "Batch 118, Loss: 0.841253, Accuracy: 90.17%\n",
      "Batch 119, Loss: 0.817198, Accuracy: 90.19%\n",
      "Batch 120, Loss: 0.811076, Accuracy: 90.22%\n",
      "Batch 121, Loss: 0.837579, Accuracy: 90.22%\n",
      "Batch 122, Loss: 0.860559, Accuracy: 90.19%\n",
      "Batch 123, Loss: 0.806811, Accuracy: 90.22%\n",
      "Batch 124, Loss: 0.920712, Accuracy: 90.16%\n",
      "Batch 125, Loss: 0.898325, Accuracy: 90.11%\n",
      "Batch 126, Loss: 0.865487, Accuracy: 90.09%\n",
      "Batch 127, Loss: 0.832266, Accuracy: 90.11%\n",
      "Batch 128, Loss: 0.814170, Accuracy: 90.14%\n",
      "Batch 129, Loss: 0.848511, Accuracy: 90.15%\n",
      "Batch 130, Loss: 0.850030, Accuracy: 90.14%\n",
      "Batch 131, Loss: 0.882592, Accuracy: 90.11%\n",
      "Batch 132, Loss: 0.885227, Accuracy: 90.09%\n",
      "Batch 133, Loss: 0.819257, Accuracy: 90.12%\n",
      "Batch 134, Loss: 0.844478, Accuracy: 90.11%\n",
      "Batch 135, Loss: 0.820385, Accuracy: 90.14%\n",
      "Batch 136, Loss: 0.808800, Accuracy: 90.15%\n",
      "Batch 137, Loss: 0.902786, Accuracy: 90.10%\n",
      "Batch 138, Loss: 0.832130, Accuracy: 90.10%\n",
      "Batch 139, Loss: 0.821320, Accuracy: 90.11%\n",
      "Batch 140, Loss: 0.843733, Accuracy: 90.11%\n",
      "Batch 141, Loss: 0.842513, Accuracy: 90.12%\n",
      "Batch 142, Loss: 0.870768, Accuracy: 90.11%\n",
      "Batch 143, Loss: 0.882120, Accuracy: 90.08%\n",
      "Batch 144, Loss: 0.891520, Accuracy: 90.04%\n",
      "Batch 145, Loss: 0.838096, Accuracy: 90.05%\n",
      "Batch 146, Loss: 0.886564, Accuracy: 90.03%\n",
      "Batch 147, Loss: 0.856058, Accuracy: 90.02%\n",
      "Batch 148, Loss: 0.840554, Accuracy: 90.02%\n",
      "Batch 149, Loss: 0.870728, Accuracy: 90.00%\n",
      "Batch 150, Loss: 0.808920, Accuracy: 90.02%\n",
      "Batch 151, Loss: 0.786853, Accuracy: 90.06%\n",
      "Batch 152, Loss: 0.888222, Accuracy: 90.02%\n",
      "Batch 153, Loss: 0.826109, Accuracy: 90.04%\n",
      "Batch 154, Loss: 0.838918, Accuracy: 90.05%\n",
      "Batch 155, Loss: 0.808277, Accuracy: 90.08%\n",
      "Batch 156, Loss: 0.852778, Accuracy: 90.07%\n",
      "Batch 157, Loss: 0.835681, Accuracy: 90.08%\n",
      "Batch 158, Loss: 0.883633, Accuracy: 90.05%\n",
      "Batch 159, Loss: 0.902330, Accuracy: 90.02%\n",
      "Batch 160, Loss: 0.860904, Accuracy: 90.00%\n",
      "Batch 161, Loss: 0.846183, Accuracy: 89.99%\n",
      "Batch 162, Loss: 0.922340, Accuracy: 89.96%\n",
      "Batch 163, Loss: 0.918451, Accuracy: 89.93%\n",
      "Batch 164, Loss: 0.800534, Accuracy: 89.97%\n",
      "Batch 165, Loss: 0.855330, Accuracy: 89.96%\n",
      "Batch 166, Loss: 0.812466, Accuracy: 89.98%\n",
      "Batch 167, Loss: 0.885139, Accuracy: 89.96%\n",
      "Batch 168, Loss: 0.829650, Accuracy: 89.96%\n",
      "Batch 169, Loss: 0.815026, Accuracy: 89.99%\n",
      "Batch 170, Loss: 0.839206, Accuracy: 89.99%\n",
      "Batch 171, Loss: 0.863391, Accuracy: 89.98%\n",
      "Batch 172, Loss: 0.800573, Accuracy: 90.00%\n",
      "Batch 173, Loss: 0.818687, Accuracy: 90.01%\n",
      "Batch 174, Loss: 0.806785, Accuracy: 90.03%\n",
      "Batch 175, Loss: 0.886998, Accuracy: 90.01%\n",
      "Batch 176, Loss: 0.838245, Accuracy: 90.01%\n",
      "Batch 177, Loss: 0.845382, Accuracy: 90.02%\n",
      "Batch 178, Loss: 0.907269, Accuracy: 89.97%\n",
      "Batch 179, Loss: 0.818397, Accuracy: 89.99%\n",
      "Batch 180, Loss: 0.837720, Accuracy: 89.99%\n",
      "Batch 181, Loss: 0.878674, Accuracy: 89.96%\n",
      "Batch 182, Loss: 0.825343, Accuracy: 89.98%\n",
      "Batch 183, Loss: 0.889731, Accuracy: 89.95%\n",
      "Batch 184, Loss: 0.793914, Accuracy: 89.99%\n",
      "Batch 185, Loss: 0.866355, Accuracy: 89.97%\n",
      "Batch 186, Loss: 0.813676, Accuracy: 89.99%\n",
      "Batch 187, Loss: 0.857580, Accuracy: 89.99%\n",
      "Batch 188, Loss: 0.801246, Accuracy: 90.02%\n",
      "Batch 189, Loss: 0.800323, Accuracy: 90.04%\n",
      "Batch 190, Loss: 0.857007, Accuracy: 90.03%\n",
      "Batch 191, Loss: 0.809705, Accuracy: 90.04%\n",
      "Batch 192, Loss: 0.853992, Accuracy: 90.04%\n",
      "Batch 193, Loss: 0.873905, Accuracy: 90.03%\n",
      "Batch 194, Loss: 0.888908, Accuracy: 90.00%\n",
      "Batch 195, Loss: 0.799139, Accuracy: 90.03%\n",
      "Batch 196, Loss: 0.851147, Accuracy: 90.03%\n",
      "Batch 197, Loss: 0.811895, Accuracy: 90.05%\n",
      "Batch 198, Loss: 0.851018, Accuracy: 90.04%\n",
      "Batch 199, Loss: 0.882245, Accuracy: 90.02%\n",
      "Batch 200, Loss: 0.866619, Accuracy: 90.01%\n",
      "Batch 201, Loss: 0.790974, Accuracy: 90.03%\n",
      "Batch 202, Loss: 0.819691, Accuracy: 90.04%\n",
      "Batch 203, Loss: 0.901381, Accuracy: 90.02%\n",
      "Batch 204, Loss: 0.844216, Accuracy: 90.02%\n",
      "Batch 205, Loss: 0.831434, Accuracy: 90.03%\n",
      "Batch 206, Loss: 0.825015, Accuracy: 90.04%\n",
      "Batch 207, Loss: 0.824096, Accuracy: 90.06%\n",
      "Batch 208, Loss: 0.879854, Accuracy: 90.04%\n",
      "Batch 209, Loss: 0.859855, Accuracy: 90.03%\n",
      "Batch 210, Loss: 0.877210, Accuracy: 90.01%\n",
      "Batch 211, Loss: 0.828059, Accuracy: 90.03%\n",
      "Batch 212, Loss: 0.863940, Accuracy: 90.02%\n",
      "Batch 213, Loss: 0.832632, Accuracy: 90.02%\n",
      "Training - Epoch 6, Loss: 0.844696, Accuracy: 90.02%\n",
      "Validation Batch 1, Loss: 0.819102, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.791677, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.815089, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.834622, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.816326, Accuracy: 93.44%\n",
      "Validation Batch 6, Loss: 0.792327, Accuracy: 93.75%\n",
      "Validation Batch 7, Loss: 0.794490, Accuracy: 93.97%\n",
      "Validation Batch 8, Loss: 0.855315, Accuracy: 93.55%\n",
      "Validation Batch 9, Loss: 0.887098, Accuracy: 92.53%\n",
      "Validation Batch 10, Loss: 0.786830, Accuracy: 92.97%\n",
      "Validation Batch 11, Loss: 0.825850, Accuracy: 92.90%\n",
      "Validation Batch 12, Loss: 0.826680, Accuracy: 92.84%\n",
      "Validation Batch 13, Loss: 0.832877, Accuracy: 92.67%\n",
      "Validation Batch 14, Loss: 0.833096, Accuracy: 92.63%\n",
      "Validation Batch 15, Loss: 0.797719, Accuracy: 92.71%\n",
      "Validation Batch 16, Loss: 0.837278, Accuracy: 92.58%\n",
      "Validation Batch 17, Loss: 0.856275, Accuracy: 92.37%\n",
      "Validation Batch 18, Loss: 0.796942, Accuracy: 92.53%\n",
      "Validation Batch 19, Loss: 0.845956, Accuracy: 92.43%\n",
      "Validation Batch 20, Loss: 0.815023, Accuracy: 92.58%\n",
      "Validation Batch 21, Loss: 0.858843, Accuracy: 92.41%\n",
      "Validation Batch 22, Loss: 0.794486, Accuracy: 92.61%\n",
      "Validation Batch 23, Loss: 0.859704, Accuracy: 92.46%\n",
      "Validation Batch 24, Loss: 0.827231, Accuracy: 92.45%\n",
      "Validation Batch 25, Loss: 0.795070, Accuracy: 92.56%\n",
      "Validation Batch 26, Loss: 0.836029, Accuracy: 92.49%\n",
      "Validation Batch 27, Loss: 0.792218, Accuracy: 92.54%\n",
      "Validation - Epoch 6, Loss: 0.823117, Accuracy: 92.54%\n",
      "Patience—6\n",
      "Epoch 7\n",
      "Batch 1, Loss: 0.833409, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.857351, Accuracy: 89.06%\n",
      "Batch 3, Loss: 0.856861, Accuracy: 89.06%\n",
      "Batch 4, Loss: 0.903982, Accuracy: 87.50%\n",
      "Batch 5, Loss: 0.787309, Accuracy: 89.38%\n",
      "Batch 6, Loss: 0.871133, Accuracy: 89.06%\n",
      "Batch 7, Loss: 0.862970, Accuracy: 88.84%\n",
      "Batch 8, Loss: 0.864741, Accuracy: 88.67%\n",
      "Batch 9, Loss: 0.829538, Accuracy: 89.06%\n",
      "Batch 10, Loss: 0.839340, Accuracy: 89.22%\n",
      "Batch 11, Loss: 0.798045, Accuracy: 89.77%\n",
      "Batch 12, Loss: 0.827745, Accuracy: 89.97%\n",
      "Batch 13, Loss: 0.796330, Accuracy: 90.38%\n",
      "Batch 14, Loss: 0.781325, Accuracy: 90.85%\n",
      "Batch 15, Loss: 0.814543, Accuracy: 90.94%\n",
      "Batch 16, Loss: 0.890888, Accuracy: 90.43%\n",
      "Batch 17, Loss: 0.789888, Accuracy: 90.72%\n",
      "Batch 18, Loss: 0.859075, Accuracy: 90.62%\n",
      "Batch 19, Loss: 0.813022, Accuracy: 90.71%\n",
      "Batch 20, Loss: 0.851097, Accuracy: 90.62%\n",
      "Batch 21, Loss: 0.852047, Accuracy: 90.62%\n",
      "Batch 22, Loss: 0.874804, Accuracy: 90.48%\n",
      "Batch 23, Loss: 0.784064, Accuracy: 90.69%\n",
      "Batch 24, Loss: 0.881856, Accuracy: 90.49%\n",
      "Batch 25, Loss: 0.835501, Accuracy: 90.56%\n",
      "Batch 26, Loss: 0.845976, Accuracy: 90.56%\n",
      "Batch 27, Loss: 0.844867, Accuracy: 90.57%\n",
      "Batch 28, Loss: 0.837789, Accuracy: 90.57%\n",
      "Batch 29, Loss: 0.898930, Accuracy: 90.36%\n",
      "Batch 30, Loss: 0.859573, Accuracy: 90.31%\n",
      "Batch 31, Loss: 0.887369, Accuracy: 90.22%\n",
      "Batch 32, Loss: 0.854369, Accuracy: 90.23%\n",
      "Batch 33, Loss: 0.887678, Accuracy: 90.10%\n",
      "Batch 34, Loss: 0.882665, Accuracy: 89.94%\n",
      "Batch 35, Loss: 0.854141, Accuracy: 89.91%\n",
      "Batch 36, Loss: 0.795103, Accuracy: 90.06%\n",
      "Batch 37, Loss: 0.820142, Accuracy: 90.12%\n",
      "Batch 38, Loss: 0.798950, Accuracy: 90.25%\n",
      "Batch 39, Loss: 0.815549, Accuracy: 90.34%\n",
      "Batch 40, Loss: 0.839322, Accuracy: 90.35%\n",
      "Batch 41, Loss: 0.804599, Accuracy: 90.47%\n",
      "Batch 42, Loss: 0.821657, Accuracy: 90.51%\n",
      "Batch 43, Loss: 0.828557, Accuracy: 90.52%\n",
      "Batch 44, Loss: 0.883785, Accuracy: 90.41%\n",
      "Batch 45, Loss: 0.816736, Accuracy: 90.45%\n",
      "Batch 46, Loss: 0.898999, Accuracy: 90.29%\n",
      "Batch 47, Loss: 0.821187, Accuracy: 90.36%\n",
      "Batch 48, Loss: 0.951820, Accuracy: 90.14%\n",
      "Batch 49, Loss: 0.850497, Accuracy: 90.15%\n",
      "Batch 50, Loss: 0.814839, Accuracy: 90.22%\n",
      "Batch 51, Loss: 0.844321, Accuracy: 90.20%\n",
      "Batch 52, Loss: 0.853837, Accuracy: 90.20%\n",
      "Batch 53, Loss: 0.867959, Accuracy: 90.15%\n",
      "Batch 54, Loss: 0.801467, Accuracy: 90.25%\n",
      "Batch 55, Loss: 0.833559, Accuracy: 90.26%\n",
      "Batch 56, Loss: 0.824810, Accuracy: 90.29%\n",
      "Batch 57, Loss: 0.846374, Accuracy: 90.27%\n",
      "Batch 58, Loss: 0.887951, Accuracy: 90.19%\n",
      "Batch 59, Loss: 0.803788, Accuracy: 90.25%\n",
      "Batch 60, Loss: 0.849793, Accuracy: 90.26%\n",
      "Batch 61, Loss: 0.840493, Accuracy: 90.27%\n",
      "Batch 62, Loss: 0.894800, Accuracy: 90.17%\n",
      "Batch 63, Loss: 0.890178, Accuracy: 90.08%\n",
      "Batch 64, Loss: 0.877055, Accuracy: 90.01%\n",
      "Batch 65, Loss: 0.844710, Accuracy: 90.00%\n",
      "Batch 66, Loss: 0.829535, Accuracy: 90.03%\n",
      "Batch 67, Loss: 0.829205, Accuracy: 90.07%\n",
      "Batch 68, Loss: 0.860102, Accuracy: 90.03%\n",
      "Batch 69, Loss: 0.877814, Accuracy: 89.99%\n",
      "Batch 70, Loss: 0.855570, Accuracy: 89.98%\n",
      "Batch 71, Loss: 0.836822, Accuracy: 89.99%\n",
      "Batch 72, Loss: 0.852514, Accuracy: 89.97%\n",
      "Batch 73, Loss: 0.856653, Accuracy: 89.96%\n",
      "Batch 74, Loss: 0.835112, Accuracy: 89.97%\n",
      "Batch 75, Loss: 0.831177, Accuracy: 90.02%\n",
      "Batch 76, Loss: 0.847398, Accuracy: 90.03%\n",
      "Batch 77, Loss: 0.888211, Accuracy: 89.96%\n",
      "Batch 78, Loss: 0.897443, Accuracy: 89.88%\n",
      "Batch 79, Loss: 0.901086, Accuracy: 89.79%\n",
      "Batch 80, Loss: 0.854682, Accuracy: 89.79%\n",
      "Batch 81, Loss: 0.840357, Accuracy: 89.80%\n",
      "Batch 82, Loss: 0.827187, Accuracy: 89.82%\n",
      "Batch 83, Loss: 0.847422, Accuracy: 89.83%\n",
      "Batch 84, Loss: 0.843524, Accuracy: 89.83%\n",
      "Batch 85, Loss: 0.796412, Accuracy: 89.89%\n",
      "Batch 86, Loss: 0.871721, Accuracy: 89.84%\n",
      "Batch 87, Loss: 0.887134, Accuracy: 89.82%\n",
      "Batch 88, Loss: 0.865322, Accuracy: 89.79%\n",
      "Batch 89, Loss: 0.781956, Accuracy: 89.87%\n",
      "Batch 90, Loss: 0.814982, Accuracy: 89.90%\n",
      "Batch 91, Loss: 0.843892, Accuracy: 89.90%\n",
      "Batch 92, Loss: 0.914566, Accuracy: 89.83%\n",
      "Batch 93, Loss: 0.805630, Accuracy: 89.87%\n",
      "Batch 94, Loss: 0.924221, Accuracy: 89.78%\n",
      "Batch 95, Loss: 0.832538, Accuracy: 89.79%\n",
      "Batch 96, Loss: 0.884604, Accuracy: 89.73%\n",
      "Batch 97, Loss: 0.810868, Accuracy: 89.76%\n",
      "Batch 98, Loss: 0.859398, Accuracy: 89.76%\n",
      "Batch 99, Loss: 0.884697, Accuracy: 89.73%\n",
      "Batch 100, Loss: 0.820624, Accuracy: 89.73%\n",
      "Batch 101, Loss: 0.846911, Accuracy: 89.73%\n",
      "Batch 102, Loss: 0.850052, Accuracy: 89.72%\n",
      "Batch 103, Loss: 0.932921, Accuracy: 89.64%\n",
      "Batch 104, Loss: 0.837926, Accuracy: 89.65%\n",
      "Batch 105, Loss: 0.876460, Accuracy: 89.61%\n",
      "Batch 106, Loss: 0.815825, Accuracy: 89.65%\n",
      "Batch 107, Loss: 0.882722, Accuracy: 89.62%\n",
      "Batch 108, Loss: 0.905394, Accuracy: 89.54%\n",
      "Batch 109, Loss: 0.831357, Accuracy: 89.56%\n",
      "Batch 110, Loss: 0.832841, Accuracy: 89.57%\n",
      "Batch 111, Loss: 0.859626, Accuracy: 89.56%\n",
      "Batch 112, Loss: 0.934894, Accuracy: 89.48%\n",
      "Batch 113, Loss: 0.876366, Accuracy: 89.46%\n",
      "Batch 114, Loss: 0.850741, Accuracy: 89.46%\n",
      "Batch 115, Loss: 0.838741, Accuracy: 89.47%\n",
      "Batch 116, Loss: 0.793753, Accuracy: 89.52%\n",
      "Batch 117, Loss: 0.880846, Accuracy: 89.49%\n",
      "Batch 118, Loss: 0.890586, Accuracy: 89.46%\n",
      "Batch 119, Loss: 0.792349, Accuracy: 89.51%\n",
      "Batch 120, Loss: 0.852338, Accuracy: 89.51%\n",
      "Batch 121, Loss: 0.808379, Accuracy: 89.55%\n",
      "Batch 122, Loss: 0.856399, Accuracy: 89.55%\n",
      "Batch 123, Loss: 0.879542, Accuracy: 89.51%\n",
      "Batch 124, Loss: 0.864537, Accuracy: 89.49%\n",
      "Batch 125, Loss: 0.875899, Accuracy: 89.47%\n",
      "Batch 126, Loss: 0.838560, Accuracy: 89.50%\n",
      "Batch 127, Loss: 0.782492, Accuracy: 89.55%\n",
      "Batch 128, Loss: 0.806775, Accuracy: 89.59%\n",
      "Batch 129, Loss: 0.816618, Accuracy: 89.61%\n",
      "Batch 130, Loss: 0.818955, Accuracy: 89.63%\n",
      "Batch 131, Loss: 0.808993, Accuracy: 89.66%\n",
      "Batch 132, Loss: 0.898879, Accuracy: 89.61%\n",
      "Batch 133, Loss: 0.830492, Accuracy: 89.63%\n",
      "Batch 134, Loss: 0.908974, Accuracy: 89.58%\n",
      "Batch 135, Loss: 0.805181, Accuracy: 89.61%\n",
      "Batch 136, Loss: 0.853565, Accuracy: 89.60%\n",
      "Batch 137, Loss: 0.852113, Accuracy: 89.59%\n",
      "Batch 138, Loss: 0.876889, Accuracy: 89.57%\n",
      "Batch 139, Loss: 0.854591, Accuracy: 89.57%\n",
      "Batch 140, Loss: 0.848381, Accuracy: 89.58%\n",
      "Batch 141, Loss: 0.887623, Accuracy: 89.56%\n",
      "Batch 142, Loss: 0.820174, Accuracy: 89.58%\n",
      "Batch 143, Loss: 0.789413, Accuracy: 89.62%\n",
      "Batch 144, Loss: 0.851304, Accuracy: 89.61%\n",
      "Batch 145, Loss: 0.847949, Accuracy: 89.62%\n",
      "Batch 146, Loss: 0.867958, Accuracy: 89.61%\n",
      "Batch 147, Loss: 0.829434, Accuracy: 89.62%\n",
      "Batch 148, Loss: 0.826533, Accuracy: 89.63%\n",
      "Batch 149, Loss: 0.801992, Accuracy: 89.66%\n",
      "Batch 150, Loss: 0.856004, Accuracy: 89.66%\n",
      "Batch 151, Loss: 0.909261, Accuracy: 89.62%\n",
      "Batch 152, Loss: 0.852180, Accuracy: 89.61%\n",
      "Batch 153, Loss: 0.826043, Accuracy: 89.62%\n",
      "Batch 154, Loss: 0.810353, Accuracy: 89.65%\n",
      "Batch 155, Loss: 0.874802, Accuracy: 89.63%\n",
      "Batch 156, Loss: 0.784147, Accuracy: 89.66%\n",
      "Batch 157, Loss: 0.870307, Accuracy: 89.65%\n",
      "Batch 158, Loss: 0.893249, Accuracy: 89.63%\n",
      "Batch 159, Loss: 0.852829, Accuracy: 89.61%\n",
      "Batch 160, Loss: 0.861966, Accuracy: 89.60%\n",
      "Batch 161, Loss: 0.785412, Accuracy: 89.64%\n",
      "Batch 162, Loss: 0.826999, Accuracy: 89.67%\n",
      "Batch 163, Loss: 0.877654, Accuracy: 89.65%\n",
      "Batch 164, Loss: 0.864558, Accuracy: 89.64%\n",
      "Batch 165, Loss: 0.867994, Accuracy: 89.63%\n",
      "Batch 166, Loss: 0.830938, Accuracy: 89.64%\n",
      "Batch 167, Loss: 0.839065, Accuracy: 89.63%\n",
      "Batch 168, Loss: 0.801327, Accuracy: 89.66%\n",
      "Batch 169, Loss: 0.861147, Accuracy: 89.64%\n",
      "Batch 170, Loss: 0.839463, Accuracy: 89.65%\n",
      "Batch 171, Loss: 0.769468, Accuracy: 89.70%\n",
      "Batch 172, Loss: 0.880914, Accuracy: 89.68%\n",
      "Batch 173, Loss: 0.897989, Accuracy: 89.64%\n",
      "Batch 174, Loss: 0.807979, Accuracy: 89.66%\n",
      "Batch 175, Loss: 0.910268, Accuracy: 89.62%\n",
      "Batch 176, Loss: 0.857451, Accuracy: 89.61%\n",
      "Batch 177, Loss: 0.921796, Accuracy: 89.57%\n",
      "Batch 178, Loss: 0.829938, Accuracy: 89.58%\n",
      "Batch 179, Loss: 0.822472, Accuracy: 89.60%\n",
      "Batch 180, Loss: 0.771621, Accuracy: 89.64%\n",
      "Batch 181, Loss: 0.820372, Accuracy: 89.67%\n",
      "Batch 182, Loss: 0.815551, Accuracy: 89.69%\n",
      "Batch 183, Loss: 0.824214, Accuracy: 89.70%\n",
      "Batch 184, Loss: 0.843654, Accuracy: 89.71%\n",
      "Batch 185, Loss: 0.874495, Accuracy: 89.70%\n",
      "Batch 186, Loss: 0.850651, Accuracy: 89.69%\n",
      "Batch 187, Loss: 0.820916, Accuracy: 89.71%\n",
      "Batch 188, Loss: 0.865184, Accuracy: 89.70%\n",
      "Batch 189, Loss: 0.877299, Accuracy: 89.69%\n",
      "Batch 190, Loss: 0.833629, Accuracy: 89.70%\n",
      "Batch 191, Loss: 0.821459, Accuracy: 89.71%\n",
      "Batch 192, Loss: 0.801455, Accuracy: 89.74%\n",
      "Batch 193, Loss: 0.834269, Accuracy: 89.75%\n",
      "Batch 194, Loss: 0.806382, Accuracy: 89.77%\n",
      "Batch 195, Loss: 0.853689, Accuracy: 89.76%\n",
      "Batch 196, Loss: 0.803853, Accuracy: 89.78%\n",
      "Batch 197, Loss: 0.898406, Accuracy: 89.75%\n",
      "Batch 198, Loss: 0.802421, Accuracy: 89.78%\n",
      "Batch 199, Loss: 0.801528, Accuracy: 89.81%\n",
      "Batch 200, Loss: 0.877436, Accuracy: 89.79%\n",
      "Batch 201, Loss: 0.868441, Accuracy: 89.78%\n",
      "Batch 202, Loss: 0.814853, Accuracy: 89.79%\n",
      "Batch 203, Loss: 0.895186, Accuracy: 89.76%\n",
      "Batch 204, Loss: 0.898372, Accuracy: 89.72%\n",
      "Batch 205, Loss: 0.869298, Accuracy: 89.71%\n",
      "Batch 206, Loss: 0.836941, Accuracy: 89.71%\n",
      "Batch 207, Loss: 0.831741, Accuracy: 89.73%\n",
      "Batch 208, Loss: 0.908130, Accuracy: 89.69%\n",
      "Batch 209, Loss: 0.876652, Accuracy: 89.68%\n",
      "Batch 210, Loss: 0.856779, Accuracy: 89.67%\n",
      "Batch 211, Loss: 0.826384, Accuracy: 89.68%\n",
      "Batch 212, Loss: 0.905768, Accuracy: 89.66%\n",
      "Batch 213, Loss: 0.824995, Accuracy: 89.66%\n",
      "Training - Epoch 7, Loss: 0.847562, Accuracy: 89.66%\n",
      "Validation Batch 1, Loss: 0.819459, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.791990, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.815798, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.834795, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.817626, Accuracy: 93.12%\n",
      "Validation Batch 6, Loss: 0.792784, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.794517, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.855496, Accuracy: 93.16%\n",
      "Validation Batch 9, Loss: 0.887586, Accuracy: 92.19%\n",
      "Validation Batch 10, Loss: 0.787442, Accuracy: 92.66%\n",
      "Validation Batch 11, Loss: 0.826764, Accuracy: 92.61%\n",
      "Validation Batch 12, Loss: 0.826684, Accuracy: 92.58%\n",
      "Validation Batch 13, Loss: 0.833274, Accuracy: 92.43%\n",
      "Validation Batch 14, Loss: 0.833383, Accuracy: 92.41%\n",
      "Validation Batch 15, Loss: 0.797870, Accuracy: 92.50%\n",
      "Validation Batch 16, Loss: 0.837295, Accuracy: 92.38%\n",
      "Validation Batch 17, Loss: 0.856380, Accuracy: 92.19%\n",
      "Validation Batch 18, Loss: 0.797246, Accuracy: 92.36%\n",
      "Validation Batch 19, Loss: 0.845591, Accuracy: 92.27%\n",
      "Validation Batch 20, Loss: 0.815882, Accuracy: 92.42%\n",
      "Validation Batch 21, Loss: 0.858883, Accuracy: 92.26%\n",
      "Validation Batch 22, Loss: 0.794498, Accuracy: 92.47%\n",
      "Validation Batch 23, Loss: 0.859537, Accuracy: 92.32%\n",
      "Validation Batch 24, Loss: 0.827873, Accuracy: 92.32%\n",
      "Validation Batch 25, Loss: 0.795567, Accuracy: 92.44%\n",
      "Validation Batch 26, Loss: 0.835769, Accuracy: 92.37%\n",
      "Validation Batch 27, Loss: 0.792610, Accuracy: 92.43%\n",
      "Validation - Epoch 7, Loss: 0.823429, Accuracy: 92.43%\n",
      "Patience—7\n",
      "Epoch 8\n",
      "Batch 1, Loss: 0.890544, Accuracy: 84.38%\n",
      "Batch 2, Loss: 0.792087, Accuracy: 89.84%\n",
      "Batch 3, Loss: 0.820685, Accuracy: 90.10%\n",
      "Batch 4, Loss: 0.889219, Accuracy: 88.67%\n",
      "Batch 5, Loss: 0.837339, Accuracy: 89.06%\n",
      "Batch 6, Loss: 0.854108, Accuracy: 89.06%\n",
      "Batch 7, Loss: 0.792277, Accuracy: 89.96%\n",
      "Batch 8, Loss: 0.801602, Accuracy: 90.62%\n",
      "Batch 9, Loss: 0.843158, Accuracy: 90.62%\n",
      "Batch 10, Loss: 0.843249, Accuracy: 90.62%\n",
      "Batch 11, Loss: 0.870418, Accuracy: 90.20%\n",
      "Batch 12, Loss: 0.877742, Accuracy: 89.84%\n",
      "Batch 13, Loss: 0.787162, Accuracy: 90.26%\n",
      "Batch 14, Loss: 0.819809, Accuracy: 90.29%\n",
      "Batch 15, Loss: 0.819338, Accuracy: 90.42%\n",
      "Batch 16, Loss: 0.856250, Accuracy: 90.23%\n",
      "Batch 17, Loss: 0.890653, Accuracy: 89.98%\n",
      "Batch 18, Loss: 0.814620, Accuracy: 90.10%\n",
      "Batch 19, Loss: 0.798719, Accuracy: 90.38%\n",
      "Batch 20, Loss: 0.877498, Accuracy: 90.16%\n",
      "Batch 21, Loss: 0.837699, Accuracy: 90.18%\n",
      "Batch 22, Loss: 0.960823, Accuracy: 89.63%\n",
      "Batch 23, Loss: 0.904430, Accuracy: 89.40%\n",
      "Batch 24, Loss: 0.854258, Accuracy: 89.32%\n",
      "Batch 25, Loss: 0.913311, Accuracy: 89.06%\n",
      "Batch 26, Loss: 0.798005, Accuracy: 89.24%\n",
      "Batch 27, Loss: 0.824376, Accuracy: 89.29%\n",
      "Batch 28, Loss: 0.879106, Accuracy: 89.17%\n",
      "Batch 29, Loss: 0.850338, Accuracy: 89.17%\n",
      "Batch 30, Loss: 0.911275, Accuracy: 88.91%\n",
      "Batch 31, Loss: 0.843217, Accuracy: 88.96%\n",
      "Batch 32, Loss: 0.815395, Accuracy: 89.06%\n",
      "Batch 33, Loss: 0.887420, Accuracy: 88.97%\n",
      "Batch 34, Loss: 0.880286, Accuracy: 88.88%\n",
      "Batch 35, Loss: 0.855321, Accuracy: 88.88%\n",
      "Batch 36, Loss: 0.899182, Accuracy: 88.72%\n",
      "Batch 37, Loss: 0.797988, Accuracy: 88.85%\n",
      "Batch 38, Loss: 0.832748, Accuracy: 88.90%\n",
      "Batch 39, Loss: 0.880544, Accuracy: 88.86%\n",
      "Batch 40, Loss: 0.885295, Accuracy: 88.75%\n",
      "Batch 41, Loss: 0.839768, Accuracy: 88.80%\n",
      "Batch 42, Loss: 0.817952, Accuracy: 88.88%\n",
      "Batch 43, Loss: 0.830262, Accuracy: 88.95%\n",
      "Batch 44, Loss: 0.836609, Accuracy: 88.99%\n",
      "Batch 45, Loss: 0.814139, Accuracy: 89.10%\n",
      "Batch 46, Loss: 0.808353, Accuracy: 89.20%\n",
      "Batch 47, Loss: 0.818270, Accuracy: 89.26%\n",
      "Batch 48, Loss: 0.894151, Accuracy: 89.13%\n",
      "Batch 49, Loss: 0.841174, Accuracy: 89.16%\n",
      "Batch 50, Loss: 0.856714, Accuracy: 89.16%\n",
      "Batch 51, Loss: 0.902733, Accuracy: 89.03%\n",
      "Batch 52, Loss: 0.777667, Accuracy: 89.18%\n",
      "Batch 53, Loss: 0.808512, Accuracy: 89.27%\n",
      "Batch 54, Loss: 0.860961, Accuracy: 89.24%\n",
      "Batch 55, Loss: 0.766330, Accuracy: 89.40%\n",
      "Batch 56, Loss: 0.871243, Accuracy: 89.34%\n",
      "Batch 57, Loss: 0.876450, Accuracy: 89.31%\n",
      "Batch 58, Loss: 0.874291, Accuracy: 89.25%\n",
      "Batch 59, Loss: 0.844855, Accuracy: 89.27%\n",
      "Batch 60, Loss: 0.824070, Accuracy: 89.30%\n",
      "Batch 61, Loss: 0.806214, Accuracy: 89.34%\n",
      "Batch 62, Loss: 0.899174, Accuracy: 89.29%\n",
      "Batch 63, Loss: 0.814460, Accuracy: 89.36%\n",
      "Batch 64, Loss: 0.824230, Accuracy: 89.40%\n",
      "Batch 65, Loss: 0.795869, Accuracy: 89.52%\n",
      "Batch 66, Loss: 0.866557, Accuracy: 89.49%\n",
      "Batch 67, Loss: 0.880392, Accuracy: 89.46%\n",
      "Batch 68, Loss: 0.924638, Accuracy: 89.34%\n",
      "Batch 69, Loss: 0.856588, Accuracy: 89.33%\n",
      "Batch 70, Loss: 0.879833, Accuracy: 89.29%\n",
      "Batch 71, Loss: 0.808963, Accuracy: 89.35%\n",
      "Batch 72, Loss: 0.822571, Accuracy: 89.39%\n",
      "Batch 73, Loss: 0.841556, Accuracy: 89.40%\n",
      "Batch 74, Loss: 0.823773, Accuracy: 89.44%\n",
      "Batch 75, Loss: 0.769783, Accuracy: 89.56%\n",
      "Batch 76, Loss: 0.820036, Accuracy: 89.60%\n",
      "Batch 77, Loss: 0.795714, Accuracy: 89.67%\n",
      "Batch 78, Loss: 0.861002, Accuracy: 89.66%\n",
      "Batch 79, Loss: 0.851691, Accuracy: 89.66%\n",
      "Batch 80, Loss: 0.843300, Accuracy: 89.67%\n",
      "Batch 81, Loss: 0.860747, Accuracy: 89.66%\n",
      "Batch 82, Loss: 0.848733, Accuracy: 89.67%\n",
      "Batch 83, Loss: 0.889039, Accuracy: 89.63%\n",
      "Batch 84, Loss: 0.827013, Accuracy: 89.66%\n",
      "Batch 85, Loss: 0.801747, Accuracy: 89.71%\n",
      "Batch 86, Loss: 0.903113, Accuracy: 89.63%\n",
      "Batch 87, Loss: 0.805516, Accuracy: 89.67%\n",
      "Batch 88, Loss: 0.841328, Accuracy: 89.68%\n",
      "Batch 89, Loss: 0.807074, Accuracy: 89.73%\n",
      "Batch 90, Loss: 0.817243, Accuracy: 89.77%\n",
      "Batch 91, Loss: 0.802193, Accuracy: 89.84%\n",
      "Batch 92, Loss: 0.913131, Accuracy: 89.76%\n",
      "Batch 93, Loss: 0.870651, Accuracy: 89.72%\n",
      "Batch 94, Loss: 0.898476, Accuracy: 89.66%\n",
      "Batch 95, Loss: 0.918050, Accuracy: 89.59%\n",
      "Batch 96, Loss: 0.870602, Accuracy: 89.58%\n",
      "Batch 97, Loss: 0.862951, Accuracy: 89.56%\n",
      "Batch 98, Loss: 0.771775, Accuracy: 89.64%\n",
      "Batch 99, Loss: 0.799304, Accuracy: 89.69%\n",
      "Batch 100, Loss: 0.803790, Accuracy: 89.73%\n",
      "Batch 101, Loss: 0.889390, Accuracy: 89.68%\n",
      "Batch 102, Loss: 0.869112, Accuracy: 89.66%\n",
      "Batch 103, Loss: 0.855308, Accuracy: 89.65%\n",
      "Batch 104, Loss: 0.845263, Accuracy: 89.65%\n",
      "Batch 105, Loss: 0.836615, Accuracy: 89.66%\n",
      "Batch 106, Loss: 0.892873, Accuracy: 89.62%\n",
      "Batch 107, Loss: 0.811917, Accuracy: 89.66%\n",
      "Batch 108, Loss: 0.819775, Accuracy: 89.68%\n",
      "Batch 109, Loss: 0.838904, Accuracy: 89.69%\n",
      "Batch 110, Loss: 0.860222, Accuracy: 89.67%\n",
      "Batch 111, Loss: 0.884608, Accuracy: 89.63%\n",
      "Batch 112, Loss: 0.815579, Accuracy: 89.66%\n",
      "Batch 113, Loss: 0.809829, Accuracy: 89.70%\n",
      "Batch 114, Loss: 0.859390, Accuracy: 89.71%\n",
      "Batch 115, Loss: 0.852516, Accuracy: 89.69%\n",
      "Batch 116, Loss: 0.902417, Accuracy: 89.64%\n",
      "Batch 117, Loss: 0.808340, Accuracy: 89.68%\n",
      "Batch 118, Loss: 0.800410, Accuracy: 89.72%\n",
      "Batch 119, Loss: 0.766013, Accuracy: 89.80%\n",
      "Batch 120, Loss: 0.833828, Accuracy: 89.80%\n",
      "Batch 121, Loss: 0.848227, Accuracy: 89.79%\n",
      "Batch 122, Loss: 0.909093, Accuracy: 89.73%\n",
      "Batch 123, Loss: 0.864008, Accuracy: 89.70%\n",
      "Batch 124, Loss: 0.862675, Accuracy: 89.68%\n",
      "Batch 125, Loss: 0.851392, Accuracy: 89.67%\n",
      "Batch 126, Loss: 0.820554, Accuracy: 89.69%\n",
      "Batch 127, Loss: 0.810372, Accuracy: 89.73%\n",
      "Batch 128, Loss: 0.807572, Accuracy: 89.76%\n",
      "Batch 129, Loss: 0.788779, Accuracy: 89.81%\n",
      "Batch 130, Loss: 0.850914, Accuracy: 89.82%\n",
      "Batch 131, Loss: 0.829336, Accuracy: 89.84%\n",
      "Batch 132, Loss: 0.883768, Accuracy: 89.81%\n",
      "Batch 133, Loss: 0.821630, Accuracy: 89.84%\n",
      "Batch 134, Loss: 0.830404, Accuracy: 89.86%\n",
      "Batch 135, Loss: 0.845277, Accuracy: 89.85%\n",
      "Batch 136, Loss: 0.845465, Accuracy: 89.86%\n",
      "Batch 137, Loss: 0.788646, Accuracy: 89.90%\n",
      "Batch 138, Loss: 0.843101, Accuracy: 89.89%\n",
      "Batch 139, Loss: 0.840843, Accuracy: 89.89%\n",
      "Batch 140, Loss: 0.854656, Accuracy: 89.90%\n",
      "Batch 141, Loss: 0.852533, Accuracy: 89.89%\n",
      "Batch 142, Loss: 0.843190, Accuracy: 89.90%\n",
      "Batch 143, Loss: 0.822001, Accuracy: 89.91%\n",
      "Batch 144, Loss: 0.811637, Accuracy: 89.94%\n",
      "Batch 145, Loss: 0.806840, Accuracy: 89.97%\n",
      "Batch 146, Loss: 0.851978, Accuracy: 89.97%\n",
      "Batch 147, Loss: 0.838808, Accuracy: 89.98%\n",
      "Batch 148, Loss: 0.884438, Accuracy: 89.95%\n",
      "Batch 149, Loss: 0.891698, Accuracy: 89.91%\n",
      "Batch 150, Loss: 0.853528, Accuracy: 89.91%\n",
      "Batch 151, Loss: 0.803283, Accuracy: 89.94%\n",
      "Batch 152, Loss: 0.813353, Accuracy: 89.97%\n",
      "Batch 153, Loss: 0.816714, Accuracy: 89.98%\n",
      "Batch 154, Loss: 0.887346, Accuracy: 89.96%\n",
      "Batch 155, Loss: 0.783678, Accuracy: 90.00%\n",
      "Batch 156, Loss: 0.876766, Accuracy: 89.98%\n",
      "Batch 157, Loss: 0.909406, Accuracy: 89.95%\n",
      "Batch 158, Loss: 0.895975, Accuracy: 89.92%\n",
      "Batch 159, Loss: 0.861841, Accuracy: 89.91%\n",
      "Batch 160, Loss: 0.850076, Accuracy: 89.90%\n",
      "Batch 161, Loss: 0.789165, Accuracy: 89.95%\n",
      "Batch 162, Loss: 0.865301, Accuracy: 89.93%\n",
      "Batch 163, Loss: 0.872853, Accuracy: 89.93%\n",
      "Batch 164, Loss: 0.814511, Accuracy: 89.95%\n",
      "Batch 165, Loss: 0.866277, Accuracy: 89.93%\n",
      "Batch 166, Loss: 0.810212, Accuracy: 89.97%\n",
      "Batch 167, Loss: 0.844000, Accuracy: 89.96%\n",
      "Batch 168, Loss: 0.822010, Accuracy: 89.97%\n",
      "Batch 169, Loss: 0.828803, Accuracy: 89.98%\n",
      "Batch 170, Loss: 0.821633, Accuracy: 89.99%\n",
      "Batch 171, Loss: 0.853762, Accuracy: 89.99%\n",
      "Batch 172, Loss: 0.831721, Accuracy: 89.99%\n",
      "Batch 173, Loss: 0.877124, Accuracy: 89.97%\n",
      "Batch 174, Loss: 0.796997, Accuracy: 90.00%\n",
      "Batch 175, Loss: 0.927338, Accuracy: 89.95%\n",
      "Batch 176, Loss: 0.908152, Accuracy: 89.91%\n",
      "Batch 177, Loss: 0.852428, Accuracy: 89.92%\n",
      "Batch 178, Loss: 0.878057, Accuracy: 89.90%\n",
      "Batch 179, Loss: 0.932084, Accuracy: 89.85%\n",
      "Batch 180, Loss: 0.844025, Accuracy: 89.84%\n",
      "Batch 181, Loss: 0.811806, Accuracy: 89.86%\n",
      "Batch 182, Loss: 0.942648, Accuracy: 89.81%\n",
      "Batch 183, Loss: 0.802367, Accuracy: 89.83%\n",
      "Batch 184, Loss: 0.827221, Accuracy: 89.84%\n",
      "Batch 185, Loss: 0.820843, Accuracy: 89.86%\n",
      "Batch 186, Loss: 0.814527, Accuracy: 89.88%\n",
      "Batch 187, Loss: 0.889495, Accuracy: 89.85%\n",
      "Batch 188, Loss: 0.867386, Accuracy: 89.84%\n",
      "Batch 189, Loss: 0.871928, Accuracy: 89.82%\n",
      "Batch 190, Loss: 0.858995, Accuracy: 89.82%\n",
      "Batch 191, Loss: 0.786902, Accuracy: 89.86%\n",
      "Batch 192, Loss: 0.875251, Accuracy: 89.84%\n",
      "Batch 193, Loss: 0.886633, Accuracy: 89.81%\n",
      "Batch 194, Loss: 0.839863, Accuracy: 89.80%\n",
      "Batch 195, Loss: 0.875382, Accuracy: 89.78%\n",
      "Batch 196, Loss: 0.875469, Accuracy: 89.78%\n",
      "Batch 197, Loss: 0.830541, Accuracy: 89.78%\n",
      "Batch 198, Loss: 0.821993, Accuracy: 89.79%\n",
      "Batch 199, Loss: 0.856221, Accuracy: 89.78%\n",
      "Batch 200, Loss: 0.870073, Accuracy: 89.77%\n",
      "Batch 201, Loss: 0.862701, Accuracy: 89.76%\n",
      "Batch 202, Loss: 0.953866, Accuracy: 89.70%\n",
      "Batch 203, Loss: 0.822161, Accuracy: 89.71%\n",
      "Batch 204, Loss: 0.815117, Accuracy: 89.73%\n",
      "Batch 205, Loss: 0.861417, Accuracy: 89.72%\n",
      "Batch 206, Loss: 0.866996, Accuracy: 89.71%\n",
      "Batch 207, Loss: 0.880305, Accuracy: 89.70%\n",
      "Batch 208, Loss: 0.804219, Accuracy: 89.72%\n",
      "Batch 209, Loss: 0.882563, Accuracy: 89.71%\n",
      "Batch 210, Loss: 0.884343, Accuracy: 89.69%\n",
      "Batch 211, Loss: 0.846032, Accuracy: 89.68%\n",
      "Batch 212, Loss: 0.907719, Accuracy: 89.65%\n",
      "Batch 213, Loss: 0.854447, Accuracy: 89.65%\n",
      "Training - Epoch 8, Loss: 0.847224, Accuracy: 89.65%\n",
      "Validation Batch 1, Loss: 0.820422, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.791594, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.817000, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.835497, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.819736, Accuracy: 93.12%\n",
      "Validation Batch 6, Loss: 0.793493, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.794848, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.857117, Accuracy: 92.97%\n",
      "Validation Batch 9, Loss: 0.890071, Accuracy: 92.01%\n",
      "Validation Batch 10, Loss: 0.787865, Accuracy: 92.50%\n",
      "Validation Batch 11, Loss: 0.828160, Accuracy: 92.47%\n",
      "Validation Batch 12, Loss: 0.827391, Accuracy: 92.45%\n",
      "Validation Batch 13, Loss: 0.833046, Accuracy: 92.31%\n",
      "Validation Batch 14, Loss: 0.834113, Accuracy: 92.30%\n",
      "Validation Batch 15, Loss: 0.799048, Accuracy: 92.40%\n",
      "Validation Batch 16, Loss: 0.839153, Accuracy: 92.29%\n",
      "Validation Batch 17, Loss: 0.857178, Accuracy: 92.10%\n",
      "Validation Batch 18, Loss: 0.797753, Accuracy: 92.27%\n",
      "Validation Batch 19, Loss: 0.845701, Accuracy: 92.19%\n",
      "Validation Batch 20, Loss: 0.818241, Accuracy: 92.34%\n",
      "Validation Batch 21, Loss: 0.858630, Accuracy: 92.19%\n",
      "Validation Batch 22, Loss: 0.795620, Accuracy: 92.40%\n",
      "Validation Batch 23, Loss: 0.859200, Accuracy: 92.26%\n",
      "Validation Batch 24, Loss: 0.828552, Accuracy: 92.19%\n",
      "Validation Batch 25, Loss: 0.796608, Accuracy: 92.31%\n",
      "Validation Batch 26, Loss: 0.836368, Accuracy: 92.25%\n",
      "Validation Batch 27, Loss: 0.793746, Accuracy: 92.31%\n",
      "Validation - Epoch 8, Loss: 0.824302, Accuracy: 92.31%\n",
      "Patience—8\n",
      "Epoch 9\n",
      "Batch 1, Loss: 0.836592, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.809666, Accuracy: 92.19%\n",
      "Batch 3, Loss: 0.854686, Accuracy: 91.15%\n",
      "Batch 4, Loss: 0.799743, Accuracy: 92.19%\n",
      "Batch 5, Loss: 0.900656, Accuracy: 90.62%\n",
      "Batch 6, Loss: 0.815149, Accuracy: 90.89%\n",
      "Batch 7, Loss: 0.789727, Accuracy: 91.52%\n",
      "Batch 8, Loss: 0.873117, Accuracy: 91.02%\n",
      "Batch 9, Loss: 0.856110, Accuracy: 90.97%\n",
      "Batch 10, Loss: 0.862406, Accuracy: 90.62%\n",
      "Batch 11, Loss: 0.915582, Accuracy: 89.77%\n",
      "Batch 12, Loss: 0.776030, Accuracy: 90.36%\n",
      "Batch 13, Loss: 0.863212, Accuracy: 90.14%\n",
      "Batch 14, Loss: 0.828770, Accuracy: 90.29%\n",
      "Batch 15, Loss: 0.814555, Accuracy: 90.42%\n",
      "Batch 16, Loss: 0.788694, Accuracy: 90.82%\n",
      "Batch 17, Loss: 0.832216, Accuracy: 90.81%\n",
      "Batch 18, Loss: 0.837055, Accuracy: 90.80%\n",
      "Batch 19, Loss: 0.818241, Accuracy: 90.95%\n",
      "Batch 20, Loss: 0.832785, Accuracy: 90.94%\n",
      "Batch 21, Loss: 0.827723, Accuracy: 91.00%\n",
      "Batch 22, Loss: 0.860321, Accuracy: 90.84%\n",
      "Batch 23, Loss: 0.809849, Accuracy: 91.03%\n",
      "Batch 24, Loss: 0.883812, Accuracy: 90.82%\n",
      "Batch 25, Loss: 0.842000, Accuracy: 90.81%\n",
      "Batch 26, Loss: 0.851435, Accuracy: 90.75%\n",
      "Batch 27, Loss: 0.908665, Accuracy: 90.51%\n",
      "Batch 28, Loss: 0.821725, Accuracy: 90.57%\n",
      "Batch 29, Loss: 0.927712, Accuracy: 90.25%\n",
      "Batch 30, Loss: 0.798329, Accuracy: 90.36%\n",
      "Batch 31, Loss: 0.851284, Accuracy: 90.27%\n",
      "Batch 32, Loss: 0.876493, Accuracy: 90.14%\n",
      "Batch 33, Loss: 0.865637, Accuracy: 90.10%\n",
      "Batch 34, Loss: 0.864138, Accuracy: 90.03%\n",
      "Batch 35, Loss: 0.905514, Accuracy: 89.87%\n",
      "Batch 36, Loss: 0.854952, Accuracy: 89.84%\n",
      "Batch 37, Loss: 0.785910, Accuracy: 90.03%\n",
      "Batch 38, Loss: 0.852476, Accuracy: 89.93%\n",
      "Batch 39, Loss: 0.815299, Accuracy: 89.98%\n",
      "Batch 40, Loss: 0.826319, Accuracy: 90.04%\n",
      "Batch 41, Loss: 0.857882, Accuracy: 90.02%\n",
      "Batch 42, Loss: 0.890635, Accuracy: 89.92%\n",
      "Batch 43, Loss: 0.797719, Accuracy: 90.01%\n",
      "Batch 44, Loss: 0.838523, Accuracy: 90.02%\n",
      "Batch 45, Loss: 0.794632, Accuracy: 90.14%\n",
      "Batch 46, Loss: 0.854878, Accuracy: 90.15%\n",
      "Batch 47, Loss: 0.860345, Accuracy: 90.13%\n",
      "Batch 48, Loss: 0.835799, Accuracy: 90.14%\n",
      "Batch 49, Loss: 0.816267, Accuracy: 90.21%\n",
      "Batch 50, Loss: 0.858288, Accuracy: 90.16%\n",
      "Batch 51, Loss: 0.863808, Accuracy: 90.10%\n",
      "Batch 52, Loss: 0.881966, Accuracy: 89.99%\n",
      "Batch 53, Loss: 0.877456, Accuracy: 89.92%\n",
      "Batch 54, Loss: 0.819958, Accuracy: 89.96%\n",
      "Batch 55, Loss: 0.835494, Accuracy: 89.97%\n",
      "Batch 56, Loss: 0.892104, Accuracy: 89.87%\n",
      "Batch 57, Loss: 0.805930, Accuracy: 89.91%\n",
      "Batch 58, Loss: 0.803795, Accuracy: 90.03%\n",
      "Batch 59, Loss: 0.812073, Accuracy: 90.10%\n",
      "Batch 60, Loss: 0.815860, Accuracy: 90.16%\n",
      "Batch 61, Loss: 0.891120, Accuracy: 90.04%\n",
      "Batch 62, Loss: 0.847084, Accuracy: 90.05%\n",
      "Batch 63, Loss: 0.781945, Accuracy: 90.15%\n",
      "Batch 64, Loss: 0.862588, Accuracy: 90.14%\n",
      "Batch 65, Loss: 0.883023, Accuracy: 90.05%\n",
      "Batch 66, Loss: 0.885745, Accuracy: 90.01%\n",
      "Batch 67, Loss: 0.881521, Accuracy: 89.97%\n",
      "Batch 68, Loss: 0.866103, Accuracy: 89.96%\n",
      "Batch 69, Loss: 0.832677, Accuracy: 89.95%\n",
      "Batch 70, Loss: 0.905671, Accuracy: 89.87%\n",
      "Batch 71, Loss: 0.889056, Accuracy: 89.81%\n",
      "Batch 72, Loss: 0.822454, Accuracy: 89.84%\n",
      "Batch 73, Loss: 0.852214, Accuracy: 89.83%\n",
      "Batch 74, Loss: 0.822363, Accuracy: 89.91%\n",
      "Batch 75, Loss: 0.825538, Accuracy: 89.96%\n",
      "Batch 76, Loss: 0.825292, Accuracy: 89.99%\n",
      "Batch 77, Loss: 0.792775, Accuracy: 90.06%\n",
      "Batch 78, Loss: 0.847136, Accuracy: 90.06%\n",
      "Batch 79, Loss: 0.845846, Accuracy: 90.07%\n",
      "Batch 80, Loss: 0.813810, Accuracy: 90.14%\n",
      "Batch 81, Loss: 0.827393, Accuracy: 90.16%\n",
      "Batch 82, Loss: 0.799545, Accuracy: 90.22%\n",
      "Batch 83, Loss: 0.853560, Accuracy: 90.21%\n",
      "Batch 84, Loss: 0.881212, Accuracy: 90.16%\n",
      "Batch 85, Loss: 0.880492, Accuracy: 90.11%\n",
      "Batch 86, Loss: 0.828835, Accuracy: 90.13%\n",
      "Batch 87, Loss: 0.901448, Accuracy: 90.07%\n",
      "Batch 88, Loss: 0.852928, Accuracy: 90.06%\n",
      "Batch 89, Loss: 0.820650, Accuracy: 90.10%\n",
      "Batch 90, Loss: 0.877465, Accuracy: 90.07%\n",
      "Batch 91, Loss: 0.866364, Accuracy: 90.04%\n",
      "Batch 92, Loss: 0.841888, Accuracy: 90.05%\n",
      "Batch 93, Loss: 0.901476, Accuracy: 90.00%\n",
      "Batch 94, Loss: 0.888024, Accuracy: 89.96%\n",
      "Batch 95, Loss: 0.895560, Accuracy: 89.90%\n",
      "Batch 96, Loss: 0.854820, Accuracy: 89.89%\n",
      "Batch 97, Loss: 0.847555, Accuracy: 89.88%\n",
      "Batch 98, Loss: 0.841841, Accuracy: 89.89%\n",
      "Batch 99, Loss: 0.866876, Accuracy: 89.85%\n",
      "Batch 100, Loss: 0.838572, Accuracy: 89.86%\n",
      "Batch 101, Loss: 0.891940, Accuracy: 89.82%\n",
      "Batch 102, Loss: 0.796476, Accuracy: 89.86%\n",
      "Batch 103, Loss: 0.830244, Accuracy: 89.88%\n",
      "Batch 104, Loss: 0.849254, Accuracy: 89.89%\n",
      "Batch 105, Loss: 0.851372, Accuracy: 89.90%\n",
      "Batch 106, Loss: 0.842145, Accuracy: 89.92%\n",
      "Batch 107, Loss: 0.816734, Accuracy: 89.94%\n",
      "Batch 108, Loss: 0.796249, Accuracy: 89.99%\n",
      "Batch 109, Loss: 0.823557, Accuracy: 90.01%\n",
      "Batch 110, Loss: 0.864814, Accuracy: 89.99%\n",
      "Batch 111, Loss: 0.886699, Accuracy: 89.92%\n",
      "Batch 112, Loss: 0.854449, Accuracy: 89.91%\n",
      "Batch 113, Loss: 0.864500, Accuracy: 89.89%\n",
      "Batch 114, Loss: 0.859965, Accuracy: 89.88%\n",
      "Batch 115, Loss: 0.848952, Accuracy: 89.88%\n",
      "Batch 116, Loss: 0.843090, Accuracy: 89.88%\n",
      "Batch 117, Loss: 0.766019, Accuracy: 89.96%\n",
      "Batch 118, Loss: 0.768546, Accuracy: 90.02%\n",
      "Batch 119, Loss: 0.763032, Accuracy: 90.09%\n",
      "Batch 120, Loss: 0.873548, Accuracy: 90.07%\n",
      "Batch 121, Loss: 0.807293, Accuracy: 90.10%\n",
      "Batch 122, Loss: 0.804958, Accuracy: 90.13%\n",
      "Batch 123, Loss: 0.841605, Accuracy: 90.12%\n",
      "Batch 124, Loss: 0.895067, Accuracy: 90.08%\n",
      "Batch 125, Loss: 0.880117, Accuracy: 90.05%\n",
      "Batch 126, Loss: 0.817098, Accuracy: 90.07%\n",
      "Batch 127, Loss: 0.850923, Accuracy: 90.07%\n",
      "Batch 128, Loss: 0.812198, Accuracy: 90.10%\n",
      "Batch 129, Loss: 0.829720, Accuracy: 90.12%\n",
      "Batch 130, Loss: 0.817118, Accuracy: 90.14%\n",
      "Batch 131, Loss: 0.837330, Accuracy: 90.16%\n",
      "Batch 132, Loss: 0.776586, Accuracy: 90.21%\n",
      "Batch 133, Loss: 0.792921, Accuracy: 90.25%\n",
      "Batch 134, Loss: 0.823403, Accuracy: 90.25%\n",
      "Batch 135, Loss: 0.818582, Accuracy: 90.27%\n",
      "Batch 136, Loss: 0.791273, Accuracy: 90.30%\n",
      "Batch 137, Loss: 0.872529, Accuracy: 90.27%\n",
      "Batch 138, Loss: 0.912127, Accuracy: 90.22%\n",
      "Batch 139, Loss: 0.868785, Accuracy: 90.20%\n",
      "Batch 140, Loss: 0.805375, Accuracy: 90.23%\n",
      "Batch 141, Loss: 0.785210, Accuracy: 90.27%\n",
      "Batch 142, Loss: 0.806808, Accuracy: 90.31%\n",
      "Batch 143, Loss: 0.853690, Accuracy: 90.30%\n",
      "Batch 144, Loss: 0.863154, Accuracy: 90.28%\n",
      "Batch 145, Loss: 0.931960, Accuracy: 90.22%\n",
      "Batch 146, Loss: 0.858371, Accuracy: 90.20%\n",
      "Batch 147, Loss: 0.892406, Accuracy: 90.16%\n",
      "Batch 148, Loss: 0.794623, Accuracy: 90.19%\n",
      "Batch 149, Loss: 0.888443, Accuracy: 90.16%\n",
      "Batch 150, Loss: 0.826834, Accuracy: 90.18%\n",
      "Batch 151, Loss: 0.827787, Accuracy: 90.19%\n",
      "Batch 152, Loss: 0.853019, Accuracy: 90.17%\n",
      "Batch 153, Loss: 0.850105, Accuracy: 90.17%\n",
      "Batch 154, Loss: 0.849443, Accuracy: 90.16%\n",
      "Batch 155, Loss: 0.927436, Accuracy: 90.10%\n",
      "Batch 156, Loss: 0.874803, Accuracy: 90.08%\n",
      "Batch 157, Loss: 0.883693, Accuracy: 90.06%\n",
      "Batch 158, Loss: 0.866749, Accuracy: 90.04%\n",
      "Batch 159, Loss: 0.872527, Accuracy: 90.02%\n",
      "Batch 160, Loss: 0.916016, Accuracy: 89.96%\n",
      "Batch 161, Loss: 0.842661, Accuracy: 89.97%\n",
      "Batch 162, Loss: 0.866750, Accuracy: 89.95%\n",
      "Batch 163, Loss: 0.879459, Accuracy: 89.93%\n",
      "Batch 164, Loss: 0.832167, Accuracy: 89.94%\n",
      "Batch 165, Loss: 0.805678, Accuracy: 89.96%\n",
      "Batch 166, Loss: 0.868782, Accuracy: 89.95%\n",
      "Batch 167, Loss: 0.878768, Accuracy: 89.93%\n",
      "Batch 168, Loss: 0.873941, Accuracy: 89.92%\n",
      "Batch 169, Loss: 0.794855, Accuracy: 89.94%\n",
      "Batch 170, Loss: 0.838797, Accuracy: 89.94%\n",
      "Batch 171, Loss: 0.892043, Accuracy: 89.91%\n",
      "Batch 172, Loss: 0.819172, Accuracy: 89.93%\n",
      "Batch 173, Loss: 0.866511, Accuracy: 89.91%\n",
      "Batch 174, Loss: 0.864084, Accuracy: 89.90%\n",
      "Batch 175, Loss: 0.896201, Accuracy: 89.87%\n",
      "Batch 176, Loss: 0.845630, Accuracy: 89.87%\n",
      "Batch 177, Loss: 0.800380, Accuracy: 89.90%\n",
      "Batch 178, Loss: 0.862906, Accuracy: 89.91%\n",
      "Batch 179, Loss: 0.784156, Accuracy: 89.94%\n",
      "Batch 180, Loss: 0.853704, Accuracy: 89.94%\n",
      "Batch 181, Loss: 0.854742, Accuracy: 89.93%\n",
      "Batch 182, Loss: 0.836482, Accuracy: 89.95%\n",
      "Batch 183, Loss: 0.843930, Accuracy: 89.94%\n",
      "Batch 184, Loss: 0.858114, Accuracy: 89.94%\n",
      "Batch 185, Loss: 0.855491, Accuracy: 89.92%\n",
      "Batch 186, Loss: 0.837383, Accuracy: 89.94%\n",
      "Batch 187, Loss: 0.807432, Accuracy: 89.96%\n",
      "Batch 188, Loss: 0.882318, Accuracy: 89.94%\n",
      "Batch 189, Loss: 0.848710, Accuracy: 89.93%\n",
      "Batch 190, Loss: 0.847653, Accuracy: 89.93%\n",
      "Batch 191, Loss: 0.829487, Accuracy: 89.95%\n",
      "Batch 192, Loss: 0.815886, Accuracy: 89.97%\n",
      "Batch 193, Loss: 0.902592, Accuracy: 89.94%\n",
      "Batch 194, Loss: 0.830500, Accuracy: 89.95%\n",
      "Batch 195, Loss: 0.858881, Accuracy: 89.94%\n",
      "Batch 196, Loss: 0.894759, Accuracy: 89.92%\n",
      "Batch 197, Loss: 0.898192, Accuracy: 89.89%\n",
      "Batch 198, Loss: 0.817542, Accuracy: 89.91%\n",
      "Batch 199, Loss: 0.831877, Accuracy: 89.92%\n",
      "Batch 200, Loss: 0.905927, Accuracy: 89.89%\n",
      "Batch 201, Loss: 0.866990, Accuracy: 89.88%\n",
      "Batch 202, Loss: 0.888304, Accuracy: 89.85%\n",
      "Batch 203, Loss: 0.866279, Accuracy: 89.84%\n",
      "Batch 204, Loss: 0.856994, Accuracy: 89.84%\n",
      "Batch 205, Loss: 0.818455, Accuracy: 89.86%\n",
      "Batch 206, Loss: 0.869586, Accuracy: 89.85%\n",
      "Batch 207, Loss: 0.829573, Accuracy: 89.86%\n",
      "Batch 208, Loss: 0.907044, Accuracy: 89.81%\n",
      "Batch 209, Loss: 0.859825, Accuracy: 89.81%\n",
      "Batch 210, Loss: 0.850040, Accuracy: 89.81%\n",
      "Batch 211, Loss: 0.833348, Accuracy: 89.81%\n",
      "Batch 212, Loss: 0.852404, Accuracy: 89.81%\n",
      "Batch 213, Loss: 0.841569, Accuracy: 89.82%\n",
      "Training - Epoch 9, Loss: 0.847057, Accuracy: 89.82%\n",
      "Validation Batch 1, Loss: 0.819313, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.791186, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.815157, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.834877, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.817829, Accuracy: 93.12%\n",
      "Validation Batch 6, Loss: 0.792774, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.794501, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.855832, Accuracy: 93.16%\n",
      "Validation Batch 9, Loss: 0.888192, Accuracy: 92.19%\n",
      "Validation Batch 10, Loss: 0.787236, Accuracy: 92.66%\n",
      "Validation Batch 11, Loss: 0.826527, Accuracy: 92.61%\n",
      "Validation Batch 12, Loss: 0.826900, Accuracy: 92.58%\n",
      "Validation Batch 13, Loss: 0.832164, Accuracy: 92.43%\n",
      "Validation Batch 14, Loss: 0.833029, Accuracy: 92.41%\n",
      "Validation Batch 15, Loss: 0.798350, Accuracy: 92.50%\n",
      "Validation Batch 16, Loss: 0.837606, Accuracy: 92.38%\n",
      "Validation Batch 17, Loss: 0.856842, Accuracy: 92.19%\n",
      "Validation Batch 18, Loss: 0.796884, Accuracy: 92.36%\n",
      "Validation Batch 19, Loss: 0.845356, Accuracy: 92.27%\n",
      "Validation Batch 20, Loss: 0.815004, Accuracy: 92.42%\n",
      "Validation Batch 21, Loss: 0.858552, Accuracy: 92.26%\n",
      "Validation Batch 22, Loss: 0.793972, Accuracy: 92.47%\n",
      "Validation Batch 23, Loss: 0.858829, Accuracy: 92.32%\n",
      "Validation Batch 24, Loss: 0.827315, Accuracy: 92.32%\n",
      "Validation Batch 25, Loss: 0.795356, Accuracy: 92.44%\n",
      "Validation Batch 26, Loss: 0.835809, Accuracy: 92.37%\n",
      "Validation Batch 27, Loss: 0.792202, Accuracy: 92.43%\n",
      "Validation - Epoch 9, Loss: 0.823244, Accuracy: 92.43%\n",
      "Patience—9\n",
      "Epoch 10\n",
      "Batch 1, Loss: 0.838604, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.883336, Accuracy: 88.28%\n",
      "Batch 3, Loss: 0.842999, Accuracy: 88.54%\n",
      "Batch 4, Loss: 0.908298, Accuracy: 87.89%\n",
      "Batch 5, Loss: 0.853724, Accuracy: 88.44%\n",
      "Batch 6, Loss: 0.833047, Accuracy: 88.54%\n",
      "Batch 7, Loss: 0.870094, Accuracy: 88.39%\n",
      "Batch 8, Loss: 0.797097, Accuracy: 89.26%\n",
      "Batch 9, Loss: 0.826852, Accuracy: 89.58%\n",
      "Batch 10, Loss: 0.846079, Accuracy: 89.69%\n",
      "Batch 11, Loss: 0.831518, Accuracy: 89.77%\n",
      "Batch 12, Loss: 0.827675, Accuracy: 89.97%\n",
      "Batch 13, Loss: 0.886444, Accuracy: 89.66%\n",
      "Batch 14, Loss: 0.842641, Accuracy: 89.62%\n",
      "Batch 15, Loss: 0.852646, Accuracy: 89.58%\n",
      "Batch 16, Loss: 0.840042, Accuracy: 89.75%\n",
      "Batch 17, Loss: 0.790365, Accuracy: 90.07%\n",
      "Batch 18, Loss: 0.823869, Accuracy: 90.10%\n",
      "Batch 19, Loss: 0.837874, Accuracy: 90.13%\n",
      "Batch 20, Loss: 0.780603, Accuracy: 90.47%\n",
      "Batch 21, Loss: 0.855876, Accuracy: 90.40%\n",
      "Batch 22, Loss: 0.821647, Accuracy: 90.48%\n",
      "Batch 23, Loss: 0.853654, Accuracy: 90.42%\n",
      "Batch 24, Loss: 0.812665, Accuracy: 90.56%\n",
      "Batch 25, Loss: 0.907196, Accuracy: 90.25%\n",
      "Batch 26, Loss: 0.857887, Accuracy: 90.14%\n",
      "Batch 27, Loss: 0.913518, Accuracy: 89.87%\n",
      "Batch 28, Loss: 0.874164, Accuracy: 89.79%\n",
      "Batch 29, Loss: 0.860627, Accuracy: 89.71%\n",
      "Batch 30, Loss: 0.892419, Accuracy: 89.53%\n",
      "Batch 31, Loss: 0.870608, Accuracy: 89.47%\n",
      "Batch 32, Loss: 0.866045, Accuracy: 89.40%\n",
      "Batch 33, Loss: 0.790941, Accuracy: 89.58%\n",
      "Batch 34, Loss: 0.860360, Accuracy: 89.57%\n",
      "Batch 35, Loss: 0.808339, Accuracy: 89.69%\n",
      "Batch 36, Loss: 0.837075, Accuracy: 89.71%\n",
      "Batch 37, Loss: 0.858192, Accuracy: 89.65%\n",
      "Batch 38, Loss: 0.862339, Accuracy: 89.60%\n",
      "Batch 39, Loss: 0.905077, Accuracy: 89.46%\n",
      "Batch 40, Loss: 0.820744, Accuracy: 89.53%\n",
      "Batch 41, Loss: 0.817468, Accuracy: 89.63%\n",
      "Batch 42, Loss: 0.817026, Accuracy: 89.73%\n",
      "Batch 43, Loss: 0.834842, Accuracy: 89.75%\n",
      "Batch 44, Loss: 0.838639, Accuracy: 89.77%\n",
      "Batch 45, Loss: 0.893202, Accuracy: 89.62%\n",
      "Batch 46, Loss: 0.784007, Accuracy: 89.74%\n",
      "Batch 47, Loss: 0.851543, Accuracy: 89.73%\n",
      "Batch 48, Loss: 0.838473, Accuracy: 89.78%\n",
      "Batch 49, Loss: 0.840003, Accuracy: 89.80%\n",
      "Batch 50, Loss: 0.831163, Accuracy: 89.84%\n",
      "Batch 51, Loss: 0.874450, Accuracy: 89.77%\n",
      "Batch 52, Loss: 0.850980, Accuracy: 89.75%\n",
      "Batch 53, Loss: 0.888328, Accuracy: 89.68%\n",
      "Batch 54, Loss: 0.879956, Accuracy: 89.61%\n",
      "Batch 55, Loss: 0.878582, Accuracy: 89.57%\n",
      "Batch 56, Loss: 0.842065, Accuracy: 89.59%\n",
      "Batch 57, Loss: 0.843522, Accuracy: 89.61%\n",
      "Batch 58, Loss: 0.816444, Accuracy: 89.66%\n",
      "Batch 59, Loss: 0.815637, Accuracy: 89.72%\n",
      "Batch 60, Loss: 0.843692, Accuracy: 89.74%\n",
      "Batch 61, Loss: 0.886827, Accuracy: 89.65%\n",
      "Batch 62, Loss: 0.831717, Accuracy: 89.69%\n",
      "Batch 63, Loss: 0.849981, Accuracy: 89.68%\n",
      "Batch 64, Loss: 0.816474, Accuracy: 89.75%\n",
      "Batch 65, Loss: 0.921856, Accuracy: 89.62%\n",
      "Batch 66, Loss: 0.834576, Accuracy: 89.63%\n",
      "Batch 67, Loss: 0.864857, Accuracy: 89.60%\n",
      "Batch 68, Loss: 0.819588, Accuracy: 89.64%\n",
      "Batch 69, Loss: 0.837817, Accuracy: 89.65%\n",
      "Batch 70, Loss: 0.808566, Accuracy: 89.69%\n",
      "Batch 71, Loss: 0.869369, Accuracy: 89.66%\n",
      "Batch 72, Loss: 0.801470, Accuracy: 89.74%\n",
      "Batch 73, Loss: 0.835096, Accuracy: 89.75%\n",
      "Batch 74, Loss: 0.850318, Accuracy: 89.74%\n",
      "Batch 75, Loss: 0.828224, Accuracy: 89.77%\n",
      "Batch 76, Loss: 0.816719, Accuracy: 89.80%\n",
      "Batch 77, Loss: 0.794232, Accuracy: 89.87%\n",
      "Batch 78, Loss: 0.825737, Accuracy: 89.90%\n",
      "Batch 79, Loss: 0.817594, Accuracy: 89.95%\n",
      "Batch 80, Loss: 0.819885, Accuracy: 89.98%\n",
      "Batch 81, Loss: 0.836766, Accuracy: 90.01%\n",
      "Batch 82, Loss: 0.832551, Accuracy: 90.03%\n",
      "Batch 83, Loss: 0.859382, Accuracy: 90.00%\n",
      "Batch 84, Loss: 0.834468, Accuracy: 90.03%\n",
      "Batch 85, Loss: 0.842125, Accuracy: 90.04%\n",
      "Batch 86, Loss: 0.830042, Accuracy: 90.04%\n",
      "Batch 87, Loss: 0.818178, Accuracy: 90.09%\n",
      "Batch 88, Loss: 0.804067, Accuracy: 90.15%\n",
      "Batch 89, Loss: 0.842831, Accuracy: 90.13%\n",
      "Batch 90, Loss: 0.827383, Accuracy: 90.14%\n",
      "Batch 91, Loss: 0.810929, Accuracy: 90.18%\n",
      "Batch 92, Loss: 0.804069, Accuracy: 90.23%\n",
      "Batch 93, Loss: 0.857038, Accuracy: 90.22%\n",
      "Batch 94, Loss: 0.833097, Accuracy: 90.23%\n",
      "Batch 95, Loss: 0.884939, Accuracy: 90.18%\n",
      "Batch 96, Loss: 0.878039, Accuracy: 90.14%\n",
      "Batch 97, Loss: 0.890426, Accuracy: 90.08%\n",
      "Batch 98, Loss: 0.854957, Accuracy: 90.07%\n",
      "Batch 99, Loss: 0.906543, Accuracy: 89.99%\n",
      "Batch 100, Loss: 0.841592, Accuracy: 90.00%\n",
      "Batch 101, Loss: 0.854696, Accuracy: 89.98%\n",
      "Batch 102, Loss: 0.876077, Accuracy: 89.94%\n",
      "Batch 103, Loss: 0.766663, Accuracy: 90.02%\n",
      "Batch 104, Loss: 0.827601, Accuracy: 90.04%\n",
      "Batch 105, Loss: 0.766764, Accuracy: 90.12%\n",
      "Batch 106, Loss: 0.913049, Accuracy: 90.06%\n",
      "Batch 107, Loss: 0.773217, Accuracy: 90.14%\n",
      "Batch 108, Loss: 0.810349, Accuracy: 90.18%\n",
      "Batch 109, Loss: 0.869439, Accuracy: 90.14%\n",
      "Batch 110, Loss: 0.883412, Accuracy: 90.10%\n",
      "Batch 111, Loss: 0.829869, Accuracy: 90.12%\n",
      "Batch 112, Loss: 0.855672, Accuracy: 90.11%\n",
      "Batch 113, Loss: 0.859637, Accuracy: 90.10%\n",
      "Batch 114, Loss: 0.770011, Accuracy: 90.16%\n",
      "Batch 115, Loss: 0.918121, Accuracy: 90.08%\n",
      "Batch 116, Loss: 0.896686, Accuracy: 90.03%\n",
      "Batch 117, Loss: 0.974673, Accuracy: 89.93%\n",
      "Batch 118, Loss: 0.826151, Accuracy: 89.94%\n",
      "Batch 119, Loss: 0.840460, Accuracy: 89.94%\n",
      "Batch 120, Loss: 0.844015, Accuracy: 89.93%\n",
      "Batch 121, Loss: 0.842750, Accuracy: 89.94%\n",
      "Batch 122, Loss: 0.862785, Accuracy: 89.92%\n",
      "Batch 123, Loss: 0.855854, Accuracy: 89.91%\n",
      "Batch 124, Loss: 0.881919, Accuracy: 89.88%\n",
      "Batch 125, Loss: 0.805863, Accuracy: 89.91%\n",
      "Batch 126, Loss: 0.828042, Accuracy: 89.92%\n",
      "Batch 127, Loss: 0.811718, Accuracy: 89.94%\n",
      "Batch 128, Loss: 0.818626, Accuracy: 89.97%\n",
      "Batch 129, Loss: 0.895510, Accuracy: 89.92%\n",
      "Batch 130, Loss: 0.825520, Accuracy: 89.94%\n",
      "Batch 131, Loss: 0.874440, Accuracy: 89.91%\n",
      "Batch 132, Loss: 0.823272, Accuracy: 89.93%\n",
      "Batch 133, Loss: 0.800887, Accuracy: 89.96%\n",
      "Batch 134, Loss: 0.819378, Accuracy: 89.98%\n",
      "Batch 135, Loss: 0.860957, Accuracy: 89.95%\n",
      "Batch 136, Loss: 0.839266, Accuracy: 89.96%\n",
      "Batch 137, Loss: 0.867229, Accuracy: 89.94%\n",
      "Batch 138, Loss: 0.913887, Accuracy: 89.89%\n",
      "Batch 139, Loss: 0.833088, Accuracy: 89.89%\n",
      "Batch 140, Loss: 0.862964, Accuracy: 89.90%\n",
      "Batch 141, Loss: 0.871381, Accuracy: 89.88%\n",
      "Batch 142, Loss: 0.898157, Accuracy: 89.84%\n",
      "Batch 143, Loss: 0.850230, Accuracy: 89.84%\n",
      "Batch 144, Loss: 0.868728, Accuracy: 89.82%\n",
      "Batch 145, Loss: 0.795262, Accuracy: 89.86%\n",
      "Batch 146, Loss: 0.797452, Accuracy: 89.90%\n",
      "Batch 147, Loss: 0.849540, Accuracy: 89.89%\n",
      "Batch 148, Loss: 0.847572, Accuracy: 89.89%\n",
      "Batch 149, Loss: 0.844315, Accuracy: 89.89%\n",
      "Batch 150, Loss: 0.876811, Accuracy: 89.86%\n",
      "Batch 151, Loss: 0.853989, Accuracy: 89.86%\n",
      "Batch 152, Loss: 0.908362, Accuracy: 89.82%\n",
      "Batch 153, Loss: 0.803554, Accuracy: 89.85%\n",
      "Batch 154, Loss: 0.798296, Accuracy: 89.88%\n",
      "Batch 155, Loss: 0.838714, Accuracy: 89.89%\n",
      "Batch 156, Loss: 0.844762, Accuracy: 89.89%\n",
      "Batch 157, Loss: 0.882672, Accuracy: 89.87%\n",
      "Batch 158, Loss: 0.843053, Accuracy: 89.87%\n",
      "Batch 159, Loss: 0.861688, Accuracy: 89.87%\n",
      "Batch 160, Loss: 0.817295, Accuracy: 89.88%\n",
      "Batch 161, Loss: 0.816240, Accuracy: 89.90%\n",
      "Batch 162, Loss: 0.822470, Accuracy: 89.91%\n",
      "Batch 163, Loss: 0.818144, Accuracy: 89.93%\n",
      "Batch 164, Loss: 0.845876, Accuracy: 89.93%\n",
      "Batch 165, Loss: 0.802810, Accuracy: 89.96%\n",
      "Batch 166, Loss: 0.888993, Accuracy: 89.94%\n",
      "Batch 167, Loss: 0.864844, Accuracy: 89.92%\n",
      "Batch 168, Loss: 0.857958, Accuracy: 89.91%\n",
      "Batch 169, Loss: 0.882983, Accuracy: 89.89%\n",
      "Batch 170, Loss: 0.895113, Accuracy: 89.86%\n",
      "Batch 171, Loss: 0.841970, Accuracy: 89.86%\n",
      "Batch 172, Loss: 0.833202, Accuracy: 89.86%\n",
      "Batch 173, Loss: 0.814164, Accuracy: 89.88%\n",
      "Batch 174, Loss: 0.801438, Accuracy: 89.91%\n",
      "Batch 175, Loss: 0.848928, Accuracy: 89.90%\n",
      "Batch 176, Loss: 0.823463, Accuracy: 89.91%\n",
      "Batch 177, Loss: 0.876854, Accuracy: 89.91%\n",
      "Batch 178, Loss: 0.894755, Accuracy: 89.89%\n",
      "Batch 179, Loss: 0.869393, Accuracy: 89.87%\n",
      "Batch 180, Loss: 0.838203, Accuracy: 89.89%\n",
      "Batch 181, Loss: 0.819921, Accuracy: 89.92%\n",
      "Batch 182, Loss: 0.871613, Accuracy: 89.91%\n",
      "Batch 183, Loss: 0.816613, Accuracy: 89.93%\n",
      "Batch 184, Loss: 0.864857, Accuracy: 89.93%\n",
      "Batch 185, Loss: 0.823015, Accuracy: 89.94%\n",
      "Batch 186, Loss: 0.803898, Accuracy: 89.97%\n",
      "Batch 187, Loss: 0.850868, Accuracy: 89.97%\n",
      "Batch 188, Loss: 0.850270, Accuracy: 89.96%\n",
      "Batch 189, Loss: 0.872332, Accuracy: 89.95%\n",
      "Batch 190, Loss: 0.815583, Accuracy: 89.96%\n",
      "Batch 191, Loss: 0.855139, Accuracy: 89.95%\n",
      "Batch 192, Loss: 0.795193, Accuracy: 89.98%\n",
      "Batch 193, Loss: 0.788963, Accuracy: 90.01%\n",
      "Batch 194, Loss: 0.791807, Accuracy: 90.04%\n",
      "Batch 195, Loss: 0.845458, Accuracy: 90.04%\n",
      "Batch 196, Loss: 0.850492, Accuracy: 90.04%\n",
      "Batch 197, Loss: 0.800058, Accuracy: 90.07%\n",
      "Batch 198, Loss: 0.809780, Accuracy: 90.08%\n",
      "Batch 199, Loss: 0.880174, Accuracy: 90.06%\n",
      "Batch 200, Loss: 0.850238, Accuracy: 90.05%\n",
      "Batch 201, Loss: 0.889178, Accuracy: 90.03%\n",
      "Batch 202, Loss: 0.925552, Accuracy: 90.00%\n",
      "Batch 203, Loss: 0.918616, Accuracy: 89.95%\n",
      "Batch 204, Loss: 0.864167, Accuracy: 89.94%\n",
      "Batch 205, Loss: 0.818422, Accuracy: 89.96%\n",
      "Batch 206, Loss: 0.880882, Accuracy: 89.95%\n",
      "Batch 207, Loss: 0.847049, Accuracy: 89.95%\n",
      "Batch 208, Loss: 0.815616, Accuracy: 89.96%\n",
      "Batch 209, Loss: 0.799857, Accuracy: 89.99%\n",
      "Batch 210, Loss: 0.843259, Accuracy: 89.99%\n",
      "Batch 211, Loss: 0.830444, Accuracy: 90.00%\n",
      "Batch 212, Loss: 0.859715, Accuracy: 89.99%\n",
      "Batch 213, Loss: 0.864451, Accuracy: 89.98%\n",
      "Training - Epoch 10, Loss: 0.845198, Accuracy: 89.98%\n",
      "Validation Batch 1, Loss: 0.820367, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.792376, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.816640, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.835335, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.820019, Accuracy: 93.12%\n",
      "Validation Batch 6, Loss: 0.793537, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.794740, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.856966, Accuracy: 92.97%\n",
      "Validation Batch 9, Loss: 0.890082, Accuracy: 92.01%\n",
      "Validation Batch 10, Loss: 0.788031, Accuracy: 92.50%\n",
      "Validation Batch 11, Loss: 0.827981, Accuracy: 92.47%\n",
      "Validation Batch 12, Loss: 0.827241, Accuracy: 92.45%\n",
      "Validation Batch 13, Loss: 0.832912, Accuracy: 92.31%\n",
      "Validation Batch 14, Loss: 0.834384, Accuracy: 92.30%\n",
      "Validation Batch 15, Loss: 0.798639, Accuracy: 92.40%\n",
      "Validation Batch 16, Loss: 0.839251, Accuracy: 92.29%\n",
      "Validation Batch 17, Loss: 0.857161, Accuracy: 92.10%\n",
      "Validation Batch 18, Loss: 0.797875, Accuracy: 92.27%\n",
      "Validation Batch 19, Loss: 0.845968, Accuracy: 92.19%\n",
      "Validation Batch 20, Loss: 0.817625, Accuracy: 92.34%\n",
      "Validation Batch 21, Loss: 0.858872, Accuracy: 92.19%\n",
      "Validation Batch 22, Loss: 0.795303, Accuracy: 92.40%\n",
      "Validation Batch 23, Loss: 0.859100, Accuracy: 92.26%\n",
      "Validation Batch 24, Loss: 0.828402, Accuracy: 92.19%\n",
      "Validation Batch 25, Loss: 0.796627, Accuracy: 92.31%\n",
      "Validation Batch 26, Loss: 0.836474, Accuracy: 92.25%\n",
      "Validation Batch 27, Loss: 0.793211, Accuracy: 92.31%\n",
      "Validation - Epoch 10, Loss: 0.824264, Accuracy: 92.31%\n",
      "Patience—10\n",
      "Epoch 11\n",
      "Batch 1, Loss: 0.804645, Accuracy: 93.75%\n",
      "Batch 2, Loss: 0.838224, Accuracy: 92.19%\n",
      "Batch 3, Loss: 0.853802, Accuracy: 91.67%\n",
      "Batch 4, Loss: 0.843567, Accuracy: 91.02%\n",
      "Batch 5, Loss: 0.809048, Accuracy: 91.56%\n",
      "Batch 6, Loss: 0.833795, Accuracy: 91.67%\n",
      "Batch 7, Loss: 0.843023, Accuracy: 91.52%\n",
      "Batch 8, Loss: 0.853345, Accuracy: 91.21%\n",
      "Batch 9, Loss: 0.835225, Accuracy: 91.32%\n",
      "Batch 10, Loss: 0.854368, Accuracy: 91.09%\n",
      "Batch 11, Loss: 0.810116, Accuracy: 91.34%\n",
      "Batch 12, Loss: 0.839388, Accuracy: 91.28%\n",
      "Batch 13, Loss: 0.885255, Accuracy: 90.87%\n",
      "Batch 14, Loss: 0.849693, Accuracy: 90.85%\n",
      "Batch 15, Loss: 0.793889, Accuracy: 91.15%\n",
      "Batch 16, Loss: 0.850208, Accuracy: 91.02%\n",
      "Batch 17, Loss: 0.778034, Accuracy: 91.36%\n",
      "Batch 18, Loss: 0.859332, Accuracy: 91.23%\n",
      "Batch 19, Loss: 0.789787, Accuracy: 91.45%\n",
      "Batch 20, Loss: 0.820809, Accuracy: 91.41%\n",
      "Batch 21, Loss: 0.857194, Accuracy: 91.29%\n",
      "Batch 22, Loss: 0.855581, Accuracy: 91.19%\n",
      "Batch 23, Loss: 0.820228, Accuracy: 91.24%\n",
      "Batch 24, Loss: 0.861112, Accuracy: 91.21%\n",
      "Batch 25, Loss: 0.892564, Accuracy: 90.94%\n",
      "Batch 26, Loss: 0.868634, Accuracy: 90.75%\n",
      "Batch 27, Loss: 0.817232, Accuracy: 90.86%\n",
      "Batch 28, Loss: 0.820752, Accuracy: 90.90%\n",
      "Batch 29, Loss: 0.901621, Accuracy: 90.68%\n",
      "Batch 30, Loss: 0.871214, Accuracy: 90.57%\n",
      "Batch 31, Loss: 0.798015, Accuracy: 90.68%\n",
      "Batch 32, Loss: 0.845028, Accuracy: 90.62%\n",
      "Batch 33, Loss: 0.824051, Accuracy: 90.72%\n",
      "Batch 34, Loss: 0.843018, Accuracy: 90.67%\n",
      "Batch 35, Loss: 0.824001, Accuracy: 90.71%\n",
      "Batch 36, Loss: 0.824087, Accuracy: 90.76%\n",
      "Batch 37, Loss: 0.894489, Accuracy: 90.58%\n",
      "Batch 38, Loss: 0.864464, Accuracy: 90.54%\n",
      "Batch 39, Loss: 0.884167, Accuracy: 90.42%\n",
      "Batch 40, Loss: 0.864579, Accuracy: 90.35%\n",
      "Batch 41, Loss: 0.803553, Accuracy: 90.47%\n",
      "Batch 42, Loss: 0.913509, Accuracy: 90.29%\n",
      "Batch 43, Loss: 0.885866, Accuracy: 90.19%\n",
      "Batch 44, Loss: 0.840817, Accuracy: 90.23%\n",
      "Batch 45, Loss: 0.853090, Accuracy: 90.21%\n",
      "Batch 46, Loss: 0.922113, Accuracy: 90.01%\n",
      "Batch 47, Loss: 0.819370, Accuracy: 90.06%\n",
      "Batch 48, Loss: 0.817716, Accuracy: 90.07%\n",
      "Batch 49, Loss: 0.862082, Accuracy: 90.05%\n",
      "Batch 50, Loss: 0.829705, Accuracy: 90.09%\n",
      "Batch 51, Loss: 0.911687, Accuracy: 89.95%\n",
      "Batch 52, Loss: 0.878093, Accuracy: 89.90%\n",
      "Batch 53, Loss: 0.846700, Accuracy: 89.89%\n",
      "Batch 54, Loss: 0.851708, Accuracy: 89.87%\n",
      "Batch 55, Loss: 0.867921, Accuracy: 89.83%\n",
      "Batch 56, Loss: 0.812562, Accuracy: 89.90%\n",
      "Batch 57, Loss: 0.893345, Accuracy: 89.80%\n",
      "Batch 58, Loss: 0.822219, Accuracy: 89.84%\n",
      "Batch 59, Loss: 0.816965, Accuracy: 89.91%\n",
      "Batch 60, Loss: 0.834466, Accuracy: 89.95%\n",
      "Batch 61, Loss: 0.849677, Accuracy: 89.91%\n",
      "Batch 62, Loss: 0.827533, Accuracy: 89.97%\n",
      "Batch 63, Loss: 0.781237, Accuracy: 90.08%\n",
      "Batch 64, Loss: 0.796172, Accuracy: 90.16%\n",
      "Batch 65, Loss: 0.817582, Accuracy: 90.19%\n",
      "Batch 66, Loss: 0.832925, Accuracy: 90.22%\n",
      "Batch 67, Loss: 0.837902, Accuracy: 90.23%\n",
      "Batch 68, Loss: 0.840114, Accuracy: 90.23%\n",
      "Batch 69, Loss: 0.793892, Accuracy: 90.29%\n",
      "Batch 70, Loss: 0.792584, Accuracy: 90.38%\n",
      "Batch 71, Loss: 0.831414, Accuracy: 90.38%\n",
      "Batch 72, Loss: 0.857922, Accuracy: 90.36%\n",
      "Batch 73, Loss: 0.887066, Accuracy: 90.30%\n",
      "Batch 74, Loss: 0.866772, Accuracy: 90.29%\n",
      "Batch 75, Loss: 0.896630, Accuracy: 90.21%\n",
      "Batch 76, Loss: 0.841854, Accuracy: 90.19%\n",
      "Batch 77, Loss: 0.867147, Accuracy: 90.18%\n",
      "Batch 78, Loss: 0.912618, Accuracy: 90.08%\n",
      "Batch 79, Loss: 0.789412, Accuracy: 90.15%\n",
      "Batch 80, Loss: 0.880607, Accuracy: 90.12%\n",
      "Batch 81, Loss: 0.877631, Accuracy: 90.08%\n",
      "Batch 82, Loss: 0.855418, Accuracy: 90.07%\n",
      "Batch 83, Loss: 0.832078, Accuracy: 90.08%\n",
      "Batch 84, Loss: 0.810321, Accuracy: 90.12%\n",
      "Batch 85, Loss: 0.897158, Accuracy: 90.04%\n",
      "Batch 86, Loss: 0.867586, Accuracy: 89.99%\n",
      "Batch 87, Loss: 0.884723, Accuracy: 89.96%\n",
      "Batch 88, Loss: 0.880506, Accuracy: 89.91%\n",
      "Batch 89, Loss: 0.876255, Accuracy: 89.87%\n",
      "Batch 90, Loss: 0.802195, Accuracy: 89.93%\n",
      "Batch 91, Loss: 0.822988, Accuracy: 89.97%\n",
      "Batch 92, Loss: 0.792127, Accuracy: 90.03%\n",
      "Batch 93, Loss: 0.859755, Accuracy: 90.02%\n",
      "Batch 94, Loss: 0.865593, Accuracy: 89.98%\n",
      "Batch 95, Loss: 0.810526, Accuracy: 90.02%\n",
      "Batch 96, Loss: 0.803607, Accuracy: 90.06%\n",
      "Batch 97, Loss: 0.862653, Accuracy: 90.03%\n",
      "Batch 98, Loss: 0.819885, Accuracy: 90.05%\n",
      "Batch 99, Loss: 0.830520, Accuracy: 90.07%\n",
      "Batch 100, Loss: 0.885444, Accuracy: 90.02%\n",
      "Batch 101, Loss: 0.877748, Accuracy: 89.99%\n",
      "Batch 102, Loss: 0.890038, Accuracy: 89.95%\n",
      "Batch 103, Loss: 0.915091, Accuracy: 89.87%\n",
      "Batch 104, Loss: 0.804814, Accuracy: 89.92%\n",
      "Batch 105, Loss: 0.895717, Accuracy: 89.87%\n",
      "Batch 106, Loss: 0.891166, Accuracy: 89.83%\n",
      "Batch 107, Loss: 0.816445, Accuracy: 89.87%\n",
      "Batch 108, Loss: 0.872838, Accuracy: 89.83%\n",
      "Batch 109, Loss: 0.883254, Accuracy: 89.79%\n",
      "Batch 110, Loss: 0.923444, Accuracy: 89.70%\n",
      "Batch 111, Loss: 0.858425, Accuracy: 89.70%\n",
      "Batch 112, Loss: 0.888956, Accuracy: 89.66%\n",
      "Batch 113, Loss: 0.860331, Accuracy: 89.66%\n",
      "Batch 114, Loss: 0.865589, Accuracy: 89.64%\n",
      "Batch 115, Loss: 0.853673, Accuracy: 89.63%\n",
      "Batch 116, Loss: 0.827116, Accuracy: 89.64%\n",
      "Batch 117, Loss: 0.836338, Accuracy: 89.68%\n",
      "Batch 118, Loss: 0.832214, Accuracy: 89.68%\n",
      "Batch 119, Loss: 0.817430, Accuracy: 89.72%\n",
      "Batch 120, Loss: 0.791136, Accuracy: 89.77%\n",
      "Batch 121, Loss: 0.827046, Accuracy: 89.77%\n",
      "Batch 122, Loss: 0.893736, Accuracy: 89.73%\n",
      "Batch 123, Loss: 0.833830, Accuracy: 89.72%\n",
      "Batch 124, Loss: 0.821644, Accuracy: 89.74%\n",
      "Batch 125, Loss: 0.859095, Accuracy: 89.72%\n",
      "Batch 126, Loss: 0.831265, Accuracy: 89.74%\n",
      "Batch 127, Loss: 0.848557, Accuracy: 89.76%\n",
      "Batch 128, Loss: 0.806737, Accuracy: 89.79%\n",
      "Batch 129, Loss: 0.817980, Accuracy: 89.81%\n",
      "Batch 130, Loss: 0.824733, Accuracy: 89.83%\n",
      "Batch 131, Loss: 0.838528, Accuracy: 89.84%\n",
      "Batch 132, Loss: 0.813072, Accuracy: 89.87%\n",
      "Batch 133, Loss: 0.854318, Accuracy: 89.85%\n",
      "Batch 134, Loss: 0.854823, Accuracy: 89.84%\n",
      "Batch 135, Loss: 0.870562, Accuracy: 89.81%\n",
      "Batch 136, Loss: 0.890327, Accuracy: 89.79%\n",
      "Batch 137, Loss: 0.846139, Accuracy: 89.79%\n",
      "Batch 138, Loss: 0.834350, Accuracy: 89.81%\n",
      "Batch 139, Loss: 0.855952, Accuracy: 89.79%\n",
      "Batch 140, Loss: 0.821219, Accuracy: 89.82%\n",
      "Batch 141, Loss: 0.822155, Accuracy: 89.84%\n",
      "Batch 142, Loss: 0.802568, Accuracy: 89.88%\n",
      "Batch 143, Loss: 0.898387, Accuracy: 89.84%\n",
      "Batch 144, Loss: 0.849124, Accuracy: 89.84%\n",
      "Batch 145, Loss: 0.900275, Accuracy: 89.81%\n",
      "Batch 146, Loss: 0.813550, Accuracy: 89.83%\n",
      "Batch 147, Loss: 0.812154, Accuracy: 89.86%\n",
      "Batch 148, Loss: 0.847913, Accuracy: 89.85%\n",
      "Batch 149, Loss: 0.832037, Accuracy: 89.86%\n",
      "Batch 150, Loss: 0.920764, Accuracy: 89.82%\n",
      "Batch 151, Loss: 0.804567, Accuracy: 89.85%\n",
      "Batch 152, Loss: 0.809012, Accuracy: 89.87%\n",
      "Batch 153, Loss: 0.787146, Accuracy: 89.91%\n",
      "Batch 154, Loss: 0.837943, Accuracy: 89.91%\n",
      "Batch 155, Loss: 0.828239, Accuracy: 89.92%\n",
      "Batch 156, Loss: 0.896621, Accuracy: 89.88%\n",
      "Batch 157, Loss: 0.818296, Accuracy: 89.91%\n",
      "Batch 158, Loss: 0.928903, Accuracy: 89.85%\n",
      "Batch 159, Loss: 0.836509, Accuracy: 89.85%\n",
      "Batch 160, Loss: 0.878343, Accuracy: 89.81%\n",
      "Batch 161, Loss: 0.904810, Accuracy: 89.77%\n",
      "Batch 162, Loss: 0.834467, Accuracy: 89.78%\n",
      "Batch 163, Loss: 0.885032, Accuracy: 89.76%\n",
      "Batch 164, Loss: 0.825997, Accuracy: 89.77%\n",
      "Batch 165, Loss: 0.789928, Accuracy: 89.80%\n",
      "Batch 166, Loss: 0.889776, Accuracy: 89.78%\n",
      "Batch 167, Loss: 0.894911, Accuracy: 89.75%\n",
      "Batch 168, Loss: 0.858682, Accuracy: 89.75%\n",
      "Batch 169, Loss: 0.814884, Accuracy: 89.77%\n",
      "Batch 170, Loss: 0.809791, Accuracy: 89.78%\n",
      "Batch 171, Loss: 0.851881, Accuracy: 89.78%\n",
      "Batch 172, Loss: 0.837310, Accuracy: 89.78%\n",
      "Batch 173, Loss: 0.890714, Accuracy: 89.75%\n",
      "Batch 174, Loss: 0.828376, Accuracy: 89.75%\n",
      "Batch 175, Loss: 0.841523, Accuracy: 89.76%\n",
      "Batch 176, Loss: 0.824299, Accuracy: 89.77%\n",
      "Batch 177, Loss: 0.834760, Accuracy: 89.78%\n",
      "Batch 178, Loss: 0.892777, Accuracy: 89.75%\n",
      "Batch 179, Loss: 0.895584, Accuracy: 89.72%\n",
      "Batch 180, Loss: 0.816351, Accuracy: 89.74%\n",
      "Batch 181, Loss: 0.804833, Accuracy: 89.76%\n",
      "Batch 182, Loss: 0.889633, Accuracy: 89.73%\n",
      "Batch 183, Loss: 0.789137, Accuracy: 89.77%\n",
      "Batch 184, Loss: 0.806157, Accuracy: 89.79%\n",
      "Batch 185, Loss: 0.814839, Accuracy: 89.81%\n",
      "Batch 186, Loss: 0.799333, Accuracy: 89.84%\n",
      "Batch 187, Loss: 0.843054, Accuracy: 89.84%\n",
      "Batch 188, Loss: 0.889967, Accuracy: 89.83%\n",
      "Batch 189, Loss: 0.811975, Accuracy: 89.85%\n",
      "Batch 190, Loss: 0.917483, Accuracy: 89.81%\n",
      "Batch 191, Loss: 0.832721, Accuracy: 89.82%\n",
      "Batch 192, Loss: 0.836401, Accuracy: 89.83%\n",
      "Batch 193, Loss: 0.795042, Accuracy: 89.85%\n",
      "Batch 194, Loss: 0.917566, Accuracy: 89.81%\n",
      "Batch 195, Loss: 0.894690, Accuracy: 89.78%\n",
      "Batch 196, Loss: 0.835564, Accuracy: 89.78%\n",
      "Batch 197, Loss: 0.822563, Accuracy: 89.79%\n",
      "Batch 198, Loss: 0.833118, Accuracy: 89.80%\n",
      "Batch 199, Loss: 0.851723, Accuracy: 89.80%\n",
      "Batch 200, Loss: 0.836336, Accuracy: 89.81%\n",
      "Batch 201, Loss: 0.881043, Accuracy: 89.79%\n",
      "Batch 202, Loss: 0.843852, Accuracy: 89.81%\n",
      "Batch 203, Loss: 0.896528, Accuracy: 89.78%\n",
      "Batch 204, Loss: 0.809503, Accuracy: 89.80%\n",
      "Batch 205, Loss: 0.814788, Accuracy: 89.81%\n",
      "Batch 206, Loss: 0.807656, Accuracy: 89.83%\n",
      "Batch 207, Loss: 0.833463, Accuracy: 89.83%\n",
      "Batch 208, Loss: 0.863103, Accuracy: 89.83%\n",
      "Batch 209, Loss: 0.789763, Accuracy: 89.86%\n",
      "Batch 210, Loss: 0.844248, Accuracy: 89.86%\n",
      "Batch 211, Loss: 0.885567, Accuracy: 89.84%\n",
      "Batch 212, Loss: 0.838871, Accuracy: 89.84%\n",
      "Batch 213, Loss: 0.884791, Accuracy: 89.82%\n",
      "Training - Epoch 11, Loss: 0.846509, Accuracy: 89.82%\n",
      "Validation Batch 1, Loss: 0.820036, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.791697, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.815977, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.835207, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.818835, Accuracy: 93.12%\n",
      "Validation Batch 6, Loss: 0.793198, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.794737, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.856187, Accuracy: 93.16%\n",
      "Validation Batch 9, Loss: 0.889048, Accuracy: 92.19%\n",
      "Validation Batch 10, Loss: 0.787795, Accuracy: 92.66%\n",
      "Validation Batch 11, Loss: 0.827369, Accuracy: 92.61%\n",
      "Validation Batch 12, Loss: 0.827260, Accuracy: 92.58%\n",
      "Validation Batch 13, Loss: 0.832894, Accuracy: 92.43%\n",
      "Validation Batch 14, Loss: 0.833730, Accuracy: 92.41%\n",
      "Validation Batch 15, Loss: 0.798600, Accuracy: 92.50%\n",
      "Validation Batch 16, Loss: 0.838356, Accuracy: 92.38%\n",
      "Validation Batch 17, Loss: 0.856699, Accuracy: 92.19%\n",
      "Validation Batch 18, Loss: 0.797272, Accuracy: 92.36%\n",
      "Validation Batch 19, Loss: 0.845876, Accuracy: 92.27%\n",
      "Validation Batch 20, Loss: 0.816975, Accuracy: 92.42%\n",
      "Validation Batch 21, Loss: 0.858695, Accuracy: 92.26%\n",
      "Validation Batch 22, Loss: 0.794618, Accuracy: 92.47%\n",
      "Validation Batch 23, Loss: 0.859183, Accuracy: 92.32%\n",
      "Validation Batch 24, Loss: 0.828022, Accuracy: 92.32%\n",
      "Validation Batch 25, Loss: 0.796215, Accuracy: 92.44%\n",
      "Validation Batch 26, Loss: 0.836375, Accuracy: 92.37%\n",
      "Validation Batch 27, Loss: 0.793180, Accuracy: 92.43%\n",
      "Validation - Epoch 11, Loss: 0.823853, Accuracy: 92.43%\n",
      "Patience—11\n",
      "Epoch 12\n",
      "Batch 1, Loss: 0.837765, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.772984, Accuracy: 94.53%\n",
      "Batch 3, Loss: 0.881890, Accuracy: 91.67%\n",
      "Batch 4, Loss: 0.856549, Accuracy: 91.02%\n",
      "Batch 5, Loss: 0.807877, Accuracy: 91.56%\n",
      "Batch 6, Loss: 0.856355, Accuracy: 91.15%\n",
      "Batch 7, Loss: 0.877399, Accuracy: 90.18%\n",
      "Batch 8, Loss: 0.888914, Accuracy: 89.65%\n",
      "Batch 9, Loss: 0.850495, Accuracy: 89.58%\n",
      "Batch 10, Loss: 0.835049, Accuracy: 89.69%\n",
      "Batch 11, Loss: 0.887027, Accuracy: 89.35%\n",
      "Batch 12, Loss: 0.792443, Accuracy: 89.97%\n",
      "Batch 13, Loss: 0.867736, Accuracy: 89.90%\n",
      "Batch 14, Loss: 0.831121, Accuracy: 89.96%\n",
      "Batch 15, Loss: 0.859596, Accuracy: 89.79%\n",
      "Batch 16, Loss: 0.887958, Accuracy: 89.55%\n",
      "Batch 17, Loss: 0.807196, Accuracy: 89.80%\n",
      "Batch 18, Loss: 0.884199, Accuracy: 89.67%\n",
      "Batch 19, Loss: 0.825399, Accuracy: 89.80%\n",
      "Batch 20, Loss: 0.825085, Accuracy: 89.92%\n",
      "Batch 21, Loss: 0.871607, Accuracy: 89.73%\n",
      "Batch 22, Loss: 0.899498, Accuracy: 89.56%\n",
      "Batch 23, Loss: 0.854080, Accuracy: 89.54%\n",
      "Batch 24, Loss: 0.811567, Accuracy: 89.65%\n",
      "Batch 25, Loss: 0.874461, Accuracy: 89.56%\n",
      "Batch 26, Loss: 0.816337, Accuracy: 89.66%\n",
      "Batch 27, Loss: 0.876680, Accuracy: 89.53%\n",
      "Batch 28, Loss: 0.852549, Accuracy: 89.51%\n",
      "Batch 29, Loss: 0.910001, Accuracy: 89.33%\n",
      "Batch 30, Loss: 0.841643, Accuracy: 89.32%\n",
      "Batch 31, Loss: 0.894438, Accuracy: 89.21%\n",
      "Batch 32, Loss: 0.881119, Accuracy: 89.06%\n",
      "Batch 33, Loss: 0.844458, Accuracy: 89.06%\n",
      "Batch 34, Loss: 0.846297, Accuracy: 89.11%\n",
      "Batch 35, Loss: 0.835959, Accuracy: 89.15%\n",
      "Batch 36, Loss: 0.812425, Accuracy: 89.28%\n",
      "Batch 37, Loss: 0.795358, Accuracy: 89.40%\n",
      "Batch 38, Loss: 0.839408, Accuracy: 89.47%\n",
      "Batch 39, Loss: 0.858822, Accuracy: 89.46%\n",
      "Batch 40, Loss: 0.872961, Accuracy: 89.41%\n",
      "Batch 41, Loss: 0.808458, Accuracy: 89.52%\n",
      "Batch 42, Loss: 0.864375, Accuracy: 89.43%\n",
      "Batch 43, Loss: 0.842910, Accuracy: 89.46%\n",
      "Batch 44, Loss: 0.813102, Accuracy: 89.49%\n",
      "Batch 45, Loss: 0.789808, Accuracy: 89.62%\n",
      "Batch 46, Loss: 0.881967, Accuracy: 89.57%\n",
      "Batch 47, Loss: 0.840545, Accuracy: 89.56%\n",
      "Batch 48, Loss: 0.839561, Accuracy: 89.58%\n",
      "Batch 49, Loss: 0.805801, Accuracy: 89.67%\n",
      "Batch 50, Loss: 0.844959, Accuracy: 89.69%\n",
      "Batch 51, Loss: 0.822149, Accuracy: 89.74%\n",
      "Batch 52, Loss: 0.809249, Accuracy: 89.81%\n",
      "Batch 53, Loss: 0.849866, Accuracy: 89.80%\n",
      "Batch 54, Loss: 0.884087, Accuracy: 89.73%\n",
      "Batch 55, Loss: 0.832498, Accuracy: 89.74%\n",
      "Batch 56, Loss: 0.793995, Accuracy: 89.87%\n",
      "Batch 57, Loss: 0.862284, Accuracy: 89.80%\n",
      "Batch 58, Loss: 0.859189, Accuracy: 89.79%\n",
      "Batch 59, Loss: 0.845234, Accuracy: 89.78%\n",
      "Batch 60, Loss: 0.794156, Accuracy: 89.87%\n",
      "Batch 61, Loss: 0.815875, Accuracy: 89.91%\n",
      "Batch 62, Loss: 0.840476, Accuracy: 89.94%\n",
      "Batch 63, Loss: 0.840788, Accuracy: 89.96%\n",
      "Batch 64, Loss: 0.817428, Accuracy: 89.99%\n",
      "Batch 65, Loss: 0.830111, Accuracy: 90.02%\n",
      "Batch 66, Loss: 0.847605, Accuracy: 90.01%\n",
      "Batch 67, Loss: 0.858642, Accuracy: 90.00%\n",
      "Batch 68, Loss: 0.839396, Accuracy: 89.98%\n",
      "Batch 69, Loss: 0.826169, Accuracy: 90.01%\n",
      "Batch 70, Loss: 0.818453, Accuracy: 90.07%\n",
      "Batch 71, Loss: 0.837958, Accuracy: 90.10%\n",
      "Batch 72, Loss: 0.797709, Accuracy: 90.17%\n",
      "Batch 73, Loss: 0.824104, Accuracy: 90.20%\n",
      "Batch 74, Loss: 0.899586, Accuracy: 90.14%\n",
      "Batch 75, Loss: 0.848954, Accuracy: 90.12%\n",
      "Batch 76, Loss: 0.834111, Accuracy: 90.11%\n",
      "Batch 77, Loss: 0.792620, Accuracy: 90.20%\n",
      "Batch 78, Loss: 0.883870, Accuracy: 90.14%\n",
      "Batch 79, Loss: 0.851749, Accuracy: 90.13%\n",
      "Batch 80, Loss: 0.866983, Accuracy: 90.10%\n",
      "Batch 81, Loss: 0.803440, Accuracy: 90.12%\n",
      "Batch 82, Loss: 0.857596, Accuracy: 90.09%\n",
      "Batch 83, Loss: 0.786964, Accuracy: 90.15%\n",
      "Batch 84, Loss: 0.824330, Accuracy: 90.18%\n",
      "Batch 85, Loss: 0.836765, Accuracy: 90.20%\n",
      "Batch 86, Loss: 0.853214, Accuracy: 90.17%\n",
      "Batch 87, Loss: 0.827776, Accuracy: 90.19%\n",
      "Batch 88, Loss: 0.874465, Accuracy: 90.16%\n",
      "Batch 89, Loss: 0.832274, Accuracy: 90.19%\n",
      "Batch 90, Loss: 0.858509, Accuracy: 90.17%\n",
      "Batch 91, Loss: 0.812449, Accuracy: 90.21%\n",
      "Batch 92, Loss: 0.898170, Accuracy: 90.15%\n",
      "Batch 93, Loss: 0.815974, Accuracy: 90.19%\n",
      "Batch 94, Loss: 0.857730, Accuracy: 90.14%\n",
      "Batch 95, Loss: 0.855901, Accuracy: 90.12%\n",
      "Batch 96, Loss: 0.881563, Accuracy: 90.07%\n",
      "Batch 97, Loss: 0.878828, Accuracy: 90.05%\n",
      "Batch 98, Loss: 0.877999, Accuracy: 90.00%\n",
      "Batch 99, Loss: 0.855549, Accuracy: 89.99%\n",
      "Batch 100, Loss: 0.836919, Accuracy: 90.02%\n",
      "Batch 101, Loss: 0.862292, Accuracy: 90.01%\n",
      "Batch 102, Loss: 0.915596, Accuracy: 89.92%\n",
      "Batch 103, Loss: 0.844175, Accuracy: 89.93%\n",
      "Batch 104, Loss: 0.862239, Accuracy: 89.90%\n",
      "Batch 105, Loss: 0.823639, Accuracy: 89.91%\n",
      "Batch 106, Loss: 0.820492, Accuracy: 89.93%\n",
      "Batch 107, Loss: 0.922825, Accuracy: 89.87%\n",
      "Batch 108, Loss: 0.862466, Accuracy: 89.84%\n",
      "Batch 109, Loss: 0.869981, Accuracy: 89.82%\n",
      "Batch 110, Loss: 0.880231, Accuracy: 89.79%\n",
      "Batch 111, Loss: 0.830938, Accuracy: 89.79%\n",
      "Batch 112, Loss: 0.842522, Accuracy: 89.80%\n",
      "Batch 113, Loss: 0.820116, Accuracy: 89.82%\n",
      "Batch 114, Loss: 0.842587, Accuracy: 89.82%\n",
      "Batch 115, Loss: 0.849911, Accuracy: 89.81%\n",
      "Batch 116, Loss: 0.816401, Accuracy: 89.83%\n",
      "Batch 117, Loss: 0.839662, Accuracy: 89.84%\n",
      "Batch 118, Loss: 0.904071, Accuracy: 89.78%\n",
      "Batch 119, Loss: 0.831131, Accuracy: 89.78%\n",
      "Batch 120, Loss: 0.856654, Accuracy: 89.78%\n",
      "Batch 121, Loss: 0.877362, Accuracy: 89.75%\n",
      "Batch 122, Loss: 0.858475, Accuracy: 89.74%\n",
      "Batch 123, Loss: 0.880449, Accuracy: 89.71%\n",
      "Batch 124, Loss: 0.834985, Accuracy: 89.72%\n",
      "Batch 125, Loss: 0.789639, Accuracy: 89.76%\n",
      "Batch 126, Loss: 0.830100, Accuracy: 89.77%\n",
      "Batch 127, Loss: 0.835991, Accuracy: 89.78%\n",
      "Batch 128, Loss: 0.840715, Accuracy: 89.78%\n",
      "Batch 129, Loss: 0.825711, Accuracy: 89.80%\n",
      "Batch 130, Loss: 0.884205, Accuracy: 89.77%\n",
      "Batch 131, Loss: 0.837306, Accuracy: 89.78%\n",
      "Batch 132, Loss: 0.877166, Accuracy: 89.75%\n",
      "Batch 133, Loss: 0.818489, Accuracy: 89.78%\n",
      "Batch 134, Loss: 0.851699, Accuracy: 89.77%\n",
      "Batch 135, Loss: 0.898165, Accuracy: 89.73%\n",
      "Batch 136, Loss: 0.841131, Accuracy: 89.73%\n",
      "Batch 137, Loss: 0.848878, Accuracy: 89.71%\n",
      "Batch 138, Loss: 0.863428, Accuracy: 89.71%\n",
      "Batch 139, Loss: 0.841479, Accuracy: 89.73%\n",
      "Batch 140, Loss: 0.862489, Accuracy: 89.72%\n",
      "Batch 141, Loss: 0.858045, Accuracy: 89.71%\n",
      "Batch 142, Loss: 0.873172, Accuracy: 89.69%\n",
      "Batch 143, Loss: 0.862101, Accuracy: 89.69%\n",
      "Batch 144, Loss: 0.875865, Accuracy: 89.66%\n",
      "Batch 145, Loss: 0.910982, Accuracy: 89.61%\n",
      "Batch 146, Loss: 0.796148, Accuracy: 89.65%\n",
      "Batch 147, Loss: 0.910318, Accuracy: 89.60%\n",
      "Batch 148, Loss: 0.828574, Accuracy: 89.61%\n",
      "Batch 149, Loss: 0.825853, Accuracy: 89.63%\n",
      "Batch 150, Loss: 0.819924, Accuracy: 89.66%\n",
      "Batch 151, Loss: 0.870571, Accuracy: 89.64%\n",
      "Batch 152, Loss: 0.812490, Accuracy: 89.68%\n",
      "Batch 153, Loss: 0.881554, Accuracy: 89.65%\n",
      "Batch 154, Loss: 0.834965, Accuracy: 89.65%\n",
      "Batch 155, Loss: 0.818454, Accuracy: 89.67%\n",
      "Batch 156, Loss: 0.850071, Accuracy: 89.66%\n",
      "Batch 157, Loss: 0.879168, Accuracy: 89.64%\n",
      "Batch 158, Loss: 0.840532, Accuracy: 89.65%\n",
      "Batch 159, Loss: 0.859881, Accuracy: 89.64%\n",
      "Batch 160, Loss: 0.866642, Accuracy: 89.63%\n",
      "Batch 161, Loss: 0.869894, Accuracy: 89.62%\n",
      "Batch 162, Loss: 0.850911, Accuracy: 89.62%\n",
      "Batch 163, Loss: 0.841202, Accuracy: 89.63%\n",
      "Batch 164, Loss: 0.826289, Accuracy: 89.64%\n",
      "Batch 165, Loss: 0.890064, Accuracy: 89.62%\n",
      "Batch 166, Loss: 0.884702, Accuracy: 89.60%\n",
      "Batch 167, Loss: 0.819393, Accuracy: 89.62%\n",
      "Batch 168, Loss: 0.827416, Accuracy: 89.64%\n",
      "Batch 169, Loss: 0.847834, Accuracy: 89.63%\n",
      "Batch 170, Loss: 0.831160, Accuracy: 89.64%\n",
      "Batch 171, Loss: 0.829966, Accuracy: 89.65%\n",
      "Batch 172, Loss: 0.869850, Accuracy: 89.63%\n",
      "Batch 173, Loss: 0.876155, Accuracy: 89.62%\n",
      "Batch 174, Loss: 0.813220, Accuracy: 89.65%\n",
      "Batch 175, Loss: 0.860927, Accuracy: 89.64%\n",
      "Batch 176, Loss: 0.892601, Accuracy: 89.62%\n",
      "Batch 177, Loss: 0.815580, Accuracy: 89.65%\n",
      "Batch 178, Loss: 0.829599, Accuracy: 89.66%\n",
      "Batch 179, Loss: 0.833770, Accuracy: 89.66%\n",
      "Batch 180, Loss: 0.829872, Accuracy: 89.68%\n",
      "Batch 181, Loss: 0.847536, Accuracy: 89.69%\n",
      "Batch 182, Loss: 0.834262, Accuracy: 89.70%\n",
      "Batch 183, Loss: 0.798456, Accuracy: 89.73%\n",
      "Batch 184, Loss: 0.826772, Accuracy: 89.73%\n",
      "Batch 185, Loss: 0.878692, Accuracy: 89.71%\n",
      "Batch 186, Loss: 0.781020, Accuracy: 89.75%\n",
      "Batch 187, Loss: 0.861889, Accuracy: 89.74%\n",
      "Batch 188, Loss: 0.837647, Accuracy: 89.74%\n",
      "Batch 189, Loss: 0.831790, Accuracy: 89.75%\n",
      "Batch 190, Loss: 0.827741, Accuracy: 89.76%\n",
      "Batch 191, Loss: 0.851154, Accuracy: 89.76%\n",
      "Batch 192, Loss: 0.826944, Accuracy: 89.77%\n",
      "Batch 193, Loss: 0.846562, Accuracy: 89.77%\n",
      "Batch 194, Loss: 0.859384, Accuracy: 89.77%\n",
      "Batch 195, Loss: 0.794161, Accuracy: 89.80%\n",
      "Batch 196, Loss: 0.874678, Accuracy: 89.78%\n",
      "Batch 197, Loss: 0.844846, Accuracy: 89.78%\n",
      "Batch 198, Loss: 0.802639, Accuracy: 89.80%\n",
      "Batch 199, Loss: 0.859315, Accuracy: 89.79%\n",
      "Batch 200, Loss: 0.889294, Accuracy: 89.77%\n",
      "Batch 201, Loss: 0.805045, Accuracy: 89.79%\n",
      "Batch 202, Loss: 0.867088, Accuracy: 89.78%\n",
      "Batch 203, Loss: 0.850097, Accuracy: 89.79%\n",
      "Batch 204, Loss: 0.853922, Accuracy: 89.77%\n",
      "Batch 205, Loss: 0.850009, Accuracy: 89.77%\n",
      "Batch 206, Loss: 0.872220, Accuracy: 89.76%\n",
      "Batch 207, Loss: 0.827546, Accuracy: 89.77%\n",
      "Batch 208, Loss: 0.892303, Accuracy: 89.75%\n",
      "Batch 209, Loss: 0.840851, Accuracy: 89.74%\n",
      "Batch 210, Loss: 0.854124, Accuracy: 89.75%\n",
      "Batch 211, Loss: 0.821651, Accuracy: 89.77%\n",
      "Batch 212, Loss: 0.854735, Accuracy: 89.76%\n",
      "Batch 213, Loss: 0.866816, Accuracy: 89.76%\n",
      "Training - Epoch 12, Loss: 0.846707, Accuracy: 89.76%\n",
      "Validation Batch 1, Loss: 0.819740, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.791573, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.815717, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.835151, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.818487, Accuracy: 93.12%\n",
      "Validation Batch 6, Loss: 0.793306, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.794630, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.855828, Accuracy: 93.16%\n",
      "Validation Batch 9, Loss: 0.889142, Accuracy: 92.19%\n",
      "Validation Batch 10, Loss: 0.787683, Accuracy: 92.66%\n",
      "Validation Batch 11, Loss: 0.826947, Accuracy: 92.61%\n",
      "Validation Batch 12, Loss: 0.826997, Accuracy: 92.58%\n",
      "Validation Batch 13, Loss: 0.832584, Accuracy: 92.43%\n",
      "Validation Batch 14, Loss: 0.833530, Accuracy: 92.41%\n",
      "Validation Batch 15, Loss: 0.798324, Accuracy: 92.50%\n",
      "Validation Batch 16, Loss: 0.838264, Accuracy: 92.38%\n",
      "Validation Batch 17, Loss: 0.857329, Accuracy: 92.19%\n",
      "Validation Batch 18, Loss: 0.797428, Accuracy: 92.36%\n",
      "Validation Batch 19, Loss: 0.845722, Accuracy: 92.27%\n",
      "Validation Batch 20, Loss: 0.815932, Accuracy: 92.50%\n",
      "Validation Batch 21, Loss: 0.858780, Accuracy: 92.34%\n",
      "Validation Batch 22, Loss: 0.794575, Accuracy: 92.54%\n",
      "Validation Batch 23, Loss: 0.858972, Accuracy: 92.39%\n",
      "Validation Batch 24, Loss: 0.827570, Accuracy: 92.38%\n",
      "Validation Batch 25, Loss: 0.795648, Accuracy: 92.50%\n",
      "Validation Batch 26, Loss: 0.835902, Accuracy: 92.43%\n",
      "Validation Batch 27, Loss: 0.792530, Accuracy: 92.48%\n",
      "Validation - Epoch 12, Loss: 0.823640, Accuracy: 92.48%\n",
      "Patience—12\n",
      "Epoch 13\n",
      "Batch 1, Loss: 0.870804, Accuracy: 87.50%\n",
      "Batch 2, Loss: 0.817156, Accuracy: 89.84%\n",
      "Batch 3, Loss: 0.886147, Accuracy: 88.02%\n",
      "Batch 4, Loss: 0.864804, Accuracy: 87.89%\n",
      "Batch 5, Loss: 0.858236, Accuracy: 88.12%\n",
      "Batch 6, Loss: 0.807598, Accuracy: 89.06%\n",
      "Batch 7, Loss: 0.842306, Accuracy: 89.29%\n",
      "Batch 8, Loss: 0.909670, Accuracy: 88.48%\n",
      "Batch 9, Loss: 0.822632, Accuracy: 88.89%\n",
      "Batch 10, Loss: 0.867308, Accuracy: 88.75%\n",
      "Batch 11, Loss: 0.853646, Accuracy: 88.78%\n",
      "Batch 12, Loss: 0.860889, Accuracy: 88.67%\n",
      "Batch 13, Loss: 0.838538, Accuracy: 88.82%\n",
      "Batch 14, Loss: 0.822440, Accuracy: 89.06%\n",
      "Batch 15, Loss: 0.872656, Accuracy: 88.96%\n",
      "Batch 16, Loss: 0.853025, Accuracy: 88.96%\n",
      "Batch 17, Loss: 0.851011, Accuracy: 89.06%\n",
      "Batch 18, Loss: 0.821503, Accuracy: 89.15%\n",
      "Batch 19, Loss: 0.909901, Accuracy: 88.82%\n",
      "Batch 20, Loss: 0.856350, Accuracy: 88.91%\n",
      "Batch 21, Loss: 0.790561, Accuracy: 89.21%\n",
      "Batch 22, Loss: 0.881437, Accuracy: 89.06%\n",
      "Batch 23, Loss: 0.856700, Accuracy: 89.06%\n",
      "Batch 24, Loss: 0.871883, Accuracy: 89.06%\n",
      "Batch 25, Loss: 0.811643, Accuracy: 89.19%\n",
      "Batch 26, Loss: 0.876976, Accuracy: 89.00%\n",
      "Batch 27, Loss: 0.822518, Accuracy: 89.12%\n",
      "Batch 28, Loss: 0.792521, Accuracy: 89.40%\n",
      "Batch 29, Loss: 0.874709, Accuracy: 89.28%\n",
      "Batch 30, Loss: 0.835608, Accuracy: 89.38%\n",
      "Batch 31, Loss: 0.816662, Accuracy: 89.52%\n",
      "Batch 32, Loss: 0.844995, Accuracy: 89.50%\n",
      "Batch 33, Loss: 0.843375, Accuracy: 89.49%\n",
      "Batch 34, Loss: 0.829409, Accuracy: 89.52%\n",
      "Batch 35, Loss: 0.848583, Accuracy: 89.51%\n",
      "Batch 36, Loss: 0.853066, Accuracy: 89.50%\n",
      "Batch 37, Loss: 0.861614, Accuracy: 89.44%\n",
      "Batch 38, Loss: 0.844305, Accuracy: 89.47%\n",
      "Batch 39, Loss: 0.782318, Accuracy: 89.66%\n",
      "Batch 40, Loss: 0.866314, Accuracy: 89.65%\n",
      "Batch 41, Loss: 0.840283, Accuracy: 89.63%\n",
      "Batch 42, Loss: 0.852293, Accuracy: 89.66%\n",
      "Batch 43, Loss: 0.849279, Accuracy: 89.68%\n",
      "Batch 44, Loss: 0.856799, Accuracy: 89.67%\n",
      "Batch 45, Loss: 0.860566, Accuracy: 89.69%\n",
      "Batch 46, Loss: 0.836620, Accuracy: 89.71%\n",
      "Batch 47, Loss: 0.803494, Accuracy: 89.83%\n",
      "Batch 48, Loss: 0.838279, Accuracy: 89.81%\n",
      "Batch 49, Loss: 0.800945, Accuracy: 89.92%\n",
      "Batch 50, Loss: 0.825489, Accuracy: 89.94%\n",
      "Batch 51, Loss: 0.779832, Accuracy: 90.04%\n",
      "Batch 52, Loss: 0.812125, Accuracy: 90.14%\n",
      "Batch 53, Loss: 0.821466, Accuracy: 90.18%\n",
      "Batch 54, Loss: 0.868773, Accuracy: 90.13%\n",
      "Batch 55, Loss: 0.841337, Accuracy: 90.14%\n",
      "Batch 56, Loss: 0.863998, Accuracy: 90.04%\n",
      "Batch 57, Loss: 0.810675, Accuracy: 90.13%\n",
      "Batch 58, Loss: 0.872954, Accuracy: 90.09%\n",
      "Batch 59, Loss: 0.816446, Accuracy: 90.10%\n",
      "Batch 60, Loss: 0.830998, Accuracy: 90.13%\n",
      "Batch 61, Loss: 0.853306, Accuracy: 90.11%\n",
      "Batch 62, Loss: 0.829385, Accuracy: 90.12%\n",
      "Batch 63, Loss: 0.829740, Accuracy: 90.13%\n",
      "Batch 64, Loss: 0.792704, Accuracy: 90.23%\n",
      "Batch 65, Loss: 0.881509, Accuracy: 90.14%\n",
      "Batch 66, Loss: 0.819512, Accuracy: 90.18%\n",
      "Batch 67, Loss: 0.882772, Accuracy: 90.11%\n",
      "Batch 68, Loss: 0.855170, Accuracy: 90.10%\n",
      "Batch 69, Loss: 0.795594, Accuracy: 90.17%\n",
      "Batch 70, Loss: 0.814428, Accuracy: 90.20%\n",
      "Batch 71, Loss: 0.915064, Accuracy: 90.10%\n",
      "Batch 72, Loss: 0.834243, Accuracy: 90.13%\n",
      "Batch 73, Loss: 0.841020, Accuracy: 90.13%\n",
      "Batch 74, Loss: 0.824233, Accuracy: 90.16%\n",
      "Batch 75, Loss: 0.922479, Accuracy: 90.08%\n",
      "Batch 76, Loss: 0.820951, Accuracy: 90.13%\n",
      "Batch 77, Loss: 0.834066, Accuracy: 90.16%\n",
      "Batch 78, Loss: 0.882588, Accuracy: 90.10%\n",
      "Batch 79, Loss: 0.818381, Accuracy: 90.15%\n",
      "Batch 80, Loss: 0.861829, Accuracy: 90.12%\n",
      "Batch 81, Loss: 0.834458, Accuracy: 90.12%\n",
      "Batch 82, Loss: 0.884909, Accuracy: 90.09%\n",
      "Batch 83, Loss: 0.809758, Accuracy: 90.14%\n",
      "Batch 84, Loss: 0.840815, Accuracy: 90.14%\n",
      "Batch 85, Loss: 0.826117, Accuracy: 90.15%\n",
      "Batch 86, Loss: 0.915281, Accuracy: 90.06%\n",
      "Batch 87, Loss: 0.874662, Accuracy: 90.03%\n",
      "Batch 88, Loss: 0.846087, Accuracy: 90.06%\n",
      "Batch 89, Loss: 0.895899, Accuracy: 89.99%\n",
      "Batch 90, Loss: 0.818289, Accuracy: 90.02%\n",
      "Batch 91, Loss: 0.833522, Accuracy: 90.04%\n",
      "Batch 92, Loss: 0.840799, Accuracy: 90.05%\n",
      "Batch 93, Loss: 0.819277, Accuracy: 90.07%\n",
      "Batch 94, Loss: 0.840599, Accuracy: 90.09%\n",
      "Batch 95, Loss: 0.903882, Accuracy: 90.02%\n",
      "Batch 96, Loss: 0.763426, Accuracy: 90.10%\n",
      "Batch 97, Loss: 0.865022, Accuracy: 90.08%\n",
      "Batch 98, Loss: 0.821504, Accuracy: 90.11%\n",
      "Batch 99, Loss: 0.901481, Accuracy: 90.04%\n",
      "Batch 100, Loss: 0.807467, Accuracy: 90.08%\n",
      "Batch 101, Loss: 0.800256, Accuracy: 90.11%\n",
      "Batch 102, Loss: 0.784837, Accuracy: 90.18%\n",
      "Batch 103, Loss: 0.832215, Accuracy: 90.20%\n",
      "Batch 104, Loss: 0.861495, Accuracy: 90.17%\n",
      "Batch 105, Loss: 0.847197, Accuracy: 90.15%\n",
      "Batch 106, Loss: 0.836452, Accuracy: 90.15%\n",
      "Batch 107, Loss: 0.866774, Accuracy: 90.13%\n",
      "Batch 108, Loss: 0.810500, Accuracy: 90.15%\n",
      "Batch 109, Loss: 0.818222, Accuracy: 90.17%\n",
      "Batch 110, Loss: 0.868935, Accuracy: 90.14%\n",
      "Batch 111, Loss: 0.810626, Accuracy: 90.16%\n",
      "Batch 112, Loss: 0.808167, Accuracy: 90.19%\n",
      "Batch 113, Loss: 0.849809, Accuracy: 90.18%\n",
      "Batch 114, Loss: 0.833573, Accuracy: 90.19%\n",
      "Batch 115, Loss: 0.872102, Accuracy: 90.15%\n",
      "Batch 116, Loss: 0.873017, Accuracy: 90.11%\n",
      "Batch 117, Loss: 0.862616, Accuracy: 90.10%\n",
      "Batch 118, Loss: 0.845521, Accuracy: 90.10%\n",
      "Batch 119, Loss: 0.873986, Accuracy: 90.07%\n",
      "Batch 120, Loss: 0.877080, Accuracy: 90.05%\n",
      "Batch 121, Loss: 0.838519, Accuracy: 90.04%\n",
      "Batch 122, Loss: 0.897702, Accuracy: 89.98%\n",
      "Batch 123, Loss: 0.901756, Accuracy: 89.94%\n",
      "Batch 124, Loss: 0.835360, Accuracy: 89.93%\n",
      "Batch 125, Loss: 0.823801, Accuracy: 89.95%\n",
      "Batch 126, Loss: 0.867192, Accuracy: 89.92%\n",
      "Batch 127, Loss: 0.861788, Accuracy: 89.91%\n",
      "Batch 128, Loss: 0.816809, Accuracy: 89.94%\n",
      "Batch 129, Loss: 0.808244, Accuracy: 89.96%\n",
      "Batch 130, Loss: 0.787641, Accuracy: 90.00%\n",
      "Batch 131, Loss: 0.875736, Accuracy: 89.99%\n",
      "Batch 132, Loss: 0.855286, Accuracy: 89.97%\n",
      "Batch 133, Loss: 0.811094, Accuracy: 90.00%\n",
      "Batch 134, Loss: 0.829896, Accuracy: 90.02%\n",
      "Batch 135, Loss: 0.812545, Accuracy: 90.05%\n",
      "Batch 136, Loss: 0.808708, Accuracy: 90.07%\n",
      "Batch 137, Loss: 0.887668, Accuracy: 90.03%\n",
      "Batch 138, Loss: 0.824865, Accuracy: 90.06%\n",
      "Batch 139, Loss: 0.879434, Accuracy: 90.04%\n",
      "Batch 140, Loss: 0.825392, Accuracy: 90.06%\n",
      "Batch 141, Loss: 0.861052, Accuracy: 90.05%\n",
      "Batch 142, Loss: 0.839227, Accuracy: 90.05%\n",
      "Batch 143, Loss: 0.852630, Accuracy: 90.06%\n",
      "Batch 144, Loss: 0.833126, Accuracy: 90.06%\n",
      "Batch 145, Loss: 0.844190, Accuracy: 90.06%\n",
      "Batch 146, Loss: 0.827433, Accuracy: 90.08%\n",
      "Batch 147, Loss: 0.888129, Accuracy: 90.04%\n",
      "Batch 148, Loss: 0.798421, Accuracy: 90.08%\n",
      "Batch 149, Loss: 0.839686, Accuracy: 90.09%\n",
      "Batch 150, Loss: 0.862380, Accuracy: 90.08%\n",
      "Batch 151, Loss: 0.780876, Accuracy: 90.12%\n",
      "Batch 152, Loss: 0.876971, Accuracy: 90.09%\n",
      "Batch 153, Loss: 0.844508, Accuracy: 90.08%\n",
      "Batch 154, Loss: 0.825953, Accuracy: 90.10%\n",
      "Batch 155, Loss: 0.858577, Accuracy: 90.09%\n",
      "Batch 156, Loss: 0.875605, Accuracy: 90.06%\n",
      "Batch 157, Loss: 0.886356, Accuracy: 90.04%\n",
      "Batch 158, Loss: 0.849876, Accuracy: 90.04%\n",
      "Batch 159, Loss: 0.851651, Accuracy: 90.05%\n",
      "Batch 160, Loss: 0.866197, Accuracy: 90.04%\n",
      "Batch 161, Loss: 0.863238, Accuracy: 90.03%\n",
      "Batch 162, Loss: 0.779639, Accuracy: 90.08%\n",
      "Batch 163, Loss: 0.791345, Accuracy: 90.11%\n",
      "Batch 164, Loss: 0.815706, Accuracy: 90.13%\n",
      "Batch 165, Loss: 0.833263, Accuracy: 90.13%\n",
      "Batch 166, Loss: 0.898840, Accuracy: 90.10%\n",
      "Batch 167, Loss: 0.801711, Accuracy: 90.13%\n",
      "Batch 168, Loss: 0.812899, Accuracy: 90.15%\n",
      "Batch 169, Loss: 0.871878, Accuracy: 90.12%\n",
      "Batch 170, Loss: 0.816071, Accuracy: 90.14%\n",
      "Batch 171, Loss: 0.862930, Accuracy: 90.12%\n",
      "Batch 172, Loss: 0.780293, Accuracy: 90.16%\n",
      "Batch 173, Loss: 0.852330, Accuracy: 90.16%\n",
      "Batch 174, Loss: 0.785493, Accuracy: 90.18%\n",
      "Batch 175, Loss: 0.869126, Accuracy: 90.17%\n",
      "Batch 176, Loss: 0.827871, Accuracy: 90.18%\n",
      "Batch 177, Loss: 0.807294, Accuracy: 90.21%\n",
      "Batch 178, Loss: 0.852972, Accuracy: 90.21%\n",
      "Batch 179, Loss: 0.842119, Accuracy: 90.21%\n",
      "Batch 180, Loss: 0.830114, Accuracy: 90.23%\n",
      "Batch 181, Loss: 0.865755, Accuracy: 90.22%\n",
      "Batch 182, Loss: 0.842381, Accuracy: 90.22%\n",
      "Batch 183, Loss: 0.874783, Accuracy: 90.20%\n",
      "Batch 184, Loss: 0.915925, Accuracy: 90.17%\n",
      "Batch 185, Loss: 0.845040, Accuracy: 90.16%\n",
      "Batch 186, Loss: 0.815526, Accuracy: 90.18%\n",
      "Batch 187, Loss: 0.870780, Accuracy: 90.17%\n",
      "Batch 188, Loss: 0.812672, Accuracy: 90.18%\n",
      "Batch 189, Loss: 0.879495, Accuracy: 90.15%\n",
      "Batch 190, Loss: 0.855729, Accuracy: 90.15%\n",
      "Batch 191, Loss: 0.871371, Accuracy: 90.12%\n",
      "Batch 192, Loss: 0.853569, Accuracy: 90.12%\n",
      "Batch 193, Loss: 0.823744, Accuracy: 90.13%\n",
      "Batch 194, Loss: 0.884749, Accuracy: 90.11%\n",
      "Batch 195, Loss: 0.870127, Accuracy: 90.10%\n",
      "Batch 196, Loss: 0.824373, Accuracy: 90.11%\n",
      "Batch 197, Loss: 0.872204, Accuracy: 90.09%\n",
      "Batch 198, Loss: 0.803008, Accuracy: 90.11%\n",
      "Batch 199, Loss: 0.829498, Accuracy: 90.12%\n",
      "Batch 200, Loss: 0.832865, Accuracy: 90.12%\n",
      "Batch 201, Loss: 0.871011, Accuracy: 90.12%\n",
      "Batch 202, Loss: 0.922231, Accuracy: 90.08%\n",
      "Batch 203, Loss: 0.870973, Accuracy: 90.06%\n",
      "Batch 204, Loss: 0.823471, Accuracy: 90.07%\n",
      "Batch 205, Loss: 0.788595, Accuracy: 90.09%\n",
      "Batch 206, Loss: 0.792851, Accuracy: 90.12%\n",
      "Batch 207, Loss: 0.852632, Accuracy: 90.10%\n",
      "Batch 208, Loss: 0.824489, Accuracy: 90.11%\n",
      "Batch 209, Loss: 0.833844, Accuracy: 90.11%\n",
      "Batch 210, Loss: 0.891758, Accuracy: 90.09%\n",
      "Batch 211, Loss: 0.858576, Accuracy: 90.08%\n",
      "Batch 212, Loss: 0.861686, Accuracy: 90.07%\n",
      "Batch 213, Loss: 0.775915, Accuracy: 90.10%\n",
      "Training - Epoch 13, Loss: 0.843564, Accuracy: 90.10%\n",
      "Validation Batch 1, Loss: 0.819818, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.791612, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.815960, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.835268, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.818645, Accuracy: 93.12%\n",
      "Validation Batch 6, Loss: 0.793163, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.794668, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.856729, Accuracy: 92.97%\n",
      "Validation Batch 9, Loss: 0.889159, Accuracy: 92.01%\n",
      "Validation Batch 10, Loss: 0.787537, Accuracy: 92.50%\n",
      "Validation Batch 11, Loss: 0.827256, Accuracy: 92.47%\n",
      "Validation Batch 12, Loss: 0.827212, Accuracy: 92.45%\n",
      "Validation Batch 13, Loss: 0.832599, Accuracy: 92.31%\n",
      "Validation Batch 14, Loss: 0.833637, Accuracy: 92.30%\n",
      "Validation Batch 15, Loss: 0.798812, Accuracy: 92.40%\n",
      "Validation Batch 16, Loss: 0.838354, Accuracy: 92.29%\n",
      "Validation Batch 17, Loss: 0.857131, Accuracy: 92.10%\n",
      "Validation Batch 18, Loss: 0.797100, Accuracy: 92.27%\n",
      "Validation Batch 19, Loss: 0.845565, Accuracy: 92.19%\n",
      "Validation Batch 20, Loss: 0.817180, Accuracy: 92.34%\n",
      "Validation Batch 21, Loss: 0.858383, Accuracy: 92.19%\n",
      "Validation Batch 22, Loss: 0.794879, Accuracy: 92.40%\n",
      "Validation Batch 23, Loss: 0.858887, Accuracy: 92.26%\n",
      "Validation Batch 24, Loss: 0.827908, Accuracy: 92.19%\n",
      "Validation Batch 25, Loss: 0.796233, Accuracy: 92.31%\n",
      "Validation Batch 26, Loss: 0.836130, Accuracy: 92.25%\n",
      "Validation Batch 27, Loss: 0.793068, Accuracy: 92.31%\n",
      "Validation - Epoch 13, Loss: 0.823811, Accuracy: 92.31%\n",
      "Patience—13\n",
      "Epoch 14\n",
      "Batch 1, Loss: 0.822527, Accuracy: 92.19%\n",
      "Batch 2, Loss: 0.845062, Accuracy: 90.62%\n",
      "Batch 3, Loss: 0.842193, Accuracy: 90.62%\n",
      "Batch 4, Loss: 0.815499, Accuracy: 91.41%\n",
      "Batch 5, Loss: 0.848063, Accuracy: 91.25%\n",
      "Batch 6, Loss: 0.854425, Accuracy: 91.15%\n",
      "Batch 7, Loss: 0.801595, Accuracy: 91.74%\n",
      "Batch 8, Loss: 0.863734, Accuracy: 91.02%\n",
      "Batch 9, Loss: 0.886122, Accuracy: 90.45%\n",
      "Batch 10, Loss: 0.829932, Accuracy: 90.62%\n",
      "Batch 11, Loss: 0.819074, Accuracy: 90.77%\n",
      "Batch 12, Loss: 0.958110, Accuracy: 89.71%\n",
      "Batch 13, Loss: 0.868863, Accuracy: 89.42%\n",
      "Batch 14, Loss: 0.885323, Accuracy: 89.29%\n",
      "Batch 15, Loss: 0.772242, Accuracy: 89.79%\n",
      "Batch 16, Loss: 0.821667, Accuracy: 89.94%\n",
      "Batch 17, Loss: 0.829055, Accuracy: 89.98%\n",
      "Batch 18, Loss: 0.923407, Accuracy: 89.50%\n",
      "Batch 19, Loss: 0.832555, Accuracy: 89.56%\n",
      "Batch 20, Loss: 0.831316, Accuracy: 89.69%\n",
      "Batch 21, Loss: 0.799755, Accuracy: 89.88%\n",
      "Batch 22, Loss: 0.823073, Accuracy: 90.06%\n",
      "Batch 23, Loss: 0.817345, Accuracy: 90.22%\n",
      "Batch 24, Loss: 0.853673, Accuracy: 90.23%\n",
      "Batch 25, Loss: 0.876398, Accuracy: 90.12%\n",
      "Batch 26, Loss: 0.808671, Accuracy: 90.26%\n",
      "Batch 27, Loss: 0.855758, Accuracy: 90.22%\n",
      "Batch 28, Loss: 0.881381, Accuracy: 90.01%\n",
      "Batch 29, Loss: 0.823562, Accuracy: 90.09%\n",
      "Batch 30, Loss: 0.888161, Accuracy: 89.90%\n",
      "Batch 31, Loss: 0.835987, Accuracy: 89.92%\n",
      "Batch 32, Loss: 0.882048, Accuracy: 89.79%\n",
      "Batch 33, Loss: 0.801398, Accuracy: 89.96%\n",
      "Batch 34, Loss: 0.900906, Accuracy: 89.75%\n",
      "Batch 35, Loss: 0.824496, Accuracy: 89.82%\n",
      "Batch 36, Loss: 0.849119, Accuracy: 89.80%\n",
      "Batch 37, Loss: 0.843748, Accuracy: 89.82%\n",
      "Batch 38, Loss: 0.885768, Accuracy: 89.72%\n",
      "Batch 39, Loss: 0.946828, Accuracy: 89.42%\n",
      "Batch 40, Loss: 0.867321, Accuracy: 89.41%\n",
      "Batch 41, Loss: 0.812338, Accuracy: 89.48%\n",
      "Batch 42, Loss: 0.830087, Accuracy: 89.55%\n",
      "Batch 43, Loss: 0.836677, Accuracy: 89.53%\n",
      "Batch 44, Loss: 0.896421, Accuracy: 89.45%\n",
      "Batch 45, Loss: 0.874895, Accuracy: 89.34%\n",
      "Batch 46, Loss: 0.827346, Accuracy: 89.37%\n",
      "Batch 47, Loss: 0.811465, Accuracy: 89.46%\n",
      "Batch 48, Loss: 0.946618, Accuracy: 89.23%\n",
      "Batch 49, Loss: 0.882592, Accuracy: 89.19%\n",
      "Batch 50, Loss: 0.820176, Accuracy: 89.28%\n",
      "Batch 51, Loss: 0.790962, Accuracy: 89.43%\n",
      "Batch 52, Loss: 0.816177, Accuracy: 89.48%\n",
      "Batch 53, Loss: 0.863953, Accuracy: 89.48%\n",
      "Batch 54, Loss: 0.808304, Accuracy: 89.58%\n",
      "Batch 55, Loss: 0.850954, Accuracy: 89.57%\n",
      "Batch 56, Loss: 0.842123, Accuracy: 89.56%\n",
      "Batch 57, Loss: 0.857011, Accuracy: 89.50%\n",
      "Batch 58, Loss: 0.810651, Accuracy: 89.57%\n",
      "Batch 59, Loss: 0.829725, Accuracy: 89.57%\n",
      "Batch 60, Loss: 0.877922, Accuracy: 89.53%\n",
      "Batch 61, Loss: 0.843026, Accuracy: 89.55%\n",
      "Batch 62, Loss: 0.819509, Accuracy: 89.59%\n",
      "Batch 63, Loss: 0.802230, Accuracy: 89.66%\n",
      "Batch 64, Loss: 0.845033, Accuracy: 89.67%\n",
      "Batch 65, Loss: 0.885564, Accuracy: 89.62%\n",
      "Batch 66, Loss: 0.887274, Accuracy: 89.56%\n",
      "Batch 67, Loss: 0.783300, Accuracy: 89.67%\n",
      "Batch 68, Loss: 0.822969, Accuracy: 89.71%\n",
      "Batch 69, Loss: 0.832982, Accuracy: 89.72%\n",
      "Batch 70, Loss: 0.870700, Accuracy: 89.69%\n",
      "Batch 71, Loss: 0.812529, Accuracy: 89.74%\n",
      "Batch 72, Loss: 0.880835, Accuracy: 89.69%\n",
      "Batch 73, Loss: 0.835533, Accuracy: 89.73%\n",
      "Batch 74, Loss: 0.852280, Accuracy: 89.72%\n",
      "Batch 75, Loss: 0.870275, Accuracy: 89.69%\n",
      "Batch 76, Loss: 0.908586, Accuracy: 89.60%\n",
      "Batch 77, Loss: 0.879345, Accuracy: 89.51%\n",
      "Batch 78, Loss: 0.828389, Accuracy: 89.54%\n",
      "Batch 79, Loss: 0.816935, Accuracy: 89.58%\n",
      "Batch 80, Loss: 0.847887, Accuracy: 89.59%\n",
      "Batch 81, Loss: 0.794555, Accuracy: 89.66%\n",
      "Batch 82, Loss: 0.833885, Accuracy: 89.65%\n",
      "Batch 83, Loss: 0.843469, Accuracy: 89.65%\n",
      "Batch 84, Loss: 0.890126, Accuracy: 89.58%\n",
      "Batch 85, Loss: 0.916199, Accuracy: 89.50%\n",
      "Batch 86, Loss: 0.920071, Accuracy: 89.43%\n",
      "Batch 87, Loss: 0.873142, Accuracy: 89.40%\n",
      "Batch 88, Loss: 0.856790, Accuracy: 89.38%\n",
      "Batch 89, Loss: 0.765017, Accuracy: 89.48%\n",
      "Batch 90, Loss: 0.905016, Accuracy: 89.41%\n",
      "Batch 91, Loss: 0.774140, Accuracy: 89.49%\n",
      "Batch 92, Loss: 0.861888, Accuracy: 89.47%\n",
      "Batch 93, Loss: 0.821812, Accuracy: 89.50%\n",
      "Batch 94, Loss: 0.838190, Accuracy: 89.49%\n",
      "Batch 95, Loss: 0.866268, Accuracy: 89.47%\n",
      "Batch 96, Loss: 0.878772, Accuracy: 89.42%\n",
      "Batch 97, Loss: 0.796808, Accuracy: 89.48%\n",
      "Batch 98, Loss: 0.822136, Accuracy: 89.49%\n",
      "Batch 99, Loss: 0.874510, Accuracy: 89.44%\n",
      "Batch 100, Loss: 0.808733, Accuracy: 89.48%\n",
      "Batch 101, Loss: 0.839205, Accuracy: 89.51%\n",
      "Batch 102, Loss: 0.794546, Accuracy: 89.55%\n",
      "Batch 103, Loss: 0.852964, Accuracy: 89.55%\n",
      "Batch 104, Loss: 0.792694, Accuracy: 89.60%\n",
      "Batch 105, Loss: 0.852012, Accuracy: 89.60%\n",
      "Batch 106, Loss: 0.897422, Accuracy: 89.55%\n",
      "Batch 107, Loss: 0.877067, Accuracy: 89.52%\n",
      "Batch 108, Loss: 0.854309, Accuracy: 89.51%\n",
      "Batch 109, Loss: 0.841453, Accuracy: 89.51%\n",
      "Batch 110, Loss: 0.862082, Accuracy: 89.47%\n",
      "Batch 111, Loss: 0.802048, Accuracy: 89.53%\n",
      "Batch 112, Loss: 0.825000, Accuracy: 89.55%\n",
      "Batch 113, Loss: 0.833013, Accuracy: 89.56%\n",
      "Batch 114, Loss: 0.839710, Accuracy: 89.58%\n",
      "Batch 115, Loss: 0.817995, Accuracy: 89.62%\n",
      "Batch 116, Loss: 0.921151, Accuracy: 89.55%\n",
      "Batch 117, Loss: 0.880550, Accuracy: 89.52%\n",
      "Batch 118, Loss: 0.797685, Accuracy: 89.57%\n",
      "Batch 119, Loss: 0.815441, Accuracy: 89.59%\n",
      "Batch 120, Loss: 0.877821, Accuracy: 89.54%\n",
      "Batch 121, Loss: 0.887089, Accuracy: 89.51%\n",
      "Batch 122, Loss: 0.816705, Accuracy: 89.55%\n",
      "Batch 123, Loss: 0.863685, Accuracy: 89.53%\n",
      "Batch 124, Loss: 0.802379, Accuracy: 89.57%\n",
      "Batch 125, Loss: 0.868145, Accuracy: 89.56%\n",
      "Batch 126, Loss: 0.878337, Accuracy: 89.53%\n",
      "Batch 127, Loss: 0.833098, Accuracy: 89.55%\n",
      "Batch 128, Loss: 0.839176, Accuracy: 89.56%\n",
      "Batch 129, Loss: 0.880602, Accuracy: 89.55%\n",
      "Batch 130, Loss: 0.783642, Accuracy: 89.60%\n",
      "Batch 131, Loss: 0.863506, Accuracy: 89.59%\n",
      "Batch 132, Loss: 0.805123, Accuracy: 89.63%\n",
      "Batch 133, Loss: 0.849746, Accuracy: 89.63%\n",
      "Batch 134, Loss: 0.818632, Accuracy: 89.66%\n",
      "Batch 135, Loss: 0.782411, Accuracy: 89.70%\n",
      "Batch 136, Loss: 0.879116, Accuracy: 89.67%\n",
      "Batch 137, Loss: 0.838916, Accuracy: 89.68%\n",
      "Batch 138, Loss: 0.845492, Accuracy: 89.67%\n",
      "Batch 139, Loss: 0.828213, Accuracy: 89.68%\n",
      "Batch 140, Loss: 0.817958, Accuracy: 89.70%\n",
      "Batch 141, Loss: 0.864413, Accuracy: 89.69%\n",
      "Batch 142, Loss: 0.863250, Accuracy: 89.68%\n",
      "Batch 143, Loss: 0.910057, Accuracy: 89.63%\n",
      "Batch 144, Loss: 0.788633, Accuracy: 89.69%\n",
      "Batch 145, Loss: 0.817704, Accuracy: 89.71%\n",
      "Batch 146, Loss: 0.848988, Accuracy: 89.72%\n",
      "Batch 147, Loss: 0.868586, Accuracy: 89.71%\n",
      "Batch 148, Loss: 0.791397, Accuracy: 89.75%\n",
      "Batch 149, Loss: 0.883006, Accuracy: 89.72%\n",
      "Batch 150, Loss: 0.874526, Accuracy: 89.71%\n",
      "Batch 151, Loss: 0.823707, Accuracy: 89.71%\n",
      "Batch 152, Loss: 0.820051, Accuracy: 89.73%\n",
      "Batch 153, Loss: 0.815542, Accuracy: 89.75%\n",
      "Batch 154, Loss: 0.873421, Accuracy: 89.73%\n",
      "Batch 155, Loss: 0.859787, Accuracy: 89.73%\n",
      "Batch 156, Loss: 0.861762, Accuracy: 89.71%\n",
      "Batch 157, Loss: 0.842456, Accuracy: 89.72%\n",
      "Batch 158, Loss: 0.809009, Accuracy: 89.74%\n",
      "Batch 159, Loss: 0.848023, Accuracy: 89.74%\n",
      "Batch 160, Loss: 0.867786, Accuracy: 89.72%\n",
      "Batch 161, Loss: 0.863697, Accuracy: 89.71%\n",
      "Batch 162, Loss: 0.847035, Accuracy: 89.70%\n",
      "Batch 163, Loss: 0.857893, Accuracy: 89.70%\n",
      "Batch 164, Loss: 0.897309, Accuracy: 89.67%\n",
      "Batch 165, Loss: 0.849717, Accuracy: 89.68%\n",
      "Batch 166, Loss: 0.841682, Accuracy: 89.67%\n",
      "Batch 167, Loss: 0.850158, Accuracy: 89.67%\n",
      "Batch 168, Loss: 0.907013, Accuracy: 89.63%\n",
      "Batch 169, Loss: 0.858207, Accuracy: 89.63%\n",
      "Batch 170, Loss: 0.841600, Accuracy: 89.63%\n",
      "Batch 171, Loss: 0.850727, Accuracy: 89.63%\n",
      "Batch 172, Loss: 0.806243, Accuracy: 89.65%\n",
      "Batch 173, Loss: 0.859561, Accuracy: 89.64%\n",
      "Batch 174, Loss: 0.884344, Accuracy: 89.62%\n",
      "Batch 175, Loss: 0.851718, Accuracy: 89.62%\n",
      "Batch 176, Loss: 0.857562, Accuracy: 89.61%\n",
      "Batch 177, Loss: 0.829111, Accuracy: 89.61%\n",
      "Batch 178, Loss: 0.802478, Accuracy: 89.63%\n",
      "Batch 179, Loss: 0.871712, Accuracy: 89.62%\n",
      "Batch 180, Loss: 0.830669, Accuracy: 89.64%\n",
      "Batch 181, Loss: 0.887533, Accuracy: 89.62%\n",
      "Batch 182, Loss: 0.886517, Accuracy: 89.61%\n",
      "Batch 183, Loss: 0.879266, Accuracy: 89.60%\n",
      "Batch 184, Loss: 0.820667, Accuracy: 89.61%\n",
      "Batch 185, Loss: 0.882302, Accuracy: 89.59%\n",
      "Batch 186, Loss: 0.853792, Accuracy: 89.58%\n",
      "Batch 187, Loss: 0.839930, Accuracy: 89.59%\n",
      "Batch 188, Loss: 0.800428, Accuracy: 89.62%\n",
      "Batch 189, Loss: 0.893837, Accuracy: 89.58%\n",
      "Batch 190, Loss: 0.847768, Accuracy: 89.59%\n",
      "Batch 191, Loss: 0.863674, Accuracy: 89.59%\n",
      "Batch 192, Loss: 0.835465, Accuracy: 89.59%\n",
      "Batch 193, Loss: 0.857783, Accuracy: 89.59%\n",
      "Batch 194, Loss: 0.810841, Accuracy: 89.61%\n",
      "Batch 195, Loss: 0.832403, Accuracy: 89.62%\n",
      "Batch 196, Loss: 0.850460, Accuracy: 89.62%\n",
      "Batch 197, Loss: 0.913230, Accuracy: 89.59%\n",
      "Batch 198, Loss: 0.826532, Accuracy: 89.60%\n",
      "Batch 199, Loss: 0.802484, Accuracy: 89.62%\n",
      "Batch 200, Loss: 0.773026, Accuracy: 89.66%\n",
      "Batch 201, Loss: 0.912787, Accuracy: 89.62%\n",
      "Batch 202, Loss: 0.904369, Accuracy: 89.60%\n",
      "Batch 203, Loss: 0.833953, Accuracy: 89.60%\n",
      "Batch 204, Loss: 0.826352, Accuracy: 89.61%\n",
      "Batch 205, Loss: 0.823743, Accuracy: 89.63%\n",
      "Batch 206, Loss: 0.868997, Accuracy: 89.61%\n",
      "Batch 207, Loss: 0.836953, Accuracy: 89.61%\n",
      "Batch 208, Loss: 0.835300, Accuracy: 89.62%\n",
      "Batch 209, Loss: 0.840144, Accuracy: 89.62%\n",
      "Batch 210, Loss: 0.887513, Accuracy: 89.60%\n",
      "Batch 211, Loss: 0.867504, Accuracy: 89.60%\n",
      "Batch 212, Loss: 0.794438, Accuracy: 89.62%\n",
      "Batch 213, Loss: 0.889569, Accuracy: 89.60%\n",
      "Training - Epoch 14, Loss: 0.847397, Accuracy: 89.60%\n",
      "Validation Batch 1, Loss: 0.820319, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.792718, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.816908, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.835428, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.819372, Accuracy: 93.12%\n",
      "Validation Batch 6, Loss: 0.793403, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.794948, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.856528, Accuracy: 93.16%\n",
      "Validation Batch 9, Loss: 0.889742, Accuracy: 92.19%\n",
      "Validation Batch 10, Loss: 0.788080, Accuracy: 92.66%\n",
      "Validation Batch 11, Loss: 0.827830, Accuracy: 92.61%\n",
      "Validation Batch 12, Loss: 0.827300, Accuracy: 92.58%\n",
      "Validation Batch 13, Loss: 0.833593, Accuracy: 92.43%\n",
      "Validation Batch 14, Loss: 0.834353, Accuracy: 92.41%\n",
      "Validation Batch 15, Loss: 0.798275, Accuracy: 92.50%\n",
      "Validation Batch 16, Loss: 0.839518, Accuracy: 92.38%\n",
      "Validation Batch 17, Loss: 0.857061, Accuracy: 92.19%\n",
      "Validation Batch 18, Loss: 0.798368, Accuracy: 92.36%\n",
      "Validation Batch 19, Loss: 0.846867, Accuracy: 92.27%\n",
      "Validation Batch 20, Loss: 0.818205, Accuracy: 92.34%\n",
      "Validation Batch 21, Loss: 0.859341, Accuracy: 92.19%\n",
      "Validation Batch 22, Loss: 0.795525, Accuracy: 92.40%\n",
      "Validation Batch 23, Loss: 0.859499, Accuracy: 92.26%\n",
      "Validation Batch 24, Loss: 0.828769, Accuracy: 92.19%\n",
      "Validation Batch 25, Loss: 0.797280, Accuracy: 92.31%\n",
      "Validation Batch 26, Loss: 0.836605, Accuracy: 92.25%\n",
      "Validation Batch 27, Loss: 0.793696, Accuracy: 92.31%\n",
      "Validation - Epoch 14, Loss: 0.824427, Accuracy: 92.31%\n",
      "Patience—14\n",
      "Epoch 15\n",
      "Batch 1, Loss: 0.853515, Accuracy: 89.06%\n",
      "Batch 2, Loss: 0.801072, Accuracy: 92.19%\n",
      "Batch 3, Loss: 0.814875, Accuracy: 92.19%\n",
      "Batch 4, Loss: 0.881571, Accuracy: 90.62%\n",
      "Batch 5, Loss: 0.840831, Accuracy: 90.62%\n",
      "Batch 6, Loss: 0.821811, Accuracy: 90.89%\n",
      "Batch 7, Loss: 0.786274, Accuracy: 91.74%\n",
      "Batch 8, Loss: 0.851947, Accuracy: 91.41%\n",
      "Batch 9, Loss: 0.871723, Accuracy: 90.97%\n",
      "Batch 10, Loss: 0.861581, Accuracy: 90.78%\n",
      "Batch 11, Loss: 0.798838, Accuracy: 91.19%\n",
      "Batch 12, Loss: 0.835580, Accuracy: 91.15%\n",
      "Batch 13, Loss: 0.906721, Accuracy: 90.62%\n",
      "Batch 14, Loss: 0.826458, Accuracy: 90.62%\n",
      "Batch 15, Loss: 0.923972, Accuracy: 90.10%\n",
      "Batch 16, Loss: 0.826752, Accuracy: 90.23%\n",
      "Batch 17, Loss: 0.805470, Accuracy: 90.44%\n",
      "Batch 18, Loss: 0.865721, Accuracy: 90.19%\n",
      "Batch 19, Loss: 0.848468, Accuracy: 90.13%\n",
      "Batch 20, Loss: 0.853297, Accuracy: 90.16%\n",
      "Batch 21, Loss: 0.899161, Accuracy: 89.88%\n",
      "Batch 22, Loss: 0.808576, Accuracy: 89.99%\n",
      "Batch 23, Loss: 0.820049, Accuracy: 90.15%\n",
      "Batch 24, Loss: 0.851443, Accuracy: 90.04%\n",
      "Batch 25, Loss: 0.791832, Accuracy: 90.25%\n",
      "Batch 26, Loss: 0.815574, Accuracy: 90.38%\n",
      "Batch 27, Loss: 0.876959, Accuracy: 90.28%\n",
      "Batch 28, Loss: 0.789901, Accuracy: 90.46%\n",
      "Batch 29, Loss: 0.838497, Accuracy: 90.46%\n",
      "Batch 30, Loss: 0.849403, Accuracy: 90.42%\n",
      "Batch 31, Loss: 0.863861, Accuracy: 90.27%\n",
      "Batch 32, Loss: 0.848176, Accuracy: 90.19%\n",
      "Batch 33, Loss: 0.818766, Accuracy: 90.25%\n",
      "Batch 34, Loss: 0.864925, Accuracy: 90.21%\n",
      "Batch 35, Loss: 0.873205, Accuracy: 90.13%\n",
      "Batch 36, Loss: 0.800341, Accuracy: 90.28%\n",
      "Batch 37, Loss: 0.845721, Accuracy: 90.29%\n",
      "Batch 38, Loss: 0.821678, Accuracy: 90.30%\n",
      "Batch 39, Loss: 0.836664, Accuracy: 90.30%\n",
      "Batch 40, Loss: 0.847600, Accuracy: 90.27%\n",
      "Batch 41, Loss: 0.844451, Accuracy: 90.28%\n",
      "Batch 42, Loss: 0.884674, Accuracy: 90.18%\n",
      "Batch 43, Loss: 0.817283, Accuracy: 90.26%\n",
      "Batch 44, Loss: 0.873661, Accuracy: 90.16%\n",
      "Batch 45, Loss: 0.862407, Accuracy: 90.14%\n",
      "Batch 46, Loss: 0.883929, Accuracy: 90.01%\n",
      "Batch 47, Loss: 0.814861, Accuracy: 90.09%\n",
      "Batch 48, Loss: 0.815348, Accuracy: 90.20%\n",
      "Batch 49, Loss: 0.888592, Accuracy: 90.08%\n",
      "Batch 50, Loss: 0.825200, Accuracy: 90.09%\n",
      "Batch 51, Loss: 0.819128, Accuracy: 90.13%\n",
      "Batch 52, Loss: 0.865684, Accuracy: 90.08%\n",
      "Batch 53, Loss: 0.865997, Accuracy: 90.04%\n",
      "Batch 54, Loss: 0.854227, Accuracy: 90.05%\n",
      "Batch 55, Loss: 0.850055, Accuracy: 90.03%\n",
      "Batch 56, Loss: 0.828860, Accuracy: 90.04%\n",
      "Batch 57, Loss: 0.866632, Accuracy: 89.99%\n",
      "Batch 58, Loss: 0.831365, Accuracy: 90.03%\n",
      "Batch 59, Loss: 0.898762, Accuracy: 89.94%\n",
      "Batch 60, Loss: 0.850380, Accuracy: 89.92%\n",
      "Batch 61, Loss: 0.854559, Accuracy: 89.91%\n",
      "Batch 62, Loss: 0.866745, Accuracy: 89.87%\n",
      "Batch 63, Loss: 0.843513, Accuracy: 89.86%\n",
      "Batch 64, Loss: 0.831702, Accuracy: 89.87%\n",
      "Batch 65, Loss: 0.821327, Accuracy: 89.93%\n",
      "Batch 66, Loss: 0.860347, Accuracy: 89.91%\n",
      "Batch 67, Loss: 0.838823, Accuracy: 89.93%\n",
      "Batch 68, Loss: 0.897058, Accuracy: 89.84%\n",
      "Batch 69, Loss: 0.862758, Accuracy: 89.79%\n",
      "Batch 70, Loss: 0.878670, Accuracy: 89.75%\n",
      "Batch 71, Loss: 0.912337, Accuracy: 89.68%\n",
      "Batch 72, Loss: 0.879100, Accuracy: 89.63%\n",
      "Batch 73, Loss: 0.885678, Accuracy: 89.58%\n",
      "Batch 74, Loss: 0.818403, Accuracy: 89.61%\n",
      "Batch 75, Loss: 0.913837, Accuracy: 89.50%\n",
      "Batch 76, Loss: 0.780568, Accuracy: 89.60%\n",
      "Batch 77, Loss: 0.797635, Accuracy: 89.67%\n",
      "Batch 78, Loss: 0.858224, Accuracy: 89.66%\n",
      "Batch 79, Loss: 0.820294, Accuracy: 89.70%\n",
      "Batch 80, Loss: 0.851080, Accuracy: 89.71%\n",
      "Batch 81, Loss: 0.800655, Accuracy: 89.76%\n",
      "Batch 82, Loss: 0.811505, Accuracy: 89.79%\n",
      "Batch 83, Loss: 0.787970, Accuracy: 89.87%\n",
      "Batch 84, Loss: 0.834261, Accuracy: 89.90%\n",
      "Batch 85, Loss: 0.827846, Accuracy: 89.93%\n",
      "Batch 86, Loss: 0.804313, Accuracy: 89.99%\n",
      "Batch 87, Loss: 0.843164, Accuracy: 89.98%\n",
      "Batch 88, Loss: 0.955069, Accuracy: 89.88%\n",
      "Batch 89, Loss: 0.847584, Accuracy: 89.87%\n",
      "Batch 90, Loss: 0.845786, Accuracy: 89.86%\n",
      "Batch 91, Loss: 0.852000, Accuracy: 89.85%\n",
      "Batch 92, Loss: 0.835521, Accuracy: 89.84%\n",
      "Batch 93, Loss: 0.873126, Accuracy: 89.82%\n",
      "Batch 94, Loss: 0.812394, Accuracy: 89.84%\n",
      "Batch 95, Loss: 0.768666, Accuracy: 89.92%\n",
      "Batch 96, Loss: 0.832436, Accuracy: 89.93%\n",
      "Batch 97, Loss: 0.846487, Accuracy: 89.93%\n",
      "Batch 98, Loss: 0.921275, Accuracy: 89.86%\n",
      "Batch 99, Loss: 0.847174, Accuracy: 89.87%\n",
      "Batch 100, Loss: 0.827126, Accuracy: 89.89%\n",
      "Batch 101, Loss: 0.799441, Accuracy: 89.96%\n",
      "Batch 102, Loss: 0.843943, Accuracy: 89.95%\n",
      "Batch 103, Loss: 0.872045, Accuracy: 89.93%\n",
      "Batch 104, Loss: 0.848377, Accuracy: 89.92%\n",
      "Batch 105, Loss: 0.852594, Accuracy: 89.91%\n",
      "Batch 106, Loss: 0.904949, Accuracy: 89.86%\n",
      "Batch 107, Loss: 0.845791, Accuracy: 89.87%\n",
      "Batch 108, Loss: 0.818557, Accuracy: 89.89%\n",
      "Batch 109, Loss: 0.854749, Accuracy: 89.88%\n",
      "Batch 110, Loss: 0.818428, Accuracy: 89.91%\n",
      "Batch 111, Loss: 0.834437, Accuracy: 89.94%\n",
      "Batch 112, Loss: 0.871326, Accuracy: 89.90%\n",
      "Batch 113, Loss: 0.793568, Accuracy: 89.96%\n",
      "Batch 114, Loss: 0.875304, Accuracy: 89.94%\n",
      "Batch 115, Loss: 0.837988, Accuracy: 89.93%\n",
      "Batch 116, Loss: 0.801772, Accuracy: 89.98%\n",
      "Batch 117, Loss: 0.811540, Accuracy: 90.01%\n",
      "Batch 118, Loss: 0.808146, Accuracy: 90.04%\n",
      "Batch 119, Loss: 0.810764, Accuracy: 90.07%\n",
      "Batch 120, Loss: 0.804474, Accuracy: 90.10%\n",
      "Batch 121, Loss: 0.848925, Accuracy: 90.10%\n",
      "Batch 122, Loss: 0.798215, Accuracy: 90.14%\n",
      "Batch 123, Loss: 0.883322, Accuracy: 90.10%\n",
      "Batch 124, Loss: 0.818724, Accuracy: 90.12%\n",
      "Batch 125, Loss: 0.793586, Accuracy: 90.16%\n",
      "Batch 126, Loss: 0.895087, Accuracy: 90.12%\n",
      "Batch 127, Loss: 0.828032, Accuracy: 90.13%\n",
      "Batch 128, Loss: 0.816179, Accuracy: 90.15%\n",
      "Batch 129, Loss: 0.769099, Accuracy: 90.21%\n",
      "Batch 130, Loss: 0.852208, Accuracy: 90.20%\n",
      "Batch 131, Loss: 0.827479, Accuracy: 90.22%\n",
      "Batch 132, Loss: 0.788451, Accuracy: 90.26%\n",
      "Batch 133, Loss: 0.839828, Accuracy: 90.26%\n",
      "Batch 134, Loss: 0.866940, Accuracy: 90.24%\n",
      "Batch 135, Loss: 0.833623, Accuracy: 90.24%\n",
      "Batch 136, Loss: 0.829544, Accuracy: 90.25%\n",
      "Batch 137, Loss: 0.882818, Accuracy: 90.21%\n",
      "Batch 138, Loss: 0.840593, Accuracy: 90.21%\n",
      "Batch 139, Loss: 0.823707, Accuracy: 90.22%\n",
      "Batch 140, Loss: 0.883581, Accuracy: 90.19%\n",
      "Batch 141, Loss: 0.832351, Accuracy: 90.19%\n",
      "Batch 142, Loss: 0.868589, Accuracy: 90.17%\n",
      "Batch 143, Loss: 0.838711, Accuracy: 90.19%\n",
      "Batch 144, Loss: 0.866148, Accuracy: 90.16%\n",
      "Batch 145, Loss: 0.846569, Accuracy: 90.16%\n",
      "Batch 146, Loss: 0.913428, Accuracy: 90.12%\n",
      "Batch 147, Loss: 0.840546, Accuracy: 90.13%\n",
      "Batch 148, Loss: 0.822433, Accuracy: 90.14%\n",
      "Batch 149, Loss: 0.829342, Accuracy: 90.14%\n",
      "Batch 150, Loss: 0.861995, Accuracy: 90.12%\n",
      "Batch 151, Loss: 0.831750, Accuracy: 90.14%\n",
      "Batch 152, Loss: 0.837728, Accuracy: 90.14%\n",
      "Batch 153, Loss: 0.798599, Accuracy: 90.18%\n",
      "Batch 154, Loss: 0.851070, Accuracy: 90.17%\n",
      "Batch 155, Loss: 0.857386, Accuracy: 90.16%\n",
      "Batch 156, Loss: 0.830707, Accuracy: 90.16%\n",
      "Batch 157, Loss: 0.880966, Accuracy: 90.15%\n",
      "Batch 158, Loss: 0.855861, Accuracy: 90.14%\n",
      "Batch 159, Loss: 0.886622, Accuracy: 90.11%\n",
      "Batch 160, Loss: 0.858805, Accuracy: 90.11%\n",
      "Batch 161, Loss: 0.795629, Accuracy: 90.14%\n",
      "Batch 162, Loss: 0.901038, Accuracy: 90.09%\n",
      "Batch 163, Loss: 0.885895, Accuracy: 90.07%\n",
      "Batch 164, Loss: 0.868381, Accuracy: 90.05%\n",
      "Batch 165, Loss: 0.868461, Accuracy: 90.03%\n",
      "Batch 166, Loss: 0.808937, Accuracy: 90.05%\n",
      "Batch 167, Loss: 0.936843, Accuracy: 90.00%\n",
      "Batch 168, Loss: 0.852198, Accuracy: 90.00%\n",
      "Batch 169, Loss: 0.850716, Accuracy: 90.01%\n",
      "Batch 170, Loss: 0.853308, Accuracy: 90.01%\n",
      "Batch 171, Loss: 0.842214, Accuracy: 90.01%\n",
      "Batch 172, Loss: 0.783856, Accuracy: 90.04%\n",
      "Batch 173, Loss: 0.804185, Accuracy: 90.07%\n",
      "Batch 174, Loss: 0.898155, Accuracy: 90.03%\n",
      "Batch 175, Loss: 0.893339, Accuracy: 90.00%\n",
      "Batch 176, Loss: 0.852035, Accuracy: 90.00%\n",
      "Batch 177, Loss: 0.826719, Accuracy: 90.02%\n",
      "Batch 178, Loss: 0.832111, Accuracy: 90.02%\n",
      "Batch 179, Loss: 0.849773, Accuracy: 90.02%\n",
      "Batch 180, Loss: 0.832972, Accuracy: 90.02%\n",
      "Batch 181, Loss: 0.764462, Accuracy: 90.06%\n",
      "Batch 182, Loss: 0.793457, Accuracy: 90.09%\n",
      "Batch 183, Loss: 0.837552, Accuracy: 90.10%\n",
      "Batch 184, Loss: 0.890018, Accuracy: 90.06%\n",
      "Batch 185, Loss: 0.856079, Accuracy: 90.06%\n",
      "Batch 186, Loss: 0.906977, Accuracy: 90.02%\n",
      "Batch 187, Loss: 0.862622, Accuracy: 90.01%\n",
      "Batch 188, Loss: 0.803661, Accuracy: 90.03%\n",
      "Batch 189, Loss: 0.907390, Accuracy: 89.99%\n",
      "Batch 190, Loss: 0.875448, Accuracy: 89.98%\n",
      "Batch 191, Loss: 0.818073, Accuracy: 90.00%\n",
      "Batch 192, Loss: 0.802582, Accuracy: 90.01%\n",
      "Batch 193, Loss: 0.777318, Accuracy: 90.04%\n",
      "Batch 194, Loss: 0.853244, Accuracy: 90.04%\n",
      "Batch 195, Loss: 0.807809, Accuracy: 90.06%\n",
      "Batch 196, Loss: 0.801250, Accuracy: 90.07%\n",
      "Batch 197, Loss: 0.827219, Accuracy: 90.09%\n",
      "Batch 198, Loss: 0.867158, Accuracy: 90.07%\n",
      "Batch 199, Loss: 0.825428, Accuracy: 90.08%\n",
      "Batch 200, Loss: 0.818629, Accuracy: 90.10%\n",
      "Batch 201, Loss: 0.863059, Accuracy: 90.10%\n",
      "Batch 202, Loss: 0.871166, Accuracy: 90.09%\n",
      "Batch 203, Loss: 0.797885, Accuracy: 90.12%\n",
      "Batch 204, Loss: 0.828092, Accuracy: 90.13%\n",
      "Batch 205, Loss: 0.894796, Accuracy: 90.11%\n",
      "Batch 206, Loss: 0.795632, Accuracy: 90.12%\n",
      "Batch 207, Loss: 0.845340, Accuracy: 90.12%\n",
      "Batch 208, Loss: 0.819619, Accuracy: 90.14%\n",
      "Batch 209, Loss: 0.827040, Accuracy: 90.15%\n",
      "Batch 210, Loss: 0.861676, Accuracy: 90.13%\n",
      "Batch 211, Loss: 0.794003, Accuracy: 90.16%\n",
      "Batch 212, Loss: 0.877220, Accuracy: 90.15%\n",
      "Batch 213, Loss: 0.888111, Accuracy: 90.12%\n",
      "Training - Epoch 15, Loss: 0.843483, Accuracy: 90.12%\n",
      "Validation Batch 1, Loss: 0.819604, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.792121, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.816170, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.835107, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.818401, Accuracy: 93.12%\n",
      "Validation Batch 6, Loss: 0.793197, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.794692, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.855979, Accuracy: 93.16%\n",
      "Validation Batch 9, Loss: 0.888656, Accuracy: 92.19%\n",
      "Validation Batch 10, Loss: 0.787515, Accuracy: 92.66%\n",
      "Validation Batch 11, Loss: 0.826936, Accuracy: 92.61%\n",
      "Validation Batch 12, Loss: 0.826984, Accuracy: 92.58%\n",
      "Validation Batch 13, Loss: 0.832952, Accuracy: 92.43%\n",
      "Validation Batch 14, Loss: 0.833640, Accuracy: 92.41%\n",
      "Validation Batch 15, Loss: 0.797854, Accuracy: 92.50%\n",
      "Validation Batch 16, Loss: 0.838251, Accuracy: 92.38%\n",
      "Validation Batch 17, Loss: 0.856738, Accuracy: 92.19%\n",
      "Validation Batch 18, Loss: 0.797611, Accuracy: 92.36%\n",
      "Validation Batch 19, Loss: 0.845806, Accuracy: 92.27%\n",
      "Validation Batch 20, Loss: 0.816298, Accuracy: 92.42%\n",
      "Validation Batch 21, Loss: 0.858975, Accuracy: 92.26%\n",
      "Validation Batch 22, Loss: 0.795029, Accuracy: 92.47%\n",
      "Validation Batch 23, Loss: 0.859206, Accuracy: 92.32%\n",
      "Validation Batch 24, Loss: 0.828056, Accuracy: 92.32%\n",
      "Validation Batch 25, Loss: 0.795873, Accuracy: 92.44%\n",
      "Validation Batch 26, Loss: 0.836088, Accuracy: 92.37%\n",
      "Validation Batch 27, Loss: 0.792854, Accuracy: 92.43%\n",
      "Validation - Epoch 15, Loss: 0.823726, Accuracy: 92.43%\n",
      "Patience—15\n",
      "Epoch 16\n",
      "Batch 1, Loss: 0.861553, Accuracy: 87.50%\n",
      "Batch 2, Loss: 0.863108, Accuracy: 87.50%\n",
      "Batch 3, Loss: 0.781555, Accuracy: 90.62%\n",
      "Batch 4, Loss: 0.859119, Accuracy: 90.23%\n",
      "Batch 5, Loss: 0.827656, Accuracy: 90.62%\n",
      "Batch 6, Loss: 0.874505, Accuracy: 89.84%\n",
      "Batch 7, Loss: 0.902198, Accuracy: 89.06%\n",
      "Batch 8, Loss: 0.845586, Accuracy: 89.06%\n",
      "Batch 9, Loss: 0.832660, Accuracy: 89.24%\n",
      "Batch 10, Loss: 0.842391, Accuracy: 89.22%\n",
      "Batch 11, Loss: 0.795803, Accuracy: 89.77%\n",
      "Batch 12, Loss: 0.813196, Accuracy: 90.10%\n",
      "Batch 13, Loss: 0.797388, Accuracy: 90.50%\n",
      "Batch 14, Loss: 0.791648, Accuracy: 90.85%\n",
      "Batch 15, Loss: 0.819658, Accuracy: 91.04%\n",
      "Batch 16, Loss: 0.860993, Accuracy: 90.72%\n",
      "Batch 17, Loss: 0.843713, Accuracy: 90.62%\n",
      "Batch 18, Loss: 0.863986, Accuracy: 90.54%\n",
      "Batch 19, Loss: 0.874040, Accuracy: 90.46%\n",
      "Batch 20, Loss: 0.888816, Accuracy: 90.16%\n",
      "Batch 21, Loss: 0.839645, Accuracy: 90.18%\n",
      "Batch 22, Loss: 0.862430, Accuracy: 90.13%\n",
      "Batch 23, Loss: 0.818096, Accuracy: 90.29%\n",
      "Batch 24, Loss: 0.814339, Accuracy: 90.43%\n",
      "Batch 25, Loss: 0.898777, Accuracy: 90.12%\n",
      "Batch 26, Loss: 0.856645, Accuracy: 90.02%\n",
      "Batch 27, Loss: 0.803329, Accuracy: 90.22%\n",
      "Batch 28, Loss: 0.841279, Accuracy: 90.23%\n",
      "Batch 29, Loss: 0.787864, Accuracy: 90.46%\n",
      "Batch 30, Loss: 0.791255, Accuracy: 90.62%\n",
      "Batch 31, Loss: 0.857095, Accuracy: 90.52%\n",
      "Batch 32, Loss: 0.847735, Accuracy: 90.48%\n",
      "Batch 33, Loss: 0.820494, Accuracy: 90.53%\n",
      "Batch 34, Loss: 0.842038, Accuracy: 90.53%\n",
      "Batch 35, Loss: 0.765607, Accuracy: 90.76%\n",
      "Batch 36, Loss: 0.861819, Accuracy: 90.71%\n",
      "Batch 37, Loss: 0.792397, Accuracy: 90.79%\n",
      "Batch 38, Loss: 0.893718, Accuracy: 90.62%\n",
      "Batch 39, Loss: 0.815402, Accuracy: 90.67%\n",
      "Batch 40, Loss: 0.891876, Accuracy: 90.51%\n",
      "Batch 41, Loss: 0.909077, Accuracy: 90.32%\n",
      "Batch 42, Loss: 0.887305, Accuracy: 90.22%\n",
      "Batch 43, Loss: 0.808141, Accuracy: 90.30%\n",
      "Batch 44, Loss: 0.885791, Accuracy: 90.20%\n",
      "Batch 45, Loss: 0.832418, Accuracy: 90.21%\n",
      "Batch 46, Loss: 0.837506, Accuracy: 90.22%\n",
      "Batch 47, Loss: 0.843785, Accuracy: 90.23%\n",
      "Batch 48, Loss: 0.857681, Accuracy: 90.20%\n",
      "Batch 49, Loss: 0.853760, Accuracy: 90.21%\n",
      "Batch 50, Loss: 0.818847, Accuracy: 90.25%\n",
      "Batch 51, Loss: 0.846987, Accuracy: 90.26%\n",
      "Batch 52, Loss: 0.851032, Accuracy: 90.20%\n",
      "Batch 53, Loss: 0.842205, Accuracy: 90.21%\n",
      "Batch 54, Loss: 0.769665, Accuracy: 90.34%\n",
      "Batch 55, Loss: 0.805492, Accuracy: 90.40%\n",
      "Batch 56, Loss: 0.938917, Accuracy: 90.21%\n",
      "Batch 57, Loss: 0.835280, Accuracy: 90.21%\n",
      "Batch 58, Loss: 0.818762, Accuracy: 90.25%\n",
      "Batch 59, Loss: 0.844986, Accuracy: 90.23%\n",
      "Batch 60, Loss: 0.807526, Accuracy: 90.29%\n",
      "Batch 61, Loss: 0.798154, Accuracy: 90.37%\n",
      "Batch 62, Loss: 0.819952, Accuracy: 90.40%\n",
      "Batch 63, Loss: 0.846662, Accuracy: 90.40%\n",
      "Batch 64, Loss: 0.837354, Accuracy: 90.41%\n",
      "Batch 65, Loss: 0.845161, Accuracy: 90.41%\n",
      "Batch 66, Loss: 0.894526, Accuracy: 90.34%\n",
      "Batch 67, Loss: 0.900261, Accuracy: 90.25%\n",
      "Batch 68, Loss: 0.812470, Accuracy: 90.30%\n",
      "Batch 69, Loss: 0.890641, Accuracy: 90.24%\n",
      "Batch 70, Loss: 0.813003, Accuracy: 90.29%\n",
      "Batch 71, Loss: 0.912763, Accuracy: 90.18%\n",
      "Batch 72, Loss: 0.804090, Accuracy: 90.26%\n",
      "Batch 73, Loss: 0.835579, Accuracy: 90.26%\n",
      "Batch 74, Loss: 0.834166, Accuracy: 90.27%\n",
      "Batch 75, Loss: 0.822805, Accuracy: 90.29%\n",
      "Batch 76, Loss: 0.853991, Accuracy: 90.25%\n",
      "Batch 77, Loss: 0.843742, Accuracy: 90.24%\n",
      "Batch 78, Loss: 0.895600, Accuracy: 90.14%\n",
      "Batch 79, Loss: 0.866765, Accuracy: 90.11%\n",
      "Batch 80, Loss: 0.808576, Accuracy: 90.16%\n",
      "Batch 81, Loss: 0.787253, Accuracy: 90.24%\n",
      "Batch 82, Loss: 0.872079, Accuracy: 90.21%\n",
      "Batch 83, Loss: 0.819927, Accuracy: 90.25%\n",
      "Batch 84, Loss: 0.864439, Accuracy: 90.22%\n",
      "Batch 85, Loss: 0.807241, Accuracy: 90.26%\n",
      "Batch 86, Loss: 0.805570, Accuracy: 90.30%\n",
      "Batch 87, Loss: 0.910740, Accuracy: 90.23%\n",
      "Batch 88, Loss: 0.819559, Accuracy: 90.25%\n",
      "Batch 89, Loss: 0.893471, Accuracy: 90.19%\n",
      "Batch 90, Loss: 0.806214, Accuracy: 90.24%\n",
      "Batch 91, Loss: 0.837097, Accuracy: 90.26%\n",
      "Batch 92, Loss: 0.818367, Accuracy: 90.29%\n",
      "Batch 93, Loss: 0.850569, Accuracy: 90.26%\n",
      "Batch 94, Loss: 0.848745, Accuracy: 90.24%\n",
      "Batch 95, Loss: 0.974952, Accuracy: 90.10%\n",
      "Batch 96, Loss: 0.815929, Accuracy: 90.12%\n",
      "Batch 97, Loss: 0.841164, Accuracy: 90.14%\n",
      "Batch 98, Loss: 0.809091, Accuracy: 90.18%\n",
      "Batch 99, Loss: 0.822155, Accuracy: 90.20%\n",
      "Batch 100, Loss: 0.903210, Accuracy: 90.14%\n",
      "Batch 101, Loss: 0.886485, Accuracy: 90.08%\n",
      "Batch 102, Loss: 0.826466, Accuracy: 90.10%\n",
      "Batch 103, Loss: 0.796492, Accuracy: 90.15%\n",
      "Batch 104, Loss: 0.773074, Accuracy: 90.22%\n",
      "Batch 105, Loss: 0.804459, Accuracy: 90.25%\n",
      "Batch 106, Loss: 0.805642, Accuracy: 90.30%\n",
      "Batch 107, Loss: 0.858196, Accuracy: 90.29%\n",
      "Batch 108, Loss: 0.836024, Accuracy: 90.29%\n",
      "Batch 109, Loss: 0.837687, Accuracy: 90.28%\n",
      "Batch 110, Loss: 0.818646, Accuracy: 90.30%\n",
      "Batch 111, Loss: 0.912000, Accuracy: 90.23%\n",
      "Batch 112, Loss: 0.844817, Accuracy: 90.23%\n",
      "Batch 113, Loss: 0.831933, Accuracy: 90.24%\n",
      "Batch 114, Loss: 0.835179, Accuracy: 90.24%\n",
      "Batch 115, Loss: 0.869160, Accuracy: 90.22%\n",
      "Batch 116, Loss: 0.882076, Accuracy: 90.18%\n",
      "Batch 117, Loss: 0.844299, Accuracy: 90.17%\n",
      "Batch 118, Loss: 0.833687, Accuracy: 90.17%\n",
      "Batch 119, Loss: 0.808747, Accuracy: 90.20%\n",
      "Batch 120, Loss: 0.863709, Accuracy: 90.18%\n",
      "Batch 121, Loss: 0.798279, Accuracy: 90.22%\n",
      "Batch 122, Loss: 0.866548, Accuracy: 90.20%\n",
      "Batch 123, Loss: 0.882865, Accuracy: 90.15%\n",
      "Batch 124, Loss: 0.771209, Accuracy: 90.22%\n",
      "Batch 125, Loss: 0.892786, Accuracy: 90.17%\n",
      "Batch 126, Loss: 0.857826, Accuracy: 90.17%\n",
      "Batch 127, Loss: 0.829597, Accuracy: 90.18%\n",
      "Batch 128, Loss: 0.827650, Accuracy: 90.20%\n",
      "Batch 129, Loss: 0.826532, Accuracy: 90.21%\n",
      "Batch 130, Loss: 0.784762, Accuracy: 90.26%\n",
      "Batch 131, Loss: 0.868821, Accuracy: 90.23%\n",
      "Batch 132, Loss: 0.777493, Accuracy: 90.28%\n",
      "Batch 133, Loss: 0.938815, Accuracy: 90.21%\n",
      "Batch 134, Loss: 0.802342, Accuracy: 90.24%\n",
      "Batch 135, Loss: 0.920269, Accuracy: 90.19%\n",
      "Batch 136, Loss: 0.893144, Accuracy: 90.15%\n",
      "Batch 137, Loss: 0.866826, Accuracy: 90.12%\n",
      "Batch 138, Loss: 0.842857, Accuracy: 90.12%\n",
      "Batch 139, Loss: 0.916067, Accuracy: 90.06%\n",
      "Batch 140, Loss: 0.846073, Accuracy: 90.06%\n",
      "Batch 141, Loss: 0.802093, Accuracy: 90.08%\n",
      "Batch 142, Loss: 0.855324, Accuracy: 90.07%\n",
      "Batch 143, Loss: 0.904367, Accuracy: 90.05%\n",
      "Batch 144, Loss: 0.817647, Accuracy: 90.07%\n",
      "Batch 145, Loss: 0.938000, Accuracy: 90.01%\n",
      "Batch 146, Loss: 0.940376, Accuracy: 89.94%\n",
      "Batch 147, Loss: 0.820563, Accuracy: 89.96%\n",
      "Batch 148, Loss: 0.826873, Accuracy: 89.96%\n",
      "Batch 149, Loss: 0.873737, Accuracy: 89.93%\n",
      "Batch 150, Loss: 0.861019, Accuracy: 89.92%\n",
      "Batch 151, Loss: 0.846807, Accuracy: 89.91%\n",
      "Batch 152, Loss: 0.793750, Accuracy: 89.94%\n",
      "Batch 153, Loss: 0.805763, Accuracy: 89.96%\n",
      "Batch 154, Loss: 0.905723, Accuracy: 89.92%\n",
      "Batch 155, Loss: 0.903683, Accuracy: 89.88%\n",
      "Batch 156, Loss: 0.850776, Accuracy: 89.87%\n",
      "Batch 157, Loss: 0.868280, Accuracy: 89.86%\n",
      "Batch 158, Loss: 0.835630, Accuracy: 89.86%\n",
      "Batch 159, Loss: 0.828731, Accuracy: 89.88%\n",
      "Batch 160, Loss: 0.866301, Accuracy: 89.87%\n",
      "Batch 161, Loss: 0.857922, Accuracy: 89.87%\n",
      "Batch 162, Loss: 0.839247, Accuracy: 89.88%\n",
      "Batch 163, Loss: 0.823910, Accuracy: 89.90%\n",
      "Batch 164, Loss: 0.869452, Accuracy: 89.88%\n",
      "Batch 165, Loss: 0.892410, Accuracy: 89.84%\n",
      "Batch 166, Loss: 0.848723, Accuracy: 89.83%\n",
      "Batch 167, Loss: 0.844645, Accuracy: 89.84%\n",
      "Batch 168, Loss: 0.835407, Accuracy: 89.85%\n",
      "Batch 169, Loss: 0.851133, Accuracy: 89.85%\n",
      "Batch 170, Loss: 0.883631, Accuracy: 89.83%\n",
      "Batch 171, Loss: 0.809250, Accuracy: 89.85%\n",
      "Batch 172, Loss: 0.876629, Accuracy: 89.83%\n",
      "Batch 173, Loss: 0.885104, Accuracy: 89.80%\n",
      "Batch 174, Loss: 0.836259, Accuracy: 89.81%\n",
      "Batch 175, Loss: 0.841944, Accuracy: 89.81%\n",
      "Batch 176, Loss: 0.859241, Accuracy: 89.81%\n",
      "Batch 177, Loss: 0.857791, Accuracy: 89.80%\n",
      "Batch 178, Loss: 0.828917, Accuracy: 89.81%\n",
      "Batch 179, Loss: 0.813979, Accuracy: 89.82%\n",
      "Batch 180, Loss: 0.856762, Accuracy: 89.82%\n",
      "Batch 181, Loss: 0.820205, Accuracy: 89.83%\n",
      "Batch 182, Loss: 0.813322, Accuracy: 89.85%\n",
      "Batch 183, Loss: 0.830202, Accuracy: 89.87%\n",
      "Batch 184, Loss: 0.883211, Accuracy: 89.85%\n",
      "Batch 185, Loss: 0.829810, Accuracy: 89.86%\n",
      "Batch 186, Loss: 0.901485, Accuracy: 89.84%\n",
      "Batch 187, Loss: 0.881870, Accuracy: 89.81%\n",
      "Batch 188, Loss: 0.892639, Accuracy: 89.79%\n",
      "Batch 189, Loss: 0.914360, Accuracy: 89.76%\n",
      "Batch 190, Loss: 0.808393, Accuracy: 89.78%\n",
      "Batch 191, Loss: 0.860477, Accuracy: 89.77%\n",
      "Batch 192, Loss: 0.808467, Accuracy: 89.79%\n",
      "Batch 193, Loss: 0.796892, Accuracy: 89.82%\n",
      "Batch 194, Loss: 0.880964, Accuracy: 89.79%\n",
      "Batch 195, Loss: 0.829154, Accuracy: 89.80%\n",
      "Batch 196, Loss: 0.822185, Accuracy: 89.81%\n",
      "Batch 197, Loss: 0.860542, Accuracy: 89.81%\n",
      "Batch 198, Loss: 0.776058, Accuracy: 89.84%\n",
      "Batch 199, Loss: 0.793312, Accuracy: 89.87%\n",
      "Batch 200, Loss: 0.824397, Accuracy: 89.88%\n",
      "Batch 201, Loss: 0.816777, Accuracy: 89.89%\n",
      "Batch 202, Loss: 0.964248, Accuracy: 89.84%\n",
      "Batch 203, Loss: 0.837716, Accuracy: 89.85%\n",
      "Batch 204, Loss: 0.821700, Accuracy: 89.87%\n",
      "Batch 205, Loss: 0.818548, Accuracy: 89.89%\n",
      "Batch 206, Loss: 0.831519, Accuracy: 89.89%\n",
      "Batch 207, Loss: 0.856870, Accuracy: 89.89%\n",
      "Batch 208, Loss: 0.843028, Accuracy: 89.88%\n",
      "Batch 209, Loss: 0.846272, Accuracy: 89.88%\n",
      "Batch 210, Loss: 0.889746, Accuracy: 89.87%\n",
      "Batch 211, Loss: 0.851679, Accuracy: 89.87%\n",
      "Batch 212, Loss: 0.831199, Accuracy: 89.87%\n",
      "Batch 213, Loss: 0.850686, Accuracy: 89.88%\n",
      "Training - Epoch 16, Loss: 0.845567, Accuracy: 89.88%\n",
      "Validation Batch 1, Loss: 0.819955, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.791551, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.815688, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.835109, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.818219, Accuracy: 93.12%\n",
      "Validation Batch 6, Loss: 0.793146, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.794620, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.856101, Accuracy: 93.16%\n",
      "Validation Batch 9, Loss: 0.888748, Accuracy: 92.19%\n",
      "Validation Batch 10, Loss: 0.787388, Accuracy: 92.66%\n",
      "Validation Batch 11, Loss: 0.826916, Accuracy: 92.61%\n",
      "Validation Batch 12, Loss: 0.827075, Accuracy: 92.58%\n",
      "Validation Batch 13, Loss: 0.832863, Accuracy: 92.43%\n",
      "Validation Batch 14, Loss: 0.833720, Accuracy: 92.41%\n",
      "Validation Batch 15, Loss: 0.798471, Accuracy: 92.50%\n",
      "Validation Batch 16, Loss: 0.838092, Accuracy: 92.38%\n",
      "Validation Batch 17, Loss: 0.856855, Accuracy: 92.19%\n",
      "Validation Batch 18, Loss: 0.797389, Accuracy: 92.36%\n",
      "Validation Batch 19, Loss: 0.845621, Accuracy: 92.27%\n",
      "Validation Batch 20, Loss: 0.816598, Accuracy: 92.42%\n",
      "Validation Batch 21, Loss: 0.858733, Accuracy: 92.26%\n",
      "Validation Batch 22, Loss: 0.794628, Accuracy: 92.47%\n",
      "Validation Batch 23, Loss: 0.859301, Accuracy: 92.32%\n",
      "Validation Batch 24, Loss: 0.827691, Accuracy: 92.32%\n",
      "Validation Batch 25, Loss: 0.795654, Accuracy: 92.44%\n",
      "Validation Batch 26, Loss: 0.836239, Accuracy: 92.37%\n",
      "Validation Batch 27, Loss: 0.792778, Accuracy: 92.43%\n",
      "Validation - Epoch 16, Loss: 0.823672, Accuracy: 92.43%\n",
      "Patience—16\n",
      "Epoch 17\n",
      "Batch 1, Loss: 0.866757, Accuracy: 85.94%\n",
      "Batch 2, Loss: 0.824359, Accuracy: 89.84%\n",
      "Batch 3, Loss: 0.881148, Accuracy: 88.54%\n",
      "Batch 4, Loss: 0.891312, Accuracy: 87.89%\n",
      "Batch 5, Loss: 0.793987, Accuracy: 89.38%\n",
      "Batch 6, Loss: 0.823710, Accuracy: 89.84%\n",
      "Batch 7, Loss: 0.838361, Accuracy: 89.96%\n",
      "Batch 8, Loss: 0.846402, Accuracy: 89.84%\n",
      "Batch 9, Loss: 0.888686, Accuracy: 89.41%\n",
      "Batch 10, Loss: 0.809884, Accuracy: 89.84%\n",
      "Batch 11, Loss: 0.784919, Accuracy: 90.48%\n",
      "Batch 12, Loss: 0.803966, Accuracy: 90.89%\n",
      "Batch 13, Loss: 0.838177, Accuracy: 90.87%\n",
      "Batch 14, Loss: 0.828622, Accuracy: 91.07%\n",
      "Batch 15, Loss: 0.824649, Accuracy: 91.15%\n",
      "Batch 16, Loss: 0.834439, Accuracy: 91.11%\n",
      "Batch 17, Loss: 0.806751, Accuracy: 91.27%\n",
      "Batch 18, Loss: 0.881874, Accuracy: 90.97%\n",
      "Batch 19, Loss: 0.880660, Accuracy: 90.71%\n",
      "Batch 20, Loss: 0.887198, Accuracy: 90.47%\n",
      "Batch 21, Loss: 0.778132, Accuracy: 90.77%\n",
      "Batch 22, Loss: 0.818888, Accuracy: 90.77%\n",
      "Batch 23, Loss: 0.868359, Accuracy: 90.62%\n",
      "Batch 24, Loss: 0.858799, Accuracy: 90.49%\n",
      "Batch 25, Loss: 0.798313, Accuracy: 90.62%\n",
      "Batch 26, Loss: 0.846288, Accuracy: 90.62%\n",
      "Batch 27, Loss: 0.868320, Accuracy: 90.57%\n",
      "Batch 28, Loss: 0.864861, Accuracy: 90.51%\n",
      "Batch 29, Loss: 0.829340, Accuracy: 90.57%\n",
      "Batch 30, Loss: 0.823834, Accuracy: 90.62%\n",
      "Batch 31, Loss: 0.845772, Accuracy: 90.57%\n",
      "Batch 32, Loss: 0.861641, Accuracy: 90.53%\n",
      "Batch 33, Loss: 0.856331, Accuracy: 90.44%\n",
      "Batch 34, Loss: 0.843647, Accuracy: 90.44%\n",
      "Batch 35, Loss: 0.838562, Accuracy: 90.45%\n",
      "Batch 36, Loss: 0.940309, Accuracy: 90.15%\n",
      "Batch 37, Loss: 0.793753, Accuracy: 90.29%\n",
      "Batch 38, Loss: 0.837685, Accuracy: 90.30%\n",
      "Batch 39, Loss: 0.832850, Accuracy: 90.30%\n",
      "Batch 40, Loss: 0.798863, Accuracy: 90.43%\n",
      "Batch 41, Loss: 0.821337, Accuracy: 90.47%\n",
      "Batch 42, Loss: 0.869810, Accuracy: 90.40%\n",
      "Batch 43, Loss: 0.803636, Accuracy: 90.48%\n",
      "Batch 44, Loss: 0.763441, Accuracy: 90.66%\n",
      "Batch 45, Loss: 0.877523, Accuracy: 90.59%\n",
      "Batch 46, Loss: 0.831719, Accuracy: 90.59%\n",
      "Batch 47, Loss: 0.838607, Accuracy: 90.56%\n",
      "Batch 48, Loss: 0.855060, Accuracy: 90.53%\n",
      "Batch 49, Loss: 0.926865, Accuracy: 90.34%\n",
      "Batch 50, Loss: 0.891955, Accuracy: 90.25%\n",
      "Batch 51, Loss: 0.847229, Accuracy: 90.23%\n",
      "Batch 52, Loss: 0.872918, Accuracy: 90.14%\n",
      "Batch 53, Loss: 0.857966, Accuracy: 90.12%\n",
      "Batch 54, Loss: 0.851702, Accuracy: 90.08%\n",
      "Batch 55, Loss: 0.910605, Accuracy: 89.97%\n",
      "Batch 56, Loss: 0.871491, Accuracy: 89.96%\n",
      "Batch 57, Loss: 0.824000, Accuracy: 89.99%\n",
      "Batch 58, Loss: 0.846778, Accuracy: 90.01%\n",
      "Batch 59, Loss: 0.814610, Accuracy: 90.07%\n",
      "Batch 60, Loss: 0.932478, Accuracy: 89.92%\n",
      "Batch 61, Loss: 0.895469, Accuracy: 89.83%\n",
      "Batch 62, Loss: 0.810980, Accuracy: 89.87%\n",
      "Batch 63, Loss: 0.817675, Accuracy: 89.91%\n",
      "Batch 64, Loss: 0.815217, Accuracy: 89.97%\n",
      "Batch 65, Loss: 0.868942, Accuracy: 89.93%\n",
      "Batch 66, Loss: 0.807093, Accuracy: 89.99%\n",
      "Batch 67, Loss: 0.815225, Accuracy: 90.02%\n",
      "Batch 68, Loss: 0.838024, Accuracy: 90.05%\n",
      "Batch 69, Loss: 0.815964, Accuracy: 90.08%\n",
      "Batch 70, Loss: 0.862752, Accuracy: 90.07%\n",
      "Batch 71, Loss: 0.795499, Accuracy: 90.14%\n",
      "Batch 72, Loss: 0.864249, Accuracy: 90.10%\n",
      "Batch 73, Loss: 0.838237, Accuracy: 90.11%\n",
      "Batch 74, Loss: 0.822674, Accuracy: 90.14%\n",
      "Batch 75, Loss: 0.873622, Accuracy: 90.10%\n",
      "Batch 76, Loss: 0.896454, Accuracy: 90.03%\n",
      "Batch 77, Loss: 0.812324, Accuracy: 90.08%\n",
      "Batch 78, Loss: 0.805684, Accuracy: 90.14%\n",
      "Batch 79, Loss: 0.821795, Accuracy: 90.15%\n",
      "Batch 80, Loss: 0.799560, Accuracy: 90.20%\n",
      "Batch 81, Loss: 0.851487, Accuracy: 90.18%\n",
      "Batch 82, Loss: 0.798403, Accuracy: 90.22%\n",
      "Batch 83, Loss: 0.897476, Accuracy: 90.15%\n",
      "Batch 84, Loss: 0.821573, Accuracy: 90.16%\n",
      "Batch 85, Loss: 0.857751, Accuracy: 90.15%\n",
      "Batch 86, Loss: 0.857209, Accuracy: 90.13%\n",
      "Batch 87, Loss: 0.945822, Accuracy: 90.00%\n",
      "Batch 88, Loss: 0.895099, Accuracy: 89.95%\n",
      "Batch 89, Loss: 0.872385, Accuracy: 89.91%\n",
      "Batch 90, Loss: 0.832343, Accuracy: 89.91%\n",
      "Batch 91, Loss: 0.785597, Accuracy: 89.97%\n",
      "Batch 92, Loss: 0.811343, Accuracy: 90.01%\n",
      "Batch 93, Loss: 0.788436, Accuracy: 90.10%\n",
      "Batch 94, Loss: 0.859712, Accuracy: 90.09%\n",
      "Batch 95, Loss: 0.860152, Accuracy: 90.08%\n",
      "Batch 96, Loss: 0.893458, Accuracy: 90.04%\n",
      "Batch 97, Loss: 0.843644, Accuracy: 90.05%\n",
      "Batch 98, Loss: 0.823667, Accuracy: 90.07%\n",
      "Batch 99, Loss: 0.890022, Accuracy: 90.03%\n",
      "Batch 100, Loss: 0.893233, Accuracy: 89.97%\n",
      "Batch 101, Loss: 0.853905, Accuracy: 89.98%\n",
      "Batch 102, Loss: 0.890666, Accuracy: 89.92%\n",
      "Batch 103, Loss: 0.821939, Accuracy: 89.96%\n",
      "Batch 104, Loss: 0.822198, Accuracy: 89.98%\n",
      "Batch 105, Loss: 0.832182, Accuracy: 90.00%\n",
      "Batch 106, Loss: 0.871282, Accuracy: 89.98%\n",
      "Batch 107, Loss: 0.862661, Accuracy: 89.97%\n",
      "Batch 108, Loss: 0.869503, Accuracy: 89.95%\n",
      "Batch 109, Loss: 0.832145, Accuracy: 89.95%\n",
      "Batch 110, Loss: 0.857966, Accuracy: 89.93%\n",
      "Batch 111, Loss: 0.903758, Accuracy: 89.88%\n",
      "Batch 112, Loss: 0.778956, Accuracy: 89.94%\n",
      "Batch 113, Loss: 0.881836, Accuracy: 89.89%\n",
      "Batch 114, Loss: 0.852454, Accuracy: 89.88%\n",
      "Batch 115, Loss: 0.824961, Accuracy: 89.90%\n",
      "Batch 116, Loss: 0.901620, Accuracy: 89.86%\n",
      "Batch 117, Loss: 0.869159, Accuracy: 89.84%\n",
      "Batch 118, Loss: 0.852982, Accuracy: 89.83%\n",
      "Batch 119, Loss: 0.798967, Accuracy: 89.88%\n",
      "Batch 120, Loss: 0.861466, Accuracy: 89.87%\n",
      "Batch 121, Loss: 0.827271, Accuracy: 89.90%\n",
      "Batch 122, Loss: 0.883029, Accuracy: 89.84%\n",
      "Batch 123, Loss: 0.844679, Accuracy: 89.85%\n",
      "Batch 124, Loss: 0.822294, Accuracy: 89.87%\n",
      "Batch 125, Loss: 0.870184, Accuracy: 89.86%\n",
      "Batch 126, Loss: 0.854298, Accuracy: 89.86%\n",
      "Batch 127, Loss: 0.860351, Accuracy: 89.84%\n",
      "Batch 128, Loss: 0.942531, Accuracy: 89.76%\n",
      "Batch 129, Loss: 0.819469, Accuracy: 89.80%\n",
      "Batch 130, Loss: 0.820228, Accuracy: 89.82%\n",
      "Batch 131, Loss: 0.816016, Accuracy: 89.85%\n",
      "Batch 132, Loss: 0.817231, Accuracy: 89.87%\n",
      "Batch 133, Loss: 0.916490, Accuracy: 89.81%\n",
      "Batch 134, Loss: 0.811838, Accuracy: 89.84%\n",
      "Batch 135, Loss: 0.875956, Accuracy: 89.83%\n",
      "Batch 136, Loss: 0.852919, Accuracy: 89.82%\n",
      "Batch 137, Loss: 0.808982, Accuracy: 89.85%\n",
      "Batch 138, Loss: 0.819126, Accuracy: 89.88%\n",
      "Batch 139, Loss: 0.781569, Accuracy: 89.93%\n",
      "Batch 140, Loss: 0.860465, Accuracy: 89.91%\n",
      "Batch 141, Loss: 0.884574, Accuracy: 89.88%\n",
      "Batch 142, Loss: 0.927628, Accuracy: 89.81%\n",
      "Batch 143, Loss: 0.866240, Accuracy: 89.79%\n",
      "Batch 144, Loss: 0.838440, Accuracy: 89.80%\n",
      "Batch 145, Loss: 0.813419, Accuracy: 89.84%\n",
      "Batch 146, Loss: 0.920171, Accuracy: 89.79%\n",
      "Batch 147, Loss: 0.923738, Accuracy: 89.75%\n",
      "Batch 148, Loss: 0.884869, Accuracy: 89.74%\n",
      "Batch 149, Loss: 0.833338, Accuracy: 89.74%\n",
      "Batch 150, Loss: 0.801860, Accuracy: 89.78%\n",
      "Batch 151, Loss: 0.854595, Accuracy: 89.78%\n",
      "Batch 152, Loss: 0.891910, Accuracy: 89.74%\n",
      "Batch 153, Loss: 0.840926, Accuracy: 89.74%\n",
      "Batch 154, Loss: 0.893662, Accuracy: 89.70%\n",
      "Batch 155, Loss: 0.837870, Accuracy: 89.71%\n",
      "Batch 156, Loss: 0.808424, Accuracy: 89.73%\n",
      "Batch 157, Loss: 0.877201, Accuracy: 89.71%\n",
      "Batch 158, Loss: 0.840127, Accuracy: 89.72%\n",
      "Batch 159, Loss: 0.802854, Accuracy: 89.74%\n",
      "Batch 160, Loss: 0.843439, Accuracy: 89.74%\n",
      "Batch 161, Loss: 0.877218, Accuracy: 89.71%\n",
      "Batch 162, Loss: 0.828586, Accuracy: 89.72%\n",
      "Batch 163, Loss: 0.864944, Accuracy: 89.71%\n",
      "Batch 164, Loss: 0.836858, Accuracy: 89.73%\n",
      "Batch 165, Loss: 0.852340, Accuracy: 89.73%\n",
      "Batch 166, Loss: 0.856507, Accuracy: 89.72%\n",
      "Batch 167, Loss: 0.798737, Accuracy: 89.75%\n",
      "Batch 168, Loss: 0.830999, Accuracy: 89.76%\n",
      "Batch 169, Loss: 0.785134, Accuracy: 89.80%\n",
      "Batch 170, Loss: 0.842096, Accuracy: 89.80%\n",
      "Batch 171, Loss: 0.866016, Accuracy: 89.78%\n",
      "Batch 172, Loss: 0.860865, Accuracy: 89.76%\n",
      "Batch 173, Loss: 0.854898, Accuracy: 89.76%\n",
      "Batch 174, Loss: 0.844942, Accuracy: 89.76%\n",
      "Batch 175, Loss: 0.848992, Accuracy: 89.77%\n",
      "Batch 176, Loss: 0.949221, Accuracy: 89.70%\n",
      "Batch 177, Loss: 0.901966, Accuracy: 89.67%\n",
      "Batch 178, Loss: 0.821647, Accuracy: 89.69%\n",
      "Batch 179, Loss: 0.822330, Accuracy: 89.71%\n",
      "Batch 180, Loss: 0.854160, Accuracy: 89.70%\n",
      "Batch 181, Loss: 0.818954, Accuracy: 89.72%\n",
      "Batch 182, Loss: 0.844490, Accuracy: 89.72%\n",
      "Batch 183, Loss: 0.826319, Accuracy: 89.74%\n",
      "Batch 184, Loss: 0.833510, Accuracy: 89.73%\n",
      "Batch 185, Loss: 0.831559, Accuracy: 89.74%\n",
      "Batch 186, Loss: 0.868101, Accuracy: 89.73%\n",
      "Batch 187, Loss: 0.869289, Accuracy: 89.71%\n",
      "Batch 188, Loss: 0.849326, Accuracy: 89.72%\n",
      "Batch 189, Loss: 0.868478, Accuracy: 89.71%\n",
      "Batch 190, Loss: 0.851038, Accuracy: 89.70%\n",
      "Batch 191, Loss: 0.798791, Accuracy: 89.73%\n",
      "Batch 192, Loss: 0.794609, Accuracy: 89.76%\n",
      "Batch 193, Loss: 0.857443, Accuracy: 89.76%\n",
      "Batch 194, Loss: 0.875574, Accuracy: 89.75%\n",
      "Batch 195, Loss: 0.847027, Accuracy: 89.74%\n",
      "Batch 196, Loss: 0.824731, Accuracy: 89.76%\n",
      "Batch 197, Loss: 0.811820, Accuracy: 89.77%\n",
      "Batch 198, Loss: 0.824428, Accuracy: 89.77%\n",
      "Batch 199, Loss: 0.890048, Accuracy: 89.75%\n",
      "Batch 200, Loss: 0.871119, Accuracy: 89.73%\n",
      "Batch 201, Loss: 0.858551, Accuracy: 89.72%\n",
      "Batch 202, Loss: 0.839728, Accuracy: 89.72%\n",
      "Batch 203, Loss: 0.828639, Accuracy: 89.72%\n",
      "Batch 204, Loss: 0.852968, Accuracy: 89.72%\n",
      "Batch 205, Loss: 0.828028, Accuracy: 89.73%\n",
      "Batch 206, Loss: 0.809521, Accuracy: 89.75%\n",
      "Batch 207, Loss: 0.828359, Accuracy: 89.76%\n",
      "Batch 208, Loss: 0.820404, Accuracy: 89.78%\n",
      "Batch 209, Loss: 0.848892, Accuracy: 89.78%\n",
      "Batch 210, Loss: 0.899714, Accuracy: 89.75%\n",
      "Batch 211, Loss: 0.829560, Accuracy: 89.77%\n",
      "Batch 212, Loss: 0.822189, Accuracy: 89.78%\n",
      "Batch 213, Loss: 0.853312, Accuracy: 89.78%\n",
      "Training - Epoch 17, Loss: 0.847006, Accuracy: 89.78%\n",
      "Validation Batch 1, Loss: 0.820891, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.792633, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.816753, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.835793, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.819182, Accuracy: 93.12%\n",
      "Validation Batch 6, Loss: 0.793560, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.795064, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.856569, Accuracy: 93.16%\n",
      "Validation Batch 9, Loss: 0.889338, Accuracy: 92.19%\n",
      "Validation Batch 10, Loss: 0.788191, Accuracy: 92.66%\n",
      "Validation Batch 11, Loss: 0.827764, Accuracy: 92.61%\n",
      "Validation Batch 12, Loss: 0.827205, Accuracy: 92.58%\n",
      "Validation Batch 13, Loss: 0.833907, Accuracy: 92.43%\n",
      "Validation Batch 14, Loss: 0.834537, Accuracy: 92.41%\n",
      "Validation Batch 15, Loss: 0.798688, Accuracy: 92.50%\n",
      "Validation Batch 16, Loss: 0.838802, Accuracy: 92.38%\n",
      "Validation Batch 17, Loss: 0.857233, Accuracy: 92.19%\n",
      "Validation Batch 18, Loss: 0.798321, Accuracy: 92.36%\n",
      "Validation Batch 19, Loss: 0.846341, Accuracy: 92.27%\n",
      "Validation Batch 20, Loss: 0.819409, Accuracy: 92.42%\n",
      "Validation Batch 21, Loss: 0.859015, Accuracy: 92.26%\n",
      "Validation Batch 22, Loss: 0.795895, Accuracy: 92.47%\n",
      "Validation Batch 23, Loss: 0.859897, Accuracy: 92.32%\n",
      "Validation Batch 24, Loss: 0.828797, Accuracy: 92.25%\n",
      "Validation Batch 25, Loss: 0.796806, Accuracy: 92.38%\n",
      "Validation Batch 26, Loss: 0.836548, Accuracy: 92.31%\n",
      "Validation Batch 27, Loss: 0.793465, Accuracy: 92.37%\n",
      "Validation - Epoch 17, Loss: 0.824467, Accuracy: 92.37%\n",
      "Patience—17\n",
      "Epoch 18\n",
      "Batch 1, Loss: 0.824231, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.853094, Accuracy: 90.62%\n",
      "Batch 3, Loss: 0.826378, Accuracy: 91.15%\n",
      "Batch 4, Loss: 0.877362, Accuracy: 89.84%\n",
      "Batch 5, Loss: 0.891089, Accuracy: 89.06%\n",
      "Batch 6, Loss: 0.819810, Accuracy: 89.58%\n",
      "Batch 7, Loss: 0.779031, Accuracy: 90.62%\n",
      "Batch 8, Loss: 0.790733, Accuracy: 91.21%\n",
      "Batch 9, Loss: 0.902788, Accuracy: 90.62%\n",
      "Batch 10, Loss: 0.887946, Accuracy: 90.16%\n",
      "Batch 11, Loss: 0.877538, Accuracy: 89.91%\n",
      "Batch 12, Loss: 0.840022, Accuracy: 89.97%\n",
      "Batch 13, Loss: 0.852494, Accuracy: 90.02%\n",
      "Batch 14, Loss: 0.926077, Accuracy: 89.29%\n",
      "Batch 15, Loss: 0.782438, Accuracy: 89.79%\n",
      "Batch 16, Loss: 0.820378, Accuracy: 89.94%\n",
      "Batch 17, Loss: 0.872003, Accuracy: 89.80%\n",
      "Batch 18, Loss: 0.863869, Accuracy: 89.76%\n",
      "Batch 19, Loss: 0.792344, Accuracy: 90.05%\n",
      "Batch 20, Loss: 0.802859, Accuracy: 90.23%\n",
      "Batch 21, Loss: 0.858006, Accuracy: 90.18%\n",
      "Batch 22, Loss: 0.783552, Accuracy: 90.48%\n",
      "Batch 23, Loss: 0.828720, Accuracy: 90.56%\n",
      "Batch 24, Loss: 0.841048, Accuracy: 90.62%\n",
      "Batch 25, Loss: 0.888194, Accuracy: 90.50%\n",
      "Batch 26, Loss: 0.841229, Accuracy: 90.50%\n",
      "Batch 27, Loss: 0.874523, Accuracy: 90.39%\n",
      "Batch 28, Loss: 0.780127, Accuracy: 90.62%\n",
      "Batch 29, Loss: 0.846532, Accuracy: 90.52%\n",
      "Batch 30, Loss: 0.858298, Accuracy: 90.42%\n",
      "Batch 31, Loss: 0.805040, Accuracy: 90.57%\n",
      "Batch 32, Loss: 0.851162, Accuracy: 90.58%\n",
      "Batch 33, Loss: 0.864091, Accuracy: 90.53%\n",
      "Batch 34, Loss: 0.840097, Accuracy: 90.58%\n",
      "Batch 35, Loss: 0.825008, Accuracy: 90.62%\n",
      "Batch 36, Loss: 0.817538, Accuracy: 90.71%\n",
      "Batch 37, Loss: 0.867265, Accuracy: 90.62%\n",
      "Batch 38, Loss: 0.866739, Accuracy: 90.54%\n",
      "Batch 39, Loss: 0.828570, Accuracy: 90.58%\n",
      "Batch 40, Loss: 0.883204, Accuracy: 90.47%\n",
      "Batch 41, Loss: 0.904573, Accuracy: 90.24%\n",
      "Batch 42, Loss: 0.785531, Accuracy: 90.36%\n",
      "Batch 43, Loss: 0.852892, Accuracy: 90.33%\n",
      "Batch 44, Loss: 0.842922, Accuracy: 90.31%\n",
      "Batch 45, Loss: 0.898827, Accuracy: 90.17%\n",
      "Batch 46, Loss: 0.881674, Accuracy: 90.08%\n",
      "Batch 47, Loss: 0.895314, Accuracy: 89.96%\n",
      "Batch 48, Loss: 0.959668, Accuracy: 89.71%\n",
      "Batch 49, Loss: 0.844403, Accuracy: 89.73%\n",
      "Batch 50, Loss: 0.817664, Accuracy: 89.81%\n",
      "Batch 51, Loss: 0.851506, Accuracy: 89.80%\n",
      "Batch 52, Loss: 0.801596, Accuracy: 89.87%\n",
      "Batch 53, Loss: 0.850109, Accuracy: 89.86%\n",
      "Batch 54, Loss: 0.813680, Accuracy: 89.93%\n",
      "Batch 55, Loss: 0.871716, Accuracy: 89.89%\n",
      "Batch 56, Loss: 0.821230, Accuracy: 89.90%\n",
      "Batch 57, Loss: 0.841556, Accuracy: 89.91%\n",
      "Batch 58, Loss: 0.869438, Accuracy: 89.87%\n",
      "Batch 59, Loss: 0.784330, Accuracy: 89.99%\n",
      "Batch 60, Loss: 0.788529, Accuracy: 90.10%\n",
      "Batch 61, Loss: 0.868025, Accuracy: 90.09%\n",
      "Batch 62, Loss: 0.840723, Accuracy: 90.12%\n",
      "Batch 63, Loss: 0.882882, Accuracy: 90.03%\n",
      "Batch 64, Loss: 0.835096, Accuracy: 90.01%\n",
      "Batch 65, Loss: 0.861944, Accuracy: 89.95%\n",
      "Batch 66, Loss: 0.888786, Accuracy: 89.87%\n",
      "Batch 67, Loss: 0.851658, Accuracy: 89.88%\n",
      "Batch 68, Loss: 0.848454, Accuracy: 89.89%\n",
      "Batch 69, Loss: 0.861910, Accuracy: 89.88%\n",
      "Batch 70, Loss: 0.793541, Accuracy: 89.98%\n",
      "Batch 71, Loss: 0.802030, Accuracy: 90.05%\n",
      "Batch 72, Loss: 0.865724, Accuracy: 90.04%\n",
      "Batch 73, Loss: 0.849313, Accuracy: 90.05%\n",
      "Batch 74, Loss: 0.833925, Accuracy: 90.08%\n",
      "Batch 75, Loss: 0.860402, Accuracy: 90.02%\n",
      "Batch 76, Loss: 0.862198, Accuracy: 89.99%\n",
      "Batch 77, Loss: 0.874379, Accuracy: 89.96%\n",
      "Batch 78, Loss: 0.893397, Accuracy: 89.88%\n",
      "Batch 79, Loss: 0.848008, Accuracy: 89.87%\n",
      "Batch 80, Loss: 0.838151, Accuracy: 89.88%\n",
      "Batch 81, Loss: 0.878390, Accuracy: 89.85%\n",
      "Batch 82, Loss: 0.871639, Accuracy: 89.82%\n",
      "Batch 83, Loss: 0.799387, Accuracy: 89.89%\n",
      "Batch 84, Loss: 0.865878, Accuracy: 89.86%\n",
      "Batch 85, Loss: 0.823934, Accuracy: 89.87%\n",
      "Batch 86, Loss: 0.825018, Accuracy: 89.90%\n",
      "Batch 87, Loss: 0.807856, Accuracy: 89.94%\n",
      "Batch 88, Loss: 0.868778, Accuracy: 89.91%\n",
      "Batch 89, Loss: 0.807610, Accuracy: 89.96%\n",
      "Batch 90, Loss: 0.913414, Accuracy: 89.88%\n",
      "Batch 91, Loss: 0.796604, Accuracy: 89.94%\n",
      "Batch 92, Loss: 0.853840, Accuracy: 89.95%\n",
      "Batch 93, Loss: 0.870455, Accuracy: 89.94%\n",
      "Batch 94, Loss: 0.867933, Accuracy: 89.91%\n",
      "Batch 95, Loss: 0.822518, Accuracy: 89.93%\n",
      "Batch 96, Loss: 0.829466, Accuracy: 89.94%\n",
      "Batch 97, Loss: 0.876684, Accuracy: 89.90%\n",
      "Batch 98, Loss: 0.851360, Accuracy: 89.91%\n",
      "Batch 99, Loss: 0.849617, Accuracy: 89.90%\n",
      "Batch 100, Loss: 0.890769, Accuracy: 89.84%\n",
      "Batch 101, Loss: 0.857816, Accuracy: 89.82%\n",
      "Batch 102, Loss: 0.914256, Accuracy: 89.75%\n",
      "Batch 103, Loss: 0.870425, Accuracy: 89.71%\n",
      "Batch 104, Loss: 0.830166, Accuracy: 89.74%\n",
      "Batch 105, Loss: 0.872020, Accuracy: 89.72%\n",
      "Batch 106, Loss: 0.853682, Accuracy: 89.71%\n",
      "Batch 107, Loss: 0.784290, Accuracy: 89.78%\n",
      "Batch 108, Loss: 0.844149, Accuracy: 89.77%\n",
      "Batch 109, Loss: 0.827923, Accuracy: 89.79%\n",
      "Batch 110, Loss: 0.833560, Accuracy: 89.80%\n",
      "Batch 111, Loss: 0.852896, Accuracy: 89.78%\n",
      "Batch 112, Loss: 0.906278, Accuracy: 89.73%\n",
      "Batch 113, Loss: 0.834716, Accuracy: 89.74%\n",
      "Batch 114, Loss: 0.908970, Accuracy: 89.69%\n",
      "Batch 115, Loss: 0.817663, Accuracy: 89.71%\n",
      "Batch 116, Loss: 0.840501, Accuracy: 89.74%\n",
      "Batch 117, Loss: 0.820372, Accuracy: 89.76%\n",
      "Batch 118, Loss: 0.810937, Accuracy: 89.79%\n",
      "Batch 119, Loss: 0.907157, Accuracy: 89.73%\n",
      "Batch 120, Loss: 0.830748, Accuracy: 89.73%\n",
      "Batch 121, Loss: 0.851337, Accuracy: 89.72%\n",
      "Batch 122, Loss: 0.839558, Accuracy: 89.73%\n",
      "Batch 123, Loss: 0.861562, Accuracy: 89.71%\n",
      "Batch 124, Loss: 0.834586, Accuracy: 89.71%\n",
      "Batch 125, Loss: 0.791003, Accuracy: 89.75%\n",
      "Batch 126, Loss: 0.879776, Accuracy: 89.72%\n",
      "Batch 127, Loss: 0.831773, Accuracy: 89.74%\n",
      "Batch 128, Loss: 0.788126, Accuracy: 89.78%\n",
      "Batch 129, Loss: 0.842141, Accuracy: 89.79%\n",
      "Batch 130, Loss: 0.839228, Accuracy: 89.80%\n",
      "Batch 131, Loss: 0.827375, Accuracy: 89.81%\n",
      "Batch 132, Loss: 0.850344, Accuracy: 89.81%\n",
      "Batch 133, Loss: 0.786165, Accuracy: 89.85%\n",
      "Batch 134, Loss: 0.868818, Accuracy: 89.82%\n",
      "Batch 135, Loss: 0.798374, Accuracy: 89.86%\n",
      "Batch 136, Loss: 0.813960, Accuracy: 89.90%\n",
      "Batch 137, Loss: 0.819105, Accuracy: 89.92%\n",
      "Batch 138, Loss: 0.868688, Accuracy: 89.90%\n",
      "Batch 139, Loss: 0.854585, Accuracy: 89.88%\n",
      "Batch 140, Loss: 0.911539, Accuracy: 89.83%\n",
      "Batch 141, Loss: 0.867627, Accuracy: 89.82%\n",
      "Batch 142, Loss: 0.842806, Accuracy: 89.82%\n",
      "Batch 143, Loss: 0.844576, Accuracy: 89.82%\n",
      "Batch 144, Loss: 0.820849, Accuracy: 89.83%\n",
      "Batch 145, Loss: 0.818867, Accuracy: 89.85%\n",
      "Batch 146, Loss: 0.855651, Accuracy: 89.84%\n",
      "Batch 147, Loss: 0.857925, Accuracy: 89.85%\n",
      "Batch 148, Loss: 0.807262, Accuracy: 89.88%\n",
      "Batch 149, Loss: 0.821163, Accuracy: 89.90%\n",
      "Batch 150, Loss: 0.866045, Accuracy: 89.89%\n",
      "Batch 151, Loss: 0.851398, Accuracy: 89.88%\n",
      "Batch 152, Loss: 0.911624, Accuracy: 89.84%\n",
      "Batch 153, Loss: 0.802933, Accuracy: 89.87%\n",
      "Batch 154, Loss: 0.880150, Accuracy: 89.84%\n",
      "Batch 155, Loss: 0.834643, Accuracy: 89.85%\n",
      "Batch 156, Loss: 0.825252, Accuracy: 89.86%\n",
      "Batch 157, Loss: 0.843902, Accuracy: 89.85%\n",
      "Batch 158, Loss: 0.830575, Accuracy: 89.86%\n",
      "Batch 159, Loss: 0.892035, Accuracy: 89.84%\n",
      "Batch 160, Loss: 0.879340, Accuracy: 89.81%\n",
      "Batch 161, Loss: 0.848873, Accuracy: 89.81%\n",
      "Batch 162, Loss: 0.874163, Accuracy: 89.80%\n",
      "Batch 163, Loss: 0.883210, Accuracy: 89.76%\n",
      "Batch 164, Loss: 0.812347, Accuracy: 89.78%\n",
      "Batch 165, Loss: 0.840394, Accuracy: 89.78%\n",
      "Batch 166, Loss: 0.883857, Accuracy: 89.77%\n",
      "Batch 167, Loss: 0.845626, Accuracy: 89.76%\n",
      "Batch 168, Loss: 0.888909, Accuracy: 89.74%\n",
      "Batch 169, Loss: 0.845784, Accuracy: 89.75%\n",
      "Batch 170, Loss: 0.869176, Accuracy: 89.73%\n",
      "Batch 171, Loss: 0.927794, Accuracy: 89.70%\n",
      "Batch 172, Loss: 0.864696, Accuracy: 89.68%\n",
      "Batch 173, Loss: 0.830242, Accuracy: 89.69%\n",
      "Batch 174, Loss: 0.819118, Accuracy: 89.71%\n",
      "Batch 175, Loss: 0.837559, Accuracy: 89.71%\n",
      "Batch 176, Loss: 0.822967, Accuracy: 89.73%\n",
      "Batch 177, Loss: 0.860553, Accuracy: 89.72%\n",
      "Batch 178, Loss: 0.824253, Accuracy: 89.72%\n",
      "Batch 179, Loss: 0.827327, Accuracy: 89.73%\n",
      "Batch 180, Loss: 0.855932, Accuracy: 89.72%\n",
      "Batch 181, Loss: 0.843804, Accuracy: 89.73%\n",
      "Batch 182, Loss: 0.826348, Accuracy: 89.73%\n",
      "Batch 183, Loss: 0.806265, Accuracy: 89.75%\n",
      "Batch 184, Loss: 0.849249, Accuracy: 89.76%\n",
      "Batch 185, Loss: 0.811898, Accuracy: 89.79%\n",
      "Batch 186, Loss: 0.846520, Accuracy: 89.79%\n",
      "Batch 187, Loss: 0.843411, Accuracy: 89.80%\n",
      "Batch 188, Loss: 0.870795, Accuracy: 89.78%\n",
      "Batch 189, Loss: 0.768969, Accuracy: 89.82%\n",
      "Batch 190, Loss: 0.814856, Accuracy: 89.84%\n",
      "Batch 191, Loss: 0.809822, Accuracy: 89.86%\n",
      "Batch 192, Loss: 0.835438, Accuracy: 89.86%\n",
      "Batch 193, Loss: 0.830451, Accuracy: 89.87%\n",
      "Batch 194, Loss: 0.800576, Accuracy: 89.89%\n",
      "Batch 195, Loss: 0.831924, Accuracy: 89.90%\n",
      "Batch 196, Loss: 0.829779, Accuracy: 89.91%\n",
      "Batch 197, Loss: 0.851226, Accuracy: 89.90%\n",
      "Batch 198, Loss: 0.819658, Accuracy: 89.91%\n",
      "Batch 199, Loss: 0.860063, Accuracy: 89.89%\n",
      "Batch 200, Loss: 0.848575, Accuracy: 89.89%\n",
      "Batch 201, Loss: 0.846468, Accuracy: 89.89%\n",
      "Batch 202, Loss: 0.776922, Accuracy: 89.93%\n",
      "Batch 203, Loss: 0.828081, Accuracy: 89.93%\n",
      "Batch 204, Loss: 0.845217, Accuracy: 89.93%\n",
      "Batch 205, Loss: 0.827095, Accuracy: 89.94%\n",
      "Batch 206, Loss: 0.877019, Accuracy: 89.91%\n",
      "Batch 207, Loss: 0.876729, Accuracy: 89.89%\n",
      "Batch 208, Loss: 0.857760, Accuracy: 89.89%\n",
      "Batch 209, Loss: 0.795421, Accuracy: 89.92%\n",
      "Batch 210, Loss: 0.836870, Accuracy: 89.93%\n",
      "Batch 211, Loss: 0.818818, Accuracy: 89.94%\n",
      "Batch 212, Loss: 0.854225, Accuracy: 89.93%\n",
      "Batch 213, Loss: 0.860476, Accuracy: 89.93%\n",
      "Training - Epoch 18, Loss: 0.845235, Accuracy: 89.93%\n",
      "Validation Batch 1, Loss: 0.819093, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.791592, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.815268, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.834745, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.816760, Accuracy: 93.12%\n",
      "Validation Batch 6, Loss: 0.792838, Accuracy: 93.49%\n",
      "Validation Batch 7, Loss: 0.794605, Accuracy: 93.75%\n",
      "Validation Batch 8, Loss: 0.855137, Accuracy: 93.36%\n",
      "Validation Batch 9, Loss: 0.887657, Accuracy: 92.36%\n",
      "Validation Batch 10, Loss: 0.787275, Accuracy: 92.81%\n",
      "Validation Batch 11, Loss: 0.826037, Accuracy: 92.76%\n",
      "Validation Batch 12, Loss: 0.826982, Accuracy: 92.71%\n",
      "Validation Batch 13, Loss: 0.832926, Accuracy: 92.55%\n",
      "Validation Batch 14, Loss: 0.832907, Accuracy: 92.52%\n",
      "Validation Batch 15, Loss: 0.798000, Accuracy: 92.60%\n",
      "Validation Batch 16, Loss: 0.837974, Accuracy: 92.48%\n",
      "Validation Batch 17, Loss: 0.856290, Accuracy: 92.28%\n",
      "Validation Batch 18, Loss: 0.796946, Accuracy: 92.45%\n",
      "Validation Batch 19, Loss: 0.845923, Accuracy: 92.35%\n",
      "Validation Batch 20, Loss: 0.814914, Accuracy: 92.50%\n",
      "Validation Batch 21, Loss: 0.858735, Accuracy: 92.34%\n",
      "Validation Batch 22, Loss: 0.794272, Accuracy: 92.54%\n",
      "Validation Batch 23, Loss: 0.859526, Accuracy: 92.39%\n",
      "Validation Batch 24, Loss: 0.827091, Accuracy: 92.38%\n",
      "Validation Batch 25, Loss: 0.794880, Accuracy: 92.50%\n",
      "Validation Batch 26, Loss: 0.835927, Accuracy: 92.43%\n",
      "Validation Batch 27, Loss: 0.792430, Accuracy: 92.48%\n",
      "Validation - Epoch 18, Loss: 0.823212, Accuracy: 92.48%\n",
      "Patience—18\n",
      "Epoch 19\n",
      "Batch 1, Loss: 0.824406, Accuracy: 92.19%\n",
      "Batch 2, Loss: 0.852284, Accuracy: 90.62%\n",
      "Batch 3, Loss: 0.857687, Accuracy: 90.10%\n",
      "Batch 4, Loss: 0.860218, Accuracy: 89.45%\n",
      "Batch 5, Loss: 0.834538, Accuracy: 89.69%\n",
      "Batch 6, Loss: 0.812680, Accuracy: 90.36%\n",
      "Batch 7, Loss: 0.858379, Accuracy: 90.18%\n",
      "Batch 8, Loss: 0.883529, Accuracy: 89.45%\n",
      "Batch 9, Loss: 0.798003, Accuracy: 90.10%\n",
      "Batch 10, Loss: 0.874129, Accuracy: 89.69%\n",
      "Batch 11, Loss: 0.887660, Accuracy: 89.35%\n",
      "Batch 12, Loss: 0.867253, Accuracy: 89.19%\n",
      "Batch 13, Loss: 0.863622, Accuracy: 89.06%\n",
      "Batch 14, Loss: 0.867021, Accuracy: 88.95%\n",
      "Batch 15, Loss: 0.874035, Accuracy: 88.85%\n",
      "Batch 16, Loss: 0.826134, Accuracy: 89.06%\n",
      "Batch 17, Loss: 0.897678, Accuracy: 88.79%\n",
      "Batch 18, Loss: 0.842401, Accuracy: 88.80%\n",
      "Batch 19, Loss: 0.866108, Accuracy: 88.73%\n",
      "Batch 20, Loss: 0.895447, Accuracy: 88.52%\n",
      "Batch 21, Loss: 0.829492, Accuracy: 88.69%\n",
      "Batch 22, Loss: 0.850978, Accuracy: 88.71%\n",
      "Batch 23, Loss: 0.776074, Accuracy: 89.06%\n",
      "Batch 24, Loss: 0.889071, Accuracy: 88.93%\n",
      "Batch 25, Loss: 0.859412, Accuracy: 88.94%\n",
      "Batch 26, Loss: 0.854457, Accuracy: 88.94%\n",
      "Batch 27, Loss: 0.857175, Accuracy: 88.95%\n",
      "Batch 28, Loss: 0.879467, Accuracy: 88.90%\n",
      "Batch 29, Loss: 0.822085, Accuracy: 89.01%\n",
      "Batch 30, Loss: 0.848488, Accuracy: 89.06%\n",
      "Batch 31, Loss: 0.832951, Accuracy: 89.11%\n",
      "Batch 32, Loss: 0.814045, Accuracy: 89.21%\n",
      "Batch 33, Loss: 0.814400, Accuracy: 89.35%\n",
      "Batch 34, Loss: 0.852544, Accuracy: 89.34%\n",
      "Batch 35, Loss: 0.846285, Accuracy: 89.33%\n",
      "Batch 36, Loss: 0.915058, Accuracy: 89.15%\n",
      "Batch 37, Loss: 0.792244, Accuracy: 89.32%\n",
      "Batch 38, Loss: 0.788890, Accuracy: 89.47%\n",
      "Batch 39, Loss: 0.832908, Accuracy: 89.50%\n",
      "Batch 40, Loss: 0.819816, Accuracy: 89.57%\n",
      "Batch 41, Loss: 0.840505, Accuracy: 89.56%\n",
      "Batch 42, Loss: 0.838223, Accuracy: 89.62%\n",
      "Batch 43, Loss: 0.819266, Accuracy: 89.68%\n",
      "Batch 44, Loss: 0.847170, Accuracy: 89.70%\n",
      "Batch 45, Loss: 0.862533, Accuracy: 89.62%\n",
      "Batch 46, Loss: 0.888783, Accuracy: 89.54%\n",
      "Batch 47, Loss: 0.811517, Accuracy: 89.63%\n",
      "Batch 48, Loss: 0.817529, Accuracy: 89.71%\n",
      "Batch 49, Loss: 0.827534, Accuracy: 89.73%\n",
      "Batch 50, Loss: 0.874613, Accuracy: 89.69%\n",
      "Batch 51, Loss: 0.823424, Accuracy: 89.74%\n",
      "Batch 52, Loss: 0.866459, Accuracy: 89.69%\n",
      "Batch 53, Loss: 0.887041, Accuracy: 89.62%\n",
      "Batch 54, Loss: 0.810205, Accuracy: 89.67%\n",
      "Batch 55, Loss: 0.848353, Accuracy: 89.66%\n",
      "Batch 56, Loss: 0.837567, Accuracy: 89.68%\n",
      "Batch 57, Loss: 0.813058, Accuracy: 89.75%\n",
      "Batch 58, Loss: 0.883900, Accuracy: 89.68%\n",
      "Batch 59, Loss: 0.830893, Accuracy: 89.72%\n",
      "Batch 60, Loss: 0.823064, Accuracy: 89.77%\n",
      "Batch 61, Loss: 0.819792, Accuracy: 89.83%\n",
      "Batch 62, Loss: 0.816833, Accuracy: 89.87%\n",
      "Batch 63, Loss: 0.803286, Accuracy: 89.93%\n",
      "Batch 64, Loss: 0.795239, Accuracy: 90.01%\n",
      "Batch 65, Loss: 0.850163, Accuracy: 90.00%\n",
      "Batch 66, Loss: 0.818789, Accuracy: 90.03%\n",
      "Batch 67, Loss: 0.831569, Accuracy: 90.07%\n",
      "Batch 68, Loss: 0.870433, Accuracy: 90.03%\n",
      "Batch 69, Loss: 0.838040, Accuracy: 90.04%\n",
      "Batch 70, Loss: 0.838208, Accuracy: 90.04%\n",
      "Batch 71, Loss: 0.822259, Accuracy: 90.05%\n",
      "Batch 72, Loss: 0.885988, Accuracy: 89.97%\n",
      "Batch 73, Loss: 0.868573, Accuracy: 89.96%\n",
      "Batch 74, Loss: 0.854797, Accuracy: 89.93%\n",
      "Batch 75, Loss: 0.799372, Accuracy: 90.00%\n",
      "Batch 76, Loss: 0.875653, Accuracy: 89.93%\n",
      "Batch 77, Loss: 0.873014, Accuracy: 89.89%\n",
      "Batch 78, Loss: 0.832458, Accuracy: 89.90%\n",
      "Batch 79, Loss: 0.838956, Accuracy: 89.89%\n",
      "Batch 80, Loss: 0.856542, Accuracy: 89.88%\n",
      "Batch 81, Loss: 0.830452, Accuracy: 89.91%\n",
      "Batch 82, Loss: 0.822410, Accuracy: 89.94%\n",
      "Batch 83, Loss: 0.836376, Accuracy: 89.95%\n",
      "Batch 84, Loss: 0.812432, Accuracy: 89.97%\n",
      "Batch 85, Loss: 0.890151, Accuracy: 89.91%\n",
      "Batch 86, Loss: 0.876774, Accuracy: 89.86%\n",
      "Batch 87, Loss: 0.800251, Accuracy: 89.92%\n",
      "Batch 88, Loss: 0.893550, Accuracy: 89.84%\n",
      "Batch 89, Loss: 0.836699, Accuracy: 89.85%\n",
      "Batch 90, Loss: 0.894076, Accuracy: 89.79%\n",
      "Batch 91, Loss: 0.830290, Accuracy: 89.80%\n",
      "Batch 92, Loss: 0.788266, Accuracy: 89.86%\n",
      "Batch 93, Loss: 0.899235, Accuracy: 89.82%\n",
      "Batch 94, Loss: 0.814154, Accuracy: 89.83%\n",
      "Batch 95, Loss: 0.859018, Accuracy: 89.82%\n",
      "Batch 96, Loss: 0.853476, Accuracy: 89.81%\n",
      "Batch 97, Loss: 0.799473, Accuracy: 89.87%\n",
      "Batch 98, Loss: 0.864479, Accuracy: 89.86%\n",
      "Batch 99, Loss: 0.829079, Accuracy: 89.87%\n",
      "Batch 100, Loss: 0.828850, Accuracy: 89.89%\n",
      "Batch 101, Loss: 0.829234, Accuracy: 89.90%\n",
      "Batch 102, Loss: 0.850402, Accuracy: 89.89%\n",
      "Batch 103, Loss: 0.844390, Accuracy: 89.90%\n",
      "Batch 104, Loss: 0.937875, Accuracy: 89.81%\n",
      "Batch 105, Loss: 0.910226, Accuracy: 89.75%\n",
      "Batch 106, Loss: 0.887199, Accuracy: 89.71%\n",
      "Batch 107, Loss: 0.815664, Accuracy: 89.75%\n",
      "Batch 108, Loss: 0.814858, Accuracy: 89.79%\n",
      "Batch 109, Loss: 0.835896, Accuracy: 89.79%\n",
      "Batch 110, Loss: 0.822576, Accuracy: 89.82%\n",
      "Batch 111, Loss: 0.841059, Accuracy: 89.82%\n",
      "Batch 112, Loss: 0.807002, Accuracy: 89.86%\n",
      "Batch 113, Loss: 0.908631, Accuracy: 89.78%\n",
      "Batch 114, Loss: 0.868687, Accuracy: 89.76%\n",
      "Batch 115, Loss: 0.819132, Accuracy: 89.80%\n",
      "Batch 116, Loss: 0.929762, Accuracy: 89.72%\n",
      "Batch 117, Loss: 0.835382, Accuracy: 89.73%\n",
      "Batch 118, Loss: 0.852971, Accuracy: 89.72%\n",
      "Batch 119, Loss: 0.853371, Accuracy: 89.72%\n",
      "Batch 120, Loss: 0.822272, Accuracy: 89.73%\n",
      "Batch 121, Loss: 0.872161, Accuracy: 89.71%\n",
      "Batch 122, Loss: 0.882249, Accuracy: 89.66%\n",
      "Batch 123, Loss: 0.872576, Accuracy: 89.65%\n",
      "Batch 124, Loss: 0.794794, Accuracy: 89.69%\n",
      "Batch 125, Loss: 0.890200, Accuracy: 89.66%\n",
      "Batch 126, Loss: 0.850187, Accuracy: 89.67%\n",
      "Batch 127, Loss: 0.843747, Accuracy: 89.67%\n",
      "Batch 128, Loss: 0.857767, Accuracy: 89.66%\n",
      "Batch 129, Loss: 0.875832, Accuracy: 89.63%\n",
      "Batch 130, Loss: 0.902775, Accuracy: 89.57%\n",
      "Batch 131, Loss: 0.791186, Accuracy: 89.61%\n",
      "Batch 132, Loss: 0.810412, Accuracy: 89.64%\n",
      "Batch 133, Loss: 0.875900, Accuracy: 89.63%\n",
      "Batch 134, Loss: 0.801682, Accuracy: 89.66%\n",
      "Batch 135, Loss: 0.815729, Accuracy: 89.69%\n",
      "Batch 136, Loss: 0.832302, Accuracy: 89.69%\n",
      "Batch 137, Loss: 0.830244, Accuracy: 89.71%\n",
      "Batch 138, Loss: 0.820362, Accuracy: 89.73%\n",
      "Batch 139, Loss: 0.840304, Accuracy: 89.74%\n",
      "Batch 140, Loss: 0.803884, Accuracy: 89.77%\n",
      "Batch 141, Loss: 0.908003, Accuracy: 89.73%\n",
      "Batch 142, Loss: 0.807663, Accuracy: 89.74%\n",
      "Batch 143, Loss: 0.811421, Accuracy: 89.77%\n",
      "Batch 144, Loss: 0.883118, Accuracy: 89.76%\n",
      "Batch 145, Loss: 0.882376, Accuracy: 89.73%\n",
      "Batch 146, Loss: 0.924134, Accuracy: 89.67%\n",
      "Batch 147, Loss: 0.886153, Accuracy: 89.65%\n",
      "Batch 148, Loss: 0.841197, Accuracy: 89.65%\n",
      "Batch 149, Loss: 0.873483, Accuracy: 89.64%\n",
      "Batch 150, Loss: 0.787600, Accuracy: 89.69%\n",
      "Batch 151, Loss: 0.839057, Accuracy: 89.68%\n",
      "Batch 152, Loss: 0.835236, Accuracy: 89.70%\n",
      "Batch 153, Loss: 0.807062, Accuracy: 89.73%\n",
      "Batch 154, Loss: 0.905086, Accuracy: 89.69%\n",
      "Batch 155, Loss: 0.855857, Accuracy: 89.68%\n",
      "Batch 156, Loss: 0.859498, Accuracy: 89.67%\n",
      "Batch 157, Loss: 0.869506, Accuracy: 89.66%\n",
      "Batch 158, Loss: 0.824572, Accuracy: 89.68%\n",
      "Batch 159, Loss: 0.849501, Accuracy: 89.67%\n",
      "Batch 160, Loss: 0.856375, Accuracy: 89.66%\n",
      "Batch 161, Loss: 0.879696, Accuracy: 89.64%\n",
      "Batch 162, Loss: 0.793800, Accuracy: 89.68%\n",
      "Batch 163, Loss: 0.918369, Accuracy: 89.65%\n",
      "Batch 164, Loss: 0.940404, Accuracy: 89.59%\n",
      "Batch 165, Loss: 0.847408, Accuracy: 89.57%\n",
      "Batch 166, Loss: 0.826971, Accuracy: 89.59%\n",
      "Batch 167, Loss: 0.818549, Accuracy: 89.61%\n",
      "Batch 168, Loss: 0.796708, Accuracy: 89.64%\n",
      "Batch 169, Loss: 0.809345, Accuracy: 89.66%\n",
      "Batch 170, Loss: 0.847209, Accuracy: 89.66%\n",
      "Batch 171, Loss: 0.856165, Accuracy: 89.66%\n",
      "Batch 172, Loss: 0.855580, Accuracy: 89.64%\n",
      "Batch 173, Loss: 0.834967, Accuracy: 89.66%\n",
      "Batch 174, Loss: 0.840353, Accuracy: 89.66%\n",
      "Batch 175, Loss: 0.827691, Accuracy: 89.68%\n",
      "Batch 176, Loss: 0.912386, Accuracy: 89.64%\n",
      "Batch 177, Loss: 0.860506, Accuracy: 89.64%\n",
      "Batch 178, Loss: 0.841302, Accuracy: 89.65%\n",
      "Batch 179, Loss: 0.945754, Accuracy: 89.59%\n",
      "Batch 180, Loss: 0.892551, Accuracy: 89.56%\n",
      "Batch 181, Loss: 0.885163, Accuracy: 89.54%\n",
      "Batch 182, Loss: 0.849302, Accuracy: 89.53%\n",
      "Batch 183, Loss: 0.895200, Accuracy: 89.51%\n",
      "Batch 184, Loss: 0.825344, Accuracy: 89.52%\n",
      "Batch 185, Loss: 0.814235, Accuracy: 89.53%\n",
      "Batch 186, Loss: 0.842686, Accuracy: 89.54%\n",
      "Batch 187, Loss: 0.823416, Accuracy: 89.56%\n",
      "Batch 188, Loss: 0.847399, Accuracy: 89.55%\n",
      "Batch 189, Loss: 0.825449, Accuracy: 89.57%\n",
      "Batch 190, Loss: 0.905098, Accuracy: 89.53%\n",
      "Batch 191, Loss: 0.846535, Accuracy: 89.53%\n",
      "Batch 192, Loss: 0.861108, Accuracy: 89.53%\n",
      "Batch 193, Loss: 0.825960, Accuracy: 89.54%\n",
      "Batch 194, Loss: 0.948935, Accuracy: 89.48%\n",
      "Batch 195, Loss: 0.866683, Accuracy: 89.46%\n",
      "Batch 196, Loss: 0.866154, Accuracy: 89.45%\n",
      "Batch 197, Loss: 0.833380, Accuracy: 89.46%\n",
      "Batch 198, Loss: 0.867017, Accuracy: 89.44%\n",
      "Batch 199, Loss: 0.839735, Accuracy: 89.45%\n",
      "Batch 200, Loss: 0.849030, Accuracy: 89.45%\n",
      "Batch 201, Loss: 0.783373, Accuracy: 89.48%\n",
      "Batch 202, Loss: 0.851669, Accuracy: 89.47%\n",
      "Batch 203, Loss: 0.842995, Accuracy: 89.48%\n",
      "Batch 204, Loss: 0.841101, Accuracy: 89.48%\n",
      "Batch 205, Loss: 0.867559, Accuracy: 89.47%\n",
      "Batch 206, Loss: 0.826098, Accuracy: 89.49%\n",
      "Batch 207, Loss: 0.853022, Accuracy: 89.49%\n",
      "Batch 208, Loss: 0.837179, Accuracy: 89.50%\n",
      "Batch 209, Loss: 0.839329, Accuracy: 89.51%\n",
      "Batch 210, Loss: 0.835866, Accuracy: 89.52%\n",
      "Batch 211, Loss: 0.827569, Accuracy: 89.54%\n",
      "Batch 212, Loss: 0.842181, Accuracy: 89.54%\n",
      "Batch 213, Loss: 0.865372, Accuracy: 89.54%\n",
      "Training - Epoch 19, Loss: 0.848145, Accuracy: 89.54%\n",
      "Validation Batch 1, Loss: 0.820439, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.792521, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.816493, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.835291, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.819266, Accuracy: 93.12%\n",
      "Validation Batch 6, Loss: 0.793347, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.794800, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.856603, Accuracy: 93.16%\n",
      "Validation Batch 9, Loss: 0.889081, Accuracy: 92.19%\n",
      "Validation Batch 10, Loss: 0.787887, Accuracy: 92.66%\n",
      "Validation Batch 11, Loss: 0.827658, Accuracy: 92.61%\n",
      "Validation Batch 12, Loss: 0.827229, Accuracy: 92.58%\n",
      "Validation Batch 13, Loss: 0.833317, Accuracy: 92.43%\n",
      "Validation Batch 14, Loss: 0.834211, Accuracy: 92.41%\n",
      "Validation Batch 15, Loss: 0.798616, Accuracy: 92.50%\n",
      "Validation Batch 16, Loss: 0.838612, Accuracy: 92.38%\n",
      "Validation Batch 17, Loss: 0.856732, Accuracy: 92.19%\n",
      "Validation Batch 18, Loss: 0.797867, Accuracy: 92.36%\n",
      "Validation Batch 19, Loss: 0.845977, Accuracy: 92.27%\n",
      "Validation Batch 20, Loss: 0.818387, Accuracy: 92.42%\n",
      "Validation Batch 21, Loss: 0.858949, Accuracy: 92.26%\n",
      "Validation Batch 22, Loss: 0.795465, Accuracy: 92.47%\n",
      "Validation Batch 23, Loss: 0.859619, Accuracy: 92.32%\n",
      "Validation Batch 24, Loss: 0.828784, Accuracy: 92.25%\n",
      "Validation Batch 25, Loss: 0.796901, Accuracy: 92.38%\n",
      "Validation Batch 26, Loss: 0.836566, Accuracy: 92.31%\n",
      "Validation Batch 27, Loss: 0.793511, Accuracy: 92.37%\n",
      "Validation - Epoch 19, Loss: 0.824227, Accuracy: 92.37%\n",
      "Patience—19\n",
      "Epoch 20\n",
      "Batch 1, Loss: 0.909326, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.835211, Accuracy: 86.72%\n",
      "Batch 3, Loss: 0.830284, Accuracy: 88.54%\n",
      "Batch 4, Loss: 0.876107, Accuracy: 87.50%\n",
      "Batch 5, Loss: 0.829938, Accuracy: 88.44%\n",
      "Batch 6, Loss: 0.810305, Accuracy: 89.32%\n",
      "Batch 7, Loss: 0.771567, Accuracy: 90.40%\n",
      "Batch 8, Loss: 0.834191, Accuracy: 90.62%\n",
      "Batch 9, Loss: 0.841516, Accuracy: 90.62%\n",
      "Batch 10, Loss: 0.840778, Accuracy: 90.62%\n",
      "Batch 11, Loss: 0.865662, Accuracy: 90.34%\n",
      "Batch 12, Loss: 0.838528, Accuracy: 90.36%\n",
      "Batch 13, Loss: 0.828650, Accuracy: 90.50%\n",
      "Batch 14, Loss: 0.878988, Accuracy: 90.18%\n",
      "Batch 15, Loss: 0.913217, Accuracy: 89.79%\n",
      "Batch 16, Loss: 0.935222, Accuracy: 89.16%\n",
      "Batch 17, Loss: 0.825360, Accuracy: 89.34%\n",
      "Batch 18, Loss: 0.850028, Accuracy: 89.32%\n",
      "Batch 19, Loss: 0.879115, Accuracy: 89.14%\n",
      "Batch 20, Loss: 0.901142, Accuracy: 88.91%\n",
      "Batch 21, Loss: 0.871987, Accuracy: 88.76%\n",
      "Batch 22, Loss: 0.793118, Accuracy: 89.06%\n",
      "Batch 23, Loss: 0.857794, Accuracy: 89.06%\n",
      "Batch 24, Loss: 0.797167, Accuracy: 89.26%\n",
      "Batch 25, Loss: 0.860117, Accuracy: 89.19%\n",
      "Batch 26, Loss: 0.850201, Accuracy: 89.18%\n",
      "Batch 27, Loss: 0.891044, Accuracy: 89.00%\n",
      "Batch 28, Loss: 0.849911, Accuracy: 89.01%\n",
      "Batch 29, Loss: 0.898551, Accuracy: 88.90%\n",
      "Batch 30, Loss: 0.877322, Accuracy: 88.80%\n",
      "Batch 31, Loss: 0.807240, Accuracy: 88.96%\n",
      "Batch 32, Loss: 0.881231, Accuracy: 88.92%\n",
      "Batch 33, Loss: 0.842032, Accuracy: 88.97%\n",
      "Batch 34, Loss: 0.846767, Accuracy: 88.97%\n",
      "Batch 35, Loss: 0.793226, Accuracy: 89.15%\n",
      "Batch 36, Loss: 0.901541, Accuracy: 88.98%\n",
      "Batch 37, Loss: 0.927371, Accuracy: 88.72%\n",
      "Batch 38, Loss: 0.871254, Accuracy: 88.73%\n",
      "Batch 39, Loss: 0.831407, Accuracy: 88.78%\n",
      "Batch 40, Loss: 0.879683, Accuracy: 88.71%\n",
      "Batch 41, Loss: 0.863441, Accuracy: 88.68%\n",
      "Batch 42, Loss: 0.874522, Accuracy: 88.62%\n",
      "Batch 43, Loss: 0.813322, Accuracy: 88.70%\n",
      "Batch 44, Loss: 0.872890, Accuracy: 88.64%\n",
      "Batch 45, Loss: 0.765540, Accuracy: 88.85%\n",
      "Batch 46, Loss: 0.874320, Accuracy: 88.82%\n",
      "Batch 47, Loss: 0.812113, Accuracy: 88.93%\n",
      "Batch 48, Loss: 0.905513, Accuracy: 88.77%\n",
      "Batch 49, Loss: 0.861022, Accuracy: 88.74%\n",
      "Batch 50, Loss: 0.850342, Accuracy: 88.75%\n",
      "Batch 51, Loss: 0.774693, Accuracy: 88.91%\n",
      "Batch 52, Loss: 0.821404, Accuracy: 88.97%\n",
      "Batch 53, Loss: 0.850379, Accuracy: 88.94%\n",
      "Batch 54, Loss: 0.855523, Accuracy: 88.95%\n",
      "Batch 55, Loss: 0.922892, Accuracy: 88.75%\n",
      "Batch 56, Loss: 0.832813, Accuracy: 88.78%\n",
      "Batch 57, Loss: 0.829730, Accuracy: 88.87%\n",
      "Batch 58, Loss: 0.847731, Accuracy: 88.87%\n",
      "Batch 59, Loss: 0.910001, Accuracy: 88.77%\n",
      "Batch 60, Loss: 0.814673, Accuracy: 88.83%\n",
      "Batch 61, Loss: 0.785257, Accuracy: 88.93%\n",
      "Batch 62, Loss: 0.839113, Accuracy: 88.96%\n",
      "Batch 63, Loss: 0.841273, Accuracy: 88.96%\n",
      "Batch 64, Loss: 0.857052, Accuracy: 88.96%\n",
      "Batch 65, Loss: 0.804775, Accuracy: 89.06%\n",
      "Batch 66, Loss: 0.841784, Accuracy: 89.06%\n",
      "Batch 67, Loss: 0.857648, Accuracy: 89.06%\n",
      "Batch 68, Loss: 0.822569, Accuracy: 89.11%\n",
      "Batch 69, Loss: 0.786735, Accuracy: 89.24%\n",
      "Batch 70, Loss: 0.809006, Accuracy: 89.31%\n",
      "Batch 71, Loss: 0.846652, Accuracy: 89.30%\n",
      "Batch 72, Loss: 0.822492, Accuracy: 89.34%\n",
      "Batch 73, Loss: 0.853676, Accuracy: 89.34%\n",
      "Batch 74, Loss: 0.855657, Accuracy: 89.34%\n",
      "Batch 75, Loss: 0.865978, Accuracy: 89.31%\n",
      "Batch 76, Loss: 0.885613, Accuracy: 89.29%\n",
      "Batch 77, Loss: 0.825072, Accuracy: 89.33%\n",
      "Batch 78, Loss: 0.860069, Accuracy: 89.28%\n",
      "Batch 79, Loss: 0.856306, Accuracy: 89.28%\n",
      "Batch 80, Loss: 0.817334, Accuracy: 89.32%\n",
      "Batch 81, Loss: 0.821306, Accuracy: 89.35%\n",
      "Batch 82, Loss: 0.851920, Accuracy: 89.33%\n",
      "Batch 83, Loss: 0.854207, Accuracy: 89.34%\n",
      "Batch 84, Loss: 0.845331, Accuracy: 89.36%\n",
      "Batch 85, Loss: 0.872718, Accuracy: 89.34%\n",
      "Batch 86, Loss: 0.873816, Accuracy: 89.32%\n",
      "Batch 87, Loss: 0.928548, Accuracy: 89.21%\n",
      "Batch 88, Loss: 0.852077, Accuracy: 89.20%\n",
      "Batch 89, Loss: 0.804671, Accuracy: 89.26%\n",
      "Batch 90, Loss: 0.912053, Accuracy: 89.18%\n",
      "Batch 91, Loss: 0.811621, Accuracy: 89.23%\n",
      "Batch 92, Loss: 0.890283, Accuracy: 89.18%\n",
      "Batch 93, Loss: 0.860812, Accuracy: 89.18%\n",
      "Batch 94, Loss: 0.827323, Accuracy: 89.21%\n",
      "Batch 95, Loss: 0.825599, Accuracy: 89.23%\n",
      "Batch 96, Loss: 0.855694, Accuracy: 89.23%\n",
      "Batch 97, Loss: 0.856406, Accuracy: 89.21%\n",
      "Batch 98, Loss: 0.874489, Accuracy: 89.19%\n",
      "Batch 99, Loss: 0.786617, Accuracy: 89.27%\n",
      "Batch 100, Loss: 0.840040, Accuracy: 89.27%\n",
      "Batch 101, Loss: 0.853683, Accuracy: 89.26%\n",
      "Batch 102, Loss: 0.848463, Accuracy: 89.28%\n",
      "Batch 103, Loss: 0.839838, Accuracy: 89.29%\n",
      "Batch 104, Loss: 0.830994, Accuracy: 89.32%\n",
      "Batch 105, Loss: 0.871995, Accuracy: 89.30%\n",
      "Batch 106, Loss: 0.809948, Accuracy: 89.33%\n",
      "Batch 107, Loss: 0.839174, Accuracy: 89.34%\n",
      "Batch 108, Loss: 0.813621, Accuracy: 89.37%\n",
      "Batch 109, Loss: 0.872081, Accuracy: 89.36%\n",
      "Batch 110, Loss: 0.852831, Accuracy: 89.36%\n",
      "Batch 111, Loss: 0.870276, Accuracy: 89.34%\n",
      "Batch 112, Loss: 0.873915, Accuracy: 89.31%\n",
      "Batch 113, Loss: 0.803970, Accuracy: 89.37%\n",
      "Batch 114, Loss: 0.869720, Accuracy: 89.35%\n",
      "Batch 115, Loss: 0.830972, Accuracy: 89.36%\n",
      "Batch 116, Loss: 0.846083, Accuracy: 89.37%\n",
      "Batch 117, Loss: 0.829282, Accuracy: 89.40%\n",
      "Batch 118, Loss: 0.808883, Accuracy: 89.45%\n",
      "Batch 119, Loss: 0.811506, Accuracy: 89.48%\n",
      "Batch 120, Loss: 0.815529, Accuracy: 89.51%\n",
      "Batch 121, Loss: 0.834116, Accuracy: 89.51%\n",
      "Batch 122, Loss: 0.813218, Accuracy: 89.55%\n",
      "Batch 123, Loss: 0.813869, Accuracy: 89.58%\n",
      "Batch 124, Loss: 0.868830, Accuracy: 89.55%\n",
      "Batch 125, Loss: 0.787030, Accuracy: 89.61%\n",
      "Batch 126, Loss: 0.814418, Accuracy: 89.65%\n",
      "Batch 127, Loss: 0.837909, Accuracy: 89.67%\n",
      "Batch 128, Loss: 0.800202, Accuracy: 89.71%\n",
      "Batch 129, Loss: 0.795176, Accuracy: 89.75%\n",
      "Batch 130, Loss: 0.808112, Accuracy: 89.78%\n",
      "Batch 131, Loss: 0.854042, Accuracy: 89.78%\n",
      "Batch 132, Loss: 0.875228, Accuracy: 89.76%\n",
      "Batch 133, Loss: 0.803833, Accuracy: 89.80%\n",
      "Batch 134, Loss: 0.802792, Accuracy: 89.84%\n",
      "Batch 135, Loss: 0.791924, Accuracy: 89.88%\n",
      "Batch 136, Loss: 0.839921, Accuracy: 89.89%\n",
      "Batch 137, Loss: 0.828514, Accuracy: 89.91%\n",
      "Batch 138, Loss: 0.826949, Accuracy: 89.91%\n",
      "Batch 139, Loss: 0.954498, Accuracy: 89.83%\n",
      "Batch 140, Loss: 0.897856, Accuracy: 89.80%\n",
      "Batch 141, Loss: 0.827654, Accuracy: 89.82%\n",
      "Batch 142, Loss: 0.854325, Accuracy: 89.81%\n",
      "Batch 143, Loss: 0.828156, Accuracy: 89.83%\n",
      "Batch 144, Loss: 0.808838, Accuracy: 89.85%\n",
      "Batch 145, Loss: 0.832173, Accuracy: 89.87%\n",
      "Batch 146, Loss: 0.849999, Accuracy: 89.87%\n",
      "Batch 147, Loss: 0.855806, Accuracy: 89.86%\n",
      "Batch 148, Loss: 0.810533, Accuracy: 89.88%\n",
      "Batch 149, Loss: 0.854817, Accuracy: 89.87%\n",
      "Batch 150, Loss: 0.894386, Accuracy: 89.84%\n",
      "Batch 151, Loss: 0.856777, Accuracy: 89.84%\n",
      "Batch 152, Loss: 0.843715, Accuracy: 89.84%\n",
      "Batch 153, Loss: 0.891575, Accuracy: 89.81%\n",
      "Batch 154, Loss: 0.867206, Accuracy: 89.80%\n",
      "Batch 155, Loss: 0.833249, Accuracy: 89.81%\n",
      "Batch 156, Loss: 0.848327, Accuracy: 89.80%\n",
      "Batch 157, Loss: 0.865510, Accuracy: 89.80%\n",
      "Batch 158, Loss: 0.843845, Accuracy: 89.80%\n",
      "Batch 159, Loss: 0.823559, Accuracy: 89.82%\n",
      "Batch 160, Loss: 0.877272, Accuracy: 89.80%\n",
      "Batch 161, Loss: 0.834368, Accuracy: 89.81%\n",
      "Batch 162, Loss: 0.843931, Accuracy: 89.81%\n",
      "Batch 163, Loss: 0.900551, Accuracy: 89.77%\n",
      "Batch 164, Loss: 0.878505, Accuracy: 89.74%\n",
      "Batch 165, Loss: 0.808937, Accuracy: 89.76%\n",
      "Batch 166, Loss: 0.851918, Accuracy: 89.76%\n",
      "Batch 167, Loss: 0.832921, Accuracy: 89.76%\n",
      "Batch 168, Loss: 0.811133, Accuracy: 89.79%\n",
      "Batch 169, Loss: 0.876086, Accuracy: 89.77%\n",
      "Batch 170, Loss: 0.833473, Accuracy: 89.78%\n",
      "Batch 171, Loss: 0.847662, Accuracy: 89.78%\n",
      "Batch 172, Loss: 0.847706, Accuracy: 89.78%\n",
      "Batch 173, Loss: 0.888751, Accuracy: 89.76%\n",
      "Batch 174, Loss: 0.872533, Accuracy: 89.74%\n",
      "Batch 175, Loss: 0.809620, Accuracy: 89.76%\n",
      "Batch 176, Loss: 0.931265, Accuracy: 89.70%\n",
      "Batch 177, Loss: 0.822307, Accuracy: 89.72%\n",
      "Batch 178, Loss: 0.788892, Accuracy: 89.75%\n",
      "Batch 179, Loss: 0.801680, Accuracy: 89.78%\n",
      "Batch 180, Loss: 0.846117, Accuracy: 89.77%\n",
      "Batch 181, Loss: 0.852049, Accuracy: 89.77%\n",
      "Batch 182, Loss: 0.859292, Accuracy: 89.75%\n",
      "Batch 183, Loss: 0.819643, Accuracy: 89.77%\n",
      "Batch 184, Loss: 0.801327, Accuracy: 89.80%\n",
      "Batch 185, Loss: 0.937317, Accuracy: 89.75%\n",
      "Batch 186, Loss: 0.822663, Accuracy: 89.76%\n",
      "Batch 187, Loss: 0.864425, Accuracy: 89.76%\n",
      "Batch 188, Loss: 0.796489, Accuracy: 89.79%\n",
      "Batch 189, Loss: 0.815805, Accuracy: 89.80%\n",
      "Batch 190, Loss: 0.831876, Accuracy: 89.81%\n",
      "Batch 191, Loss: 0.921709, Accuracy: 89.76%\n",
      "Batch 192, Loss: 0.849817, Accuracy: 89.76%\n",
      "Batch 193, Loss: 0.857923, Accuracy: 89.75%\n",
      "Batch 194, Loss: 0.854976, Accuracy: 89.75%\n",
      "Batch 195, Loss: 0.858448, Accuracy: 89.74%\n",
      "Batch 196, Loss: 0.831230, Accuracy: 89.75%\n",
      "Batch 197, Loss: 0.893683, Accuracy: 89.72%\n",
      "Batch 198, Loss: 0.808699, Accuracy: 89.74%\n",
      "Batch 199, Loss: 0.815821, Accuracy: 89.75%\n",
      "Batch 200, Loss: 0.865052, Accuracy: 89.74%\n",
      "Batch 201, Loss: 0.861228, Accuracy: 89.74%\n",
      "Batch 202, Loss: 0.903231, Accuracy: 89.70%\n",
      "Batch 203, Loss: 0.855435, Accuracy: 89.69%\n",
      "Batch 204, Loss: 0.826980, Accuracy: 89.69%\n",
      "Batch 205, Loss: 0.865304, Accuracy: 89.68%\n",
      "Batch 206, Loss: 0.834530, Accuracy: 89.69%\n",
      "Batch 207, Loss: 0.783243, Accuracy: 89.72%\n",
      "Batch 208, Loss: 0.863110, Accuracy: 89.71%\n",
      "Batch 209, Loss: 0.808009, Accuracy: 89.73%\n",
      "Batch 210, Loss: 0.815206, Accuracy: 89.75%\n",
      "Batch 211, Loss: 0.877490, Accuracy: 89.74%\n",
      "Batch 212, Loss: 0.791003, Accuracy: 89.76%\n",
      "Batch 213, Loss: 0.864954, Accuracy: 89.75%\n",
      "Training - Epoch 20, Loss: 0.846018, Accuracy: 89.75%\n",
      "Validation Batch 1, Loss: 0.820344, Accuracy: 93.75%\n",
      "Validation Batch 2, Loss: 0.792469, Accuracy: 94.53%\n",
      "Validation Batch 3, Loss: 0.816678, Accuracy: 93.75%\n",
      "Validation Batch 4, Loss: 0.835377, Accuracy: 93.36%\n",
      "Validation Batch 5, Loss: 0.818758, Accuracy: 93.12%\n",
      "Validation Batch 6, Loss: 0.793411, Accuracy: 93.23%\n",
      "Validation Batch 7, Loss: 0.794792, Accuracy: 93.53%\n",
      "Validation Batch 8, Loss: 0.856413, Accuracy: 93.16%\n",
      "Validation Batch 9, Loss: 0.889189, Accuracy: 92.19%\n",
      "Validation Batch 10, Loss: 0.787848, Accuracy: 92.66%\n",
      "Validation Batch 11, Loss: 0.827326, Accuracy: 92.61%\n",
      "Validation Batch 12, Loss: 0.827046, Accuracy: 92.58%\n",
      "Validation Batch 13, Loss: 0.833660, Accuracy: 92.43%\n",
      "Validation Batch 14, Loss: 0.834226, Accuracy: 92.41%\n",
      "Validation Batch 15, Loss: 0.798302, Accuracy: 92.50%\n",
      "Validation Batch 16, Loss: 0.838519, Accuracy: 92.38%\n",
      "Validation Batch 17, Loss: 0.857169, Accuracy: 92.19%\n",
      "Validation Batch 18, Loss: 0.798051, Accuracy: 92.36%\n",
      "Validation Batch 19, Loss: 0.846239, Accuracy: 92.27%\n",
      "Validation Batch 20, Loss: 0.817140, Accuracy: 92.42%\n",
      "Validation Batch 21, Loss: 0.859170, Accuracy: 92.26%\n",
      "Validation Batch 22, Loss: 0.795389, Accuracy: 92.47%\n",
      "Validation Batch 23, Loss: 0.859742, Accuracy: 92.32%\n",
      "Validation Batch 24, Loss: 0.828367, Accuracy: 92.32%\n",
      "Validation Batch 25, Loss: 0.796294, Accuracy: 92.44%\n",
      "Validation Batch 26, Loss: 0.836409, Accuracy: 92.37%\n",
      "Validation Batch 27, Loss: 0.793027, Accuracy: 92.43%\n",
      "Validation - Epoch 20, Loss: 0.824124, Accuracy: 92.43%\n",
      "Patience—20\n",
      "Early stopping at epoch 20\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 130  \n",
    "patience = 20   \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)  \n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(images)  \n",
    "        loss = criterion(outputs, labels)  \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Batch {batch_idx+1}, Loss: {loss.item():.6f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Training - Epoch {epoch+1}, Loss: {train_loss:.6f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()  \n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for batch_idx, (images, labels) in enumerate(val_loader):\n",
    "            images, labels = images.to(device), labels.to(device)  \n",
    "\n",
    "            outputs = model(images) \n",
    "            loss = criterion(outputs, labels) \n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            print(f\"Validation Batch {batch_idx+1}, Loss: {loss.item():.6f}, Accuracy: {100 * val_correct / val_total:.2f}%\")\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    print(f\"Validation - Epoch {epoch+1}, Loss: {val_loss:.6f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accuracy_list.append(val_accuracy)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    print(f'Patience—{patience_counter}')\n",
    "\n",
    "    # Check for early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "model = best_model\n",
    "torch.save(model.state_dict(), f'{experiment}-{model_type}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
