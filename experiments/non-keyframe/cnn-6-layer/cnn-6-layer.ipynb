{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_5Layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_5Layer, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        dummy_input = torch.randn(1, 3, 224, 224)\n",
    "        conv_output_size = self._get_conv_output_size(dummy_input)\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(conv_output_size, 4, bias=True),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.l1_regularizer = nn.L1Loss()\n",
    "\n",
    "    def _get_conv_output_size(self, x):\n",
    "        with torch.no_grad():\n",
    "            conv_output = self.conv_layers(x)\n",
    "        return conv_output.view(x.size(0), -1).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_6Layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_6Layer, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        dummy_input = torch.randn(1, 3, 224, 224)\n",
    "        conv_output_size = self._get_conv_output_size(dummy_input)\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(conv_output_size, 4, bias=True),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.l1_regularizer = nn.L1Loss()\n",
    "\n",
    "    def _get_conv_output_size(self, x):\n",
    "        with torch.no_grad():\n",
    "            conv_output = self.conv_layers(x)\n",
    "        return conv_output.view(x.size(0), -1).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(model_name):\n",
    "    if model_name == '5-layer': model = CNN_5Layer()\n",
    "    if model_name == '6-layer': model = CNN_6Layer()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "dataset = datasets.ImageFolder(root='/home/ubuntu/Parkinson-Project/non-keyframes/energy_images', transform=transform)\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_size = int(total_size * 0.8) \n",
    "validation_size = int(total_size * 0.1) \n",
    "test_size = total_size - train_size - validation_size\n",
    "generator = torch.Generator().manual_seed(0) \n",
    "train_dataset, validation_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size], generator=generator)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'cnn-6-layer-130-epochs-early-stopping-with-regularization'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_6Layer(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU()\n",
      "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Dropout(p=0.5, inplace=False)\n",
      "    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Dropout(p=0.5, inplace=False)\n",
      "    (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): ReLU()\n",
      "    (17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Dropout(p=0.5, inplace=False)\n",
      "    (20): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): ReLU()\n",
      "    (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=3136, out_features=4, bias=True)\n",
      "    (2): Softmax(dim=1)\n",
      "  )\n",
      "  (l1_regularizer): L1Loss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_type = '6-layer'\n",
    "model = create_cnn(model_type)\n",
    "\n",
    "# Model summary to check architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Loss: 1.407307, Accuracy: 25.00%\n",
      "Batch 2, Loss: 1.395795, Accuracy: 23.44%\n",
      "Batch 3, Loss: 1.424106, Accuracy: 23.44%\n",
      "Batch 4, Loss: 1.349818, Accuracy: 25.00%\n",
      "Batch 5, Loss: 1.370748, Accuracy: 26.56%\n",
      "Batch 6, Loss: 1.410257, Accuracy: 26.82%\n",
      "Batch 7, Loss: 1.367382, Accuracy: 27.23%\n",
      "Batch 8, Loss: 1.318323, Accuracy: 28.32%\n",
      "Batch 9, Loss: 1.400960, Accuracy: 28.65%\n",
      "Batch 10, Loss: 1.334257, Accuracy: 29.69%\n",
      "Batch 11, Loss: 1.357157, Accuracy: 30.40%\n",
      "Batch 12, Loss: 1.372787, Accuracy: 30.47%\n",
      "Batch 13, Loss: 1.368748, Accuracy: 30.89%\n",
      "Batch 14, Loss: 1.323061, Accuracy: 31.25%\n",
      "Batch 15, Loss: 1.349883, Accuracy: 31.46%\n",
      "Batch 16, Loss: 1.348542, Accuracy: 32.03%\n",
      "Batch 17, Loss: 1.339627, Accuracy: 31.99%\n",
      "Batch 18, Loss: 1.319380, Accuracy: 32.47%\n",
      "Batch 19, Loss: 1.356905, Accuracy: 32.81%\n",
      "Batch 20, Loss: 1.301410, Accuracy: 33.44%\n",
      "Batch 21, Loss: 1.310957, Accuracy: 34.00%\n",
      "Batch 22, Loss: 1.338154, Accuracy: 34.16%\n",
      "Batch 23, Loss: 1.383393, Accuracy: 33.90%\n",
      "Batch 24, Loss: 1.361146, Accuracy: 33.85%\n",
      "Batch 25, Loss: 1.331782, Accuracy: 34.06%\n",
      "Batch 26, Loss: 1.311183, Accuracy: 34.31%\n",
      "Batch 27, Loss: 1.316146, Accuracy: 34.61%\n",
      "Batch 28, Loss: 1.314564, Accuracy: 34.82%\n",
      "Batch 29, Loss: 1.277940, Accuracy: 35.18%\n",
      "Batch 30, Loss: 1.301867, Accuracy: 35.42%\n",
      "Batch 31, Loss: 1.210211, Accuracy: 36.14%\n",
      "Batch 32, Loss: 1.339367, Accuracy: 36.33%\n",
      "Batch 33, Loss: 1.358213, Accuracy: 36.32%\n",
      "Batch 34, Loss: 1.290929, Accuracy: 36.53%\n",
      "Batch 35, Loss: 1.276924, Accuracy: 36.70%\n",
      "Batch 36, Loss: 1.355511, Accuracy: 36.68%\n",
      "Batch 37, Loss: 1.254813, Accuracy: 36.99%\n",
      "Batch 38, Loss: 1.216735, Accuracy: 37.34%\n",
      "Batch 39, Loss: 1.313398, Accuracy: 37.50%\n",
      "Batch 40, Loss: 1.360245, Accuracy: 37.46%\n",
      "Batch 41, Loss: 1.354702, Accuracy: 37.23%\n",
      "Batch 42, Loss: 1.367397, Accuracy: 37.05%\n",
      "Batch 43, Loss: 1.344831, Accuracy: 37.03%\n",
      "Batch 44, Loss: 1.291975, Accuracy: 37.18%\n",
      "Batch 45, Loss: 1.315461, Accuracy: 37.22%\n",
      "Batch 46, Loss: 1.334587, Accuracy: 37.13%\n",
      "Batch 47, Loss: 1.347444, Accuracy: 37.13%\n",
      "Batch 48, Loss: 1.329309, Accuracy: 37.08%\n",
      "Batch 49, Loss: 1.290029, Accuracy: 37.21%\n",
      "Batch 50, Loss: 1.333759, Accuracy: 37.25%\n",
      "Batch 51, Loss: 1.329360, Accuracy: 37.35%\n",
      "Batch 52, Loss: 1.236697, Accuracy: 37.62%\n",
      "Batch 53, Loss: 1.294819, Accuracy: 37.74%\n",
      "Batch 54, Loss: 1.308688, Accuracy: 37.82%\n",
      "Batch 55, Loss: 1.253457, Accuracy: 37.95%\n",
      "Batch 56, Loss: 1.309155, Accuracy: 38.06%\n",
      "Batch 57, Loss: 1.303644, Accuracy: 38.13%\n",
      "Batch 58, Loss: 1.330291, Accuracy: 38.04%\n",
      "Batch 59, Loss: 1.294648, Accuracy: 38.08%\n",
      "Batch 60, Loss: 1.314518, Accuracy: 38.26%\n",
      "Batch 61, Loss: 1.323091, Accuracy: 38.29%\n",
      "Batch 62, Loss: 1.318651, Accuracy: 38.38%\n",
      "Batch 63, Loss: 1.290867, Accuracy: 38.47%\n",
      "Batch 64, Loss: 1.288703, Accuracy: 38.53%\n",
      "Batch 65, Loss: 1.285303, Accuracy: 38.61%\n",
      "Batch 66, Loss: 1.317906, Accuracy: 38.61%\n",
      "Batch 67, Loss: 1.286341, Accuracy: 38.78%\n",
      "Batch 68, Loss: 1.256304, Accuracy: 38.83%\n",
      "Batch 69, Loss: 1.298122, Accuracy: 38.88%\n",
      "Batch 70, Loss: 1.254516, Accuracy: 38.93%\n",
      "Batch 71, Loss: 1.295110, Accuracy: 39.00%\n",
      "Batch 72, Loss: 1.252147, Accuracy: 39.15%\n",
      "Batch 73, Loss: 1.242024, Accuracy: 39.28%\n",
      "Batch 74, Loss: 1.299622, Accuracy: 39.32%\n",
      "Batch 75, Loss: 1.296821, Accuracy: 39.33%\n",
      "Batch 76, Loss: 1.228366, Accuracy: 39.51%\n",
      "Batch 77, Loss: 1.317909, Accuracy: 39.61%\n",
      "Batch 78, Loss: 1.306838, Accuracy: 39.62%\n",
      "Batch 79, Loss: 1.362872, Accuracy: 39.60%\n",
      "Batch 80, Loss: 1.275282, Accuracy: 39.65%\n",
      "Batch 81, Loss: 1.292213, Accuracy: 39.66%\n",
      "Batch 82, Loss: 1.354554, Accuracy: 39.62%\n",
      "Batch 83, Loss: 1.166288, Accuracy: 39.87%\n",
      "Batch 84, Loss: 1.257684, Accuracy: 39.97%\n",
      "Batch 85, Loss: 1.248972, Accuracy: 40.06%\n",
      "Batch 86, Loss: 1.292302, Accuracy: 40.04%\n",
      "Batch 87, Loss: 1.247127, Accuracy: 40.12%\n",
      "Batch 88, Loss: 1.264200, Accuracy: 40.23%\n",
      "Batch 89, Loss: 1.331993, Accuracy: 40.22%\n",
      "Batch 90, Loss: 1.328678, Accuracy: 40.19%\n",
      "Batch 91, Loss: 1.171734, Accuracy: 40.44%\n",
      "Batch 92, Loss: 1.299626, Accuracy: 40.47%\n",
      "Batch 93, Loss: 1.261330, Accuracy: 40.54%\n",
      "Batch 94, Loss: 1.282601, Accuracy: 40.59%\n",
      "Batch 95, Loss: 1.302094, Accuracy: 40.58%\n",
      "Batch 96, Loss: 1.303967, Accuracy: 40.56%\n",
      "Batch 97, Loss: 1.327856, Accuracy: 40.54%\n",
      "Batch 98, Loss: 1.211993, Accuracy: 40.69%\n",
      "Batch 99, Loss: 1.218746, Accuracy: 40.80%\n",
      "Batch 100, Loss: 1.201984, Accuracy: 41.00%\n",
      "Batch 101, Loss: 1.260568, Accuracy: 41.07%\n",
      "Batch 102, Loss: 1.281176, Accuracy: 41.07%\n",
      "Batch 103, Loss: 1.353612, Accuracy: 41.03%\n",
      "Batch 104, Loss: 1.282059, Accuracy: 41.06%\n",
      "Batch 105, Loss: 1.317846, Accuracy: 41.06%\n",
      "Batch 106, Loss: 1.279503, Accuracy: 41.07%\n",
      "Batch 107, Loss: 1.295733, Accuracy: 41.08%\n",
      "Batch 108, Loss: 1.258619, Accuracy: 41.19%\n",
      "Batch 109, Loss: 1.298243, Accuracy: 41.18%\n",
      "Batch 110, Loss: 1.326156, Accuracy: 41.15%\n",
      "Batch 111, Loss: 1.301940, Accuracy: 41.19%\n",
      "Batch 112, Loss: 1.219425, Accuracy: 41.28%\n",
      "Batch 113, Loss: 1.266172, Accuracy: 41.34%\n",
      "Batch 114, Loss: 1.242956, Accuracy: 41.41%\n",
      "Batch 115, Loss: 1.243694, Accuracy: 41.52%\n",
      "Batch 116, Loss: 1.243983, Accuracy: 41.58%\n",
      "Batch 117, Loss: 1.311200, Accuracy: 41.52%\n",
      "Batch 118, Loss: 1.195705, Accuracy: 41.66%\n",
      "Batch 119, Loss: 1.271774, Accuracy: 41.66%\n",
      "Batch 120, Loss: 1.157146, Accuracy: 41.81%\n",
      "Batch 121, Loss: 1.241821, Accuracy: 41.88%\n",
      "Batch 122, Loss: 1.370273, Accuracy: 41.82%\n",
      "Batch 123, Loss: 1.165432, Accuracy: 41.98%\n",
      "Batch 124, Loss: 1.329994, Accuracy: 41.97%\n",
      "Batch 125, Loss: 1.373847, Accuracy: 41.90%\n",
      "Batch 126, Loss: 1.304907, Accuracy: 41.89%\n",
      "Batch 127, Loss: 1.296068, Accuracy: 41.88%\n",
      "Batch 128, Loss: 1.243552, Accuracy: 41.93%\n",
      "Batch 129, Loss: 1.268706, Accuracy: 41.96%\n",
      "Batch 130, Loss: 1.344520, Accuracy: 41.91%\n",
      "Batch 131, Loss: 1.220609, Accuracy: 42.00%\n",
      "Batch 132, Loss: 1.242300, Accuracy: 42.05%\n",
      "Batch 133, Loss: 1.183231, Accuracy: 42.14%\n",
      "Batch 134, Loss: 1.336548, Accuracy: 42.13%\n",
      "Batch 135, Loss: 1.280061, Accuracy: 42.18%\n",
      "Batch 136, Loss: 1.313068, Accuracy: 42.18%\n",
      "Batch 137, Loss: 1.346824, Accuracy: 42.13%\n",
      "Batch 138, Loss: 1.255107, Accuracy: 42.18%\n",
      "Batch 139, Loss: 1.272013, Accuracy: 42.22%\n",
      "Batch 140, Loss: 1.271387, Accuracy: 42.24%\n",
      "Batch 141, Loss: 1.270881, Accuracy: 42.25%\n",
      "Batch 142, Loss: 1.271209, Accuracy: 42.28%\n",
      "Batch 143, Loss: 1.319581, Accuracy: 42.26%\n",
      "Batch 144, Loss: 1.249691, Accuracy: 42.31%\n",
      "Batch 145, Loss: 1.255057, Accuracy: 42.34%\n",
      "Batch 146, Loss: 1.209237, Accuracy: 42.40%\n",
      "Batch 147, Loss: 1.266820, Accuracy: 42.41%\n",
      "Batch 148, Loss: 1.237074, Accuracy: 42.49%\n",
      "Batch 149, Loss: 1.306782, Accuracy: 42.48%\n",
      "Batch 150, Loss: 1.220889, Accuracy: 42.57%\n",
      "Batch 151, Loss: 1.226090, Accuracy: 42.65%\n",
      "Batch 152, Loss: 1.281276, Accuracy: 42.67%\n",
      "Batch 153, Loss: 1.291452, Accuracy: 42.70%\n",
      "Batch 154, Loss: 1.260183, Accuracy: 42.73%\n",
      "Batch 155, Loss: 1.170521, Accuracy: 42.81%\n",
      "Batch 156, Loss: 1.202661, Accuracy: 42.88%\n",
      "Batch 157, Loss: 1.274113, Accuracy: 42.88%\n",
      "Batch 158, Loss: 1.307093, Accuracy: 42.91%\n",
      "Batch 159, Loss: 1.208400, Accuracy: 42.95%\n",
      "Batch 160, Loss: 1.286878, Accuracy: 42.97%\n",
      "Batch 161, Loss: 1.271730, Accuracy: 42.98%\n",
      "Batch 162, Loss: 1.222663, Accuracy: 43.05%\n",
      "Batch 163, Loss: 1.212024, Accuracy: 43.09%\n",
      "Batch 164, Loss: 1.207198, Accuracy: 43.14%\n",
      "Batch 165, Loss: 1.348281, Accuracy: 43.11%\n",
      "Batch 166, Loss: 1.283047, Accuracy: 43.12%\n",
      "Batch 167, Loss: 1.284651, Accuracy: 43.12%\n",
      "Batch 168, Loss: 1.293453, Accuracy: 43.11%\n",
      "Batch 169, Loss: 1.223166, Accuracy: 43.17%\n",
      "Batch 170, Loss: 1.266371, Accuracy: 43.18%\n",
      "Batch 171, Loss: 1.220567, Accuracy: 43.24%\n",
      "Batch 172, Loss: 1.290903, Accuracy: 43.24%\n",
      "Batch 173, Loss: 1.180609, Accuracy: 43.30%\n",
      "Batch 174, Loss: 1.266356, Accuracy: 43.33%\n",
      "Batch 175, Loss: 1.223527, Accuracy: 43.38%\n",
      "Batch 176, Loss: 1.205887, Accuracy: 43.45%\n",
      "Batch 177, Loss: 1.268378, Accuracy: 43.47%\n",
      "Batch 178, Loss: 1.228608, Accuracy: 43.48%\n",
      "Batch 179, Loss: 1.226023, Accuracy: 43.51%\n",
      "Batch 180, Loss: 1.244069, Accuracy: 43.54%\n",
      "Batch 181, Loss: 1.266666, Accuracy: 43.57%\n",
      "Batch 182, Loss: 1.315745, Accuracy: 43.56%\n",
      "Batch 183, Loss: 1.262760, Accuracy: 43.59%\n",
      "Batch 184, Loss: 1.276347, Accuracy: 43.60%\n",
      "Batch 185, Loss: 1.241036, Accuracy: 43.61%\n",
      "Batch 186, Loss: 1.294992, Accuracy: 43.62%\n",
      "Batch 187, Loss: 1.271254, Accuracy: 43.63%\n",
      "Batch 188, Loss: 1.292233, Accuracy: 43.63%\n",
      "Batch 189, Loss: 1.242928, Accuracy: 43.66%\n",
      "Batch 190, Loss: 1.248985, Accuracy: 43.69%\n",
      "Batch 191, Loss: 1.320489, Accuracy: 43.68%\n",
      "Batch 192, Loss: 1.281358, Accuracy: 43.68%\n",
      "Batch 193, Loss: 1.200203, Accuracy: 43.73%\n",
      "Batch 194, Loss: 1.242329, Accuracy: 43.77%\n",
      "Batch 195, Loss: 1.177972, Accuracy: 43.83%\n",
      "Batch 196, Loss: 1.241982, Accuracy: 43.85%\n",
      "Batch 197, Loss: 1.228568, Accuracy: 43.88%\n",
      "Batch 198, Loss: 1.318589, Accuracy: 43.88%\n",
      "Batch 199, Loss: 1.305083, Accuracy: 43.88%\n",
      "Batch 200, Loss: 1.200874, Accuracy: 43.95%\n",
      "Batch 201, Loss: 1.260822, Accuracy: 43.96%\n",
      "Batch 202, Loss: 1.319810, Accuracy: 43.93%\n",
      "Batch 203, Loss: 1.204836, Accuracy: 43.95%\n",
      "Batch 204, Loss: 1.334496, Accuracy: 43.93%\n",
      "Batch 205, Loss: 1.270100, Accuracy: 43.95%\n",
      "Batch 206, Loss: 1.272080, Accuracy: 43.96%\n",
      "Batch 207, Loss: 1.201790, Accuracy: 43.99%\n",
      "Batch 208, Loss: 1.293327, Accuracy: 43.99%\n",
      "Batch 209, Loss: 1.269778, Accuracy: 44.01%\n",
      "Batch 210, Loss: 1.219968, Accuracy: 44.03%\n",
      "Batch 211, Loss: 1.209268, Accuracy: 44.09%\n",
      "Batch 212, Loss: 1.255137, Accuracy: 44.09%\n",
      "Batch 213, Loss: 1.273701, Accuracy: 44.09%\n",
      "Training - Epoch 1, Loss: 1.284761, Accuracy: 44.09%\n",
      "Validation Batch 1, Loss: 1.343685, Accuracy: 37.50%\n",
      "Validation Batch 2, Loss: 1.332871, Accuracy: 38.28%\n",
      "Validation Batch 3, Loss: 1.395054, Accuracy: 35.42%\n",
      "Validation Batch 4, Loss: 1.323337, Accuracy: 36.72%\n",
      "Validation Batch 5, Loss: 1.380731, Accuracy: 35.62%\n",
      "Validation Batch 6, Loss: 1.422079, Accuracy: 33.59%\n",
      "Validation Batch 7, Loss: 1.360798, Accuracy: 33.71%\n",
      "Validation Batch 8, Loss: 1.352331, Accuracy: 33.98%\n",
      "Validation Batch 9, Loss: 1.377496, Accuracy: 33.85%\n",
      "Validation Batch 10, Loss: 1.393588, Accuracy: 33.44%\n",
      "Validation Batch 11, Loss: 1.350666, Accuracy: 33.66%\n",
      "Validation Batch 12, Loss: 1.349208, Accuracy: 33.85%\n",
      "Validation Batch 13, Loss: 1.384291, Accuracy: 33.65%\n",
      "Validation Batch 14, Loss: 1.295618, Accuracy: 34.49%\n",
      "Validation Batch 15, Loss: 1.339062, Accuracy: 34.69%\n",
      "Validation Batch 16, Loss: 1.407913, Accuracy: 34.18%\n",
      "Validation Batch 17, Loss: 1.359256, Accuracy: 34.19%\n",
      "Validation Batch 18, Loss: 1.360670, Accuracy: 34.20%\n",
      "Validation Batch 19, Loss: 1.365129, Accuracy: 34.21%\n",
      "Validation Batch 20, Loss: 1.379220, Accuracy: 34.14%\n",
      "Validation Batch 21, Loss: 1.349384, Accuracy: 34.23%\n",
      "Validation Batch 22, Loss: 1.364867, Accuracy: 34.23%\n",
      "Validation Batch 23, Loss: 1.361972, Accuracy: 34.24%\n",
      "Validation Batch 24, Loss: 1.395257, Accuracy: 33.98%\n",
      "Validation Batch 25, Loss: 1.325873, Accuracy: 34.19%\n",
      "Validation Batch 26, Loss: 1.306651, Accuracy: 34.50%\n",
      "Validation Batch 27, Loss: 1.364793, Accuracy: 34.47%\n",
      "Validation - Epoch 1, Loss: 1.360807, Accuracy: 34.47%\n",
      "Patienceâ€”0\n",
      "Epoch 2\n",
      "Batch 1, Loss: 1.272980, Accuracy: 45.31%\n",
      "Batch 2, Loss: 1.215621, Accuracy: 48.44%\n",
      "Batch 3, Loss: 1.220086, Accuracy: 48.96%\n",
      "Batch 4, Loss: 1.197901, Accuracy: 50.39%\n",
      "Batch 5, Loss: 1.239874, Accuracy: 50.94%\n",
      "Batch 6, Loss: 1.216520, Accuracy: 51.04%\n",
      "Batch 7, Loss: 1.286023, Accuracy: 49.78%\n",
      "Batch 8, Loss: 1.203989, Accuracy: 50.00%\n",
      "Batch 9, Loss: 1.269936, Accuracy: 49.65%\n",
      "Batch 10, Loss: 1.314573, Accuracy: 48.75%\n",
      "Batch 11, Loss: 1.296845, Accuracy: 48.44%\n",
      "Batch 12, Loss: 1.207337, Accuracy: 48.96%\n",
      "Batch 13, Loss: 1.202606, Accuracy: 49.28%\n",
      "Batch 14, Loss: 1.282466, Accuracy: 49.00%\n",
      "Batch 15, Loss: 1.189352, Accuracy: 49.17%\n",
      "Batch 16, Loss: 1.221875, Accuracy: 49.41%\n",
      "Batch 17, Loss: 1.277161, Accuracy: 49.08%\n",
      "Batch 18, Loss: 1.259465, Accuracy: 48.96%\n",
      "Batch 19, Loss: 1.340182, Accuracy: 48.36%\n",
      "Batch 20, Loss: 1.261512, Accuracy: 48.28%\n",
      "Batch 21, Loss: 1.268778, Accuracy: 48.21%\n",
      "Batch 22, Loss: 1.284638, Accuracy: 48.01%\n",
      "Batch 23, Loss: 1.207229, Accuracy: 48.17%\n",
      "Batch 24, Loss: 1.300718, Accuracy: 47.98%\n",
      "Batch 25, Loss: 1.278249, Accuracy: 47.94%\n",
      "Batch 26, Loss: 1.190084, Accuracy: 48.20%\n",
      "Batch 27, Loss: 1.222861, Accuracy: 48.32%\n",
      "Batch 28, Loss: 1.134369, Accuracy: 48.83%\n",
      "Batch 29, Loss: 1.258544, Accuracy: 48.65%\n",
      "Batch 30, Loss: 1.174300, Accuracy: 48.91%\n",
      "Batch 31, Loss: 1.201756, Accuracy: 49.09%\n",
      "Batch 32, Loss: 1.192514, Accuracy: 49.37%\n",
      "Batch 33, Loss: 1.189126, Accuracy: 49.57%\n",
      "Batch 34, Loss: 1.276434, Accuracy: 49.40%\n",
      "Batch 35, Loss: 1.282903, Accuracy: 49.24%\n",
      "Batch 36, Loss: 1.238009, Accuracy: 49.18%\n",
      "Batch 37, Loss: 1.204539, Accuracy: 49.28%\n",
      "Batch 38, Loss: 1.186543, Accuracy: 49.38%\n",
      "Batch 39, Loss: 1.254544, Accuracy: 49.40%\n",
      "Batch 40, Loss: 1.255641, Accuracy: 49.38%\n",
      "Batch 41, Loss: 1.179326, Accuracy: 49.58%\n",
      "Batch 42, Loss: 1.246785, Accuracy: 49.55%\n",
      "Batch 43, Loss: 1.246241, Accuracy: 49.45%\n",
      "Batch 44, Loss: 1.243801, Accuracy: 49.47%\n",
      "Batch 45, Loss: 1.212361, Accuracy: 49.62%\n",
      "Batch 46, Loss: 1.222991, Accuracy: 49.66%\n",
      "Batch 47, Loss: 1.238771, Accuracy: 49.70%\n",
      "Batch 48, Loss: 1.187937, Accuracy: 49.77%\n",
      "Batch 49, Loss: 1.184058, Accuracy: 49.90%\n",
      "Batch 50, Loss: 1.292554, Accuracy: 49.84%\n",
      "Batch 51, Loss: 1.326175, Accuracy: 49.63%\n",
      "Batch 52, Loss: 1.274931, Accuracy: 49.55%\n",
      "Batch 53, Loss: 1.263126, Accuracy: 49.50%\n",
      "Batch 54, Loss: 1.207703, Accuracy: 49.57%\n",
      "Batch 55, Loss: 1.255517, Accuracy: 49.55%\n",
      "Batch 56, Loss: 1.241613, Accuracy: 49.55%\n",
      "Batch 57, Loss: 1.314062, Accuracy: 49.42%\n",
      "Batch 58, Loss: 1.253762, Accuracy: 49.43%\n",
      "Batch 59, Loss: 1.243009, Accuracy: 49.42%\n",
      "Batch 60, Loss: 1.261910, Accuracy: 49.43%\n",
      "Batch 61, Loss: 1.370939, Accuracy: 49.18%\n",
      "Batch 62, Loss: 1.196652, Accuracy: 49.24%\n",
      "Batch 63, Loss: 1.254001, Accuracy: 49.21%\n",
      "Batch 64, Loss: 1.202004, Accuracy: 49.29%\n",
      "Batch 65, Loss: 1.374950, Accuracy: 49.06%\n",
      "Batch 66, Loss: 1.179340, Accuracy: 49.17%\n",
      "Batch 67, Loss: 1.232689, Accuracy: 49.14%\n",
      "Batch 68, Loss: 1.204135, Accuracy: 49.24%\n",
      "Batch 69, Loss: 1.161641, Accuracy: 49.37%\n",
      "Batch 70, Loss: 1.221434, Accuracy: 49.35%\n",
      "Batch 71, Loss: 1.243998, Accuracy: 49.34%\n",
      "Batch 72, Loss: 1.291175, Accuracy: 49.26%\n",
      "Batch 73, Loss: 1.226207, Accuracy: 49.27%\n",
      "Batch 74, Loss: 1.230732, Accuracy: 49.28%\n",
      "Batch 75, Loss: 1.276568, Accuracy: 49.23%\n",
      "Batch 76, Loss: 1.308383, Accuracy: 49.10%\n",
      "Batch 77, Loss: 1.300982, Accuracy: 48.97%\n",
      "Batch 78, Loss: 1.307937, Accuracy: 48.90%\n",
      "Batch 79, Loss: 1.292908, Accuracy: 48.81%\n",
      "Batch 80, Loss: 1.226361, Accuracy: 48.83%\n",
      "Batch 81, Loss: 1.116466, Accuracy: 49.02%\n",
      "Batch 82, Loss: 1.310616, Accuracy: 48.93%\n",
      "Batch 83, Loss: 1.291895, Accuracy: 48.85%\n",
      "Batch 84, Loss: 1.227314, Accuracy: 48.88%\n",
      "Batch 85, Loss: 1.287714, Accuracy: 48.77%\n",
      "Batch 86, Loss: 1.183036, Accuracy: 48.82%\n",
      "Batch 87, Loss: 1.189188, Accuracy: 48.89%\n",
      "Batch 88, Loss: 1.254581, Accuracy: 48.86%\n",
      "Batch 89, Loss: 1.254556, Accuracy: 48.84%\n",
      "Batch 90, Loss: 1.266363, Accuracy: 48.78%\n",
      "Batch 91, Loss: 1.296759, Accuracy: 48.70%\n",
      "Batch 92, Loss: 1.243195, Accuracy: 48.71%\n",
      "Batch 93, Loss: 1.253325, Accuracy: 48.69%\n",
      "Batch 94, Loss: 1.276757, Accuracy: 48.67%\n",
      "Batch 95, Loss: 1.241084, Accuracy: 48.67%\n",
      "Batch 96, Loss: 1.237048, Accuracy: 48.70%\n",
      "Batch 97, Loss: 1.204500, Accuracy: 48.73%\n",
      "Batch 98, Loss: 1.235042, Accuracy: 48.72%\n",
      "Batch 99, Loss: 1.207170, Accuracy: 48.75%\n",
      "Batch 100, Loss: 1.199727, Accuracy: 48.81%\n",
      "Batch 101, Loss: 1.172780, Accuracy: 48.89%\n",
      "Batch 102, Loss: 1.219493, Accuracy: 48.96%\n",
      "Batch 103, Loss: 1.264293, Accuracy: 48.92%\n",
      "Batch 104, Loss: 1.153949, Accuracy: 49.04%\n",
      "Batch 105, Loss: 1.421967, Accuracy: 48.85%\n",
      "Batch 106, Loss: 1.134557, Accuracy: 48.95%\n",
      "Batch 107, Loss: 1.238997, Accuracy: 48.95%\n",
      "Batch 108, Loss: 1.307437, Accuracy: 48.89%\n",
      "Batch 109, Loss: 1.239557, Accuracy: 48.87%\n",
      "Batch 110, Loss: 1.257744, Accuracy: 48.85%\n",
      "Batch 111, Loss: 1.269512, Accuracy: 48.83%\n",
      "Batch 112, Loss: 1.191074, Accuracy: 48.88%\n",
      "Batch 113, Loss: 1.238631, Accuracy: 48.88%\n",
      "Batch 114, Loss: 1.280289, Accuracy: 48.85%\n",
      "Batch 115, Loss: 1.248149, Accuracy: 48.82%\n",
      "Batch 116, Loss: 1.276163, Accuracy: 48.76%\n",
      "Batch 117, Loss: 1.290541, Accuracy: 48.68%\n",
      "Batch 118, Loss: 1.229477, Accuracy: 48.65%\n",
      "Batch 119, Loss: 1.238865, Accuracy: 48.69%\n",
      "Batch 120, Loss: 1.140949, Accuracy: 48.79%\n",
      "Batch 121, Loss: 1.265849, Accuracy: 48.79%\n",
      "Batch 122, Loss: 1.254969, Accuracy: 48.78%\n",
      "Batch 123, Loss: 1.253840, Accuracy: 48.78%\n",
      "Batch 124, Loss: 1.127205, Accuracy: 48.88%\n",
      "Batch 125, Loss: 1.249398, Accuracy: 48.88%\n",
      "Batch 126, Loss: 1.229016, Accuracy: 48.88%\n",
      "Batch 127, Loss: 1.287458, Accuracy: 48.87%\n",
      "Batch 128, Loss: 1.301502, Accuracy: 48.83%\n",
      "Batch 129, Loss: 1.239213, Accuracy: 48.83%\n",
      "Batch 130, Loss: 1.268156, Accuracy: 48.82%\n",
      "Batch 131, Loss: 1.178632, Accuracy: 48.89%\n",
      "Batch 132, Loss: 1.195533, Accuracy: 48.93%\n",
      "Batch 133, Loss: 1.259116, Accuracy: 48.92%\n",
      "Batch 134, Loss: 1.174648, Accuracy: 49.01%\n",
      "Batch 135, Loss: 1.303784, Accuracy: 48.96%\n",
      "Batch 136, Loss: 1.324713, Accuracy: 48.89%\n",
      "Batch 137, Loss: 1.315804, Accuracy: 48.83%\n",
      "Batch 138, Loss: 1.234721, Accuracy: 48.83%\n",
      "Batch 139, Loss: 1.151862, Accuracy: 48.90%\n",
      "Batch 140, Loss: 1.245747, Accuracy: 48.91%\n",
      "Batch 141, Loss: 1.202850, Accuracy: 48.94%\n",
      "Batch 142, Loss: 1.275509, Accuracy: 48.93%\n",
      "Batch 143, Loss: 1.145248, Accuracy: 49.01%\n",
      "Batch 144, Loss: 1.248331, Accuracy: 49.01%\n",
      "Batch 145, Loss: 1.223786, Accuracy: 49.03%\n",
      "Batch 146, Loss: 1.274610, Accuracy: 49.00%\n",
      "Batch 147, Loss: 1.189463, Accuracy: 49.04%\n",
      "Batch 148, Loss: 1.234743, Accuracy: 49.07%\n",
      "Batch 149, Loss: 1.204682, Accuracy: 49.12%\n",
      "Batch 150, Loss: 1.349450, Accuracy: 49.04%\n",
      "Batch 151, Loss: 1.167357, Accuracy: 49.10%\n",
      "Batch 152, Loss: 1.222703, Accuracy: 49.13%\n",
      "Batch 153, Loss: 1.222725, Accuracy: 49.14%\n",
      "Batch 154, Loss: 1.152163, Accuracy: 49.22%\n",
      "Batch 155, Loss: 1.285031, Accuracy: 49.18%\n",
      "Batch 156, Loss: 1.174287, Accuracy: 49.24%\n",
      "Batch 157, Loss: 1.262022, Accuracy: 49.22%\n",
      "Batch 158, Loss: 1.207355, Accuracy: 49.26%\n",
      "Batch 159, Loss: 1.275215, Accuracy: 49.24%\n",
      "Batch 160, Loss: 1.269152, Accuracy: 49.19%\n",
      "Batch 161, Loss: 1.233189, Accuracy: 49.20%\n",
      "Batch 162, Loss: 1.181759, Accuracy: 49.24%\n",
      "Batch 163, Loss: 1.186733, Accuracy: 49.26%\n",
      "Batch 164, Loss: 1.313231, Accuracy: 49.21%\n",
      "Batch 165, Loss: 1.301542, Accuracy: 49.15%\n",
      "Batch 166, Loss: 1.202420, Accuracy: 49.16%\n",
      "Batch 167, Loss: 1.247326, Accuracy: 49.16%\n",
      "Batch 168, Loss: 1.207505, Accuracy: 49.19%\n",
      "Batch 169, Loss: 1.362429, Accuracy: 49.09%\n",
      "Batch 170, Loss: 1.287418, Accuracy: 49.06%\n",
      "Batch 171, Loss: 1.271119, Accuracy: 49.05%\n",
      "Batch 172, Loss: 1.152400, Accuracy: 49.11%\n",
      "Batch 173, Loss: 1.219046, Accuracy: 49.12%\n",
      "Batch 174, Loss: 1.224873, Accuracy: 49.11%\n",
      "Batch 175, Loss: 1.277289, Accuracy: 49.08%\n",
      "Batch 176, Loss: 1.201052, Accuracy: 49.10%\n",
      "Batch 177, Loss: 1.274831, Accuracy: 49.06%\n",
      "Batch 178, Loss: 1.268869, Accuracy: 49.06%\n",
      "Batch 179, Loss: 1.312129, Accuracy: 49.00%\n",
      "Batch 180, Loss: 1.233727, Accuracy: 49.02%\n",
      "Batch 181, Loss: 1.265325, Accuracy: 49.00%\n",
      "Batch 182, Loss: 1.239286, Accuracy: 49.01%\n",
      "Batch 183, Loss: 1.254109, Accuracy: 49.00%\n",
      "Batch 184, Loss: 1.242441, Accuracy: 48.99%\n",
      "Batch 185, Loss: 1.252591, Accuracy: 48.99%\n",
      "Batch 186, Loss: 1.268902, Accuracy: 48.98%\n",
      "Batch 187, Loss: 1.254914, Accuracy: 48.96%\n",
      "Batch 188, Loss: 1.144304, Accuracy: 49.01%\n",
      "Batch 189, Loss: 1.221967, Accuracy: 49.00%\n",
      "Batch 190, Loss: 1.273545, Accuracy: 48.99%\n",
      "Batch 191, Loss: 1.161690, Accuracy: 49.03%\n",
      "Batch 192, Loss: 1.190821, Accuracy: 49.06%\n",
      "Batch 193, Loss: 1.234554, Accuracy: 49.06%\n",
      "Batch 194, Loss: 1.297903, Accuracy: 49.03%\n",
      "Batch 195, Loss: 1.326920, Accuracy: 48.97%\n",
      "Batch 196, Loss: 1.146317, Accuracy: 49.03%\n",
      "Batch 197, Loss: 1.184426, Accuracy: 49.06%\n",
      "Batch 198, Loss: 1.343447, Accuracy: 49.01%\n",
      "Batch 199, Loss: 1.252509, Accuracy: 49.00%\n",
      "Batch 200, Loss: 1.217370, Accuracy: 49.01%\n",
      "Batch 201, Loss: 1.297186, Accuracy: 48.99%\n",
      "Batch 202, Loss: 1.247442, Accuracy: 48.99%\n",
      "Batch 203, Loss: 1.270957, Accuracy: 48.97%\n",
      "Batch 204, Loss: 1.179027, Accuracy: 49.00%\n",
      "Batch 205, Loss: 1.149587, Accuracy: 49.05%\n",
      "Batch 206, Loss: 1.175275, Accuracy: 49.10%\n",
      "Batch 207, Loss: 1.219422, Accuracy: 49.10%\n",
      "Batch 208, Loss: 1.203858, Accuracy: 49.14%\n",
      "Batch 209, Loss: 1.272991, Accuracy: 49.13%\n",
      "Batch 210, Loss: 1.186992, Accuracy: 49.15%\n",
      "Batch 211, Loss: 1.313748, Accuracy: 49.11%\n",
      "Batch 212, Loss: 1.275994, Accuracy: 49.09%\n",
      "Batch 213, Loss: 1.243143, Accuracy: 49.09%\n",
      "Training - Epoch 2, Loss: 1.241240, Accuracy: 49.09%\n",
      "Validation Batch 1, Loss: 1.346152, Accuracy: 37.50%\n",
      "Validation Batch 2, Loss: 1.333772, Accuracy: 38.28%\n",
      "Validation Batch 3, Loss: 1.408255, Accuracy: 35.42%\n",
      "Validation Batch 4, Loss: 1.320709, Accuracy: 36.72%\n",
      "Validation Batch 5, Loss: 1.394908, Accuracy: 35.62%\n",
      "Validation Batch 6, Loss: 1.446158, Accuracy: 33.59%\n",
      "Validation Batch 7, Loss: 1.366715, Accuracy: 33.71%\n",
      "Validation Batch 8, Loss: 1.358790, Accuracy: 33.98%\n",
      "Validation Batch 9, Loss: 1.386325, Accuracy: 33.85%\n",
      "Validation Batch 10, Loss: 1.408371, Accuracy: 33.44%\n",
      "Validation Batch 11, Loss: 1.355684, Accuracy: 33.66%\n",
      "Validation Batch 12, Loss: 1.352292, Accuracy: 33.85%\n",
      "Validation Batch 13, Loss: 1.396682, Accuracy: 33.65%\n",
      "Validation Batch 14, Loss: 1.287052, Accuracy: 34.49%\n",
      "Validation Batch 15, Loss: 1.343308, Accuracy: 34.69%\n",
      "Validation Batch 16, Loss: 1.426532, Accuracy: 34.18%\n",
      "Validation Batch 17, Loss: 1.368609, Accuracy: 34.19%\n",
      "Validation Batch 18, Loss: 1.368827, Accuracy: 34.20%\n",
      "Validation Batch 19, Loss: 1.372309, Accuracy: 34.21%\n",
      "Validation Batch 20, Loss: 1.387140, Accuracy: 34.14%\n",
      "Validation Batch 21, Loss: 1.354998, Accuracy: 34.23%\n",
      "Validation Batch 22, Loss: 1.374022, Accuracy: 34.23%\n",
      "Validation Batch 23, Loss: 1.369951, Accuracy: 34.24%\n",
      "Validation Batch 24, Loss: 1.412739, Accuracy: 33.98%\n",
      "Validation Batch 25, Loss: 1.327383, Accuracy: 34.19%\n",
      "Validation Batch 26, Loss: 1.303077, Accuracy: 34.50%\n",
      "Validation Batch 27, Loss: 1.376102, Accuracy: 34.47%\n",
      "Validation - Epoch 2, Loss: 1.368402, Accuracy: 34.47%\n",
      "Patienceâ€”1\n",
      "Epoch 3\n",
      "Batch 1, Loss: 1.224183, Accuracy: 48.44%\n",
      "Batch 2, Loss: 1.235478, Accuracy: 49.22%\n",
      "Batch 3, Loss: 1.184374, Accuracy: 52.08%\n",
      "Batch 4, Loss: 1.217561, Accuracy: 51.95%\n",
      "Batch 5, Loss: 1.279150, Accuracy: 50.94%\n",
      "Batch 6, Loss: 1.312704, Accuracy: 49.22%\n",
      "Batch 7, Loss: 1.209365, Accuracy: 49.33%\n",
      "Batch 8, Loss: 1.198961, Accuracy: 50.00%\n",
      "Batch 9, Loss: 1.140869, Accuracy: 51.22%\n",
      "Batch 10, Loss: 1.371949, Accuracy: 49.84%\n",
      "Batch 11, Loss: 1.319360, Accuracy: 48.86%\n",
      "Batch 12, Loss: 1.185790, Accuracy: 49.35%\n",
      "Batch 13, Loss: 1.223238, Accuracy: 49.40%\n",
      "Batch 14, Loss: 1.202212, Accuracy: 49.55%\n",
      "Batch 15, Loss: 1.270433, Accuracy: 49.38%\n",
      "Batch 16, Loss: 1.200487, Accuracy: 49.61%\n",
      "Batch 17, Loss: 1.200457, Accuracy: 49.63%\n",
      "Batch 18, Loss: 1.236074, Accuracy: 49.57%\n",
      "Batch 19, Loss: 1.180657, Accuracy: 49.92%\n",
      "Batch 20, Loss: 1.229829, Accuracy: 49.92%\n",
      "Batch 21, Loss: 1.195439, Accuracy: 49.93%\n",
      "Batch 22, Loss: 1.320961, Accuracy: 49.29%\n",
      "Batch 23, Loss: 1.159465, Accuracy: 49.80%\n",
      "Batch 24, Loss: 1.222178, Accuracy: 49.87%\n",
      "Batch 25, Loss: 1.190261, Accuracy: 50.25%\n",
      "Batch 26, Loss: 1.229045, Accuracy: 50.30%\n",
      "Batch 27, Loss: 1.180050, Accuracy: 50.46%\n",
      "Batch 28, Loss: 1.165567, Accuracy: 50.67%\n",
      "Batch 29, Loss: 1.214492, Accuracy: 50.70%\n",
      "Batch 30, Loss: 1.298905, Accuracy: 50.36%\n",
      "Batch 31, Loss: 1.328400, Accuracy: 50.05%\n",
      "Batch 32, Loss: 1.237261, Accuracy: 50.00%\n",
      "Batch 33, Loss: 1.242258, Accuracy: 49.86%\n",
      "Batch 34, Loss: 1.179456, Accuracy: 50.09%\n",
      "Batch 35, Loss: 1.246217, Accuracy: 50.04%\n",
      "Batch 36, Loss: 1.255200, Accuracy: 49.96%\n",
      "Batch 37, Loss: 1.283381, Accuracy: 49.75%\n",
      "Batch 38, Loss: 1.105407, Accuracy: 50.16%\n",
      "Batch 39, Loss: 1.306870, Accuracy: 49.96%\n",
      "Batch 40, Loss: 1.145292, Accuracy: 50.16%\n",
      "Batch 41, Loss: 1.213464, Accuracy: 50.11%\n",
      "Batch 42, Loss: 1.222112, Accuracy: 50.22%\n",
      "Batch 43, Loss: 1.121896, Accuracy: 50.55%\n",
      "Batch 44, Loss: 1.217119, Accuracy: 50.50%\n",
      "Batch 45, Loss: 1.255192, Accuracy: 50.42%\n",
      "Batch 46, Loss: 1.312249, Accuracy: 50.27%\n",
      "Batch 47, Loss: 1.211876, Accuracy: 50.40%\n",
      "Batch 48, Loss: 1.230260, Accuracy: 50.42%\n",
      "Batch 49, Loss: 1.213636, Accuracy: 50.45%\n",
      "Batch 50, Loss: 1.139382, Accuracy: 50.56%\n",
      "Batch 51, Loss: 1.214788, Accuracy: 50.64%\n",
      "Batch 52, Loss: 1.291385, Accuracy: 50.42%\n",
      "Batch 53, Loss: 1.174806, Accuracy: 50.53%\n",
      "Batch 54, Loss: 1.248272, Accuracy: 50.46%\n",
      "Batch 55, Loss: 1.266980, Accuracy: 50.37%\n",
      "Batch 56, Loss: 1.185168, Accuracy: 50.45%\n",
      "Batch 57, Loss: 1.290435, Accuracy: 50.30%\n",
      "Batch 58, Loss: 1.268865, Accuracy: 50.24%\n",
      "Batch 59, Loss: 1.240527, Accuracy: 50.19%\n",
      "Batch 60, Loss: 1.258793, Accuracy: 50.10%\n",
      "Batch 61, Loss: 1.194118, Accuracy: 50.13%\n",
      "Batch 62, Loss: 1.180042, Accuracy: 50.25%\n",
      "Batch 63, Loss: 1.204513, Accuracy: 50.32%\n",
      "Batch 64, Loss: 1.277990, Accuracy: 50.24%\n",
      "Batch 65, Loss: 1.291385, Accuracy: 50.12%\n",
      "Batch 66, Loss: 1.234518, Accuracy: 50.02%\n",
      "Batch 67, Loss: 1.296529, Accuracy: 49.88%\n",
      "Batch 68, Loss: 1.232905, Accuracy: 49.86%\n",
      "Batch 69, Loss: 1.175639, Accuracy: 49.93%\n",
      "Batch 70, Loss: 1.170015, Accuracy: 50.02%\n",
      "Batch 71, Loss: 1.243041, Accuracy: 50.00%\n",
      "Batch 72, Loss: 1.204015, Accuracy: 50.00%\n",
      "Batch 73, Loss: 1.226172, Accuracy: 50.00%\n",
      "Batch 74, Loss: 1.270211, Accuracy: 49.89%\n",
      "Batch 75, Loss: 1.216748, Accuracy: 49.90%\n",
      "Batch 76, Loss: 1.257781, Accuracy: 49.86%\n",
      "Batch 77, Loss: 1.220379, Accuracy: 49.90%\n",
      "Batch 78, Loss: 1.205283, Accuracy: 49.96%\n",
      "Batch 79, Loss: 1.152062, Accuracy: 50.08%\n",
      "Batch 80, Loss: 1.278033, Accuracy: 50.00%\n",
      "Batch 81, Loss: 1.213924, Accuracy: 50.04%\n",
      "Batch 82, Loss: 1.286462, Accuracy: 49.98%\n",
      "Batch 83, Loss: 1.274046, Accuracy: 49.92%\n",
      "Batch 84, Loss: 1.293910, Accuracy: 49.83%\n",
      "Batch 85, Loss: 1.239284, Accuracy: 49.82%\n",
      "Batch 86, Loss: 1.200956, Accuracy: 49.84%\n",
      "Batch 87, Loss: 1.267498, Accuracy: 49.78%\n",
      "Batch 88, Loss: 1.228016, Accuracy: 49.80%\n",
      "Batch 89, Loss: 1.274105, Accuracy: 49.74%\n",
      "Batch 90, Loss: 1.218121, Accuracy: 49.77%\n",
      "Batch 91, Loss: 1.240879, Accuracy: 49.76%\n",
      "Batch 92, Loss: 1.241121, Accuracy: 49.76%\n",
      "Batch 93, Loss: 1.302996, Accuracy: 49.70%\n",
      "Batch 94, Loss: 1.255334, Accuracy: 49.63%\n",
      "Batch 95, Loss: 1.219849, Accuracy: 49.69%\n",
      "Batch 96, Loss: 1.282874, Accuracy: 49.66%\n",
      "Batch 97, Loss: 1.208200, Accuracy: 49.65%\n",
      "Batch 98, Loss: 1.143317, Accuracy: 49.73%\n",
      "Batch 99, Loss: 1.326455, Accuracy: 49.62%\n",
      "Batch 100, Loss: 1.146536, Accuracy: 49.73%\n",
      "Batch 101, Loss: 1.260903, Accuracy: 49.72%\n",
      "Batch 102, Loss: 1.231827, Accuracy: 49.69%\n",
      "Batch 103, Loss: 1.181626, Accuracy: 49.76%\n",
      "Batch 104, Loss: 1.240812, Accuracy: 49.71%\n",
      "Batch 105, Loss: 1.262222, Accuracy: 49.67%\n",
      "Batch 106, Loss: 1.256145, Accuracy: 49.68%\n",
      "Batch 107, Loss: 1.292952, Accuracy: 49.61%\n",
      "Batch 108, Loss: 1.317752, Accuracy: 49.54%\n",
      "Batch 109, Loss: 1.277909, Accuracy: 49.48%\n",
      "Batch 110, Loss: 1.360159, Accuracy: 49.35%\n",
      "Batch 111, Loss: 1.233928, Accuracy: 49.35%\n",
      "Batch 112, Loss: 1.284904, Accuracy: 49.33%\n",
      "Batch 113, Loss: 1.216334, Accuracy: 49.35%\n",
      "Batch 114, Loss: 1.200260, Accuracy: 49.37%\n",
      "Batch 115, Loss: 1.226755, Accuracy: 49.39%\n",
      "Batch 116, Loss: 1.220786, Accuracy: 49.38%\n",
      "Batch 117, Loss: 1.194646, Accuracy: 49.41%\n",
      "Batch 118, Loss: 1.172782, Accuracy: 49.50%\n",
      "Batch 119, Loss: 1.307977, Accuracy: 49.42%\n",
      "Batch 120, Loss: 1.150459, Accuracy: 49.52%\n",
      "Batch 121, Loss: 1.223845, Accuracy: 49.54%\n",
      "Batch 122, Loss: 1.204083, Accuracy: 49.55%\n",
      "Batch 123, Loss: 1.218871, Accuracy: 49.56%\n",
      "Batch 124, Loss: 1.229530, Accuracy: 49.56%\n",
      "Batch 125, Loss: 1.173628, Accuracy: 49.61%\n",
      "Batch 126, Loss: 1.302406, Accuracy: 49.57%\n",
      "Batch 127, Loss: 1.147115, Accuracy: 49.63%\n",
      "Batch 128, Loss: 1.199921, Accuracy: 49.65%\n",
      "Batch 129, Loss: 1.246440, Accuracy: 49.67%\n",
      "Batch 130, Loss: 1.186190, Accuracy: 49.70%\n",
      "Batch 131, Loss: 1.077563, Accuracy: 49.83%\n",
      "Batch 132, Loss: 1.263492, Accuracy: 49.82%\n",
      "Batch 133, Loss: 1.104581, Accuracy: 49.92%\n",
      "Batch 134, Loss: 1.140542, Accuracy: 50.00%\n",
      "Batch 135, Loss: 1.125280, Accuracy: 50.09%\n",
      "Batch 136, Loss: 1.234222, Accuracy: 50.09%\n",
      "Batch 137, Loss: 1.217768, Accuracy: 50.11%\n",
      "Batch 138, Loss: 1.221655, Accuracy: 50.11%\n",
      "Batch 139, Loss: 1.156067, Accuracy: 50.21%\n",
      "Batch 140, Loss: 1.258350, Accuracy: 50.18%\n",
      "Batch 141, Loss: 1.217576, Accuracy: 50.16%\n",
      "Batch 142, Loss: 1.206658, Accuracy: 50.18%\n",
      "Batch 143, Loss: 1.175989, Accuracy: 50.22%\n",
      "Batch 144, Loss: 1.210235, Accuracy: 50.24%\n",
      "Batch 145, Loss: 1.238737, Accuracy: 50.24%\n",
      "Batch 146, Loss: 1.308340, Accuracy: 50.20%\n",
      "Batch 147, Loss: 1.279714, Accuracy: 50.15%\n",
      "Batch 148, Loss: 1.205466, Accuracy: 50.16%\n",
      "Batch 149, Loss: 1.248792, Accuracy: 50.16%\n",
      "Batch 150, Loss: 1.243871, Accuracy: 50.15%\n",
      "Batch 151, Loss: 1.228842, Accuracy: 50.16%\n",
      "Batch 152, Loss: 1.301955, Accuracy: 50.09%\n",
      "Batch 153, Loss: 1.160247, Accuracy: 50.15%\n",
      "Batch 154, Loss: 1.264258, Accuracy: 50.12%\n",
      "Batch 155, Loss: 1.243277, Accuracy: 50.10%\n",
      "Batch 156, Loss: 1.158780, Accuracy: 50.15%\n",
      "Batch 157, Loss: 1.242127, Accuracy: 50.13%\n",
      "Batch 158, Loss: 1.178118, Accuracy: 50.17%\n",
      "Batch 159, Loss: 1.306576, Accuracy: 50.10%\n",
      "Batch 160, Loss: 1.196931, Accuracy: 50.14%\n",
      "Batch 161, Loss: 1.237716, Accuracy: 50.12%\n",
      "Batch 162, Loss: 1.272222, Accuracy: 50.09%\n",
      "Batch 163, Loss: 1.194062, Accuracy: 50.12%\n",
      "Batch 164, Loss: 1.246517, Accuracy: 50.10%\n",
      "Batch 165, Loss: 1.274650, Accuracy: 50.09%\n",
      "Batch 166, Loss: 1.201893, Accuracy: 50.11%\n",
      "Batch 167, Loss: 1.329357, Accuracy: 50.06%\n",
      "Batch 168, Loss: 1.232675, Accuracy: 50.06%\n",
      "Batch 169, Loss: 1.161163, Accuracy: 50.11%\n",
      "Batch 170, Loss: 1.214995, Accuracy: 50.12%\n",
      "Batch 171, Loss: 1.196061, Accuracy: 50.16%\n",
      "Batch 172, Loss: 1.265389, Accuracy: 50.13%\n",
      "Batch 173, Loss: 1.218952, Accuracy: 50.12%\n",
      "Batch 174, Loss: 1.165543, Accuracy: 50.14%\n",
      "Batch 175, Loss: 1.164798, Accuracy: 50.19%\n",
      "Batch 176, Loss: 1.192916, Accuracy: 50.20%\n",
      "Batch 177, Loss: 1.189766, Accuracy: 50.23%\n",
      "Batch 178, Loss: 1.176691, Accuracy: 50.27%\n",
      "Batch 179, Loss: 1.227268, Accuracy: 50.27%\n",
      "Batch 180, Loss: 1.184227, Accuracy: 50.31%\n",
      "Batch 181, Loss: 1.317156, Accuracy: 50.26%\n",
      "Batch 182, Loss: 1.124412, Accuracy: 50.33%\n",
      "Batch 183, Loss: 1.270328, Accuracy: 50.30%\n",
      "Batch 184, Loss: 1.230168, Accuracy: 50.30%\n",
      "Batch 185, Loss: 1.310673, Accuracy: 50.23%\n",
      "Batch 186, Loss: 1.200381, Accuracy: 50.25%\n",
      "Batch 187, Loss: 1.247421, Accuracy: 50.23%\n",
      "Batch 188, Loss: 1.217359, Accuracy: 50.24%\n",
      "Batch 189, Loss: 1.252591, Accuracy: 50.24%\n",
      "Batch 190, Loss: 1.176106, Accuracy: 50.25%\n",
      "Batch 191, Loss: 1.168235, Accuracy: 50.30%\n",
      "Batch 192, Loss: 1.197565, Accuracy: 50.32%\n",
      "Batch 193, Loss: 1.122485, Accuracy: 50.39%\n",
      "Batch 194, Loss: 1.251415, Accuracy: 50.36%\n",
      "Batch 195, Loss: 1.210375, Accuracy: 50.37%\n",
      "Batch 196, Loss: 1.247748, Accuracy: 50.34%\n",
      "Batch 197, Loss: 1.177786, Accuracy: 50.36%\n",
      "Batch 198, Loss: 1.299188, Accuracy: 50.31%\n",
      "Batch 199, Loss: 1.234260, Accuracy: 50.30%\n",
      "Batch 200, Loss: 1.240807, Accuracy: 50.30%\n",
      "Batch 201, Loss: 1.307748, Accuracy: 50.26%\n",
      "Batch 202, Loss: 1.221611, Accuracy: 50.27%\n",
      "Batch 203, Loss: 1.153106, Accuracy: 50.31%\n",
      "Batch 204, Loss: 1.245534, Accuracy: 50.31%\n",
      "Batch 205, Loss: 1.170671, Accuracy: 50.34%\n",
      "Batch 206, Loss: 1.267479, Accuracy: 50.33%\n",
      "Batch 207, Loss: 1.284572, Accuracy: 50.29%\n",
      "Batch 208, Loss: 1.234096, Accuracy: 50.28%\n",
      "Batch 209, Loss: 1.315470, Accuracy: 50.22%\n",
      "Batch 210, Loss: 1.233829, Accuracy: 50.21%\n",
      "Batch 211, Loss: 1.246316, Accuracy: 50.19%\n",
      "Batch 212, Loss: 1.187094, Accuracy: 50.22%\n",
      "Batch 213, Loss: 1.153138, Accuracy: 50.26%\n",
      "Training - Epoch 3, Loss: 1.227253, Accuracy: 50.26%\n",
      "Validation Batch 1, Loss: 1.349144, Accuracy: 37.50%\n",
      "Validation Batch 2, Loss: 1.336983, Accuracy: 38.28%\n",
      "Validation Batch 3, Loss: 1.418644, Accuracy: 35.42%\n",
      "Validation Batch 4, Loss: 1.322657, Accuracy: 36.72%\n",
      "Validation Batch 5, Loss: 1.405284, Accuracy: 35.62%\n",
      "Validation Batch 6, Loss: 1.462832, Accuracy: 33.59%\n",
      "Validation Batch 7, Loss: 1.373275, Accuracy: 33.71%\n",
      "Validation Batch 8, Loss: 1.364992, Accuracy: 33.98%\n",
      "Validation Batch 9, Loss: 1.392351, Accuracy: 33.85%\n",
      "Validation Batch 10, Loss: 1.418303, Accuracy: 33.44%\n",
      "Validation Batch 11, Loss: 1.361674, Accuracy: 33.66%\n",
      "Validation Batch 12, Loss: 1.357260, Accuracy: 33.85%\n",
      "Validation Batch 13, Loss: 1.405102, Accuracy: 33.65%\n",
      "Validation Batch 14, Loss: 1.283442, Accuracy: 34.49%\n",
      "Validation Batch 15, Loss: 1.347522, Accuracy: 34.69%\n",
      "Validation Batch 16, Loss: 1.440819, Accuracy: 34.18%\n",
      "Validation Batch 17, Loss: 1.376822, Accuracy: 34.19%\n",
      "Validation Batch 18, Loss: 1.376554, Accuracy: 34.20%\n",
      "Validation Batch 19, Loss: 1.379362, Accuracy: 34.21%\n",
      "Validation Batch 20, Loss: 1.393296, Accuracy: 34.14%\n",
      "Validation Batch 21, Loss: 1.360700, Accuracy: 34.23%\n",
      "Validation Batch 22, Loss: 1.380438, Accuracy: 34.23%\n",
      "Validation Batch 23, Loss: 1.376396, Accuracy: 34.24%\n",
      "Validation Batch 24, Loss: 1.426778, Accuracy: 33.98%\n",
      "Validation Batch 25, Loss: 1.330633, Accuracy: 34.19%\n",
      "Validation Batch 26, Loss: 1.303913, Accuracy: 34.50%\n",
      "Validation Batch 27, Loss: 1.383059, Accuracy: 34.47%\n",
      "Validation - Epoch 3, Loss: 1.375120, Accuracy: 34.47%\n",
      "Patienceâ€”2\n",
      "Epoch 4\n",
      "Batch 1, Loss: 1.253110, Accuracy: 45.31%\n",
      "Batch 2, Loss: 1.156198, Accuracy: 51.56%\n",
      "Batch 3, Loss: 1.271861, Accuracy: 49.48%\n",
      "Batch 4, Loss: 1.100848, Accuracy: 53.12%\n",
      "Batch 5, Loss: 1.222406, Accuracy: 53.12%\n",
      "Batch 6, Loss: 1.206635, Accuracy: 52.60%\n",
      "Batch 7, Loss: 1.220837, Accuracy: 52.01%\n",
      "Batch 8, Loss: 1.219947, Accuracy: 51.56%\n",
      "Batch 9, Loss: 1.291620, Accuracy: 50.69%\n",
      "Batch 10, Loss: 1.214123, Accuracy: 50.78%\n",
      "Batch 11, Loss: 1.194875, Accuracy: 51.42%\n",
      "Batch 12, Loss: 1.240774, Accuracy: 51.30%\n",
      "Batch 13, Loss: 1.183934, Accuracy: 51.56%\n",
      "Batch 14, Loss: 1.206303, Accuracy: 51.56%\n",
      "Batch 15, Loss: 1.245746, Accuracy: 51.25%\n",
      "Batch 16, Loss: 1.177246, Accuracy: 51.56%\n",
      "Batch 17, Loss: 1.171447, Accuracy: 51.75%\n",
      "Batch 18, Loss: 1.278806, Accuracy: 51.30%\n",
      "Batch 19, Loss: 1.302254, Accuracy: 50.66%\n",
      "Batch 20, Loss: 1.242717, Accuracy: 50.47%\n",
      "Batch 21, Loss: 1.231169, Accuracy: 50.52%\n",
      "Batch 22, Loss: 1.220876, Accuracy: 50.57%\n",
      "Batch 23, Loss: 1.162210, Accuracy: 50.95%\n",
      "Batch 24, Loss: 1.158644, Accuracy: 51.24%\n",
      "Batch 25, Loss: 1.257452, Accuracy: 51.19%\n",
      "Batch 26, Loss: 1.297501, Accuracy: 50.78%\n",
      "Batch 27, Loss: 1.237314, Accuracy: 50.69%\n",
      "Batch 28, Loss: 1.248021, Accuracy: 50.67%\n",
      "Batch 29, Loss: 1.190641, Accuracy: 50.86%\n",
      "Batch 30, Loss: 1.194558, Accuracy: 50.99%\n",
      "Batch 31, Loss: 1.215374, Accuracy: 50.91%\n",
      "Batch 32, Loss: 1.153382, Accuracy: 51.17%\n",
      "Batch 33, Loss: 1.152155, Accuracy: 51.33%\n",
      "Batch 34, Loss: 1.237406, Accuracy: 51.33%\n",
      "Batch 35, Loss: 1.221852, Accuracy: 51.38%\n",
      "Batch 36, Loss: 1.161382, Accuracy: 51.56%\n",
      "Batch 37, Loss: 1.146758, Accuracy: 51.82%\n",
      "Batch 38, Loss: 1.285414, Accuracy: 51.56%\n",
      "Batch 39, Loss: 1.221940, Accuracy: 51.64%\n",
      "Batch 40, Loss: 1.253767, Accuracy: 51.56%\n",
      "Batch 41, Loss: 1.237504, Accuracy: 51.52%\n",
      "Batch 42, Loss: 1.202705, Accuracy: 51.49%\n",
      "Batch 43, Loss: 1.270393, Accuracy: 51.38%\n",
      "Batch 44, Loss: 1.152925, Accuracy: 51.53%\n",
      "Batch 45, Loss: 1.236811, Accuracy: 51.42%\n",
      "Batch 46, Loss: 1.090367, Accuracy: 51.80%\n",
      "Batch 47, Loss: 1.218570, Accuracy: 51.80%\n",
      "Batch 48, Loss: 1.163511, Accuracy: 51.92%\n",
      "Batch 49, Loss: 1.203037, Accuracy: 51.98%\n",
      "Batch 50, Loss: 1.114106, Accuracy: 52.25%\n",
      "Batch 51, Loss: 1.212943, Accuracy: 52.24%\n",
      "Batch 52, Loss: 1.164298, Accuracy: 52.31%\n",
      "Batch 53, Loss: 1.237257, Accuracy: 52.30%\n",
      "Batch 54, Loss: 1.240768, Accuracy: 52.20%\n",
      "Batch 55, Loss: 1.286489, Accuracy: 52.13%\n",
      "Batch 56, Loss: 1.158109, Accuracy: 52.26%\n",
      "Batch 57, Loss: 1.233380, Accuracy: 52.19%\n",
      "Batch 58, Loss: 1.193731, Accuracy: 52.21%\n",
      "Batch 59, Loss: 1.273598, Accuracy: 52.04%\n",
      "Batch 60, Loss: 1.250583, Accuracy: 51.98%\n",
      "Batch 61, Loss: 1.275737, Accuracy: 51.87%\n",
      "Batch 62, Loss: 1.134076, Accuracy: 52.04%\n",
      "Batch 63, Loss: 1.222571, Accuracy: 52.01%\n",
      "Batch 64, Loss: 1.260894, Accuracy: 51.95%\n",
      "Batch 65, Loss: 1.138577, Accuracy: 52.07%\n",
      "Batch 66, Loss: 1.185716, Accuracy: 52.13%\n",
      "Batch 67, Loss: 1.198877, Accuracy: 52.10%\n",
      "Batch 68, Loss: 1.268907, Accuracy: 51.95%\n",
      "Batch 69, Loss: 1.281204, Accuracy: 51.83%\n",
      "Batch 70, Loss: 1.268040, Accuracy: 51.72%\n",
      "Batch 71, Loss: 1.163695, Accuracy: 51.85%\n",
      "Batch 72, Loss: 1.179630, Accuracy: 51.95%\n",
      "Batch 73, Loss: 1.188293, Accuracy: 51.97%\n",
      "Batch 74, Loss: 1.253263, Accuracy: 51.90%\n",
      "Batch 75, Loss: 1.179772, Accuracy: 51.94%\n",
      "Batch 76, Loss: 1.263321, Accuracy: 51.87%\n",
      "Batch 77, Loss: 1.207764, Accuracy: 51.85%\n",
      "Batch 78, Loss: 1.277451, Accuracy: 51.74%\n",
      "Batch 79, Loss: 1.145877, Accuracy: 51.84%\n",
      "Batch 80, Loss: 1.225294, Accuracy: 51.84%\n",
      "Batch 81, Loss: 1.223523, Accuracy: 51.87%\n",
      "Batch 82, Loss: 1.288146, Accuracy: 51.75%\n",
      "Batch 83, Loss: 1.145606, Accuracy: 51.84%\n",
      "Batch 84, Loss: 1.289533, Accuracy: 51.73%\n",
      "Batch 85, Loss: 1.158231, Accuracy: 51.84%\n",
      "Batch 86, Loss: 1.234321, Accuracy: 51.82%\n",
      "Batch 87, Loss: 1.191725, Accuracy: 51.83%\n",
      "Batch 88, Loss: 1.179546, Accuracy: 51.88%\n",
      "Batch 89, Loss: 1.237726, Accuracy: 51.86%\n",
      "Batch 90, Loss: 1.127903, Accuracy: 51.93%\n",
      "Batch 91, Loss: 1.246080, Accuracy: 51.89%\n",
      "Batch 92, Loss: 1.232566, Accuracy: 51.87%\n",
      "Batch 93, Loss: 1.294043, Accuracy: 51.80%\n",
      "Batch 94, Loss: 1.220290, Accuracy: 51.78%\n",
      "Batch 95, Loss: 1.307618, Accuracy: 51.68%\n",
      "Batch 96, Loss: 1.265683, Accuracy: 51.61%\n",
      "Batch 97, Loss: 1.154890, Accuracy: 51.68%\n",
      "Batch 98, Loss: 1.194921, Accuracy: 51.74%\n",
      "Batch 99, Loss: 1.225362, Accuracy: 51.69%\n",
      "Batch 100, Loss: 1.230085, Accuracy: 51.66%\n",
      "Batch 101, Loss: 1.229562, Accuracy: 51.67%\n",
      "Batch 102, Loss: 1.154088, Accuracy: 51.76%\n",
      "Batch 103, Loss: 1.135112, Accuracy: 51.84%\n",
      "Batch 104, Loss: 1.328141, Accuracy: 51.71%\n",
      "Batch 105, Loss: 1.184124, Accuracy: 51.74%\n",
      "Batch 106, Loss: 1.274400, Accuracy: 51.68%\n",
      "Batch 107, Loss: 1.196442, Accuracy: 51.71%\n",
      "Batch 108, Loss: 1.315071, Accuracy: 51.61%\n",
      "Batch 109, Loss: 1.111916, Accuracy: 51.72%\n",
      "Batch 110, Loss: 1.173946, Accuracy: 51.76%\n",
      "Batch 111, Loss: 1.202854, Accuracy: 51.76%\n",
      "Batch 112, Loss: 1.249599, Accuracy: 51.70%\n",
      "Batch 113, Loss: 1.232712, Accuracy: 51.69%\n",
      "Batch 114, Loss: 1.190465, Accuracy: 51.70%\n",
      "Batch 115, Loss: 1.247257, Accuracy: 51.66%\n",
      "Batch 116, Loss: 1.171088, Accuracy: 51.71%\n",
      "Batch 117, Loss: 1.247513, Accuracy: 51.67%\n",
      "Batch 118, Loss: 1.169517, Accuracy: 51.73%\n",
      "Batch 119, Loss: 1.169413, Accuracy: 51.77%\n",
      "Batch 120, Loss: 1.193213, Accuracy: 51.78%\n",
      "Batch 121, Loss: 1.232566, Accuracy: 51.76%\n",
      "Batch 122, Loss: 1.272232, Accuracy: 51.72%\n",
      "Batch 123, Loss: 1.252262, Accuracy: 51.66%\n",
      "Batch 124, Loss: 1.206501, Accuracy: 51.68%\n",
      "Batch 125, Loss: 1.207787, Accuracy: 51.67%\n",
      "Batch 126, Loss: 1.198513, Accuracy: 51.69%\n",
      "Batch 127, Loss: 1.201447, Accuracy: 51.71%\n",
      "Batch 128, Loss: 1.148315, Accuracy: 51.79%\n",
      "Batch 129, Loss: 1.155205, Accuracy: 51.87%\n",
      "Batch 130, Loss: 1.218715, Accuracy: 51.84%\n",
      "Batch 131, Loss: 1.180137, Accuracy: 51.86%\n",
      "Batch 132, Loss: 1.201736, Accuracy: 51.88%\n",
      "Batch 133, Loss: 1.231216, Accuracy: 51.87%\n",
      "Batch 134, Loss: 1.306147, Accuracy: 51.77%\n",
      "Batch 135, Loss: 1.227919, Accuracy: 51.77%\n",
      "Batch 136, Loss: 1.268083, Accuracy: 51.73%\n",
      "Batch 137, Loss: 1.269465, Accuracy: 51.69%\n",
      "Batch 138, Loss: 1.301859, Accuracy: 51.61%\n",
      "Batch 139, Loss: 1.177532, Accuracy: 51.64%\n",
      "Batch 140, Loss: 1.198599, Accuracy: 51.66%\n",
      "Batch 141, Loss: 1.209178, Accuracy: 51.65%\n",
      "Batch 142, Loss: 1.181072, Accuracy: 51.68%\n",
      "Batch 143, Loss: 1.155980, Accuracy: 51.72%\n",
      "Batch 144, Loss: 1.308093, Accuracy: 51.66%\n",
      "Batch 145, Loss: 1.188596, Accuracy: 51.68%\n",
      "Batch 146, Loss: 1.234583, Accuracy: 51.65%\n",
      "Batch 147, Loss: 1.186434, Accuracy: 51.70%\n",
      "Batch 148, Loss: 1.249157, Accuracy: 51.66%\n",
      "Batch 149, Loss: 1.284531, Accuracy: 51.59%\n",
      "Batch 150, Loss: 1.184863, Accuracy: 51.61%\n",
      "Batch 151, Loss: 1.311343, Accuracy: 51.54%\n",
      "Batch 152, Loss: 1.246894, Accuracy: 51.53%\n",
      "Batch 153, Loss: 1.159611, Accuracy: 51.56%\n",
      "Batch 154, Loss: 1.214619, Accuracy: 51.56%\n",
      "Batch 155, Loss: 1.201307, Accuracy: 51.57%\n",
      "Batch 156, Loss: 1.155356, Accuracy: 51.63%\n",
      "Batch 157, Loss: 1.285819, Accuracy: 51.55%\n",
      "Batch 158, Loss: 1.231133, Accuracy: 51.55%\n",
      "Batch 159, Loss: 1.245424, Accuracy: 51.53%\n",
      "Batch 160, Loss: 1.193568, Accuracy: 51.53%\n",
      "Batch 161, Loss: 1.308304, Accuracy: 51.49%\n",
      "Batch 162, Loss: 1.272276, Accuracy: 51.43%\n",
      "Batch 163, Loss: 1.201368, Accuracy: 51.44%\n",
      "Batch 164, Loss: 1.264341, Accuracy: 51.40%\n",
      "Batch 165, Loss: 1.324233, Accuracy: 51.32%\n",
      "Batch 166, Loss: 1.154594, Accuracy: 51.36%\n",
      "Batch 167, Loss: 1.308102, Accuracy: 51.31%\n",
      "Batch 168, Loss: 1.206208, Accuracy: 51.33%\n",
      "Batch 169, Loss: 1.200979, Accuracy: 51.34%\n",
      "Batch 170, Loss: 1.170588, Accuracy: 51.37%\n",
      "Batch 171, Loss: 1.122770, Accuracy: 51.44%\n",
      "Batch 172, Loss: 1.178350, Accuracy: 51.48%\n",
      "Batch 173, Loss: 1.226393, Accuracy: 51.47%\n",
      "Batch 174, Loss: 1.186426, Accuracy: 51.48%\n",
      "Batch 175, Loss: 1.208860, Accuracy: 51.46%\n",
      "Batch 176, Loss: 1.175399, Accuracy: 51.51%\n",
      "Batch 177, Loss: 1.290357, Accuracy: 51.46%\n",
      "Batch 178, Loss: 1.172624, Accuracy: 51.48%\n",
      "Batch 179, Loss: 1.265473, Accuracy: 51.48%\n",
      "Batch 180, Loss: 1.195410, Accuracy: 51.49%\n",
      "Batch 181, Loss: 1.290549, Accuracy: 51.45%\n",
      "Batch 182, Loss: 1.192645, Accuracy: 51.49%\n",
      "Batch 183, Loss: 1.185926, Accuracy: 51.50%\n",
      "Batch 184, Loss: 1.255744, Accuracy: 51.47%\n",
      "Batch 185, Loss: 1.212420, Accuracy: 51.48%\n",
      "Batch 186, Loss: 1.207392, Accuracy: 51.49%\n",
      "Batch 187, Loss: 1.204253, Accuracy: 51.50%\n",
      "Batch 188, Loss: 1.209719, Accuracy: 51.50%\n",
      "Batch 189, Loss: 1.200780, Accuracy: 51.51%\n",
      "Batch 190, Loss: 1.263828, Accuracy: 51.49%\n",
      "Batch 191, Loss: 1.106977, Accuracy: 51.55%\n",
      "Batch 192, Loss: 1.363122, Accuracy: 51.46%\n",
      "Batch 193, Loss: 1.211096, Accuracy: 51.46%\n",
      "Batch 194, Loss: 1.145186, Accuracy: 51.50%\n",
      "Batch 195, Loss: 1.222319, Accuracy: 51.47%\n",
      "Batch 196, Loss: 1.261895, Accuracy: 51.46%\n",
      "Batch 197, Loss: 1.198582, Accuracy: 51.48%\n",
      "Batch 198, Loss: 1.190605, Accuracy: 51.49%\n",
      "Batch 199, Loss: 1.238979, Accuracy: 51.48%\n",
      "Batch 200, Loss: 1.282340, Accuracy: 51.42%\n",
      "Batch 201, Loss: 1.209658, Accuracy: 51.45%\n",
      "Batch 202, Loss: 1.281433, Accuracy: 51.40%\n",
      "Batch 203, Loss: 1.252375, Accuracy: 51.37%\n",
      "Batch 204, Loss: 1.205819, Accuracy: 51.39%\n",
      "Batch 205, Loss: 1.211714, Accuracy: 51.39%\n",
      "Batch 206, Loss: 1.243615, Accuracy: 51.37%\n",
      "Batch 207, Loss: 1.168384, Accuracy: 51.42%\n",
      "Batch 208, Loss: 1.275900, Accuracy: 51.40%\n",
      "Batch 209, Loss: 1.294061, Accuracy: 51.35%\n",
      "Batch 210, Loss: 1.234939, Accuracy: 51.35%\n",
      "Batch 211, Loss: 1.308993, Accuracy: 51.30%\n",
      "Batch 212, Loss: 1.213636, Accuracy: 51.28%\n",
      "Batch 213, Loss: 1.179151, Accuracy: 51.29%\n",
      "Training - Epoch 4, Loss: 1.218660, Accuracy: 51.29%\n",
      "Validation Batch 1, Loss: 1.349745, Accuracy: 37.50%\n",
      "Validation Batch 2, Loss: 1.338061, Accuracy: 38.28%\n",
      "Validation Batch 3, Loss: 1.421374, Accuracy: 35.42%\n",
      "Validation Batch 4, Loss: 1.323524, Accuracy: 36.72%\n",
      "Validation Batch 5, Loss: 1.407062, Accuracy: 35.62%\n",
      "Validation Batch 6, Loss: 1.466648, Accuracy: 33.59%\n",
      "Validation Batch 7, Loss: 1.375330, Accuracy: 33.71%\n",
      "Validation Batch 8, Loss: 1.365626, Accuracy: 33.98%\n",
      "Validation Batch 9, Loss: 1.394633, Accuracy: 33.85%\n",
      "Validation Batch 10, Loss: 1.420648, Accuracy: 33.44%\n",
      "Validation Batch 11, Loss: 1.363566, Accuracy: 33.66%\n",
      "Validation Batch 12, Loss: 1.359980, Accuracy: 33.85%\n",
      "Validation Batch 13, Loss: 1.407008, Accuracy: 33.65%\n",
      "Validation Batch 14, Loss: 1.283074, Accuracy: 34.49%\n",
      "Validation Batch 15, Loss: 1.348837, Accuracy: 34.69%\n",
      "Validation Batch 16, Loss: 1.443896, Accuracy: 34.18%\n",
      "Validation Batch 17, Loss: 1.379163, Accuracy: 34.19%\n",
      "Validation Batch 18, Loss: 1.378672, Accuracy: 34.20%\n",
      "Validation Batch 19, Loss: 1.381148, Accuracy: 34.21%\n",
      "Validation Batch 20, Loss: 1.395914, Accuracy: 34.14%\n",
      "Validation Batch 21, Loss: 1.361008, Accuracy: 34.23%\n",
      "Validation Batch 22, Loss: 1.381898, Accuracy: 34.23%\n",
      "Validation Batch 23, Loss: 1.378487, Accuracy: 34.24%\n",
      "Validation Batch 24, Loss: 1.429947, Accuracy: 33.98%\n",
      "Validation Batch 25, Loss: 1.331337, Accuracy: 34.19%\n",
      "Validation Batch 26, Loss: 1.304886, Accuracy: 34.50%\n",
      "Validation Batch 27, Loss: 1.385091, Accuracy: 34.47%\n",
      "Validation - Epoch 4, Loss: 1.376910, Accuracy: 34.47%\n",
      "Patienceâ€”3\n",
      "Epoch 5\n",
      "Batch 1, Loss: 1.305402, Accuracy: 37.50%\n",
      "Batch 2, Loss: 1.130731, Accuracy: 49.22%\n",
      "Batch 3, Loss: 1.196170, Accuracy: 50.52%\n",
      "Batch 4, Loss: 1.214367, Accuracy: 50.78%\n",
      "Batch 5, Loss: 1.146872, Accuracy: 52.81%\n",
      "Batch 6, Loss: 1.300544, Accuracy: 51.30%\n",
      "Batch 7, Loss: 1.220096, Accuracy: 52.01%\n",
      "Batch 8, Loss: 1.181144, Accuracy: 51.95%\n",
      "Batch 9, Loss: 1.263958, Accuracy: 50.87%\n",
      "Batch 10, Loss: 1.307373, Accuracy: 50.16%\n",
      "Batch 11, Loss: 1.169882, Accuracy: 50.71%\n",
      "Batch 12, Loss: 1.164541, Accuracy: 51.30%\n",
      "Batch 13, Loss: 1.138489, Accuracy: 52.28%\n",
      "Batch 14, Loss: 1.273595, Accuracy: 51.67%\n",
      "Batch 15, Loss: 1.178644, Accuracy: 51.77%\n",
      "Batch 16, Loss: 1.234720, Accuracy: 51.56%\n",
      "Batch 17, Loss: 1.268936, Accuracy: 51.29%\n",
      "Batch 18, Loss: 1.208535, Accuracy: 51.39%\n",
      "Batch 19, Loss: 1.127978, Accuracy: 51.89%\n",
      "Batch 20, Loss: 1.212931, Accuracy: 51.72%\n",
      "Batch 21, Loss: 1.165056, Accuracy: 52.08%\n",
      "Batch 22, Loss: 1.225155, Accuracy: 51.92%\n",
      "Batch 23, Loss: 1.252043, Accuracy: 51.77%\n",
      "Batch 24, Loss: 1.202883, Accuracy: 51.69%\n",
      "Batch 25, Loss: 1.258585, Accuracy: 51.50%\n",
      "Batch 26, Loss: 1.283391, Accuracy: 51.26%\n",
      "Batch 27, Loss: 1.213654, Accuracy: 51.33%\n",
      "Batch 28, Loss: 1.159364, Accuracy: 51.51%\n",
      "Batch 29, Loss: 1.212806, Accuracy: 51.62%\n",
      "Batch 30, Loss: 1.202654, Accuracy: 51.56%\n",
      "Batch 31, Loss: 1.204538, Accuracy: 51.61%\n",
      "Batch 32, Loss: 1.282439, Accuracy: 51.42%\n",
      "Batch 33, Loss: 1.209491, Accuracy: 51.47%\n",
      "Batch 34, Loss: 1.194401, Accuracy: 51.61%\n",
      "Batch 35, Loss: 1.210455, Accuracy: 51.70%\n",
      "Batch 36, Loss: 1.238434, Accuracy: 51.61%\n",
      "Batch 37, Loss: 1.210339, Accuracy: 51.65%\n",
      "Batch 38, Loss: 1.170281, Accuracy: 51.77%\n",
      "Batch 39, Loss: 1.176776, Accuracy: 51.88%\n",
      "Batch 40, Loss: 1.243574, Accuracy: 51.84%\n",
      "Batch 41, Loss: 1.254534, Accuracy: 51.71%\n",
      "Batch 42, Loss: 1.212733, Accuracy: 51.71%\n",
      "Batch 43, Loss: 1.242355, Accuracy: 51.67%\n",
      "Batch 44, Loss: 1.272863, Accuracy: 51.49%\n",
      "Batch 45, Loss: 1.227377, Accuracy: 51.46%\n",
      "Batch 46, Loss: 1.208527, Accuracy: 51.46%\n",
      "Batch 47, Loss: 1.172825, Accuracy: 51.63%\n",
      "Batch 48, Loss: 1.196077, Accuracy: 51.63%\n",
      "Batch 49, Loss: 1.244731, Accuracy: 51.56%\n",
      "Batch 50, Loss: 1.229560, Accuracy: 51.56%\n",
      "Batch 51, Loss: 1.147700, Accuracy: 51.69%\n",
      "Batch 52, Loss: 1.238772, Accuracy: 51.62%\n",
      "Batch 53, Loss: 1.304409, Accuracy: 51.44%\n",
      "Batch 54, Loss: 1.174169, Accuracy: 51.50%\n",
      "Batch 55, Loss: 1.155073, Accuracy: 51.62%\n",
      "Batch 56, Loss: 1.166526, Accuracy: 51.73%\n",
      "Batch 57, Loss: 1.218364, Accuracy: 51.75%\n",
      "Batch 58, Loss: 1.267260, Accuracy: 51.70%\n",
      "Batch 59, Loss: 1.168664, Accuracy: 51.80%\n",
      "Batch 60, Loss: 1.248353, Accuracy: 51.69%\n",
      "Batch 61, Loss: 1.209828, Accuracy: 51.69%\n",
      "Batch 62, Loss: 1.245337, Accuracy: 51.66%\n",
      "Batch 63, Loss: 1.203181, Accuracy: 51.64%\n",
      "Batch 64, Loss: 1.226795, Accuracy: 51.56%\n",
      "Batch 65, Loss: 1.231810, Accuracy: 51.56%\n",
      "Batch 66, Loss: 1.169470, Accuracy: 51.66%\n",
      "Batch 67, Loss: 1.178570, Accuracy: 51.73%\n",
      "Batch 68, Loss: 1.245421, Accuracy: 51.65%\n",
      "Batch 69, Loss: 1.217880, Accuracy: 51.63%\n",
      "Batch 70, Loss: 1.220944, Accuracy: 51.56%\n",
      "Batch 71, Loss: 1.133271, Accuracy: 51.63%\n",
      "Batch 72, Loss: 1.279573, Accuracy: 51.50%\n",
      "Batch 73, Loss: 1.174668, Accuracy: 51.52%\n",
      "Batch 74, Loss: 1.159593, Accuracy: 51.60%\n",
      "Batch 75, Loss: 1.193069, Accuracy: 51.67%\n",
      "Batch 76, Loss: 1.265806, Accuracy: 51.56%\n",
      "Batch 77, Loss: 1.235911, Accuracy: 51.52%\n",
      "Batch 78, Loss: 1.235545, Accuracy: 51.48%\n",
      "Batch 79, Loss: 1.160789, Accuracy: 51.54%\n",
      "Batch 80, Loss: 1.144213, Accuracy: 51.62%\n",
      "Batch 81, Loss: 1.223055, Accuracy: 51.64%\n",
      "Batch 82, Loss: 1.203516, Accuracy: 51.62%\n",
      "Batch 83, Loss: 1.298685, Accuracy: 51.51%\n",
      "Batch 84, Loss: 1.200685, Accuracy: 51.51%\n",
      "Batch 85, Loss: 1.247106, Accuracy: 51.45%\n",
      "Batch 86, Loss: 1.164178, Accuracy: 51.53%\n",
      "Batch 87, Loss: 1.266978, Accuracy: 51.45%\n",
      "Batch 88, Loss: 1.244611, Accuracy: 51.42%\n",
      "Batch 89, Loss: 1.215268, Accuracy: 51.44%\n",
      "Batch 90, Loss: 1.229271, Accuracy: 51.42%\n",
      "Batch 91, Loss: 1.209132, Accuracy: 51.43%\n",
      "Batch 92, Loss: 1.157091, Accuracy: 51.49%\n",
      "Batch 93, Loss: 1.192483, Accuracy: 51.53%\n",
      "Batch 94, Loss: 1.147983, Accuracy: 51.66%\n",
      "Batch 95, Loss: 1.178770, Accuracy: 51.69%\n",
      "Batch 96, Loss: 1.188230, Accuracy: 51.71%\n",
      "Batch 97, Loss: 1.133871, Accuracy: 51.84%\n",
      "Batch 98, Loss: 1.200626, Accuracy: 51.83%\n",
      "Batch 99, Loss: 1.129736, Accuracy: 51.94%\n",
      "Batch 100, Loss: 1.140362, Accuracy: 52.02%\n",
      "Batch 101, Loss: 1.242484, Accuracy: 51.98%\n",
      "Batch 102, Loss: 1.220383, Accuracy: 51.96%\n",
      "Batch 103, Loss: 1.294284, Accuracy: 51.87%\n",
      "Batch 104, Loss: 1.183425, Accuracy: 51.91%\n",
      "Batch 105, Loss: 1.180601, Accuracy: 51.96%\n",
      "Batch 106, Loss: 1.153204, Accuracy: 52.02%\n",
      "Batch 107, Loss: 1.266329, Accuracy: 51.94%\n",
      "Batch 108, Loss: 1.132674, Accuracy: 52.01%\n",
      "Batch 109, Loss: 1.314346, Accuracy: 51.91%\n",
      "Batch 110, Loss: 1.209172, Accuracy: 51.90%\n",
      "Batch 111, Loss: 1.200034, Accuracy: 51.91%\n",
      "Batch 112, Loss: 1.131161, Accuracy: 51.99%\n",
      "Batch 113, Loss: 1.224892, Accuracy: 51.99%\n",
      "Batch 114, Loss: 1.187954, Accuracy: 51.99%\n",
      "Batch 115, Loss: 1.137640, Accuracy: 52.04%\n",
      "Batch 116, Loss: 1.178893, Accuracy: 52.06%\n",
      "Batch 117, Loss: 1.265560, Accuracy: 52.02%\n",
      "Batch 118, Loss: 1.202112, Accuracy: 52.05%\n",
      "Batch 119, Loss: 1.160339, Accuracy: 52.10%\n",
      "Batch 120, Loss: 1.169171, Accuracy: 52.12%\n",
      "Batch 121, Loss: 1.164213, Accuracy: 52.18%\n",
      "Batch 122, Loss: 1.228647, Accuracy: 52.15%\n",
      "Batch 123, Loss: 1.188022, Accuracy: 52.18%\n",
      "Batch 124, Loss: 1.174858, Accuracy: 52.21%\n",
      "Batch 125, Loss: 1.165203, Accuracy: 52.27%\n",
      "Batch 126, Loss: 1.277623, Accuracy: 52.23%\n",
      "Batch 127, Loss: 1.195424, Accuracy: 52.26%\n",
      "Batch 128, Loss: 1.190797, Accuracy: 52.28%\n",
      "Batch 129, Loss: 1.216328, Accuracy: 52.27%\n",
      "Batch 130, Loss: 1.146570, Accuracy: 52.32%\n",
      "Batch 131, Loss: 1.152194, Accuracy: 52.37%\n",
      "Batch 132, Loss: 1.238103, Accuracy: 52.38%\n",
      "Batch 133, Loss: 1.231509, Accuracy: 52.36%\n",
      "Batch 134, Loss: 1.177863, Accuracy: 52.40%\n",
      "Batch 135, Loss: 1.264096, Accuracy: 52.35%\n",
      "Batch 136, Loss: 1.158170, Accuracy: 52.40%\n",
      "Batch 137, Loss: 1.149881, Accuracy: 52.47%\n",
      "Batch 138, Loss: 1.223704, Accuracy: 52.46%\n",
      "Batch 139, Loss: 1.205499, Accuracy: 52.46%\n",
      "Batch 140, Loss: 1.275990, Accuracy: 52.39%\n",
      "Batch 141, Loss: 1.199718, Accuracy: 52.37%\n",
      "Batch 142, Loss: 1.194381, Accuracy: 52.38%\n",
      "Batch 143, Loss: 1.324358, Accuracy: 52.31%\n",
      "Batch 144, Loss: 1.171225, Accuracy: 52.33%\n",
      "Batch 145, Loss: 1.290695, Accuracy: 52.23%\n",
      "Batch 146, Loss: 1.181591, Accuracy: 52.28%\n",
      "Batch 147, Loss: 1.213401, Accuracy: 52.27%\n",
      "Batch 148, Loss: 1.298783, Accuracy: 52.21%\n",
      "Batch 149, Loss: 1.211962, Accuracy: 52.20%\n",
      "Batch 150, Loss: 1.157760, Accuracy: 52.23%\n",
      "Batch 151, Loss: 1.150038, Accuracy: 52.29%\n",
      "Batch 152, Loss: 1.142076, Accuracy: 52.34%\n",
      "Batch 153, Loss: 1.167326, Accuracy: 52.38%\n",
      "Batch 154, Loss: 1.192014, Accuracy: 52.37%\n",
      "Batch 155, Loss: 1.283271, Accuracy: 52.31%\n",
      "Batch 156, Loss: 1.216791, Accuracy: 52.32%\n",
      "Batch 157, Loss: 1.172908, Accuracy: 52.37%\n",
      "Batch 158, Loss: 1.242690, Accuracy: 52.35%\n",
      "Batch 159, Loss: 1.225498, Accuracy: 52.35%\n",
      "Batch 160, Loss: 1.193461, Accuracy: 52.35%\n",
      "Batch 161, Loss: 1.180183, Accuracy: 52.36%\n",
      "Batch 162, Loss: 1.198397, Accuracy: 52.38%\n",
      "Batch 163, Loss: 1.219193, Accuracy: 52.37%\n",
      "Batch 164, Loss: 1.202374, Accuracy: 52.38%\n",
      "Batch 165, Loss: 1.162598, Accuracy: 52.39%\n",
      "Batch 166, Loss: 1.135134, Accuracy: 52.46%\n",
      "Batch 167, Loss: 1.300230, Accuracy: 52.40%\n",
      "Batch 168, Loss: 1.138412, Accuracy: 52.49%\n",
      "Batch 169, Loss: 1.201546, Accuracy: 52.51%\n",
      "Batch 170, Loss: 1.151947, Accuracy: 52.56%\n",
      "Batch 171, Loss: 1.166564, Accuracy: 52.61%\n",
      "Batch 172, Loss: 1.138039, Accuracy: 52.68%\n",
      "Batch 173, Loss: 1.175569, Accuracy: 52.73%\n",
      "Batch 174, Loss: 1.160672, Accuracy: 52.77%\n",
      "Batch 175, Loss: 1.170371, Accuracy: 52.77%\n",
      "Batch 176, Loss: 1.180046, Accuracy: 52.77%\n",
      "Batch 177, Loss: 1.272748, Accuracy: 52.73%\n",
      "Batch 178, Loss: 1.249274, Accuracy: 52.70%\n",
      "Batch 179, Loss: 1.119324, Accuracy: 52.77%\n",
      "Batch 180, Loss: 1.155840, Accuracy: 52.79%\n",
      "Batch 181, Loss: 1.252401, Accuracy: 52.74%\n",
      "Batch 182, Loss: 1.195971, Accuracy: 52.75%\n",
      "Batch 183, Loss: 1.232823, Accuracy: 52.72%\n",
      "Batch 184, Loss: 1.150604, Accuracy: 52.74%\n",
      "Batch 185, Loss: 1.224732, Accuracy: 52.72%\n",
      "Batch 186, Loss: 1.221817, Accuracy: 52.70%\n",
      "Batch 187, Loss: 1.205158, Accuracy: 52.67%\n",
      "Batch 188, Loss: 1.204083, Accuracy: 52.68%\n",
      "Batch 189, Loss: 1.226946, Accuracy: 52.69%\n",
      "Batch 190, Loss: 1.190778, Accuracy: 52.69%\n",
      "Batch 191, Loss: 1.092449, Accuracy: 52.77%\n",
      "Batch 192, Loss: 1.139835, Accuracy: 52.82%\n",
      "Batch 193, Loss: 1.256304, Accuracy: 52.79%\n",
      "Batch 194, Loss: 1.105933, Accuracy: 52.85%\n",
      "Batch 195, Loss: 1.168107, Accuracy: 52.87%\n",
      "Batch 196, Loss: 1.299162, Accuracy: 52.82%\n",
      "Batch 197, Loss: 1.185963, Accuracy: 52.83%\n",
      "Batch 198, Loss: 1.205027, Accuracy: 52.83%\n",
      "Batch 199, Loss: 1.230532, Accuracy: 52.80%\n",
      "Batch 200, Loss: 1.171235, Accuracy: 52.80%\n",
      "Batch 201, Loss: 1.245609, Accuracy: 52.77%\n",
      "Batch 202, Loss: 1.211338, Accuracy: 52.76%\n",
      "Batch 203, Loss: 1.105219, Accuracy: 52.81%\n",
      "Batch 204, Loss: 1.181688, Accuracy: 52.80%\n",
      "Batch 205, Loss: 1.140947, Accuracy: 52.84%\n",
      "Batch 206, Loss: 1.210881, Accuracy: 52.85%\n",
      "Batch 207, Loss: 1.180455, Accuracy: 52.87%\n",
      "Batch 208, Loss: 1.031914, Accuracy: 52.97%\n",
      "Batch 209, Loss: 1.267783, Accuracy: 52.92%\n",
      "Batch 210, Loss: 1.187760, Accuracy: 52.92%\n",
      "Batch 211, Loss: 1.157915, Accuracy: 52.95%\n",
      "Batch 212, Loss: 1.215020, Accuracy: 52.97%\n",
      "Batch 213, Loss: 1.269452, Accuracy: 52.94%\n",
      "Training - Epoch 5, Loss: 1.203610, Accuracy: 52.94%\n",
      "Validation Batch 1, Loss: 1.350454, Accuracy: 37.50%\n",
      "Validation Batch 2, Loss: 1.339666, Accuracy: 38.28%\n",
      "Validation Batch 3, Loss: 1.425934, Accuracy: 35.42%\n",
      "Validation Batch 4, Loss: 1.324040, Accuracy: 36.72%\n",
      "Validation Batch 5, Loss: 1.411178, Accuracy: 35.62%\n",
      "Validation Batch 6, Loss: 1.473816, Accuracy: 33.59%\n",
      "Validation Batch 7, Loss: 1.377872, Accuracy: 33.71%\n",
      "Validation Batch 8, Loss: 1.368371, Accuracy: 33.98%\n",
      "Validation Batch 9, Loss: 1.396783, Accuracy: 33.85%\n",
      "Validation Batch 10, Loss: 1.423834, Accuracy: 33.44%\n",
      "Validation Batch 11, Loss: 1.365821, Accuracy: 33.66%\n",
      "Validation Batch 12, Loss: 1.362205, Accuracy: 33.85%\n",
      "Validation Batch 13, Loss: 1.410331, Accuracy: 33.65%\n",
      "Validation Batch 14, Loss: 1.280866, Accuracy: 34.49%\n",
      "Validation Batch 15, Loss: 1.350488, Accuracy: 34.69%\n",
      "Validation Batch 16, Loss: 1.450101, Accuracy: 34.18%\n",
      "Validation Batch 17, Loss: 1.383202, Accuracy: 34.19%\n",
      "Validation Batch 18, Loss: 1.381091, Accuracy: 34.20%\n",
      "Validation Batch 19, Loss: 1.384065, Accuracy: 34.21%\n",
      "Validation Batch 20, Loss: 1.397658, Accuracy: 34.14%\n",
      "Validation Batch 21, Loss: 1.363665, Accuracy: 34.23%\n",
      "Validation Batch 22, Loss: 1.384337, Accuracy: 34.23%\n",
      "Validation Batch 23, Loss: 1.381547, Accuracy: 34.24%\n",
      "Validation Batch 24, Loss: 1.435419, Accuracy: 33.98%\n",
      "Validation Batch 25, Loss: 1.332646, Accuracy: 34.19%\n",
      "Validation Batch 26, Loss: 1.304788, Accuracy: 34.50%\n",
      "Validation Batch 27, Loss: 1.387970, Accuracy: 34.47%\n",
      "Validation - Epoch 5, Loss: 1.379561, Accuracy: 34.47%\n",
      "Patienceâ€”4\n",
      "Epoch 6\n",
      "Batch 1, Loss: 1.080410, Accuracy: 70.31%\n",
      "Batch 2, Loss: 1.135521, Accuracy: 64.84%\n",
      "Batch 3, Loss: 1.152533, Accuracy: 61.98%\n",
      "Batch 4, Loss: 1.195912, Accuracy: 59.38%\n",
      "Batch 5, Loss: 1.208408, Accuracy: 57.19%\n",
      "Batch 6, Loss: 1.158311, Accuracy: 57.29%\n",
      "Batch 7, Loss: 1.188570, Accuracy: 56.47%\n",
      "Batch 8, Loss: 1.244455, Accuracy: 55.66%\n",
      "Batch 9, Loss: 1.237407, Accuracy: 54.69%\n",
      "Batch 10, Loss: 1.200869, Accuracy: 54.69%\n",
      "Batch 11, Loss: 1.269205, Accuracy: 53.84%\n",
      "Batch 12, Loss: 1.094048, Accuracy: 55.08%\n",
      "Batch 13, Loss: 1.182768, Accuracy: 54.81%\n",
      "Batch 14, Loss: 1.138825, Accuracy: 55.36%\n",
      "Batch 15, Loss: 1.266256, Accuracy: 54.90%\n",
      "Batch 16, Loss: 1.241818, Accuracy: 54.39%\n",
      "Batch 17, Loss: 1.173764, Accuracy: 54.60%\n",
      "Batch 18, Loss: 1.121026, Accuracy: 54.95%\n",
      "Batch 19, Loss: 1.130464, Accuracy: 55.51%\n",
      "Batch 20, Loss: 1.178868, Accuracy: 55.70%\n",
      "Batch 21, Loss: 1.162783, Accuracy: 55.65%\n",
      "Batch 22, Loss: 1.196102, Accuracy: 55.40%\n",
      "Batch 23, Loss: 1.203995, Accuracy: 55.30%\n",
      "Batch 24, Loss: 1.197641, Accuracy: 55.27%\n",
      "Batch 25, Loss: 1.224203, Accuracy: 55.25%\n",
      "Batch 26, Loss: 1.228914, Accuracy: 54.99%\n",
      "Batch 27, Loss: 1.188949, Accuracy: 55.03%\n",
      "Batch 28, Loss: 1.155522, Accuracy: 55.13%\n",
      "Batch 29, Loss: 1.292348, Accuracy: 54.69%\n",
      "Batch 30, Loss: 1.272661, Accuracy: 54.27%\n",
      "Batch 31, Loss: 1.160627, Accuracy: 54.33%\n",
      "Batch 32, Loss: 1.248876, Accuracy: 54.10%\n",
      "Batch 33, Loss: 1.184580, Accuracy: 54.17%\n",
      "Batch 34, Loss: 1.192549, Accuracy: 54.23%\n",
      "Batch 35, Loss: 1.231427, Accuracy: 54.11%\n",
      "Batch 36, Loss: 1.174324, Accuracy: 54.25%\n",
      "Batch 37, Loss: 1.178974, Accuracy: 54.22%\n",
      "Batch 38, Loss: 1.081114, Accuracy: 54.56%\n",
      "Batch 39, Loss: 1.119502, Accuracy: 54.73%\n",
      "Batch 40, Loss: 1.242301, Accuracy: 54.61%\n",
      "Batch 41, Loss: 1.255092, Accuracy: 54.42%\n",
      "Batch 42, Loss: 1.231315, Accuracy: 54.35%\n",
      "Batch 43, Loss: 1.104206, Accuracy: 54.58%\n",
      "Batch 44, Loss: 1.233050, Accuracy: 54.44%\n",
      "Batch 45, Loss: 1.191195, Accuracy: 54.34%\n",
      "Batch 46, Loss: 1.130195, Accuracy: 54.48%\n",
      "Batch 47, Loss: 1.207142, Accuracy: 54.45%\n",
      "Batch 48, Loss: 1.218784, Accuracy: 54.39%\n",
      "Batch 49, Loss: 1.202245, Accuracy: 54.43%\n",
      "Batch 50, Loss: 1.152075, Accuracy: 54.62%\n",
      "Batch 51, Loss: 1.339008, Accuracy: 54.32%\n",
      "Batch 52, Loss: 1.195943, Accuracy: 54.36%\n",
      "Batch 53, Loss: 1.294256, Accuracy: 54.19%\n",
      "Batch 54, Loss: 1.176040, Accuracy: 54.22%\n",
      "Batch 55, Loss: 1.182519, Accuracy: 54.26%\n",
      "Batch 56, Loss: 1.228349, Accuracy: 54.24%\n",
      "Batch 57, Loss: 1.117694, Accuracy: 54.44%\n",
      "Batch 58, Loss: 1.186596, Accuracy: 54.45%\n",
      "Batch 59, Loss: 1.189190, Accuracy: 54.50%\n",
      "Batch 60, Loss: 1.219097, Accuracy: 54.45%\n",
      "Batch 61, Loss: 1.229433, Accuracy: 54.33%\n",
      "Batch 62, Loss: 1.273785, Accuracy: 54.13%\n",
      "Batch 63, Loss: 1.084884, Accuracy: 54.24%\n",
      "Batch 64, Loss: 1.108906, Accuracy: 54.39%\n",
      "Batch 65, Loss: 1.119149, Accuracy: 54.52%\n",
      "Batch 66, Loss: 1.132139, Accuracy: 54.57%\n",
      "Batch 67, Loss: 1.124370, Accuracy: 54.64%\n",
      "Batch 68, Loss: 1.240145, Accuracy: 54.57%\n",
      "Batch 69, Loss: 1.213750, Accuracy: 54.53%\n",
      "Batch 70, Loss: 1.149426, Accuracy: 54.60%\n",
      "Batch 71, Loss: 1.259297, Accuracy: 54.51%\n",
      "Batch 72, Loss: 1.184152, Accuracy: 54.51%\n",
      "Batch 73, Loss: 1.187629, Accuracy: 54.54%\n",
      "Batch 74, Loss: 1.179075, Accuracy: 54.56%\n",
      "Batch 75, Loss: 1.090321, Accuracy: 54.79%\n",
      "Batch 76, Loss: 1.256496, Accuracy: 54.69%\n",
      "Batch 77, Loss: 1.136689, Accuracy: 54.75%\n",
      "Batch 78, Loss: 1.243924, Accuracy: 54.71%\n",
      "Batch 79, Loss: 1.251096, Accuracy: 54.61%\n",
      "Batch 80, Loss: 1.198960, Accuracy: 54.61%\n",
      "Batch 81, Loss: 1.202207, Accuracy: 54.63%\n",
      "Batch 82, Loss: 1.196355, Accuracy: 54.65%\n",
      "Batch 83, Loss: 1.224359, Accuracy: 54.57%\n",
      "Batch 84, Loss: 1.190327, Accuracy: 54.61%\n",
      "Batch 85, Loss: 1.168103, Accuracy: 54.61%\n",
      "Batch 86, Loss: 1.153729, Accuracy: 54.69%\n",
      "Batch 87, Loss: 1.103207, Accuracy: 54.83%\n",
      "Batch 88, Loss: 1.212893, Accuracy: 54.79%\n",
      "Batch 89, Loss: 1.154083, Accuracy: 54.85%\n",
      "Batch 90, Loss: 1.181978, Accuracy: 54.86%\n",
      "Batch 91, Loss: 1.161667, Accuracy: 54.89%\n",
      "Batch 92, Loss: 1.177964, Accuracy: 54.86%\n",
      "Batch 93, Loss: 1.215745, Accuracy: 54.79%\n",
      "Batch 94, Loss: 1.166683, Accuracy: 54.82%\n",
      "Batch 95, Loss: 1.225153, Accuracy: 54.77%\n",
      "Batch 96, Loss: 1.195486, Accuracy: 54.77%\n",
      "Batch 97, Loss: 1.185131, Accuracy: 54.74%\n",
      "Batch 98, Loss: 1.212332, Accuracy: 54.70%\n",
      "Batch 99, Loss: 1.101891, Accuracy: 54.83%\n",
      "Batch 100, Loss: 1.188373, Accuracy: 54.83%\n",
      "Batch 101, Loss: 1.217642, Accuracy: 54.75%\n",
      "Batch 102, Loss: 1.173405, Accuracy: 54.76%\n",
      "Batch 103, Loss: 1.211421, Accuracy: 54.75%\n",
      "Batch 104, Loss: 1.191960, Accuracy: 54.70%\n",
      "Batch 105, Loss: 1.203602, Accuracy: 54.67%\n",
      "Batch 106, Loss: 1.169625, Accuracy: 54.72%\n",
      "Batch 107, Loss: 1.189605, Accuracy: 54.70%\n",
      "Batch 108, Loss: 1.234764, Accuracy: 54.66%\n",
      "Batch 109, Loss: 1.157129, Accuracy: 54.67%\n",
      "Batch 110, Loss: 1.095164, Accuracy: 54.74%\n",
      "Batch 111, Loss: 1.167926, Accuracy: 54.77%\n",
      "Batch 112, Loss: 1.208146, Accuracy: 54.74%\n",
      "Batch 113, Loss: 1.211328, Accuracy: 54.72%\n",
      "Batch 114, Loss: 1.191723, Accuracy: 54.70%\n",
      "Batch 115, Loss: 1.211492, Accuracy: 54.67%\n",
      "Batch 116, Loss: 1.171851, Accuracy: 54.69%\n",
      "Batch 117, Loss: 1.136191, Accuracy: 54.74%\n",
      "Batch 118, Loss: 1.152317, Accuracy: 54.79%\n",
      "Batch 119, Loss: 1.194697, Accuracy: 54.81%\n",
      "Batch 120, Loss: 1.114082, Accuracy: 54.87%\n",
      "Batch 121, Loss: 1.125331, Accuracy: 54.92%\n",
      "Batch 122, Loss: 1.170092, Accuracy: 54.92%\n",
      "Batch 123, Loss: 1.210696, Accuracy: 54.88%\n",
      "Batch 124, Loss: 1.187946, Accuracy: 54.88%\n",
      "Batch 125, Loss: 1.219916, Accuracy: 54.84%\n",
      "Batch 126, Loss: 1.249379, Accuracy: 54.77%\n",
      "Batch 127, Loss: 1.218984, Accuracy: 54.74%\n",
      "Batch 128, Loss: 1.171838, Accuracy: 54.76%\n",
      "Batch 129, Loss: 1.217594, Accuracy: 54.74%\n",
      "Batch 130, Loss: 1.186497, Accuracy: 54.74%\n",
      "Batch 131, Loss: 1.171233, Accuracy: 54.75%\n",
      "Batch 132, Loss: 1.157769, Accuracy: 54.76%\n",
      "Batch 133, Loss: 1.234100, Accuracy: 54.69%\n",
      "Batch 134, Loss: 1.150050, Accuracy: 54.72%\n",
      "Batch 135, Loss: 1.228723, Accuracy: 54.69%\n",
      "Batch 136, Loss: 1.184741, Accuracy: 54.66%\n",
      "Batch 137, Loss: 1.182564, Accuracy: 54.66%\n",
      "Batch 138, Loss: 1.276793, Accuracy: 54.56%\n",
      "Batch 139, Loss: 1.138378, Accuracy: 54.58%\n",
      "Batch 140, Loss: 1.142210, Accuracy: 54.63%\n",
      "Batch 141, Loss: 1.127668, Accuracy: 54.70%\n",
      "Batch 142, Loss: 1.169360, Accuracy: 54.72%\n",
      "Batch 143, Loss: 1.167885, Accuracy: 54.74%\n",
      "Batch 144, Loss: 1.224932, Accuracy: 54.72%\n",
      "Batch 145, Loss: 1.120679, Accuracy: 54.77%\n",
      "Batch 146, Loss: 1.116633, Accuracy: 54.82%\n",
      "Batch 147, Loss: 1.266667, Accuracy: 54.76%\n",
      "Batch 148, Loss: 1.159983, Accuracy: 54.77%\n",
      "Batch 149, Loss: 1.219043, Accuracy: 54.72%\n",
      "Batch 150, Loss: 1.205297, Accuracy: 54.70%\n",
      "Batch 151, Loss: 1.204132, Accuracy: 54.69%\n",
      "Batch 152, Loss: 1.254503, Accuracy: 54.63%\n",
      "Batch 153, Loss: 1.201577, Accuracy: 54.63%\n",
      "Batch 154, Loss: 1.204721, Accuracy: 54.60%\n",
      "Batch 155, Loss: 1.198409, Accuracy: 54.60%\n",
      "Batch 156, Loss: 1.151425, Accuracy: 54.64%\n",
      "Batch 157, Loss: 1.109933, Accuracy: 54.67%\n",
      "Batch 158, Loss: 1.212030, Accuracy: 54.66%\n",
      "Batch 159, Loss: 1.245405, Accuracy: 54.62%\n",
      "Batch 160, Loss: 1.209944, Accuracy: 54.58%\n",
      "Batch 161, Loss: 1.230905, Accuracy: 54.52%\n",
      "Batch 162, Loss: 1.228031, Accuracy: 54.46%\n",
      "Batch 163, Loss: 1.241851, Accuracy: 54.44%\n",
      "Batch 164, Loss: 1.281353, Accuracy: 54.37%\n",
      "Batch 165, Loss: 1.130923, Accuracy: 54.39%\n",
      "Batch 166, Loss: 1.278553, Accuracy: 54.35%\n",
      "Batch 167, Loss: 1.154700, Accuracy: 54.39%\n",
      "Batch 168, Loss: 1.194953, Accuracy: 54.39%\n",
      "Batch 169, Loss: 1.139779, Accuracy: 54.42%\n",
      "Batch 170, Loss: 1.231778, Accuracy: 54.36%\n",
      "Batch 171, Loss: 1.210391, Accuracy: 54.31%\n",
      "Batch 172, Loss: 1.206076, Accuracy: 54.29%\n",
      "Batch 173, Loss: 1.237851, Accuracy: 54.24%\n",
      "Batch 174, Loss: 1.286520, Accuracy: 54.17%\n",
      "Batch 175, Loss: 1.130463, Accuracy: 54.20%\n",
      "Batch 176, Loss: 1.182738, Accuracy: 54.19%\n",
      "Batch 177, Loss: 1.208276, Accuracy: 54.16%\n",
      "Batch 178, Loss: 1.188186, Accuracy: 54.16%\n",
      "Batch 179, Loss: 1.244612, Accuracy: 54.14%\n",
      "Batch 180, Loss: 1.217625, Accuracy: 54.14%\n",
      "Batch 181, Loss: 1.139850, Accuracy: 54.16%\n",
      "Batch 182, Loss: 1.159394, Accuracy: 54.18%\n",
      "Batch 183, Loss: 1.201067, Accuracy: 54.18%\n",
      "Batch 184, Loss: 1.215473, Accuracy: 54.17%\n",
      "Batch 185, Loss: 1.201291, Accuracy: 54.16%\n",
      "Batch 186, Loss: 1.194488, Accuracy: 54.16%\n",
      "Batch 187, Loss: 1.261909, Accuracy: 54.11%\n",
      "Batch 188, Loss: 1.164283, Accuracy: 54.13%\n",
      "Batch 189, Loss: 1.177544, Accuracy: 54.15%\n",
      "Batch 190, Loss: 1.204069, Accuracy: 54.14%\n",
      "Batch 191, Loss: 1.112392, Accuracy: 54.20%\n",
      "Batch 192, Loss: 1.242661, Accuracy: 54.17%\n",
      "Batch 193, Loss: 1.149963, Accuracy: 54.19%\n",
      "Batch 194, Loss: 1.155386, Accuracy: 54.20%\n",
      "Batch 195, Loss: 1.251761, Accuracy: 54.17%\n",
      "Batch 196, Loss: 1.139491, Accuracy: 54.20%\n",
      "Batch 197, Loss: 1.144196, Accuracy: 54.24%\n",
      "Batch 198, Loss: 1.223429, Accuracy: 54.21%\n",
      "Batch 199, Loss: 1.128411, Accuracy: 54.22%\n",
      "Batch 200, Loss: 1.159073, Accuracy: 54.25%\n",
      "Batch 201, Loss: 1.102733, Accuracy: 54.29%\n",
      "Batch 202, Loss: 1.156291, Accuracy: 54.30%\n",
      "Batch 203, Loss: 1.226881, Accuracy: 54.29%\n",
      "Batch 204, Loss: 1.139266, Accuracy: 54.33%\n",
      "Batch 205, Loss: 1.175148, Accuracy: 54.34%\n",
      "Batch 206, Loss: 1.146664, Accuracy: 54.36%\n",
      "Batch 207, Loss: 1.174684, Accuracy: 54.39%\n",
      "Batch 208, Loss: 1.167355, Accuracy: 54.39%\n",
      "Batch 209, Loss: 1.147304, Accuracy: 54.43%\n",
      "Batch 210, Loss: 1.181908, Accuracy: 54.43%\n",
      "Batch 211, Loss: 1.205744, Accuracy: 54.42%\n",
      "Batch 212, Loss: 1.170961, Accuracy: 54.43%\n",
      "Batch 213, Loss: 1.263232, Accuracy: 54.39%\n",
      "Training - Epoch 6, Loss: 1.188644, Accuracy: 54.39%\n",
      "Validation Batch 1, Loss: 1.350073, Accuracy: 37.50%\n",
      "Validation Batch 2, Loss: 1.340573, Accuracy: 38.28%\n",
      "Validation Batch 3, Loss: 1.428766, Accuracy: 35.42%\n",
      "Validation Batch 4, Loss: 1.323680, Accuracy: 36.72%\n",
      "Validation Batch 5, Loss: 1.413392, Accuracy: 35.62%\n",
      "Validation Batch 6, Loss: 1.477965, Accuracy: 33.59%\n",
      "Validation Batch 7, Loss: 1.379413, Accuracy: 33.71%\n",
      "Validation Batch 8, Loss: 1.369608, Accuracy: 33.98%\n",
      "Validation Batch 9, Loss: 1.398073, Accuracy: 33.85%\n",
      "Validation Batch 10, Loss: 1.425164, Accuracy: 33.44%\n",
      "Validation Batch 11, Loss: 1.366908, Accuracy: 33.66%\n",
      "Validation Batch 12, Loss: 1.363046, Accuracy: 33.85%\n",
      "Validation Batch 13, Loss: 1.411476, Accuracy: 33.65%\n",
      "Validation Batch 14, Loss: 1.279002, Accuracy: 34.49%\n",
      "Validation Batch 15, Loss: 1.351193, Accuracy: 34.69%\n",
      "Validation Batch 16, Loss: 1.452361, Accuracy: 34.18%\n",
      "Validation Batch 17, Loss: 1.385242, Accuracy: 34.19%\n",
      "Validation Batch 18, Loss: 1.382200, Accuracy: 34.20%\n",
      "Validation Batch 19, Loss: 1.385224, Accuracy: 34.21%\n",
      "Validation Batch 20, Loss: 1.397833, Accuracy: 34.14%\n",
      "Validation Batch 21, Loss: 1.365014, Accuracy: 34.23%\n",
      "Validation Batch 22, Loss: 1.384880, Accuracy: 34.23%\n",
      "Validation Batch 23, Loss: 1.383423, Accuracy: 34.24%\n",
      "Validation Batch 24, Loss: 1.438612, Accuracy: 33.98%\n",
      "Validation Batch 25, Loss: 1.333754, Accuracy: 34.19%\n",
      "Validation Batch 26, Loss: 1.304941, Accuracy: 34.50%\n",
      "Validation Batch 27, Loss: 1.389585, Accuracy: 34.47%\n",
      "Validation - Epoch 6, Loss: 1.380793, Accuracy: 34.47%\n",
      "Patienceâ€”5\n",
      "Epoch 7\n",
      "Batch 1, Loss: 1.156869, Accuracy: 56.25%\n",
      "Batch 2, Loss: 1.127406, Accuracy: 57.81%\n",
      "Batch 3, Loss: 1.210938, Accuracy: 55.21%\n",
      "Batch 4, Loss: 1.186156, Accuracy: 54.69%\n",
      "Batch 5, Loss: 1.225135, Accuracy: 53.44%\n",
      "Batch 6, Loss: 1.201556, Accuracy: 52.86%\n",
      "Batch 7, Loss: 1.251014, Accuracy: 52.46%\n",
      "Batch 8, Loss: 1.149320, Accuracy: 53.32%\n",
      "Batch 9, Loss: 1.184275, Accuracy: 53.47%\n",
      "Batch 10, Loss: 1.128059, Accuracy: 54.38%\n",
      "Batch 11, Loss: 1.245766, Accuracy: 53.69%\n",
      "Batch 12, Loss: 1.165309, Accuracy: 53.78%\n",
      "Batch 13, Loss: 1.119354, Accuracy: 54.33%\n",
      "Batch 14, Loss: 1.195189, Accuracy: 54.13%\n",
      "Batch 15, Loss: 1.172893, Accuracy: 54.27%\n",
      "Batch 16, Loss: 1.170242, Accuracy: 54.49%\n",
      "Batch 17, Loss: 1.157997, Accuracy: 54.50%\n",
      "Batch 18, Loss: 1.221557, Accuracy: 54.34%\n",
      "Batch 19, Loss: 1.215249, Accuracy: 54.11%\n",
      "Batch 20, Loss: 1.127334, Accuracy: 54.53%\n",
      "Batch 21, Loss: 1.128713, Accuracy: 54.99%\n",
      "Batch 22, Loss: 1.140728, Accuracy: 55.26%\n",
      "Batch 23, Loss: 1.136016, Accuracy: 55.37%\n",
      "Batch 24, Loss: 1.148609, Accuracy: 55.60%\n",
      "Batch 25, Loss: 1.158390, Accuracy: 55.75%\n",
      "Batch 26, Loss: 1.118197, Accuracy: 56.07%\n",
      "Batch 27, Loss: 1.102060, Accuracy: 56.48%\n",
      "Batch 28, Loss: 1.122530, Accuracy: 56.64%\n",
      "Batch 29, Loss: 1.231962, Accuracy: 56.36%\n",
      "Batch 30, Loss: 1.215795, Accuracy: 56.35%\n",
      "Batch 31, Loss: 1.139875, Accuracy: 56.45%\n",
      "Batch 32, Loss: 1.123437, Accuracy: 56.59%\n",
      "Batch 33, Loss: 1.162842, Accuracy: 56.58%\n",
      "Batch 34, Loss: 1.198070, Accuracy: 56.62%\n",
      "Batch 35, Loss: 1.167644, Accuracy: 56.65%\n",
      "Batch 36, Loss: 1.077061, Accuracy: 56.99%\n",
      "Batch 37, Loss: 1.205114, Accuracy: 56.88%\n",
      "Batch 38, Loss: 1.198818, Accuracy: 56.74%\n",
      "Batch 39, Loss: 1.234074, Accuracy: 56.57%\n",
      "Batch 40, Loss: 1.113727, Accuracy: 56.72%\n",
      "Batch 41, Loss: 1.140140, Accuracy: 56.86%\n",
      "Batch 42, Loss: 1.107819, Accuracy: 56.99%\n",
      "Batch 43, Loss: 1.163787, Accuracy: 56.90%\n",
      "Batch 44, Loss: 1.206433, Accuracy: 56.75%\n",
      "Batch 45, Loss: 1.180851, Accuracy: 56.67%\n",
      "Batch 46, Loss: 1.217148, Accuracy: 56.59%\n",
      "Batch 47, Loss: 1.223340, Accuracy: 56.42%\n",
      "Batch 48, Loss: 1.285943, Accuracy: 56.15%\n",
      "Batch 49, Loss: 1.187815, Accuracy: 56.19%\n",
      "Batch 50, Loss: 1.241086, Accuracy: 56.06%\n",
      "Batch 51, Loss: 1.281890, Accuracy: 55.85%\n",
      "Batch 52, Loss: 1.167741, Accuracy: 55.86%\n",
      "Batch 53, Loss: 1.102959, Accuracy: 56.07%\n",
      "Batch 54, Loss: 1.174893, Accuracy: 56.05%\n",
      "Batch 55, Loss: 1.196168, Accuracy: 55.97%\n",
      "Batch 56, Loss: 1.231771, Accuracy: 55.86%\n",
      "Batch 57, Loss: 1.165906, Accuracy: 55.89%\n",
      "Batch 58, Loss: 1.189501, Accuracy: 55.87%\n",
      "Batch 59, Loss: 1.168405, Accuracy: 55.85%\n",
      "Batch 60, Loss: 1.213077, Accuracy: 55.76%\n",
      "Batch 61, Loss: 1.165933, Accuracy: 55.79%\n",
      "Batch 62, Loss: 1.149876, Accuracy: 55.85%\n",
      "Batch 63, Loss: 1.146004, Accuracy: 55.95%\n",
      "Batch 64, Loss: 1.141868, Accuracy: 55.98%\n",
      "Batch 65, Loss: 1.210925, Accuracy: 55.91%\n",
      "Batch 66, Loss: 1.125117, Accuracy: 56.04%\n",
      "Batch 67, Loss: 1.244966, Accuracy: 55.95%\n",
      "Batch 68, Loss: 1.170860, Accuracy: 55.95%\n",
      "Batch 69, Loss: 1.199261, Accuracy: 55.89%\n",
      "Batch 70, Loss: 1.108173, Accuracy: 55.98%\n",
      "Batch 71, Loss: 1.140918, Accuracy: 56.07%\n",
      "Batch 72, Loss: 1.171578, Accuracy: 56.08%\n",
      "Batch 73, Loss: 1.165912, Accuracy: 56.08%\n",
      "Batch 74, Loss: 1.141650, Accuracy: 56.14%\n",
      "Batch 75, Loss: 1.170497, Accuracy: 56.19%\n",
      "Batch 76, Loss: 1.225038, Accuracy: 56.11%\n",
      "Batch 77, Loss: 1.261987, Accuracy: 55.99%\n",
      "Batch 78, Loss: 1.213052, Accuracy: 55.99%\n",
      "Batch 79, Loss: 1.252420, Accuracy: 55.85%\n",
      "Batch 80, Loss: 1.227920, Accuracy: 55.82%\n",
      "Batch 81, Loss: 1.213917, Accuracy: 55.73%\n",
      "Batch 82, Loss: 1.172600, Accuracy: 55.74%\n",
      "Batch 83, Loss: 1.365273, Accuracy: 55.50%\n",
      "Batch 84, Loss: 1.270288, Accuracy: 55.36%\n",
      "Batch 85, Loss: 1.215772, Accuracy: 55.31%\n",
      "Batch 86, Loss: 1.217527, Accuracy: 55.25%\n",
      "Batch 87, Loss: 1.117694, Accuracy: 55.33%\n",
      "Batch 88, Loss: 1.133976, Accuracy: 55.38%\n",
      "Batch 89, Loss: 1.198369, Accuracy: 55.37%\n",
      "Batch 90, Loss: 1.131452, Accuracy: 55.45%\n",
      "Batch 91, Loss: 1.214061, Accuracy: 55.37%\n",
      "Batch 92, Loss: 1.165651, Accuracy: 55.40%\n",
      "Batch 93, Loss: 1.142365, Accuracy: 55.49%\n",
      "Batch 94, Loss: 1.169213, Accuracy: 55.45%\n",
      "Batch 95, Loss: 1.155714, Accuracy: 55.48%\n",
      "Batch 96, Loss: 1.236362, Accuracy: 55.40%\n",
      "Batch 97, Loss: 1.137689, Accuracy: 55.49%\n",
      "Batch 98, Loss: 1.195067, Accuracy: 55.45%\n",
      "Batch 99, Loss: 1.146314, Accuracy: 55.48%\n",
      "Batch 100, Loss: 1.177065, Accuracy: 55.48%\n",
      "Batch 101, Loss: 1.245270, Accuracy: 55.38%\n",
      "Batch 102, Loss: 1.150417, Accuracy: 55.44%\n",
      "Batch 103, Loss: 1.146552, Accuracy: 55.45%\n",
      "Batch 104, Loss: 1.108405, Accuracy: 55.53%\n",
      "Batch 105, Loss: 1.212338, Accuracy: 55.49%\n",
      "Batch 106, Loss: 1.213210, Accuracy: 55.44%\n",
      "Batch 107, Loss: 1.252192, Accuracy: 55.34%\n",
      "Batch 108, Loss: 1.129264, Accuracy: 55.40%\n",
      "Batch 109, Loss: 1.261829, Accuracy: 55.29%\n",
      "Batch 110, Loss: 1.163120, Accuracy: 55.31%\n",
      "Batch 111, Loss: 1.137137, Accuracy: 55.38%\n",
      "Batch 112, Loss: 1.168687, Accuracy: 55.41%\n",
      "Batch 113, Loss: 1.145281, Accuracy: 55.45%\n",
      "Batch 114, Loss: 1.178097, Accuracy: 55.48%\n",
      "Batch 115, Loss: 1.193509, Accuracy: 55.50%\n",
      "Batch 116, Loss: 1.199998, Accuracy: 55.51%\n",
      "Batch 117, Loss: 1.188115, Accuracy: 55.50%\n",
      "Batch 118, Loss: 1.192335, Accuracy: 55.51%\n",
      "Batch 119, Loss: 1.143614, Accuracy: 55.53%\n",
      "Batch 120, Loss: 1.187114, Accuracy: 55.53%\n",
      "Batch 121, Loss: 1.140906, Accuracy: 55.59%\n",
      "Batch 122, Loss: 1.169827, Accuracy: 55.60%\n",
      "Batch 123, Loss: 1.145195, Accuracy: 55.63%\n",
      "Batch 124, Loss: 1.140845, Accuracy: 55.71%\n",
      "Batch 125, Loss: 1.130800, Accuracy: 55.76%\n",
      "Batch 126, Loss: 1.183998, Accuracy: 55.78%\n",
      "Batch 127, Loss: 1.155455, Accuracy: 55.81%\n",
      "Batch 128, Loss: 1.133425, Accuracy: 55.87%\n",
      "Batch 129, Loss: 1.182950, Accuracy: 55.87%\n",
      "Batch 130, Loss: 1.285055, Accuracy: 55.75%\n",
      "Batch 131, Loss: 1.184294, Accuracy: 55.75%\n",
      "Batch 132, Loss: 1.212072, Accuracy: 55.74%\n",
      "Batch 133, Loss: 1.236273, Accuracy: 55.70%\n",
      "Batch 134, Loss: 1.180806, Accuracy: 55.70%\n",
      "Batch 135, Loss: 1.187907, Accuracy: 55.71%\n",
      "Batch 136, Loss: 1.143235, Accuracy: 55.78%\n",
      "Batch 137, Loss: 1.197888, Accuracy: 55.77%\n",
      "Batch 138, Loss: 1.113758, Accuracy: 55.84%\n",
      "Batch 139, Loss: 1.143794, Accuracy: 55.90%\n",
      "Batch 140, Loss: 1.204734, Accuracy: 55.89%\n",
      "Batch 141, Loss: 1.136736, Accuracy: 55.95%\n",
      "Batch 142, Loss: 1.133803, Accuracy: 55.95%\n",
      "Batch 143, Loss: 1.103956, Accuracy: 56.01%\n",
      "Batch 144, Loss: 1.227997, Accuracy: 55.95%\n",
      "Batch 145, Loss: 1.209237, Accuracy: 55.91%\n",
      "Batch 146, Loss: 1.160284, Accuracy: 55.92%\n",
      "Batch 147, Loss: 1.199986, Accuracy: 55.87%\n",
      "Batch 148, Loss: 1.255948, Accuracy: 55.80%\n",
      "Batch 149, Loss: 1.131844, Accuracy: 55.85%\n",
      "Batch 150, Loss: 1.216155, Accuracy: 55.84%\n",
      "Batch 151, Loss: 1.165671, Accuracy: 55.86%\n",
      "Batch 152, Loss: 1.243545, Accuracy: 55.80%\n",
      "Batch 153, Loss: 1.122335, Accuracy: 55.83%\n",
      "Batch 154, Loss: 1.143563, Accuracy: 55.88%\n",
      "Batch 155, Loss: 1.159719, Accuracy: 55.90%\n",
      "Batch 156, Loss: 1.185969, Accuracy: 55.92%\n",
      "Batch 157, Loss: 1.137306, Accuracy: 55.95%\n",
      "Batch 158, Loss: 1.141423, Accuracy: 55.96%\n",
      "Batch 159, Loss: 1.197564, Accuracy: 55.95%\n",
      "Batch 160, Loss: 1.198314, Accuracy: 55.94%\n",
      "Batch 161, Loss: 1.154160, Accuracy: 55.97%\n",
      "Batch 162, Loss: 1.162915, Accuracy: 55.97%\n",
      "Batch 163, Loss: 1.145381, Accuracy: 55.99%\n",
      "Batch 164, Loss: 1.221944, Accuracy: 55.95%\n",
      "Batch 165, Loss: 1.126422, Accuracy: 55.98%\n",
      "Batch 166, Loss: 1.153903, Accuracy: 56.01%\n",
      "Batch 167, Loss: 1.114115, Accuracy: 56.04%\n",
      "Batch 168, Loss: 1.190777, Accuracy: 56.02%\n",
      "Batch 169, Loss: 1.153453, Accuracy: 56.05%\n",
      "Batch 170, Loss: 1.241559, Accuracy: 56.01%\n",
      "Batch 171, Loss: 1.162126, Accuracy: 56.03%\n",
      "Batch 172, Loss: 1.127448, Accuracy: 56.06%\n",
      "Batch 173, Loss: 1.143548, Accuracy: 56.09%\n",
      "Batch 174, Loss: 1.162674, Accuracy: 56.10%\n",
      "Batch 175, Loss: 1.167563, Accuracy: 56.10%\n",
      "Batch 176, Loss: 1.211433, Accuracy: 56.07%\n",
      "Batch 177, Loss: 1.212783, Accuracy: 56.04%\n",
      "Batch 178, Loss: 1.295048, Accuracy: 55.95%\n",
      "Batch 179, Loss: 1.162069, Accuracy: 55.94%\n",
      "Batch 180, Loss: 1.225469, Accuracy: 55.92%\n",
      "Batch 181, Loss: 1.210269, Accuracy: 55.90%\n",
      "Batch 182, Loss: 1.288219, Accuracy: 55.84%\n",
      "Batch 183, Loss: 1.135738, Accuracy: 55.86%\n",
      "Batch 184, Loss: 1.015641, Accuracy: 55.96%\n",
      "Batch 185, Loss: 1.125238, Accuracy: 56.00%\n",
      "Batch 186, Loss: 1.180915, Accuracy: 55.99%\n",
      "Batch 187, Loss: 1.178269, Accuracy: 55.97%\n",
      "Batch 188, Loss: 1.172114, Accuracy: 55.97%\n",
      "Batch 189, Loss: 1.225905, Accuracy: 55.94%\n",
      "Batch 190, Loss: 1.165157, Accuracy: 55.95%\n",
      "Batch 191, Loss: 1.270373, Accuracy: 55.88%\n",
      "Batch 192, Loss: 1.101724, Accuracy: 55.94%\n",
      "Batch 193, Loss: 1.164605, Accuracy: 55.93%\n",
      "Batch 194, Loss: 1.112467, Accuracy: 55.98%\n",
      "Batch 195, Loss: 1.222225, Accuracy: 55.95%\n",
      "Batch 196, Loss: 1.187283, Accuracy: 55.93%\n",
      "Batch 197, Loss: 1.210256, Accuracy: 55.91%\n",
      "Batch 198, Loss: 1.169223, Accuracy: 55.91%\n",
      "Batch 199, Loss: 1.188578, Accuracy: 55.90%\n",
      "Batch 200, Loss: 1.135799, Accuracy: 55.95%\n",
      "Batch 201, Loss: 1.240140, Accuracy: 55.91%\n",
      "Batch 202, Loss: 1.130650, Accuracy: 55.95%\n",
      "Batch 203, Loss: 1.125737, Accuracy: 55.97%\n",
      "Batch 204, Loss: 1.070793, Accuracy: 56.04%\n",
      "Batch 205, Loss: 1.152443, Accuracy: 56.05%\n",
      "Batch 206, Loss: 1.235654, Accuracy: 56.01%\n",
      "Batch 207, Loss: 1.146989, Accuracy: 56.02%\n",
      "Batch 208, Loss: 1.082096, Accuracy: 56.08%\n",
      "Batch 209, Loss: 1.173919, Accuracy: 56.04%\n",
      "Batch 210, Loss: 1.163629, Accuracy: 56.05%\n",
      "Batch 211, Loss: 1.146531, Accuracy: 56.05%\n",
      "Batch 212, Loss: 1.142094, Accuracy: 56.07%\n",
      "Batch 213, Loss: 1.151680, Accuracy: 56.10%\n",
      "Training - Epoch 7, Loss: 1.176106, Accuracy: 56.10%\n",
      "Validation Batch 1, Loss: 1.350123, Accuracy: 37.50%\n",
      "Validation Batch 2, Loss: 1.341193, Accuracy: 38.28%\n",
      "Validation Batch 3, Loss: 1.430328, Accuracy: 35.42%\n",
      "Validation Batch 4, Loss: 1.322719, Accuracy: 36.72%\n",
      "Validation Batch 5, Loss: 1.414852, Accuracy: 35.62%\n",
      "Validation Batch 6, Loss: 1.480870, Accuracy: 33.59%\n",
      "Validation Batch 7, Loss: 1.380611, Accuracy: 33.71%\n",
      "Validation Batch 8, Loss: 1.369923, Accuracy: 33.98%\n",
      "Validation Batch 9, Loss: 1.399457, Accuracy: 33.85%\n",
      "Validation Batch 10, Loss: 1.425544, Accuracy: 33.44%\n",
      "Validation Batch 11, Loss: 1.367307, Accuracy: 33.66%\n",
      "Validation Batch 12, Loss: 1.363917, Accuracy: 33.85%\n",
      "Validation Batch 13, Loss: 1.412078, Accuracy: 33.65%\n",
      "Validation Batch 14, Loss: 1.277680, Accuracy: 34.49%\n",
      "Validation Batch 15, Loss: 1.351365, Accuracy: 34.69%\n",
      "Validation Batch 16, Loss: 1.453921, Accuracy: 34.18%\n",
      "Validation Batch 17, Loss: 1.387251, Accuracy: 34.19%\n",
      "Validation Batch 18, Loss: 1.382373, Accuracy: 34.20%\n",
      "Validation Batch 19, Loss: 1.385774, Accuracy: 34.21%\n",
      "Validation Batch 20, Loss: 1.397697, Accuracy: 34.14%\n",
      "Validation Batch 21, Loss: 1.366441, Accuracy: 34.23%\n",
      "Validation Batch 22, Loss: 1.384368, Accuracy: 34.23%\n",
      "Validation Batch 23, Loss: 1.384815, Accuracy: 34.24%\n",
      "Validation Batch 24, Loss: 1.440998, Accuracy: 33.98%\n",
      "Validation Batch 25, Loss: 1.334800, Accuracy: 34.19%\n",
      "Validation Batch 26, Loss: 1.305597, Accuracy: 34.50%\n",
      "Validation Batch 27, Loss: 1.390552, Accuracy: 34.47%\n",
      "Validation - Epoch 7, Loss: 1.381576, Accuracy: 34.47%\n",
      "Patienceâ€”6\n",
      "Epoch 8\n",
      "Batch 1, Loss: 1.139461, Accuracy: 62.50%\n",
      "Batch 2, Loss: 1.135232, Accuracy: 62.50%\n",
      "Batch 3, Loss: 1.142629, Accuracy: 62.50%\n",
      "Batch 4, Loss: 1.171659, Accuracy: 60.55%\n",
      "Batch 5, Loss: 1.117217, Accuracy: 60.94%\n",
      "Batch 6, Loss: 1.238039, Accuracy: 58.85%\n",
      "Batch 7, Loss: 1.194459, Accuracy: 57.37%\n",
      "Batch 8, Loss: 1.299998, Accuracy: 55.47%\n",
      "Batch 9, Loss: 1.239633, Accuracy: 54.51%\n",
      "Batch 10, Loss: 1.115735, Accuracy: 55.16%\n",
      "Batch 11, Loss: 1.182117, Accuracy: 54.83%\n",
      "Batch 12, Loss: 1.085951, Accuracy: 55.73%\n",
      "Batch 13, Loss: 1.049685, Accuracy: 56.85%\n",
      "Batch 14, Loss: 1.137547, Accuracy: 57.03%\n",
      "Batch 15, Loss: 1.212187, Accuracy: 56.56%\n",
      "Batch 16, Loss: 1.121401, Accuracy: 57.13%\n",
      "Batch 17, Loss: 1.169924, Accuracy: 56.99%\n",
      "Batch 18, Loss: 1.181435, Accuracy: 57.03%\n",
      "Batch 19, Loss: 1.264268, Accuracy: 56.58%\n",
      "Batch 20, Loss: 1.133004, Accuracy: 56.80%\n",
      "Batch 21, Loss: 1.171112, Accuracy: 56.70%\n",
      "Batch 22, Loss: 1.208753, Accuracy: 56.61%\n",
      "Batch 23, Loss: 1.135137, Accuracy: 56.73%\n",
      "Batch 24, Loss: 1.119630, Accuracy: 56.84%\n",
      "Batch 25, Loss: 1.136068, Accuracy: 57.06%\n",
      "Batch 26, Loss: 1.231428, Accuracy: 56.85%\n",
      "Batch 27, Loss: 1.193425, Accuracy: 56.60%\n",
      "Batch 28, Loss: 1.142672, Accuracy: 56.70%\n",
      "Batch 29, Loss: 1.208942, Accuracy: 56.52%\n",
      "Batch 30, Loss: 1.253042, Accuracy: 56.25%\n",
      "Batch 31, Loss: 1.177068, Accuracy: 56.25%\n",
      "Batch 32, Loss: 1.153102, Accuracy: 56.40%\n",
      "Batch 33, Loss: 1.212881, Accuracy: 56.30%\n",
      "Batch 34, Loss: 1.123921, Accuracy: 56.48%\n",
      "Batch 35, Loss: 1.167547, Accuracy: 56.52%\n",
      "Batch 36, Loss: 1.146125, Accuracy: 56.55%\n",
      "Batch 37, Loss: 1.117901, Accuracy: 56.67%\n",
      "Batch 38, Loss: 1.157748, Accuracy: 56.78%\n",
      "Batch 39, Loss: 1.187970, Accuracy: 56.77%\n",
      "Batch 40, Loss: 1.151387, Accuracy: 56.76%\n",
      "Batch 41, Loss: 1.142357, Accuracy: 56.78%\n",
      "Batch 42, Loss: 1.160019, Accuracy: 56.77%\n",
      "Batch 43, Loss: 1.074139, Accuracy: 56.94%\n",
      "Batch 44, Loss: 1.153541, Accuracy: 56.96%\n",
      "Batch 45, Loss: 1.121577, Accuracy: 57.05%\n",
      "Batch 46, Loss: 1.311424, Accuracy: 56.73%\n",
      "Batch 47, Loss: 1.123995, Accuracy: 56.82%\n",
      "Batch 48, Loss: 1.155528, Accuracy: 56.84%\n",
      "Batch 49, Loss: 1.232356, Accuracy: 56.73%\n",
      "Batch 50, Loss: 1.146744, Accuracy: 56.88%\n",
      "Batch 51, Loss: 1.197602, Accuracy: 56.77%\n",
      "Batch 52, Loss: 1.166641, Accuracy: 56.73%\n",
      "Batch 53, Loss: 1.101426, Accuracy: 56.90%\n",
      "Batch 54, Loss: 1.112768, Accuracy: 57.00%\n",
      "Batch 55, Loss: 1.121037, Accuracy: 57.07%\n",
      "Batch 56, Loss: 1.187419, Accuracy: 57.03%\n",
      "Batch 57, Loss: 1.169279, Accuracy: 56.99%\n",
      "Batch 58, Loss: 1.048160, Accuracy: 57.25%\n",
      "Batch 59, Loss: 1.258717, Accuracy: 57.10%\n",
      "Batch 60, Loss: 1.117109, Accuracy: 57.21%\n",
      "Batch 61, Loss: 1.125737, Accuracy: 57.25%\n",
      "Batch 62, Loss: 1.198762, Accuracy: 57.21%\n",
      "Batch 63, Loss: 1.202987, Accuracy: 57.19%\n",
      "Batch 64, Loss: 1.160811, Accuracy: 57.23%\n",
      "Batch 65, Loss: 1.176113, Accuracy: 57.24%\n",
      "Batch 66, Loss: 1.106850, Accuracy: 57.34%\n",
      "Batch 67, Loss: 1.172366, Accuracy: 57.28%\n",
      "Batch 68, Loss: 1.151393, Accuracy: 57.35%\n",
      "Batch 69, Loss: 1.138335, Accuracy: 57.36%\n",
      "Batch 70, Loss: 1.157957, Accuracy: 57.41%\n",
      "Batch 71, Loss: 1.205306, Accuracy: 57.33%\n",
      "Batch 72, Loss: 1.119878, Accuracy: 57.38%\n",
      "Batch 73, Loss: 1.218168, Accuracy: 57.23%\n",
      "Batch 74, Loss: 1.126615, Accuracy: 57.28%\n",
      "Batch 75, Loss: 1.294306, Accuracy: 57.06%\n",
      "Batch 76, Loss: 1.082516, Accuracy: 57.22%\n",
      "Batch 77, Loss: 1.166206, Accuracy: 57.20%\n",
      "Batch 78, Loss: 1.134226, Accuracy: 57.27%\n",
      "Batch 79, Loss: 1.168220, Accuracy: 57.26%\n",
      "Batch 80, Loss: 1.115889, Accuracy: 57.36%\n",
      "Batch 81, Loss: 1.154747, Accuracy: 57.35%\n",
      "Batch 82, Loss: 1.159283, Accuracy: 57.36%\n",
      "Batch 83, Loss: 1.119341, Accuracy: 57.44%\n",
      "Batch 84, Loss: 1.087212, Accuracy: 57.53%\n",
      "Batch 85, Loss: 1.275493, Accuracy: 57.37%\n",
      "Batch 86, Loss: 1.103625, Accuracy: 57.43%\n",
      "Batch 87, Loss: 1.143851, Accuracy: 57.44%\n",
      "Batch 88, Loss: 1.179311, Accuracy: 57.44%\n",
      "Batch 89, Loss: 1.238217, Accuracy: 57.32%\n",
      "Batch 90, Loss: 1.221809, Accuracy: 57.26%\n",
      "Batch 91, Loss: 1.179528, Accuracy: 57.23%\n",
      "Batch 92, Loss: 1.108046, Accuracy: 57.27%\n",
      "Batch 93, Loss: 1.136939, Accuracy: 57.31%\n",
      "Batch 94, Loss: 1.243054, Accuracy: 57.18%\n",
      "Batch 95, Loss: 1.159146, Accuracy: 57.19%\n",
      "Batch 96, Loss: 1.122935, Accuracy: 57.28%\n",
      "Batch 97, Loss: 1.220104, Accuracy: 57.17%\n",
      "Batch 98, Loss: 1.150605, Accuracy: 57.19%\n",
      "Batch 99, Loss: 1.155849, Accuracy: 57.21%\n",
      "Batch 100, Loss: 1.267920, Accuracy: 57.09%\n",
      "Batch 101, Loss: 1.070737, Accuracy: 57.24%\n",
      "Batch 102, Loss: 1.145916, Accuracy: 57.26%\n",
      "Batch 103, Loss: 1.184582, Accuracy: 57.25%\n",
      "Batch 104, Loss: 1.188455, Accuracy: 57.23%\n",
      "Batch 105, Loss: 1.215377, Accuracy: 57.17%\n",
      "Batch 106, Loss: 1.250930, Accuracy: 57.06%\n",
      "Batch 107, Loss: 1.066615, Accuracy: 57.14%\n",
      "Batch 108, Loss: 1.179280, Accuracy: 57.13%\n",
      "Batch 109, Loss: 1.144417, Accuracy: 57.14%\n",
      "Batch 110, Loss: 1.059490, Accuracy: 57.24%\n",
      "Batch 111, Loss: 1.178798, Accuracy: 57.22%\n",
      "Batch 112, Loss: 1.159615, Accuracy: 57.21%\n",
      "Batch 113, Loss: 1.166181, Accuracy: 57.25%\n",
      "Batch 114, Loss: 1.108250, Accuracy: 57.33%\n",
      "Batch 115, Loss: 1.219174, Accuracy: 57.31%\n",
      "Batch 116, Loss: 1.147915, Accuracy: 57.30%\n",
      "Batch 117, Loss: 1.137395, Accuracy: 57.31%\n",
      "Batch 118, Loss: 1.201703, Accuracy: 57.27%\n",
      "Batch 119, Loss: 1.174997, Accuracy: 57.29%\n",
      "Batch 120, Loss: 1.142884, Accuracy: 57.33%\n",
      "Batch 121, Loss: 1.194471, Accuracy: 57.30%\n",
      "Batch 122, Loss: 1.075653, Accuracy: 57.39%\n",
      "Batch 123, Loss: 1.092218, Accuracy: 57.48%\n",
      "Batch 124, Loss: 1.120727, Accuracy: 57.54%\n",
      "Batch 125, Loss: 1.152767, Accuracy: 57.54%\n",
      "Batch 126, Loss: 1.183487, Accuracy: 57.54%\n",
      "Batch 127, Loss: 1.181728, Accuracy: 57.49%\n",
      "Batch 128, Loss: 1.117184, Accuracy: 57.53%\n",
      "Batch 129, Loss: 1.218721, Accuracy: 57.49%\n",
      "Batch 130, Loss: 1.125008, Accuracy: 57.51%\n",
      "Batch 131, Loss: 1.152844, Accuracy: 57.50%\n",
      "Batch 132, Loss: 1.122162, Accuracy: 57.56%\n",
      "Batch 133, Loss: 1.144959, Accuracy: 57.60%\n",
      "Batch 134, Loss: 1.162938, Accuracy: 57.60%\n",
      "Batch 135, Loss: 1.256611, Accuracy: 57.55%\n",
      "Batch 136, Loss: 1.211858, Accuracy: 57.54%\n",
      "Batch 137, Loss: 1.189766, Accuracy: 57.53%\n",
      "Batch 138, Loss: 1.074558, Accuracy: 57.61%\n",
      "Batch 139, Loss: 1.167933, Accuracy: 57.58%\n",
      "Batch 140, Loss: 1.089926, Accuracy: 57.62%\n",
      "Batch 141, Loss: 1.191259, Accuracy: 57.59%\n",
      "Batch 142, Loss: 1.151186, Accuracy: 57.60%\n",
      "Batch 143, Loss: 1.074360, Accuracy: 57.69%\n",
      "Batch 144, Loss: 1.127582, Accuracy: 57.71%\n",
      "Batch 145, Loss: 1.191076, Accuracy: 57.68%\n",
      "Batch 146, Loss: 1.118427, Accuracy: 57.72%\n",
      "Batch 147, Loss: 1.227377, Accuracy: 57.65%\n",
      "Batch 148, Loss: 1.052829, Accuracy: 57.75%\n",
      "Batch 149, Loss: 1.063436, Accuracy: 57.81%\n",
      "Batch 150, Loss: 1.203339, Accuracy: 57.78%\n",
      "Batch 151, Loss: 1.137612, Accuracy: 57.79%\n",
      "Batch 152, Loss: 1.159585, Accuracy: 57.79%\n",
      "Batch 153, Loss: 1.138644, Accuracy: 57.80%\n",
      "Batch 154, Loss: 1.211609, Accuracy: 57.79%\n",
      "Batch 155, Loss: 1.179457, Accuracy: 57.78%\n",
      "Batch 156, Loss: 1.108896, Accuracy: 57.84%\n",
      "Batch 157, Loss: 1.124913, Accuracy: 57.83%\n",
      "Batch 158, Loss: 1.131483, Accuracy: 57.87%\n",
      "Batch 159, Loss: 1.199026, Accuracy: 57.85%\n",
      "Batch 160, Loss: 1.227479, Accuracy: 57.80%\n",
      "Batch 161, Loss: 1.095846, Accuracy: 57.84%\n",
      "Batch 162, Loss: 1.185514, Accuracy: 57.83%\n",
      "Batch 163, Loss: 1.111254, Accuracy: 57.88%\n",
      "Batch 164, Loss: 1.162747, Accuracy: 57.87%\n",
      "Batch 165, Loss: 1.149447, Accuracy: 57.88%\n",
      "Batch 166, Loss: 1.197540, Accuracy: 57.85%\n",
      "Batch 167, Loss: 1.192960, Accuracy: 57.82%\n",
      "Batch 168, Loss: 1.160894, Accuracy: 57.81%\n",
      "Batch 169, Loss: 1.139216, Accuracy: 57.81%\n",
      "Batch 170, Loss: 1.179045, Accuracy: 57.78%\n",
      "Batch 171, Loss: 1.134412, Accuracy: 57.79%\n",
      "Batch 172, Loss: 1.197793, Accuracy: 57.74%\n",
      "Batch 173, Loss: 1.111184, Accuracy: 57.79%\n",
      "Batch 174, Loss: 1.253471, Accuracy: 57.74%\n",
      "Batch 175, Loss: 1.124285, Accuracy: 57.76%\n",
      "Batch 176, Loss: 1.119324, Accuracy: 57.78%\n",
      "Batch 177, Loss: 1.132445, Accuracy: 57.79%\n",
      "Batch 178, Loss: 1.116361, Accuracy: 57.82%\n",
      "Batch 179, Loss: 1.098666, Accuracy: 57.86%\n",
      "Batch 180, Loss: 1.137765, Accuracy: 57.86%\n",
      "Batch 181, Loss: 1.092884, Accuracy: 57.89%\n",
      "Batch 182, Loss: 1.133441, Accuracy: 57.90%\n",
      "Batch 183, Loss: 1.189706, Accuracy: 57.87%\n",
      "Batch 184, Loss: 1.203120, Accuracy: 57.84%\n",
      "Batch 185, Loss: 1.201577, Accuracy: 57.80%\n",
      "Batch 186, Loss: 1.147982, Accuracy: 57.81%\n",
      "Batch 187, Loss: 1.216216, Accuracy: 57.79%\n",
      "Batch 188, Loss: 1.212827, Accuracy: 57.74%\n",
      "Batch 189, Loss: 1.155249, Accuracy: 57.73%\n",
      "Batch 190, Loss: 1.169046, Accuracy: 57.74%\n",
      "Batch 191, Loss: 1.134165, Accuracy: 57.77%\n",
      "Batch 192, Loss: 1.155274, Accuracy: 57.77%\n",
      "Batch 193, Loss: 1.244404, Accuracy: 57.72%\n",
      "Batch 194, Loss: 1.165892, Accuracy: 57.72%\n",
      "Batch 195, Loss: 1.122424, Accuracy: 57.76%\n",
      "Batch 196, Loss: 1.149638, Accuracy: 57.76%\n",
      "Batch 197, Loss: 1.179906, Accuracy: 57.76%\n",
      "Batch 198, Loss: 1.088943, Accuracy: 57.80%\n",
      "Batch 199, Loss: 1.195634, Accuracy: 57.77%\n",
      "Batch 200, Loss: 1.124121, Accuracy: 57.79%\n",
      "Batch 201, Loss: 1.102638, Accuracy: 57.81%\n",
      "Batch 202, Loss: 1.181831, Accuracy: 57.81%\n",
      "Batch 203, Loss: 1.130872, Accuracy: 57.82%\n",
      "Batch 204, Loss: 1.202487, Accuracy: 57.80%\n",
      "Batch 205, Loss: 1.213223, Accuracy: 57.76%\n",
      "Batch 206, Loss: 1.149011, Accuracy: 57.77%\n",
      "Batch 207, Loss: 1.201705, Accuracy: 57.74%\n",
      "Batch 208, Loss: 1.161667, Accuracy: 57.74%\n",
      "Batch 209, Loss: 1.094316, Accuracy: 57.78%\n",
      "Batch 210, Loss: 1.202024, Accuracy: 57.77%\n",
      "Batch 211, Loss: 1.150265, Accuracy: 57.78%\n",
      "Batch 212, Loss: 1.115496, Accuracy: 57.78%\n",
      "Batch 213, Loss: 1.148767, Accuracy: 57.79%\n",
      "Training - Epoch 8, Loss: 1.159689, Accuracy: 57.79%\n",
      "Validation Batch 1, Loss: 1.338597, Accuracy: 37.50%\n",
      "Validation Batch 2, Loss: 1.333995, Accuracy: 38.28%\n",
      "Validation Batch 3, Loss: 1.421171, Accuracy: 35.42%\n",
      "Validation Batch 4, Loss: 1.312067, Accuracy: 36.72%\n",
      "Validation Batch 5, Loss: 1.405953, Accuracy: 35.62%\n",
      "Validation Batch 6, Loss: 1.465413, Accuracy: 33.59%\n",
      "Validation Batch 7, Loss: 1.369302, Accuracy: 33.71%\n",
      "Validation Batch 8, Loss: 1.360416, Accuracy: 33.98%\n",
      "Validation Batch 9, Loss: 1.391764, Accuracy: 33.85%\n",
      "Validation Batch 10, Loss: 1.413092, Accuracy: 33.44%\n",
      "Validation Batch 11, Loss: 1.357151, Accuracy: 33.66%\n",
      "Validation Batch 12, Loss: 1.351008, Accuracy: 33.85%\n",
      "Validation Batch 13, Loss: 1.400875, Accuracy: 33.65%\n",
      "Validation Batch 14, Loss: 1.269634, Accuracy: 34.49%\n",
      "Validation Batch 15, Loss: 1.341212, Accuracy: 34.69%\n",
      "Validation Batch 16, Loss: 1.439431, Accuracy: 34.18%\n",
      "Validation Batch 17, Loss: 1.380840, Accuracy: 34.19%\n",
      "Validation Batch 18, Loss: 1.370672, Accuracy: 34.20%\n",
      "Validation Batch 19, Loss: 1.377809, Accuracy: 34.21%\n",
      "Validation Batch 20, Loss: 1.385725, Accuracy: 34.14%\n",
      "Validation Batch 21, Loss: 1.357043, Accuracy: 34.23%\n",
      "Validation Batch 22, Loss: 1.373466, Accuracy: 34.23%\n",
      "Validation Batch 23, Loss: 1.377084, Accuracy: 34.24%\n",
      "Validation Batch 24, Loss: 1.429159, Accuracy: 33.98%\n",
      "Validation Batch 25, Loss: 1.324645, Accuracy: 34.19%\n",
      "Validation Batch 26, Loss: 1.295610, Accuracy: 34.50%\n",
      "Validation Batch 27, Loss: 1.381080, Accuracy: 34.47%\n",
      "Validation - Epoch 8, Loss: 1.371267, Accuracy: 34.47%\n",
      "Patienceâ€”7\n",
      "Epoch 9\n",
      "Batch 1, Loss: 1.132985, Accuracy: 62.50%\n",
      "Batch 2, Loss: 1.242551, Accuracy: 55.47%\n",
      "Batch 3, Loss: 1.145623, Accuracy: 57.81%\n",
      "Batch 4, Loss: 1.193220, Accuracy: 56.64%\n",
      "Batch 5, Loss: 1.136048, Accuracy: 57.81%\n",
      "Batch 6, Loss: 1.136247, Accuracy: 58.59%\n",
      "Batch 7, Loss: 1.180747, Accuracy: 58.48%\n",
      "Batch 8, Loss: 1.148975, Accuracy: 58.59%\n",
      "Batch 9, Loss: 1.186963, Accuracy: 58.33%\n",
      "Batch 10, Loss: 1.093396, Accuracy: 59.38%\n",
      "Batch 11, Loss: 1.189288, Accuracy: 59.09%\n",
      "Batch 12, Loss: 1.104177, Accuracy: 59.24%\n",
      "Batch 13, Loss: 1.131097, Accuracy: 59.50%\n",
      "Batch 14, Loss: 1.200038, Accuracy: 58.82%\n",
      "Batch 15, Loss: 1.042363, Accuracy: 59.69%\n",
      "Batch 16, Loss: 1.140294, Accuracy: 59.77%\n",
      "Batch 17, Loss: 1.196030, Accuracy: 59.28%\n",
      "Batch 18, Loss: 1.165180, Accuracy: 59.20%\n",
      "Batch 19, Loss: 1.224446, Accuracy: 58.55%\n",
      "Batch 20, Loss: 1.153005, Accuracy: 58.52%\n",
      "Batch 21, Loss: 1.146677, Accuracy: 58.56%\n",
      "Batch 22, Loss: 1.151692, Accuracy: 58.52%\n",
      "Batch 23, Loss: 1.165158, Accuracy: 58.42%\n",
      "Batch 24, Loss: 1.135627, Accuracy: 58.53%\n",
      "Batch 25, Loss: 1.141199, Accuracy: 58.75%\n",
      "Batch 26, Loss: 1.161627, Accuracy: 58.71%\n",
      "Batch 27, Loss: 1.143863, Accuracy: 58.74%\n",
      "Batch 28, Loss: 1.215043, Accuracy: 58.31%\n",
      "Batch 29, Loss: 1.102346, Accuracy: 58.35%\n",
      "Batch 30, Loss: 1.205535, Accuracy: 58.18%\n",
      "Batch 31, Loss: 1.160101, Accuracy: 58.17%\n",
      "Batch 32, Loss: 1.221400, Accuracy: 57.91%\n",
      "Batch 33, Loss: 1.187363, Accuracy: 57.72%\n",
      "Batch 34, Loss: 1.123095, Accuracy: 57.86%\n",
      "Batch 35, Loss: 1.062696, Accuracy: 58.08%\n",
      "Batch 36, Loss: 1.102380, Accuracy: 58.25%\n",
      "Batch 37, Loss: 1.136072, Accuracy: 58.36%\n",
      "Batch 38, Loss: 1.116212, Accuracy: 58.51%\n",
      "Batch 39, Loss: 1.066021, Accuracy: 58.69%\n",
      "Batch 40, Loss: 1.181977, Accuracy: 58.59%\n",
      "Batch 41, Loss: 1.134926, Accuracy: 58.61%\n",
      "Batch 42, Loss: 1.111228, Accuracy: 58.78%\n",
      "Batch 43, Loss: 1.193328, Accuracy: 58.65%\n",
      "Batch 44, Loss: 1.074582, Accuracy: 58.95%\n",
      "Batch 45, Loss: 1.156827, Accuracy: 58.92%\n",
      "Batch 46, Loss: 1.189605, Accuracy: 58.73%\n",
      "Batch 47, Loss: 1.225125, Accuracy: 58.48%\n",
      "Batch 48, Loss: 1.094342, Accuracy: 58.56%\n",
      "Batch 49, Loss: 1.191359, Accuracy: 58.45%\n",
      "Batch 50, Loss: 1.087486, Accuracy: 58.69%\n",
      "Batch 51, Loss: 1.096800, Accuracy: 58.82%\n",
      "Batch 52, Loss: 1.127906, Accuracy: 58.86%\n",
      "Batch 53, Loss: 1.104164, Accuracy: 58.93%\n",
      "Batch 54, Loss: 1.097231, Accuracy: 59.00%\n",
      "Batch 55, Loss: 1.178283, Accuracy: 58.95%\n",
      "Batch 56, Loss: 1.163464, Accuracy: 58.96%\n",
      "Batch 57, Loss: 1.142908, Accuracy: 58.96%\n",
      "Batch 58, Loss: 1.161964, Accuracy: 58.92%\n",
      "Batch 59, Loss: 1.107598, Accuracy: 59.03%\n",
      "Batch 60, Loss: 1.108238, Accuracy: 59.01%\n",
      "Batch 61, Loss: 1.163223, Accuracy: 59.02%\n",
      "Batch 62, Loss: 1.217213, Accuracy: 58.87%\n",
      "Batch 63, Loss: 1.112140, Accuracy: 58.90%\n",
      "Batch 64, Loss: 1.165358, Accuracy: 58.89%\n",
      "Batch 65, Loss: 1.175138, Accuracy: 58.87%\n",
      "Batch 66, Loss: 1.022202, Accuracy: 59.09%\n",
      "Batch 67, Loss: 1.175149, Accuracy: 59.07%\n",
      "Batch 68, Loss: 1.191515, Accuracy: 59.05%\n",
      "Batch 69, Loss: 1.146358, Accuracy: 59.10%\n",
      "Batch 70, Loss: 1.121135, Accuracy: 59.11%\n",
      "Batch 71, Loss: 1.121292, Accuracy: 59.13%\n",
      "Batch 72, Loss: 1.158347, Accuracy: 59.11%\n",
      "Batch 73, Loss: 1.177536, Accuracy: 59.03%\n",
      "Batch 74, Loss: 1.138462, Accuracy: 59.10%\n",
      "Batch 75, Loss: 1.116735, Accuracy: 59.10%\n",
      "Batch 76, Loss: 1.158704, Accuracy: 59.09%\n",
      "Batch 77, Loss: 1.207782, Accuracy: 58.99%\n",
      "Batch 78, Loss: 1.182774, Accuracy: 58.95%\n",
      "Batch 79, Loss: 1.154141, Accuracy: 58.96%\n",
      "Batch 80, Loss: 1.094334, Accuracy: 59.04%\n",
      "Batch 81, Loss: 1.131413, Accuracy: 59.05%\n",
      "Batch 82, Loss: 1.142743, Accuracy: 59.07%\n",
      "Batch 83, Loss: 1.157533, Accuracy: 59.05%\n",
      "Batch 84, Loss: 1.131360, Accuracy: 59.08%\n",
      "Batch 85, Loss: 1.202468, Accuracy: 58.97%\n",
      "Batch 86, Loss: 1.159809, Accuracy: 58.92%\n",
      "Batch 87, Loss: 1.204603, Accuracy: 58.84%\n",
      "Batch 88, Loss: 1.133173, Accuracy: 58.82%\n",
      "Batch 89, Loss: 1.175853, Accuracy: 58.81%\n",
      "Batch 90, Loss: 1.120301, Accuracy: 58.84%\n",
      "Batch 91, Loss: 1.119472, Accuracy: 58.93%\n",
      "Batch 92, Loss: 1.066087, Accuracy: 59.05%\n",
      "Batch 93, Loss: 1.093880, Accuracy: 59.09%\n",
      "Batch 94, Loss: 1.143689, Accuracy: 59.06%\n",
      "Batch 95, Loss: 1.063351, Accuracy: 59.14%\n",
      "Batch 96, Loss: 1.187736, Accuracy: 59.10%\n",
      "Batch 97, Loss: 1.095557, Accuracy: 59.13%\n",
      "Batch 98, Loss: 1.134143, Accuracy: 59.18%\n",
      "Batch 99, Loss: 1.147503, Accuracy: 59.20%\n",
      "Batch 100, Loss: 1.176332, Accuracy: 59.17%\n",
      "Batch 101, Loss: 1.139625, Accuracy: 59.16%\n",
      "Batch 102, Loss: 1.250643, Accuracy: 59.02%\n",
      "Batch 103, Loss: 1.237476, Accuracy: 58.92%\n",
      "Batch 104, Loss: 1.180992, Accuracy: 58.91%\n",
      "Batch 105, Loss: 1.153288, Accuracy: 58.94%\n",
      "Batch 106, Loss: 1.071615, Accuracy: 59.01%\n",
      "Batch 107, Loss: 1.204959, Accuracy: 58.94%\n",
      "Batch 108, Loss: 1.104233, Accuracy: 58.98%\n",
      "Batch 109, Loss: 1.182490, Accuracy: 58.94%\n",
      "Batch 110, Loss: 1.115041, Accuracy: 58.93%\n",
      "Batch 111, Loss: 1.134350, Accuracy: 58.95%\n",
      "Batch 112, Loss: 1.200773, Accuracy: 58.86%\n",
      "Batch 113, Loss: 1.079554, Accuracy: 58.95%\n",
      "Batch 114, Loss: 1.231561, Accuracy: 58.90%\n",
      "Batch 115, Loss: 1.175990, Accuracy: 58.87%\n",
      "Batch 116, Loss: 1.130880, Accuracy: 58.92%\n",
      "Batch 117, Loss: 1.125774, Accuracy: 58.92%\n",
      "Batch 118, Loss: 1.154790, Accuracy: 58.91%\n",
      "Batch 119, Loss: 1.091232, Accuracy: 58.97%\n",
      "Batch 120, Loss: 1.117622, Accuracy: 59.01%\n",
      "Batch 121, Loss: 1.176182, Accuracy: 58.97%\n",
      "Batch 122, Loss: 1.171948, Accuracy: 58.94%\n",
      "Batch 123, Loss: 1.202387, Accuracy: 58.90%\n",
      "Batch 124, Loss: 1.154904, Accuracy: 58.90%\n",
      "Batch 125, Loss: 1.117954, Accuracy: 58.94%\n",
      "Batch 126, Loss: 1.054263, Accuracy: 59.03%\n",
      "Batch 127, Loss: 1.217147, Accuracy: 58.96%\n",
      "Batch 128, Loss: 1.174093, Accuracy: 58.94%\n",
      "Batch 129, Loss: 1.115777, Accuracy: 58.98%\n",
      "Batch 130, Loss: 1.210426, Accuracy: 58.88%\n",
      "Batch 131, Loss: 1.171559, Accuracy: 58.87%\n",
      "Batch 132, Loss: 1.121449, Accuracy: 58.90%\n",
      "Batch 133, Loss: 1.053352, Accuracy: 58.99%\n",
      "Batch 134, Loss: 1.116049, Accuracy: 59.01%\n",
      "Batch 135, Loss: 1.151463, Accuracy: 58.99%\n",
      "Batch 136, Loss: 1.067491, Accuracy: 59.06%\n",
      "Batch 137, Loss: 1.182559, Accuracy: 59.02%\n",
      "Batch 138, Loss: 1.212584, Accuracy: 58.97%\n",
      "Batch 139, Loss: 1.075941, Accuracy: 59.03%\n",
      "Batch 140, Loss: 1.178578, Accuracy: 59.01%\n",
      "Batch 141, Loss: 1.111274, Accuracy: 59.04%\n",
      "Batch 142, Loss: 1.040268, Accuracy: 59.13%\n",
      "Batch 143, Loss: 1.134463, Accuracy: 59.12%\n",
      "Batch 144, Loss: 1.160136, Accuracy: 59.13%\n",
      "Batch 145, Loss: 1.131139, Accuracy: 59.15%\n",
      "Batch 146, Loss: 1.044065, Accuracy: 59.23%\n",
      "Batch 147, Loss: 1.178015, Accuracy: 59.18%\n",
      "Batch 148, Loss: 1.174364, Accuracy: 59.15%\n",
      "Batch 149, Loss: 1.152729, Accuracy: 59.15%\n",
      "Batch 150, Loss: 1.075304, Accuracy: 59.23%\n",
      "Batch 151, Loss: 1.094383, Accuracy: 59.26%\n",
      "Batch 152, Loss: 1.103197, Accuracy: 59.29%\n",
      "Batch 153, Loss: 1.170391, Accuracy: 59.28%\n",
      "Batch 154, Loss: 1.204811, Accuracy: 59.25%\n",
      "Batch 155, Loss: 1.175032, Accuracy: 59.22%\n",
      "Batch 156, Loss: 1.214462, Accuracy: 59.16%\n",
      "Batch 157, Loss: 1.139305, Accuracy: 59.16%\n",
      "Batch 158, Loss: 1.148311, Accuracy: 59.15%\n",
      "Batch 159, Loss: 1.137249, Accuracy: 59.17%\n",
      "Batch 160, Loss: 1.153773, Accuracy: 59.17%\n",
      "Batch 161, Loss: 1.195021, Accuracy: 59.14%\n",
      "Batch 162, Loss: 1.169643, Accuracy: 59.10%\n",
      "Batch 163, Loss: 1.165763, Accuracy: 59.08%\n",
      "Batch 164, Loss: 1.113149, Accuracy: 59.08%\n",
      "Batch 165, Loss: 1.221231, Accuracy: 59.02%\n",
      "Batch 166, Loss: 1.178987, Accuracy: 58.98%\n",
      "Batch 167, Loss: 1.152561, Accuracy: 58.96%\n",
      "Batch 168, Loss: 1.176374, Accuracy: 58.95%\n",
      "Batch 169, Loss: 1.112810, Accuracy: 59.00%\n",
      "Batch 170, Loss: 1.131957, Accuracy: 58.99%\n",
      "Batch 171, Loss: 1.198098, Accuracy: 58.97%\n",
      "Batch 172, Loss: 1.156118, Accuracy: 58.98%\n",
      "Batch 173, Loss: 1.163839, Accuracy: 59.00%\n",
      "Batch 174, Loss: 1.145357, Accuracy: 59.00%\n",
      "Batch 175, Loss: 1.113717, Accuracy: 59.03%\n",
      "Batch 176, Loss: 1.081840, Accuracy: 59.07%\n",
      "Batch 177, Loss: 1.143667, Accuracy: 59.07%\n",
      "Batch 178, Loss: 1.210472, Accuracy: 59.03%\n",
      "Batch 179, Loss: 1.202304, Accuracy: 59.00%\n",
      "Batch 180, Loss: 1.214037, Accuracy: 58.95%\n",
      "Batch 181, Loss: 1.149882, Accuracy: 58.93%\n",
      "Batch 182, Loss: 1.072520, Accuracy: 58.98%\n",
      "Batch 183, Loss: 1.219275, Accuracy: 58.92%\n",
      "Batch 184, Loss: 1.077854, Accuracy: 58.97%\n",
      "Batch 185, Loss: 1.119953, Accuracy: 58.96%\n",
      "Batch 186, Loss: 1.169623, Accuracy: 58.95%\n",
      "Batch 187, Loss: 1.154275, Accuracy: 58.95%\n",
      "Batch 188, Loss: 1.171466, Accuracy: 58.95%\n",
      "Batch 189, Loss: 1.128360, Accuracy: 58.98%\n",
      "Batch 190, Loss: 1.085155, Accuracy: 59.01%\n",
      "Batch 191, Loss: 1.117696, Accuracy: 59.03%\n",
      "Batch 192, Loss: 1.166215, Accuracy: 59.01%\n",
      "Batch 193, Loss: 1.104296, Accuracy: 59.03%\n",
      "Batch 194, Loss: 1.143288, Accuracy: 59.04%\n",
      "Batch 195, Loss: 1.160660, Accuracy: 59.01%\n",
      "Batch 196, Loss: 1.185825, Accuracy: 59.00%\n",
      "Batch 197, Loss: 1.240002, Accuracy: 58.95%\n",
      "Batch 198, Loss: 1.076739, Accuracy: 58.98%\n",
      "Batch 199, Loss: 1.093150, Accuracy: 59.02%\n",
      "Batch 200, Loss: 1.167443, Accuracy: 59.00%\n",
      "Batch 201, Loss: 1.175557, Accuracy: 58.99%\n",
      "Batch 202, Loss: 1.124999, Accuracy: 59.00%\n",
      "Batch 203, Loss: 1.080917, Accuracy: 59.04%\n",
      "Batch 204, Loss: 1.144950, Accuracy: 59.02%\n",
      "Batch 205, Loss: 1.129809, Accuracy: 59.03%\n",
      "Batch 206, Loss: 1.119843, Accuracy: 59.06%\n",
      "Batch 207, Loss: 1.087568, Accuracy: 59.10%\n",
      "Batch 208, Loss: 1.095571, Accuracy: 59.13%\n",
      "Batch 209, Loss: 1.194893, Accuracy: 59.10%\n",
      "Batch 210, Loss: 1.159814, Accuracy: 59.10%\n",
      "Batch 211, Loss: 1.136761, Accuracy: 59.12%\n",
      "Batch 212, Loss: 1.162816, Accuracy: 59.10%\n",
      "Batch 213, Loss: 1.189234, Accuracy: 59.08%\n",
      "Training - Epoch 9, Loss: 1.146515, Accuracy: 59.08%\n",
      "Validation Batch 1, Loss: 1.333737, Accuracy: 37.50%\n",
      "Validation Batch 2, Loss: 1.331201, Accuracy: 38.28%\n",
      "Validation Batch 3, Loss: 1.419020, Accuracy: 35.42%\n",
      "Validation Batch 4, Loss: 1.306713, Accuracy: 36.72%\n",
      "Validation Batch 5, Loss: 1.404536, Accuracy: 35.62%\n",
      "Validation Batch 6, Loss: 1.459341, Accuracy: 33.59%\n",
      "Validation Batch 7, Loss: 1.362508, Accuracy: 33.71%\n",
      "Validation Batch 8, Loss: 1.356620, Accuracy: 33.98%\n",
      "Validation Batch 9, Loss: 1.390707, Accuracy: 33.85%\n",
      "Validation Batch 10, Loss: 1.408753, Accuracy: 33.44%\n",
      "Validation Batch 11, Loss: 1.353510, Accuracy: 33.66%\n",
      "Validation Batch 12, Loss: 1.342986, Accuracy: 33.85%\n",
      "Validation Batch 13, Loss: 1.394723, Accuracy: 33.65%\n",
      "Validation Batch 14, Loss: 1.265053, Accuracy: 34.49%\n",
      "Validation Batch 15, Loss: 1.336981, Accuracy: 34.69%\n",
      "Validation Batch 16, Loss: 1.432885, Accuracy: 34.18%\n",
      "Validation Batch 17, Loss: 1.379609, Accuracy: 34.19%\n",
      "Validation Batch 18, Loss: 1.364382, Accuracy: 34.20%\n",
      "Validation Batch 19, Loss: 1.374968, Accuracy: 34.21%\n",
      "Validation Batch 20, Loss: 1.377861, Accuracy: 34.14%\n",
      "Validation Batch 21, Loss: 1.355514, Accuracy: 34.23%\n",
      "Validation Batch 22, Loss: 1.367472, Accuracy: 34.23%\n",
      "Validation Batch 23, Loss: 1.375490, Accuracy: 34.24%\n",
      "Validation Batch 24, Loss: 1.423841, Accuracy: 33.98%\n",
      "Validation Batch 25, Loss: 1.320910, Accuracy: 34.19%\n",
      "Validation Batch 26, Loss: 1.289598, Accuracy: 34.50%\n",
      "Validation Batch 27, Loss: 1.379444, Accuracy: 34.47%\n",
      "Validation - Epoch 9, Loss: 1.366976, Accuracy: 34.47%\n",
      "Patienceâ€”8\n",
      "Epoch 10\n",
      "Batch 1, Loss: 1.129697, Accuracy: 59.38%\n",
      "Batch 2, Loss: 1.113430, Accuracy: 62.50%\n",
      "Batch 3, Loss: 1.084641, Accuracy: 63.54%\n",
      "Batch 4, Loss: 1.198053, Accuracy: 60.55%\n",
      "Batch 5, Loss: 1.180104, Accuracy: 59.69%\n",
      "Batch 6, Loss: 1.107633, Accuracy: 60.42%\n",
      "Batch 7, Loss: 1.099450, Accuracy: 61.16%\n",
      "Batch 8, Loss: 1.067437, Accuracy: 62.11%\n",
      "Batch 9, Loss: 1.100006, Accuracy: 62.67%\n",
      "Batch 10, Loss: 1.132213, Accuracy: 62.66%\n",
      "Batch 11, Loss: 1.083865, Accuracy: 62.78%\n",
      "Batch 12, Loss: 1.201288, Accuracy: 61.98%\n",
      "Batch 13, Loss: 1.152202, Accuracy: 61.54%\n",
      "Batch 14, Loss: 1.093509, Accuracy: 61.83%\n",
      "Batch 15, Loss: 1.153562, Accuracy: 61.46%\n",
      "Batch 16, Loss: 1.108931, Accuracy: 61.62%\n",
      "Batch 17, Loss: 1.150263, Accuracy: 61.49%\n",
      "Batch 18, Loss: 1.120487, Accuracy: 61.46%\n",
      "Batch 19, Loss: 1.125615, Accuracy: 61.35%\n",
      "Batch 20, Loss: 1.027635, Accuracy: 61.88%\n",
      "Batch 21, Loss: 1.152295, Accuracy: 61.76%\n",
      "Batch 22, Loss: 1.107591, Accuracy: 61.93%\n",
      "Batch 23, Loss: 1.176751, Accuracy: 61.75%\n",
      "Batch 24, Loss: 1.143728, Accuracy: 61.52%\n",
      "Batch 25, Loss: 1.119507, Accuracy: 61.56%\n",
      "Batch 26, Loss: 1.077939, Accuracy: 61.72%\n",
      "Batch 27, Loss: 1.142968, Accuracy: 61.75%\n",
      "Batch 28, Loss: 1.179610, Accuracy: 61.44%\n",
      "Batch 29, Loss: 1.177811, Accuracy: 61.21%\n",
      "Batch 30, Loss: 1.177954, Accuracy: 61.04%\n",
      "Batch 31, Loss: 1.176529, Accuracy: 60.99%\n",
      "Batch 32, Loss: 1.125512, Accuracy: 61.04%\n",
      "Batch 33, Loss: 1.154748, Accuracy: 61.03%\n",
      "Batch 34, Loss: 1.108433, Accuracy: 61.12%\n",
      "Batch 35, Loss: 1.176889, Accuracy: 60.94%\n",
      "Batch 36, Loss: 1.081285, Accuracy: 61.11%\n",
      "Batch 37, Loss: 1.062917, Accuracy: 61.36%\n",
      "Batch 38, Loss: 1.122124, Accuracy: 61.39%\n",
      "Batch 39, Loss: 1.205655, Accuracy: 61.10%\n",
      "Batch 40, Loss: 1.127338, Accuracy: 61.17%\n",
      "Batch 41, Loss: 1.153295, Accuracy: 61.05%\n",
      "Batch 42, Loss: 1.158951, Accuracy: 60.94%\n",
      "Batch 43, Loss: 1.157781, Accuracy: 60.90%\n",
      "Batch 44, Loss: 1.160157, Accuracy: 60.80%\n",
      "Batch 45, Loss: 1.211107, Accuracy: 60.62%\n",
      "Batch 46, Loss: 1.148298, Accuracy: 60.56%\n",
      "Batch 47, Loss: 1.154104, Accuracy: 60.51%\n",
      "Batch 48, Loss: 1.087659, Accuracy: 60.61%\n",
      "Batch 49, Loss: 1.216809, Accuracy: 60.43%\n",
      "Batch 50, Loss: 1.102695, Accuracy: 60.53%\n",
      "Batch 51, Loss: 1.120491, Accuracy: 60.57%\n",
      "Batch 52, Loss: 1.152822, Accuracy: 60.52%\n",
      "Batch 53, Loss: 1.138100, Accuracy: 60.50%\n",
      "Batch 54, Loss: 1.150168, Accuracy: 60.47%\n",
      "Batch 55, Loss: 1.113255, Accuracy: 60.48%\n",
      "Batch 56, Loss: 1.177354, Accuracy: 60.41%\n",
      "Batch 57, Loss: 1.099230, Accuracy: 60.47%\n",
      "Batch 58, Loss: 1.109122, Accuracy: 60.56%\n",
      "Batch 59, Loss: 1.162817, Accuracy: 60.54%\n",
      "Batch 60, Loss: 1.163766, Accuracy: 60.42%\n",
      "Batch 61, Loss: 1.154898, Accuracy: 60.35%\n",
      "Batch 62, Loss: 1.194699, Accuracy: 60.28%\n",
      "Batch 63, Loss: 1.085183, Accuracy: 60.37%\n",
      "Batch 64, Loss: 1.131502, Accuracy: 60.40%\n",
      "Batch 65, Loss: 1.054300, Accuracy: 60.55%\n",
      "Batch 66, Loss: 1.153367, Accuracy: 60.49%\n",
      "Batch 67, Loss: 1.090761, Accuracy: 60.52%\n",
      "Batch 68, Loss: 1.124668, Accuracy: 60.52%\n",
      "Batch 69, Loss: 1.056284, Accuracy: 60.62%\n",
      "Batch 70, Loss: 1.111366, Accuracy: 60.65%\n",
      "Batch 71, Loss: 1.058463, Accuracy: 60.78%\n",
      "Batch 72, Loss: 1.113273, Accuracy: 60.81%\n",
      "Batch 73, Loss: 1.078718, Accuracy: 60.92%\n",
      "Batch 74, Loss: 1.140592, Accuracy: 60.90%\n",
      "Batch 75, Loss: 1.176628, Accuracy: 60.79%\n",
      "Batch 76, Loss: 1.253141, Accuracy: 60.63%\n",
      "Batch 77, Loss: 1.152990, Accuracy: 60.59%\n",
      "Batch 78, Loss: 1.133260, Accuracy: 60.60%\n",
      "Batch 79, Loss: 1.083827, Accuracy: 60.68%\n",
      "Batch 80, Loss: 1.130666, Accuracy: 60.70%\n",
      "Batch 81, Loss: 1.195443, Accuracy: 60.59%\n",
      "Batch 82, Loss: 1.109252, Accuracy: 60.61%\n",
      "Batch 83, Loss: 1.121637, Accuracy: 60.66%\n",
      "Batch 84, Loss: 1.198643, Accuracy: 60.53%\n",
      "Batch 85, Loss: 1.198039, Accuracy: 60.46%\n",
      "Batch 86, Loss: 1.174874, Accuracy: 60.41%\n",
      "Batch 87, Loss: 1.104489, Accuracy: 60.45%\n",
      "Batch 88, Loss: 1.088623, Accuracy: 60.51%\n",
      "Batch 89, Loss: 1.193393, Accuracy: 60.46%\n",
      "Batch 90, Loss: 1.121984, Accuracy: 60.45%\n",
      "Batch 91, Loss: 1.123844, Accuracy: 60.44%\n",
      "Batch 92, Loss: 1.115746, Accuracy: 60.43%\n",
      "Batch 93, Loss: 1.150623, Accuracy: 60.38%\n",
      "Batch 94, Loss: 1.086902, Accuracy: 60.46%\n",
      "Batch 95, Loss: 1.128756, Accuracy: 60.46%\n",
      "Batch 96, Loss: 1.233360, Accuracy: 60.38%\n",
      "Batch 97, Loss: 1.221908, Accuracy: 60.26%\n",
      "Batch 98, Loss: 1.158600, Accuracy: 60.24%\n",
      "Batch 99, Loss: 1.128357, Accuracy: 60.26%\n",
      "Batch 100, Loss: 1.135348, Accuracy: 60.27%\n",
      "Batch 101, Loss: 1.090897, Accuracy: 60.33%\n",
      "Batch 102, Loss: 1.084744, Accuracy: 60.42%\n",
      "Batch 103, Loss: 1.203574, Accuracy: 60.33%\n",
      "Batch 104, Loss: 1.195050, Accuracy: 60.26%\n",
      "Batch 105, Loss: 1.129712, Accuracy: 60.24%\n",
      "Batch 106, Loss: 1.096939, Accuracy: 60.30%\n",
      "Batch 107, Loss: 1.131431, Accuracy: 60.27%\n",
      "Batch 108, Loss: 1.063475, Accuracy: 60.34%\n",
      "Batch 109, Loss: 1.088074, Accuracy: 60.39%\n",
      "Batch 110, Loss: 1.169334, Accuracy: 60.36%\n",
      "Batch 111, Loss: 1.143458, Accuracy: 60.36%\n",
      "Batch 112, Loss: 1.133281, Accuracy: 60.34%\n",
      "Batch 113, Loss: 1.182913, Accuracy: 60.30%\n",
      "Batch 114, Loss: 1.123169, Accuracy: 60.32%\n",
      "Batch 115, Loss: 1.199816, Accuracy: 60.24%\n",
      "Batch 116, Loss: 1.180859, Accuracy: 60.18%\n",
      "Batch 117, Loss: 1.197844, Accuracy: 60.15%\n",
      "Batch 118, Loss: 1.179941, Accuracy: 60.10%\n",
      "Batch 119, Loss: 1.120387, Accuracy: 60.12%\n",
      "Batch 120, Loss: 1.092269, Accuracy: 60.18%\n",
      "Batch 121, Loss: 1.053425, Accuracy: 60.24%\n",
      "Batch 122, Loss: 1.152604, Accuracy: 60.21%\n",
      "Batch 123, Loss: 1.041193, Accuracy: 60.32%\n",
      "Batch 124, Loss: 1.166578, Accuracy: 60.29%\n",
      "Batch 125, Loss: 1.150585, Accuracy: 60.30%\n",
      "Batch 126, Loss: 1.224537, Accuracy: 60.23%\n",
      "Batch 127, Loss: 1.097783, Accuracy: 60.27%\n",
      "Batch 128, Loss: 1.088708, Accuracy: 60.30%\n",
      "Batch 129, Loss: 1.113792, Accuracy: 60.30%\n",
      "Batch 130, Loss: 1.132414, Accuracy: 60.30%\n",
      "Batch 131, Loss: 1.176594, Accuracy: 60.26%\n",
      "Batch 132, Loss: 1.089679, Accuracy: 60.31%\n",
      "Batch 133, Loss: 1.084971, Accuracy: 60.35%\n",
      "Batch 134, Loss: 1.119237, Accuracy: 60.34%\n",
      "Batch 135, Loss: 1.132569, Accuracy: 60.34%\n",
      "Batch 136, Loss: 1.138338, Accuracy: 60.32%\n",
      "Batch 137, Loss: 1.171998, Accuracy: 60.25%\n",
      "Batch 138, Loss: 1.175625, Accuracy: 60.22%\n",
      "Batch 139, Loss: 1.121862, Accuracy: 60.24%\n",
      "Batch 140, Loss: 1.183793, Accuracy: 60.21%\n",
      "Batch 141, Loss: 1.167781, Accuracy: 60.17%\n",
      "Batch 142, Loss: 1.194527, Accuracy: 60.11%\n",
      "Batch 143, Loss: 1.161523, Accuracy: 60.07%\n",
      "Batch 144, Loss: 1.171650, Accuracy: 60.07%\n",
      "Batch 145, Loss: 1.163777, Accuracy: 60.06%\n",
      "Batch 146, Loss: 1.181020, Accuracy: 60.02%\n",
      "Batch 147, Loss: 1.121259, Accuracy: 60.03%\n",
      "Batch 148, Loss: 1.100330, Accuracy: 60.08%\n",
      "Batch 149, Loss: 1.181532, Accuracy: 60.06%\n",
      "Batch 150, Loss: 1.151814, Accuracy: 60.03%\n",
      "Batch 151, Loss: 1.102740, Accuracy: 60.05%\n",
      "Batch 152, Loss: 1.068375, Accuracy: 60.13%\n",
      "Batch 153, Loss: 1.142753, Accuracy: 60.12%\n",
      "Batch 154, Loss: 1.183468, Accuracy: 60.09%\n",
      "Batch 155, Loss: 1.071490, Accuracy: 60.14%\n",
      "Batch 156, Loss: 1.112363, Accuracy: 60.15%\n",
      "Batch 157, Loss: 1.118464, Accuracy: 60.17%\n",
      "Batch 158, Loss: 1.107400, Accuracy: 60.17%\n",
      "Batch 159, Loss: 1.111805, Accuracy: 60.18%\n",
      "Batch 160, Loss: 1.151552, Accuracy: 60.16%\n",
      "Batch 161, Loss: 1.079705, Accuracy: 60.20%\n",
      "Batch 162, Loss: 1.120257, Accuracy: 60.22%\n",
      "Batch 163, Loss: 1.106903, Accuracy: 60.26%\n",
      "Batch 164, Loss: 1.127070, Accuracy: 60.26%\n",
      "Batch 165, Loss: 1.247943, Accuracy: 60.19%\n",
      "Batch 166, Loss: 1.214443, Accuracy: 60.13%\n",
      "Batch 167, Loss: 1.160276, Accuracy: 60.10%\n",
      "Batch 168, Loss: 1.079013, Accuracy: 60.15%\n",
      "Batch 169, Loss: 1.163016, Accuracy: 60.11%\n",
      "Batch 170, Loss: 1.174280, Accuracy: 60.09%\n",
      "Batch 171, Loss: 1.230196, Accuracy: 60.03%\n",
      "Batch 172, Loss: 1.108079, Accuracy: 60.05%\n",
      "Batch 173, Loss: 1.129085, Accuracy: 60.05%\n",
      "Batch 174, Loss: 1.084285, Accuracy: 60.08%\n",
      "Batch 175, Loss: 1.214309, Accuracy: 60.04%\n",
      "Batch 176, Loss: 1.079707, Accuracy: 60.09%\n",
      "Batch 177, Loss: 1.085786, Accuracy: 60.13%\n",
      "Batch 178, Loss: 1.129338, Accuracy: 60.12%\n",
      "Batch 179, Loss: 1.092013, Accuracy: 60.14%\n",
      "Batch 180, Loss: 1.122756, Accuracy: 60.14%\n",
      "Batch 181, Loss: 1.084541, Accuracy: 60.18%\n",
      "Batch 182, Loss: 1.117148, Accuracy: 60.18%\n",
      "Batch 183, Loss: 1.105228, Accuracy: 60.19%\n",
      "Batch 184, Loss: 1.138947, Accuracy: 60.20%\n",
      "Batch 185, Loss: 1.141532, Accuracy: 60.19%\n",
      "Batch 186, Loss: 1.233504, Accuracy: 60.12%\n",
      "Batch 187, Loss: 1.151510, Accuracy: 60.10%\n",
      "Batch 188, Loss: 1.162073, Accuracy: 60.10%\n",
      "Batch 189, Loss: 1.092060, Accuracy: 60.13%\n",
      "Batch 190, Loss: 1.127625, Accuracy: 60.12%\n",
      "Batch 191, Loss: 1.161393, Accuracy: 60.12%\n",
      "Batch 192, Loss: 1.185066, Accuracy: 60.08%\n",
      "Batch 193, Loss: 1.155873, Accuracy: 60.09%\n",
      "Batch 194, Loss: 1.209481, Accuracy: 60.07%\n",
      "Batch 195, Loss: 1.035846, Accuracy: 60.12%\n",
      "Batch 196, Loss: 1.225913, Accuracy: 60.06%\n",
      "Batch 197, Loss: 1.117003, Accuracy: 60.07%\n",
      "Batch 198, Loss: 1.109179, Accuracy: 60.09%\n",
      "Batch 199, Loss: 1.234446, Accuracy: 60.05%\n",
      "Batch 200, Loss: 1.044595, Accuracy: 60.11%\n",
      "Batch 201, Loss: 1.102751, Accuracy: 60.11%\n",
      "Batch 202, Loss: 1.158209, Accuracy: 60.10%\n",
      "Batch 203, Loss: 1.174555, Accuracy: 60.08%\n",
      "Batch 204, Loss: 1.143625, Accuracy: 60.08%\n",
      "Batch 205, Loss: 1.170526, Accuracy: 60.06%\n",
      "Batch 206, Loss: 1.073480, Accuracy: 60.10%\n",
      "Batch 207, Loss: 1.140885, Accuracy: 60.09%\n",
      "Batch 208, Loss: 1.040074, Accuracy: 60.13%\n",
      "Batch 209, Loss: 1.069343, Accuracy: 60.17%\n",
      "Batch 210, Loss: 1.194920, Accuracy: 60.13%\n",
      "Batch 211, Loss: 1.239530, Accuracy: 60.06%\n",
      "Batch 212, Loss: 1.234641, Accuracy: 60.00%\n",
      "Batch 213, Loss: 1.113658, Accuracy: 60.01%\n",
      "Training - Epoch 10, Loss: 1.137374, Accuracy: 60.01%\n",
      "Validation Batch 1, Loss: 1.328506, Accuracy: 37.50%\n",
      "Validation Batch 2, Loss: 1.328023, Accuracy: 38.28%\n",
      "Validation Batch 3, Loss: 1.415083, Accuracy: 35.42%\n",
      "Validation Batch 4, Loss: 1.300825, Accuracy: 36.72%\n",
      "Validation Batch 5, Loss: 1.400916, Accuracy: 35.62%\n",
      "Validation Batch 6, Loss: 1.447803, Accuracy: 33.59%\n",
      "Validation Batch 7, Loss: 1.355651, Accuracy: 33.71%\n",
      "Validation Batch 8, Loss: 1.350438, Accuracy: 33.98%\n",
      "Validation Batch 9, Loss: 1.388530, Accuracy: 33.85%\n",
      "Validation Batch 10, Loss: 1.403443, Accuracy: 33.44%\n",
      "Validation Batch 11, Loss: 1.347760, Accuracy: 33.66%\n",
      "Validation Batch 12, Loss: 1.333597, Accuracy: 33.85%\n",
      "Validation Batch 13, Loss: 1.388016, Accuracy: 33.65%\n",
      "Validation Batch 14, Loss: 1.261480, Accuracy: 34.49%\n",
      "Validation Batch 15, Loss: 1.333501, Accuracy: 34.69%\n",
      "Validation Batch 16, Loss: 1.426266, Accuracy: 34.18%\n",
      "Validation Batch 17, Loss: 1.377180, Accuracy: 34.19%\n",
      "Validation Batch 18, Loss: 1.357031, Accuracy: 34.20%\n",
      "Validation Batch 19, Loss: 1.370346, Accuracy: 34.21%\n",
      "Validation Batch 20, Loss: 1.369710, Accuracy: 34.14%\n",
      "Validation Batch 21, Loss: 1.353259, Accuracy: 34.23%\n",
      "Validation Batch 22, Loss: 1.364112, Accuracy: 34.23%\n",
      "Validation Batch 23, Loss: 1.371769, Accuracy: 34.24%\n",
      "Validation Batch 24, Loss: 1.415397, Accuracy: 33.98%\n",
      "Validation Batch 25, Loss: 1.315619, Accuracy: 34.19%\n",
      "Validation Batch 26, Loss: 1.281048, Accuracy: 34.50%\n",
      "Validation Batch 27, Loss: 1.376861, Accuracy: 34.47%\n",
      "Validation - Epoch 10, Loss: 1.361562, Accuracy: 34.47%\n",
      "Patienceâ€”9\n",
      "Epoch 11\n",
      "Batch 1, Loss: 1.154900, Accuracy: 59.38%\n",
      "Batch 2, Loss: 1.088863, Accuracy: 62.50%\n",
      "Batch 3, Loss: 1.122317, Accuracy: 61.46%\n",
      "Batch 4, Loss: 1.125113, Accuracy: 62.11%\n",
      "Batch 5, Loss: 1.072878, Accuracy: 63.12%\n",
      "Batch 6, Loss: 1.089832, Accuracy: 63.54%\n",
      "Batch 7, Loss: 1.190318, Accuracy: 62.05%\n",
      "Batch 8, Loss: 1.067909, Accuracy: 62.70%\n",
      "Batch 9, Loss: 1.180058, Accuracy: 61.81%\n",
      "Batch 10, Loss: 1.089668, Accuracy: 62.19%\n",
      "Batch 11, Loss: 1.219837, Accuracy: 61.08%\n",
      "Batch 12, Loss: 1.222127, Accuracy: 60.16%\n",
      "Batch 13, Loss: 1.063332, Accuracy: 60.94%\n",
      "Batch 14, Loss: 1.050700, Accuracy: 61.38%\n",
      "Batch 15, Loss: 1.108587, Accuracy: 61.67%\n",
      "Batch 16, Loss: 1.048724, Accuracy: 62.30%\n",
      "Batch 17, Loss: 1.147450, Accuracy: 62.13%\n",
      "Batch 18, Loss: 1.109565, Accuracy: 62.07%\n",
      "Batch 19, Loss: 1.181075, Accuracy: 61.68%\n",
      "Batch 20, Loss: 1.096650, Accuracy: 61.95%\n",
      "Batch 21, Loss: 1.064484, Accuracy: 62.13%\n",
      "Batch 22, Loss: 1.122721, Accuracy: 61.86%\n",
      "Batch 23, Loss: 1.118969, Accuracy: 61.89%\n",
      "Batch 24, Loss: 1.186295, Accuracy: 61.59%\n",
      "Batch 25, Loss: 1.166340, Accuracy: 61.44%\n",
      "Batch 26, Loss: 1.055294, Accuracy: 61.78%\n",
      "Batch 27, Loss: 1.102605, Accuracy: 61.75%\n",
      "Batch 28, Loss: 1.148194, Accuracy: 61.66%\n",
      "Batch 29, Loss: 1.135095, Accuracy: 61.42%\n",
      "Batch 30, Loss: 1.145684, Accuracy: 61.30%\n",
      "Batch 31, Loss: 1.220933, Accuracy: 60.94%\n",
      "Batch 32, Loss: 1.081224, Accuracy: 61.08%\n",
      "Batch 33, Loss: 1.096555, Accuracy: 61.22%\n",
      "Batch 34, Loss: 1.100067, Accuracy: 61.26%\n",
      "Batch 35, Loss: 1.097546, Accuracy: 61.38%\n",
      "Batch 36, Loss: 1.151458, Accuracy: 61.37%\n",
      "Batch 37, Loss: 1.188865, Accuracy: 61.11%\n",
      "Batch 38, Loss: 1.180384, Accuracy: 60.94%\n",
      "Batch 39, Loss: 1.114426, Accuracy: 61.02%\n",
      "Batch 40, Loss: 1.069871, Accuracy: 61.21%\n",
      "Batch 41, Loss: 1.109536, Accuracy: 61.32%\n",
      "Batch 42, Loss: 1.121976, Accuracy: 61.35%\n",
      "Batch 43, Loss: 1.131261, Accuracy: 61.30%\n",
      "Batch 44, Loss: 1.179378, Accuracy: 61.08%\n",
      "Batch 45, Loss: 1.144479, Accuracy: 61.01%\n",
      "Batch 46, Loss: 1.196182, Accuracy: 60.84%\n",
      "Batch 47, Loss: 1.213341, Accuracy: 60.57%\n",
      "Batch 48, Loss: 1.173122, Accuracy: 60.48%\n",
      "Batch 49, Loss: 1.140318, Accuracy: 60.49%\n",
      "Batch 50, Loss: 1.142972, Accuracy: 60.44%\n",
      "Batch 51, Loss: 1.232399, Accuracy: 60.23%\n",
      "Batch 52, Loss: 1.108887, Accuracy: 60.31%\n",
      "Batch 53, Loss: 1.043994, Accuracy: 60.50%\n",
      "Batch 54, Loss: 1.078588, Accuracy: 60.65%\n",
      "Batch 55, Loss: 1.186726, Accuracy: 60.54%\n",
      "Batch 56, Loss: 1.172213, Accuracy: 60.46%\n",
      "Batch 57, Loss: 1.164441, Accuracy: 60.39%\n",
      "Batch 58, Loss: 1.064109, Accuracy: 60.56%\n",
      "Batch 59, Loss: 1.097678, Accuracy: 60.65%\n",
      "Batch 60, Loss: 1.180935, Accuracy: 60.55%\n",
      "Batch 61, Loss: 1.161083, Accuracy: 60.53%\n",
      "Batch 62, Loss: 1.137738, Accuracy: 60.51%\n",
      "Batch 63, Loss: 1.167294, Accuracy: 60.49%\n",
      "Batch 64, Loss: 1.139661, Accuracy: 60.40%\n",
      "Batch 65, Loss: 1.106406, Accuracy: 60.46%\n",
      "Batch 66, Loss: 1.010024, Accuracy: 60.63%\n",
      "Batch 67, Loss: 1.103857, Accuracy: 60.66%\n",
      "Batch 68, Loss: 1.182969, Accuracy: 60.59%\n",
      "Batch 69, Loss: 1.138680, Accuracy: 60.60%\n",
      "Batch 70, Loss: 1.202611, Accuracy: 60.51%\n",
      "Batch 71, Loss: 1.131951, Accuracy: 60.56%\n",
      "Batch 72, Loss: 1.129676, Accuracy: 60.61%\n",
      "Batch 73, Loss: 1.100786, Accuracy: 60.70%\n",
      "Batch 74, Loss: 1.211925, Accuracy: 60.56%\n",
      "Batch 75, Loss: 1.129174, Accuracy: 60.60%\n",
      "Batch 76, Loss: 1.113611, Accuracy: 60.63%\n",
      "Batch 77, Loss: 1.076603, Accuracy: 60.75%\n",
      "Batch 78, Loss: 1.093623, Accuracy: 60.80%\n",
      "Batch 79, Loss: 1.222061, Accuracy: 60.68%\n",
      "Batch 80, Loss: 1.077231, Accuracy: 60.76%\n",
      "Batch 81, Loss: 1.158383, Accuracy: 60.69%\n",
      "Batch 82, Loss: 1.154694, Accuracy: 60.63%\n",
      "Batch 83, Loss: 1.158139, Accuracy: 60.58%\n",
      "Batch 84, Loss: 1.117272, Accuracy: 60.60%\n",
      "Batch 85, Loss: 1.157716, Accuracy: 60.53%\n",
      "Batch 86, Loss: 1.124769, Accuracy: 60.57%\n",
      "Batch 87, Loss: 1.183967, Accuracy: 60.47%\n",
      "Batch 88, Loss: 1.064857, Accuracy: 60.55%\n",
      "Batch 89, Loss: 1.103391, Accuracy: 60.59%\n",
      "Batch 90, Loss: 1.168951, Accuracy: 60.56%\n",
      "Batch 91, Loss: 1.179099, Accuracy: 60.49%\n",
      "Batch 92, Loss: 1.163038, Accuracy: 60.46%\n",
      "Batch 93, Loss: 1.113970, Accuracy: 60.48%\n",
      "Batch 94, Loss: 1.099630, Accuracy: 60.57%\n",
      "Batch 95, Loss: 1.065838, Accuracy: 60.64%\n",
      "Batch 96, Loss: 1.158472, Accuracy: 60.63%\n",
      "Batch 97, Loss: 1.159068, Accuracy: 60.57%\n",
      "Batch 98, Loss: 1.134988, Accuracy: 60.55%\n",
      "Batch 99, Loss: 1.120907, Accuracy: 60.57%\n",
      "Batch 100, Loss: 1.093870, Accuracy: 60.61%\n",
      "Batch 101, Loss: 1.086008, Accuracy: 60.66%\n",
      "Batch 102, Loss: 1.234186, Accuracy: 60.55%\n",
      "Batch 103, Loss: 1.182915, Accuracy: 60.48%\n",
      "Batch 104, Loss: 1.144668, Accuracy: 60.46%\n",
      "Batch 105, Loss: 1.136690, Accuracy: 60.48%\n",
      "Batch 106, Loss: 1.079385, Accuracy: 60.51%\n",
      "Batch 107, Loss: 1.157498, Accuracy: 60.51%\n",
      "Batch 108, Loss: 1.104288, Accuracy: 60.56%\n",
      "Batch 109, Loss: 1.178076, Accuracy: 60.54%\n",
      "Batch 110, Loss: 1.079342, Accuracy: 60.57%\n",
      "Batch 111, Loss: 1.016523, Accuracy: 60.66%\n",
      "Batch 112, Loss: 1.106122, Accuracy: 60.70%\n",
      "Batch 113, Loss: 1.139414, Accuracy: 60.72%\n",
      "Batch 114, Loss: 1.125484, Accuracy: 60.73%\n",
      "Batch 115, Loss: 1.104590, Accuracy: 60.76%\n",
      "Batch 116, Loss: 1.099696, Accuracy: 60.79%\n",
      "Batch 117, Loss: 1.064583, Accuracy: 60.83%\n",
      "Batch 118, Loss: 1.109983, Accuracy: 60.83%\n",
      "Batch 119, Loss: 1.206629, Accuracy: 60.78%\n",
      "Batch 120, Loss: 1.071645, Accuracy: 60.86%\n",
      "Batch 121, Loss: 1.237762, Accuracy: 60.77%\n",
      "Batch 122, Loss: 1.209452, Accuracy: 60.71%\n",
      "Batch 123, Loss: 1.107294, Accuracy: 60.73%\n",
      "Batch 124, Loss: 1.149431, Accuracy: 60.70%\n",
      "Batch 125, Loss: 1.072785, Accuracy: 60.73%\n",
      "Batch 126, Loss: 1.190634, Accuracy: 60.70%\n",
      "Batch 127, Loss: 1.111915, Accuracy: 60.72%\n",
      "Batch 128, Loss: 1.129340, Accuracy: 60.69%\n",
      "Batch 129, Loss: 1.095953, Accuracy: 60.72%\n",
      "Batch 130, Loss: 1.127772, Accuracy: 60.72%\n",
      "Batch 131, Loss: 1.064519, Accuracy: 60.77%\n",
      "Batch 132, Loss: 1.180853, Accuracy: 60.72%\n",
      "Batch 133, Loss: 1.113287, Accuracy: 60.73%\n",
      "Batch 134, Loss: 1.128957, Accuracy: 60.74%\n",
      "Batch 135, Loss: 1.072751, Accuracy: 60.79%\n",
      "Batch 136, Loss: 1.114600, Accuracy: 60.80%\n",
      "Batch 137, Loss: 1.051577, Accuracy: 60.87%\n",
      "Batch 138, Loss: 1.223300, Accuracy: 60.77%\n",
      "Batch 139, Loss: 1.144580, Accuracy: 60.76%\n",
      "Batch 140, Loss: 1.188298, Accuracy: 60.73%\n",
      "Batch 141, Loss: 1.058484, Accuracy: 60.82%\n",
      "Batch 142, Loss: 1.077791, Accuracy: 60.85%\n",
      "Batch 143, Loss: 1.142292, Accuracy: 60.85%\n",
      "Batch 144, Loss: 1.198342, Accuracy: 60.79%\n",
      "Batch 145, Loss: 1.167193, Accuracy: 60.75%\n",
      "Batch 146, Loss: 1.211677, Accuracy: 60.68%\n",
      "Batch 147, Loss: 1.013912, Accuracy: 60.79%\n",
      "Batch 148, Loss: 1.157656, Accuracy: 60.77%\n",
      "Batch 149, Loss: 1.069691, Accuracy: 60.82%\n",
      "Batch 150, Loss: 1.154136, Accuracy: 60.82%\n",
      "Batch 151, Loss: 1.082766, Accuracy: 60.82%\n",
      "Batch 152, Loss: 1.074732, Accuracy: 60.86%\n",
      "Batch 153, Loss: 1.134869, Accuracy: 60.86%\n",
      "Batch 154, Loss: 1.007484, Accuracy: 60.96%\n",
      "Batch 155, Loss: 1.143728, Accuracy: 60.94%\n",
      "Batch 156, Loss: 1.102720, Accuracy: 60.95%\n",
      "Batch 157, Loss: 1.218099, Accuracy: 60.88%\n",
      "Batch 158, Loss: 1.025879, Accuracy: 60.94%\n",
      "Batch 159, Loss: 1.020615, Accuracy: 61.01%\n",
      "Batch 160, Loss: 1.148058, Accuracy: 61.00%\n",
      "Batch 161, Loss: 1.001217, Accuracy: 61.09%\n",
      "Batch 162, Loss: 1.123248, Accuracy: 61.09%\n",
      "Batch 163, Loss: 1.176181, Accuracy: 61.05%\n",
      "Batch 164, Loss: 1.122730, Accuracy: 61.03%\n",
      "Batch 165, Loss: 1.165476, Accuracy: 61.01%\n",
      "Batch 166, Loss: 1.073844, Accuracy: 61.05%\n",
      "Batch 167, Loss: 1.128282, Accuracy: 61.06%\n",
      "Batch 168, Loss: 1.156219, Accuracy: 61.02%\n",
      "Batch 169, Loss: 1.093537, Accuracy: 61.05%\n",
      "Batch 170, Loss: 1.114943, Accuracy: 61.06%\n",
      "Batch 171, Loss: 1.074552, Accuracy: 61.12%\n",
      "Batch 172, Loss: 1.182101, Accuracy: 61.07%\n",
      "Batch 173, Loss: 1.097433, Accuracy: 61.07%\n",
      "Batch 174, Loss: 1.129905, Accuracy: 61.09%\n",
      "Batch 175, Loss: 1.128068, Accuracy: 61.09%\n",
      "Batch 176, Loss: 1.102414, Accuracy: 61.12%\n",
      "Batch 177, Loss: 1.170298, Accuracy: 61.08%\n",
      "Batch 178, Loss: 1.098047, Accuracy: 61.10%\n",
      "Batch 179, Loss: 1.051503, Accuracy: 61.14%\n",
      "Batch 180, Loss: 1.107674, Accuracy: 61.13%\n",
      "Batch 181, Loss: 1.247099, Accuracy: 61.05%\n",
      "Batch 182, Loss: 1.008636, Accuracy: 61.13%\n",
      "Batch 183, Loss: 1.113078, Accuracy: 61.13%\n",
      "Batch 184, Loss: 1.172257, Accuracy: 61.10%\n",
      "Batch 185, Loss: 1.095199, Accuracy: 61.13%\n",
      "Batch 186, Loss: 1.159242, Accuracy: 61.12%\n",
      "Batch 187, Loss: 1.129868, Accuracy: 61.14%\n",
      "Batch 188, Loss: 1.174395, Accuracy: 61.12%\n",
      "Batch 189, Loss: 1.147397, Accuracy: 61.12%\n",
      "Batch 190, Loss: 1.130429, Accuracy: 61.13%\n",
      "Batch 191, Loss: 1.159632, Accuracy: 61.09%\n",
      "Batch 192, Loss: 1.112578, Accuracy: 61.12%\n",
      "Batch 193, Loss: 1.144235, Accuracy: 61.10%\n",
      "Batch 194, Loss: 1.060722, Accuracy: 61.15%\n",
      "Batch 195, Loss: 1.159692, Accuracy: 61.13%\n",
      "Batch 196, Loss: 1.126792, Accuracy: 61.14%\n",
      "Batch 197, Loss: 1.117881, Accuracy: 61.14%\n",
      "Batch 198, Loss: 1.082721, Accuracy: 61.16%\n",
      "Batch 199, Loss: 1.099784, Accuracy: 61.15%\n",
      "Batch 200, Loss: 1.143596, Accuracy: 61.14%\n",
      "Batch 201, Loss: 1.148110, Accuracy: 61.13%\n",
      "Batch 202, Loss: 1.066337, Accuracy: 61.17%\n",
      "Batch 203, Loss: 0.958624, Accuracy: 61.28%\n",
      "Batch 204, Loss: 1.017769, Accuracy: 61.34%\n",
      "Batch 205, Loss: 1.074076, Accuracy: 61.38%\n",
      "Batch 206, Loss: 1.118618, Accuracy: 61.38%\n",
      "Batch 207, Loss: 1.088375, Accuracy: 61.41%\n",
      "Batch 208, Loss: 1.164244, Accuracy: 61.37%\n",
      "Batch 209, Loss: 1.052395, Accuracy: 61.41%\n",
      "Batch 210, Loss: 1.123780, Accuracy: 61.40%\n",
      "Batch 211, Loss: 1.079653, Accuracy: 61.41%\n",
      "Batch 212, Loss: 1.220572, Accuracy: 61.38%\n",
      "Batch 213, Loss: 1.137144, Accuracy: 61.38%\n",
      "Training - Epoch 11, Loss: 1.125178, Accuracy: 61.38%\n",
      "Validation Batch 1, Loss: 1.317625, Accuracy: 37.50%\n",
      "Validation Batch 2, Loss: 1.320785, Accuracy: 39.06%\n",
      "Validation Batch 3, Loss: 1.408241, Accuracy: 35.94%\n",
      "Validation Batch 4, Loss: 1.290783, Accuracy: 37.11%\n",
      "Validation Batch 5, Loss: 1.394347, Accuracy: 35.94%\n",
      "Validation Batch 6, Loss: 1.429694, Accuracy: 33.85%\n",
      "Validation Batch 7, Loss: 1.341911, Accuracy: 33.93%\n",
      "Validation Batch 8, Loss: 1.340539, Accuracy: 34.18%\n",
      "Validation Batch 9, Loss: 1.382333, Accuracy: 34.03%\n",
      "Validation Batch 10, Loss: 1.389344, Accuracy: 33.59%\n",
      "Validation Batch 11, Loss: 1.338683, Accuracy: 33.81%\n",
      "Validation Batch 12, Loss: 1.316100, Accuracy: 33.98%\n",
      "Validation Batch 13, Loss: 1.373261, Accuracy: 33.89%\n",
      "Validation Batch 14, Loss: 1.251477, Accuracy: 34.71%\n",
      "Validation Batch 15, Loss: 1.322110, Accuracy: 34.90%\n",
      "Validation Batch 16, Loss: 1.409915, Accuracy: 34.38%\n",
      "Validation Batch 17, Loss: 1.372594, Accuracy: 34.38%\n",
      "Validation Batch 18, Loss: 1.341136, Accuracy: 34.38%\n",
      "Validation Batch 19, Loss: 1.362157, Accuracy: 34.46%\n",
      "Validation Batch 20, Loss: 1.353283, Accuracy: 34.38%\n",
      "Validation Batch 21, Loss: 1.348250, Accuracy: 34.45%\n",
      "Validation Batch 22, Loss: 1.351732, Accuracy: 34.52%\n",
      "Validation Batch 23, Loss: 1.365699, Accuracy: 34.51%\n",
      "Validation Batch 24, Loss: 1.401750, Accuracy: 34.24%\n",
      "Validation Batch 25, Loss: 1.305909, Accuracy: 34.44%\n",
      "Validation Batch 26, Loss: 1.270749, Accuracy: 34.74%\n",
      "Validation Batch 27, Loss: 1.368502, Accuracy: 34.70%\n",
      "Validation - Epoch 11, Loss: 1.350700, Accuracy: 34.70%\n",
      "Patienceâ€”0\n",
      "Epoch 12\n",
      "Batch 1, Loss: 1.129504, Accuracy: 60.94%\n",
      "Batch 2, Loss: 1.115348, Accuracy: 60.94%\n",
      "Batch 3, Loss: 1.101008, Accuracy: 62.50%\n",
      "Batch 4, Loss: 1.173830, Accuracy: 61.33%\n",
      "Batch 5, Loss: 1.165149, Accuracy: 61.25%\n",
      "Batch 6, Loss: 1.127577, Accuracy: 60.94%\n",
      "Batch 7, Loss: 1.173844, Accuracy: 60.71%\n",
      "Batch 8, Loss: 1.100103, Accuracy: 61.13%\n",
      "Batch 9, Loss: 1.116345, Accuracy: 61.28%\n",
      "Batch 10, Loss: 1.114839, Accuracy: 61.56%\n",
      "Batch 11, Loss: 1.079971, Accuracy: 62.07%\n",
      "Batch 12, Loss: 1.196290, Accuracy: 61.33%\n",
      "Batch 13, Loss: 1.149908, Accuracy: 61.06%\n",
      "Batch 14, Loss: 1.105163, Accuracy: 61.16%\n",
      "Batch 15, Loss: 1.142135, Accuracy: 60.94%\n",
      "Batch 16, Loss: 1.108489, Accuracy: 61.04%\n",
      "Batch 17, Loss: 1.153868, Accuracy: 61.03%\n",
      "Batch 18, Loss: 1.224879, Accuracy: 60.33%\n",
      "Batch 19, Loss: 1.143979, Accuracy: 60.28%\n",
      "Batch 20, Loss: 1.105601, Accuracy: 60.47%\n",
      "Batch 21, Loss: 1.083891, Accuracy: 60.71%\n",
      "Batch 22, Loss: 1.144544, Accuracy: 60.65%\n",
      "Batch 23, Loss: 1.156896, Accuracy: 60.53%\n",
      "Batch 24, Loss: 1.120211, Accuracy: 60.61%\n",
      "Batch 25, Loss: 1.047637, Accuracy: 61.06%\n",
      "Batch 26, Loss: 1.166979, Accuracy: 60.94%\n",
      "Batch 27, Loss: 1.151576, Accuracy: 60.71%\n",
      "Batch 28, Loss: 1.028037, Accuracy: 61.05%\n",
      "Batch 29, Loss: 1.209326, Accuracy: 60.72%\n",
      "Batch 30, Loss: 1.173674, Accuracy: 60.73%\n",
      "Batch 31, Loss: 1.095233, Accuracy: 60.84%\n",
      "Batch 32, Loss: 1.117155, Accuracy: 60.79%\n",
      "Batch 33, Loss: 1.142066, Accuracy: 60.70%\n",
      "Batch 34, Loss: 1.131285, Accuracy: 60.71%\n",
      "Batch 35, Loss: 1.083191, Accuracy: 60.85%\n",
      "Batch 36, Loss: 1.126218, Accuracy: 60.89%\n",
      "Batch 37, Loss: 1.168789, Accuracy: 60.81%\n",
      "Batch 38, Loss: 1.109299, Accuracy: 60.77%\n",
      "Batch 39, Loss: 1.101688, Accuracy: 60.86%\n",
      "Batch 40, Loss: 1.175381, Accuracy: 60.66%\n",
      "Batch 41, Loss: 1.098294, Accuracy: 60.82%\n",
      "Batch 42, Loss: 1.127567, Accuracy: 60.90%\n",
      "Batch 43, Loss: 1.097191, Accuracy: 60.94%\n",
      "Batch 44, Loss: 1.067541, Accuracy: 61.12%\n",
      "Batch 45, Loss: 1.083279, Accuracy: 61.18%\n",
      "Batch 46, Loss: 1.134079, Accuracy: 61.14%\n",
      "Batch 47, Loss: 1.153192, Accuracy: 61.14%\n",
      "Batch 48, Loss: 1.266579, Accuracy: 60.87%\n",
      "Batch 49, Loss: 1.089902, Accuracy: 60.91%\n",
      "Batch 50, Loss: 1.123525, Accuracy: 60.91%\n",
      "Batch 51, Loss: 0.985002, Accuracy: 61.21%\n",
      "Batch 52, Loss: 1.163165, Accuracy: 61.12%\n",
      "Batch 53, Loss: 1.104111, Accuracy: 61.17%\n",
      "Batch 54, Loss: 1.131283, Accuracy: 61.14%\n",
      "Batch 55, Loss: 1.009163, Accuracy: 61.39%\n",
      "Batch 56, Loss: 1.150233, Accuracy: 61.36%\n",
      "Batch 57, Loss: 1.078769, Accuracy: 61.46%\n",
      "Batch 58, Loss: 1.208645, Accuracy: 61.21%\n",
      "Batch 59, Loss: 1.175105, Accuracy: 61.10%\n",
      "Batch 60, Loss: 1.136406, Accuracy: 61.12%\n",
      "Batch 61, Loss: 1.149644, Accuracy: 61.01%\n",
      "Batch 62, Loss: 1.086846, Accuracy: 61.06%\n",
      "Batch 63, Loss: 1.066931, Accuracy: 61.19%\n",
      "Batch 64, Loss: 1.062141, Accuracy: 61.30%\n",
      "Batch 65, Loss: 1.036422, Accuracy: 61.44%\n",
      "Batch 66, Loss: 1.138895, Accuracy: 61.43%\n",
      "Batch 67, Loss: 1.149360, Accuracy: 61.43%\n",
      "Batch 68, Loss: 1.136776, Accuracy: 61.47%\n",
      "Batch 69, Loss: 1.222158, Accuracy: 61.30%\n",
      "Batch 70, Loss: 1.132620, Accuracy: 61.29%\n",
      "Batch 71, Loss: 1.178103, Accuracy: 61.22%\n",
      "Batch 72, Loss: 1.204312, Accuracy: 61.09%\n",
      "Batch 73, Loss: 1.175068, Accuracy: 61.00%\n",
      "Batch 74, Loss: 1.187292, Accuracy: 60.87%\n",
      "Batch 75, Loss: 1.101546, Accuracy: 60.92%\n",
      "Batch 76, Loss: 1.104725, Accuracy: 60.92%\n",
      "Batch 77, Loss: 1.105036, Accuracy: 60.94%\n",
      "Batch 78, Loss: 1.154814, Accuracy: 60.92%\n",
      "Batch 79, Loss: 1.188705, Accuracy: 60.84%\n",
      "Batch 80, Loss: 1.143664, Accuracy: 60.84%\n",
      "Batch 81, Loss: 1.103027, Accuracy: 60.86%\n",
      "Batch 82, Loss: 1.080917, Accuracy: 60.96%\n",
      "Batch 83, Loss: 1.086086, Accuracy: 60.99%\n",
      "Batch 84, Loss: 1.155507, Accuracy: 60.94%\n",
      "Batch 85, Loss: 1.054736, Accuracy: 60.99%\n",
      "Batch 86, Loss: 1.146027, Accuracy: 60.96%\n",
      "Batch 87, Loss: 1.108139, Accuracy: 60.99%\n",
      "Batch 88, Loss: 1.146499, Accuracy: 60.97%\n",
      "Batch 89, Loss: 1.114737, Accuracy: 60.96%\n",
      "Batch 90, Loss: 1.144206, Accuracy: 60.94%\n",
      "Batch 91, Loss: 1.119525, Accuracy: 60.94%\n",
      "Batch 92, Loss: 1.124838, Accuracy: 60.95%\n",
      "Batch 93, Loss: 1.154015, Accuracy: 60.92%\n",
      "Batch 94, Loss: 1.125609, Accuracy: 60.92%\n",
      "Batch 95, Loss: 1.092971, Accuracy: 61.00%\n",
      "Batch 96, Loss: 1.163664, Accuracy: 60.99%\n",
      "Batch 97, Loss: 1.219242, Accuracy: 60.87%\n",
      "Batch 98, Loss: 1.079028, Accuracy: 60.92%\n",
      "Batch 99, Loss: 1.017292, Accuracy: 61.03%\n",
      "Batch 100, Loss: 1.179218, Accuracy: 60.97%\n",
      "Batch 101, Loss: 1.167779, Accuracy: 60.94%\n",
      "Batch 102, Loss: 1.089675, Accuracy: 60.98%\n",
      "Batch 103, Loss: 1.080477, Accuracy: 61.03%\n",
      "Batch 104, Loss: 1.108588, Accuracy: 61.06%\n",
      "Batch 105, Loss: 1.213145, Accuracy: 60.97%\n",
      "Batch 106, Loss: 1.067667, Accuracy: 61.04%\n",
      "Batch 107, Loss: 1.098508, Accuracy: 61.08%\n",
      "Batch 108, Loss: 1.188426, Accuracy: 61.02%\n",
      "Batch 109, Loss: 1.021560, Accuracy: 61.17%\n",
      "Batch 110, Loss: 1.074648, Accuracy: 61.21%\n",
      "Batch 111, Loss: 1.029139, Accuracy: 61.30%\n",
      "Batch 112, Loss: 1.031977, Accuracy: 61.41%\n",
      "Batch 113, Loss: 1.150265, Accuracy: 61.41%\n",
      "Batch 114, Loss: 1.017333, Accuracy: 61.54%\n",
      "Batch 115, Loss: 1.163261, Accuracy: 61.48%\n",
      "Batch 116, Loss: 1.095454, Accuracy: 61.50%\n",
      "Batch 117, Loss: 1.093012, Accuracy: 61.54%\n",
      "Batch 118, Loss: 1.262346, Accuracy: 61.40%\n",
      "Batch 119, Loss: 1.041204, Accuracy: 61.46%\n",
      "Batch 120, Loss: 1.063397, Accuracy: 61.51%\n",
      "Batch 121, Loss: 1.161505, Accuracy: 61.47%\n",
      "Batch 122, Loss: 1.106648, Accuracy: 61.49%\n",
      "Batch 123, Loss: 1.123033, Accuracy: 61.50%\n",
      "Batch 124, Loss: 1.140813, Accuracy: 61.50%\n",
      "Batch 125, Loss: 1.045797, Accuracy: 61.55%\n",
      "Batch 126, Loss: 1.115143, Accuracy: 61.58%\n",
      "Batch 127, Loss: 1.070679, Accuracy: 61.64%\n",
      "Batch 128, Loss: 1.084720, Accuracy: 61.68%\n",
      "Batch 129, Loss: 1.081580, Accuracy: 61.76%\n",
      "Batch 130, Loss: 1.013906, Accuracy: 61.85%\n",
      "Batch 131, Loss: 1.019142, Accuracy: 61.92%\n",
      "Batch 132, Loss: 1.115908, Accuracy: 61.91%\n",
      "Batch 133, Loss: 1.139840, Accuracy: 61.90%\n",
      "Batch 134, Loss: 1.128908, Accuracy: 61.89%\n",
      "Batch 135, Loss: 1.084889, Accuracy: 61.92%\n",
      "Batch 136, Loss: 1.145142, Accuracy: 61.90%\n",
      "Batch 137, Loss: 1.108521, Accuracy: 61.92%\n",
      "Batch 138, Loss: 1.112604, Accuracy: 61.92%\n",
      "Batch 139, Loss: 1.137499, Accuracy: 61.93%\n",
      "Batch 140, Loss: 1.137116, Accuracy: 61.91%\n",
      "Batch 141, Loss: 1.029445, Accuracy: 61.97%\n",
      "Batch 142, Loss: 1.076706, Accuracy: 62.00%\n",
      "Batch 143, Loss: 1.089301, Accuracy: 62.02%\n",
      "Batch 144, Loss: 1.148741, Accuracy: 62.00%\n",
      "Batch 145, Loss: 1.131920, Accuracy: 62.02%\n",
      "Batch 146, Loss: 1.206398, Accuracy: 61.95%\n",
      "Batch 147, Loss: 1.067274, Accuracy: 61.97%\n",
      "Batch 148, Loss: 1.172461, Accuracy: 61.92%\n",
      "Batch 149, Loss: 1.119265, Accuracy: 61.92%\n",
      "Batch 150, Loss: 1.138144, Accuracy: 61.90%\n",
      "Batch 151, Loss: 1.046482, Accuracy: 61.95%\n",
      "Batch 152, Loss: 1.060561, Accuracy: 61.98%\n",
      "Batch 153, Loss: 1.189501, Accuracy: 61.94%\n",
      "Batch 154, Loss: 1.120020, Accuracy: 61.93%\n",
      "Batch 155, Loss: 1.044359, Accuracy: 61.98%\n",
      "Batch 156, Loss: 1.099521, Accuracy: 61.99%\n",
      "Batch 157, Loss: 1.203069, Accuracy: 61.92%\n",
      "Batch 158, Loss: 1.222060, Accuracy: 61.85%\n",
      "Batch 159, Loss: 1.012127, Accuracy: 61.93%\n",
      "Batch 160, Loss: 1.148335, Accuracy: 61.89%\n",
      "Batch 161, Loss: 1.060394, Accuracy: 61.94%\n",
      "Batch 162, Loss: 1.183304, Accuracy: 61.88%\n",
      "Batch 163, Loss: 1.133321, Accuracy: 61.87%\n",
      "Batch 164, Loss: 1.112405, Accuracy: 61.87%\n",
      "Batch 165, Loss: 1.123569, Accuracy: 61.88%\n",
      "Batch 166, Loss: 0.981993, Accuracy: 61.98%\n",
      "Batch 167, Loss: 1.150609, Accuracy: 61.98%\n",
      "Batch 168, Loss: 1.087305, Accuracy: 61.99%\n",
      "Batch 169, Loss: 1.068469, Accuracy: 62.00%\n",
      "Batch 170, Loss: 1.023599, Accuracy: 62.04%\n",
      "Batch 171, Loss: 1.086868, Accuracy: 62.06%\n",
      "Batch 172, Loss: 1.116382, Accuracy: 62.08%\n",
      "Batch 173, Loss: 1.077385, Accuracy: 62.11%\n",
      "Batch 174, Loss: 1.129409, Accuracy: 62.09%\n",
      "Batch 175, Loss: 1.070328, Accuracy: 62.12%\n",
      "Batch 176, Loss: 1.099246, Accuracy: 62.13%\n",
      "Batch 177, Loss: 1.234360, Accuracy: 62.05%\n",
      "Batch 178, Loss: 1.139522, Accuracy: 62.03%\n",
      "Batch 179, Loss: 1.117675, Accuracy: 62.04%\n",
      "Batch 180, Loss: 1.180386, Accuracy: 61.97%\n",
      "Batch 181, Loss: 1.146752, Accuracy: 61.96%\n",
      "Batch 182, Loss: 1.163335, Accuracy: 61.92%\n",
      "Batch 183, Loss: 1.141175, Accuracy: 61.89%\n",
      "Batch 184, Loss: 1.085085, Accuracy: 61.91%\n",
      "Batch 185, Loss: 1.086690, Accuracy: 61.93%\n",
      "Batch 186, Loss: 1.115818, Accuracy: 61.94%\n",
      "Batch 187, Loss: 1.166081, Accuracy: 61.91%\n",
      "Batch 188, Loss: 1.180107, Accuracy: 61.87%\n",
      "Batch 189, Loss: 1.063795, Accuracy: 61.90%\n",
      "Batch 190, Loss: 1.153951, Accuracy: 61.88%\n",
      "Batch 191, Loss: 1.082002, Accuracy: 61.89%\n",
      "Batch 192, Loss: 1.182003, Accuracy: 61.86%\n",
      "Batch 193, Loss: 1.184015, Accuracy: 61.81%\n",
      "Batch 194, Loss: 1.180799, Accuracy: 61.78%\n",
      "Batch 195, Loss: 1.131928, Accuracy: 61.77%\n",
      "Batch 196, Loss: 1.070074, Accuracy: 61.80%\n",
      "Batch 197, Loss: 0.999684, Accuracy: 61.87%\n",
      "Batch 198, Loss: 1.129654, Accuracy: 61.87%\n",
      "Batch 199, Loss: 1.126933, Accuracy: 61.86%\n",
      "Batch 200, Loss: 1.157060, Accuracy: 61.85%\n",
      "Batch 201, Loss: 1.087108, Accuracy: 61.88%\n",
      "Batch 202, Loss: 1.059983, Accuracy: 61.90%\n",
      "Batch 203, Loss: 1.167636, Accuracy: 61.88%\n",
      "Batch 204, Loss: 1.192528, Accuracy: 61.83%\n",
      "Batch 205, Loss: 1.131310, Accuracy: 61.82%\n",
      "Batch 206, Loss: 1.151815, Accuracy: 61.80%\n",
      "Batch 207, Loss: 1.104989, Accuracy: 61.81%\n",
      "Batch 208, Loss: 1.094634, Accuracy: 61.85%\n",
      "Batch 209, Loss: 1.157325, Accuracy: 61.82%\n",
      "Batch 210, Loss: 1.077441, Accuracy: 61.85%\n",
      "Batch 211, Loss: 1.176292, Accuracy: 61.82%\n",
      "Batch 212, Loss: 1.070081, Accuracy: 61.85%\n",
      "Batch 213, Loss: 1.241500, Accuracy: 61.78%\n",
      "Training - Epoch 12, Loss: 1.120551, Accuracy: 61.78%\n",
      "Validation Batch 1, Loss: 1.304918, Accuracy: 37.50%\n",
      "Validation Batch 2, Loss: 1.314597, Accuracy: 39.06%\n",
      "Validation Batch 3, Loss: 1.399418, Accuracy: 35.94%\n",
      "Validation Batch 4, Loss: 1.280581, Accuracy: 37.11%\n",
      "Validation Batch 5, Loss: 1.385685, Accuracy: 35.94%\n",
      "Validation Batch 6, Loss: 1.410046, Accuracy: 34.11%\n",
      "Validation Batch 7, Loss: 1.328015, Accuracy: 34.38%\n",
      "Validation Batch 8, Loss: 1.329054, Accuracy: 34.96%\n",
      "Validation Batch 9, Loss: 1.374492, Accuracy: 34.72%\n",
      "Validation Batch 10, Loss: 1.377677, Accuracy: 34.22%\n",
      "Validation Batch 11, Loss: 1.327189, Accuracy: 34.52%\n",
      "Validation Batch 12, Loss: 1.298408, Accuracy: 34.77%\n",
      "Validation Batch 13, Loss: 1.363401, Accuracy: 34.74%\n",
      "Validation Batch 14, Loss: 1.245946, Accuracy: 35.49%\n",
      "Validation Batch 15, Loss: 1.312968, Accuracy: 35.73%\n",
      "Validation Batch 16, Loss: 1.393114, Accuracy: 35.25%\n",
      "Validation Batch 17, Loss: 1.366649, Accuracy: 35.20%\n",
      "Validation Batch 18, Loss: 1.327071, Accuracy: 35.33%\n",
      "Validation Batch 19, Loss: 1.353236, Accuracy: 35.44%\n",
      "Validation Batch 20, Loss: 1.336439, Accuracy: 35.55%\n",
      "Validation Batch 21, Loss: 1.340473, Accuracy: 35.57%\n",
      "Validation Batch 22, Loss: 1.344517, Accuracy: 35.58%\n",
      "Validation Batch 23, Loss: 1.358909, Accuracy: 35.53%\n",
      "Validation Batch 24, Loss: 1.386785, Accuracy: 35.22%\n",
      "Validation Batch 25, Loss: 1.296384, Accuracy: 35.50%\n",
      "Validation Batch 26, Loss: 1.260052, Accuracy: 35.76%\n",
      "Validation Batch 27, Loss: 1.362114, Accuracy: 35.70%\n",
      "Validation - Epoch 12, Loss: 1.339931, Accuracy: 35.70%\n",
      "Patienceâ€”0\n",
      "Epoch 13\n",
      "Batch 1, Loss: 1.109002, Accuracy: 59.38%\n",
      "Batch 2, Loss: 1.163184, Accuracy: 58.59%\n",
      "Batch 3, Loss: 1.112178, Accuracy: 60.94%\n",
      "Batch 4, Loss: 1.194990, Accuracy: 59.38%\n",
      "Batch 5, Loss: 1.104166, Accuracy: 60.00%\n",
      "Batch 6, Loss: 1.119748, Accuracy: 59.64%\n",
      "Batch 7, Loss: 1.069826, Accuracy: 60.71%\n",
      "Batch 8, Loss: 1.114823, Accuracy: 61.13%\n",
      "Batch 9, Loss: 1.088194, Accuracy: 61.28%\n",
      "Batch 10, Loss: 1.030106, Accuracy: 62.34%\n",
      "Batch 11, Loss: 1.172395, Accuracy: 61.65%\n",
      "Batch 12, Loss: 1.118049, Accuracy: 61.85%\n",
      "Batch 13, Loss: 1.131205, Accuracy: 61.90%\n",
      "Batch 14, Loss: 1.095591, Accuracy: 62.05%\n",
      "Batch 15, Loss: 1.121310, Accuracy: 61.98%\n",
      "Batch 16, Loss: 1.156963, Accuracy: 61.82%\n",
      "Batch 17, Loss: 1.114192, Accuracy: 61.67%\n",
      "Batch 18, Loss: 1.171872, Accuracy: 61.20%\n",
      "Batch 19, Loss: 1.076862, Accuracy: 61.51%\n",
      "Batch 20, Loss: 1.167299, Accuracy: 61.41%\n",
      "Batch 21, Loss: 1.093798, Accuracy: 61.61%\n",
      "Batch 22, Loss: 1.132926, Accuracy: 61.43%\n",
      "Batch 23, Loss: 1.131300, Accuracy: 61.41%\n",
      "Batch 24, Loss: 1.049671, Accuracy: 61.78%\n",
      "Batch 25, Loss: 1.159184, Accuracy: 61.69%\n",
      "Batch 26, Loss: 1.089432, Accuracy: 61.90%\n",
      "Batch 27, Loss: 1.090687, Accuracy: 62.04%\n",
      "Batch 28, Loss: 1.027127, Accuracy: 62.39%\n",
      "Batch 29, Loss: 1.116942, Accuracy: 62.39%\n",
      "Batch 30, Loss: 1.140636, Accuracy: 62.34%\n",
      "Batch 31, Loss: 1.055531, Accuracy: 62.50%\n",
      "Batch 32, Loss: 1.155941, Accuracy: 62.35%\n",
      "Batch 33, Loss: 1.136832, Accuracy: 62.26%\n",
      "Batch 34, Loss: 1.071573, Accuracy: 62.45%\n",
      "Batch 35, Loss: 1.159176, Accuracy: 62.19%\n",
      "Batch 36, Loss: 1.109173, Accuracy: 62.24%\n",
      "Batch 37, Loss: 1.255744, Accuracy: 61.82%\n",
      "Batch 38, Loss: 1.146156, Accuracy: 61.76%\n",
      "Batch 39, Loss: 1.081310, Accuracy: 61.86%\n",
      "Batch 40, Loss: 1.083070, Accuracy: 61.95%\n",
      "Batch 41, Loss: 1.012912, Accuracy: 62.31%\n",
      "Batch 42, Loss: 1.014994, Accuracy: 62.50%\n",
      "Batch 43, Loss: 1.215723, Accuracy: 62.25%\n",
      "Batch 44, Loss: 1.029949, Accuracy: 62.43%\n",
      "Batch 45, Loss: 1.154614, Accuracy: 62.36%\n",
      "Batch 46, Loss: 1.118892, Accuracy: 62.30%\n",
      "Batch 47, Loss: 1.097376, Accuracy: 62.30%\n",
      "Batch 48, Loss: 1.130230, Accuracy: 62.30%\n",
      "Batch 49, Loss: 1.089191, Accuracy: 62.37%\n",
      "Batch 50, Loss: 1.282382, Accuracy: 61.91%\n",
      "Batch 51, Loss: 1.071309, Accuracy: 62.01%\n",
      "Batch 52, Loss: 1.121630, Accuracy: 61.99%\n",
      "Batch 53, Loss: 1.038499, Accuracy: 62.15%\n",
      "Batch 54, Loss: 1.219619, Accuracy: 61.95%\n",
      "Batch 55, Loss: 1.091459, Accuracy: 61.99%\n",
      "Batch 56, Loss: 1.203483, Accuracy: 61.86%\n",
      "Batch 57, Loss: 1.164981, Accuracy: 61.81%\n",
      "Batch 58, Loss: 1.148105, Accuracy: 61.77%\n",
      "Batch 59, Loss: 1.114747, Accuracy: 61.81%\n",
      "Batch 60, Loss: 1.159912, Accuracy: 61.72%\n",
      "Batch 61, Loss: 1.174817, Accuracy: 61.68%\n",
      "Batch 62, Loss: 1.108525, Accuracy: 61.67%\n",
      "Batch 63, Loss: 1.165417, Accuracy: 61.48%\n",
      "Batch 64, Loss: 1.119753, Accuracy: 61.50%\n",
      "Batch 65, Loss: 1.041964, Accuracy: 61.66%\n",
      "Batch 66, Loss: 1.151847, Accuracy: 61.62%\n",
      "Batch 67, Loss: 1.143511, Accuracy: 61.61%\n",
      "Batch 68, Loss: 1.133122, Accuracy: 61.60%\n",
      "Batch 69, Loss: 1.177095, Accuracy: 61.53%\n",
      "Batch 70, Loss: 1.075406, Accuracy: 61.61%\n",
      "Batch 71, Loss: 1.153421, Accuracy: 61.60%\n",
      "Batch 72, Loss: 1.107024, Accuracy: 61.63%\n",
      "Batch 73, Loss: 1.207778, Accuracy: 61.54%\n",
      "Batch 74, Loss: 1.093192, Accuracy: 61.57%\n",
      "Batch 75, Loss: 1.158125, Accuracy: 61.52%\n",
      "Batch 76, Loss: 1.177321, Accuracy: 61.45%\n",
      "Batch 77, Loss: 1.103779, Accuracy: 61.51%\n",
      "Batch 78, Loss: 1.173239, Accuracy: 61.48%\n",
      "Batch 79, Loss: 1.119508, Accuracy: 61.47%\n",
      "Batch 80, Loss: 1.073169, Accuracy: 61.54%\n",
      "Batch 81, Loss: 1.214928, Accuracy: 61.44%\n",
      "Batch 82, Loss: 1.030454, Accuracy: 61.59%\n",
      "Batch 83, Loss: 1.114078, Accuracy: 61.60%\n",
      "Batch 84, Loss: 1.117985, Accuracy: 61.57%\n",
      "Batch 85, Loss: 1.062199, Accuracy: 61.69%\n",
      "Batch 86, Loss: 1.197555, Accuracy: 61.61%\n",
      "Batch 87, Loss: 1.071096, Accuracy: 61.73%\n",
      "Batch 88, Loss: 1.118407, Accuracy: 61.74%\n",
      "Batch 89, Loss: 1.177739, Accuracy: 61.66%\n",
      "Batch 90, Loss: 1.070288, Accuracy: 61.70%\n",
      "Batch 91, Loss: 1.171158, Accuracy: 61.61%\n",
      "Batch 92, Loss: 1.080746, Accuracy: 61.68%\n",
      "Batch 93, Loss: 1.115181, Accuracy: 61.66%\n",
      "Batch 94, Loss: 1.154998, Accuracy: 61.60%\n",
      "Batch 95, Loss: 1.129026, Accuracy: 61.63%\n",
      "Batch 96, Loss: 1.121035, Accuracy: 61.62%\n",
      "Batch 97, Loss: 1.107809, Accuracy: 61.66%\n",
      "Batch 98, Loss: 1.204120, Accuracy: 61.59%\n",
      "Batch 99, Loss: 1.134472, Accuracy: 61.57%\n",
      "Batch 100, Loss: 1.189192, Accuracy: 61.48%\n",
      "Batch 101, Loss: 1.111636, Accuracy: 61.49%\n",
      "Batch 102, Loss: 1.069478, Accuracy: 61.55%\n",
      "Batch 103, Loss: 1.163844, Accuracy: 61.53%\n",
      "Batch 104, Loss: 1.112124, Accuracy: 61.54%\n",
      "Batch 105, Loss: 1.041200, Accuracy: 61.64%\n",
      "Batch 106, Loss: 1.072167, Accuracy: 61.70%\n",
      "Batch 107, Loss: 1.084778, Accuracy: 61.74%\n",
      "Batch 108, Loss: 1.177179, Accuracy: 61.68%\n",
      "Batch 109, Loss: 1.167328, Accuracy: 61.61%\n",
      "Batch 110, Loss: 1.054964, Accuracy: 61.66%\n",
      "Batch 111, Loss: 1.075028, Accuracy: 61.71%\n",
      "Batch 112, Loss: 1.184360, Accuracy: 61.62%\n",
      "Batch 113, Loss: 1.171100, Accuracy: 61.57%\n",
      "Batch 114, Loss: 1.077500, Accuracy: 61.62%\n",
      "Batch 115, Loss: 1.110555, Accuracy: 61.62%\n",
      "Batch 116, Loss: 1.123656, Accuracy: 61.58%\n",
      "Batch 117, Loss: 1.128316, Accuracy: 61.58%\n",
      "Batch 118, Loss: 1.188074, Accuracy: 61.52%\n",
      "Batch 119, Loss: 1.163352, Accuracy: 61.48%\n",
      "Batch 120, Loss: 1.116035, Accuracy: 61.45%\n",
      "Batch 121, Loss: 1.092655, Accuracy: 61.44%\n",
      "Batch 122, Loss: 1.066844, Accuracy: 61.48%\n",
      "Batch 123, Loss: 1.077499, Accuracy: 61.55%\n",
      "Batch 124, Loss: 1.048171, Accuracy: 61.63%\n",
      "Batch 125, Loss: 1.188387, Accuracy: 61.58%\n",
      "Batch 126, Loss: 1.086517, Accuracy: 61.58%\n",
      "Batch 127, Loss: 1.053555, Accuracy: 61.65%\n",
      "Batch 128, Loss: 1.132865, Accuracy: 61.66%\n",
      "Batch 129, Loss: 1.140952, Accuracy: 61.63%\n",
      "Batch 130, Loss: 1.108660, Accuracy: 61.65%\n",
      "Batch 131, Loss: 1.164678, Accuracy: 61.63%\n",
      "Batch 132, Loss: 1.066803, Accuracy: 61.66%\n",
      "Batch 133, Loss: 1.096445, Accuracy: 61.69%\n",
      "Batch 134, Loss: 0.977167, Accuracy: 61.84%\n",
      "Batch 135, Loss: 1.219473, Accuracy: 61.75%\n",
      "Batch 136, Loss: 1.095106, Accuracy: 61.76%\n",
      "Batch 137, Loss: 1.131950, Accuracy: 61.76%\n",
      "Batch 138, Loss: 1.073603, Accuracy: 61.82%\n",
      "Batch 139, Loss: 1.139596, Accuracy: 61.79%\n",
      "Batch 140, Loss: 1.048832, Accuracy: 61.84%\n",
      "Batch 141, Loss: 1.158759, Accuracy: 61.82%\n",
      "Batch 142, Loss: 1.178961, Accuracy: 61.76%\n",
      "Batch 143, Loss: 1.089810, Accuracy: 61.79%\n",
      "Batch 144, Loss: 1.077098, Accuracy: 61.83%\n",
      "Batch 145, Loss: 1.101819, Accuracy: 61.84%\n",
      "Batch 146, Loss: 1.122284, Accuracy: 61.85%\n",
      "Batch 147, Loss: 1.054673, Accuracy: 61.88%\n",
      "Batch 148, Loss: 1.122095, Accuracy: 61.86%\n",
      "Batch 149, Loss: 1.092001, Accuracy: 61.87%\n",
      "Batch 150, Loss: 1.133061, Accuracy: 61.85%\n",
      "Batch 151, Loss: 1.193525, Accuracy: 61.80%\n",
      "Batch 152, Loss: 1.152928, Accuracy: 61.78%\n",
      "Batch 153, Loss: 1.065860, Accuracy: 61.84%\n",
      "Batch 154, Loss: 1.096637, Accuracy: 61.84%\n",
      "Batch 155, Loss: 1.183720, Accuracy: 61.80%\n",
      "Batch 156, Loss: 1.098588, Accuracy: 61.83%\n",
      "Batch 157, Loss: 1.166292, Accuracy: 61.79%\n",
      "Batch 158, Loss: 1.102631, Accuracy: 61.81%\n",
      "Batch 159, Loss: 1.110919, Accuracy: 61.79%\n",
      "Batch 160, Loss: 1.111044, Accuracy: 61.82%\n",
      "Batch 161, Loss: 1.205668, Accuracy: 61.75%\n",
      "Batch 162, Loss: 1.099020, Accuracy: 61.76%\n",
      "Batch 163, Loss: 1.178580, Accuracy: 61.69%\n",
      "Batch 164, Loss: 1.014771, Accuracy: 61.77%\n",
      "Batch 165, Loss: 1.043431, Accuracy: 61.83%\n",
      "Batch 166, Loss: 1.155704, Accuracy: 61.81%\n",
      "Batch 167, Loss: 1.136782, Accuracy: 61.81%\n",
      "Batch 168, Loss: 1.018208, Accuracy: 61.86%\n",
      "Batch 169, Loss: 1.115086, Accuracy: 61.87%\n",
      "Batch 170, Loss: 1.043624, Accuracy: 61.93%\n",
      "Batch 171, Loss: 1.079566, Accuracy: 61.97%\n",
      "Batch 172, Loss: 1.201051, Accuracy: 61.91%\n",
      "Batch 173, Loss: 1.120152, Accuracy: 61.91%\n",
      "Batch 174, Loss: 1.139385, Accuracy: 61.90%\n",
      "Batch 175, Loss: 1.118794, Accuracy: 61.92%\n",
      "Batch 176, Loss: 1.097550, Accuracy: 61.93%\n",
      "Batch 177, Loss: 1.122595, Accuracy: 61.95%\n",
      "Batch 178, Loss: 1.046716, Accuracy: 62.00%\n",
      "Batch 179, Loss: 1.144630, Accuracy: 61.98%\n",
      "Batch 180, Loss: 1.151051, Accuracy: 61.95%\n",
      "Batch 181, Loss: 1.003405, Accuracy: 62.03%\n",
      "Batch 182, Loss: 1.074090, Accuracy: 62.06%\n",
      "Batch 183, Loss: 1.128587, Accuracy: 62.05%\n",
      "Batch 184, Loss: 1.048866, Accuracy: 62.10%\n",
      "Batch 185, Loss: 1.077163, Accuracy: 62.13%\n",
      "Batch 186, Loss: 1.053903, Accuracy: 62.16%\n",
      "Batch 187, Loss: 1.121045, Accuracy: 62.17%\n",
      "Batch 188, Loss: 1.143956, Accuracy: 62.16%\n",
      "Batch 189, Loss: 1.193981, Accuracy: 62.12%\n",
      "Batch 190, Loss: 1.153551, Accuracy: 62.10%\n",
      "Batch 191, Loss: 1.129732, Accuracy: 62.07%\n",
      "Batch 192, Loss: 1.160962, Accuracy: 62.04%\n",
      "Batch 193, Loss: 1.083799, Accuracy: 62.06%\n",
      "Batch 194, Loss: 1.068397, Accuracy: 62.09%\n",
      "Batch 195, Loss: 1.119385, Accuracy: 62.08%\n",
      "Batch 196, Loss: 1.092914, Accuracy: 62.08%\n",
      "Batch 197, Loss: 0.972390, Accuracy: 62.16%\n",
      "Batch 198, Loss: 1.191259, Accuracy: 62.12%\n",
      "Batch 199, Loss: 1.102588, Accuracy: 62.12%\n",
      "Batch 200, Loss: 1.090341, Accuracy: 62.13%\n",
      "Batch 201, Loss: 1.166359, Accuracy: 62.11%\n",
      "Batch 202, Loss: 1.055246, Accuracy: 62.15%\n",
      "Batch 203, Loss: 1.142708, Accuracy: 62.13%\n",
      "Batch 204, Loss: 1.091283, Accuracy: 62.15%\n",
      "Batch 205, Loss: 1.156816, Accuracy: 62.13%\n",
      "Batch 206, Loss: 1.091691, Accuracy: 62.14%\n",
      "Batch 207, Loss: 1.021169, Accuracy: 62.21%\n",
      "Batch 208, Loss: 1.094594, Accuracy: 62.23%\n",
      "Batch 209, Loss: 1.130204, Accuracy: 62.22%\n",
      "Batch 210, Loss: 1.068380, Accuracy: 62.25%\n",
      "Batch 211, Loss: 1.125895, Accuracy: 62.25%\n",
      "Batch 212, Loss: 1.056338, Accuracy: 62.30%\n",
      "Batch 213, Loss: 1.133913, Accuracy: 62.30%\n",
      "Training - Epoch 13, Loss: 1.116578, Accuracy: 62.30%\n",
      "Validation Batch 1, Loss: 1.286924, Accuracy: 39.06%\n",
      "Validation Batch 2, Loss: 1.305515, Accuracy: 39.84%\n",
      "Validation Batch 3, Loss: 1.391037, Accuracy: 36.98%\n",
      "Validation Batch 4, Loss: 1.263570, Accuracy: 38.28%\n",
      "Validation Batch 5, Loss: 1.373177, Accuracy: 36.88%\n",
      "Validation Batch 6, Loss: 1.386533, Accuracy: 35.68%\n",
      "Validation Batch 7, Loss: 1.310849, Accuracy: 36.61%\n",
      "Validation Batch 8, Loss: 1.313642, Accuracy: 36.91%\n",
      "Validation Batch 9, Loss: 1.364415, Accuracy: 36.46%\n",
      "Validation Batch 10, Loss: 1.358202, Accuracy: 36.25%\n",
      "Validation Batch 11, Loss: 1.312989, Accuracy: 36.36%\n",
      "Validation Batch 12, Loss: 1.271255, Accuracy: 37.11%\n",
      "Validation Batch 13, Loss: 1.346561, Accuracy: 37.02%\n",
      "Validation Batch 14, Loss: 1.230866, Accuracy: 37.83%\n",
      "Validation Batch 15, Loss: 1.298389, Accuracy: 38.12%\n",
      "Validation Batch 16, Loss: 1.367383, Accuracy: 37.70%\n",
      "Validation Batch 17, Loss: 1.357824, Accuracy: 37.50%\n",
      "Validation Batch 18, Loss: 1.307590, Accuracy: 37.67%\n",
      "Validation Batch 19, Loss: 1.340524, Accuracy: 37.66%\n",
      "Validation Batch 20, Loss: 1.313330, Accuracy: 37.81%\n",
      "Validation Batch 21, Loss: 1.330494, Accuracy: 37.72%\n",
      "Validation Batch 22, Loss: 1.327698, Accuracy: 37.71%\n",
      "Validation Batch 23, Loss: 1.347737, Accuracy: 37.64%\n",
      "Validation Batch 24, Loss: 1.363626, Accuracy: 37.37%\n",
      "Validation Batch 25, Loss: 1.282294, Accuracy: 37.56%\n",
      "Validation Batch 26, Loss: 1.239987, Accuracy: 37.98%\n",
      "Validation Batch 27, Loss: 1.348194, Accuracy: 37.93%\n",
      "Validation - Epoch 13, Loss: 1.323726, Accuracy: 37.93%\n",
      "Patienceâ€”0\n",
      "Epoch 14\n",
      "Batch 1, Loss: 1.089049, Accuracy: 67.19%\n",
      "Batch 2, Loss: 1.082670, Accuracy: 65.62%\n",
      "Batch 3, Loss: 1.165620, Accuracy: 62.50%\n",
      "Batch 4, Loss: 1.127747, Accuracy: 62.11%\n",
      "Batch 5, Loss: 1.109824, Accuracy: 62.19%\n",
      "Batch 6, Loss: 1.034949, Accuracy: 63.28%\n",
      "Batch 7, Loss: 1.100676, Accuracy: 63.17%\n",
      "Batch 8, Loss: 1.132787, Accuracy: 62.70%\n",
      "Batch 9, Loss: 1.144665, Accuracy: 62.50%\n",
      "Batch 10, Loss: 1.163844, Accuracy: 61.88%\n",
      "Batch 11, Loss: 1.102660, Accuracy: 61.65%\n",
      "Batch 12, Loss: 1.094635, Accuracy: 61.59%\n",
      "Batch 13, Loss: 1.113137, Accuracy: 61.78%\n",
      "Batch 14, Loss: 1.163054, Accuracy: 61.38%\n",
      "Batch 15, Loss: 1.179494, Accuracy: 60.83%\n",
      "Batch 16, Loss: 1.147681, Accuracy: 60.74%\n",
      "Batch 17, Loss: 1.151070, Accuracy: 60.66%\n",
      "Batch 18, Loss: 1.081239, Accuracy: 61.02%\n",
      "Batch 19, Loss: 1.102078, Accuracy: 61.18%\n",
      "Batch 20, Loss: 1.052637, Accuracy: 61.56%\n",
      "Batch 21, Loss: 1.099949, Accuracy: 61.68%\n",
      "Batch 22, Loss: 1.151678, Accuracy: 61.36%\n",
      "Batch 23, Loss: 1.096441, Accuracy: 61.48%\n",
      "Batch 24, Loss: 1.041707, Accuracy: 61.85%\n",
      "Batch 25, Loss: 1.125295, Accuracy: 61.69%\n",
      "Batch 26, Loss: 1.158007, Accuracy: 61.54%\n",
      "Batch 27, Loss: 1.056631, Accuracy: 61.81%\n",
      "Batch 28, Loss: 1.118176, Accuracy: 61.66%\n",
      "Batch 29, Loss: 1.127120, Accuracy: 61.58%\n",
      "Batch 30, Loss: 1.028819, Accuracy: 61.88%\n",
      "Batch 31, Loss: 1.170874, Accuracy: 61.74%\n",
      "Batch 32, Loss: 1.158769, Accuracy: 61.57%\n",
      "Batch 33, Loss: 1.056080, Accuracy: 61.93%\n",
      "Batch 34, Loss: 1.123743, Accuracy: 61.95%\n",
      "Batch 35, Loss: 1.143324, Accuracy: 61.83%\n",
      "Batch 36, Loss: 1.148039, Accuracy: 61.76%\n",
      "Batch 37, Loss: 1.069421, Accuracy: 61.95%\n",
      "Batch 38, Loss: 1.144325, Accuracy: 61.76%\n",
      "Batch 39, Loss: 1.145959, Accuracy: 61.66%\n",
      "Batch 40, Loss: 1.077345, Accuracy: 61.80%\n",
      "Batch 41, Loss: 1.073602, Accuracy: 61.93%\n",
      "Batch 42, Loss: 1.095674, Accuracy: 61.98%\n",
      "Batch 43, Loss: 1.140398, Accuracy: 61.92%\n",
      "Batch 44, Loss: 1.064161, Accuracy: 62.04%\n",
      "Batch 45, Loss: 1.184653, Accuracy: 61.94%\n",
      "Batch 46, Loss: 1.094452, Accuracy: 61.99%\n",
      "Batch 47, Loss: 1.069062, Accuracy: 62.13%\n",
      "Batch 48, Loss: 1.144375, Accuracy: 62.04%\n",
      "Batch 49, Loss: 1.115569, Accuracy: 62.05%\n",
      "Batch 50, Loss: 1.141543, Accuracy: 62.03%\n",
      "Batch 51, Loss: 1.110040, Accuracy: 62.04%\n",
      "Batch 52, Loss: 1.141262, Accuracy: 62.02%\n",
      "Batch 53, Loss: 1.073813, Accuracy: 62.09%\n",
      "Batch 54, Loss: 1.101453, Accuracy: 62.12%\n",
      "Batch 55, Loss: 1.100185, Accuracy: 62.16%\n",
      "Batch 56, Loss: 1.078846, Accuracy: 62.28%\n",
      "Batch 57, Loss: 1.062552, Accuracy: 62.34%\n",
      "Batch 58, Loss: 1.130478, Accuracy: 62.20%\n",
      "Batch 59, Loss: 1.147833, Accuracy: 62.08%\n",
      "Batch 60, Loss: 1.120538, Accuracy: 62.08%\n",
      "Batch 61, Loss: 1.074561, Accuracy: 62.17%\n",
      "Batch 62, Loss: 1.140576, Accuracy: 62.15%\n",
      "Batch 63, Loss: 1.122634, Accuracy: 62.13%\n",
      "Batch 64, Loss: 1.098200, Accuracy: 62.16%\n",
      "Batch 65, Loss: 1.218691, Accuracy: 61.95%\n",
      "Batch 66, Loss: 1.143234, Accuracy: 61.91%\n",
      "Batch 67, Loss: 1.033837, Accuracy: 62.06%\n",
      "Batch 68, Loss: 1.120887, Accuracy: 62.04%\n",
      "Batch 69, Loss: 1.148987, Accuracy: 61.98%\n",
      "Batch 70, Loss: 1.154912, Accuracy: 61.94%\n",
      "Batch 71, Loss: 1.137383, Accuracy: 61.91%\n",
      "Batch 72, Loss: 1.102258, Accuracy: 61.91%\n",
      "Batch 73, Loss: 1.040984, Accuracy: 62.07%\n",
      "Batch 74, Loss: 1.005664, Accuracy: 62.23%\n",
      "Batch 75, Loss: 1.166280, Accuracy: 62.17%\n",
      "Batch 76, Loss: 1.109010, Accuracy: 62.17%\n",
      "Batch 77, Loss: 1.090831, Accuracy: 62.20%\n",
      "Batch 78, Loss: 1.101244, Accuracy: 62.20%\n",
      "Batch 79, Loss: 1.058241, Accuracy: 62.28%\n",
      "Batch 80, Loss: 1.196911, Accuracy: 62.17%\n",
      "Batch 81, Loss: 1.054013, Accuracy: 62.25%\n",
      "Batch 82, Loss: 1.196952, Accuracy: 62.16%\n",
      "Batch 83, Loss: 1.124927, Accuracy: 62.16%\n",
      "Batch 84, Loss: 1.140526, Accuracy: 62.15%\n",
      "Batch 85, Loss: 1.134133, Accuracy: 62.11%\n",
      "Batch 86, Loss: 1.099867, Accuracy: 62.12%\n",
      "Batch 87, Loss: 1.141208, Accuracy: 62.10%\n",
      "Batch 88, Loss: 1.040531, Accuracy: 62.20%\n",
      "Batch 89, Loss: 1.209313, Accuracy: 62.10%\n",
      "Batch 90, Loss: 1.027810, Accuracy: 62.20%\n",
      "Batch 91, Loss: 1.097576, Accuracy: 62.23%\n",
      "Batch 92, Loss: 1.055790, Accuracy: 62.28%\n",
      "Batch 93, Loss: 1.072610, Accuracy: 62.30%\n",
      "Batch 94, Loss: 1.085803, Accuracy: 62.32%\n",
      "Batch 95, Loss: 1.090690, Accuracy: 62.34%\n",
      "Batch 96, Loss: 1.091248, Accuracy: 62.37%\n",
      "Batch 97, Loss: 1.221675, Accuracy: 62.24%\n",
      "Batch 98, Loss: 1.165919, Accuracy: 62.18%\n",
      "Batch 99, Loss: 1.128712, Accuracy: 62.15%\n",
      "Batch 100, Loss: 1.027159, Accuracy: 62.25%\n",
      "Batch 101, Loss: 1.105940, Accuracy: 62.27%\n",
      "Batch 102, Loss: 1.088917, Accuracy: 62.33%\n",
      "Batch 103, Loss: 1.119443, Accuracy: 62.32%\n",
      "Batch 104, Loss: 1.029189, Accuracy: 62.41%\n",
      "Batch 105, Loss: 1.165093, Accuracy: 62.37%\n",
      "Batch 106, Loss: 1.149509, Accuracy: 62.34%\n",
      "Batch 107, Loss: 1.212543, Accuracy: 62.21%\n",
      "Batch 108, Loss: 1.118576, Accuracy: 62.21%\n",
      "Batch 109, Loss: 1.098304, Accuracy: 62.24%\n",
      "Batch 110, Loss: 1.090919, Accuracy: 62.26%\n",
      "Batch 111, Loss: 1.194215, Accuracy: 62.19%\n",
      "Batch 112, Loss: 1.073317, Accuracy: 62.23%\n",
      "Batch 113, Loss: 1.156040, Accuracy: 62.18%\n",
      "Batch 114, Loss: 1.175789, Accuracy: 62.13%\n",
      "Batch 115, Loss: 1.137999, Accuracy: 62.09%\n",
      "Batch 116, Loss: 1.052241, Accuracy: 62.16%\n",
      "Batch 117, Loss: 1.054977, Accuracy: 62.21%\n",
      "Batch 118, Loss: 1.125576, Accuracy: 62.20%\n",
      "Batch 119, Loss: 1.045679, Accuracy: 62.25%\n",
      "Batch 120, Loss: 1.138281, Accuracy: 62.24%\n",
      "Batch 121, Loss: 1.049890, Accuracy: 62.31%\n",
      "Batch 122, Loss: 1.093443, Accuracy: 62.32%\n",
      "Batch 123, Loss: 1.055391, Accuracy: 62.37%\n",
      "Batch 124, Loss: 1.080280, Accuracy: 62.40%\n",
      "Batch 125, Loss: 1.202768, Accuracy: 62.31%\n",
      "Batch 126, Loss: 1.086089, Accuracy: 62.33%\n",
      "Batch 127, Loss: 1.149380, Accuracy: 62.32%\n",
      "Batch 128, Loss: 1.011820, Accuracy: 62.43%\n",
      "Batch 129, Loss: 1.160821, Accuracy: 62.39%\n",
      "Batch 130, Loss: 1.127196, Accuracy: 62.37%\n",
      "Batch 131, Loss: 1.092146, Accuracy: 62.38%\n",
      "Batch 132, Loss: 1.244331, Accuracy: 62.29%\n",
      "Batch 133, Loss: 1.091033, Accuracy: 62.31%\n",
      "Batch 134, Loss: 1.114444, Accuracy: 62.33%\n",
      "Batch 135, Loss: 1.045974, Accuracy: 62.40%\n",
      "Batch 136, Loss: 1.074336, Accuracy: 62.44%\n",
      "Batch 137, Loss: 1.106820, Accuracy: 62.44%\n",
      "Batch 138, Loss: 1.133274, Accuracy: 62.43%\n",
      "Batch 139, Loss: 1.076378, Accuracy: 62.48%\n",
      "Batch 140, Loss: 1.121296, Accuracy: 62.46%\n",
      "Batch 141, Loss: 1.080253, Accuracy: 62.48%\n",
      "Batch 142, Loss: 1.040763, Accuracy: 62.54%\n",
      "Batch 143, Loss: 1.115707, Accuracy: 62.55%\n",
      "Batch 144, Loss: 1.122116, Accuracy: 62.54%\n",
      "Batch 145, Loss: 1.168008, Accuracy: 62.51%\n",
      "Batch 146, Loss: 1.104356, Accuracy: 62.50%\n",
      "Batch 147, Loss: 1.153853, Accuracy: 62.46%\n",
      "Batch 148, Loss: 1.108426, Accuracy: 62.47%\n",
      "Batch 149, Loss: 1.121913, Accuracy: 62.45%\n",
      "Batch 150, Loss: 1.103757, Accuracy: 62.46%\n",
      "Batch 151, Loss: 1.061983, Accuracy: 62.51%\n",
      "Batch 152, Loss: 1.123811, Accuracy: 62.50%\n",
      "Batch 153, Loss: 1.184862, Accuracy: 62.44%\n",
      "Batch 154, Loss: 1.016966, Accuracy: 62.51%\n",
      "Batch 155, Loss: 1.113546, Accuracy: 62.50%\n",
      "Batch 156, Loss: 1.169295, Accuracy: 62.45%\n",
      "Batch 157, Loss: 1.055673, Accuracy: 62.47%\n",
      "Batch 158, Loss: 1.151561, Accuracy: 62.43%\n",
      "Batch 159, Loss: 1.114171, Accuracy: 62.41%\n",
      "Batch 160, Loss: 1.221147, Accuracy: 62.34%\n",
      "Batch 161, Loss: 1.077702, Accuracy: 62.37%\n",
      "Batch 162, Loss: 1.124197, Accuracy: 62.36%\n",
      "Batch 163, Loss: 1.130040, Accuracy: 62.35%\n",
      "Batch 164, Loss: 1.080936, Accuracy: 62.36%\n",
      "Batch 165, Loss: 1.029653, Accuracy: 62.41%\n",
      "Batch 166, Loss: 1.156392, Accuracy: 62.38%\n",
      "Batch 167, Loss: 1.147343, Accuracy: 62.36%\n",
      "Batch 168, Loss: 1.054010, Accuracy: 62.39%\n",
      "Batch 169, Loss: 1.052726, Accuracy: 62.44%\n",
      "Batch 170, Loss: 1.038168, Accuracy: 62.49%\n",
      "Batch 171, Loss: 1.042858, Accuracy: 62.52%\n",
      "Batch 172, Loss: 1.198589, Accuracy: 62.45%\n",
      "Batch 173, Loss: 1.104274, Accuracy: 62.46%\n",
      "Batch 174, Loss: 1.116663, Accuracy: 62.47%\n",
      "Batch 175, Loss: 1.129541, Accuracy: 62.45%\n",
      "Batch 176, Loss: 1.055128, Accuracy: 62.47%\n",
      "Batch 177, Loss: 1.028133, Accuracy: 62.52%\n",
      "Batch 178, Loss: 1.112809, Accuracy: 62.53%\n",
      "Batch 179, Loss: 1.061067, Accuracy: 62.55%\n",
      "Batch 180, Loss: 1.114226, Accuracy: 62.54%\n",
      "Batch 181, Loss: 1.084846, Accuracy: 62.56%\n",
      "Batch 182, Loss: 1.020477, Accuracy: 62.61%\n",
      "Batch 183, Loss: 1.141462, Accuracy: 62.59%\n",
      "Batch 184, Loss: 1.150779, Accuracy: 62.57%\n",
      "Batch 185, Loss: 1.125914, Accuracy: 62.58%\n",
      "Batch 186, Loss: 1.139313, Accuracy: 62.58%\n",
      "Batch 187, Loss: 1.115558, Accuracy: 62.58%\n",
      "Batch 188, Loss: 1.198519, Accuracy: 62.53%\n",
      "Batch 189, Loss: 1.165804, Accuracy: 62.51%\n",
      "Batch 190, Loss: 1.118688, Accuracy: 62.50%\n",
      "Batch 191, Loss: 1.107520, Accuracy: 62.52%\n",
      "Batch 192, Loss: 1.089555, Accuracy: 62.52%\n",
      "Batch 193, Loss: 1.062096, Accuracy: 62.56%\n",
      "Batch 194, Loss: 1.128149, Accuracy: 62.54%\n",
      "Batch 195, Loss: 1.217120, Accuracy: 62.48%\n",
      "Batch 196, Loss: 1.099774, Accuracy: 62.49%\n",
      "Batch 197, Loss: 1.087705, Accuracy: 62.52%\n",
      "Batch 198, Loss: 1.108295, Accuracy: 62.53%\n",
      "Batch 199, Loss: 1.129708, Accuracy: 62.51%\n",
      "Batch 200, Loss: 1.168494, Accuracy: 62.48%\n",
      "Batch 201, Loss: 1.095675, Accuracy: 62.48%\n",
      "Batch 202, Loss: 1.144416, Accuracy: 62.45%\n",
      "Batch 203, Loss: 1.133196, Accuracy: 62.44%\n",
      "Batch 204, Loss: 1.182073, Accuracy: 62.39%\n",
      "Batch 205, Loss: 1.133942, Accuracy: 62.39%\n",
      "Batch 206, Loss: 1.064393, Accuracy: 62.42%\n",
      "Batch 207, Loss: 1.150865, Accuracy: 62.38%\n",
      "Batch 208, Loss: 1.081935, Accuracy: 62.40%\n",
      "Batch 209, Loss: 1.141944, Accuracy: 62.39%\n",
      "Batch 210, Loss: 1.007160, Accuracy: 62.45%\n",
      "Batch 211, Loss: 1.105778, Accuracy: 62.46%\n",
      "Batch 212, Loss: 1.039273, Accuracy: 62.50%\n",
      "Batch 213, Loss: 1.158012, Accuracy: 62.47%\n",
      "Training - Epoch 14, Loss: 1.111635, Accuracy: 62.47%\n",
      "Validation Batch 1, Loss: 1.274541, Accuracy: 40.62%\n",
      "Validation Batch 2, Loss: 1.300717, Accuracy: 40.62%\n",
      "Validation Batch 3, Loss: 1.384252, Accuracy: 37.50%\n",
      "Validation Batch 4, Loss: 1.252240, Accuracy: 38.67%\n",
      "Validation Batch 5, Loss: 1.363869, Accuracy: 37.50%\n",
      "Validation Batch 6, Loss: 1.374829, Accuracy: 36.72%\n",
      "Validation Batch 7, Loss: 1.299597, Accuracy: 37.50%\n",
      "Validation Batch 8, Loss: 1.303738, Accuracy: 37.70%\n",
      "Validation Batch 9, Loss: 1.358343, Accuracy: 37.15%\n",
      "Validation Batch 10, Loss: 1.346364, Accuracy: 36.88%\n",
      "Validation Batch 11, Loss: 1.305945, Accuracy: 37.07%\n",
      "Validation Batch 12, Loss: 1.262224, Accuracy: 37.76%\n",
      "Validation Batch 13, Loss: 1.338051, Accuracy: 37.86%\n",
      "Validation Batch 14, Loss: 1.228122, Accuracy: 38.62%\n",
      "Validation Batch 15, Loss: 1.292175, Accuracy: 38.85%\n",
      "Validation Batch 16, Loss: 1.354749, Accuracy: 38.48%\n",
      "Validation Batch 17, Loss: 1.354360, Accuracy: 38.33%\n",
      "Validation Batch 18, Loss: 1.297080, Accuracy: 38.54%\n",
      "Validation Batch 19, Loss: 1.335963, Accuracy: 38.49%\n",
      "Validation Batch 20, Loss: 1.304712, Accuracy: 38.59%\n",
      "Validation Batch 21, Loss: 1.324472, Accuracy: 38.47%\n",
      "Validation Batch 22, Loss: 1.319384, Accuracy: 38.49%\n",
      "Validation Batch 23, Loss: 1.345572, Accuracy: 38.38%\n",
      "Validation Batch 24, Loss: 1.352865, Accuracy: 38.22%\n",
      "Validation Batch 25, Loss: 1.276603, Accuracy: 38.38%\n",
      "Validation Batch 26, Loss: 1.231915, Accuracy: 38.76%\n",
      "Validation Batch 27, Loss: 1.340899, Accuracy: 38.70%\n",
      "Validation - Epoch 14, Loss: 1.315688, Accuracy: 38.70%\n",
      "Patienceâ€”0\n",
      "Epoch 15\n",
      "Batch 1, Loss: 1.168470, Accuracy: 57.81%\n",
      "Batch 2, Loss: 1.136315, Accuracy: 58.59%\n",
      "Batch 3, Loss: 1.136373, Accuracy: 57.81%\n",
      "Batch 4, Loss: 1.052790, Accuracy: 60.55%\n",
      "Batch 5, Loss: 1.128403, Accuracy: 60.31%\n",
      "Batch 6, Loss: 1.098579, Accuracy: 60.42%\n",
      "Batch 7, Loss: 1.119484, Accuracy: 60.49%\n",
      "Batch 8, Loss: 1.178363, Accuracy: 60.16%\n",
      "Batch 9, Loss: 1.145670, Accuracy: 60.24%\n",
      "Batch 10, Loss: 1.159269, Accuracy: 59.84%\n",
      "Batch 11, Loss: 1.006971, Accuracy: 61.36%\n",
      "Batch 12, Loss: 1.147689, Accuracy: 61.33%\n",
      "Batch 13, Loss: 1.092668, Accuracy: 61.66%\n",
      "Batch 14, Loss: 1.055511, Accuracy: 62.17%\n",
      "Batch 15, Loss: 1.136658, Accuracy: 61.98%\n",
      "Batch 16, Loss: 1.114094, Accuracy: 61.82%\n",
      "Batch 17, Loss: 1.123462, Accuracy: 61.76%\n",
      "Batch 18, Loss: 1.181840, Accuracy: 61.46%\n",
      "Batch 19, Loss: 1.103293, Accuracy: 61.43%\n",
      "Batch 20, Loss: 1.057661, Accuracy: 61.80%\n",
      "Batch 21, Loss: 1.093395, Accuracy: 61.98%\n",
      "Batch 22, Loss: 1.027683, Accuracy: 62.50%\n",
      "Batch 23, Loss: 1.136651, Accuracy: 62.30%\n",
      "Batch 24, Loss: 1.063004, Accuracy: 62.63%\n",
      "Batch 25, Loss: 0.985029, Accuracy: 63.19%\n",
      "Batch 26, Loss: 1.074177, Accuracy: 63.28%\n",
      "Batch 27, Loss: 1.094165, Accuracy: 63.43%\n",
      "Batch 28, Loss: 1.146630, Accuracy: 63.23%\n",
      "Batch 29, Loss: 1.147926, Accuracy: 63.04%\n",
      "Batch 30, Loss: 1.083252, Accuracy: 63.23%\n",
      "Batch 31, Loss: 1.062819, Accuracy: 63.41%\n",
      "Batch 32, Loss: 1.084002, Accuracy: 63.48%\n",
      "Batch 33, Loss: 1.182644, Accuracy: 63.21%\n",
      "Batch 34, Loss: 1.149987, Accuracy: 63.01%\n",
      "Batch 35, Loss: 1.042547, Accuracy: 63.21%\n",
      "Batch 36, Loss: 1.079536, Accuracy: 63.32%\n",
      "Batch 37, Loss: 1.040049, Accuracy: 63.60%\n",
      "Batch 38, Loss: 1.049482, Accuracy: 63.77%\n",
      "Batch 39, Loss: 1.145001, Accuracy: 63.62%\n",
      "Batch 40, Loss: 1.143922, Accuracy: 63.40%\n",
      "Batch 41, Loss: 1.072384, Accuracy: 63.41%\n",
      "Batch 42, Loss: 1.074698, Accuracy: 63.43%\n",
      "Batch 43, Loss: 1.193595, Accuracy: 63.19%\n",
      "Batch 44, Loss: 1.112823, Accuracy: 63.14%\n",
      "Batch 45, Loss: 1.020026, Accuracy: 63.37%\n",
      "Batch 46, Loss: 1.044589, Accuracy: 63.42%\n",
      "Batch 47, Loss: 1.081689, Accuracy: 63.46%\n",
      "Batch 48, Loss: 1.046594, Accuracy: 63.64%\n",
      "Batch 49, Loss: 1.172379, Accuracy: 63.52%\n",
      "Batch 50, Loss: 1.140048, Accuracy: 63.47%\n",
      "Batch 51, Loss: 1.116350, Accuracy: 63.51%\n",
      "Batch 52, Loss: 1.186017, Accuracy: 63.28%\n",
      "Batch 53, Loss: 1.111650, Accuracy: 63.24%\n",
      "Batch 54, Loss: 1.106742, Accuracy: 63.22%\n",
      "Batch 55, Loss: 1.091286, Accuracy: 63.24%\n",
      "Batch 56, Loss: 1.149763, Accuracy: 63.14%\n",
      "Batch 57, Loss: 1.089574, Accuracy: 63.19%\n",
      "Batch 58, Loss: 1.061621, Accuracy: 63.25%\n",
      "Batch 59, Loss: 1.153617, Accuracy: 63.16%\n",
      "Batch 60, Loss: 1.147235, Accuracy: 63.12%\n",
      "Batch 61, Loss: 1.165456, Accuracy: 63.01%\n",
      "Batch 62, Loss: 1.088346, Accuracy: 63.05%\n",
      "Batch 63, Loss: 1.108152, Accuracy: 63.07%\n",
      "Batch 64, Loss: 1.098140, Accuracy: 63.13%\n",
      "Batch 65, Loss: 1.185441, Accuracy: 62.98%\n",
      "Batch 66, Loss: 1.179317, Accuracy: 62.86%\n",
      "Batch 67, Loss: 1.098372, Accuracy: 62.87%\n",
      "Batch 68, Loss: 1.048787, Accuracy: 62.96%\n",
      "Batch 69, Loss: 1.075309, Accuracy: 63.02%\n",
      "Batch 70, Loss: 1.084636, Accuracy: 63.08%\n",
      "Batch 71, Loss: 1.095058, Accuracy: 63.12%\n",
      "Batch 72, Loss: 1.111252, Accuracy: 63.11%\n",
      "Batch 73, Loss: 1.138315, Accuracy: 63.04%\n",
      "Batch 74, Loss: 1.080809, Accuracy: 63.05%\n",
      "Batch 75, Loss: 1.029594, Accuracy: 63.17%\n",
      "Batch 76, Loss: 1.122645, Accuracy: 63.14%\n",
      "Batch 77, Loss: 1.099824, Accuracy: 63.11%\n",
      "Batch 78, Loss: 1.107921, Accuracy: 63.12%\n",
      "Batch 79, Loss: 1.146653, Accuracy: 63.03%\n",
      "Batch 80, Loss: 1.116104, Accuracy: 63.05%\n",
      "Batch 81, Loss: 1.021489, Accuracy: 63.19%\n",
      "Batch 82, Loss: 1.084679, Accuracy: 63.22%\n",
      "Batch 83, Loss: 1.118013, Accuracy: 63.22%\n",
      "Batch 84, Loss: 1.085012, Accuracy: 63.26%\n",
      "Batch 85, Loss: 1.173781, Accuracy: 63.18%\n",
      "Batch 86, Loss: 1.024541, Accuracy: 63.26%\n",
      "Batch 87, Loss: 1.149154, Accuracy: 63.18%\n",
      "Batch 88, Loss: 1.075849, Accuracy: 63.21%\n",
      "Batch 89, Loss: 1.155493, Accuracy: 63.18%\n",
      "Batch 90, Loss: 1.137986, Accuracy: 63.11%\n",
      "Batch 91, Loss: 1.129630, Accuracy: 63.08%\n",
      "Batch 92, Loss: 1.062132, Accuracy: 63.15%\n",
      "Batch 93, Loss: 1.085485, Accuracy: 63.16%\n",
      "Batch 94, Loss: 1.129918, Accuracy: 63.12%\n",
      "Batch 95, Loss: 1.159374, Accuracy: 63.04%\n",
      "Batch 96, Loss: 1.150852, Accuracy: 62.99%\n",
      "Batch 97, Loss: 1.085910, Accuracy: 63.03%\n",
      "Batch 98, Loss: 1.127734, Accuracy: 63.03%\n",
      "Batch 99, Loss: 1.074393, Accuracy: 63.07%\n",
      "Batch 100, Loss: 1.056167, Accuracy: 63.12%\n",
      "Batch 101, Loss: 1.116538, Accuracy: 63.15%\n",
      "Batch 102, Loss: 1.100255, Accuracy: 63.14%\n",
      "Batch 103, Loss: 1.129552, Accuracy: 63.12%\n",
      "Batch 104, Loss: 1.137738, Accuracy: 63.09%\n",
      "Batch 105, Loss: 1.097890, Accuracy: 63.08%\n",
      "Batch 106, Loss: 1.091244, Accuracy: 63.07%\n",
      "Batch 107, Loss: 1.045332, Accuracy: 63.13%\n",
      "Batch 108, Loss: 1.098132, Accuracy: 63.15%\n",
      "Batch 109, Loss: 1.190153, Accuracy: 63.07%\n",
      "Batch 110, Loss: 1.166762, Accuracy: 63.03%\n",
      "Batch 111, Loss: 1.122352, Accuracy: 63.01%\n",
      "Batch 112, Loss: 1.040386, Accuracy: 63.09%\n",
      "Batch 113, Loss: 1.073610, Accuracy: 63.15%\n",
      "Batch 114, Loss: 1.175624, Accuracy: 63.08%\n",
      "Batch 115, Loss: 1.151491, Accuracy: 63.03%\n",
      "Batch 116, Loss: 1.107404, Accuracy: 63.04%\n",
      "Batch 117, Loss: 1.108755, Accuracy: 63.03%\n",
      "Batch 118, Loss: 1.076831, Accuracy: 63.06%\n",
      "Batch 119, Loss: 1.165810, Accuracy: 62.99%\n",
      "Batch 120, Loss: 1.207164, Accuracy: 62.89%\n",
      "Batch 121, Loss: 1.095474, Accuracy: 62.86%\n",
      "Batch 122, Loss: 1.098665, Accuracy: 62.88%\n",
      "Batch 123, Loss: 1.070747, Accuracy: 62.92%\n",
      "Batch 124, Loss: 1.075258, Accuracy: 62.97%\n",
      "Batch 125, Loss: 1.130176, Accuracy: 62.92%\n",
      "Batch 126, Loss: 1.017992, Accuracy: 63.01%\n",
      "Batch 127, Loss: 1.101144, Accuracy: 62.98%\n",
      "Batch 128, Loss: 1.175246, Accuracy: 62.89%\n",
      "Batch 129, Loss: 1.119530, Accuracy: 62.89%\n",
      "Batch 130, Loss: 1.063896, Accuracy: 62.87%\n",
      "Batch 131, Loss: 1.055471, Accuracy: 62.93%\n",
      "Batch 132, Loss: 1.158975, Accuracy: 62.88%\n",
      "Batch 133, Loss: 1.167216, Accuracy: 62.81%\n",
      "Batch 134, Loss: 1.157800, Accuracy: 62.76%\n",
      "Batch 135, Loss: 1.180383, Accuracy: 62.72%\n",
      "Batch 136, Loss: 1.093618, Accuracy: 62.74%\n",
      "Batch 137, Loss: 1.122880, Accuracy: 62.72%\n",
      "Batch 138, Loss: 1.100146, Accuracy: 62.73%\n",
      "Batch 139, Loss: 1.071063, Accuracy: 62.76%\n",
      "Batch 140, Loss: 1.099004, Accuracy: 62.77%\n",
      "Batch 141, Loss: 1.140317, Accuracy: 62.74%\n",
      "Batch 142, Loss: 1.131431, Accuracy: 62.74%\n",
      "Batch 143, Loss: 1.049530, Accuracy: 62.80%\n",
      "Batch 144, Loss: 1.083090, Accuracy: 62.83%\n",
      "Batch 145, Loss: 1.115461, Accuracy: 62.83%\n",
      "Batch 146, Loss: 1.110971, Accuracy: 62.83%\n",
      "Batch 147, Loss: 1.074199, Accuracy: 62.85%\n",
      "Batch 148, Loss: 1.079831, Accuracy: 62.87%\n",
      "Batch 149, Loss: 1.158653, Accuracy: 62.81%\n",
      "Batch 150, Loss: 1.132886, Accuracy: 62.78%\n",
      "Batch 151, Loss: 1.076485, Accuracy: 62.81%\n",
      "Batch 152, Loss: 1.062294, Accuracy: 62.85%\n",
      "Batch 153, Loss: 1.110622, Accuracy: 62.83%\n",
      "Batch 154, Loss: 1.069723, Accuracy: 62.88%\n",
      "Batch 155, Loss: 1.135315, Accuracy: 62.85%\n",
      "Batch 156, Loss: 1.167066, Accuracy: 62.81%\n",
      "Batch 157, Loss: 1.095451, Accuracy: 62.85%\n",
      "Batch 158, Loss: 1.056774, Accuracy: 62.92%\n",
      "Batch 159, Loss: 1.028471, Accuracy: 62.99%\n",
      "Batch 160, Loss: 1.026128, Accuracy: 63.05%\n",
      "Batch 161, Loss: 1.158923, Accuracy: 63.00%\n",
      "Batch 162, Loss: 1.148428, Accuracy: 62.96%\n",
      "Batch 163, Loss: 1.065637, Accuracy: 62.98%\n",
      "Batch 164, Loss: 1.051693, Accuracy: 63.02%\n",
      "Batch 165, Loss: 1.082935, Accuracy: 63.03%\n",
      "Batch 166, Loss: 1.116909, Accuracy: 63.03%\n",
      "Batch 167, Loss: 1.085317, Accuracy: 63.06%\n",
      "Batch 168, Loss: 1.078254, Accuracy: 63.10%\n",
      "Batch 169, Loss: 1.093881, Accuracy: 63.11%\n",
      "Batch 170, Loss: 1.084448, Accuracy: 63.14%\n",
      "Batch 171, Loss: 1.211291, Accuracy: 63.08%\n",
      "Batch 172, Loss: 1.131055, Accuracy: 63.07%\n",
      "Batch 173, Loss: 1.058360, Accuracy: 63.11%\n",
      "Batch 174, Loss: 1.014457, Accuracy: 63.15%\n",
      "Batch 175, Loss: 1.075825, Accuracy: 63.19%\n",
      "Batch 176, Loss: 0.997466, Accuracy: 63.26%\n",
      "Batch 177, Loss: 1.141275, Accuracy: 63.23%\n",
      "Batch 178, Loss: 0.996355, Accuracy: 63.30%\n",
      "Batch 179, Loss: 1.128620, Accuracy: 63.30%\n",
      "Batch 180, Loss: 1.099316, Accuracy: 63.28%\n",
      "Batch 181, Loss: 1.120224, Accuracy: 63.28%\n",
      "Batch 182, Loss: 1.093944, Accuracy: 63.27%\n",
      "Batch 183, Loss: 1.095900, Accuracy: 63.29%\n",
      "Batch 184, Loss: 1.111235, Accuracy: 63.29%\n",
      "Batch 185, Loss: 1.121100, Accuracy: 63.29%\n",
      "Batch 186, Loss: 1.156285, Accuracy: 63.24%\n",
      "Batch 187, Loss: 1.101816, Accuracy: 63.25%\n",
      "Batch 188, Loss: 1.185377, Accuracy: 63.19%\n",
      "Batch 189, Loss: 1.133348, Accuracy: 63.19%\n",
      "Batch 190, Loss: 1.159733, Accuracy: 63.16%\n",
      "Batch 191, Loss: 1.147981, Accuracy: 63.14%\n",
      "Batch 192, Loss: 1.102213, Accuracy: 63.15%\n",
      "Batch 193, Loss: 1.092672, Accuracy: 63.18%\n",
      "Batch 194, Loss: 1.140260, Accuracy: 63.14%\n",
      "Batch 195, Loss: 1.139148, Accuracy: 63.11%\n",
      "Batch 196, Loss: 1.023506, Accuracy: 63.15%\n",
      "Batch 197, Loss: 1.185626, Accuracy: 63.10%\n",
      "Batch 198, Loss: 1.059544, Accuracy: 63.14%\n",
      "Batch 199, Loss: 1.112848, Accuracy: 63.13%\n",
      "Batch 200, Loss: 1.037853, Accuracy: 63.16%\n",
      "Batch 201, Loss: 1.167355, Accuracy: 63.14%\n",
      "Batch 202, Loss: 1.058044, Accuracy: 63.16%\n",
      "Batch 203, Loss: 0.995994, Accuracy: 63.22%\n",
      "Batch 204, Loss: 1.098216, Accuracy: 63.22%\n",
      "Batch 205, Loss: 1.129238, Accuracy: 63.20%\n",
      "Batch 206, Loss: 1.068015, Accuracy: 63.20%\n",
      "Batch 207, Loss: 1.012937, Accuracy: 63.23%\n",
      "Batch 208, Loss: 1.111366, Accuracy: 63.23%\n",
      "Batch 209, Loss: 1.016273, Accuracy: 63.28%\n",
      "Batch 210, Loss: 1.063660, Accuracy: 63.30%\n",
      "Batch 211, Loss: 1.121027, Accuracy: 63.29%\n",
      "Batch 212, Loss: 1.003353, Accuracy: 63.35%\n",
      "Batch 213, Loss: 1.227526, Accuracy: 63.32%\n",
      "Training - Epoch 15, Loss: 1.104659, Accuracy: 63.32%\n",
      "Validation Batch 1, Loss: 1.265465, Accuracy: 40.62%\n",
      "Validation Batch 2, Loss: 1.297823, Accuracy: 40.62%\n",
      "Validation Batch 3, Loss: 1.381044, Accuracy: 37.50%\n",
      "Validation Batch 4, Loss: 1.243895, Accuracy: 39.45%\n",
      "Validation Batch 5, Loss: 1.354420, Accuracy: 38.44%\n",
      "Validation Batch 6, Loss: 1.365953, Accuracy: 37.50%\n",
      "Validation Batch 7, Loss: 1.295442, Accuracy: 38.17%\n",
      "Validation Batch 8, Loss: 1.294960, Accuracy: 38.48%\n",
      "Validation Batch 9, Loss: 1.354783, Accuracy: 37.85%\n",
      "Validation Batch 10, Loss: 1.338489, Accuracy: 37.50%\n",
      "Validation Batch 11, Loss: 1.301616, Accuracy: 37.64%\n",
      "Validation Batch 12, Loss: 1.252389, Accuracy: 38.41%\n",
      "Validation Batch 13, Loss: 1.333125, Accuracy: 38.58%\n",
      "Validation Batch 14, Loss: 1.222326, Accuracy: 39.29%\n",
      "Validation Batch 15, Loss: 1.287277, Accuracy: 39.48%\n",
      "Validation Batch 16, Loss: 1.347004, Accuracy: 39.06%\n",
      "Validation Batch 17, Loss: 1.350523, Accuracy: 38.88%\n",
      "Validation Batch 18, Loss: 1.290095, Accuracy: 39.15%\n",
      "Validation Batch 19, Loss: 1.329776, Accuracy: 39.06%\n",
      "Validation Batch 20, Loss: 1.298560, Accuracy: 39.22%\n",
      "Validation Batch 21, Loss: 1.322150, Accuracy: 39.06%\n",
      "Validation Batch 22, Loss: 1.314141, Accuracy: 39.06%\n",
      "Validation Batch 23, Loss: 1.341894, Accuracy: 38.99%\n",
      "Validation Batch 24, Loss: 1.341658, Accuracy: 38.87%\n",
      "Validation Batch 25, Loss: 1.271108, Accuracy: 39.06%\n",
      "Validation Batch 26, Loss: 1.221709, Accuracy: 39.54%\n",
      "Validation Batch 27, Loss: 1.336587, Accuracy: 39.46%\n",
      "Validation - Epoch 15, Loss: 1.309415, Accuracy: 39.46%\n",
      "Patienceâ€”0\n",
      "Epoch 16\n",
      "Batch 1, Loss: 1.065199, Accuracy: 67.19%\n",
      "Batch 2, Loss: 1.133785, Accuracy: 63.28%\n",
      "Batch 3, Loss: 1.081879, Accuracy: 63.02%\n",
      "Batch 4, Loss: 1.049302, Accuracy: 65.23%\n",
      "Batch 5, Loss: 1.099099, Accuracy: 65.62%\n",
      "Batch 6, Loss: 1.182429, Accuracy: 63.28%\n",
      "Batch 7, Loss: 1.143614, Accuracy: 62.95%\n",
      "Batch 8, Loss: 1.151325, Accuracy: 62.11%\n",
      "Batch 9, Loss: 1.089525, Accuracy: 62.67%\n",
      "Batch 10, Loss: 1.044938, Accuracy: 63.28%\n",
      "Batch 11, Loss: 1.114348, Accuracy: 63.21%\n",
      "Batch 12, Loss: 1.148003, Accuracy: 63.02%\n",
      "Batch 13, Loss: 1.130499, Accuracy: 62.98%\n",
      "Batch 14, Loss: 1.054605, Accuracy: 63.39%\n",
      "Batch 15, Loss: 1.092355, Accuracy: 63.65%\n",
      "Batch 16, Loss: 1.129136, Accuracy: 63.28%\n",
      "Batch 17, Loss: 1.076344, Accuracy: 63.42%\n",
      "Batch 18, Loss: 1.169502, Accuracy: 63.11%\n",
      "Batch 19, Loss: 1.121885, Accuracy: 62.91%\n",
      "Batch 20, Loss: 1.036752, Accuracy: 63.44%\n",
      "Batch 21, Loss: 1.143405, Accuracy: 63.24%\n",
      "Batch 22, Loss: 1.231145, Accuracy: 62.64%\n",
      "Batch 23, Loss: 1.108786, Accuracy: 62.50%\n",
      "Batch 24, Loss: 1.054648, Accuracy: 62.83%\n",
      "Batch 25, Loss: 1.125712, Accuracy: 62.75%\n",
      "Batch 26, Loss: 1.100758, Accuracy: 62.86%\n",
      "Batch 27, Loss: 1.099323, Accuracy: 62.85%\n",
      "Batch 28, Loss: 1.113761, Accuracy: 62.83%\n",
      "Batch 29, Loss: 1.169936, Accuracy: 62.61%\n",
      "Batch 30, Loss: 1.099713, Accuracy: 62.71%\n",
      "Batch 31, Loss: 1.193125, Accuracy: 62.35%\n",
      "Batch 32, Loss: 1.120830, Accuracy: 62.35%\n",
      "Batch 33, Loss: 1.220555, Accuracy: 61.98%\n",
      "Batch 34, Loss: 1.169283, Accuracy: 61.81%\n",
      "Batch 35, Loss: 1.066777, Accuracy: 61.96%\n",
      "Batch 36, Loss: 1.121937, Accuracy: 62.07%\n",
      "Batch 37, Loss: 1.045123, Accuracy: 62.33%\n",
      "Batch 38, Loss: 1.197280, Accuracy: 62.09%\n",
      "Batch 39, Loss: 1.131230, Accuracy: 62.10%\n",
      "Batch 40, Loss: 0.935012, Accuracy: 62.62%\n",
      "Batch 41, Loss: 1.128617, Accuracy: 62.50%\n",
      "Batch 42, Loss: 1.077762, Accuracy: 62.65%\n",
      "Batch 43, Loss: 1.109011, Accuracy: 62.65%\n",
      "Batch 44, Loss: 1.068916, Accuracy: 62.71%\n",
      "Batch 45, Loss: 1.116049, Accuracy: 62.74%\n",
      "Batch 46, Loss: 1.099215, Accuracy: 62.70%\n",
      "Batch 47, Loss: 1.079334, Accuracy: 62.80%\n",
      "Batch 48, Loss: 1.043638, Accuracy: 62.99%\n",
      "Batch 49, Loss: 1.054929, Accuracy: 63.17%\n",
      "Batch 50, Loss: 1.031286, Accuracy: 63.34%\n",
      "Batch 51, Loss: 1.072110, Accuracy: 63.39%\n",
      "Batch 52, Loss: 1.146701, Accuracy: 63.28%\n",
      "Batch 53, Loss: 1.153783, Accuracy: 63.18%\n",
      "Batch 54, Loss: 1.081838, Accuracy: 63.28%\n",
      "Batch 55, Loss: 1.068096, Accuracy: 63.32%\n",
      "Batch 56, Loss: 1.152082, Accuracy: 63.23%\n",
      "Batch 57, Loss: 1.102039, Accuracy: 63.27%\n",
      "Batch 58, Loss: 1.082263, Accuracy: 63.28%\n",
      "Batch 59, Loss: 1.184824, Accuracy: 63.16%\n",
      "Batch 60, Loss: 1.103562, Accuracy: 63.15%\n",
      "Batch 61, Loss: 1.056997, Accuracy: 63.29%\n",
      "Batch 62, Loss: 1.091266, Accuracy: 63.33%\n",
      "Batch 63, Loss: 1.080682, Accuracy: 63.37%\n",
      "Batch 64, Loss: 1.071487, Accuracy: 63.40%\n",
      "Batch 65, Loss: 1.115159, Accuracy: 63.37%\n",
      "Batch 66, Loss: 1.104561, Accuracy: 63.38%\n",
      "Batch 67, Loss: 1.124623, Accuracy: 63.32%\n",
      "Batch 68, Loss: 1.023832, Accuracy: 63.47%\n",
      "Batch 69, Loss: 1.073812, Accuracy: 63.54%\n",
      "Batch 70, Loss: 1.133752, Accuracy: 63.46%\n",
      "Batch 71, Loss: 1.040991, Accuracy: 63.58%\n",
      "Batch 72, Loss: 1.085238, Accuracy: 63.59%\n",
      "Batch 73, Loss: 1.181719, Accuracy: 63.48%\n",
      "Batch 74, Loss: 1.106271, Accuracy: 63.49%\n",
      "Batch 75, Loss: 1.094972, Accuracy: 63.48%\n",
      "Batch 76, Loss: 1.203541, Accuracy: 63.34%\n",
      "Batch 77, Loss: 1.142450, Accuracy: 63.29%\n",
      "Batch 78, Loss: 1.018532, Accuracy: 63.42%\n",
      "Batch 79, Loss: 1.137112, Accuracy: 63.35%\n",
      "Batch 80, Loss: 1.069153, Accuracy: 63.42%\n",
      "Batch 81, Loss: 1.107492, Accuracy: 63.41%\n",
      "Batch 82, Loss: 1.079038, Accuracy: 63.41%\n",
      "Batch 83, Loss: 1.229756, Accuracy: 63.22%\n",
      "Batch 84, Loss: 1.081374, Accuracy: 63.26%\n",
      "Batch 85, Loss: 1.058197, Accuracy: 63.31%\n",
      "Batch 86, Loss: 1.093703, Accuracy: 63.32%\n",
      "Batch 87, Loss: 1.129472, Accuracy: 63.29%\n",
      "Batch 88, Loss: 1.229974, Accuracy: 63.12%\n",
      "Batch 89, Loss: 1.081591, Accuracy: 63.13%\n",
      "Batch 90, Loss: 1.096309, Accuracy: 63.14%\n",
      "Batch 91, Loss: 1.129319, Accuracy: 63.14%\n",
      "Batch 92, Loss: 1.115894, Accuracy: 63.11%\n",
      "Batch 93, Loss: 1.166883, Accuracy: 63.05%\n",
      "Batch 94, Loss: 1.094891, Accuracy: 63.10%\n",
      "Batch 95, Loss: 1.129366, Accuracy: 63.08%\n",
      "Batch 96, Loss: 1.208968, Accuracy: 62.99%\n",
      "Batch 97, Loss: 1.084819, Accuracy: 62.98%\n",
      "Batch 98, Loss: 1.159069, Accuracy: 62.91%\n",
      "Batch 99, Loss: 1.117507, Accuracy: 62.91%\n",
      "Batch 100, Loss: 1.041334, Accuracy: 62.98%\n",
      "Batch 101, Loss: 1.089964, Accuracy: 63.01%\n",
      "Batch 102, Loss: 1.144571, Accuracy: 62.99%\n",
      "Batch 103, Loss: 1.111101, Accuracy: 63.02%\n",
      "Batch 104, Loss: 1.129578, Accuracy: 62.98%\n",
      "Batch 105, Loss: 1.134479, Accuracy: 62.95%\n",
      "Batch 106, Loss: 1.177951, Accuracy: 62.87%\n",
      "Batch 107, Loss: 1.077177, Accuracy: 62.94%\n",
      "Batch 108, Loss: 1.075717, Accuracy: 62.99%\n",
      "Batch 109, Loss: 1.094247, Accuracy: 62.96%\n",
      "Batch 110, Loss: 1.084557, Accuracy: 63.01%\n",
      "Batch 111, Loss: 1.111336, Accuracy: 63.02%\n",
      "Batch 112, Loss: 0.975375, Accuracy: 63.14%\n",
      "Batch 113, Loss: 1.061479, Accuracy: 63.18%\n",
      "Batch 114, Loss: 1.067938, Accuracy: 63.23%\n",
      "Batch 115, Loss: 1.046708, Accuracy: 63.29%\n",
      "Batch 116, Loss: 1.137617, Accuracy: 63.29%\n",
      "Batch 117, Loss: 1.095551, Accuracy: 63.30%\n",
      "Batch 118, Loss: 1.131470, Accuracy: 63.24%\n",
      "Batch 119, Loss: 1.021598, Accuracy: 63.30%\n",
      "Batch 120, Loss: 1.163410, Accuracy: 63.28%\n",
      "Batch 121, Loss: 1.065286, Accuracy: 63.31%\n",
      "Batch 122, Loss: 1.093734, Accuracy: 63.29%\n",
      "Batch 123, Loss: 1.180255, Accuracy: 63.21%\n",
      "Batch 124, Loss: 1.008219, Accuracy: 63.29%\n",
      "Batch 125, Loss: 1.156435, Accuracy: 63.24%\n",
      "Batch 126, Loss: 1.079257, Accuracy: 63.28%\n",
      "Batch 127, Loss: 1.033579, Accuracy: 63.34%\n",
      "Batch 128, Loss: 1.107151, Accuracy: 63.33%\n",
      "Batch 129, Loss: 1.026975, Accuracy: 63.41%\n",
      "Batch 130, Loss: 1.118542, Accuracy: 63.39%\n",
      "Batch 131, Loss: 1.042004, Accuracy: 63.47%\n",
      "Batch 132, Loss: 1.087799, Accuracy: 63.49%\n",
      "Batch 133, Loss: 1.041879, Accuracy: 63.56%\n",
      "Batch 134, Loss: 1.219407, Accuracy: 63.48%\n",
      "Batch 135, Loss: 1.160266, Accuracy: 63.43%\n",
      "Batch 136, Loss: 0.980755, Accuracy: 63.55%\n",
      "Batch 137, Loss: 1.097898, Accuracy: 63.54%\n",
      "Batch 138, Loss: 1.125518, Accuracy: 63.53%\n",
      "Batch 139, Loss: 1.035866, Accuracy: 63.60%\n",
      "Batch 140, Loss: 1.055280, Accuracy: 63.62%\n",
      "Batch 141, Loss: 1.083528, Accuracy: 63.62%\n",
      "Batch 142, Loss: 1.074338, Accuracy: 63.64%\n",
      "Batch 143, Loss: 1.087762, Accuracy: 63.66%\n",
      "Batch 144, Loss: 1.131558, Accuracy: 63.63%\n",
      "Batch 145, Loss: 1.029382, Accuracy: 63.69%\n",
      "Batch 146, Loss: 1.164327, Accuracy: 63.65%\n",
      "Batch 147, Loss: 1.145164, Accuracy: 63.61%\n",
      "Batch 148, Loss: 1.066222, Accuracy: 63.64%\n",
      "Batch 149, Loss: 1.165151, Accuracy: 63.60%\n",
      "Batch 150, Loss: 1.048776, Accuracy: 63.62%\n",
      "Batch 151, Loss: 1.046647, Accuracy: 63.67%\n",
      "Batch 152, Loss: 1.125101, Accuracy: 63.67%\n",
      "Batch 153, Loss: 1.060015, Accuracy: 63.73%\n",
      "Batch 154, Loss: 1.120846, Accuracy: 63.69%\n",
      "Batch 155, Loss: 1.096471, Accuracy: 63.68%\n",
      "Batch 156, Loss: 1.069538, Accuracy: 63.70%\n",
      "Batch 157, Loss: 1.083249, Accuracy: 63.69%\n",
      "Batch 158, Loss: 1.108920, Accuracy: 63.68%\n",
      "Batch 159, Loss: 1.088412, Accuracy: 63.67%\n",
      "Batch 160, Loss: 1.010380, Accuracy: 63.74%\n",
      "Batch 161, Loss: 1.077521, Accuracy: 63.76%\n",
      "Batch 162, Loss: 1.074031, Accuracy: 63.76%\n",
      "Batch 163, Loss: 1.060154, Accuracy: 63.80%\n",
      "Batch 164, Loss: 0.965921, Accuracy: 63.89%\n",
      "Batch 165, Loss: 1.103496, Accuracy: 63.88%\n",
      "Batch 166, Loss: 1.100845, Accuracy: 63.88%\n",
      "Batch 167, Loss: 1.139382, Accuracy: 63.88%\n",
      "Batch 168, Loss: 1.072261, Accuracy: 63.89%\n",
      "Batch 169, Loss: 1.156972, Accuracy: 63.84%\n",
      "Batch 170, Loss: 1.148040, Accuracy: 63.81%\n",
      "Batch 171, Loss: 1.083964, Accuracy: 63.82%\n",
      "Batch 172, Loss: 1.129744, Accuracy: 63.83%\n",
      "Batch 173, Loss: 0.989795, Accuracy: 63.90%\n",
      "Batch 174, Loss: 1.089572, Accuracy: 63.90%\n",
      "Batch 175, Loss: 1.099078, Accuracy: 63.91%\n",
      "Batch 176, Loss: 1.124502, Accuracy: 63.89%\n",
      "Batch 177, Loss: 1.099040, Accuracy: 63.89%\n",
      "Batch 178, Loss: 1.046057, Accuracy: 63.93%\n",
      "Batch 179, Loss: 1.179455, Accuracy: 63.89%\n",
      "Batch 180, Loss: 1.121297, Accuracy: 63.87%\n",
      "Batch 181, Loss: 1.109113, Accuracy: 63.88%\n",
      "Batch 182, Loss: 1.068510, Accuracy: 63.90%\n",
      "Batch 183, Loss: 1.171103, Accuracy: 63.84%\n",
      "Batch 184, Loss: 1.201107, Accuracy: 63.77%\n",
      "Batch 185, Loss: 1.067451, Accuracy: 63.78%\n",
      "Batch 186, Loss: 1.064211, Accuracy: 63.80%\n",
      "Batch 187, Loss: 1.035150, Accuracy: 63.84%\n",
      "Batch 188, Loss: 1.085938, Accuracy: 63.85%\n",
      "Batch 189, Loss: 1.151914, Accuracy: 63.82%\n",
      "Batch 190, Loss: 1.079643, Accuracy: 63.83%\n",
      "Batch 191, Loss: 1.137840, Accuracy: 63.81%\n",
      "Batch 192, Loss: 1.154346, Accuracy: 63.77%\n",
      "Batch 193, Loss: 1.010392, Accuracy: 63.82%\n",
      "Batch 194, Loss: 1.105613, Accuracy: 63.81%\n",
      "Batch 195, Loss: 1.056114, Accuracy: 63.83%\n",
      "Batch 196, Loss: 1.119046, Accuracy: 63.82%\n",
      "Batch 197, Loss: 1.137218, Accuracy: 63.80%\n",
      "Batch 198, Loss: 1.095648, Accuracy: 63.79%\n",
      "Batch 199, Loss: 1.159490, Accuracy: 63.78%\n",
      "Batch 200, Loss: 1.092957, Accuracy: 63.77%\n",
      "Batch 201, Loss: 1.117279, Accuracy: 63.77%\n",
      "Batch 202, Loss: 1.096488, Accuracy: 63.77%\n",
      "Batch 203, Loss: 1.139056, Accuracy: 63.74%\n",
      "Batch 204, Loss: 1.022563, Accuracy: 63.79%\n",
      "Batch 205, Loss: 1.028235, Accuracy: 63.82%\n",
      "Batch 206, Loss: 1.099482, Accuracy: 63.82%\n",
      "Batch 207, Loss: 1.111806, Accuracy: 63.81%\n",
      "Batch 208, Loss: 1.008847, Accuracy: 63.86%\n",
      "Batch 209, Loss: 1.122027, Accuracy: 63.86%\n",
      "Batch 210, Loss: 1.000560, Accuracy: 63.91%\n",
      "Batch 211, Loss: 1.081398, Accuracy: 63.93%\n",
      "Batch 212, Loss: 1.125027, Accuracy: 63.92%\n",
      "Batch 213, Loss: 1.084950, Accuracy: 63.91%\n",
      "Training - Epoch 16, Loss: 1.100307, Accuracy: 63.91%\n",
      "Validation Batch 1, Loss: 1.233924, Accuracy: 48.44%\n",
      "Validation Batch 2, Loss: 1.284429, Accuracy: 44.53%\n",
      "Validation Batch 3, Loss: 1.354509, Accuracy: 40.10%\n",
      "Validation Batch 4, Loss: 1.209737, Accuracy: 42.19%\n",
      "Validation Batch 5, Loss: 1.318846, Accuracy: 40.62%\n",
      "Validation Batch 6, Loss: 1.320722, Accuracy: 39.58%\n",
      "Validation Batch 7, Loss: 1.270618, Accuracy: 40.40%\n",
      "Validation Batch 8, Loss: 1.266832, Accuracy: 40.82%\n",
      "Validation Batch 9, Loss: 1.332425, Accuracy: 40.45%\n",
      "Validation Batch 10, Loss: 1.306702, Accuracy: 40.00%\n",
      "Validation Batch 11, Loss: 1.269980, Accuracy: 39.91%\n",
      "Validation Batch 12, Loss: 1.224791, Accuracy: 40.76%\n",
      "Validation Batch 13, Loss: 1.316671, Accuracy: 40.75%\n",
      "Validation Batch 14, Loss: 1.209304, Accuracy: 41.29%\n",
      "Validation Batch 15, Loss: 1.266905, Accuracy: 41.56%\n",
      "Validation Batch 16, Loss: 1.309803, Accuracy: 41.41%\n",
      "Validation Batch 17, Loss: 1.333578, Accuracy: 41.18%\n",
      "Validation Batch 18, Loss: 1.260660, Accuracy: 41.41%\n",
      "Validation Batch 19, Loss: 1.310569, Accuracy: 41.28%\n",
      "Validation Batch 20, Loss: 1.274245, Accuracy: 41.41%\n",
      "Validation Batch 21, Loss: 1.291198, Accuracy: 41.29%\n",
      "Validation Batch 22, Loss: 1.300155, Accuracy: 41.26%\n",
      "Validation Batch 23, Loss: 1.325041, Accuracy: 41.10%\n",
      "Validation Batch 24, Loss: 1.302843, Accuracy: 41.08%\n",
      "Validation Batch 25, Loss: 1.244924, Accuracy: 41.25%\n",
      "Validation Batch 26, Loss: 1.198605, Accuracy: 41.65%\n",
      "Validation Batch 27, Loss: 1.317708, Accuracy: 41.57%\n",
      "Validation - Epoch 16, Loss: 1.283545, Accuracy: 41.57%\n",
      "Patienceâ€”0\n",
      "Epoch 17\n",
      "Batch 1, Loss: 1.079747, Accuracy: 60.94%\n",
      "Batch 2, Loss: 1.050966, Accuracy: 64.84%\n",
      "Batch 3, Loss: 1.017273, Accuracy: 67.71%\n",
      "Batch 4, Loss: 1.186318, Accuracy: 64.84%\n",
      "Batch 5, Loss: 1.185757, Accuracy: 62.50%\n",
      "Batch 6, Loss: 1.142314, Accuracy: 61.72%\n",
      "Batch 7, Loss: 1.161025, Accuracy: 60.94%\n",
      "Batch 8, Loss: 1.108380, Accuracy: 61.13%\n",
      "Batch 9, Loss: 1.193437, Accuracy: 60.42%\n",
      "Batch 10, Loss: 1.137178, Accuracy: 60.31%\n",
      "Batch 11, Loss: 1.074388, Accuracy: 60.94%\n",
      "Batch 12, Loss: 1.144846, Accuracy: 60.81%\n",
      "Batch 13, Loss: 1.063437, Accuracy: 61.42%\n",
      "Batch 14, Loss: 1.063039, Accuracy: 61.94%\n",
      "Batch 15, Loss: 1.102184, Accuracy: 61.98%\n",
      "Batch 16, Loss: 1.179168, Accuracy: 61.52%\n",
      "Batch 17, Loss: 1.072296, Accuracy: 61.86%\n",
      "Batch 18, Loss: 1.107460, Accuracy: 61.81%\n",
      "Batch 19, Loss: 1.101461, Accuracy: 61.84%\n",
      "Batch 20, Loss: 1.171512, Accuracy: 61.56%\n",
      "Batch 21, Loss: 1.064296, Accuracy: 61.90%\n",
      "Batch 22, Loss: 1.012737, Accuracy: 62.36%\n",
      "Batch 23, Loss: 1.101470, Accuracy: 62.36%\n",
      "Batch 24, Loss: 1.018613, Accuracy: 62.76%\n",
      "Batch 25, Loss: 1.036308, Accuracy: 62.94%\n",
      "Batch 26, Loss: 1.061625, Accuracy: 63.28%\n",
      "Batch 27, Loss: 1.032981, Accuracy: 63.60%\n",
      "Batch 28, Loss: 1.075178, Accuracy: 63.67%\n",
      "Batch 29, Loss: 1.230915, Accuracy: 63.25%\n",
      "Batch 30, Loss: 1.118274, Accuracy: 63.18%\n",
      "Batch 31, Loss: 1.109873, Accuracy: 63.21%\n",
      "Batch 32, Loss: 1.145349, Accuracy: 63.09%\n",
      "Batch 33, Loss: 1.097441, Accuracy: 63.02%\n",
      "Batch 34, Loss: 1.046914, Accuracy: 63.19%\n",
      "Batch 35, Loss: 1.107452, Accuracy: 63.08%\n",
      "Batch 36, Loss: 1.090392, Accuracy: 63.11%\n",
      "Batch 37, Loss: 1.081804, Accuracy: 63.09%\n",
      "Batch 38, Loss: 1.095661, Accuracy: 63.12%\n",
      "Batch 39, Loss: 0.985759, Accuracy: 63.54%\n",
      "Batch 40, Loss: 1.036098, Accuracy: 63.75%\n",
      "Batch 41, Loss: 1.036855, Accuracy: 63.99%\n",
      "Batch 42, Loss: 1.078365, Accuracy: 64.14%\n",
      "Batch 43, Loss: 1.095599, Accuracy: 64.17%\n",
      "Batch 44, Loss: 1.070374, Accuracy: 64.20%\n",
      "Batch 45, Loss: 1.063803, Accuracy: 64.27%\n",
      "Batch 46, Loss: 1.141213, Accuracy: 64.27%\n",
      "Batch 47, Loss: 1.085291, Accuracy: 64.36%\n",
      "Batch 48, Loss: 1.044133, Accuracy: 64.42%\n",
      "Batch 49, Loss: 1.043055, Accuracy: 64.54%\n",
      "Batch 50, Loss: 1.056881, Accuracy: 64.66%\n",
      "Batch 51, Loss: 1.011040, Accuracy: 64.86%\n",
      "Batch 52, Loss: 1.020535, Accuracy: 65.02%\n",
      "Batch 53, Loss: 1.223954, Accuracy: 64.71%\n",
      "Batch 54, Loss: 1.126524, Accuracy: 64.67%\n",
      "Batch 55, Loss: 1.065884, Accuracy: 64.69%\n",
      "Batch 56, Loss: 1.110644, Accuracy: 64.62%\n",
      "Batch 57, Loss: 1.044602, Accuracy: 64.72%\n",
      "Batch 58, Loss: 1.035232, Accuracy: 64.79%\n",
      "Batch 59, Loss: 1.111471, Accuracy: 64.72%\n",
      "Batch 60, Loss: 1.117931, Accuracy: 64.69%\n",
      "Batch 61, Loss: 1.065307, Accuracy: 64.73%\n",
      "Batch 62, Loss: 1.098501, Accuracy: 64.74%\n",
      "Batch 63, Loss: 1.102587, Accuracy: 64.78%\n",
      "Batch 64, Loss: 1.029714, Accuracy: 64.89%\n",
      "Batch 65, Loss: 1.092820, Accuracy: 64.88%\n",
      "Batch 66, Loss: 1.088061, Accuracy: 64.91%\n",
      "Batch 67, Loss: 1.085454, Accuracy: 64.86%\n",
      "Batch 68, Loss: 1.171233, Accuracy: 64.75%\n",
      "Batch 69, Loss: 1.111145, Accuracy: 64.76%\n",
      "Batch 70, Loss: 1.073106, Accuracy: 64.78%\n",
      "Batch 71, Loss: 1.059231, Accuracy: 64.81%\n",
      "Batch 72, Loss: 1.116655, Accuracy: 64.78%\n",
      "Batch 73, Loss: 1.091277, Accuracy: 64.79%\n",
      "Batch 74, Loss: 1.137623, Accuracy: 64.72%\n",
      "Batch 75, Loss: 1.104907, Accuracy: 64.75%\n",
      "Batch 76, Loss: 1.103336, Accuracy: 64.76%\n",
      "Batch 77, Loss: 1.000203, Accuracy: 64.91%\n",
      "Batch 78, Loss: 1.073365, Accuracy: 64.94%\n",
      "Batch 79, Loss: 1.142195, Accuracy: 64.85%\n",
      "Batch 80, Loss: 1.087444, Accuracy: 64.86%\n",
      "Batch 81, Loss: 1.072949, Accuracy: 64.89%\n",
      "Batch 82, Loss: 1.118386, Accuracy: 64.82%\n",
      "Batch 83, Loss: 1.060404, Accuracy: 64.85%\n",
      "Batch 84, Loss: 1.043831, Accuracy: 64.96%\n",
      "Batch 85, Loss: 1.116519, Accuracy: 64.93%\n",
      "Batch 86, Loss: 1.037932, Accuracy: 64.97%\n",
      "Batch 87, Loss: 1.117884, Accuracy: 64.92%\n",
      "Batch 88, Loss: 1.001569, Accuracy: 65.00%\n",
      "Batch 89, Loss: 1.090844, Accuracy: 65.01%\n",
      "Batch 90, Loss: 1.089724, Accuracy: 65.02%\n",
      "Batch 91, Loss: 1.114272, Accuracy: 64.99%\n",
      "Batch 92, Loss: 1.114330, Accuracy: 64.98%\n",
      "Batch 93, Loss: 1.244126, Accuracy: 64.80%\n",
      "Batch 94, Loss: 1.054945, Accuracy: 64.88%\n",
      "Batch 95, Loss: 1.063270, Accuracy: 64.88%\n",
      "Batch 96, Loss: 1.001134, Accuracy: 64.97%\n",
      "Batch 97, Loss: 1.091300, Accuracy: 64.98%\n",
      "Batch 98, Loss: 1.083322, Accuracy: 64.97%\n",
      "Batch 99, Loss: 1.066825, Accuracy: 65.01%\n",
      "Batch 100, Loss: 1.056478, Accuracy: 65.06%\n",
      "Batch 101, Loss: 1.128572, Accuracy: 65.04%\n",
      "Batch 102, Loss: 1.103389, Accuracy: 65.00%\n",
      "Batch 103, Loss: 0.979739, Accuracy: 65.14%\n",
      "Batch 104, Loss: 1.037075, Accuracy: 65.22%\n",
      "Batch 105, Loss: 1.126579, Accuracy: 65.12%\n",
      "Batch 106, Loss: 1.172188, Accuracy: 65.01%\n",
      "Batch 107, Loss: 1.136281, Accuracy: 64.95%\n",
      "Batch 108, Loss: 1.029870, Accuracy: 65.03%\n",
      "Batch 109, Loss: 1.058429, Accuracy: 65.08%\n",
      "Batch 110, Loss: 1.130545, Accuracy: 65.06%\n",
      "Batch 111, Loss: 1.194381, Accuracy: 64.94%\n",
      "Batch 112, Loss: 1.127596, Accuracy: 64.89%\n",
      "Batch 113, Loss: 1.139687, Accuracy: 64.85%\n",
      "Batch 114, Loss: 1.128634, Accuracy: 64.83%\n",
      "Batch 115, Loss: 1.037573, Accuracy: 64.85%\n",
      "Batch 116, Loss: 1.131747, Accuracy: 64.82%\n",
      "Batch 117, Loss: 1.034454, Accuracy: 64.86%\n",
      "Batch 118, Loss: 1.073615, Accuracy: 64.88%\n",
      "Batch 119, Loss: 1.097949, Accuracy: 64.89%\n",
      "Batch 120, Loss: 1.075112, Accuracy: 64.91%\n",
      "Batch 121, Loss: 1.114244, Accuracy: 64.90%\n",
      "Batch 122, Loss: 1.128996, Accuracy: 64.84%\n",
      "Batch 123, Loss: 1.138565, Accuracy: 64.79%\n",
      "Batch 124, Loss: 1.132540, Accuracy: 64.76%\n",
      "Batch 125, Loss: 1.092840, Accuracy: 64.76%\n",
      "Batch 126, Loss: 1.147543, Accuracy: 64.72%\n",
      "Batch 127, Loss: 1.086825, Accuracy: 64.74%\n",
      "Batch 128, Loss: 1.105923, Accuracy: 64.75%\n",
      "Batch 129, Loss: 0.942286, Accuracy: 64.86%\n",
      "Batch 130, Loss: 1.084176, Accuracy: 64.86%\n",
      "Batch 131, Loss: 1.148177, Accuracy: 64.83%\n",
      "Batch 132, Loss: 1.070601, Accuracy: 64.86%\n",
      "Batch 133, Loss: 1.080799, Accuracy: 64.86%\n",
      "Batch 134, Loss: 1.080218, Accuracy: 64.87%\n",
      "Batch 135, Loss: 1.039879, Accuracy: 64.91%\n",
      "Batch 136, Loss: 1.135418, Accuracy: 64.89%\n",
      "Batch 137, Loss: 1.016533, Accuracy: 64.95%\n",
      "Batch 138, Loss: 1.140467, Accuracy: 64.90%\n",
      "Batch 139, Loss: 1.033251, Accuracy: 64.95%\n",
      "Batch 140, Loss: 1.111190, Accuracy: 64.94%\n",
      "Batch 141, Loss: 1.040009, Accuracy: 64.98%\n",
      "Batch 142, Loss: 1.007827, Accuracy: 65.05%\n",
      "Batch 143, Loss: 1.065793, Accuracy: 65.07%\n",
      "Batch 144, Loss: 1.140246, Accuracy: 65.04%\n",
      "Batch 145, Loss: 1.010299, Accuracy: 65.09%\n",
      "Batch 146, Loss: 1.137245, Accuracy: 65.04%\n",
      "Batch 147, Loss: 1.061176, Accuracy: 65.05%\n",
      "Batch 148, Loss: 1.093729, Accuracy: 65.04%\n",
      "Batch 149, Loss: 0.993716, Accuracy: 65.11%\n",
      "Batch 150, Loss: 1.102471, Accuracy: 65.10%\n",
      "Batch 151, Loss: 1.006944, Accuracy: 65.17%\n",
      "Batch 152, Loss: 1.308023, Accuracy: 65.00%\n",
      "Batch 153, Loss: 1.114370, Accuracy: 64.98%\n",
      "Batch 154, Loss: 1.059885, Accuracy: 65.00%\n",
      "Batch 155, Loss: 1.060336, Accuracy: 65.01%\n",
      "Batch 156, Loss: 1.230778, Accuracy: 64.90%\n",
      "Batch 157, Loss: 1.032290, Accuracy: 64.93%\n",
      "Batch 158, Loss: 1.146571, Accuracy: 64.87%\n",
      "Batch 159, Loss: 1.012877, Accuracy: 64.92%\n",
      "Batch 160, Loss: 1.183525, Accuracy: 64.85%\n",
      "Batch 161, Loss: 1.025832, Accuracy: 64.88%\n",
      "Batch 162, Loss: 1.092869, Accuracy: 64.88%\n",
      "Batch 163, Loss: 1.123678, Accuracy: 64.86%\n",
      "Batch 164, Loss: 1.025383, Accuracy: 64.90%\n",
      "Batch 165, Loss: 0.998770, Accuracy: 64.95%\n",
      "Batch 166, Loss: 1.125877, Accuracy: 64.94%\n",
      "Batch 167, Loss: 1.063854, Accuracy: 64.97%\n",
      "Batch 168, Loss: 1.179053, Accuracy: 64.91%\n",
      "Batch 169, Loss: 1.016675, Accuracy: 64.96%\n",
      "Batch 170, Loss: 1.132007, Accuracy: 64.92%\n",
      "Batch 171, Loss: 1.104508, Accuracy: 64.91%\n",
      "Batch 172, Loss: 1.177693, Accuracy: 64.86%\n",
      "Batch 173, Loss: 1.097138, Accuracy: 64.86%\n",
      "Batch 174, Loss: 1.017480, Accuracy: 64.91%\n",
      "Batch 175, Loss: 1.086061, Accuracy: 64.91%\n",
      "Batch 176, Loss: 1.053910, Accuracy: 64.94%\n",
      "Batch 177, Loss: 1.071415, Accuracy: 64.96%\n",
      "Batch 178, Loss: 1.066180, Accuracy: 64.97%\n",
      "Batch 179, Loss: 1.161278, Accuracy: 64.94%\n",
      "Batch 180, Loss: 1.101259, Accuracy: 64.92%\n",
      "Batch 181, Loss: 1.144706, Accuracy: 64.88%\n",
      "Batch 182, Loss: 1.171929, Accuracy: 64.84%\n",
      "Batch 183, Loss: 1.174953, Accuracy: 64.78%\n",
      "Batch 184, Loss: 1.100615, Accuracy: 64.78%\n",
      "Batch 185, Loss: 1.072806, Accuracy: 64.79%\n",
      "Batch 186, Loss: 1.060625, Accuracy: 64.79%\n",
      "Batch 187, Loss: 1.060183, Accuracy: 64.82%\n",
      "Batch 188, Loss: 1.076849, Accuracy: 64.84%\n",
      "Batch 189, Loss: 1.081071, Accuracy: 64.86%\n",
      "Batch 190, Loss: 1.132700, Accuracy: 64.84%\n",
      "Batch 191, Loss: 1.031746, Accuracy: 64.86%\n",
      "Batch 192, Loss: 1.107071, Accuracy: 64.85%\n",
      "Batch 193, Loss: 1.127030, Accuracy: 64.83%\n",
      "Batch 194, Loss: 1.090177, Accuracy: 64.83%\n",
      "Batch 195, Loss: 1.081795, Accuracy: 64.84%\n",
      "Batch 196, Loss: 1.060890, Accuracy: 64.84%\n",
      "Batch 197, Loss: 1.168169, Accuracy: 64.80%\n",
      "Batch 198, Loss: 1.120677, Accuracy: 64.77%\n",
      "Batch 199, Loss: 1.074215, Accuracy: 64.79%\n",
      "Batch 200, Loss: 1.137989, Accuracy: 64.77%\n",
      "Batch 201, Loss: 1.123306, Accuracy: 64.75%\n",
      "Batch 202, Loss: 1.105943, Accuracy: 64.74%\n",
      "Batch 203, Loss: 1.067118, Accuracy: 64.74%\n",
      "Batch 204, Loss: 1.248875, Accuracy: 64.66%\n",
      "Batch 205, Loss: 1.132538, Accuracy: 64.65%\n",
      "Batch 206, Loss: 1.123316, Accuracy: 64.64%\n",
      "Batch 207, Loss: 1.116119, Accuracy: 64.61%\n",
      "Batch 208, Loss: 1.079252, Accuracy: 64.63%\n",
      "Batch 209, Loss: 1.045961, Accuracy: 64.65%\n",
      "Batch 210, Loss: 1.093884, Accuracy: 64.64%\n",
      "Batch 211, Loss: 1.114621, Accuracy: 64.64%\n",
      "Batch 212, Loss: 1.032292, Accuracy: 64.67%\n",
      "Batch 213, Loss: 1.079990, Accuracy: 64.67%\n",
      "Training - Epoch 17, Loss: 1.092447, Accuracy: 64.67%\n",
      "Validation Batch 1, Loss: 1.217452, Accuracy: 48.44%\n",
      "Validation Batch 2, Loss: 1.276923, Accuracy: 44.53%\n",
      "Validation Batch 3, Loss: 1.344772, Accuracy: 41.15%\n",
      "Validation Batch 4, Loss: 1.195650, Accuracy: 43.75%\n",
      "Validation Batch 5, Loss: 1.309670, Accuracy: 42.50%\n",
      "Validation Batch 6, Loss: 1.302859, Accuracy: 41.41%\n",
      "Validation Batch 7, Loss: 1.260440, Accuracy: 41.96%\n",
      "Validation Batch 8, Loss: 1.259244, Accuracy: 42.58%\n",
      "Validation Batch 9, Loss: 1.319915, Accuracy: 42.19%\n",
      "Validation Batch 10, Loss: 1.292425, Accuracy: 41.72%\n",
      "Validation Batch 11, Loss: 1.258623, Accuracy: 41.62%\n",
      "Validation Batch 12, Loss: 1.213339, Accuracy: 42.58%\n",
      "Validation Batch 13, Loss: 1.305047, Accuracy: 42.43%\n",
      "Validation Batch 14, Loss: 1.199097, Accuracy: 42.86%\n",
      "Validation Batch 15, Loss: 1.257566, Accuracy: 43.02%\n",
      "Validation Batch 16, Loss: 1.294453, Accuracy: 42.77%\n",
      "Validation Batch 17, Loss: 1.326964, Accuracy: 42.46%\n",
      "Validation Batch 18, Loss: 1.247717, Accuracy: 42.80%\n",
      "Validation Batch 19, Loss: 1.304712, Accuracy: 42.68%\n",
      "Validation Batch 20, Loss: 1.263817, Accuracy: 42.73%\n",
      "Validation Batch 21, Loss: 1.276956, Accuracy: 42.71%\n",
      "Validation Batch 22, Loss: 1.290834, Accuracy: 42.61%\n",
      "Validation Batch 23, Loss: 1.318152, Accuracy: 42.46%\n",
      "Validation Batch 24, Loss: 1.287775, Accuracy: 42.45%\n",
      "Validation Batch 25, Loss: 1.240046, Accuracy: 42.62%\n",
      "Validation Batch 26, Loss: 1.191627, Accuracy: 42.97%\n",
      "Validation Batch 27, Loss: 1.299072, Accuracy: 42.87%\n",
      "Validation - Epoch 17, Loss: 1.272413, Accuracy: 42.87%\n",
      "Patienceâ€”0\n",
      "Epoch 18\n",
      "Batch 1, Loss: 1.151704, Accuracy: 57.81%\n",
      "Batch 2, Loss: 1.057824, Accuracy: 64.06%\n",
      "Batch 3, Loss: 1.071664, Accuracy: 64.58%\n",
      "Batch 4, Loss: 1.073509, Accuracy: 64.45%\n",
      "Batch 5, Loss: 1.083962, Accuracy: 64.38%\n",
      "Batch 6, Loss: 1.063777, Accuracy: 64.32%\n",
      "Batch 7, Loss: 1.138070, Accuracy: 63.62%\n",
      "Batch 8, Loss: 1.084991, Accuracy: 64.06%\n",
      "Batch 9, Loss: 1.085700, Accuracy: 64.06%\n",
      "Batch 10, Loss: 1.163302, Accuracy: 63.59%\n",
      "Batch 11, Loss: 1.115767, Accuracy: 63.35%\n",
      "Batch 12, Loss: 0.990930, Accuracy: 64.58%\n",
      "Batch 13, Loss: 1.070642, Accuracy: 64.78%\n",
      "Batch 14, Loss: 1.137898, Accuracy: 64.40%\n",
      "Batch 15, Loss: 1.072910, Accuracy: 64.69%\n",
      "Batch 16, Loss: 1.062993, Accuracy: 64.84%\n",
      "Batch 17, Loss: 1.033624, Accuracy: 65.17%\n",
      "Batch 18, Loss: 1.033638, Accuracy: 65.54%\n",
      "Batch 19, Loss: 1.128421, Accuracy: 65.30%\n",
      "Batch 20, Loss: 1.096296, Accuracy: 65.39%\n",
      "Batch 21, Loss: 1.047246, Accuracy: 65.55%\n",
      "Batch 22, Loss: 1.131532, Accuracy: 65.27%\n",
      "Batch 23, Loss: 1.137550, Accuracy: 64.88%\n",
      "Batch 24, Loss: 1.046594, Accuracy: 65.17%\n",
      "Batch 25, Loss: 1.135843, Accuracy: 64.88%\n",
      "Batch 26, Loss: 1.057861, Accuracy: 65.02%\n",
      "Batch 27, Loss: 1.025683, Accuracy: 65.39%\n",
      "Batch 28, Loss: 1.127413, Accuracy: 65.12%\n",
      "Batch 29, Loss: 1.036795, Accuracy: 65.25%\n",
      "Batch 30, Loss: 1.134149, Accuracy: 65.05%\n",
      "Batch 31, Loss: 1.111621, Accuracy: 64.92%\n",
      "Batch 32, Loss: 1.191227, Accuracy: 64.60%\n",
      "Batch 33, Loss: 1.162875, Accuracy: 64.35%\n",
      "Batch 34, Loss: 1.135691, Accuracy: 64.15%\n",
      "Batch 35, Loss: 1.107061, Accuracy: 64.11%\n",
      "Batch 36, Loss: 1.092314, Accuracy: 64.11%\n",
      "Batch 37, Loss: 1.051587, Accuracy: 64.27%\n",
      "Batch 38, Loss: 0.995829, Accuracy: 64.56%\n",
      "Batch 39, Loss: 1.165849, Accuracy: 64.34%\n",
      "Batch 40, Loss: 1.084689, Accuracy: 64.38%\n",
      "Batch 41, Loss: 1.065361, Accuracy: 64.52%\n",
      "Batch 42, Loss: 1.097504, Accuracy: 64.47%\n",
      "Batch 43, Loss: 1.042798, Accuracy: 64.64%\n",
      "Batch 44, Loss: 1.047465, Accuracy: 64.74%\n",
      "Batch 45, Loss: 1.101979, Accuracy: 64.76%\n",
      "Batch 46, Loss: 1.158721, Accuracy: 64.57%\n",
      "Batch 47, Loss: 1.099207, Accuracy: 64.56%\n",
      "Batch 48, Loss: 1.057024, Accuracy: 64.68%\n",
      "Batch 49, Loss: 1.077525, Accuracy: 64.64%\n",
      "Batch 50, Loss: 1.110022, Accuracy: 64.62%\n",
      "Batch 51, Loss: 1.084822, Accuracy: 64.61%\n",
      "Batch 52, Loss: 1.174674, Accuracy: 64.48%\n",
      "Batch 53, Loss: 1.065823, Accuracy: 64.53%\n",
      "Batch 54, Loss: 1.053889, Accuracy: 64.64%\n",
      "Batch 55, Loss: 1.029131, Accuracy: 64.74%\n",
      "Batch 56, Loss: 1.087497, Accuracy: 64.70%\n",
      "Batch 57, Loss: 1.116612, Accuracy: 64.67%\n",
      "Batch 58, Loss: 1.054219, Accuracy: 64.76%\n",
      "Batch 59, Loss: 1.113979, Accuracy: 64.72%\n",
      "Batch 60, Loss: 1.089469, Accuracy: 64.71%\n",
      "Batch 61, Loss: 1.084319, Accuracy: 64.65%\n",
      "Batch 62, Loss: 0.990255, Accuracy: 64.79%\n",
      "Batch 63, Loss: 1.113918, Accuracy: 64.76%\n",
      "Batch 64, Loss: 1.067675, Accuracy: 64.77%\n",
      "Batch 65, Loss: 1.104505, Accuracy: 64.71%\n",
      "Batch 66, Loss: 1.139939, Accuracy: 64.68%\n",
      "Batch 67, Loss: 1.105502, Accuracy: 64.65%\n",
      "Batch 68, Loss: 1.102366, Accuracy: 64.61%\n",
      "Batch 69, Loss: 1.067126, Accuracy: 64.63%\n",
      "Batch 70, Loss: 1.157890, Accuracy: 64.51%\n",
      "Batch 71, Loss: 1.079878, Accuracy: 64.55%\n",
      "Batch 72, Loss: 1.081239, Accuracy: 64.58%\n",
      "Batch 73, Loss: 1.053774, Accuracy: 64.66%\n",
      "Batch 74, Loss: 1.096758, Accuracy: 64.65%\n",
      "Batch 75, Loss: 1.089161, Accuracy: 64.67%\n",
      "Batch 76, Loss: 0.989776, Accuracy: 64.82%\n",
      "Batch 77, Loss: 1.096394, Accuracy: 64.81%\n",
      "Batch 78, Loss: 1.087540, Accuracy: 64.84%\n",
      "Batch 79, Loss: 1.060372, Accuracy: 64.89%\n",
      "Batch 80, Loss: 0.991829, Accuracy: 65.02%\n",
      "Batch 81, Loss: 1.066074, Accuracy: 65.01%\n",
      "Batch 82, Loss: 1.102767, Accuracy: 64.96%\n",
      "Batch 83, Loss: 1.101201, Accuracy: 64.95%\n",
      "Batch 84, Loss: 1.081627, Accuracy: 64.92%\n",
      "Batch 85, Loss: 1.107724, Accuracy: 64.93%\n",
      "Batch 86, Loss: 1.147996, Accuracy: 64.86%\n",
      "Batch 87, Loss: 1.013041, Accuracy: 64.98%\n",
      "Batch 88, Loss: 1.016332, Accuracy: 65.07%\n",
      "Batch 89, Loss: 1.069427, Accuracy: 65.12%\n",
      "Batch 90, Loss: 1.109013, Accuracy: 65.09%\n",
      "Batch 91, Loss: 1.100671, Accuracy: 65.04%\n",
      "Batch 92, Loss: 1.040327, Accuracy: 65.12%\n",
      "Batch 93, Loss: 1.205866, Accuracy: 64.95%\n",
      "Batch 94, Loss: 0.961708, Accuracy: 65.11%\n",
      "Batch 95, Loss: 1.101188, Accuracy: 65.10%\n",
      "Batch 96, Loss: 1.176819, Accuracy: 64.96%\n",
      "Batch 97, Loss: 1.074798, Accuracy: 64.98%\n",
      "Batch 98, Loss: 1.119691, Accuracy: 64.92%\n",
      "Batch 99, Loss: 1.031233, Accuracy: 65.01%\n",
      "Batch 100, Loss: 1.106648, Accuracy: 64.98%\n",
      "Batch 101, Loss: 1.045845, Accuracy: 65.04%\n",
      "Batch 102, Loss: 1.164912, Accuracy: 64.94%\n",
      "Batch 103, Loss: 1.033277, Accuracy: 64.99%\n",
      "Batch 104, Loss: 1.038041, Accuracy: 65.05%\n",
      "Batch 105, Loss: 1.122374, Accuracy: 65.03%\n",
      "Batch 106, Loss: 1.103283, Accuracy: 64.99%\n",
      "Batch 107, Loss: 1.092023, Accuracy: 64.98%\n",
      "Batch 108, Loss: 1.093354, Accuracy: 65.00%\n",
      "Batch 109, Loss: 1.069142, Accuracy: 65.02%\n",
      "Batch 110, Loss: 1.047286, Accuracy: 65.07%\n",
      "Batch 111, Loss: 1.102219, Accuracy: 65.06%\n",
      "Batch 112, Loss: 1.151308, Accuracy: 64.98%\n",
      "Batch 113, Loss: 1.010591, Accuracy: 65.07%\n",
      "Batch 114, Loss: 1.109349, Accuracy: 65.05%\n",
      "Batch 115, Loss: 1.064356, Accuracy: 65.07%\n",
      "Batch 116, Loss: 1.045110, Accuracy: 65.09%\n",
      "Batch 117, Loss: 1.103691, Accuracy: 65.08%\n",
      "Batch 118, Loss: 1.002528, Accuracy: 65.17%\n",
      "Batch 119, Loss: 1.038809, Accuracy: 65.23%\n",
      "Batch 120, Loss: 1.029647, Accuracy: 65.27%\n",
      "Batch 121, Loss: 1.025381, Accuracy: 65.33%\n",
      "Batch 122, Loss: 1.127767, Accuracy: 65.28%\n",
      "Batch 123, Loss: 1.140273, Accuracy: 65.23%\n",
      "Batch 124, Loss: 0.995493, Accuracy: 65.30%\n",
      "Batch 125, Loss: 1.058284, Accuracy: 65.35%\n",
      "Batch 126, Loss: 1.050556, Accuracy: 65.38%\n",
      "Batch 127, Loss: 1.174874, Accuracy: 65.29%\n",
      "Batch 128, Loss: 1.050747, Accuracy: 65.31%\n",
      "Batch 129, Loss: 1.080538, Accuracy: 65.31%\n",
      "Batch 130, Loss: 1.107573, Accuracy: 65.30%\n",
      "Batch 131, Loss: 1.147035, Accuracy: 65.27%\n",
      "Batch 132, Loss: 1.170787, Accuracy: 65.20%\n",
      "Batch 133, Loss: 1.116073, Accuracy: 65.17%\n",
      "Batch 134, Loss: 0.999326, Accuracy: 65.23%\n",
      "Batch 135, Loss: 0.985708, Accuracy: 65.30%\n",
      "Batch 136, Loss: 1.135560, Accuracy: 65.27%\n",
      "Batch 137, Loss: 1.102583, Accuracy: 65.24%\n",
      "Batch 138, Loss: 1.125407, Accuracy: 65.18%\n",
      "Batch 139, Loss: 1.156372, Accuracy: 65.14%\n",
      "Batch 140, Loss: 1.033695, Accuracy: 65.19%\n",
      "Batch 141, Loss: 1.025428, Accuracy: 65.23%\n",
      "Batch 142, Loss: 1.122226, Accuracy: 65.18%\n",
      "Batch 143, Loss: 1.115198, Accuracy: 65.16%\n",
      "Batch 144, Loss: 1.193167, Accuracy: 65.07%\n",
      "Batch 145, Loss: 1.114845, Accuracy: 65.05%\n",
      "Batch 146, Loss: 1.038355, Accuracy: 65.09%\n",
      "Batch 147, Loss: 1.095967, Accuracy: 65.08%\n",
      "Batch 148, Loss: 1.046233, Accuracy: 65.12%\n",
      "Batch 149, Loss: 1.124487, Accuracy: 65.08%\n",
      "Batch 150, Loss: 1.279060, Accuracy: 64.96%\n",
      "Batch 151, Loss: 1.110945, Accuracy: 64.91%\n",
      "Batch 152, Loss: 1.134929, Accuracy: 64.87%\n",
      "Batch 153, Loss: 1.040006, Accuracy: 64.91%\n",
      "Batch 154, Loss: 1.151320, Accuracy: 64.87%\n",
      "Batch 155, Loss: 1.031245, Accuracy: 64.89%\n",
      "Batch 156, Loss: 1.092677, Accuracy: 64.88%\n",
      "Batch 157, Loss: 1.136113, Accuracy: 64.86%\n",
      "Batch 158, Loss: 1.158735, Accuracy: 64.81%\n",
      "Batch 159, Loss: 1.064746, Accuracy: 64.82%\n",
      "Batch 160, Loss: 1.142178, Accuracy: 64.77%\n",
      "Batch 161, Loss: 1.139433, Accuracy: 64.74%\n",
      "Batch 162, Loss: 1.098453, Accuracy: 64.74%\n",
      "Batch 163, Loss: 1.041988, Accuracy: 64.77%\n",
      "Batch 164, Loss: 1.113032, Accuracy: 64.76%\n",
      "Batch 165, Loss: 1.140024, Accuracy: 64.72%\n",
      "Batch 166, Loss: 1.059039, Accuracy: 64.72%\n",
      "Batch 167, Loss: 1.151075, Accuracy: 64.67%\n",
      "Batch 168, Loss: 1.080005, Accuracy: 64.68%\n",
      "Batch 169, Loss: 1.091162, Accuracy: 64.66%\n",
      "Batch 170, Loss: 1.010701, Accuracy: 64.70%\n",
      "Batch 171, Loss: 1.181859, Accuracy: 64.64%\n",
      "Batch 172, Loss: 1.081191, Accuracy: 64.64%\n",
      "Batch 173, Loss: 1.081262, Accuracy: 64.65%\n",
      "Batch 174, Loss: 1.138659, Accuracy: 64.61%\n",
      "Batch 175, Loss: 1.148172, Accuracy: 64.59%\n",
      "Batch 176, Loss: 1.066503, Accuracy: 64.61%\n",
      "Batch 177, Loss: 1.111634, Accuracy: 64.60%\n",
      "Batch 178, Loss: 1.141857, Accuracy: 64.58%\n",
      "Batch 179, Loss: 1.075851, Accuracy: 64.59%\n",
      "Batch 180, Loss: 1.188643, Accuracy: 64.53%\n",
      "Batch 181, Loss: 1.243232, Accuracy: 64.43%\n",
      "Batch 182, Loss: 1.140916, Accuracy: 64.38%\n",
      "Batch 183, Loss: 1.148469, Accuracy: 64.34%\n",
      "Batch 184, Loss: 0.959352, Accuracy: 64.42%\n",
      "Batch 185, Loss: 1.050642, Accuracy: 64.44%\n",
      "Batch 186, Loss: 1.143093, Accuracy: 64.41%\n",
      "Batch 187, Loss: 1.124712, Accuracy: 64.41%\n",
      "Batch 188, Loss: 1.129566, Accuracy: 64.39%\n",
      "Batch 189, Loss: 1.063714, Accuracy: 64.41%\n",
      "Batch 190, Loss: 1.090260, Accuracy: 64.42%\n",
      "Batch 191, Loss: 1.110550, Accuracy: 64.38%\n",
      "Batch 192, Loss: 0.996991, Accuracy: 64.44%\n",
      "Batch 193, Loss: 1.076200, Accuracy: 64.45%\n",
      "Batch 194, Loss: 1.117082, Accuracy: 64.42%\n",
      "Batch 195, Loss: 1.085826, Accuracy: 64.42%\n",
      "Batch 196, Loss: 1.032460, Accuracy: 64.45%\n",
      "Batch 197, Loss: 1.064137, Accuracy: 64.47%\n",
      "Batch 198, Loss: 1.080580, Accuracy: 64.48%\n",
      "Batch 199, Loss: 1.035678, Accuracy: 64.51%\n",
      "Batch 200, Loss: 1.229233, Accuracy: 64.41%\n",
      "Batch 201, Loss: 1.020069, Accuracy: 64.48%\n",
      "Batch 202, Loss: 1.146824, Accuracy: 64.46%\n",
      "Batch 203, Loss: 1.091039, Accuracy: 64.46%\n",
      "Batch 204, Loss: 1.078662, Accuracy: 64.48%\n",
      "Batch 205, Loss: 1.078654, Accuracy: 64.50%\n",
      "Batch 206, Loss: 1.099778, Accuracy: 64.49%\n",
      "Batch 207, Loss: 1.025632, Accuracy: 64.54%\n",
      "Batch 208, Loss: 1.045909, Accuracy: 64.57%\n",
      "Batch 209, Loss: 1.137504, Accuracy: 64.54%\n",
      "Batch 210, Loss: 1.113807, Accuracy: 64.54%\n",
      "Batch 211, Loss: 1.054933, Accuracy: 64.57%\n",
      "Batch 212, Loss: 1.101381, Accuracy: 64.56%\n",
      "Batch 213, Loss: 1.079599, Accuracy: 64.56%\n",
      "Training - Epoch 18, Loss: 1.090729, Accuracy: 64.56%\n",
      "Validation Batch 1, Loss: 1.228988, Accuracy: 46.88%\n",
      "Validation Batch 2, Loss: 1.284094, Accuracy: 44.53%\n",
      "Validation Batch 3, Loss: 1.354417, Accuracy: 40.10%\n",
      "Validation Batch 4, Loss: 1.208440, Accuracy: 42.58%\n",
      "Validation Batch 5, Loss: 1.322519, Accuracy: 41.56%\n",
      "Validation Batch 6, Loss: 1.315275, Accuracy: 40.62%\n",
      "Validation Batch 7, Loss: 1.269384, Accuracy: 41.07%\n",
      "Validation Batch 8, Loss: 1.267608, Accuracy: 41.41%\n",
      "Validation Batch 9, Loss: 1.328837, Accuracy: 41.15%\n",
      "Validation Batch 10, Loss: 1.309222, Accuracy: 40.62%\n",
      "Validation Batch 11, Loss: 1.268504, Accuracy: 40.62%\n",
      "Validation Batch 12, Loss: 1.221008, Accuracy: 41.54%\n",
      "Validation Batch 13, Loss: 1.314088, Accuracy: 41.47%\n",
      "Validation Batch 14, Loss: 1.207736, Accuracy: 41.96%\n",
      "Validation Batch 15, Loss: 1.266238, Accuracy: 42.08%\n",
      "Validation Batch 16, Loss: 1.307420, Accuracy: 41.89%\n",
      "Validation Batch 17, Loss: 1.336262, Accuracy: 41.64%\n",
      "Validation Batch 18, Loss: 1.257757, Accuracy: 42.01%\n",
      "Validation Batch 19, Loss: 1.314228, Accuracy: 41.86%\n",
      "Validation Batch 20, Loss: 1.272293, Accuracy: 41.95%\n",
      "Validation Batch 21, Loss: 1.289826, Accuracy: 41.82%\n",
      "Validation Batch 22, Loss: 1.300346, Accuracy: 41.76%\n",
      "Validation Batch 23, Loss: 1.326492, Accuracy: 41.58%\n",
      "Validation Batch 24, Loss: 1.299764, Accuracy: 41.54%\n",
      "Validation Batch 25, Loss: 1.250477, Accuracy: 41.69%\n",
      "Validation Batch 26, Loss: 1.199282, Accuracy: 42.07%\n",
      "Validation Batch 27, Loss: 1.311258, Accuracy: 41.98%\n",
      "Validation - Epoch 18, Loss: 1.282658, Accuracy: 41.98%\n",
      "Patienceâ€”1\n",
      "Epoch 19\n",
      "Batch 1, Loss: 1.077719, Accuracy: 65.62%\n",
      "Batch 2, Loss: 1.119497, Accuracy: 64.06%\n",
      "Batch 3, Loss: 1.069942, Accuracy: 64.58%\n",
      "Batch 4, Loss: 1.120962, Accuracy: 64.06%\n",
      "Batch 5, Loss: 1.040841, Accuracy: 64.69%\n",
      "Batch 6, Loss: 1.074786, Accuracy: 64.84%\n",
      "Batch 7, Loss: 1.095185, Accuracy: 64.73%\n",
      "Batch 8, Loss: 1.162923, Accuracy: 63.48%\n",
      "Batch 9, Loss: 1.081409, Accuracy: 63.37%\n",
      "Batch 10, Loss: 1.188792, Accuracy: 62.34%\n",
      "Batch 11, Loss: 1.145701, Accuracy: 62.07%\n",
      "Batch 12, Loss: 1.132174, Accuracy: 61.98%\n",
      "Batch 13, Loss: 1.111269, Accuracy: 62.02%\n",
      "Batch 14, Loss: 1.089602, Accuracy: 62.05%\n",
      "Batch 15, Loss: 1.050119, Accuracy: 62.50%\n",
      "Batch 16, Loss: 1.106122, Accuracy: 62.50%\n",
      "Batch 17, Loss: 1.102957, Accuracy: 62.50%\n",
      "Batch 18, Loss: 1.144847, Accuracy: 62.15%\n",
      "Batch 19, Loss: 1.076845, Accuracy: 62.42%\n",
      "Batch 20, Loss: 1.019464, Accuracy: 62.97%\n",
      "Batch 21, Loss: 1.035602, Accuracy: 63.32%\n",
      "Batch 22, Loss: 1.019892, Accuracy: 63.64%\n",
      "Batch 23, Loss: 1.117709, Accuracy: 63.52%\n",
      "Batch 24, Loss: 1.093697, Accuracy: 63.61%\n",
      "Batch 25, Loss: 1.133704, Accuracy: 63.56%\n",
      "Batch 26, Loss: 1.124650, Accuracy: 63.40%\n",
      "Batch 27, Loss: 1.064231, Accuracy: 63.54%\n",
      "Batch 28, Loss: 1.066298, Accuracy: 63.73%\n",
      "Batch 29, Loss: 1.093269, Accuracy: 63.69%\n",
      "Batch 30, Loss: 1.105550, Accuracy: 63.59%\n",
      "Batch 31, Loss: 1.136813, Accuracy: 63.51%\n",
      "Batch 32, Loss: 1.082147, Accuracy: 63.48%\n",
      "Batch 33, Loss: 1.090555, Accuracy: 63.45%\n",
      "Batch 34, Loss: 1.069225, Accuracy: 63.56%\n",
      "Batch 35, Loss: 1.014649, Accuracy: 63.84%\n",
      "Batch 36, Loss: 0.982249, Accuracy: 64.19%\n",
      "Batch 37, Loss: 1.254836, Accuracy: 63.64%\n",
      "Batch 38, Loss: 1.048048, Accuracy: 63.77%\n",
      "Batch 39, Loss: 1.048846, Accuracy: 63.82%\n",
      "Batch 40, Loss: 1.033308, Accuracy: 64.02%\n",
      "Batch 41, Loss: 1.092941, Accuracy: 64.06%\n",
      "Batch 42, Loss: 1.041945, Accuracy: 64.17%\n",
      "Batch 43, Loss: 1.060557, Accuracy: 64.24%\n",
      "Batch 44, Loss: 1.116231, Accuracy: 64.28%\n",
      "Batch 45, Loss: 0.964427, Accuracy: 64.65%\n",
      "Batch 46, Loss: 0.957672, Accuracy: 64.95%\n",
      "Batch 47, Loss: 1.013027, Accuracy: 65.13%\n",
      "Batch 48, Loss: 1.070560, Accuracy: 65.07%\n",
      "Batch 49, Loss: 1.059571, Accuracy: 65.15%\n",
      "Batch 50, Loss: 1.098511, Accuracy: 65.16%\n",
      "Batch 51, Loss: 1.150580, Accuracy: 64.98%\n",
      "Batch 52, Loss: 1.124216, Accuracy: 64.90%\n",
      "Batch 53, Loss: 1.083396, Accuracy: 64.86%\n",
      "Batch 54, Loss: 1.048575, Accuracy: 64.90%\n",
      "Batch 55, Loss: 1.116100, Accuracy: 64.83%\n",
      "Batch 56, Loss: 1.153874, Accuracy: 64.68%\n",
      "Batch 57, Loss: 1.106698, Accuracy: 64.67%\n",
      "Batch 58, Loss: 1.136751, Accuracy: 64.60%\n",
      "Batch 59, Loss: 0.980236, Accuracy: 64.80%\n",
      "Batch 60, Loss: 1.114682, Accuracy: 64.69%\n",
      "Batch 61, Loss: 1.126403, Accuracy: 64.60%\n",
      "Batch 62, Loss: 1.066225, Accuracy: 64.64%\n",
      "Batch 63, Loss: 0.973747, Accuracy: 64.81%\n",
      "Batch 64, Loss: 1.139652, Accuracy: 64.72%\n",
      "Batch 65, Loss: 1.111348, Accuracy: 64.71%\n",
      "Batch 66, Loss: 1.136873, Accuracy: 64.61%\n",
      "Batch 67, Loss: 1.100308, Accuracy: 64.62%\n",
      "Batch 68, Loss: 1.114344, Accuracy: 64.61%\n",
      "Batch 69, Loss: 1.089553, Accuracy: 64.58%\n",
      "Batch 70, Loss: 1.112848, Accuracy: 64.55%\n",
      "Batch 71, Loss: 1.061252, Accuracy: 64.61%\n",
      "Batch 72, Loss: 1.110651, Accuracy: 64.61%\n",
      "Batch 73, Loss: 0.992564, Accuracy: 64.75%\n",
      "Batch 74, Loss: 1.114069, Accuracy: 64.72%\n",
      "Batch 75, Loss: 1.069368, Accuracy: 64.79%\n",
      "Batch 76, Loss: 1.114020, Accuracy: 64.76%\n",
      "Batch 77, Loss: 1.107297, Accuracy: 64.71%\n",
      "Batch 78, Loss: 1.097605, Accuracy: 64.68%\n",
      "Batch 79, Loss: 1.043851, Accuracy: 64.77%\n",
      "Batch 80, Loss: 1.098378, Accuracy: 64.77%\n",
      "Batch 81, Loss: 1.036025, Accuracy: 64.87%\n",
      "Batch 82, Loss: 1.019221, Accuracy: 64.98%\n",
      "Batch 83, Loss: 1.173207, Accuracy: 64.87%\n",
      "Batch 84, Loss: 1.029355, Accuracy: 64.92%\n",
      "Batch 85, Loss: 1.132733, Accuracy: 64.87%\n",
      "Batch 86, Loss: 0.971983, Accuracy: 65.03%\n",
      "Batch 87, Loss: 1.093853, Accuracy: 65.03%\n",
      "Batch 88, Loss: 1.086484, Accuracy: 65.04%\n",
      "Batch 89, Loss: 1.075179, Accuracy: 65.05%\n",
      "Batch 90, Loss: 1.020916, Accuracy: 65.16%\n",
      "Batch 91, Loss: 1.100732, Accuracy: 65.13%\n",
      "Batch 92, Loss: 1.074341, Accuracy: 65.17%\n",
      "Batch 93, Loss: 1.106255, Accuracy: 65.15%\n",
      "Batch 94, Loss: 1.093656, Accuracy: 65.14%\n",
      "Batch 95, Loss: 1.084879, Accuracy: 65.10%\n",
      "Batch 96, Loss: 1.084652, Accuracy: 65.12%\n",
      "Batch 97, Loss: 1.130552, Accuracy: 65.01%\n",
      "Batch 98, Loss: 1.113857, Accuracy: 64.99%\n",
      "Batch 99, Loss: 1.135676, Accuracy: 64.93%\n",
      "Batch 100, Loss: 1.095383, Accuracy: 64.92%\n",
      "Batch 101, Loss: 1.140491, Accuracy: 64.84%\n",
      "Batch 102, Loss: 1.095739, Accuracy: 64.83%\n",
      "Batch 103, Loss: 1.076452, Accuracy: 64.82%\n",
      "Batch 104, Loss: 1.122174, Accuracy: 64.80%\n",
      "Batch 105, Loss: 1.034743, Accuracy: 64.85%\n",
      "Batch 106, Loss: 1.130164, Accuracy: 64.80%\n",
      "Batch 107, Loss: 1.072365, Accuracy: 64.79%\n",
      "Batch 108, Loss: 1.011221, Accuracy: 64.87%\n",
      "Batch 109, Loss: 1.102305, Accuracy: 64.88%\n",
      "Batch 110, Loss: 1.152501, Accuracy: 64.80%\n",
      "Batch 111, Loss: 1.021387, Accuracy: 64.89%\n",
      "Batch 112, Loss: 1.083167, Accuracy: 64.89%\n",
      "Batch 113, Loss: 1.074641, Accuracy: 64.89%\n",
      "Batch 114, Loss: 1.031117, Accuracy: 64.95%\n",
      "Batch 115, Loss: 1.064736, Accuracy: 64.97%\n",
      "Batch 116, Loss: 1.114967, Accuracy: 64.91%\n",
      "Batch 117, Loss: 1.106183, Accuracy: 64.88%\n",
      "Batch 118, Loss: 1.203121, Accuracy: 64.79%\n",
      "Batch 119, Loss: 1.162502, Accuracy: 64.73%\n",
      "Batch 120, Loss: 1.092795, Accuracy: 64.75%\n",
      "Batch 121, Loss: 1.098082, Accuracy: 64.76%\n",
      "Batch 122, Loss: 1.124391, Accuracy: 64.72%\n",
      "Batch 123, Loss: 1.052370, Accuracy: 64.76%\n",
      "Batch 124, Loss: 1.100823, Accuracy: 64.74%\n",
      "Batch 125, Loss: 1.106925, Accuracy: 64.69%\n",
      "Batch 126, Loss: 1.064113, Accuracy: 64.71%\n",
      "Batch 127, Loss: 1.087177, Accuracy: 64.71%\n",
      "Batch 128, Loss: 1.056365, Accuracy: 64.76%\n",
      "Batch 129, Loss: 1.035716, Accuracy: 64.80%\n",
      "Batch 130, Loss: 1.079106, Accuracy: 64.81%\n",
      "Batch 131, Loss: 1.123445, Accuracy: 64.78%\n",
      "Batch 132, Loss: 1.114588, Accuracy: 64.76%\n",
      "Batch 133, Loss: 1.139557, Accuracy: 64.72%\n",
      "Batch 134, Loss: 1.003405, Accuracy: 64.80%\n",
      "Batch 135, Loss: 1.155397, Accuracy: 64.73%\n",
      "Batch 136, Loss: 1.096505, Accuracy: 64.73%\n",
      "Batch 137, Loss: 1.165854, Accuracy: 64.64%\n",
      "Batch 138, Loss: 1.118423, Accuracy: 64.64%\n",
      "Batch 139, Loss: 1.148392, Accuracy: 64.60%\n",
      "Batch 140, Loss: 1.105579, Accuracy: 64.59%\n",
      "Batch 141, Loss: 1.082998, Accuracy: 64.58%\n",
      "Batch 142, Loss: 1.057705, Accuracy: 64.61%\n",
      "Batch 143, Loss: 1.018660, Accuracy: 64.69%\n",
      "Batch 144, Loss: 1.122850, Accuracy: 64.68%\n",
      "Batch 145, Loss: 1.111539, Accuracy: 64.68%\n",
      "Batch 146, Loss: 1.035693, Accuracy: 64.72%\n",
      "Batch 147, Loss: 1.125389, Accuracy: 64.68%\n",
      "Batch 148, Loss: 1.181626, Accuracy: 64.64%\n",
      "Batch 149, Loss: 1.097955, Accuracy: 64.64%\n",
      "Batch 150, Loss: 1.016195, Accuracy: 64.69%\n",
      "Batch 151, Loss: 1.081461, Accuracy: 64.67%\n",
      "Batch 152, Loss: 1.040976, Accuracy: 64.70%\n",
      "Batch 153, Loss: 1.085469, Accuracy: 64.72%\n",
      "Batch 154, Loss: 1.115286, Accuracy: 64.69%\n",
      "Batch 155, Loss: 1.122911, Accuracy: 64.67%\n",
      "Batch 156, Loss: 1.092607, Accuracy: 64.67%\n",
      "Batch 157, Loss: 1.093886, Accuracy: 64.67%\n",
      "Batch 158, Loss: 1.146280, Accuracy: 64.61%\n",
      "Batch 159, Loss: 1.091139, Accuracy: 64.61%\n",
      "Batch 160, Loss: 1.043435, Accuracy: 64.64%\n",
      "Batch 161, Loss: 1.037684, Accuracy: 64.68%\n",
      "Batch 162, Loss: 1.193009, Accuracy: 64.63%\n",
      "Batch 163, Loss: 1.121454, Accuracy: 64.62%\n",
      "Batch 164, Loss: 1.094065, Accuracy: 64.59%\n",
      "Batch 165, Loss: 1.078579, Accuracy: 64.59%\n",
      "Batch 166, Loss: 1.163690, Accuracy: 64.55%\n",
      "Batch 167, Loss: 1.140341, Accuracy: 64.53%\n",
      "Batch 168, Loss: 1.084000, Accuracy: 64.56%\n",
      "Batch 169, Loss: 1.049812, Accuracy: 64.58%\n",
      "Batch 170, Loss: 1.077249, Accuracy: 64.60%\n",
      "Batch 171, Loss: 1.066836, Accuracy: 64.60%\n",
      "Batch 172, Loss: 1.059701, Accuracy: 64.63%\n",
      "Batch 173, Loss: 1.100447, Accuracy: 64.65%\n",
      "Batch 174, Loss: 1.108431, Accuracy: 64.67%\n",
      "Batch 175, Loss: 1.115063, Accuracy: 64.68%\n",
      "Batch 176, Loss: 1.026840, Accuracy: 64.72%\n",
      "Batch 177, Loss: 1.057190, Accuracy: 64.74%\n",
      "Batch 178, Loss: 1.070987, Accuracy: 64.77%\n",
      "Batch 179, Loss: 1.023243, Accuracy: 64.81%\n",
      "Batch 180, Loss: 1.070126, Accuracy: 64.82%\n",
      "Batch 181, Loss: 1.012885, Accuracy: 64.87%\n",
      "Batch 182, Loss: 1.054299, Accuracy: 64.87%\n",
      "Batch 183, Loss: 1.077713, Accuracy: 64.87%\n",
      "Batch 184, Loss: 1.092344, Accuracy: 64.86%\n",
      "Batch 185, Loss: 1.115113, Accuracy: 64.86%\n",
      "Batch 186, Loss: 1.112932, Accuracy: 64.84%\n",
      "Batch 187, Loss: 1.064554, Accuracy: 64.86%\n",
      "Batch 188, Loss: 1.096754, Accuracy: 64.87%\n",
      "Batch 189, Loss: 1.067412, Accuracy: 64.89%\n",
      "Batch 190, Loss: 1.069007, Accuracy: 64.92%\n",
      "Batch 191, Loss: 1.128668, Accuracy: 64.89%\n",
      "Batch 192, Loss: 1.125542, Accuracy: 64.87%\n",
      "Batch 193, Loss: 1.089420, Accuracy: 64.86%\n",
      "Batch 194, Loss: 1.098576, Accuracy: 64.85%\n",
      "Batch 195, Loss: 1.111584, Accuracy: 64.84%\n",
      "Batch 196, Loss: 1.172407, Accuracy: 64.80%\n",
      "Batch 197, Loss: 0.996597, Accuracy: 64.83%\n",
      "Batch 198, Loss: 1.052412, Accuracy: 64.85%\n",
      "Batch 199, Loss: 1.013498, Accuracy: 64.90%\n",
      "Batch 200, Loss: 1.195212, Accuracy: 64.84%\n",
      "Batch 201, Loss: 1.026733, Accuracy: 64.88%\n",
      "Batch 202, Loss: 1.178492, Accuracy: 64.84%\n",
      "Batch 203, Loss: 1.158178, Accuracy: 64.82%\n",
      "Batch 204, Loss: 1.111005, Accuracy: 64.81%\n",
      "Batch 205, Loss: 1.145972, Accuracy: 64.77%\n",
      "Batch 206, Loss: 1.096312, Accuracy: 64.77%\n",
      "Batch 207, Loss: 1.165777, Accuracy: 64.73%\n",
      "Batch 208, Loss: 1.079592, Accuracy: 64.73%\n",
      "Batch 209, Loss: 1.027020, Accuracy: 64.77%\n",
      "Batch 210, Loss: 1.073156, Accuracy: 64.77%\n",
      "Batch 211, Loss: 1.112481, Accuracy: 64.77%\n",
      "Batch 212, Loss: 1.072322, Accuracy: 64.77%\n",
      "Batch 213, Loss: 1.039752, Accuracy: 64.80%\n",
      "Training - Epoch 19, Loss: 1.089022, Accuracy: 64.80%\n",
      "Validation Batch 1, Loss: 1.193676, Accuracy: 54.69%\n",
      "Validation Batch 2, Loss: 1.266760, Accuracy: 49.22%\n",
      "Validation Batch 3, Loss: 1.329572, Accuracy: 45.31%\n",
      "Validation Batch 4, Loss: 1.175459, Accuracy: 46.88%\n",
      "Validation Batch 5, Loss: 1.287242, Accuracy: 45.62%\n",
      "Validation Batch 6, Loss: 1.276030, Accuracy: 44.79%\n",
      "Validation Batch 7, Loss: 1.239225, Accuracy: 45.31%\n",
      "Validation Batch 8, Loss: 1.240374, Accuracy: 45.70%\n",
      "Validation Batch 9, Loss: 1.305891, Accuracy: 44.97%\n",
      "Validation Batch 10, Loss: 1.272897, Accuracy: 44.53%\n",
      "Validation Batch 11, Loss: 1.237121, Accuracy: 44.74%\n",
      "Validation Batch 12, Loss: 1.194203, Accuracy: 45.57%\n",
      "Validation Batch 13, Loss: 1.294477, Accuracy: 45.43%\n",
      "Validation Batch 14, Loss: 1.188738, Accuracy: 45.98%\n",
      "Validation Batch 15, Loss: 1.247284, Accuracy: 46.15%\n",
      "Validation Batch 16, Loss: 1.269310, Accuracy: 46.00%\n",
      "Validation Batch 17, Loss: 1.317701, Accuracy: 45.50%\n",
      "Validation Batch 18, Loss: 1.230055, Accuracy: 45.66%\n",
      "Validation Batch 19, Loss: 1.290489, Accuracy: 45.56%\n",
      "Validation Batch 20, Loss: 1.241800, Accuracy: 45.55%\n",
      "Validation Batch 21, Loss: 1.266246, Accuracy: 45.46%\n",
      "Validation Batch 22, Loss: 1.276697, Accuracy: 45.38%\n",
      "Validation Batch 23, Loss: 1.310067, Accuracy: 45.18%\n",
      "Validation Batch 24, Loss: 1.263431, Accuracy: 45.18%\n",
      "Validation Batch 25, Loss: 1.227583, Accuracy: 45.25%\n",
      "Validation Batch 26, Loss: 1.179740, Accuracy: 45.55%\n",
      "Validation Batch 27, Loss: 1.292754, Accuracy: 45.39%\n",
      "Validation - Epoch 19, Loss: 1.256104, Accuracy: 45.39%\n",
      "Patienceâ€”0\n",
      "Epoch 20\n",
      "Batch 1, Loss: 1.104816, Accuracy: 62.50%\n",
      "Batch 2, Loss: 1.107690, Accuracy: 63.28%\n",
      "Batch 3, Loss: 1.018001, Accuracy: 66.67%\n",
      "Batch 4, Loss: 1.021495, Accuracy: 67.58%\n",
      "Batch 5, Loss: 1.083283, Accuracy: 67.19%\n",
      "Batch 6, Loss: 1.105988, Accuracy: 66.67%\n",
      "Batch 7, Loss: 0.958502, Accuracy: 68.30%\n",
      "Batch 8, Loss: 1.090574, Accuracy: 68.36%\n",
      "Batch 9, Loss: 1.092347, Accuracy: 67.88%\n",
      "Batch 10, Loss: 0.993350, Accuracy: 68.59%\n",
      "Batch 11, Loss: 1.093390, Accuracy: 68.32%\n",
      "Batch 12, Loss: 1.061556, Accuracy: 68.62%\n",
      "Batch 13, Loss: 1.193232, Accuracy: 67.43%\n",
      "Batch 14, Loss: 1.025138, Accuracy: 67.75%\n",
      "Batch 15, Loss: 1.060168, Accuracy: 67.92%\n",
      "Batch 16, Loss: 1.113105, Accuracy: 67.58%\n",
      "Batch 17, Loss: 1.042046, Accuracy: 67.92%\n",
      "Batch 18, Loss: 1.147335, Accuracy: 67.27%\n",
      "Batch 19, Loss: 1.041462, Accuracy: 67.52%\n",
      "Batch 20, Loss: 1.091722, Accuracy: 67.42%\n",
      "Batch 21, Loss: 1.058133, Accuracy: 67.41%\n",
      "Batch 22, Loss: 1.106826, Accuracy: 67.26%\n",
      "Batch 23, Loss: 1.230636, Accuracy: 66.44%\n",
      "Batch 24, Loss: 1.183519, Accuracy: 65.95%\n",
      "Batch 25, Loss: 1.014208, Accuracy: 66.25%\n",
      "Batch 26, Loss: 1.055907, Accuracy: 66.41%\n",
      "Batch 27, Loss: 1.103647, Accuracy: 66.26%\n",
      "Batch 28, Loss: 1.185648, Accuracy: 65.90%\n",
      "Batch 29, Loss: 1.018346, Accuracy: 66.16%\n",
      "Batch 30, Loss: 1.115741, Accuracy: 66.04%\n",
      "Batch 31, Loss: 1.062958, Accuracy: 66.08%\n",
      "Batch 32, Loss: 1.089379, Accuracy: 66.06%\n",
      "Batch 33, Loss: 1.128206, Accuracy: 65.86%\n",
      "Batch 34, Loss: 1.061218, Accuracy: 65.95%\n",
      "Batch 35, Loss: 1.094272, Accuracy: 65.85%\n",
      "Batch 36, Loss: 1.079614, Accuracy: 65.80%\n",
      "Batch 37, Loss: 1.115506, Accuracy: 65.71%\n",
      "Batch 38, Loss: 1.025426, Accuracy: 65.87%\n",
      "Batch 39, Loss: 1.069088, Accuracy: 65.99%\n",
      "Batch 40, Loss: 1.029873, Accuracy: 66.02%\n",
      "Batch 41, Loss: 0.999400, Accuracy: 66.27%\n",
      "Batch 42, Loss: 1.028982, Accuracy: 66.44%\n",
      "Batch 43, Loss: 1.113947, Accuracy: 66.32%\n",
      "Batch 44, Loss: 1.040029, Accuracy: 66.41%\n",
      "Batch 45, Loss: 1.096264, Accuracy: 66.35%\n",
      "Batch 46, Loss: 1.110801, Accuracy: 66.27%\n",
      "Batch 47, Loss: 1.084238, Accuracy: 66.26%\n",
      "Batch 48, Loss: 1.060748, Accuracy: 66.24%\n",
      "Batch 49, Loss: 1.058741, Accuracy: 66.33%\n",
      "Batch 50, Loss: 1.106000, Accuracy: 66.22%\n",
      "Batch 51, Loss: 1.078702, Accuracy: 66.21%\n",
      "Batch 52, Loss: 1.120430, Accuracy: 66.14%\n",
      "Batch 53, Loss: 1.025069, Accuracy: 66.24%\n",
      "Batch 54, Loss: 1.001272, Accuracy: 66.46%\n",
      "Batch 55, Loss: 1.038406, Accuracy: 66.59%\n",
      "Batch 56, Loss: 1.062983, Accuracy: 66.63%\n",
      "Batch 57, Loss: 1.103462, Accuracy: 66.64%\n",
      "Batch 58, Loss: 1.128680, Accuracy: 66.51%\n",
      "Batch 59, Loss: 1.076390, Accuracy: 66.55%\n",
      "Batch 60, Loss: 1.046888, Accuracy: 66.56%\n",
      "Batch 61, Loss: 1.091414, Accuracy: 66.52%\n",
      "Batch 62, Loss: 1.083603, Accuracy: 66.51%\n",
      "Batch 63, Loss: 1.045528, Accuracy: 66.54%\n",
      "Batch 64, Loss: 1.114185, Accuracy: 66.46%\n",
      "Batch 65, Loss: 1.007171, Accuracy: 66.59%\n",
      "Batch 66, Loss: 1.085702, Accuracy: 66.57%\n",
      "Batch 67, Loss: 0.972762, Accuracy: 66.74%\n",
      "Batch 68, Loss: 1.070255, Accuracy: 66.75%\n",
      "Batch 69, Loss: 1.094603, Accuracy: 66.71%\n",
      "Batch 70, Loss: 1.068840, Accuracy: 66.72%\n",
      "Batch 71, Loss: 0.994343, Accuracy: 66.84%\n",
      "Batch 72, Loss: 1.119688, Accuracy: 66.78%\n",
      "Batch 73, Loss: 0.980379, Accuracy: 66.95%\n",
      "Batch 74, Loss: 1.023373, Accuracy: 67.02%\n",
      "Batch 75, Loss: 1.064484, Accuracy: 67.00%\n",
      "Batch 76, Loss: 1.080199, Accuracy: 66.96%\n",
      "Batch 77, Loss: 1.092317, Accuracy: 66.90%\n",
      "Batch 78, Loss: 1.081089, Accuracy: 66.89%\n",
      "Batch 79, Loss: 1.017372, Accuracy: 66.99%\n",
      "Batch 80, Loss: 1.075467, Accuracy: 66.97%\n",
      "Batch 81, Loss: 1.114963, Accuracy: 66.94%\n",
      "Batch 82, Loss: 1.028240, Accuracy: 67.00%\n",
      "Batch 83, Loss: 1.087616, Accuracy: 66.96%\n",
      "Batch 84, Loss: 1.087106, Accuracy: 66.93%\n",
      "Batch 85, Loss: 1.118611, Accuracy: 66.89%\n",
      "Batch 86, Loss: 1.093937, Accuracy: 66.84%\n",
      "Batch 87, Loss: 1.101867, Accuracy: 66.83%\n",
      "Batch 88, Loss: 1.135771, Accuracy: 66.76%\n",
      "Batch 89, Loss: 1.000144, Accuracy: 66.84%\n",
      "Batch 90, Loss: 1.036474, Accuracy: 66.86%\n",
      "Batch 91, Loss: 1.104222, Accuracy: 66.79%\n",
      "Batch 92, Loss: 1.061313, Accuracy: 66.78%\n",
      "Batch 93, Loss: 1.090747, Accuracy: 66.75%\n",
      "Batch 94, Loss: 1.095015, Accuracy: 66.72%\n",
      "Batch 95, Loss: 1.084755, Accuracy: 66.69%\n",
      "Batch 96, Loss: 1.115522, Accuracy: 66.63%\n",
      "Batch 97, Loss: 1.102393, Accuracy: 66.61%\n",
      "Batch 98, Loss: 1.137907, Accuracy: 66.53%\n",
      "Batch 99, Loss: 1.084946, Accuracy: 66.49%\n",
      "Batch 100, Loss: 1.125664, Accuracy: 66.44%\n",
      "Batch 101, Loss: 1.180488, Accuracy: 66.32%\n",
      "Batch 102, Loss: 1.130322, Accuracy: 66.27%\n",
      "Batch 103, Loss: 1.127797, Accuracy: 66.22%\n",
      "Batch 104, Loss: 1.008011, Accuracy: 66.29%\n",
      "Batch 105, Loss: 1.018617, Accuracy: 66.35%\n",
      "Batch 106, Loss: 1.091300, Accuracy: 66.33%\n",
      "Batch 107, Loss: 0.986572, Accuracy: 66.41%\n",
      "Batch 108, Loss: 1.082778, Accuracy: 66.41%\n",
      "Batch 109, Loss: 1.082677, Accuracy: 66.40%\n",
      "Batch 110, Loss: 1.092955, Accuracy: 66.39%\n",
      "Batch 111, Loss: 1.120606, Accuracy: 66.37%\n",
      "Batch 112, Loss: 1.017285, Accuracy: 66.41%\n",
      "Batch 113, Loss: 1.070186, Accuracy: 66.39%\n",
      "Batch 114, Loss: 1.086365, Accuracy: 66.38%\n",
      "Batch 115, Loss: 1.163724, Accuracy: 66.30%\n",
      "Batch 116, Loss: 1.103126, Accuracy: 66.26%\n",
      "Batch 117, Loss: 1.078453, Accuracy: 66.25%\n",
      "Batch 118, Loss: 1.072773, Accuracy: 66.26%\n",
      "Batch 119, Loss: 1.139304, Accuracy: 66.22%\n",
      "Batch 120, Loss: 1.202771, Accuracy: 66.11%\n",
      "Batch 121, Loss: 1.055254, Accuracy: 66.13%\n",
      "Batch 122, Loss: 1.058887, Accuracy: 66.15%\n",
      "Batch 123, Loss: 1.019308, Accuracy: 66.20%\n",
      "Batch 124, Loss: 1.105414, Accuracy: 66.18%\n",
      "Batch 125, Loss: 1.075822, Accuracy: 66.19%\n",
      "Batch 126, Loss: 0.997724, Accuracy: 66.27%\n",
      "Batch 127, Loss: 1.090802, Accuracy: 66.28%\n",
      "Batch 128, Loss: 0.967060, Accuracy: 66.36%\n",
      "Batch 129, Loss: 1.157214, Accuracy: 66.28%\n",
      "Batch 130, Loss: 1.095346, Accuracy: 66.26%\n",
      "Batch 131, Loss: 1.123214, Accuracy: 66.23%\n",
      "Batch 132, Loss: 1.134721, Accuracy: 66.19%\n",
      "Batch 133, Loss: 1.071516, Accuracy: 66.19%\n",
      "Batch 134, Loss: 1.044375, Accuracy: 66.23%\n",
      "Batch 135, Loss: 1.143917, Accuracy: 66.16%\n",
      "Batch 136, Loss: 1.104922, Accuracy: 66.14%\n",
      "Batch 137, Loss: 1.029426, Accuracy: 66.17%\n",
      "Batch 138, Loss: 1.065225, Accuracy: 66.17%\n",
      "Batch 139, Loss: 1.100843, Accuracy: 66.16%\n",
      "Batch 140, Loss: 1.110884, Accuracy: 66.16%\n",
      "Batch 141, Loss: 1.056522, Accuracy: 66.18%\n",
      "Batch 142, Loss: 1.171939, Accuracy: 66.11%\n",
      "Batch 143, Loss: 1.077316, Accuracy: 66.12%\n",
      "Batch 144, Loss: 1.142976, Accuracy: 66.06%\n",
      "Batch 145, Loss: 1.183586, Accuracy: 65.95%\n",
      "Batch 146, Loss: 1.108050, Accuracy: 65.92%\n",
      "Batch 147, Loss: 1.090873, Accuracy: 65.91%\n",
      "Batch 148, Loss: 1.063400, Accuracy: 65.92%\n",
      "Batch 149, Loss: 1.110762, Accuracy: 65.90%\n",
      "Batch 150, Loss: 1.100351, Accuracy: 65.91%\n",
      "Batch 151, Loss: 1.081753, Accuracy: 65.88%\n",
      "Batch 152, Loss: 1.129024, Accuracy: 65.85%\n",
      "Batch 153, Loss: 1.131467, Accuracy: 65.80%\n",
      "Batch 154, Loss: 1.066703, Accuracy: 65.82%\n",
      "Batch 155, Loss: 1.098683, Accuracy: 65.82%\n",
      "Batch 156, Loss: 1.057678, Accuracy: 65.82%\n",
      "Batch 157, Loss: 1.199626, Accuracy: 65.74%\n",
      "Batch 158, Loss: 1.071201, Accuracy: 65.76%\n",
      "Batch 159, Loss: 1.005135, Accuracy: 65.82%\n",
      "Batch 160, Loss: 1.111984, Accuracy: 65.81%\n",
      "Batch 161, Loss: 1.130274, Accuracy: 65.79%\n",
      "Batch 162, Loss: 1.185020, Accuracy: 65.71%\n",
      "Batch 163, Loss: 1.055827, Accuracy: 65.72%\n",
      "Batch 164, Loss: 1.073838, Accuracy: 65.74%\n",
      "Batch 165, Loss: 1.062119, Accuracy: 65.75%\n",
      "Batch 166, Loss: 1.128620, Accuracy: 65.71%\n",
      "Batch 167, Loss: 1.088436, Accuracy: 65.70%\n",
      "Batch 168, Loss: 1.113239, Accuracy: 65.68%\n",
      "Batch 169, Loss: 1.026839, Accuracy: 65.73%\n",
      "Batch 170, Loss: 1.077237, Accuracy: 65.72%\n",
      "Batch 171, Loss: 1.161872, Accuracy: 65.67%\n",
      "Batch 172, Loss: 1.052653, Accuracy: 65.68%\n",
      "Batch 173, Loss: 1.128328, Accuracy: 65.65%\n",
      "Batch 174, Loss: 1.025869, Accuracy: 65.70%\n",
      "Batch 175, Loss: 1.076637, Accuracy: 65.71%\n",
      "Batch 176, Loss: 1.063090, Accuracy: 65.71%\n",
      "Batch 177, Loss: 1.014322, Accuracy: 65.77%\n",
      "Batch 178, Loss: 1.077502, Accuracy: 65.77%\n",
      "Batch 179, Loss: 1.049265, Accuracy: 65.77%\n",
      "Batch 180, Loss: 1.081019, Accuracy: 65.78%\n",
      "Batch 181, Loss: 1.149641, Accuracy: 65.75%\n",
      "Batch 182, Loss: 1.048598, Accuracy: 65.76%\n",
      "Batch 183, Loss: 1.171961, Accuracy: 65.69%\n",
      "Batch 184, Loss: 1.095351, Accuracy: 65.67%\n",
      "Batch 185, Loss: 1.185816, Accuracy: 65.60%\n",
      "Batch 186, Loss: 1.071826, Accuracy: 65.62%\n",
      "Batch 187, Loss: 1.080473, Accuracy: 65.61%\n",
      "Batch 188, Loss: 1.112801, Accuracy: 65.59%\n",
      "Batch 189, Loss: 1.082713, Accuracy: 65.59%\n",
      "Batch 190, Loss: 1.100599, Accuracy: 65.58%\n",
      "Batch 191, Loss: 1.116261, Accuracy: 65.56%\n",
      "Batch 192, Loss: 1.069054, Accuracy: 65.57%\n",
      "Batch 193, Loss: 1.071038, Accuracy: 65.57%\n",
      "Batch 194, Loss: 1.211836, Accuracy: 65.50%\n",
      "Batch 195, Loss: 1.016898, Accuracy: 65.53%\n",
      "Batch 196, Loss: 1.143373, Accuracy: 65.50%\n",
      "Batch 197, Loss: 1.037753, Accuracy: 65.52%\n",
      "Batch 198, Loss: 1.037588, Accuracy: 65.56%\n",
      "Batch 199, Loss: 1.150923, Accuracy: 65.54%\n",
      "Batch 200, Loss: 1.100885, Accuracy: 65.52%\n",
      "Batch 201, Loss: 1.001100, Accuracy: 65.55%\n",
      "Batch 202, Loss: 1.085950, Accuracy: 65.54%\n",
      "Batch 203, Loss: 1.075243, Accuracy: 65.55%\n",
      "Batch 204, Loss: 1.022715, Accuracy: 65.59%\n",
      "Batch 205, Loss: 1.061593, Accuracy: 65.59%\n",
      "Batch 206, Loss: 1.049406, Accuracy: 65.62%\n",
      "Batch 207, Loss: 1.179577, Accuracy: 65.56%\n",
      "Batch 208, Loss: 1.021543, Accuracy: 65.59%\n",
      "Batch 209, Loss: 1.107479, Accuracy: 65.57%\n",
      "Batch 210, Loss: 1.111765, Accuracy: 65.56%\n",
      "Batch 211, Loss: 1.063397, Accuracy: 65.55%\n",
      "Batch 212, Loss: 1.104715, Accuracy: 65.54%\n",
      "Batch 213, Loss: 1.102379, Accuracy: 65.52%\n",
      "Training - Epoch 20, Loss: 1.083596, Accuracy: 65.52%\n",
      "Validation Batch 1, Loss: 1.174633, Accuracy: 56.25%\n",
      "Validation Batch 2, Loss: 1.256842, Accuracy: 50.78%\n",
      "Validation Batch 3, Loss: 1.317703, Accuracy: 46.88%\n",
      "Validation Batch 4, Loss: 1.161348, Accuracy: 49.22%\n",
      "Validation Batch 5, Loss: 1.268731, Accuracy: 48.12%\n",
      "Validation Batch 6, Loss: 1.252548, Accuracy: 47.66%\n",
      "Validation Batch 7, Loss: 1.223556, Accuracy: 47.77%\n",
      "Validation Batch 8, Loss: 1.227088, Accuracy: 48.24%\n",
      "Validation Batch 9, Loss: 1.290666, Accuracy: 47.40%\n",
      "Validation Batch 10, Loss: 1.255561, Accuracy: 47.03%\n",
      "Validation Batch 11, Loss: 1.216793, Accuracy: 47.16%\n",
      "Validation Batch 12, Loss: 1.177889, Accuracy: 47.79%\n",
      "Validation Batch 13, Loss: 1.283399, Accuracy: 47.36%\n",
      "Validation Batch 14, Loss: 1.176910, Accuracy: 48.21%\n",
      "Validation Batch 15, Loss: 1.236660, Accuracy: 48.23%\n",
      "Validation Batch 16, Loss: 1.246982, Accuracy: 48.14%\n",
      "Validation Batch 17, Loss: 1.306274, Accuracy: 47.52%\n",
      "Validation Batch 18, Loss: 1.214155, Accuracy: 47.66%\n",
      "Validation Batch 19, Loss: 1.280973, Accuracy: 47.53%\n",
      "Validation Batch 20, Loss: 1.226683, Accuracy: 47.58%\n",
      "Validation Batch 21, Loss: 1.244539, Accuracy: 47.47%\n",
      "Validation Batch 22, Loss: 1.265465, Accuracy: 47.37%\n",
      "Validation Batch 23, Loss: 1.298206, Accuracy: 47.08%\n",
      "Validation Batch 24, Loss: 1.244609, Accuracy: 47.01%\n",
      "Validation Batch 25, Loss: 1.213551, Accuracy: 47.12%\n",
      "Validation Batch 26, Loss: 1.167029, Accuracy: 47.42%\n",
      "Validation Batch 27, Loss: 1.273280, Accuracy: 47.21%\n",
      "Validation - Epoch 20, Loss: 1.240817, Accuracy: 47.21%\n",
      "Patienceâ€”0\n",
      "Epoch 21\n",
      "Batch 1, Loss: 1.187642, Accuracy: 53.12%\n",
      "Batch 2, Loss: 1.171733, Accuracy: 53.12%\n",
      "Batch 3, Loss: 1.109055, Accuracy: 56.25%\n",
      "Batch 4, Loss: 1.090271, Accuracy: 58.98%\n",
      "Batch 5, Loss: 1.105855, Accuracy: 60.31%\n",
      "Batch 6, Loss: 1.068624, Accuracy: 61.72%\n",
      "Batch 7, Loss: 1.046648, Accuracy: 63.17%\n",
      "Batch 8, Loss: 1.035871, Accuracy: 63.87%\n",
      "Batch 9, Loss: 1.143771, Accuracy: 63.54%\n",
      "Batch 10, Loss: 1.143200, Accuracy: 62.81%\n",
      "Batch 11, Loss: 1.064373, Accuracy: 63.07%\n",
      "Batch 12, Loss: 1.106209, Accuracy: 62.76%\n",
      "Batch 13, Loss: 1.094913, Accuracy: 62.86%\n",
      "Batch 14, Loss: 1.110659, Accuracy: 62.61%\n",
      "Batch 15, Loss: 1.035627, Accuracy: 63.23%\n",
      "Batch 16, Loss: 1.011366, Accuracy: 63.77%\n",
      "Batch 17, Loss: 1.035468, Accuracy: 64.15%\n",
      "Batch 18, Loss: 1.045089, Accuracy: 64.58%\n",
      "Batch 19, Loss: 1.123346, Accuracy: 64.31%\n",
      "Batch 20, Loss: 1.100605, Accuracy: 64.22%\n",
      "Batch 21, Loss: 1.168318, Accuracy: 63.76%\n",
      "Batch 22, Loss: 1.005524, Accuracy: 64.28%\n",
      "Batch 23, Loss: 1.267539, Accuracy: 63.52%\n",
      "Batch 24, Loss: 1.108220, Accuracy: 63.54%\n",
      "Batch 25, Loss: 1.032825, Accuracy: 63.88%\n",
      "Batch 26, Loss: 1.092076, Accuracy: 63.76%\n",
      "Batch 27, Loss: 1.090164, Accuracy: 63.83%\n",
      "Batch 28, Loss: 1.057366, Accuracy: 64.01%\n",
      "Batch 29, Loss: 1.120009, Accuracy: 63.90%\n",
      "Batch 30, Loss: 1.157004, Accuracy: 63.70%\n",
      "Batch 31, Loss: 1.101683, Accuracy: 63.66%\n",
      "Batch 32, Loss: 1.101038, Accuracy: 63.62%\n",
      "Batch 33, Loss: 1.033605, Accuracy: 63.92%\n",
      "Batch 34, Loss: 1.107214, Accuracy: 63.97%\n",
      "Batch 35, Loss: 1.032510, Accuracy: 64.15%\n",
      "Batch 36, Loss: 1.019696, Accuracy: 64.45%\n",
      "Batch 37, Loss: 1.118505, Accuracy: 64.36%\n",
      "Batch 38, Loss: 1.090350, Accuracy: 64.27%\n",
      "Batch 39, Loss: 1.128194, Accuracy: 64.22%\n",
      "Batch 40, Loss: 0.996174, Accuracy: 64.53%\n",
      "Batch 41, Loss: 1.087136, Accuracy: 64.60%\n",
      "Batch 42, Loss: 1.115019, Accuracy: 64.51%\n",
      "Batch 43, Loss: 1.098938, Accuracy: 64.50%\n",
      "Batch 44, Loss: 1.159507, Accuracy: 64.31%\n",
      "Batch 45, Loss: 1.083500, Accuracy: 64.38%\n",
      "Batch 46, Loss: 1.082530, Accuracy: 64.47%\n",
      "Batch 47, Loss: 1.122735, Accuracy: 64.43%\n",
      "Batch 48, Loss: 1.137864, Accuracy: 64.39%\n",
      "Batch 49, Loss: 1.098443, Accuracy: 64.32%\n",
      "Batch 50, Loss: 1.042889, Accuracy: 64.38%\n",
      "Batch 51, Loss: 1.096953, Accuracy: 64.37%\n",
      "Batch 52, Loss: 1.058009, Accuracy: 64.45%\n",
      "Batch 53, Loss: 1.031401, Accuracy: 64.59%\n",
      "Batch 54, Loss: 1.105907, Accuracy: 64.55%\n",
      "Batch 55, Loss: 1.031044, Accuracy: 64.69%\n",
      "Batch 56, Loss: 1.081798, Accuracy: 64.70%\n",
      "Batch 57, Loss: 1.155764, Accuracy: 64.64%\n",
      "Batch 58, Loss: 1.146088, Accuracy: 64.55%\n",
      "Batch 59, Loss: 1.039679, Accuracy: 64.62%\n",
      "Batch 60, Loss: 1.083525, Accuracy: 64.66%\n",
      "Batch 61, Loss: 1.126202, Accuracy: 64.57%\n",
      "Batch 62, Loss: 0.964294, Accuracy: 64.74%\n",
      "Batch 63, Loss: 1.117626, Accuracy: 64.68%\n",
      "Batch 64, Loss: 1.056045, Accuracy: 64.72%\n",
      "Batch 65, Loss: 1.066807, Accuracy: 64.74%\n",
      "Batch 66, Loss: 1.125483, Accuracy: 64.70%\n",
      "Batch 67, Loss: 1.077902, Accuracy: 64.72%\n",
      "Batch 68, Loss: 1.112592, Accuracy: 64.68%\n",
      "Batch 69, Loss: 1.055224, Accuracy: 64.74%\n",
      "Batch 70, Loss: 0.989568, Accuracy: 64.91%\n",
      "Batch 71, Loss: 1.003655, Accuracy: 65.05%\n",
      "Batch 72, Loss: 1.110221, Accuracy: 65.04%\n",
      "Batch 73, Loss: 1.127024, Accuracy: 65.05%\n",
      "Batch 74, Loss: 1.103956, Accuracy: 65.03%\n",
      "Batch 75, Loss: 1.027622, Accuracy: 65.12%\n",
      "Batch 76, Loss: 1.031829, Accuracy: 65.21%\n",
      "Batch 77, Loss: 1.064606, Accuracy: 65.20%\n",
      "Batch 78, Loss: 1.064419, Accuracy: 65.22%\n",
      "Batch 79, Loss: 1.085235, Accuracy: 65.21%\n",
      "Batch 80, Loss: 1.030185, Accuracy: 65.27%\n",
      "Batch 81, Loss: 1.055714, Accuracy: 65.32%\n",
      "Batch 82, Loss: 1.046495, Accuracy: 65.34%\n",
      "Batch 83, Loss: 1.101720, Accuracy: 65.32%\n",
      "Batch 84, Loss: 1.062552, Accuracy: 65.33%\n",
      "Batch 85, Loss: 1.100225, Accuracy: 65.31%\n",
      "Batch 86, Loss: 1.055581, Accuracy: 65.33%\n",
      "Batch 87, Loss: 1.115351, Accuracy: 65.30%\n",
      "Batch 88, Loss: 1.103162, Accuracy: 65.29%\n",
      "Batch 89, Loss: 1.060703, Accuracy: 65.31%\n",
      "Batch 90, Loss: 1.028138, Accuracy: 65.42%\n",
      "Batch 91, Loss: 1.161682, Accuracy: 65.35%\n",
      "Batch 92, Loss: 1.111589, Accuracy: 65.34%\n",
      "Batch 93, Loss: 1.101753, Accuracy: 65.32%\n",
      "Batch 94, Loss: 1.054969, Accuracy: 65.33%\n",
      "Batch 95, Loss: 1.177913, Accuracy: 65.25%\n",
      "Batch 96, Loss: 1.053203, Accuracy: 65.25%\n",
      "Batch 97, Loss: 1.173870, Accuracy: 65.17%\n",
      "Batch 98, Loss: 1.099719, Accuracy: 65.18%\n",
      "Batch 99, Loss: 1.132654, Accuracy: 65.12%\n",
      "Batch 100, Loss: 1.126982, Accuracy: 65.05%\n",
      "Batch 101, Loss: 1.167718, Accuracy: 64.98%\n",
      "Batch 102, Loss: 1.046475, Accuracy: 65.03%\n",
      "Batch 103, Loss: 1.069860, Accuracy: 65.08%\n",
      "Batch 104, Loss: 1.191305, Accuracy: 64.93%\n",
      "Batch 105, Loss: 1.054055, Accuracy: 64.97%\n",
      "Batch 106, Loss: 1.074274, Accuracy: 64.99%\n",
      "Batch 107, Loss: 1.045407, Accuracy: 65.04%\n",
      "Batch 108, Loss: 0.996984, Accuracy: 65.15%\n",
      "Batch 109, Loss: 1.142266, Accuracy: 65.09%\n",
      "Batch 110, Loss: 1.126566, Accuracy: 65.06%\n",
      "Batch 111, Loss: 0.979488, Accuracy: 65.19%\n",
      "Batch 112, Loss: 1.123168, Accuracy: 65.18%\n",
      "Batch 113, Loss: 1.067728, Accuracy: 65.20%\n",
      "Batch 114, Loss: 1.138451, Accuracy: 65.17%\n",
      "Batch 115, Loss: 1.049908, Accuracy: 65.23%\n",
      "Batch 116, Loss: 1.069185, Accuracy: 65.25%\n",
      "Batch 117, Loss: 1.143574, Accuracy: 65.20%\n",
      "Batch 118, Loss: 1.151898, Accuracy: 65.12%\n",
      "Batch 119, Loss: 1.113177, Accuracy: 65.13%\n",
      "Batch 120, Loss: 1.025081, Accuracy: 65.20%\n",
      "Batch 121, Loss: 1.087260, Accuracy: 65.19%\n",
      "Batch 122, Loss: 1.032665, Accuracy: 65.22%\n",
      "Batch 123, Loss: 1.094056, Accuracy: 65.18%\n",
      "Batch 124, Loss: 1.035730, Accuracy: 65.22%\n",
      "Batch 125, Loss: 1.064990, Accuracy: 65.22%\n",
      "Batch 126, Loss: 1.036964, Accuracy: 65.25%\n",
      "Batch 127, Loss: 1.039155, Accuracy: 65.29%\n",
      "Batch 128, Loss: 1.116978, Accuracy: 65.30%\n",
      "Batch 129, Loss: 1.071028, Accuracy: 65.31%\n",
      "Batch 130, Loss: 1.188570, Accuracy: 65.23%\n",
      "Batch 131, Loss: 1.009920, Accuracy: 65.29%\n",
      "Batch 132, Loss: 1.049453, Accuracy: 65.34%\n",
      "Batch 133, Loss: 1.082845, Accuracy: 65.34%\n",
      "Batch 134, Loss: 1.049486, Accuracy: 65.37%\n",
      "Batch 135, Loss: 1.017775, Accuracy: 65.42%\n",
      "Batch 136, Loss: 1.026103, Accuracy: 65.49%\n",
      "Batch 137, Loss: 1.088618, Accuracy: 65.49%\n",
      "Batch 138, Loss: 1.130770, Accuracy: 65.46%\n",
      "Batch 139, Loss: 1.054367, Accuracy: 65.46%\n",
      "Batch 140, Loss: 0.947540, Accuracy: 65.57%\n",
      "Batch 141, Loss: 1.005581, Accuracy: 65.65%\n",
      "Batch 142, Loss: 1.048294, Accuracy: 65.69%\n",
      "Batch 143, Loss: 1.129192, Accuracy: 65.66%\n",
      "Batch 144, Loss: 1.022445, Accuracy: 65.69%\n",
      "Batch 145, Loss: 1.068492, Accuracy: 65.67%\n",
      "Batch 146, Loss: 1.022270, Accuracy: 65.71%\n",
      "Batch 147, Loss: 1.085454, Accuracy: 65.70%\n",
      "Batch 148, Loss: 1.086299, Accuracy: 65.70%\n",
      "Batch 149, Loss: 1.045123, Accuracy: 65.70%\n",
      "Batch 150, Loss: 1.136060, Accuracy: 65.65%\n",
      "Batch 151, Loss: 1.081366, Accuracy: 65.67%\n",
      "Batch 152, Loss: 1.032239, Accuracy: 65.71%\n",
      "Batch 153, Loss: 1.110115, Accuracy: 65.70%\n",
      "Batch 154, Loss: 1.040688, Accuracy: 65.75%\n",
      "Batch 155, Loss: 1.102916, Accuracy: 65.74%\n",
      "Batch 156, Loss: 0.996843, Accuracy: 65.79%\n",
      "Batch 157, Loss: 1.016204, Accuracy: 65.84%\n",
      "Batch 158, Loss: 1.100176, Accuracy: 65.83%\n",
      "Batch 159, Loss: 0.987174, Accuracy: 65.88%\n",
      "Batch 160, Loss: 1.120727, Accuracy: 65.85%\n",
      "Batch 161, Loss: 1.092268, Accuracy: 65.85%\n",
      "Batch 162, Loss: 1.152461, Accuracy: 65.79%\n",
      "Batch 163, Loss: 1.099939, Accuracy: 65.77%\n",
      "Batch 164, Loss: 1.110167, Accuracy: 65.73%\n",
      "Batch 165, Loss: 1.034198, Accuracy: 65.76%\n",
      "Batch 166, Loss: 1.007467, Accuracy: 65.79%\n",
      "Batch 167, Loss: 0.985824, Accuracy: 65.83%\n",
      "Batch 168, Loss: 1.058173, Accuracy: 65.85%\n",
      "Batch 169, Loss: 1.118245, Accuracy: 65.83%\n",
      "Batch 170, Loss: 1.024850, Accuracy: 65.85%\n",
      "Batch 171, Loss: 1.136000, Accuracy: 65.84%\n",
      "Batch 172, Loss: 1.094627, Accuracy: 65.84%\n",
      "Batch 173, Loss: 1.044322, Accuracy: 65.87%\n",
      "Batch 174, Loss: 1.096543, Accuracy: 65.86%\n",
      "Batch 175, Loss: 1.046113, Accuracy: 65.87%\n",
      "Batch 176, Loss: 1.203685, Accuracy: 65.80%\n",
      "Batch 177, Loss: 1.049992, Accuracy: 65.81%\n",
      "Batch 178, Loss: 1.169958, Accuracy: 65.74%\n",
      "Batch 179, Loss: 1.042624, Accuracy: 65.76%\n",
      "Batch 180, Loss: 1.040582, Accuracy: 65.78%\n",
      "Batch 181, Loss: 1.121425, Accuracy: 65.75%\n",
      "Batch 182, Loss: 1.124018, Accuracy: 65.73%\n",
      "Batch 183, Loss: 1.058610, Accuracy: 65.74%\n",
      "Batch 184, Loss: 1.100838, Accuracy: 65.72%\n",
      "Batch 185, Loss: 1.092890, Accuracy: 65.73%\n",
      "Batch 186, Loss: 0.980351, Accuracy: 65.79%\n",
      "Batch 187, Loss: 1.033207, Accuracy: 65.83%\n",
      "Batch 188, Loss: 1.060717, Accuracy: 65.84%\n",
      "Batch 189, Loss: 1.156952, Accuracy: 65.79%\n",
      "Batch 190, Loss: 1.100878, Accuracy: 65.77%\n",
      "Batch 191, Loss: 1.100308, Accuracy: 65.76%\n",
      "Batch 192, Loss: 1.083993, Accuracy: 65.76%\n",
      "Batch 193, Loss: 1.020910, Accuracy: 65.80%\n",
      "Batch 194, Loss: 1.064620, Accuracy: 65.82%\n",
      "Batch 195, Loss: 1.115780, Accuracy: 65.81%\n",
      "Batch 196, Loss: 1.117222, Accuracy: 65.80%\n",
      "Batch 197, Loss: 1.095416, Accuracy: 65.78%\n",
      "Batch 198, Loss: 1.056008, Accuracy: 65.79%\n",
      "Batch 199, Loss: 1.106987, Accuracy: 65.78%\n",
      "Batch 200, Loss: 1.077885, Accuracy: 65.77%\n",
      "Batch 201, Loss: 1.127021, Accuracy: 65.75%\n",
      "Batch 202, Loss: 1.127614, Accuracy: 65.73%\n",
      "Batch 203, Loss: 1.074896, Accuracy: 65.74%\n",
      "Batch 204, Loss: 1.123433, Accuracy: 65.72%\n",
      "Batch 205, Loss: 1.112029, Accuracy: 65.71%\n",
      "Batch 206, Loss: 1.150156, Accuracy: 65.68%\n",
      "Batch 207, Loss: 0.956816, Accuracy: 65.75%\n",
      "Batch 208, Loss: 1.097751, Accuracy: 65.72%\n",
      "Batch 209, Loss: 1.116359, Accuracy: 65.71%\n",
      "Batch 210, Loss: 0.987386, Accuracy: 65.77%\n",
      "Batch 211, Loss: 1.025109, Accuracy: 65.80%\n",
      "Batch 212, Loss: 1.093370, Accuracy: 65.79%\n",
      "Batch 213, Loss: 1.149840, Accuracy: 65.76%\n",
      "Training - Epoch 21, Loss: 1.081873, Accuracy: 65.76%\n",
      "Validation Batch 1, Loss: 1.169991, Accuracy: 57.81%\n",
      "Validation Batch 2, Loss: 1.254900, Accuracy: 52.34%\n",
      "Validation Batch 3, Loss: 1.311906, Accuracy: 47.92%\n",
      "Validation Batch 4, Loss: 1.159578, Accuracy: 50.39%\n",
      "Validation Batch 5, Loss: 1.269143, Accuracy: 49.06%\n",
      "Validation Batch 6, Loss: 1.245262, Accuracy: 48.96%\n",
      "Validation Batch 7, Loss: 1.218631, Accuracy: 49.11%\n",
      "Validation Batch 8, Loss: 1.227037, Accuracy: 49.41%\n",
      "Validation Batch 9, Loss: 1.289374, Accuracy: 48.44%\n",
      "Validation Batch 10, Loss: 1.253129, Accuracy: 47.81%\n",
      "Validation Batch 11, Loss: 1.211647, Accuracy: 48.01%\n",
      "Validation Batch 12, Loss: 1.170957, Accuracy: 48.57%\n",
      "Validation Batch 13, Loss: 1.279882, Accuracy: 48.32%\n",
      "Validation Batch 14, Loss: 1.174689, Accuracy: 48.88%\n",
      "Validation Batch 15, Loss: 1.233639, Accuracy: 48.85%\n",
      "Validation Batch 16, Loss: 1.247367, Accuracy: 48.83%\n",
      "Validation Batch 17, Loss: 1.307617, Accuracy: 48.16%\n",
      "Validation Batch 18, Loss: 1.212356, Accuracy: 48.26%\n",
      "Validation Batch 19, Loss: 1.283985, Accuracy: 48.11%\n",
      "Validation Batch 20, Loss: 1.224327, Accuracy: 48.20%\n",
      "Validation Batch 21, Loss: 1.246597, Accuracy: 48.14%\n",
      "Validation Batch 22, Loss: 1.265980, Accuracy: 48.08%\n",
      "Validation Batch 23, Loss: 1.292255, Accuracy: 47.76%\n",
      "Validation Batch 24, Loss: 1.241280, Accuracy: 47.72%\n",
      "Validation Batch 25, Loss: 1.209958, Accuracy: 47.88%\n",
      "Validation Batch 26, Loss: 1.165333, Accuracy: 48.14%\n",
      "Validation Batch 27, Loss: 1.268194, Accuracy: 47.92%\n",
      "Validation - Epoch 21, Loss: 1.238334, Accuracy: 47.92%\n",
      "Patienceâ€”0\n",
      "Epoch 22\n",
      "Batch 1, Loss: 1.134053, Accuracy: 62.50%\n",
      "Batch 2, Loss: 1.163987, Accuracy: 58.59%\n",
      "Batch 3, Loss: 1.080070, Accuracy: 60.94%\n",
      "Batch 4, Loss: 1.044667, Accuracy: 63.67%\n",
      "Batch 5, Loss: 1.088132, Accuracy: 63.44%\n",
      "Batch 6, Loss: 1.196433, Accuracy: 61.72%\n",
      "Batch 7, Loss: 1.090496, Accuracy: 62.05%\n",
      "Batch 8, Loss: 1.099192, Accuracy: 62.30%\n",
      "Batch 9, Loss: 1.090684, Accuracy: 62.50%\n",
      "Batch 10, Loss: 1.161099, Accuracy: 61.72%\n",
      "Batch 11, Loss: 0.993582, Accuracy: 63.07%\n",
      "Batch 12, Loss: 1.053718, Accuracy: 63.67%\n",
      "Batch 13, Loss: 1.083452, Accuracy: 63.94%\n",
      "Batch 14, Loss: 1.063267, Accuracy: 63.95%\n",
      "Batch 15, Loss: 1.063688, Accuracy: 64.17%\n",
      "Batch 16, Loss: 1.019126, Accuracy: 64.84%\n",
      "Batch 17, Loss: 1.055707, Accuracy: 64.98%\n",
      "Batch 18, Loss: 1.059345, Accuracy: 65.10%\n",
      "Batch 19, Loss: 1.021625, Accuracy: 65.46%\n",
      "Batch 20, Loss: 1.146309, Accuracy: 65.16%\n",
      "Batch 21, Loss: 0.982163, Accuracy: 65.77%\n",
      "Batch 22, Loss: 1.113857, Accuracy: 65.55%\n",
      "Batch 23, Loss: 1.065273, Accuracy: 65.69%\n",
      "Batch 24, Loss: 1.065947, Accuracy: 65.76%\n",
      "Batch 25, Loss: 1.071535, Accuracy: 65.69%\n",
      "Batch 26, Loss: 1.121760, Accuracy: 65.50%\n",
      "Batch 27, Loss: 1.020970, Accuracy: 65.80%\n",
      "Batch 28, Loss: 1.102697, Accuracy: 65.68%\n",
      "Batch 29, Loss: 0.992785, Accuracy: 66.11%\n",
      "Batch 30, Loss: 1.057668, Accuracy: 66.25%\n",
      "Batch 31, Loss: 1.063614, Accuracy: 66.33%\n",
      "Batch 32, Loss: 1.184081, Accuracy: 65.92%\n",
      "Batch 33, Loss: 1.146578, Accuracy: 65.67%\n",
      "Batch 34, Loss: 1.015553, Accuracy: 65.90%\n",
      "Batch 35, Loss: 1.054948, Accuracy: 65.98%\n",
      "Batch 36, Loss: 1.076439, Accuracy: 66.06%\n",
      "Batch 37, Loss: 1.059126, Accuracy: 66.13%\n",
      "Batch 38, Loss: 1.053575, Accuracy: 66.28%\n",
      "Batch 39, Loss: 1.095110, Accuracy: 66.23%\n",
      "Batch 40, Loss: 1.116701, Accuracy: 66.09%\n",
      "Batch 41, Loss: 1.083707, Accuracy: 66.12%\n",
      "Batch 42, Loss: 1.101570, Accuracy: 66.11%\n",
      "Batch 43, Loss: 1.123389, Accuracy: 65.99%\n",
      "Batch 44, Loss: 1.084204, Accuracy: 65.98%\n",
      "Batch 45, Loss: 1.140058, Accuracy: 65.80%\n",
      "Batch 46, Loss: 1.036665, Accuracy: 65.93%\n",
      "Batch 47, Loss: 1.030913, Accuracy: 66.02%\n",
      "Batch 48, Loss: 1.061629, Accuracy: 66.02%\n",
      "Batch 49, Loss: 1.043175, Accuracy: 66.10%\n",
      "Batch 50, Loss: 1.120842, Accuracy: 66.00%\n",
      "Batch 51, Loss: 1.060050, Accuracy: 66.05%\n",
      "Batch 52, Loss: 1.166791, Accuracy: 65.93%\n",
      "Batch 53, Loss: 1.117318, Accuracy: 65.86%\n",
      "Batch 54, Loss: 1.086570, Accuracy: 65.86%\n",
      "Batch 55, Loss: 1.060616, Accuracy: 65.91%\n",
      "Batch 56, Loss: 1.021629, Accuracy: 65.96%\n",
      "Batch 57, Loss: 1.058236, Accuracy: 66.01%\n",
      "Batch 58, Loss: 1.052366, Accuracy: 66.14%\n",
      "Batch 59, Loss: 1.074585, Accuracy: 66.15%\n",
      "Batch 60, Loss: 1.081869, Accuracy: 66.17%\n",
      "Batch 61, Loss: 1.029260, Accuracy: 66.29%\n",
      "Batch 62, Loss: 1.012720, Accuracy: 66.41%\n",
      "Batch 63, Loss: 1.145600, Accuracy: 66.27%\n",
      "Batch 64, Loss: 1.032466, Accuracy: 66.33%\n",
      "Batch 65, Loss: 1.032120, Accuracy: 66.42%\n",
      "Batch 66, Loss: 1.085921, Accuracy: 66.41%\n",
      "Batch 67, Loss: 1.097404, Accuracy: 66.32%\n",
      "Batch 68, Loss: 1.080733, Accuracy: 66.34%\n",
      "Batch 69, Loss: 1.075726, Accuracy: 66.33%\n",
      "Batch 70, Loss: 1.028339, Accuracy: 66.43%\n",
      "Batch 71, Loss: 1.032269, Accuracy: 66.48%\n",
      "Batch 72, Loss: 1.087478, Accuracy: 66.47%\n",
      "Batch 73, Loss: 1.058697, Accuracy: 66.50%\n",
      "Batch 74, Loss: 1.167984, Accuracy: 66.34%\n",
      "Batch 75, Loss: 1.129851, Accuracy: 66.27%\n",
      "Batch 76, Loss: 0.995812, Accuracy: 66.37%\n",
      "Batch 77, Loss: 0.972104, Accuracy: 66.52%\n",
      "Batch 78, Loss: 1.130382, Accuracy: 66.41%\n",
      "Batch 79, Loss: 1.052205, Accuracy: 66.44%\n",
      "Batch 80, Loss: 1.124518, Accuracy: 66.35%\n",
      "Batch 81, Loss: 0.982690, Accuracy: 66.51%\n",
      "Batch 82, Loss: 1.127974, Accuracy: 66.48%\n",
      "Batch 83, Loss: 1.019199, Accuracy: 66.57%\n",
      "Batch 84, Loss: 1.041636, Accuracy: 66.63%\n",
      "Batch 85, Loss: 0.992717, Accuracy: 66.71%\n",
      "Batch 86, Loss: 0.947715, Accuracy: 66.90%\n",
      "Batch 87, Loss: 1.047241, Accuracy: 66.94%\n",
      "Batch 88, Loss: 0.979685, Accuracy: 67.05%\n",
      "Batch 89, Loss: 1.118366, Accuracy: 66.96%\n",
      "Batch 90, Loss: 1.137149, Accuracy: 66.88%\n",
      "Batch 91, Loss: 1.038822, Accuracy: 66.93%\n",
      "Batch 92, Loss: 1.146255, Accuracy: 66.83%\n",
      "Batch 93, Loss: 1.002818, Accuracy: 66.94%\n",
      "Batch 94, Loss: 1.082274, Accuracy: 66.94%\n",
      "Batch 95, Loss: 1.032112, Accuracy: 66.97%\n",
      "Batch 96, Loss: 1.097978, Accuracy: 66.94%\n",
      "Batch 97, Loss: 1.091354, Accuracy: 66.95%\n",
      "Batch 98, Loss: 1.044560, Accuracy: 66.96%\n",
      "Batch 99, Loss: 1.088740, Accuracy: 66.93%\n",
      "Batch 100, Loss: 1.059736, Accuracy: 66.94%\n",
      "Batch 101, Loss: 1.024670, Accuracy: 66.97%\n",
      "Batch 102, Loss: 1.115777, Accuracy: 66.93%\n",
      "Batch 103, Loss: 1.038930, Accuracy: 66.96%\n",
      "Batch 104, Loss: 1.062865, Accuracy: 66.99%\n",
      "Batch 105, Loss: 1.056880, Accuracy: 67.04%\n",
      "Batch 106, Loss: 1.048726, Accuracy: 67.07%\n",
      "Batch 107, Loss: 1.122398, Accuracy: 67.04%\n",
      "Batch 108, Loss: 1.116771, Accuracy: 66.98%\n",
      "Batch 109, Loss: 1.002394, Accuracy: 67.03%\n",
      "Batch 110, Loss: 1.044240, Accuracy: 67.05%\n",
      "Batch 111, Loss: 0.931413, Accuracy: 67.19%\n",
      "Batch 112, Loss: 1.072892, Accuracy: 67.17%\n",
      "Batch 113, Loss: 1.028029, Accuracy: 67.22%\n",
      "Batch 114, Loss: 1.097999, Accuracy: 67.19%\n",
      "Batch 115, Loss: 1.172478, Accuracy: 67.08%\n",
      "Batch 116, Loss: 1.168695, Accuracy: 66.96%\n",
      "Batch 117, Loss: 1.067028, Accuracy: 66.97%\n",
      "Batch 118, Loss: 1.107874, Accuracy: 66.95%\n",
      "Batch 119, Loss: 1.056844, Accuracy: 66.94%\n",
      "Batch 120, Loss: 1.197840, Accuracy: 66.81%\n",
      "Batch 121, Loss: 1.159202, Accuracy: 66.74%\n",
      "Batch 122, Loss: 1.039096, Accuracy: 66.74%\n",
      "Batch 123, Loss: 1.118238, Accuracy: 66.72%\n",
      "Batch 124, Loss: 1.177446, Accuracy: 66.62%\n",
      "Batch 125, Loss: 1.047342, Accuracy: 66.64%\n",
      "Batch 126, Loss: 1.058047, Accuracy: 66.67%\n",
      "Batch 127, Loss: 1.068399, Accuracy: 66.68%\n",
      "Batch 128, Loss: 1.141086, Accuracy: 66.60%\n",
      "Batch 129, Loss: 1.033001, Accuracy: 66.63%\n",
      "Batch 130, Loss: 1.021420, Accuracy: 66.67%\n",
      "Batch 131, Loss: 1.115229, Accuracy: 66.61%\n",
      "Batch 132, Loss: 1.035682, Accuracy: 66.64%\n",
      "Batch 133, Loss: 1.000145, Accuracy: 66.72%\n",
      "Batch 134, Loss: 1.138830, Accuracy: 66.66%\n",
      "Batch 135, Loss: 1.087288, Accuracy: 66.68%\n",
      "Batch 136, Loss: 1.132835, Accuracy: 66.64%\n",
      "Batch 137, Loss: 1.018871, Accuracy: 66.70%\n",
      "Batch 138, Loss: 1.062063, Accuracy: 66.68%\n",
      "Batch 139, Loss: 1.018597, Accuracy: 66.73%\n",
      "Batch 140, Loss: 1.036462, Accuracy: 66.76%\n",
      "Batch 141, Loss: 1.046584, Accuracy: 66.78%\n",
      "Batch 142, Loss: 1.025837, Accuracy: 66.80%\n",
      "Batch 143, Loss: 1.066652, Accuracy: 66.81%\n",
      "Batch 144, Loss: 1.038441, Accuracy: 66.84%\n",
      "Batch 145, Loss: 1.051518, Accuracy: 66.86%\n",
      "Batch 146, Loss: 1.051445, Accuracy: 66.88%\n",
      "Batch 147, Loss: 1.105138, Accuracy: 66.87%\n",
      "Batch 148, Loss: 1.064646, Accuracy: 66.88%\n",
      "Batch 149, Loss: 1.081041, Accuracy: 66.86%\n",
      "Batch 150, Loss: 1.066424, Accuracy: 66.88%\n",
      "Batch 151, Loss: 1.080866, Accuracy: 66.87%\n",
      "Batch 152, Loss: 1.177792, Accuracy: 66.80%\n",
      "Batch 153, Loss: 1.105387, Accuracy: 66.77%\n",
      "Batch 154, Loss: 1.058601, Accuracy: 66.77%\n",
      "Batch 155, Loss: 1.102286, Accuracy: 66.72%\n",
      "Batch 156, Loss: 1.072695, Accuracy: 66.72%\n",
      "Batch 157, Loss: 1.026430, Accuracy: 66.76%\n",
      "Batch 158, Loss: 1.037571, Accuracy: 66.78%\n",
      "Batch 159, Loss: 1.109199, Accuracy: 66.77%\n",
      "Batch 160, Loss: 1.177819, Accuracy: 66.70%\n",
      "Batch 161, Loss: 1.078090, Accuracy: 66.69%\n",
      "Batch 162, Loss: 1.106753, Accuracy: 66.69%\n",
      "Batch 163, Loss: 1.093140, Accuracy: 66.66%\n",
      "Batch 164, Loss: 1.113245, Accuracy: 66.62%\n",
      "Batch 165, Loss: 1.029900, Accuracy: 66.63%\n",
      "Batch 166, Loss: 1.109097, Accuracy: 66.62%\n",
      "Batch 167, Loss: 1.047637, Accuracy: 66.64%\n",
      "Batch 168, Loss: 1.126953, Accuracy: 66.61%\n",
      "Batch 169, Loss: 1.092697, Accuracy: 66.60%\n",
      "Batch 170, Loss: 1.126238, Accuracy: 66.55%\n",
      "Batch 171, Loss: 1.006424, Accuracy: 66.58%\n",
      "Batch 172, Loss: 1.106121, Accuracy: 66.57%\n",
      "Batch 173, Loss: 1.150227, Accuracy: 66.52%\n",
      "Batch 174, Loss: 1.065980, Accuracy: 66.53%\n",
      "Batch 175, Loss: 1.053709, Accuracy: 66.54%\n",
      "Batch 176, Loss: 1.173666, Accuracy: 66.49%\n",
      "Batch 177, Loss: 1.056218, Accuracy: 66.50%\n",
      "Batch 178, Loss: 1.035800, Accuracy: 66.54%\n",
      "Batch 179, Loss: 1.171185, Accuracy: 66.48%\n",
      "Batch 180, Loss: 1.083563, Accuracy: 66.48%\n",
      "Batch 181, Loss: 1.095310, Accuracy: 66.45%\n",
      "Batch 182, Loss: 1.118869, Accuracy: 66.44%\n",
      "Batch 183, Loss: 1.115468, Accuracy: 66.42%\n",
      "Batch 184, Loss: 1.016586, Accuracy: 66.46%\n",
      "Batch 185, Loss: 1.092873, Accuracy: 66.46%\n",
      "Batch 186, Loss: 1.152800, Accuracy: 66.41%\n",
      "Batch 187, Loss: 1.053932, Accuracy: 66.43%\n",
      "Batch 188, Loss: 1.085842, Accuracy: 66.41%\n",
      "Batch 189, Loss: 1.167184, Accuracy: 66.35%\n",
      "Batch 190, Loss: 1.068080, Accuracy: 66.35%\n",
      "Batch 191, Loss: 1.081815, Accuracy: 66.35%\n",
      "Batch 192, Loss: 1.108465, Accuracy: 66.34%\n",
      "Batch 193, Loss: 1.049511, Accuracy: 66.36%\n",
      "Batch 194, Loss: 0.990752, Accuracy: 66.41%\n",
      "Batch 195, Loss: 1.120607, Accuracy: 66.37%\n",
      "Batch 196, Loss: 1.065807, Accuracy: 66.39%\n",
      "Batch 197, Loss: 1.069439, Accuracy: 66.41%\n",
      "Batch 198, Loss: 1.023111, Accuracy: 66.45%\n",
      "Batch 199, Loss: 1.131213, Accuracy: 66.41%\n",
      "Batch 200, Loss: 1.073014, Accuracy: 66.41%\n",
      "Batch 201, Loss: 1.085058, Accuracy: 66.39%\n",
      "Batch 202, Loss: 1.109241, Accuracy: 66.38%\n",
      "Batch 203, Loss: 1.025906, Accuracy: 66.41%\n",
      "Batch 204, Loss: 1.122859, Accuracy: 66.39%\n",
      "Batch 205, Loss: 1.113324, Accuracy: 66.36%\n",
      "Batch 206, Loss: 0.990637, Accuracy: 66.41%\n",
      "Batch 207, Loss: 1.059633, Accuracy: 66.43%\n",
      "Batch 208, Loss: 1.076431, Accuracy: 66.43%\n",
      "Batch 209, Loss: 1.019655, Accuracy: 66.46%\n",
      "Batch 210, Loss: 1.032073, Accuracy: 66.48%\n",
      "Batch 211, Loss: 1.120582, Accuracy: 66.46%\n",
      "Batch 212, Loss: 1.092935, Accuracy: 66.45%\n",
      "Batch 213, Loss: 1.059221, Accuracy: 66.46%\n",
      "Training - Epoch 22, Loss: 1.076202, Accuracy: 66.46%\n",
      "Validation Batch 1, Loss: 1.136748, Accuracy: 57.81%\n",
      "Validation Batch 2, Loss: 1.235559, Accuracy: 52.34%\n",
      "Validation Batch 3, Loss: 1.285594, Accuracy: 48.96%\n",
      "Validation Batch 4, Loss: 1.129574, Accuracy: 51.95%\n",
      "Validation Batch 5, Loss: 1.234661, Accuracy: 50.94%\n",
      "Validation Batch 6, Loss: 1.205601, Accuracy: 51.56%\n",
      "Validation Batch 7, Loss: 1.192936, Accuracy: 51.79%\n",
      "Validation Batch 8, Loss: 1.204496, Accuracy: 51.95%\n",
      "Validation Batch 9, Loss: 1.260420, Accuracy: 51.04%\n",
      "Validation Batch 10, Loss: 1.213307, Accuracy: 51.09%\n",
      "Validation Batch 11, Loss: 1.179371, Accuracy: 50.99%\n",
      "Validation Batch 12, Loss: 1.141850, Accuracy: 51.69%\n",
      "Validation Batch 13, Loss: 1.258674, Accuracy: 51.32%\n",
      "Validation Batch 14, Loss: 1.154931, Accuracy: 51.90%\n",
      "Validation Batch 15, Loss: 1.211762, Accuracy: 51.88%\n",
      "Validation Batch 16, Loss: 1.210116, Accuracy: 51.86%\n",
      "Validation Batch 17, Loss: 1.283440, Accuracy: 51.38%\n",
      "Validation Batch 18, Loss: 1.183228, Accuracy: 51.39%\n",
      "Validation Batch 19, Loss: 1.262868, Accuracy: 51.07%\n",
      "Validation Batch 20, Loss: 1.198175, Accuracy: 51.17%\n",
      "Validation Batch 21, Loss: 1.210781, Accuracy: 51.12%\n",
      "Validation Batch 22, Loss: 1.243892, Accuracy: 50.85%\n",
      "Validation Batch 23, Loss: 1.269303, Accuracy: 50.48%\n",
      "Validation Batch 24, Loss: 1.206034, Accuracy: 50.65%\n",
      "Validation Batch 25, Loss: 1.183372, Accuracy: 50.81%\n",
      "Validation Batch 26, Loss: 1.148466, Accuracy: 51.08%\n",
      "Validation Batch 27, Loss: 1.228662, Accuracy: 50.91%\n",
      "Validation - Epoch 22, Loss: 1.210141, Accuracy: 50.91%\n",
      "Patienceâ€”0\n",
      "Epoch 23\n",
      "Batch 1, Loss: 1.041069, Accuracy: 70.31%\n",
      "Batch 2, Loss: 1.075042, Accuracy: 68.75%\n",
      "Batch 3, Loss: 1.086764, Accuracy: 66.67%\n",
      "Batch 4, Loss: 1.093209, Accuracy: 66.02%\n",
      "Batch 5, Loss: 1.104946, Accuracy: 65.31%\n",
      "Batch 6, Loss: 1.091838, Accuracy: 64.84%\n",
      "Batch 7, Loss: 1.035487, Accuracy: 65.62%\n",
      "Batch 8, Loss: 1.095223, Accuracy: 65.62%\n",
      "Batch 9, Loss: 1.075874, Accuracy: 65.80%\n",
      "Batch 10, Loss: 1.092118, Accuracy: 65.62%\n",
      "Batch 11, Loss: 0.999136, Accuracy: 66.34%\n",
      "Batch 12, Loss: 1.161174, Accuracy: 65.62%\n",
      "Batch 13, Loss: 0.977253, Accuracy: 66.59%\n",
      "Batch 14, Loss: 1.147582, Accuracy: 66.07%\n",
      "Batch 15, Loss: 1.084171, Accuracy: 65.94%\n",
      "Batch 16, Loss: 1.063806, Accuracy: 65.72%\n",
      "Batch 17, Loss: 1.102005, Accuracy: 65.44%\n",
      "Batch 18, Loss: 1.029690, Accuracy: 65.71%\n",
      "Batch 19, Loss: 1.062756, Accuracy: 65.79%\n",
      "Batch 20, Loss: 0.995258, Accuracy: 66.33%\n",
      "Batch 21, Loss: 0.950696, Accuracy: 66.82%\n",
      "Batch 22, Loss: 1.053611, Accuracy: 66.76%\n",
      "Batch 23, Loss: 1.117101, Accuracy: 66.58%\n",
      "Batch 24, Loss: 1.024848, Accuracy: 66.86%\n",
      "Batch 25, Loss: 1.126372, Accuracy: 66.69%\n",
      "Batch 26, Loss: 1.079517, Accuracy: 66.65%\n",
      "Batch 27, Loss: 1.090129, Accuracy: 66.55%\n",
      "Batch 28, Loss: 1.062250, Accuracy: 66.57%\n",
      "Batch 29, Loss: 1.065969, Accuracy: 66.59%\n",
      "Batch 30, Loss: 1.010088, Accuracy: 66.82%\n",
      "Batch 31, Loss: 1.148230, Accuracy: 66.63%\n",
      "Batch 32, Loss: 1.077870, Accuracy: 66.65%\n",
      "Batch 33, Loss: 1.079010, Accuracy: 66.57%\n",
      "Batch 34, Loss: 1.080537, Accuracy: 66.54%\n",
      "Batch 35, Loss: 1.015575, Accuracy: 66.70%\n",
      "Batch 36, Loss: 1.044651, Accuracy: 66.84%\n",
      "Batch 37, Loss: 0.967560, Accuracy: 67.10%\n",
      "Batch 38, Loss: 0.982132, Accuracy: 67.43%\n",
      "Batch 39, Loss: 1.148989, Accuracy: 67.19%\n",
      "Batch 40, Loss: 1.040006, Accuracy: 67.23%\n",
      "Batch 41, Loss: 1.101632, Accuracy: 67.11%\n",
      "Batch 42, Loss: 1.137691, Accuracy: 66.96%\n",
      "Batch 43, Loss: 1.054069, Accuracy: 67.04%\n",
      "Batch 44, Loss: 1.204193, Accuracy: 66.76%\n",
      "Batch 45, Loss: 1.144750, Accuracy: 66.56%\n",
      "Batch 46, Loss: 1.093534, Accuracy: 66.58%\n",
      "Batch 47, Loss: 1.112179, Accuracy: 66.56%\n",
      "Batch 48, Loss: 1.098014, Accuracy: 66.44%\n",
      "Batch 49, Loss: 1.037978, Accuracy: 66.49%\n",
      "Batch 50, Loss: 1.129709, Accuracy: 66.41%\n",
      "Batch 51, Loss: 1.033962, Accuracy: 66.48%\n",
      "Batch 52, Loss: 0.998900, Accuracy: 66.62%\n",
      "Batch 53, Loss: 1.049016, Accuracy: 66.66%\n",
      "Batch 54, Loss: 1.139663, Accuracy: 66.52%\n",
      "Batch 55, Loss: 1.048141, Accuracy: 66.59%\n",
      "Batch 56, Loss: 1.053602, Accuracy: 66.63%\n",
      "Batch 57, Loss: 1.138441, Accuracy: 66.56%\n",
      "Batch 58, Loss: 1.106555, Accuracy: 66.43%\n",
      "Batch 59, Loss: 1.082803, Accuracy: 66.39%\n",
      "Batch 60, Loss: 1.147594, Accuracy: 66.25%\n",
      "Batch 61, Loss: 1.179755, Accuracy: 66.11%\n",
      "Batch 62, Loss: 1.099976, Accuracy: 66.08%\n",
      "Batch 63, Loss: 1.087251, Accuracy: 66.05%\n",
      "Batch 64, Loss: 1.112512, Accuracy: 65.99%\n",
      "Batch 65, Loss: 1.004553, Accuracy: 66.15%\n",
      "Batch 66, Loss: 1.123093, Accuracy: 66.07%\n",
      "Batch 67, Loss: 1.070535, Accuracy: 66.09%\n",
      "Batch 68, Loss: 1.115927, Accuracy: 66.04%\n",
      "Batch 69, Loss: 1.113327, Accuracy: 66.01%\n",
      "Batch 70, Loss: 1.059417, Accuracy: 66.03%\n",
      "Batch 71, Loss: 1.156619, Accuracy: 65.89%\n",
      "Batch 72, Loss: 1.185549, Accuracy: 65.76%\n",
      "Batch 73, Loss: 1.112295, Accuracy: 65.73%\n",
      "Batch 74, Loss: 1.089689, Accuracy: 65.75%\n",
      "Batch 75, Loss: 1.046921, Accuracy: 65.81%\n",
      "Batch 76, Loss: 1.041230, Accuracy: 65.89%\n",
      "Batch 77, Loss: 1.104093, Accuracy: 65.87%\n",
      "Batch 78, Loss: 1.044984, Accuracy: 65.89%\n",
      "Batch 79, Loss: 1.017006, Accuracy: 65.94%\n",
      "Batch 80, Loss: 1.061431, Accuracy: 65.98%\n",
      "Batch 81, Loss: 0.999297, Accuracy: 66.11%\n",
      "Batch 82, Loss: 1.062125, Accuracy: 66.10%\n",
      "Batch 83, Loss: 1.137637, Accuracy: 66.00%\n",
      "Batch 84, Loss: 1.063212, Accuracy: 66.02%\n",
      "Batch 85, Loss: 1.146821, Accuracy: 65.96%\n",
      "Batch 86, Loss: 1.044718, Accuracy: 66.01%\n",
      "Batch 87, Loss: 1.092783, Accuracy: 65.97%\n",
      "Batch 88, Loss: 1.197346, Accuracy: 65.82%\n",
      "Batch 89, Loss: 1.112613, Accuracy: 65.78%\n",
      "Batch 90, Loss: 1.158553, Accuracy: 65.71%\n",
      "Batch 91, Loss: 1.090524, Accuracy: 65.69%\n",
      "Batch 92, Loss: 1.056733, Accuracy: 65.74%\n",
      "Batch 93, Loss: 1.194088, Accuracy: 65.61%\n",
      "Batch 94, Loss: 0.998686, Accuracy: 65.72%\n",
      "Batch 95, Loss: 1.071508, Accuracy: 65.74%\n",
      "Batch 96, Loss: 1.079314, Accuracy: 65.74%\n",
      "Batch 97, Loss: 1.181090, Accuracy: 65.62%\n",
      "Batch 98, Loss: 1.118520, Accuracy: 65.59%\n",
      "Batch 99, Loss: 1.150584, Accuracy: 65.50%\n",
      "Batch 100, Loss: 1.147205, Accuracy: 65.41%\n",
      "Batch 101, Loss: 1.066434, Accuracy: 65.41%\n",
      "Batch 102, Loss: 1.117952, Accuracy: 65.40%\n",
      "Batch 103, Loss: 1.064403, Accuracy: 65.44%\n",
      "Batch 104, Loss: 1.007726, Accuracy: 65.53%\n",
      "Batch 105, Loss: 1.107751, Accuracy: 65.51%\n",
      "Batch 106, Loss: 1.011997, Accuracy: 65.58%\n",
      "Batch 107, Loss: 1.035299, Accuracy: 65.65%\n",
      "Batch 108, Loss: 1.037763, Accuracy: 65.70%\n",
      "Batch 109, Loss: 1.007270, Accuracy: 65.74%\n",
      "Batch 110, Loss: 1.098398, Accuracy: 65.74%\n",
      "Batch 111, Loss: 1.263726, Accuracy: 65.57%\n",
      "Batch 112, Loss: 1.109765, Accuracy: 65.54%\n",
      "Batch 113, Loss: 1.029603, Accuracy: 65.61%\n",
      "Batch 114, Loss: 1.064224, Accuracy: 65.64%\n",
      "Batch 115, Loss: 1.060205, Accuracy: 65.65%\n",
      "Batch 116, Loss: 1.081955, Accuracy: 65.64%\n",
      "Batch 117, Loss: 1.109837, Accuracy: 65.60%\n",
      "Batch 118, Loss: 1.184254, Accuracy: 65.53%\n",
      "Batch 119, Loss: 1.097416, Accuracy: 65.51%\n",
      "Batch 120, Loss: 1.094888, Accuracy: 65.48%\n",
      "Batch 121, Loss: 0.953865, Accuracy: 65.60%\n",
      "Batch 122, Loss: 1.072381, Accuracy: 65.59%\n",
      "Batch 123, Loss: 1.091072, Accuracy: 65.57%\n",
      "Batch 124, Loss: 1.070527, Accuracy: 65.60%\n",
      "Batch 125, Loss: 1.129421, Accuracy: 65.58%\n",
      "Batch 126, Loss: 1.080533, Accuracy: 65.61%\n",
      "Batch 127, Loss: 1.049935, Accuracy: 65.64%\n",
      "Batch 128, Loss: 1.092092, Accuracy: 65.65%\n",
      "Batch 129, Loss: 1.093450, Accuracy: 65.60%\n",
      "Batch 130, Loss: 1.117227, Accuracy: 65.56%\n",
      "Batch 131, Loss: 1.069365, Accuracy: 65.57%\n",
      "Batch 132, Loss: 1.002559, Accuracy: 65.64%\n",
      "Batch 133, Loss: 1.071689, Accuracy: 65.62%\n",
      "Batch 134, Loss: 1.071448, Accuracy: 65.62%\n",
      "Batch 135, Loss: 1.131175, Accuracy: 65.59%\n",
      "Batch 136, Loss: 1.049242, Accuracy: 65.61%\n",
      "Batch 137, Loss: 1.095308, Accuracy: 65.61%\n",
      "Batch 138, Loss: 1.095988, Accuracy: 65.60%\n",
      "Batch 139, Loss: 1.067179, Accuracy: 65.62%\n",
      "Batch 140, Loss: 1.122979, Accuracy: 65.57%\n",
      "Batch 141, Loss: 1.101552, Accuracy: 65.55%\n",
      "Batch 142, Loss: 1.099376, Accuracy: 65.56%\n",
      "Batch 143, Loss: 1.092746, Accuracy: 65.54%\n",
      "Batch 144, Loss: 1.111249, Accuracy: 65.54%\n",
      "Batch 145, Loss: 1.085115, Accuracy: 65.54%\n",
      "Batch 146, Loss: 1.110307, Accuracy: 65.53%\n",
      "Batch 147, Loss: 1.048303, Accuracy: 65.56%\n",
      "Batch 148, Loss: 1.038879, Accuracy: 65.57%\n",
      "Batch 149, Loss: 1.073306, Accuracy: 65.58%\n",
      "Batch 150, Loss: 1.083122, Accuracy: 65.57%\n",
      "Batch 151, Loss: 1.066179, Accuracy: 65.59%\n",
      "Batch 152, Loss: 0.994314, Accuracy: 65.66%\n",
      "Batch 153, Loss: 1.019843, Accuracy: 65.70%\n",
      "Batch 154, Loss: 1.061508, Accuracy: 65.72%\n",
      "Batch 155, Loss: 1.111504, Accuracy: 65.72%\n",
      "Batch 156, Loss: 1.031618, Accuracy: 65.76%\n",
      "Batch 157, Loss: 1.014927, Accuracy: 65.82%\n",
      "Batch 158, Loss: 1.023490, Accuracy: 65.86%\n",
      "Batch 159, Loss: 1.076440, Accuracy: 65.86%\n",
      "Batch 160, Loss: 1.087704, Accuracy: 65.85%\n",
      "Batch 161, Loss: 1.138024, Accuracy: 65.83%\n",
      "Batch 162, Loss: 1.066643, Accuracy: 65.83%\n",
      "Batch 163, Loss: 1.077975, Accuracy: 65.85%\n",
      "Batch 164, Loss: 0.964943, Accuracy: 65.94%\n",
      "Batch 165, Loss: 1.096500, Accuracy: 65.93%\n",
      "Batch 166, Loss: 1.080865, Accuracy: 65.91%\n",
      "Batch 167, Loss: 1.091940, Accuracy: 65.89%\n",
      "Batch 168, Loss: 1.036414, Accuracy: 65.92%\n",
      "Batch 169, Loss: 1.123497, Accuracy: 65.90%\n",
      "Batch 170, Loss: 1.056762, Accuracy: 65.93%\n",
      "Batch 171, Loss: 1.053341, Accuracy: 65.95%\n",
      "Batch 172, Loss: 1.000721, Accuracy: 66.02%\n",
      "Batch 173, Loss: 1.036537, Accuracy: 66.04%\n",
      "Batch 174, Loss: 1.093196, Accuracy: 66.01%\n",
      "Batch 175, Loss: 1.008905, Accuracy: 66.06%\n",
      "Batch 176, Loss: 1.134361, Accuracy: 66.03%\n",
      "Batch 177, Loss: 1.005924, Accuracy: 66.09%\n",
      "Batch 178, Loss: 1.073128, Accuracy: 66.10%\n",
      "Batch 179, Loss: 1.034553, Accuracy: 66.11%\n",
      "Batch 180, Loss: 1.091188, Accuracy: 66.11%\n",
      "Batch 181, Loss: 1.125668, Accuracy: 66.07%\n",
      "Batch 182, Loss: 1.033847, Accuracy: 66.10%\n",
      "Batch 183, Loss: 1.107546, Accuracy: 66.08%\n",
      "Batch 184, Loss: 1.070052, Accuracy: 66.08%\n",
      "Batch 185, Loss: 1.101885, Accuracy: 66.06%\n",
      "Batch 186, Loss: 1.147423, Accuracy: 66.01%\n",
      "Batch 187, Loss: 1.012422, Accuracy: 66.03%\n",
      "Batch 188, Loss: 1.195747, Accuracy: 65.98%\n",
      "Batch 189, Loss: 0.991365, Accuracy: 66.04%\n",
      "Batch 190, Loss: 1.082596, Accuracy: 66.03%\n",
      "Batch 191, Loss: 1.015447, Accuracy: 66.07%\n",
      "Batch 192, Loss: 1.051039, Accuracy: 66.09%\n",
      "Batch 193, Loss: 1.088066, Accuracy: 66.06%\n",
      "Batch 194, Loss: 1.054278, Accuracy: 66.06%\n",
      "Batch 195, Loss: 0.995600, Accuracy: 66.11%\n",
      "Batch 196, Loss: 1.107713, Accuracy: 66.08%\n",
      "Batch 197, Loss: 1.044035, Accuracy: 66.10%\n",
      "Batch 198, Loss: 1.039138, Accuracy: 66.11%\n",
      "Batch 199, Loss: 1.048450, Accuracy: 66.12%\n",
      "Batch 200, Loss: 1.076197, Accuracy: 66.12%\n",
      "Batch 201, Loss: 1.029436, Accuracy: 66.15%\n",
      "Batch 202, Loss: 0.989901, Accuracy: 66.20%\n",
      "Batch 203, Loss: 1.120395, Accuracy: 66.18%\n",
      "Batch 204, Loss: 1.097927, Accuracy: 66.15%\n",
      "Batch 205, Loss: 1.109138, Accuracy: 66.12%\n",
      "Batch 206, Loss: 0.979927, Accuracy: 66.16%\n",
      "Batch 207, Loss: 1.016431, Accuracy: 66.19%\n",
      "Batch 208, Loss: 1.011553, Accuracy: 66.22%\n",
      "Batch 209, Loss: 1.056075, Accuracy: 66.23%\n",
      "Batch 210, Loss: 1.102097, Accuracy: 66.23%\n",
      "Batch 211, Loss: 1.071208, Accuracy: 66.23%\n",
      "Batch 212, Loss: 1.038662, Accuracy: 66.27%\n",
      "Batch 213, Loss: 1.145450, Accuracy: 66.22%\n",
      "Training - Epoch 23, Loss: 1.076777, Accuracy: 66.22%\n",
      "Validation Batch 1, Loss: 1.132796, Accuracy: 59.38%\n",
      "Validation Batch 2, Loss: 1.237346, Accuracy: 52.34%\n",
      "Validation Batch 3, Loss: 1.279442, Accuracy: 49.48%\n",
      "Validation Batch 4, Loss: 1.128624, Accuracy: 52.73%\n",
      "Validation Batch 5, Loss: 1.231489, Accuracy: 51.88%\n",
      "Validation Batch 6, Loss: 1.197545, Accuracy: 52.34%\n",
      "Validation Batch 7, Loss: 1.188154, Accuracy: 52.46%\n",
      "Validation Batch 8, Loss: 1.202275, Accuracy: 52.54%\n",
      "Validation Batch 9, Loss: 1.253334, Accuracy: 51.74%\n",
      "Validation Batch 10, Loss: 1.210252, Accuracy: 51.72%\n",
      "Validation Batch 11, Loss: 1.173582, Accuracy: 51.56%\n",
      "Validation Batch 12, Loss: 1.137691, Accuracy: 52.47%\n",
      "Validation Batch 13, Loss: 1.256493, Accuracy: 52.04%\n",
      "Validation Batch 14, Loss: 1.153946, Accuracy: 52.57%\n",
      "Validation Batch 15, Loss: 1.206697, Accuracy: 52.50%\n",
      "Validation Batch 16, Loss: 1.203462, Accuracy: 52.54%\n",
      "Validation Batch 17, Loss: 1.280298, Accuracy: 51.93%\n",
      "Validation Batch 18, Loss: 1.177958, Accuracy: 51.91%\n",
      "Validation Batch 19, Loss: 1.256903, Accuracy: 51.56%\n",
      "Validation Batch 20, Loss: 1.194690, Accuracy: 51.64%\n",
      "Validation Batch 21, Loss: 1.204746, Accuracy: 51.71%\n",
      "Validation Batch 22, Loss: 1.242846, Accuracy: 51.42%\n",
      "Validation Batch 23, Loss: 1.270686, Accuracy: 51.02%\n",
      "Validation Batch 24, Loss: 1.201237, Accuracy: 51.17%\n",
      "Validation Batch 25, Loss: 1.180304, Accuracy: 51.38%\n",
      "Validation Batch 26, Loss: 1.140121, Accuracy: 51.62%\n",
      "Validation Batch 27, Loss: 1.229008, Accuracy: 51.50%\n",
      "Validation - Epoch 23, Loss: 1.206368, Accuracy: 51.50%\n",
      "Patienceâ€”0\n",
      "Epoch 24\n",
      "Batch 1, Loss: 1.095894, Accuracy: 67.19%\n",
      "Batch 2, Loss: 1.000769, Accuracy: 71.09%\n",
      "Batch 3, Loss: 0.920186, Accuracy: 75.52%\n",
      "Batch 4, Loss: 1.048414, Accuracy: 73.05%\n",
      "Batch 5, Loss: 1.148202, Accuracy: 70.00%\n",
      "Batch 6, Loss: 1.201816, Accuracy: 66.93%\n",
      "Batch 7, Loss: 1.012993, Accuracy: 67.86%\n",
      "Batch 8, Loss: 1.013500, Accuracy: 68.75%\n",
      "Batch 9, Loss: 1.087758, Accuracy: 68.40%\n",
      "Batch 10, Loss: 0.989228, Accuracy: 69.22%\n",
      "Batch 11, Loss: 1.100573, Accuracy: 68.75%\n",
      "Batch 12, Loss: 0.984136, Accuracy: 69.66%\n",
      "Batch 13, Loss: 1.079449, Accuracy: 69.11%\n",
      "Batch 14, Loss: 1.155961, Accuracy: 68.30%\n",
      "Batch 15, Loss: 1.098066, Accuracy: 68.02%\n",
      "Batch 16, Loss: 1.067571, Accuracy: 67.97%\n",
      "Batch 17, Loss: 1.039287, Accuracy: 68.11%\n",
      "Batch 18, Loss: 1.000534, Accuracy: 68.40%\n",
      "Batch 19, Loss: 0.967386, Accuracy: 68.83%\n",
      "Batch 20, Loss: 1.162389, Accuracy: 68.28%\n",
      "Batch 21, Loss: 0.981675, Accuracy: 68.75%\n",
      "Batch 22, Loss: 1.008289, Accuracy: 68.96%\n",
      "Batch 23, Loss: 1.147104, Accuracy: 68.41%\n",
      "Batch 24, Loss: 1.108001, Accuracy: 68.16%\n",
      "Batch 25, Loss: 1.061038, Accuracy: 68.12%\n",
      "Batch 26, Loss: 1.088969, Accuracy: 67.85%\n",
      "Batch 27, Loss: 1.112139, Accuracy: 67.65%\n",
      "Batch 28, Loss: 1.169951, Accuracy: 67.24%\n",
      "Batch 29, Loss: 1.104023, Accuracy: 67.13%\n",
      "Batch 30, Loss: 1.137860, Accuracy: 66.93%\n",
      "Batch 31, Loss: 1.134967, Accuracy: 66.78%\n",
      "Batch 32, Loss: 1.013496, Accuracy: 66.99%\n",
      "Batch 33, Loss: 1.089697, Accuracy: 66.90%\n",
      "Batch 34, Loss: 1.135173, Accuracy: 66.68%\n",
      "Batch 35, Loss: 1.095554, Accuracy: 66.70%\n",
      "Batch 36, Loss: 0.995973, Accuracy: 66.93%\n",
      "Batch 37, Loss: 1.078737, Accuracy: 66.93%\n",
      "Batch 38, Loss: 1.032940, Accuracy: 67.06%\n",
      "Batch 39, Loss: 1.132788, Accuracy: 66.91%\n",
      "Batch 40, Loss: 1.074540, Accuracy: 66.91%\n",
      "Batch 41, Loss: 1.100787, Accuracy: 66.84%\n",
      "Batch 42, Loss: 1.052485, Accuracy: 66.85%\n",
      "Batch 43, Loss: 1.026417, Accuracy: 67.01%\n",
      "Batch 44, Loss: 1.092496, Accuracy: 66.90%\n",
      "Batch 45, Loss: 1.065585, Accuracy: 66.91%\n",
      "Batch 46, Loss: 1.134905, Accuracy: 66.71%\n",
      "Batch 47, Loss: 0.996583, Accuracy: 66.89%\n",
      "Batch 48, Loss: 1.089578, Accuracy: 66.86%\n",
      "Batch 49, Loss: 1.041705, Accuracy: 66.90%\n",
      "Batch 50, Loss: 1.082838, Accuracy: 66.91%\n",
      "Batch 51, Loss: 1.051168, Accuracy: 66.97%\n",
      "Batch 52, Loss: 1.078092, Accuracy: 66.95%\n",
      "Batch 53, Loss: 1.060535, Accuracy: 66.95%\n",
      "Batch 54, Loss: 1.057811, Accuracy: 66.98%\n",
      "Batch 55, Loss: 1.154711, Accuracy: 66.85%\n",
      "Batch 56, Loss: 1.140268, Accuracy: 66.71%\n",
      "Batch 57, Loss: 1.045518, Accuracy: 66.80%\n",
      "Batch 58, Loss: 1.030158, Accuracy: 66.86%\n",
      "Batch 59, Loss: 1.057111, Accuracy: 66.92%\n",
      "Batch 60, Loss: 1.075217, Accuracy: 66.93%\n",
      "Batch 61, Loss: 1.008959, Accuracy: 67.01%\n",
      "Batch 62, Loss: 1.067536, Accuracy: 66.96%\n",
      "Batch 63, Loss: 1.100299, Accuracy: 66.89%\n",
      "Batch 64, Loss: 1.106350, Accuracy: 66.85%\n",
      "Batch 65, Loss: 1.074926, Accuracy: 66.85%\n",
      "Batch 66, Loss: 1.094477, Accuracy: 66.79%\n",
      "Batch 67, Loss: 1.062106, Accuracy: 66.81%\n",
      "Batch 68, Loss: 1.053777, Accuracy: 66.87%\n",
      "Batch 69, Loss: 1.111613, Accuracy: 66.76%\n",
      "Batch 70, Loss: 1.116516, Accuracy: 66.63%\n",
      "Batch 71, Loss: 1.067150, Accuracy: 66.62%\n",
      "Batch 72, Loss: 1.021523, Accuracy: 66.71%\n",
      "Batch 73, Loss: 0.993985, Accuracy: 66.82%\n",
      "Batch 74, Loss: 1.162842, Accuracy: 66.74%\n",
      "Batch 75, Loss: 1.042660, Accuracy: 66.79%\n",
      "Batch 76, Loss: 1.057132, Accuracy: 66.80%\n",
      "Batch 77, Loss: 1.021460, Accuracy: 66.88%\n",
      "Batch 78, Loss: 1.084897, Accuracy: 66.85%\n",
      "Batch 79, Loss: 0.984149, Accuracy: 66.95%\n",
      "Batch 80, Loss: 1.065025, Accuracy: 66.95%\n",
      "Batch 81, Loss: 1.061986, Accuracy: 66.98%\n",
      "Batch 82, Loss: 1.016340, Accuracy: 67.04%\n",
      "Batch 83, Loss: 1.051550, Accuracy: 67.02%\n",
      "Batch 84, Loss: 1.061271, Accuracy: 67.00%\n",
      "Batch 85, Loss: 0.971241, Accuracy: 67.15%\n",
      "Batch 86, Loss: 1.065210, Accuracy: 67.15%\n",
      "Batch 87, Loss: 1.065503, Accuracy: 67.15%\n",
      "Batch 88, Loss: 1.065422, Accuracy: 67.17%\n",
      "Batch 89, Loss: 1.112341, Accuracy: 67.12%\n",
      "Batch 90, Loss: 1.167018, Accuracy: 67.01%\n",
      "Batch 91, Loss: 1.072275, Accuracy: 67.03%\n",
      "Batch 92, Loss: 1.062629, Accuracy: 67.02%\n",
      "Batch 93, Loss: 1.039265, Accuracy: 67.02%\n",
      "Batch 94, Loss: 1.164315, Accuracy: 66.94%\n",
      "Batch 95, Loss: 1.086310, Accuracy: 66.94%\n",
      "Batch 96, Loss: 1.012713, Accuracy: 67.02%\n",
      "Batch 97, Loss: 1.089872, Accuracy: 67.01%\n",
      "Batch 98, Loss: 0.980330, Accuracy: 67.11%\n",
      "Batch 99, Loss: 1.049965, Accuracy: 67.11%\n",
      "Batch 100, Loss: 1.027575, Accuracy: 67.16%\n",
      "Batch 101, Loss: 1.011015, Accuracy: 67.23%\n",
      "Batch 102, Loss: 1.078117, Accuracy: 67.22%\n",
      "Batch 103, Loss: 1.073604, Accuracy: 67.20%\n",
      "Batch 104, Loss: 1.122435, Accuracy: 67.13%\n",
      "Batch 105, Loss: 1.101595, Accuracy: 67.10%\n",
      "Batch 106, Loss: 1.109689, Accuracy: 67.03%\n",
      "Batch 107, Loss: 1.016642, Accuracy: 67.10%\n",
      "Batch 108, Loss: 1.020915, Accuracy: 67.16%\n",
      "Batch 109, Loss: 1.030064, Accuracy: 67.19%\n",
      "Batch 110, Loss: 1.092970, Accuracy: 67.17%\n",
      "Batch 111, Loss: 1.165159, Accuracy: 67.07%\n",
      "Batch 112, Loss: 1.040062, Accuracy: 67.08%\n",
      "Batch 113, Loss: 1.052780, Accuracy: 67.09%\n",
      "Batch 114, Loss: 1.210408, Accuracy: 66.95%\n",
      "Batch 115, Loss: 1.092141, Accuracy: 66.96%\n",
      "Batch 116, Loss: 1.096517, Accuracy: 66.89%\n",
      "Batch 117, Loss: 0.961721, Accuracy: 67.00%\n",
      "Batch 118, Loss: 1.082047, Accuracy: 66.98%\n",
      "Batch 119, Loss: 1.104386, Accuracy: 66.95%\n",
      "Batch 120, Loss: 1.025565, Accuracy: 66.99%\n",
      "Batch 121, Loss: 1.045667, Accuracy: 67.01%\n",
      "Batch 122, Loss: 1.061121, Accuracy: 67.03%\n",
      "Batch 123, Loss: 1.089167, Accuracy: 67.01%\n",
      "Batch 124, Loss: 1.073725, Accuracy: 67.02%\n",
      "Batch 125, Loss: 1.090955, Accuracy: 67.01%\n",
      "Batch 126, Loss: 1.121830, Accuracy: 66.96%\n",
      "Batch 127, Loss: 1.063746, Accuracy: 66.95%\n",
      "Batch 128, Loss: 1.063095, Accuracy: 66.96%\n",
      "Batch 129, Loss: 1.053587, Accuracy: 66.98%\n",
      "Batch 130, Loss: 1.076815, Accuracy: 66.98%\n",
      "Batch 131, Loss: 1.181879, Accuracy: 66.88%\n",
      "Batch 132, Loss: 1.151096, Accuracy: 66.80%\n",
      "Batch 133, Loss: 1.046292, Accuracy: 66.82%\n",
      "Batch 134, Loss: 1.021614, Accuracy: 66.86%\n",
      "Batch 135, Loss: 1.017418, Accuracy: 66.90%\n",
      "Batch 136, Loss: 1.101468, Accuracy: 66.88%\n",
      "Batch 137, Loss: 1.165025, Accuracy: 66.80%\n",
      "Batch 138, Loss: 1.098597, Accuracy: 66.79%\n",
      "Batch 139, Loss: 1.097863, Accuracy: 66.79%\n",
      "Batch 140, Loss: 1.071821, Accuracy: 66.76%\n",
      "Batch 141, Loss: 1.119863, Accuracy: 66.73%\n",
      "Batch 142, Loss: 1.032986, Accuracy: 66.76%\n",
      "Batch 143, Loss: 1.106644, Accuracy: 66.72%\n",
      "Batch 144, Loss: 1.121722, Accuracy: 66.68%\n",
      "Batch 145, Loss: 1.143196, Accuracy: 66.63%\n",
      "Batch 146, Loss: 1.032693, Accuracy: 66.66%\n",
      "Batch 147, Loss: 1.094528, Accuracy: 66.62%\n",
      "Batch 148, Loss: 1.113096, Accuracy: 66.61%\n",
      "Batch 149, Loss: 1.028965, Accuracy: 66.63%\n",
      "Batch 150, Loss: 1.087285, Accuracy: 66.62%\n",
      "Batch 151, Loss: 1.106079, Accuracy: 66.59%\n",
      "Batch 152, Loss: 1.127125, Accuracy: 66.54%\n",
      "Batch 153, Loss: 1.102747, Accuracy: 66.52%\n",
      "Batch 154, Loss: 1.030881, Accuracy: 66.56%\n",
      "Batch 155, Loss: 1.081719, Accuracy: 66.56%\n",
      "Batch 156, Loss: 1.179401, Accuracy: 66.50%\n",
      "Batch 157, Loss: 0.993499, Accuracy: 66.55%\n",
      "Batch 158, Loss: 1.102602, Accuracy: 66.52%\n",
      "Batch 159, Loss: 1.090655, Accuracy: 66.51%\n",
      "Batch 160, Loss: 1.053096, Accuracy: 66.51%\n",
      "Batch 161, Loss: 1.052152, Accuracy: 66.54%\n",
      "Batch 162, Loss: 1.045831, Accuracy: 66.56%\n",
      "Batch 163, Loss: 1.136746, Accuracy: 66.51%\n",
      "Batch 164, Loss: 1.026466, Accuracy: 66.54%\n",
      "Batch 165, Loss: 1.055653, Accuracy: 66.56%\n",
      "Batch 166, Loss: 0.997084, Accuracy: 66.63%\n",
      "Batch 167, Loss: 1.026881, Accuracy: 66.64%\n",
      "Batch 168, Loss: 1.092711, Accuracy: 66.62%\n",
      "Batch 169, Loss: 1.179994, Accuracy: 66.55%\n",
      "Batch 170, Loss: 1.085225, Accuracy: 66.56%\n",
      "Batch 171, Loss: 1.151980, Accuracy: 66.51%\n",
      "Batch 172, Loss: 1.042423, Accuracy: 66.52%\n",
      "Batch 173, Loss: 1.025037, Accuracy: 66.56%\n",
      "Batch 174, Loss: 1.110063, Accuracy: 66.54%\n",
      "Batch 175, Loss: 1.121326, Accuracy: 66.50%\n",
      "Batch 176, Loss: 1.101408, Accuracy: 66.48%\n",
      "Batch 177, Loss: 1.034490, Accuracy: 66.50%\n",
      "Batch 178, Loss: 1.026102, Accuracy: 66.54%\n",
      "Batch 179, Loss: 1.012933, Accuracy: 66.58%\n",
      "Batch 180, Loss: 1.082284, Accuracy: 66.57%\n",
      "Batch 181, Loss: 1.112893, Accuracy: 66.56%\n",
      "Batch 182, Loss: 1.034356, Accuracy: 66.58%\n",
      "Batch 183, Loss: 1.038642, Accuracy: 66.61%\n",
      "Batch 184, Loss: 1.085751, Accuracy: 66.59%\n",
      "Batch 185, Loss: 1.029824, Accuracy: 66.61%\n",
      "Batch 186, Loss: 1.050136, Accuracy: 66.61%\n",
      "Batch 187, Loss: 1.094818, Accuracy: 66.59%\n",
      "Batch 188, Loss: 1.109993, Accuracy: 66.56%\n",
      "Batch 189, Loss: 1.061586, Accuracy: 66.58%\n",
      "Batch 190, Loss: 1.087279, Accuracy: 66.57%\n",
      "Batch 191, Loss: 1.122360, Accuracy: 66.56%\n",
      "Batch 192, Loss: 1.008743, Accuracy: 66.60%\n",
      "Batch 193, Loss: 1.103340, Accuracy: 66.57%\n",
      "Batch 194, Loss: 1.011408, Accuracy: 66.59%\n",
      "Batch 195, Loss: 1.086354, Accuracy: 66.58%\n",
      "Batch 196, Loss: 1.087887, Accuracy: 66.57%\n",
      "Batch 197, Loss: 1.065303, Accuracy: 66.56%\n",
      "Batch 198, Loss: 1.035402, Accuracy: 66.59%\n",
      "Batch 199, Loss: 1.018054, Accuracy: 66.61%\n",
      "Batch 200, Loss: 1.063696, Accuracy: 66.62%\n",
      "Batch 201, Loss: 1.056251, Accuracy: 66.64%\n",
      "Batch 202, Loss: 0.988127, Accuracy: 66.68%\n",
      "Batch 203, Loss: 1.044039, Accuracy: 66.70%\n",
      "Batch 204, Loss: 1.121635, Accuracy: 66.68%\n",
      "Batch 205, Loss: 1.024327, Accuracy: 66.71%\n",
      "Batch 206, Loss: 1.052739, Accuracy: 66.72%\n",
      "Batch 207, Loss: 1.068639, Accuracy: 66.73%\n",
      "Batch 208, Loss: 0.992792, Accuracy: 66.77%\n",
      "Batch 209, Loss: 1.071389, Accuracy: 66.78%\n",
      "Batch 210, Loss: 1.105840, Accuracy: 66.76%\n",
      "Batch 211, Loss: 1.165458, Accuracy: 66.71%\n",
      "Batch 212, Loss: 1.117994, Accuracy: 66.70%\n",
      "Batch 213, Loss: 1.020734, Accuracy: 66.72%\n",
      "Training - Epoch 24, Loss: 1.071857, Accuracy: 66.72%\n",
      "Validation Batch 1, Loss: 1.107117, Accuracy: 59.38%\n",
      "Validation Batch 2, Loss: 1.222214, Accuracy: 53.12%\n",
      "Validation Batch 3, Loss: 1.254668, Accuracy: 51.56%\n",
      "Validation Batch 4, Loss: 1.106732, Accuracy: 54.30%\n",
      "Validation Batch 5, Loss: 1.199737, Accuracy: 53.75%\n",
      "Validation Batch 6, Loss: 1.162350, Accuracy: 54.69%\n",
      "Validation Batch 7, Loss: 1.165153, Accuracy: 55.13%\n",
      "Validation Batch 8, Loss: 1.185213, Accuracy: 55.08%\n",
      "Validation Batch 9, Loss: 1.226392, Accuracy: 54.69%\n",
      "Validation Batch 10, Loss: 1.182141, Accuracy: 54.84%\n",
      "Validation Batch 11, Loss: 1.142010, Accuracy: 55.11%\n",
      "Validation Batch 12, Loss: 1.117403, Accuracy: 55.73%\n",
      "Validation Batch 13, Loss: 1.238293, Accuracy: 55.05%\n",
      "Validation Batch 14, Loss: 1.140004, Accuracy: 55.47%\n",
      "Validation Batch 15, Loss: 1.184304, Accuracy: 55.21%\n",
      "Validation Batch 16, Loss: 1.170827, Accuracy: 55.18%\n",
      "Validation Batch 17, Loss: 1.258844, Accuracy: 54.60%\n",
      "Validation Batch 18, Loss: 1.150314, Accuracy: 54.69%\n",
      "Validation Batch 19, Loss: 1.232890, Accuracy: 54.28%\n",
      "Validation Batch 20, Loss: 1.174261, Accuracy: 54.53%\n",
      "Validation Batch 21, Loss: 1.175590, Accuracy: 54.61%\n",
      "Validation Batch 22, Loss: 1.221653, Accuracy: 54.26%\n",
      "Validation Batch 23, Loss: 1.252383, Accuracy: 53.94%\n",
      "Validation Batch 24, Loss: 1.175542, Accuracy: 54.17%\n",
      "Validation Batch 25, Loss: 1.157822, Accuracy: 54.31%\n",
      "Validation Batch 26, Loss: 1.121308, Accuracy: 54.51%\n",
      "Validation Batch 27, Loss: 1.201283, Accuracy: 54.43%\n",
      "Validation - Epoch 24, Loss: 1.182461, Accuracy: 54.43%\n",
      "Patienceâ€”0\n",
      "Epoch 25\n",
      "Batch 1, Loss: 1.130681, Accuracy: 59.38%\n",
      "Batch 2, Loss: 1.104116, Accuracy: 60.94%\n",
      "Batch 3, Loss: 1.125208, Accuracy: 60.94%\n",
      "Batch 4, Loss: 1.038210, Accuracy: 63.28%\n",
      "Batch 5, Loss: 1.073181, Accuracy: 64.38%\n",
      "Batch 6, Loss: 1.028374, Accuracy: 65.36%\n",
      "Batch 7, Loss: 1.147577, Accuracy: 64.29%\n",
      "Batch 8, Loss: 1.075818, Accuracy: 64.84%\n",
      "Batch 9, Loss: 1.109163, Accuracy: 64.41%\n",
      "Batch 10, Loss: 1.131666, Accuracy: 64.06%\n",
      "Batch 11, Loss: 1.147048, Accuracy: 63.49%\n",
      "Batch 12, Loss: 1.056444, Accuracy: 63.93%\n",
      "Batch 13, Loss: 0.994590, Accuracy: 64.90%\n",
      "Batch 14, Loss: 0.999292, Accuracy: 65.74%\n",
      "Batch 15, Loss: 1.109958, Accuracy: 65.31%\n",
      "Batch 16, Loss: 1.041960, Accuracy: 65.33%\n",
      "Batch 17, Loss: 1.052842, Accuracy: 65.53%\n",
      "Batch 18, Loss: 1.075426, Accuracy: 65.54%\n",
      "Batch 19, Loss: 1.074000, Accuracy: 65.54%\n",
      "Batch 20, Loss: 1.088321, Accuracy: 65.39%\n",
      "Batch 21, Loss: 0.918118, Accuracy: 66.15%\n",
      "Batch 22, Loss: 1.020122, Accuracy: 66.41%\n",
      "Batch 23, Loss: 1.167028, Accuracy: 66.10%\n",
      "Batch 24, Loss: 1.120712, Accuracy: 65.89%\n",
      "Batch 25, Loss: 1.094810, Accuracy: 65.81%\n",
      "Batch 26, Loss: 0.984869, Accuracy: 66.35%\n",
      "Batch 27, Loss: 1.111516, Accuracy: 66.20%\n",
      "Batch 28, Loss: 1.069846, Accuracy: 66.24%\n",
      "Batch 29, Loss: 1.016413, Accuracy: 66.54%\n",
      "Batch 30, Loss: 1.067215, Accuracy: 66.67%\n",
      "Batch 31, Loss: 1.130388, Accuracy: 66.43%\n",
      "Batch 32, Loss: 0.968548, Accuracy: 66.89%\n",
      "Batch 33, Loss: 1.047354, Accuracy: 66.95%\n",
      "Batch 34, Loss: 1.079711, Accuracy: 66.96%\n",
      "Batch 35, Loss: 1.097333, Accuracy: 66.92%\n",
      "Batch 36, Loss: 1.096876, Accuracy: 66.80%\n",
      "Batch 37, Loss: 1.062565, Accuracy: 66.77%\n",
      "Batch 38, Loss: 0.950697, Accuracy: 67.15%\n",
      "Batch 39, Loss: 1.022915, Accuracy: 67.31%\n",
      "Batch 40, Loss: 1.111897, Accuracy: 67.19%\n",
      "Batch 41, Loss: 1.050757, Accuracy: 67.26%\n",
      "Batch 42, Loss: 1.122297, Accuracy: 67.15%\n",
      "Batch 43, Loss: 1.152306, Accuracy: 66.97%\n",
      "Batch 44, Loss: 1.167754, Accuracy: 66.69%\n",
      "Batch 45, Loss: 0.960384, Accuracy: 66.94%\n",
      "Batch 46, Loss: 1.067690, Accuracy: 66.98%\n",
      "Batch 47, Loss: 1.124874, Accuracy: 66.82%\n",
      "Batch 48, Loss: 1.122079, Accuracy: 66.70%\n",
      "Batch 49, Loss: 1.090039, Accuracy: 66.68%\n",
      "Batch 50, Loss: 1.076000, Accuracy: 66.66%\n",
      "Batch 51, Loss: 1.077405, Accuracy: 66.61%\n",
      "Batch 52, Loss: 1.003282, Accuracy: 66.80%\n",
      "Batch 53, Loss: 1.077556, Accuracy: 66.77%\n",
      "Batch 54, Loss: 1.078924, Accuracy: 66.78%\n",
      "Batch 55, Loss: 1.116080, Accuracy: 66.68%\n",
      "Batch 56, Loss: 1.114973, Accuracy: 66.60%\n",
      "Batch 57, Loss: 1.033457, Accuracy: 66.64%\n",
      "Batch 58, Loss: 1.108867, Accuracy: 66.54%\n",
      "Batch 59, Loss: 1.016904, Accuracy: 66.60%\n",
      "Batch 60, Loss: 1.090849, Accuracy: 66.54%\n",
      "Batch 61, Loss: 1.034919, Accuracy: 66.57%\n",
      "Batch 62, Loss: 1.116768, Accuracy: 66.53%\n",
      "Batch 63, Loss: 1.045912, Accuracy: 66.59%\n",
      "Batch 64, Loss: 1.035675, Accuracy: 66.67%\n",
      "Batch 65, Loss: 1.128611, Accuracy: 66.59%\n",
      "Batch 66, Loss: 0.988895, Accuracy: 66.74%\n",
      "Batch 67, Loss: 1.079865, Accuracy: 66.72%\n",
      "Batch 68, Loss: 1.040798, Accuracy: 66.77%\n",
      "Batch 69, Loss: 0.918891, Accuracy: 67.01%\n",
      "Batch 70, Loss: 1.104242, Accuracy: 66.96%\n",
      "Batch 71, Loss: 1.019738, Accuracy: 66.99%\n",
      "Batch 72, Loss: 1.096896, Accuracy: 66.97%\n",
      "Batch 73, Loss: 1.078959, Accuracy: 66.95%\n",
      "Batch 74, Loss: 1.049470, Accuracy: 66.96%\n",
      "Batch 75, Loss: 0.998521, Accuracy: 67.06%\n",
      "Batch 76, Loss: 1.157662, Accuracy: 66.98%\n",
      "Batch 77, Loss: 1.053986, Accuracy: 67.00%\n",
      "Batch 78, Loss: 1.144355, Accuracy: 66.91%\n",
      "Batch 79, Loss: 1.038415, Accuracy: 66.97%\n",
      "Batch 80, Loss: 1.117612, Accuracy: 66.91%\n",
      "Batch 81, Loss: 1.087429, Accuracy: 66.90%\n",
      "Batch 82, Loss: 1.174859, Accuracy: 66.77%\n",
      "Batch 83, Loss: 1.118044, Accuracy: 66.68%\n",
      "Batch 84, Loss: 1.148417, Accuracy: 66.59%\n",
      "Batch 85, Loss: 0.983296, Accuracy: 66.71%\n",
      "Batch 86, Loss: 1.067815, Accuracy: 66.72%\n",
      "Batch 87, Loss: 1.073654, Accuracy: 66.70%\n",
      "Batch 88, Loss: 1.074586, Accuracy: 66.69%\n",
      "Batch 89, Loss: 0.922294, Accuracy: 66.91%\n",
      "Batch 90, Loss: 0.956651, Accuracy: 67.01%\n",
      "Batch 91, Loss: 1.091514, Accuracy: 67.02%\n",
      "Batch 92, Loss: 1.145367, Accuracy: 66.93%\n",
      "Batch 93, Loss: 1.037362, Accuracy: 66.97%\n",
      "Batch 94, Loss: 1.123474, Accuracy: 66.90%\n",
      "Batch 95, Loss: 1.049423, Accuracy: 66.91%\n",
      "Batch 96, Loss: 1.082975, Accuracy: 66.88%\n",
      "Batch 97, Loss: 1.162835, Accuracy: 66.78%\n",
      "Batch 98, Loss: 1.040372, Accuracy: 66.80%\n",
      "Batch 99, Loss: 1.090306, Accuracy: 66.78%\n",
      "Batch 100, Loss: 1.043908, Accuracy: 66.81%\n",
      "Batch 101, Loss: 1.071594, Accuracy: 66.80%\n",
      "Batch 102, Loss: 1.136906, Accuracy: 66.71%\n",
      "Batch 103, Loss: 1.088988, Accuracy: 66.70%\n",
      "Batch 104, Loss: 1.071979, Accuracy: 66.72%\n",
      "Batch 105, Loss: 1.034615, Accuracy: 66.77%\n",
      "Batch 106, Loss: 1.092264, Accuracy: 66.77%\n",
      "Batch 107, Loss: 1.071200, Accuracy: 66.76%\n",
      "Batch 108, Loss: 0.984566, Accuracy: 66.85%\n",
      "Batch 109, Loss: 1.048849, Accuracy: 66.89%\n",
      "Batch 110, Loss: 1.097826, Accuracy: 66.86%\n",
      "Batch 111, Loss: 1.093135, Accuracy: 66.81%\n",
      "Batch 112, Loss: 1.255072, Accuracy: 66.63%\n",
      "Batch 113, Loss: 1.068035, Accuracy: 66.65%\n",
      "Batch 114, Loss: 1.076678, Accuracy: 66.65%\n",
      "Batch 115, Loss: 1.084178, Accuracy: 66.64%\n",
      "Batch 116, Loss: 1.025903, Accuracy: 66.68%\n",
      "Batch 117, Loss: 1.051036, Accuracy: 66.69%\n",
      "Batch 118, Loss: 1.050561, Accuracy: 66.71%\n",
      "Batch 119, Loss: 1.066445, Accuracy: 66.70%\n",
      "Batch 120, Loss: 1.149888, Accuracy: 66.61%\n",
      "Batch 121, Loss: 1.088601, Accuracy: 66.62%\n",
      "Batch 122, Loss: 0.947586, Accuracy: 66.74%\n",
      "Batch 123, Loss: 1.095882, Accuracy: 66.73%\n",
      "Batch 124, Loss: 0.977723, Accuracy: 66.80%\n",
      "Batch 125, Loss: 1.060352, Accuracy: 66.80%\n",
      "Batch 126, Loss: 1.133487, Accuracy: 66.77%\n",
      "Batch 127, Loss: 1.050022, Accuracy: 66.78%\n",
      "Batch 128, Loss: 1.037944, Accuracy: 66.82%\n",
      "Batch 129, Loss: 1.030916, Accuracy: 66.82%\n",
      "Batch 130, Loss: 1.074714, Accuracy: 66.85%\n",
      "Batch 131, Loss: 1.035869, Accuracy: 66.88%\n",
      "Batch 132, Loss: 1.116238, Accuracy: 66.83%\n",
      "Batch 133, Loss: 1.016493, Accuracy: 66.89%\n",
      "Batch 134, Loss: 0.988811, Accuracy: 66.98%\n",
      "Batch 135, Loss: 1.095416, Accuracy: 66.96%\n",
      "Batch 136, Loss: 1.009773, Accuracy: 66.99%\n",
      "Batch 137, Loss: 1.086031, Accuracy: 66.98%\n",
      "Batch 138, Loss: 1.055909, Accuracy: 67.00%\n",
      "Batch 139, Loss: 1.012472, Accuracy: 67.03%\n",
      "Batch 140, Loss: 1.079886, Accuracy: 67.02%\n",
      "Batch 141, Loss: 1.111669, Accuracy: 66.97%\n",
      "Batch 142, Loss: 1.044284, Accuracy: 66.99%\n",
      "Batch 143, Loss: 1.114098, Accuracy: 66.96%\n",
      "Batch 144, Loss: 1.073315, Accuracy: 66.95%\n",
      "Batch 145, Loss: 1.047757, Accuracy: 66.95%\n",
      "Batch 146, Loss: 1.061316, Accuracy: 66.95%\n",
      "Batch 147, Loss: 1.035980, Accuracy: 66.96%\n",
      "Batch 148, Loss: 1.114799, Accuracy: 66.92%\n",
      "Batch 149, Loss: 1.049976, Accuracy: 66.94%\n",
      "Batch 150, Loss: 1.062549, Accuracy: 66.95%\n",
      "Batch 151, Loss: 1.071844, Accuracy: 66.95%\n",
      "Batch 152, Loss: 1.192962, Accuracy: 66.85%\n",
      "Batch 153, Loss: 1.088197, Accuracy: 66.81%\n",
      "Batch 154, Loss: 1.091854, Accuracy: 66.79%\n",
      "Batch 155, Loss: 1.062710, Accuracy: 66.80%\n",
      "Batch 156, Loss: 1.031462, Accuracy: 66.84%\n",
      "Batch 157, Loss: 1.040692, Accuracy: 66.86%\n",
      "Batch 158, Loss: 0.988947, Accuracy: 66.91%\n",
      "Batch 159, Loss: 1.054850, Accuracy: 66.92%\n",
      "Batch 160, Loss: 1.062592, Accuracy: 66.91%\n",
      "Batch 161, Loss: 1.025453, Accuracy: 66.94%\n",
      "Batch 162, Loss: 1.084822, Accuracy: 66.94%\n",
      "Batch 163, Loss: 1.134243, Accuracy: 66.89%\n",
      "Batch 164, Loss: 1.044093, Accuracy: 66.92%\n",
      "Batch 165, Loss: 1.151586, Accuracy: 66.88%\n",
      "Batch 166, Loss: 1.050936, Accuracy: 66.88%\n",
      "Batch 167, Loss: 1.066406, Accuracy: 66.85%\n",
      "Batch 168, Loss: 1.107911, Accuracy: 66.82%\n",
      "Batch 169, Loss: 1.050709, Accuracy: 66.84%\n",
      "Batch 170, Loss: 1.125465, Accuracy: 66.82%\n",
      "Batch 171, Loss: 0.998000, Accuracy: 66.86%\n",
      "Batch 172, Loss: 1.040446, Accuracy: 66.88%\n",
      "Batch 173, Loss: 1.125241, Accuracy: 66.83%\n",
      "Batch 174, Loss: 1.020886, Accuracy: 66.86%\n",
      "Batch 175, Loss: 1.097757, Accuracy: 66.84%\n",
      "Batch 176, Loss: 1.092456, Accuracy: 66.83%\n",
      "Batch 177, Loss: 1.030521, Accuracy: 66.84%\n",
      "Batch 178, Loss: 1.054025, Accuracy: 66.86%\n",
      "Batch 179, Loss: 1.084603, Accuracy: 66.85%\n",
      "Batch 180, Loss: 1.056781, Accuracy: 66.84%\n",
      "Batch 181, Loss: 1.083608, Accuracy: 66.84%\n",
      "Batch 182, Loss: 1.070360, Accuracy: 66.85%\n",
      "Batch 183, Loss: 0.948741, Accuracy: 66.91%\n",
      "Batch 184, Loss: 1.113680, Accuracy: 66.87%\n",
      "Batch 185, Loss: 1.055218, Accuracy: 66.88%\n",
      "Batch 186, Loss: 1.100247, Accuracy: 66.87%\n",
      "Batch 187, Loss: 1.045716, Accuracy: 66.90%\n",
      "Batch 188, Loss: 1.236086, Accuracy: 66.81%\n",
      "Batch 189, Loss: 1.120284, Accuracy: 66.78%\n",
      "Batch 190, Loss: 1.082047, Accuracy: 66.78%\n",
      "Batch 191, Loss: 1.042066, Accuracy: 66.80%\n",
      "Batch 192, Loss: 1.125787, Accuracy: 66.77%\n",
      "Batch 193, Loss: 1.078617, Accuracy: 66.77%\n",
      "Batch 194, Loss: 1.024744, Accuracy: 66.81%\n",
      "Batch 195, Loss: 1.059845, Accuracy: 66.81%\n",
      "Batch 196, Loss: 0.980949, Accuracy: 66.86%\n",
      "Batch 197, Loss: 1.030332, Accuracy: 66.88%\n",
      "Batch 198, Loss: 1.085024, Accuracy: 66.86%\n",
      "Batch 199, Loss: 0.948234, Accuracy: 66.94%\n",
      "Batch 200, Loss: 1.132163, Accuracy: 66.91%\n",
      "Batch 201, Loss: 1.037249, Accuracy: 66.92%\n",
      "Batch 202, Loss: 1.108644, Accuracy: 66.91%\n",
      "Batch 203, Loss: 1.065858, Accuracy: 66.91%\n",
      "Batch 204, Loss: 1.077189, Accuracy: 66.91%\n",
      "Batch 205, Loss: 1.058161, Accuracy: 66.92%\n",
      "Batch 206, Loss: 0.993149, Accuracy: 66.94%\n",
      "Batch 207, Loss: 1.033980, Accuracy: 66.96%\n",
      "Batch 208, Loss: 1.051583, Accuracy: 66.97%\n",
      "Batch 209, Loss: 1.068111, Accuracy: 66.98%\n",
      "Batch 210, Loss: 1.046754, Accuracy: 66.99%\n",
      "Batch 211, Loss: 1.109786, Accuracy: 66.98%\n",
      "Batch 212, Loss: 1.083919, Accuracy: 66.99%\n",
      "Batch 213, Loss: 0.996244, Accuracy: 67.02%\n",
      "Training - Epoch 25, Loss: 1.068885, Accuracy: 67.02%\n",
      "Validation Batch 1, Loss: 1.109149, Accuracy: 59.38%\n",
      "Validation Batch 2, Loss: 1.223109, Accuracy: 53.12%\n",
      "Validation Batch 3, Loss: 1.259604, Accuracy: 51.04%\n",
      "Validation Batch 4, Loss: 1.107108, Accuracy: 54.30%\n",
      "Validation Batch 5, Loss: 1.200444, Accuracy: 53.75%\n",
      "Validation Batch 6, Loss: 1.166645, Accuracy: 54.43%\n",
      "Validation Batch 7, Loss: 1.170092, Accuracy: 55.13%\n",
      "Validation Batch 8, Loss: 1.185689, Accuracy: 55.08%\n",
      "Validation Batch 9, Loss: 1.229897, Accuracy: 54.51%\n",
      "Validation Batch 10, Loss: 1.187034, Accuracy: 54.53%\n",
      "Validation Batch 11, Loss: 1.145601, Accuracy: 54.69%\n",
      "Validation Batch 12, Loss: 1.119597, Accuracy: 55.34%\n",
      "Validation Batch 13, Loss: 1.240370, Accuracy: 54.69%\n",
      "Validation Batch 14, Loss: 1.141569, Accuracy: 55.02%\n",
      "Validation Batch 15, Loss: 1.189177, Accuracy: 54.79%\n",
      "Validation Batch 16, Loss: 1.171229, Accuracy: 54.79%\n",
      "Validation Batch 17, Loss: 1.264836, Accuracy: 54.23%\n",
      "Validation Batch 18, Loss: 1.150001, Accuracy: 54.43%\n",
      "Validation Batch 19, Loss: 1.236732, Accuracy: 54.03%\n",
      "Validation Batch 20, Loss: 1.179039, Accuracy: 54.22%\n",
      "Validation Batch 21, Loss: 1.181574, Accuracy: 54.32%\n",
      "Validation Batch 22, Loss: 1.222183, Accuracy: 54.05%\n",
      "Validation Batch 23, Loss: 1.251812, Accuracy: 53.74%\n",
      "Validation Batch 24, Loss: 1.176657, Accuracy: 53.97%\n",
      "Validation Batch 25, Loss: 1.158566, Accuracy: 54.12%\n",
      "Validation Batch 26, Loss: 1.127166, Accuracy: 54.27%\n",
      "Validation Batch 27, Loss: 1.202997, Accuracy: 54.26%\n",
      "Validation - Epoch 25, Loss: 1.185107, Accuracy: 54.26%\n",
      "Patienceâ€”1\n",
      "Epoch 26\n",
      "Batch 1, Loss: 1.013587, Accuracy: 75.00%\n",
      "Batch 2, Loss: 0.996090, Accuracy: 75.78%\n",
      "Batch 3, Loss: 1.015217, Accuracy: 75.00%\n",
      "Batch 4, Loss: 1.062521, Accuracy: 72.66%\n",
      "Batch 5, Loss: 1.033590, Accuracy: 72.19%\n",
      "Batch 6, Loss: 1.107674, Accuracy: 70.57%\n",
      "Batch 7, Loss: 1.121680, Accuracy: 69.64%\n",
      "Batch 8, Loss: 1.094878, Accuracy: 69.14%\n",
      "Batch 9, Loss: 1.021858, Accuracy: 69.27%\n",
      "Batch 10, Loss: 1.094990, Accuracy: 68.59%\n",
      "Batch 11, Loss: 1.053433, Accuracy: 68.47%\n",
      "Batch 12, Loss: 0.971660, Accuracy: 69.27%\n",
      "Batch 13, Loss: 1.068940, Accuracy: 69.35%\n",
      "Batch 14, Loss: 0.997569, Accuracy: 69.64%\n",
      "Batch 15, Loss: 1.040214, Accuracy: 69.90%\n",
      "Batch 16, Loss: 1.086414, Accuracy: 69.53%\n",
      "Batch 17, Loss: 1.008677, Accuracy: 69.67%\n",
      "Batch 18, Loss: 1.043337, Accuracy: 69.79%\n",
      "Batch 19, Loss: 1.129469, Accuracy: 69.24%\n",
      "Batch 20, Loss: 1.071720, Accuracy: 69.06%\n",
      "Batch 21, Loss: 1.048755, Accuracy: 69.05%\n",
      "Batch 22, Loss: 1.032800, Accuracy: 69.18%\n",
      "Batch 23, Loss: 1.179271, Accuracy: 68.55%\n",
      "Batch 24, Loss: 1.111820, Accuracy: 68.29%\n",
      "Batch 25, Loss: 1.131986, Accuracy: 67.94%\n",
      "Batch 26, Loss: 1.073745, Accuracy: 67.79%\n",
      "Batch 27, Loss: 1.046515, Accuracy: 67.88%\n",
      "Batch 28, Loss: 0.978132, Accuracy: 68.30%\n",
      "Batch 29, Loss: 1.169739, Accuracy: 67.89%\n",
      "Batch 30, Loss: 1.125201, Accuracy: 67.71%\n",
      "Batch 31, Loss: 1.086860, Accuracy: 67.69%\n",
      "Batch 32, Loss: 1.011530, Accuracy: 67.92%\n",
      "Batch 33, Loss: 1.020383, Accuracy: 68.04%\n",
      "Batch 34, Loss: 0.995799, Accuracy: 68.24%\n",
      "Batch 35, Loss: 1.006783, Accuracy: 68.53%\n",
      "Batch 36, Loss: 0.959441, Accuracy: 68.88%\n",
      "Batch 37, Loss: 0.991518, Accuracy: 69.09%\n",
      "Batch 38, Loss: 0.976153, Accuracy: 69.24%\n",
      "Batch 39, Loss: 1.081855, Accuracy: 69.15%\n",
      "Batch 40, Loss: 1.115432, Accuracy: 68.98%\n",
      "Batch 41, Loss: 1.124932, Accuracy: 68.79%\n",
      "Batch 42, Loss: 1.070739, Accuracy: 68.75%\n",
      "Batch 43, Loss: 1.085229, Accuracy: 68.60%\n",
      "Batch 44, Loss: 1.066795, Accuracy: 68.61%\n",
      "Batch 45, Loss: 1.059509, Accuracy: 68.58%\n",
      "Batch 46, Loss: 1.177706, Accuracy: 68.31%\n",
      "Batch 47, Loss: 0.991633, Accuracy: 68.45%\n",
      "Batch 48, Loss: 1.011500, Accuracy: 68.55%\n",
      "Batch 49, Loss: 1.117306, Accuracy: 68.40%\n",
      "Batch 50, Loss: 1.001079, Accuracy: 68.53%\n",
      "Batch 51, Loss: 1.021521, Accuracy: 68.60%\n",
      "Batch 52, Loss: 1.010769, Accuracy: 68.66%\n",
      "Batch 53, Loss: 1.107788, Accuracy: 68.57%\n",
      "Batch 54, Loss: 1.098975, Accuracy: 68.49%\n",
      "Batch 55, Loss: 1.054903, Accuracy: 68.49%\n",
      "Batch 56, Loss: 1.072092, Accuracy: 68.39%\n",
      "Batch 57, Loss: 1.043326, Accuracy: 68.39%\n",
      "Batch 58, Loss: 1.036855, Accuracy: 68.45%\n",
      "Batch 59, Loss: 1.019066, Accuracy: 68.51%\n",
      "Batch 60, Loss: 1.092803, Accuracy: 68.41%\n",
      "Batch 61, Loss: 1.057831, Accuracy: 68.42%\n",
      "Batch 62, Loss: 1.051649, Accuracy: 68.45%\n",
      "Batch 63, Loss: 1.124824, Accuracy: 68.30%\n",
      "Batch 64, Loss: 1.084948, Accuracy: 68.26%\n",
      "Batch 65, Loss: 0.959599, Accuracy: 68.44%\n",
      "Batch 66, Loss: 1.125993, Accuracy: 68.35%\n",
      "Batch 67, Loss: 1.029616, Accuracy: 68.38%\n",
      "Batch 68, Loss: 1.023491, Accuracy: 68.47%\n",
      "Batch 69, Loss: 1.113058, Accuracy: 68.39%\n",
      "Batch 70, Loss: 0.982414, Accuracy: 68.55%\n",
      "Batch 71, Loss: 1.109152, Accuracy: 68.46%\n",
      "Batch 72, Loss: 1.002907, Accuracy: 68.58%\n",
      "Batch 73, Loss: 1.080851, Accuracy: 68.49%\n",
      "Batch 74, Loss: 1.159021, Accuracy: 68.35%\n",
      "Batch 75, Loss: 1.066340, Accuracy: 68.33%\n",
      "Batch 76, Loss: 1.081392, Accuracy: 68.26%\n",
      "Batch 77, Loss: 1.151756, Accuracy: 68.12%\n",
      "Batch 78, Loss: 1.131883, Accuracy: 68.01%\n",
      "Batch 79, Loss: 1.014081, Accuracy: 68.08%\n",
      "Batch 80, Loss: 1.010504, Accuracy: 68.16%\n",
      "Batch 81, Loss: 1.100871, Accuracy: 68.17%\n",
      "Batch 82, Loss: 1.123754, Accuracy: 68.08%\n",
      "Batch 83, Loss: 1.044248, Accuracy: 68.15%\n",
      "Batch 84, Loss: 1.051727, Accuracy: 68.15%\n",
      "Batch 85, Loss: 1.101892, Accuracy: 68.11%\n",
      "Batch 86, Loss: 1.062423, Accuracy: 68.13%\n",
      "Batch 87, Loss: 1.035816, Accuracy: 68.16%\n",
      "Batch 88, Loss: 1.113521, Accuracy: 68.08%\n",
      "Batch 89, Loss: 0.959420, Accuracy: 68.22%\n",
      "Batch 90, Loss: 1.110000, Accuracy: 68.14%\n",
      "Batch 91, Loss: 1.029838, Accuracy: 68.17%\n",
      "Batch 92, Loss: 0.986996, Accuracy: 68.24%\n",
      "Batch 93, Loss: 1.124401, Accuracy: 68.13%\n",
      "Batch 94, Loss: 1.020606, Accuracy: 68.17%\n",
      "Batch 95, Loss: 1.074443, Accuracy: 68.16%\n",
      "Batch 96, Loss: 1.103410, Accuracy: 68.10%\n",
      "Batch 97, Loss: 1.056568, Accuracy: 68.12%\n",
      "Batch 98, Loss: 1.043358, Accuracy: 68.16%\n",
      "Batch 99, Loss: 1.114286, Accuracy: 68.07%\n",
      "Batch 100, Loss: 1.120630, Accuracy: 67.98%\n",
      "Batch 101, Loss: 1.037052, Accuracy: 68.02%\n",
      "Batch 102, Loss: 1.108368, Accuracy: 67.97%\n",
      "Batch 103, Loss: 1.062250, Accuracy: 67.98%\n",
      "Batch 104, Loss: 1.107054, Accuracy: 67.89%\n",
      "Batch 105, Loss: 1.089459, Accuracy: 67.87%\n",
      "Batch 106, Loss: 1.031066, Accuracy: 67.91%\n",
      "Batch 107, Loss: 1.137074, Accuracy: 67.82%\n",
      "Batch 108, Loss: 1.024474, Accuracy: 67.85%\n",
      "Batch 109, Loss: 1.021878, Accuracy: 67.89%\n",
      "Batch 110, Loss: 1.037194, Accuracy: 67.93%\n",
      "Batch 111, Loss: 0.996893, Accuracy: 67.98%\n",
      "Batch 112, Loss: 1.094820, Accuracy: 67.95%\n",
      "Batch 113, Loss: 1.102296, Accuracy: 67.93%\n",
      "Batch 114, Loss: 1.039789, Accuracy: 67.94%\n",
      "Batch 115, Loss: 1.069651, Accuracy: 67.95%\n",
      "Batch 116, Loss: 1.077379, Accuracy: 67.94%\n",
      "Batch 117, Loss: 1.153458, Accuracy: 67.88%\n",
      "Batch 118, Loss: 0.998219, Accuracy: 67.96%\n",
      "Batch 119, Loss: 1.001175, Accuracy: 68.00%\n",
      "Batch 120, Loss: 1.077858, Accuracy: 67.98%\n",
      "Batch 121, Loss: 1.048442, Accuracy: 68.00%\n",
      "Batch 122, Loss: 0.971129, Accuracy: 68.08%\n",
      "Batch 123, Loss: 1.127380, Accuracy: 68.01%\n",
      "Batch 124, Loss: 1.088501, Accuracy: 67.97%\n",
      "Batch 125, Loss: 1.104006, Accuracy: 67.91%\n",
      "Batch 126, Loss: 1.049304, Accuracy: 67.92%\n",
      "Batch 127, Loss: 1.029646, Accuracy: 67.96%\n",
      "Batch 128, Loss: 1.003619, Accuracy: 68.03%\n",
      "Batch 129, Loss: 1.052011, Accuracy: 68.05%\n",
      "Batch 130, Loss: 1.040443, Accuracy: 68.06%\n",
      "Batch 131, Loss: 0.991136, Accuracy: 68.12%\n",
      "Batch 132, Loss: 1.031569, Accuracy: 68.13%\n",
      "Batch 133, Loss: 1.154064, Accuracy: 68.06%\n",
      "Batch 134, Loss: 1.172281, Accuracy: 67.98%\n",
      "Batch 135, Loss: 1.106594, Accuracy: 67.95%\n",
      "Batch 136, Loss: 1.030018, Accuracy: 67.97%\n",
      "Batch 137, Loss: 1.122635, Accuracy: 67.94%\n",
      "Batch 138, Loss: 1.105162, Accuracy: 67.89%\n",
      "Batch 139, Loss: 1.077251, Accuracy: 67.88%\n",
      "Batch 140, Loss: 1.072959, Accuracy: 67.89%\n",
      "Batch 141, Loss: 1.064510, Accuracy: 67.89%\n",
      "Batch 142, Loss: 1.100904, Accuracy: 67.85%\n",
      "Batch 143, Loss: 1.119452, Accuracy: 67.81%\n",
      "Batch 144, Loss: 1.139714, Accuracy: 67.75%\n",
      "Batch 145, Loss: 1.045622, Accuracy: 67.77%\n",
      "Batch 146, Loss: 1.070179, Accuracy: 67.75%\n",
      "Batch 147, Loss: 1.084737, Accuracy: 67.74%\n",
      "Batch 148, Loss: 1.087366, Accuracy: 67.72%\n",
      "Batch 149, Loss: 1.053368, Accuracy: 67.71%\n",
      "Batch 150, Loss: 1.001961, Accuracy: 67.78%\n",
      "Batch 151, Loss: 1.025447, Accuracy: 67.82%\n",
      "Batch 152, Loss: 1.055030, Accuracy: 67.81%\n",
      "Batch 153, Loss: 1.090542, Accuracy: 67.81%\n",
      "Batch 154, Loss: 1.031212, Accuracy: 67.84%\n",
      "Batch 155, Loss: 1.078060, Accuracy: 67.82%\n",
      "Batch 156, Loss: 1.005851, Accuracy: 67.86%\n",
      "Batch 157, Loss: 0.989582, Accuracy: 67.90%\n",
      "Batch 158, Loss: 1.058521, Accuracy: 67.90%\n",
      "Batch 159, Loss: 1.187878, Accuracy: 67.81%\n",
      "Batch 160, Loss: 1.020941, Accuracy: 67.84%\n",
      "Batch 161, Loss: 1.062373, Accuracy: 67.84%\n",
      "Batch 162, Loss: 1.005727, Accuracy: 67.86%\n",
      "Batch 163, Loss: 1.020200, Accuracy: 67.89%\n",
      "Batch 164, Loss: 1.111400, Accuracy: 67.86%\n",
      "Batch 165, Loss: 1.113918, Accuracy: 67.83%\n",
      "Batch 166, Loss: 1.060514, Accuracy: 67.84%\n",
      "Batch 167, Loss: 1.120492, Accuracy: 67.81%\n",
      "Batch 168, Loss: 1.037999, Accuracy: 67.81%\n",
      "Batch 169, Loss: 1.071542, Accuracy: 67.80%\n",
      "Batch 170, Loss: 1.051168, Accuracy: 67.81%\n",
      "Batch 171, Loss: 1.084947, Accuracy: 67.78%\n",
      "Batch 172, Loss: 1.038053, Accuracy: 67.79%\n",
      "Batch 173, Loss: 1.021035, Accuracy: 67.84%\n",
      "Batch 174, Loss: 0.985081, Accuracy: 67.89%\n",
      "Batch 175, Loss: 1.140924, Accuracy: 67.84%\n",
      "Batch 176, Loss: 1.088763, Accuracy: 67.83%\n",
      "Batch 177, Loss: 1.155955, Accuracy: 67.78%\n",
      "Batch 178, Loss: 1.155380, Accuracy: 67.72%\n",
      "Batch 179, Loss: 1.022304, Accuracy: 67.75%\n",
      "Batch 180, Loss: 1.046795, Accuracy: 67.78%\n",
      "Batch 181, Loss: 1.017838, Accuracy: 67.81%\n",
      "Batch 182, Loss: 1.070714, Accuracy: 67.82%\n",
      "Batch 183, Loss: 0.983993, Accuracy: 67.86%\n",
      "Batch 184, Loss: 1.093146, Accuracy: 67.85%\n",
      "Batch 185, Loss: 1.192646, Accuracy: 67.77%\n",
      "Batch 186, Loss: 1.016870, Accuracy: 67.80%\n",
      "Batch 187, Loss: 1.104237, Accuracy: 67.78%\n",
      "Batch 188, Loss: 1.093745, Accuracy: 67.77%\n",
      "Batch 189, Loss: 1.152172, Accuracy: 67.71%\n",
      "Batch 190, Loss: 1.076434, Accuracy: 67.69%\n",
      "Batch 191, Loss: 1.028777, Accuracy: 67.72%\n",
      "Batch 192, Loss: 1.032224, Accuracy: 67.74%\n",
      "Batch 193, Loss: 1.016398, Accuracy: 67.76%\n",
      "Batch 194, Loss: 0.986099, Accuracy: 67.80%\n",
      "Batch 195, Loss: 1.039891, Accuracy: 67.80%\n",
      "Batch 196, Loss: 1.013683, Accuracy: 67.83%\n",
      "Batch 197, Loss: 1.070090, Accuracy: 67.81%\n",
      "Batch 198, Loss: 1.092197, Accuracy: 67.79%\n",
      "Batch 199, Loss: 1.023975, Accuracy: 67.82%\n",
      "Batch 200, Loss: 1.047618, Accuracy: 67.82%\n",
      "Batch 201, Loss: 1.068187, Accuracy: 67.81%\n",
      "Batch 202, Loss: 1.149239, Accuracy: 67.75%\n",
      "Batch 203, Loss: 1.014167, Accuracy: 67.77%\n",
      "Batch 204, Loss: 1.108817, Accuracy: 67.75%\n",
      "Batch 205, Loss: 1.081753, Accuracy: 67.75%\n",
      "Batch 206, Loss: 0.981163, Accuracy: 67.79%\n",
      "Batch 207, Loss: 1.103832, Accuracy: 67.75%\n",
      "Batch 208, Loss: 1.015233, Accuracy: 67.78%\n",
      "Batch 209, Loss: 0.979377, Accuracy: 67.82%\n",
      "Batch 210, Loss: 1.122893, Accuracy: 67.78%\n",
      "Batch 211, Loss: 1.043618, Accuracy: 67.79%\n",
      "Batch 212, Loss: 1.134177, Accuracy: 67.76%\n",
      "Batch 213, Loss: 0.977608, Accuracy: 67.78%\n",
      "Training - Epoch 26, Loss: 1.063216, Accuracy: 67.78%\n",
      "Validation Batch 1, Loss: 1.096770, Accuracy: 60.94%\n",
      "Validation Batch 2, Loss: 1.210110, Accuracy: 54.69%\n",
      "Validation Batch 3, Loss: 1.241599, Accuracy: 52.60%\n",
      "Validation Batch 4, Loss: 1.096836, Accuracy: 55.86%\n",
      "Validation Batch 5, Loss: 1.191545, Accuracy: 55.00%\n",
      "Validation Batch 6, Loss: 1.150952, Accuracy: 55.73%\n",
      "Validation Batch 7, Loss: 1.161524, Accuracy: 56.25%\n",
      "Validation Batch 8, Loss: 1.178150, Accuracy: 56.45%\n",
      "Validation Batch 9, Loss: 1.209085, Accuracy: 56.08%\n",
      "Validation Batch 10, Loss: 1.168314, Accuracy: 56.25%\n",
      "Validation Batch 11, Loss: 1.132283, Accuracy: 56.68%\n",
      "Validation Batch 12, Loss: 1.099883, Accuracy: 57.16%\n",
      "Validation Batch 13, Loss: 1.226811, Accuracy: 56.61%\n",
      "Validation Batch 14, Loss: 1.128941, Accuracy: 57.03%\n",
      "Validation Batch 15, Loss: 1.180231, Accuracy: 56.67%\n",
      "Validation Batch 16, Loss: 1.162603, Accuracy: 56.54%\n",
      "Validation Batch 17, Loss: 1.253840, Accuracy: 56.07%\n",
      "Validation Batch 18, Loss: 1.138603, Accuracy: 55.99%\n",
      "Validation Batch 19, Loss: 1.227726, Accuracy: 55.59%\n",
      "Validation Batch 20, Loss: 1.164474, Accuracy: 55.78%\n",
      "Validation Batch 21, Loss: 1.172073, Accuracy: 55.80%\n",
      "Validation Batch 22, Loss: 1.211894, Accuracy: 55.61%\n",
      "Validation Batch 23, Loss: 1.240688, Accuracy: 55.23%\n",
      "Validation Batch 24, Loss: 1.163742, Accuracy: 55.47%\n",
      "Validation Batch 25, Loss: 1.150424, Accuracy: 55.56%\n",
      "Validation Batch 26, Loss: 1.115953, Accuracy: 55.71%\n",
      "Validation Batch 27, Loss: 1.185349, Accuracy: 55.67%\n",
      "Validation - Epoch 26, Loss: 1.172608, Accuracy: 55.67%\n",
      "Patienceâ€”0\n",
      "Epoch 27\n",
      "Batch 1, Loss: 1.065467, Accuracy: 68.75%\n",
      "Batch 2, Loss: 1.084012, Accuracy: 67.97%\n",
      "Batch 3, Loss: 1.055386, Accuracy: 68.75%\n",
      "Batch 4, Loss: 1.049359, Accuracy: 69.14%\n",
      "Batch 5, Loss: 1.081518, Accuracy: 68.12%\n",
      "Batch 6, Loss: 1.067994, Accuracy: 68.23%\n",
      "Batch 7, Loss: 1.034784, Accuracy: 68.75%\n",
      "Batch 8, Loss: 1.156477, Accuracy: 67.19%\n",
      "Batch 9, Loss: 0.999402, Accuracy: 67.88%\n",
      "Batch 10, Loss: 1.099118, Accuracy: 67.19%\n",
      "Batch 11, Loss: 1.113671, Accuracy: 67.05%\n",
      "Batch 12, Loss: 1.033728, Accuracy: 67.45%\n",
      "Batch 13, Loss: 1.082790, Accuracy: 67.43%\n",
      "Batch 14, Loss: 1.117507, Accuracy: 66.96%\n",
      "Batch 15, Loss: 1.049983, Accuracy: 66.98%\n",
      "Batch 16, Loss: 1.166049, Accuracy: 66.41%\n",
      "Batch 17, Loss: 1.106072, Accuracy: 66.27%\n",
      "Batch 18, Loss: 0.978342, Accuracy: 66.75%\n",
      "Batch 19, Loss: 1.051693, Accuracy: 66.86%\n",
      "Batch 20, Loss: 1.110263, Accuracy: 66.56%\n",
      "Batch 21, Loss: 1.091815, Accuracy: 66.52%\n",
      "Batch 22, Loss: 1.018241, Accuracy: 66.76%\n",
      "Batch 23, Loss: 1.105382, Accuracy: 66.64%\n",
      "Batch 24, Loss: 1.047855, Accuracy: 66.73%\n",
      "Batch 25, Loss: 1.073436, Accuracy: 66.81%\n",
      "Batch 26, Loss: 0.997288, Accuracy: 67.13%\n",
      "Batch 27, Loss: 1.111858, Accuracy: 67.01%\n",
      "Batch 28, Loss: 1.257575, Accuracy: 66.24%\n",
      "Batch 29, Loss: 1.047362, Accuracy: 66.22%\n",
      "Batch 30, Loss: 1.071768, Accuracy: 66.15%\n",
      "Batch 31, Loss: 1.029396, Accuracy: 66.28%\n",
      "Batch 32, Loss: 1.102757, Accuracy: 66.26%\n",
      "Batch 33, Loss: 1.020665, Accuracy: 66.48%\n",
      "Batch 34, Loss: 1.063513, Accuracy: 66.54%\n",
      "Batch 35, Loss: 0.998340, Accuracy: 66.74%\n",
      "Batch 36, Loss: 1.061489, Accuracy: 66.71%\n",
      "Batch 37, Loss: 1.057411, Accuracy: 66.72%\n",
      "Batch 38, Loss: 0.931076, Accuracy: 67.11%\n",
      "Batch 39, Loss: 1.033727, Accuracy: 67.23%\n",
      "Batch 40, Loss: 1.057223, Accuracy: 67.23%\n",
      "Batch 41, Loss: 1.120080, Accuracy: 67.07%\n",
      "Batch 42, Loss: 0.969383, Accuracy: 67.34%\n",
      "Batch 43, Loss: 1.039194, Accuracy: 67.37%\n",
      "Batch 44, Loss: 0.970274, Accuracy: 67.58%\n",
      "Batch 45, Loss: 1.024556, Accuracy: 67.64%\n",
      "Batch 46, Loss: 1.033319, Accuracy: 67.66%\n",
      "Batch 47, Loss: 1.049929, Accuracy: 67.79%\n",
      "Batch 48, Loss: 1.065950, Accuracy: 67.77%\n",
      "Batch 49, Loss: 1.153173, Accuracy: 67.54%\n",
      "Batch 50, Loss: 1.060354, Accuracy: 67.56%\n",
      "Batch 51, Loss: 1.048277, Accuracy: 67.59%\n",
      "Batch 52, Loss: 1.031065, Accuracy: 67.61%\n",
      "Batch 53, Loss: 0.928192, Accuracy: 67.87%\n",
      "Batch 54, Loss: 1.098904, Accuracy: 67.82%\n",
      "Batch 55, Loss: 0.966364, Accuracy: 68.04%\n",
      "Batch 56, Loss: 1.145445, Accuracy: 67.86%\n",
      "Batch 57, Loss: 1.147195, Accuracy: 67.68%\n",
      "Batch 58, Loss: 1.071458, Accuracy: 67.65%\n",
      "Batch 59, Loss: 1.037748, Accuracy: 67.69%\n",
      "Batch 60, Loss: 1.082671, Accuracy: 67.63%\n",
      "Batch 61, Loss: 1.081587, Accuracy: 67.60%\n",
      "Batch 62, Loss: 1.087924, Accuracy: 67.54%\n",
      "Batch 63, Loss: 1.042616, Accuracy: 67.61%\n",
      "Batch 64, Loss: 1.114777, Accuracy: 67.55%\n",
      "Batch 65, Loss: 1.047972, Accuracy: 67.55%\n",
      "Batch 66, Loss: 1.127184, Accuracy: 67.45%\n",
      "Batch 67, Loss: 1.072194, Accuracy: 67.44%\n",
      "Batch 68, Loss: 1.057037, Accuracy: 67.46%\n",
      "Batch 69, Loss: 1.149183, Accuracy: 67.35%\n",
      "Batch 70, Loss: 0.986542, Accuracy: 67.50%\n",
      "Batch 71, Loss: 1.156012, Accuracy: 67.36%\n",
      "Batch 72, Loss: 1.060571, Accuracy: 67.34%\n",
      "Batch 73, Loss: 1.026087, Accuracy: 67.40%\n",
      "Batch 74, Loss: 1.120468, Accuracy: 67.34%\n",
      "Batch 75, Loss: 1.000999, Accuracy: 67.42%\n",
      "Batch 76, Loss: 1.023620, Accuracy: 67.50%\n",
      "Batch 77, Loss: 1.156329, Accuracy: 67.37%\n",
      "Batch 78, Loss: 1.024353, Accuracy: 67.43%\n",
      "Batch 79, Loss: 1.095273, Accuracy: 67.37%\n",
      "Batch 80, Loss: 1.058711, Accuracy: 67.38%\n",
      "Batch 81, Loss: 1.070247, Accuracy: 67.38%\n",
      "Batch 82, Loss: 0.912616, Accuracy: 67.59%\n",
      "Batch 83, Loss: 1.089603, Accuracy: 67.55%\n",
      "Batch 84, Loss: 1.053625, Accuracy: 67.54%\n",
      "Batch 85, Loss: 0.974414, Accuracy: 67.65%\n",
      "Batch 86, Loss: 1.059205, Accuracy: 67.62%\n",
      "Batch 87, Loss: 1.035334, Accuracy: 67.64%\n",
      "Batch 88, Loss: 1.067482, Accuracy: 67.65%\n",
      "Batch 89, Loss: 1.071102, Accuracy: 67.63%\n",
      "Batch 90, Loss: 1.101515, Accuracy: 67.59%\n",
      "Batch 91, Loss: 1.029036, Accuracy: 67.62%\n",
      "Batch 92, Loss: 1.082771, Accuracy: 67.63%\n",
      "Batch 93, Loss: 1.151821, Accuracy: 67.52%\n",
      "Batch 94, Loss: 1.156694, Accuracy: 67.42%\n",
      "Batch 95, Loss: 1.093695, Accuracy: 67.38%\n",
      "Batch 96, Loss: 1.138513, Accuracy: 67.30%\n",
      "Batch 97, Loss: 1.065578, Accuracy: 67.32%\n",
      "Batch 98, Loss: 1.122462, Accuracy: 67.27%\n",
      "Batch 99, Loss: 1.035697, Accuracy: 67.30%\n",
      "Batch 100, Loss: 1.023200, Accuracy: 67.34%\n",
      "Batch 101, Loss: 0.990774, Accuracy: 67.42%\n",
      "Batch 102, Loss: 1.065745, Accuracy: 67.40%\n",
      "Batch 103, Loss: 1.080884, Accuracy: 67.38%\n",
      "Batch 104, Loss: 1.076482, Accuracy: 67.38%\n",
      "Batch 105, Loss: 1.020105, Accuracy: 67.44%\n",
      "Batch 106, Loss: 1.085453, Accuracy: 67.42%\n",
      "Batch 107, Loss: 1.034279, Accuracy: 67.46%\n",
      "Batch 108, Loss: 1.015030, Accuracy: 67.51%\n",
      "Batch 109, Loss: 1.117883, Accuracy: 67.47%\n",
      "Batch 110, Loss: 1.039738, Accuracy: 67.50%\n",
      "Batch 111, Loss: 0.997803, Accuracy: 67.55%\n",
      "Batch 112, Loss: 1.075038, Accuracy: 67.56%\n",
      "Batch 113, Loss: 1.150897, Accuracy: 67.46%\n",
      "Batch 114, Loss: 1.057389, Accuracy: 67.49%\n",
      "Batch 115, Loss: 1.043182, Accuracy: 67.50%\n",
      "Batch 116, Loss: 1.016821, Accuracy: 67.55%\n",
      "Batch 117, Loss: 1.129696, Accuracy: 67.45%\n",
      "Batch 118, Loss: 0.980587, Accuracy: 67.52%\n",
      "Batch 119, Loss: 0.955039, Accuracy: 67.63%\n",
      "Batch 120, Loss: 1.051685, Accuracy: 67.66%\n",
      "Batch 121, Loss: 1.093531, Accuracy: 67.61%\n",
      "Batch 122, Loss: 1.057835, Accuracy: 67.62%\n",
      "Batch 123, Loss: 1.030603, Accuracy: 67.66%\n",
      "Batch 124, Loss: 1.156985, Accuracy: 67.58%\n",
      "Batch 125, Loss: 0.997935, Accuracy: 67.64%\n",
      "Batch 126, Loss: 1.049704, Accuracy: 67.65%\n",
      "Batch 127, Loss: 1.090121, Accuracy: 67.62%\n",
      "Batch 128, Loss: 1.059885, Accuracy: 67.61%\n",
      "Batch 129, Loss: 1.108803, Accuracy: 67.58%\n",
      "Batch 130, Loss: 1.036350, Accuracy: 67.57%\n",
      "Batch 131, Loss: 1.017859, Accuracy: 67.60%\n",
      "Batch 132, Loss: 1.150816, Accuracy: 67.50%\n",
      "Batch 133, Loss: 0.983213, Accuracy: 67.59%\n",
      "Batch 134, Loss: 1.083763, Accuracy: 67.55%\n",
      "Batch 135, Loss: 1.168521, Accuracy: 67.47%\n",
      "Batch 136, Loss: 1.065754, Accuracy: 67.45%\n",
      "Batch 137, Loss: 1.056512, Accuracy: 67.46%\n",
      "Batch 138, Loss: 1.125679, Accuracy: 67.45%\n",
      "Batch 139, Loss: 1.117393, Accuracy: 67.40%\n",
      "Batch 140, Loss: 1.107100, Accuracy: 67.34%\n",
      "Batch 141, Loss: 1.082882, Accuracy: 67.33%\n",
      "Batch 142, Loss: 1.029223, Accuracy: 67.35%\n",
      "Batch 143, Loss: 1.041783, Accuracy: 67.37%\n",
      "Batch 144, Loss: 1.112024, Accuracy: 67.34%\n",
      "Batch 145, Loss: 1.039603, Accuracy: 67.36%\n",
      "Batch 146, Loss: 1.058060, Accuracy: 67.38%\n",
      "Batch 147, Loss: 1.058491, Accuracy: 67.39%\n",
      "Batch 148, Loss: 1.119103, Accuracy: 67.38%\n",
      "Batch 149, Loss: 1.042996, Accuracy: 67.40%\n",
      "Batch 150, Loss: 1.020761, Accuracy: 67.44%\n",
      "Batch 151, Loss: 1.062239, Accuracy: 67.45%\n",
      "Batch 152, Loss: 1.065669, Accuracy: 67.44%\n",
      "Batch 153, Loss: 1.061397, Accuracy: 67.44%\n",
      "Batch 154, Loss: 1.064795, Accuracy: 67.45%\n",
      "Batch 155, Loss: 1.053964, Accuracy: 67.45%\n",
      "Batch 156, Loss: 1.051466, Accuracy: 67.44%\n",
      "Batch 157, Loss: 1.148956, Accuracy: 67.39%\n",
      "Batch 158, Loss: 1.005445, Accuracy: 67.46%\n",
      "Batch 159, Loss: 0.996152, Accuracy: 67.51%\n",
      "Batch 160, Loss: 1.068943, Accuracy: 67.51%\n",
      "Batch 161, Loss: 1.071766, Accuracy: 67.51%\n",
      "Batch 162, Loss: 1.081365, Accuracy: 67.51%\n",
      "Batch 163, Loss: 1.101230, Accuracy: 67.48%\n",
      "Batch 164, Loss: 1.132731, Accuracy: 67.42%\n",
      "Batch 165, Loss: 1.079051, Accuracy: 67.41%\n",
      "Batch 166, Loss: 1.000596, Accuracy: 67.41%\n",
      "Batch 167, Loss: 0.995858, Accuracy: 67.47%\n",
      "Batch 168, Loss: 1.044628, Accuracy: 67.47%\n",
      "Batch 169, Loss: 0.988428, Accuracy: 67.51%\n",
      "Batch 170, Loss: 1.018212, Accuracy: 67.53%\n",
      "Batch 171, Loss: 0.988577, Accuracy: 67.59%\n",
      "Batch 172, Loss: 1.103782, Accuracy: 67.54%\n",
      "Batch 173, Loss: 1.017577, Accuracy: 67.58%\n",
      "Batch 174, Loss: 1.142482, Accuracy: 67.54%\n",
      "Batch 175, Loss: 1.042773, Accuracy: 67.54%\n",
      "Batch 176, Loss: 1.120066, Accuracy: 67.51%\n",
      "Batch 177, Loss: 1.030711, Accuracy: 67.53%\n",
      "Batch 178, Loss: 1.182408, Accuracy: 67.45%\n",
      "Batch 179, Loss: 1.068686, Accuracy: 67.43%\n",
      "Batch 180, Loss: 1.034233, Accuracy: 67.47%\n",
      "Batch 181, Loss: 1.115430, Accuracy: 67.44%\n",
      "Batch 182, Loss: 0.967578, Accuracy: 67.49%\n",
      "Batch 183, Loss: 1.143859, Accuracy: 67.44%\n",
      "Batch 184, Loss: 1.049171, Accuracy: 67.43%\n",
      "Batch 185, Loss: 1.009456, Accuracy: 67.47%\n",
      "Batch 186, Loss: 1.016909, Accuracy: 67.51%\n",
      "Batch 187, Loss: 1.085742, Accuracy: 67.50%\n",
      "Batch 188, Loss: 1.078727, Accuracy: 67.46%\n",
      "Batch 189, Loss: 1.040425, Accuracy: 67.46%\n",
      "Batch 190, Loss: 1.114000, Accuracy: 67.44%\n",
      "Batch 191, Loss: 1.085054, Accuracy: 67.44%\n",
      "Batch 192, Loss: 1.018913, Accuracy: 67.46%\n",
      "Batch 193, Loss: 1.031915, Accuracy: 67.49%\n",
      "Batch 194, Loss: 1.035673, Accuracy: 67.50%\n",
      "Batch 195, Loss: 1.069225, Accuracy: 67.48%\n",
      "Batch 196, Loss: 1.085860, Accuracy: 67.48%\n",
      "Batch 197, Loss: 1.031741, Accuracy: 67.50%\n",
      "Batch 198, Loss: 1.105585, Accuracy: 67.48%\n",
      "Batch 199, Loss: 1.027041, Accuracy: 67.50%\n",
      "Batch 200, Loss: 1.092195, Accuracy: 67.48%\n",
      "Batch 201, Loss: 1.124273, Accuracy: 67.45%\n",
      "Batch 202, Loss: 0.985112, Accuracy: 67.49%\n",
      "Batch 203, Loss: 1.040742, Accuracy: 67.53%\n",
      "Batch 204, Loss: 1.101590, Accuracy: 67.50%\n",
      "Batch 205, Loss: 1.047955, Accuracy: 67.51%\n",
      "Batch 206, Loss: 0.984799, Accuracy: 67.54%\n",
      "Batch 207, Loss: 1.072377, Accuracy: 67.54%\n",
      "Batch 208, Loss: 1.041979, Accuracy: 67.55%\n",
      "Batch 209, Loss: 0.999917, Accuracy: 67.58%\n",
      "Batch 210, Loss: 1.094022, Accuracy: 67.55%\n",
      "Batch 211, Loss: 1.054007, Accuracy: 67.56%\n",
      "Batch 212, Loss: 0.963920, Accuracy: 67.61%\n",
      "Batch 213, Loss: 1.004289, Accuracy: 67.63%\n",
      "Training - Epoch 27, Loss: 1.062164, Accuracy: 67.63%\n",
      "Validation Batch 1, Loss: 1.124473, Accuracy: 59.38%\n",
      "Validation Batch 2, Loss: 1.232688, Accuracy: 53.12%\n",
      "Validation Batch 3, Loss: 1.265113, Accuracy: 50.52%\n",
      "Validation Batch 4, Loss: 1.115555, Accuracy: 53.52%\n",
      "Validation Batch 5, Loss: 1.210773, Accuracy: 53.12%\n",
      "Validation Batch 6, Loss: 1.176999, Accuracy: 53.39%\n",
      "Validation Batch 7, Loss: 1.181523, Accuracy: 54.02%\n",
      "Validation Batch 8, Loss: 1.189687, Accuracy: 54.30%\n",
      "Validation Batch 9, Loss: 1.232593, Accuracy: 53.82%\n",
      "Validation Batch 10, Loss: 1.201798, Accuracy: 53.75%\n",
      "Validation Batch 11, Loss: 1.160609, Accuracy: 53.84%\n",
      "Validation Batch 12, Loss: 1.130744, Accuracy: 54.30%\n",
      "Validation Batch 13, Loss: 1.252292, Accuracy: 53.61%\n",
      "Validation Batch 14, Loss: 1.156412, Accuracy: 53.91%\n",
      "Validation Batch 15, Loss: 1.199378, Accuracy: 53.65%\n",
      "Validation Batch 16, Loss: 1.186305, Accuracy: 53.71%\n",
      "Validation Batch 17, Loss: 1.277432, Accuracy: 53.12%\n",
      "Validation Batch 18, Loss: 1.162524, Accuracy: 53.30%\n",
      "Validation Batch 19, Loss: 1.249405, Accuracy: 52.96%\n",
      "Validation Batch 20, Loss: 1.187713, Accuracy: 53.12%\n",
      "Validation Batch 21, Loss: 1.186731, Accuracy: 53.35%\n",
      "Validation Batch 22, Loss: 1.237720, Accuracy: 52.91%\n",
      "Validation Batch 23, Loss: 1.266933, Accuracy: 52.51%\n",
      "Validation Batch 24, Loss: 1.186505, Accuracy: 52.80%\n",
      "Validation Batch 25, Loss: 1.170672, Accuracy: 52.81%\n",
      "Validation Batch 26, Loss: 1.131683, Accuracy: 53.06%\n",
      "Validation Batch 27, Loss: 1.218791, Accuracy: 52.91%\n",
      "Validation - Epoch 27, Loss: 1.196039, Accuracy: 52.91%\n",
      "Patienceâ€”1\n",
      "Epoch 28\n",
      "Batch 1, Loss: 1.052360, Accuracy: 65.62%\n",
      "Batch 2, Loss: 1.044555, Accuracy: 67.97%\n",
      "Batch 3, Loss: 1.014951, Accuracy: 69.27%\n",
      "Batch 4, Loss: 1.042072, Accuracy: 69.14%\n",
      "Batch 5, Loss: 1.089763, Accuracy: 68.75%\n",
      "Batch 6, Loss: 1.016252, Accuracy: 69.53%\n",
      "Batch 7, Loss: 1.062665, Accuracy: 69.42%\n",
      "Batch 8, Loss: 1.149615, Accuracy: 67.97%\n",
      "Batch 9, Loss: 1.072777, Accuracy: 67.71%\n",
      "Batch 10, Loss: 1.035165, Accuracy: 68.12%\n",
      "Batch 11, Loss: 1.093355, Accuracy: 67.61%\n",
      "Batch 12, Loss: 0.978346, Accuracy: 68.23%\n",
      "Batch 13, Loss: 1.036444, Accuracy: 68.51%\n",
      "Batch 14, Loss: 1.104702, Accuracy: 68.30%\n",
      "Batch 15, Loss: 0.979693, Accuracy: 68.65%\n",
      "Batch 16, Loss: 1.053306, Accuracy: 68.65%\n",
      "Batch 17, Loss: 1.059852, Accuracy: 68.66%\n",
      "Batch 18, Loss: 0.964836, Accuracy: 69.10%\n",
      "Batch 19, Loss: 1.137722, Accuracy: 68.50%\n",
      "Batch 20, Loss: 1.042635, Accuracy: 68.52%\n",
      "Batch 21, Loss: 1.029071, Accuracy: 68.60%\n",
      "Batch 22, Loss: 1.113087, Accuracy: 68.18%\n",
      "Batch 23, Loss: 1.095569, Accuracy: 68.00%\n",
      "Batch 24, Loss: 1.033310, Accuracy: 68.16%\n",
      "Batch 25, Loss: 1.108985, Accuracy: 67.94%\n",
      "Batch 26, Loss: 1.026661, Accuracy: 68.03%\n",
      "Batch 27, Loss: 0.990052, Accuracy: 68.29%\n",
      "Batch 28, Loss: 1.063666, Accuracy: 68.30%\n",
      "Batch 29, Loss: 1.100124, Accuracy: 68.16%\n",
      "Batch 30, Loss: 0.976916, Accuracy: 68.44%\n",
      "Batch 31, Loss: 1.082358, Accuracy: 68.35%\n",
      "Batch 32, Loss: 1.069346, Accuracy: 68.31%\n",
      "Batch 33, Loss: 1.034986, Accuracy: 68.37%\n",
      "Batch 34, Loss: 1.072522, Accuracy: 68.34%\n",
      "Batch 35, Loss: 1.077622, Accuracy: 68.26%\n",
      "Batch 36, Loss: 1.110932, Accuracy: 68.01%\n",
      "Batch 37, Loss: 1.042612, Accuracy: 68.03%\n",
      "Batch 38, Loss: 1.061176, Accuracy: 68.05%\n",
      "Batch 39, Loss: 1.087002, Accuracy: 67.99%\n",
      "Batch 40, Loss: 0.977519, Accuracy: 68.20%\n",
      "Batch 41, Loss: 1.052067, Accuracy: 68.14%\n",
      "Batch 42, Loss: 0.996028, Accuracy: 68.30%\n",
      "Batch 43, Loss: 1.042621, Accuracy: 68.42%\n",
      "Batch 44, Loss: 0.996095, Accuracy: 68.54%\n",
      "Batch 45, Loss: 1.169361, Accuracy: 68.26%\n",
      "Batch 46, Loss: 1.088018, Accuracy: 68.17%\n",
      "Batch 47, Loss: 1.136755, Accuracy: 67.99%\n",
      "Batch 48, Loss: 0.982334, Accuracy: 68.13%\n",
      "Batch 49, Loss: 1.048615, Accuracy: 68.21%\n",
      "Batch 50, Loss: 0.971139, Accuracy: 68.34%\n",
      "Batch 51, Loss: 1.042848, Accuracy: 68.38%\n",
      "Batch 52, Loss: 1.084933, Accuracy: 68.36%\n",
      "Batch 53, Loss: 1.093386, Accuracy: 68.28%\n",
      "Batch 54, Loss: 1.126723, Accuracy: 68.08%\n",
      "Batch 55, Loss: 1.009635, Accuracy: 68.15%\n",
      "Batch 56, Loss: 1.085148, Accuracy: 68.14%\n",
      "Batch 57, Loss: 1.077758, Accuracy: 68.09%\n",
      "Batch 58, Loss: 1.078125, Accuracy: 68.05%\n",
      "Batch 59, Loss: 1.023972, Accuracy: 68.11%\n",
      "Batch 60, Loss: 1.000769, Accuracy: 68.18%\n",
      "Batch 61, Loss: 1.048392, Accuracy: 68.21%\n",
      "Batch 62, Loss: 1.064123, Accuracy: 68.25%\n",
      "Batch 63, Loss: 1.130389, Accuracy: 68.13%\n",
      "Batch 64, Loss: 1.050649, Accuracy: 68.14%\n",
      "Batch 65, Loss: 1.001717, Accuracy: 68.20%\n",
      "Batch 66, Loss: 1.125739, Accuracy: 68.11%\n",
      "Batch 67, Loss: 1.044837, Accuracy: 68.12%\n",
      "Batch 68, Loss: 1.172842, Accuracy: 67.95%\n",
      "Batch 69, Loss: 1.009696, Accuracy: 68.03%\n",
      "Batch 70, Loss: 0.980381, Accuracy: 68.12%\n",
      "Batch 71, Loss: 1.001194, Accuracy: 68.22%\n",
      "Batch 72, Loss: 1.059148, Accuracy: 68.23%\n",
      "Batch 73, Loss: 1.084466, Accuracy: 68.21%\n",
      "Batch 74, Loss: 1.122312, Accuracy: 68.12%\n",
      "Batch 75, Loss: 1.105110, Accuracy: 68.00%\n",
      "Batch 76, Loss: 1.016210, Accuracy: 68.01%\n",
      "Batch 77, Loss: 1.065101, Accuracy: 67.98%\n",
      "Batch 78, Loss: 1.077589, Accuracy: 67.95%\n",
      "Batch 79, Loss: 1.148052, Accuracy: 67.82%\n",
      "Batch 80, Loss: 1.122225, Accuracy: 67.73%\n",
      "Batch 81, Loss: 1.068020, Accuracy: 67.73%\n",
      "Batch 82, Loss: 0.984644, Accuracy: 67.84%\n",
      "Batch 83, Loss: 1.112795, Accuracy: 67.77%\n",
      "Batch 84, Loss: 1.023679, Accuracy: 67.80%\n",
      "Batch 85, Loss: 1.083415, Accuracy: 67.76%\n",
      "Batch 86, Loss: 1.018115, Accuracy: 67.81%\n",
      "Batch 87, Loss: 1.045281, Accuracy: 67.82%\n",
      "Batch 88, Loss: 1.001384, Accuracy: 67.90%\n",
      "Batch 89, Loss: 1.059006, Accuracy: 67.87%\n",
      "Batch 90, Loss: 1.039047, Accuracy: 67.93%\n",
      "Batch 91, Loss: 1.098322, Accuracy: 67.89%\n",
      "Batch 92, Loss: 1.142494, Accuracy: 67.80%\n",
      "Batch 93, Loss: 1.023050, Accuracy: 67.84%\n",
      "Batch 94, Loss: 1.017309, Accuracy: 67.87%\n",
      "Batch 95, Loss: 0.993413, Accuracy: 67.96%\n",
      "Batch 96, Loss: 1.022906, Accuracy: 67.99%\n",
      "Batch 97, Loss: 1.044891, Accuracy: 68.01%\n",
      "Batch 98, Loss: 0.998032, Accuracy: 68.08%\n",
      "Batch 99, Loss: 1.060982, Accuracy: 68.07%\n",
      "Batch 100, Loss: 1.040432, Accuracy: 68.09%\n",
      "Batch 101, Loss: 1.059733, Accuracy: 68.08%\n",
      "Batch 102, Loss: 1.142636, Accuracy: 67.98%\n",
      "Batch 103, Loss: 1.071077, Accuracy: 67.98%\n",
      "Batch 104, Loss: 1.175505, Accuracy: 67.85%\n",
      "Batch 105, Loss: 1.085554, Accuracy: 67.83%\n",
      "Batch 106, Loss: 1.086678, Accuracy: 67.79%\n",
      "Batch 107, Loss: 1.103361, Accuracy: 67.76%\n",
      "Batch 108, Loss: 1.088398, Accuracy: 67.77%\n",
      "Batch 109, Loss: 1.043915, Accuracy: 67.78%\n",
      "Batch 110, Loss: 1.065053, Accuracy: 67.77%\n",
      "Batch 111, Loss: 1.053535, Accuracy: 67.78%\n",
      "Batch 112, Loss: 1.152060, Accuracy: 67.72%\n",
      "Batch 113, Loss: 1.047360, Accuracy: 67.74%\n",
      "Batch 114, Loss: 1.063393, Accuracy: 67.72%\n",
      "Batch 115, Loss: 1.035113, Accuracy: 67.73%\n",
      "Batch 116, Loss: 1.012220, Accuracy: 67.78%\n",
      "Batch 117, Loss: 1.072065, Accuracy: 67.78%\n",
      "Batch 118, Loss: 1.074559, Accuracy: 67.74%\n",
      "Batch 119, Loss: 1.062980, Accuracy: 67.74%\n",
      "Batch 120, Loss: 1.056058, Accuracy: 67.76%\n",
      "Batch 121, Loss: 1.139298, Accuracy: 67.68%\n",
      "Batch 122, Loss: 1.154960, Accuracy: 67.58%\n",
      "Batch 123, Loss: 0.968588, Accuracy: 67.67%\n",
      "Batch 124, Loss: 1.049286, Accuracy: 67.69%\n",
      "Batch 125, Loss: 1.057092, Accuracy: 67.66%\n",
      "Batch 126, Loss: 1.036255, Accuracy: 67.67%\n",
      "Batch 127, Loss: 1.115345, Accuracy: 67.62%\n",
      "Batch 128, Loss: 1.084765, Accuracy: 67.60%\n",
      "Batch 129, Loss: 1.101108, Accuracy: 67.56%\n",
      "Batch 130, Loss: 1.056930, Accuracy: 67.55%\n",
      "Batch 131, Loss: 1.017607, Accuracy: 67.58%\n",
      "Batch 132, Loss: 1.038284, Accuracy: 67.60%\n",
      "Batch 133, Loss: 1.103156, Accuracy: 67.55%\n",
      "Batch 134, Loss: 1.190064, Accuracy: 67.46%\n",
      "Batch 135, Loss: 1.010434, Accuracy: 67.50%\n",
      "Batch 136, Loss: 1.063609, Accuracy: 67.50%\n",
      "Batch 137, Loss: 1.085130, Accuracy: 67.47%\n",
      "Batch 138, Loss: 1.014877, Accuracy: 67.53%\n",
      "Batch 139, Loss: 1.064884, Accuracy: 67.55%\n",
      "Batch 140, Loss: 0.988968, Accuracy: 67.62%\n",
      "Batch 141, Loss: 1.173031, Accuracy: 67.55%\n",
      "Batch 142, Loss: 0.958054, Accuracy: 67.63%\n",
      "Batch 143, Loss: 1.033159, Accuracy: 67.65%\n",
      "Batch 144, Loss: 1.075298, Accuracy: 67.63%\n",
      "Batch 145, Loss: 1.025677, Accuracy: 67.67%\n",
      "Batch 146, Loss: 1.071326, Accuracy: 67.66%\n",
      "Batch 147, Loss: 1.114094, Accuracy: 67.60%\n",
      "Batch 148, Loss: 1.053870, Accuracy: 67.60%\n",
      "Batch 149, Loss: 0.994023, Accuracy: 67.64%\n",
      "Batch 150, Loss: 1.014314, Accuracy: 67.67%\n",
      "Batch 151, Loss: 1.093680, Accuracy: 67.64%\n",
      "Batch 152, Loss: 1.035505, Accuracy: 67.67%\n",
      "Batch 153, Loss: 1.077283, Accuracy: 67.65%\n",
      "Batch 154, Loss: 0.977687, Accuracy: 67.72%\n",
      "Batch 155, Loss: 1.153188, Accuracy: 67.65%\n",
      "Batch 156, Loss: 1.037533, Accuracy: 67.67%\n",
      "Batch 157, Loss: 1.132802, Accuracy: 67.64%\n",
      "Batch 158, Loss: 1.084842, Accuracy: 67.62%\n",
      "Batch 159, Loss: 1.052434, Accuracy: 67.63%\n",
      "Batch 160, Loss: 1.018654, Accuracy: 67.65%\n",
      "Batch 161, Loss: 1.192905, Accuracy: 67.56%\n",
      "Batch 162, Loss: 1.057999, Accuracy: 67.56%\n",
      "Batch 163, Loss: 1.075359, Accuracy: 67.56%\n",
      "Batch 164, Loss: 1.040395, Accuracy: 67.57%\n",
      "Batch 165, Loss: 1.078261, Accuracy: 67.58%\n",
      "Batch 166, Loss: 1.041270, Accuracy: 67.59%\n",
      "Batch 167, Loss: 1.077781, Accuracy: 67.58%\n",
      "Batch 168, Loss: 1.042821, Accuracy: 67.61%\n",
      "Batch 169, Loss: 1.049105, Accuracy: 67.61%\n",
      "Batch 170, Loss: 1.152971, Accuracy: 67.56%\n",
      "Batch 171, Loss: 1.073855, Accuracy: 67.56%\n",
      "Batch 172, Loss: 1.119911, Accuracy: 67.53%\n",
      "Batch 173, Loss: 1.039053, Accuracy: 67.54%\n",
      "Batch 174, Loss: 1.105266, Accuracy: 67.51%\n",
      "Batch 175, Loss: 1.017685, Accuracy: 67.54%\n",
      "Batch 176, Loss: 1.057229, Accuracy: 67.56%\n",
      "Batch 177, Loss: 1.099252, Accuracy: 67.54%\n",
      "Batch 178, Loss: 1.063268, Accuracy: 67.53%\n",
      "Batch 179, Loss: 0.965721, Accuracy: 67.58%\n",
      "Batch 180, Loss: 1.065306, Accuracy: 67.58%\n",
      "Batch 181, Loss: 1.013063, Accuracy: 67.60%\n",
      "Batch 182, Loss: 1.050103, Accuracy: 67.61%\n",
      "Batch 183, Loss: 1.090592, Accuracy: 67.59%\n",
      "Batch 184, Loss: 1.025015, Accuracy: 67.61%\n",
      "Batch 185, Loss: 1.099443, Accuracy: 67.59%\n",
      "Batch 186, Loss: 1.010241, Accuracy: 67.62%\n",
      "Batch 187, Loss: 1.091885, Accuracy: 67.61%\n",
      "Batch 188, Loss: 1.011958, Accuracy: 67.64%\n",
      "Batch 189, Loss: 1.113199, Accuracy: 67.62%\n",
      "Batch 190, Loss: 0.988181, Accuracy: 67.66%\n",
      "Batch 191, Loss: 1.058859, Accuracy: 67.66%\n",
      "Batch 192, Loss: 1.135459, Accuracy: 67.62%\n",
      "Batch 193, Loss: 1.047991, Accuracy: 67.63%\n",
      "Batch 194, Loss: 0.998299, Accuracy: 67.67%\n",
      "Batch 195, Loss: 1.070043, Accuracy: 67.68%\n",
      "Batch 196, Loss: 0.987422, Accuracy: 67.74%\n",
      "Batch 197, Loss: 0.975603, Accuracy: 67.79%\n",
      "Batch 198, Loss: 1.101227, Accuracy: 67.79%\n",
      "Batch 199, Loss: 1.042154, Accuracy: 67.81%\n",
      "Batch 200, Loss: 0.975581, Accuracy: 67.86%\n",
      "Batch 201, Loss: 1.098021, Accuracy: 67.83%\n",
      "Batch 202, Loss: 0.972871, Accuracy: 67.86%\n",
      "Batch 203, Loss: 1.099226, Accuracy: 67.85%\n",
      "Batch 204, Loss: 0.902297, Accuracy: 67.94%\n",
      "Batch 205, Loss: 0.992690, Accuracy: 67.97%\n",
      "Batch 206, Loss: 1.030958, Accuracy: 67.99%\n",
      "Batch 207, Loss: 1.019596, Accuracy: 68.01%\n",
      "Batch 208, Loss: 1.042159, Accuracy: 68.01%\n",
      "Batch 209, Loss: 1.106883, Accuracy: 67.98%\n",
      "Batch 210, Loss: 1.018785, Accuracy: 67.99%\n",
      "Batch 211, Loss: 1.137521, Accuracy: 67.94%\n",
      "Batch 212, Loss: 0.992954, Accuracy: 67.97%\n",
      "Batch 213, Loss: 1.049298, Accuracy: 67.98%\n",
      "Training - Epoch 28, Loss: 1.058364, Accuracy: 67.98%\n",
      "Validation Batch 1, Loss: 1.116774, Accuracy: 60.94%\n",
      "Validation Batch 2, Loss: 1.228647, Accuracy: 53.91%\n",
      "Validation Batch 3, Loss: 1.254360, Accuracy: 51.56%\n",
      "Validation Batch 4, Loss: 1.109955, Accuracy: 54.69%\n",
      "Validation Batch 5, Loss: 1.205519, Accuracy: 54.06%\n",
      "Validation Batch 6, Loss: 1.169460, Accuracy: 54.43%\n",
      "Validation Batch 7, Loss: 1.174913, Accuracy: 54.91%\n",
      "Validation Batch 8, Loss: 1.186090, Accuracy: 55.08%\n",
      "Validation Batch 9, Loss: 1.222959, Accuracy: 54.51%\n",
      "Validation Batch 10, Loss: 1.193075, Accuracy: 54.38%\n",
      "Validation Batch 11, Loss: 1.151121, Accuracy: 54.55%\n",
      "Validation Batch 12, Loss: 1.121739, Accuracy: 55.21%\n",
      "Validation Batch 13, Loss: 1.243314, Accuracy: 54.57%\n",
      "Validation Batch 14, Loss: 1.147483, Accuracy: 54.80%\n",
      "Validation Batch 15, Loss: 1.194037, Accuracy: 54.58%\n",
      "Validation Batch 16, Loss: 1.177395, Accuracy: 54.59%\n",
      "Validation Batch 17, Loss: 1.269099, Accuracy: 54.14%\n",
      "Validation Batch 18, Loss: 1.153429, Accuracy: 54.34%\n",
      "Validation Batch 19, Loss: 1.242692, Accuracy: 53.95%\n",
      "Validation Batch 20, Loss: 1.181739, Accuracy: 54.06%\n",
      "Validation Batch 21, Loss: 1.178857, Accuracy: 54.24%\n",
      "Validation Batch 22, Loss: 1.233989, Accuracy: 53.76%\n",
      "Validation Batch 23, Loss: 1.259135, Accuracy: 53.40%\n",
      "Validation Batch 24, Loss: 1.181187, Accuracy: 53.71%\n",
      "Validation Batch 25, Loss: 1.165867, Accuracy: 53.75%\n",
      "Validation Batch 26, Loss: 1.125301, Accuracy: 53.91%\n",
      "Validation Batch 27, Loss: 1.204370, Accuracy: 53.85%\n",
      "Validation - Epoch 28, Loss: 1.188611, Accuracy: 53.85%\n",
      "Patienceâ€”2\n",
      "Epoch 29\n",
      "Batch 1, Loss: 1.143093, Accuracy: 59.38%\n",
      "Batch 2, Loss: 1.164783, Accuracy: 57.81%\n",
      "Batch 3, Loss: 0.998781, Accuracy: 63.02%\n",
      "Batch 4, Loss: 1.022879, Accuracy: 65.62%\n",
      "Batch 5, Loss: 1.004354, Accuracy: 66.88%\n",
      "Batch 6, Loss: 1.131334, Accuracy: 65.89%\n",
      "Batch 7, Loss: 1.087008, Accuracy: 65.62%\n",
      "Batch 8, Loss: 1.074403, Accuracy: 65.82%\n",
      "Batch 9, Loss: 1.049915, Accuracy: 66.15%\n",
      "Batch 10, Loss: 0.978331, Accuracy: 67.50%\n",
      "Batch 11, Loss: 0.983312, Accuracy: 68.32%\n",
      "Batch 12, Loss: 1.109354, Accuracy: 67.97%\n",
      "Batch 13, Loss: 1.090643, Accuracy: 67.79%\n",
      "Batch 14, Loss: 1.079208, Accuracy: 67.52%\n",
      "Batch 15, Loss: 0.980386, Accuracy: 68.23%\n",
      "Batch 16, Loss: 1.034004, Accuracy: 68.26%\n",
      "Batch 17, Loss: 1.044877, Accuracy: 68.29%\n",
      "Batch 18, Loss: 1.127133, Accuracy: 67.88%\n",
      "Batch 19, Loss: 1.017181, Accuracy: 68.09%\n",
      "Batch 20, Loss: 1.095434, Accuracy: 67.97%\n",
      "Batch 21, Loss: 1.110078, Accuracy: 67.78%\n",
      "Batch 22, Loss: 1.005380, Accuracy: 68.11%\n",
      "Batch 23, Loss: 1.158906, Accuracy: 67.53%\n",
      "Batch 24, Loss: 1.031310, Accuracy: 67.71%\n",
      "Batch 25, Loss: 1.075604, Accuracy: 67.75%\n",
      "Batch 26, Loss: 1.143067, Accuracy: 67.49%\n",
      "Batch 27, Loss: 1.133578, Accuracy: 67.25%\n",
      "Batch 28, Loss: 1.080751, Accuracy: 67.19%\n",
      "Batch 29, Loss: 1.037006, Accuracy: 67.35%\n",
      "Batch 30, Loss: 1.020360, Accuracy: 67.45%\n",
      "Batch 31, Loss: 1.088609, Accuracy: 67.34%\n",
      "Batch 32, Loss: 1.089235, Accuracy: 67.19%\n",
      "Batch 33, Loss: 1.091483, Accuracy: 67.00%\n",
      "Batch 34, Loss: 1.025963, Accuracy: 67.05%\n",
      "Batch 35, Loss: 1.052789, Accuracy: 67.14%\n",
      "Batch 36, Loss: 1.119840, Accuracy: 66.93%\n",
      "Batch 37, Loss: 1.016384, Accuracy: 67.06%\n",
      "Batch 38, Loss: 1.110432, Accuracy: 66.90%\n",
      "Batch 39, Loss: 1.055239, Accuracy: 66.91%\n",
      "Batch 40, Loss: 1.041842, Accuracy: 66.99%\n",
      "Batch 41, Loss: 0.918112, Accuracy: 67.38%\n",
      "Batch 42, Loss: 1.051963, Accuracy: 67.37%\n",
      "Batch 43, Loss: 1.025281, Accuracy: 67.48%\n",
      "Batch 44, Loss: 1.093834, Accuracy: 67.37%\n",
      "Batch 45, Loss: 1.081314, Accuracy: 67.33%\n",
      "Batch 46, Loss: 1.135774, Accuracy: 67.12%\n",
      "Batch 47, Loss: 1.045396, Accuracy: 67.19%\n",
      "Batch 48, Loss: 0.998928, Accuracy: 67.35%\n",
      "Batch 49, Loss: 1.155259, Accuracy: 67.19%\n",
      "Batch 50, Loss: 1.083060, Accuracy: 67.06%\n",
      "Batch 51, Loss: 1.091468, Accuracy: 67.00%\n",
      "Batch 52, Loss: 1.004017, Accuracy: 67.13%\n",
      "Batch 53, Loss: 1.122207, Accuracy: 67.10%\n",
      "Batch 54, Loss: 1.096749, Accuracy: 67.04%\n",
      "Batch 55, Loss: 1.082031, Accuracy: 67.02%\n",
      "Batch 56, Loss: 1.083855, Accuracy: 66.96%\n",
      "Batch 57, Loss: 1.053489, Accuracy: 66.97%\n",
      "Batch 58, Loss: 1.053050, Accuracy: 67.00%\n",
      "Batch 59, Loss: 1.188571, Accuracy: 66.74%\n",
      "Batch 60, Loss: 1.072095, Accuracy: 66.74%\n",
      "Batch 61, Loss: 0.994389, Accuracy: 66.93%\n",
      "Batch 62, Loss: 1.051077, Accuracy: 67.01%\n",
      "Batch 63, Loss: 0.984735, Accuracy: 67.16%\n",
      "Batch 64, Loss: 1.016085, Accuracy: 67.26%\n",
      "Batch 65, Loss: 0.982715, Accuracy: 67.40%\n",
      "Batch 66, Loss: 1.044757, Accuracy: 67.45%\n",
      "Batch 67, Loss: 1.063437, Accuracy: 67.44%\n",
      "Batch 68, Loss: 1.024385, Accuracy: 67.49%\n",
      "Batch 69, Loss: 1.073708, Accuracy: 67.48%\n",
      "Batch 70, Loss: 1.100481, Accuracy: 67.43%\n",
      "Batch 71, Loss: 1.003408, Accuracy: 67.52%\n",
      "Batch 72, Loss: 1.040928, Accuracy: 67.49%\n",
      "Batch 73, Loss: 1.068580, Accuracy: 67.49%\n",
      "Batch 74, Loss: 1.009892, Accuracy: 67.59%\n",
      "Batch 75, Loss: 1.047702, Accuracy: 67.62%\n",
      "Batch 76, Loss: 1.085958, Accuracy: 67.62%\n",
      "Batch 77, Loss: 1.077936, Accuracy: 67.59%\n",
      "Batch 78, Loss: 1.062129, Accuracy: 67.59%\n",
      "Batch 79, Loss: 1.110679, Accuracy: 67.54%\n",
      "Batch 80, Loss: 1.134755, Accuracy: 67.46%\n",
      "Batch 81, Loss: 1.085151, Accuracy: 67.46%\n",
      "Batch 82, Loss: 1.178307, Accuracy: 67.32%\n",
      "Batch 83, Loss: 1.031274, Accuracy: 67.36%\n",
      "Batch 84, Loss: 1.059849, Accuracy: 67.39%\n",
      "Batch 85, Loss: 1.087816, Accuracy: 67.39%\n",
      "Batch 86, Loss: 1.003484, Accuracy: 67.44%\n",
      "Batch 87, Loss: 1.048926, Accuracy: 67.44%\n",
      "Batch 88, Loss: 1.098627, Accuracy: 67.40%\n",
      "Batch 89, Loss: 1.081779, Accuracy: 67.40%\n",
      "Batch 90, Loss: 1.055563, Accuracy: 67.41%\n",
      "Batch 91, Loss: 1.012277, Accuracy: 67.48%\n",
      "Batch 92, Loss: 0.998446, Accuracy: 67.54%\n",
      "Batch 93, Loss: 0.969210, Accuracy: 67.66%\n",
      "Batch 94, Loss: 1.153754, Accuracy: 67.54%\n",
      "Batch 95, Loss: 1.025465, Accuracy: 67.57%\n",
      "Batch 96, Loss: 1.051521, Accuracy: 67.56%\n",
      "Batch 97, Loss: 1.108510, Accuracy: 67.49%\n",
      "Batch 98, Loss: 1.007573, Accuracy: 67.57%\n",
      "Batch 99, Loss: 1.073006, Accuracy: 67.55%\n",
      "Batch 100, Loss: 1.027829, Accuracy: 67.59%\n",
      "Batch 101, Loss: 1.071671, Accuracy: 67.59%\n",
      "Batch 102, Loss: 1.081765, Accuracy: 67.57%\n",
      "Batch 103, Loss: 0.978583, Accuracy: 67.63%\n",
      "Batch 104, Loss: 1.065578, Accuracy: 67.62%\n",
      "Batch 105, Loss: 1.001015, Accuracy: 67.66%\n",
      "Batch 106, Loss: 1.037780, Accuracy: 67.69%\n",
      "Batch 107, Loss: 1.180567, Accuracy: 67.58%\n",
      "Batch 108, Loss: 1.096570, Accuracy: 67.53%\n",
      "Batch 109, Loss: 0.994297, Accuracy: 67.60%\n",
      "Batch 110, Loss: 1.094040, Accuracy: 67.57%\n",
      "Batch 111, Loss: 1.077815, Accuracy: 67.55%\n",
      "Batch 112, Loss: 1.022653, Accuracy: 67.59%\n",
      "Batch 113, Loss: 1.127617, Accuracy: 67.51%\n",
      "Batch 114, Loss: 0.993040, Accuracy: 67.58%\n",
      "Batch 115, Loss: 1.053521, Accuracy: 67.58%\n",
      "Batch 116, Loss: 1.035676, Accuracy: 67.61%\n",
      "Batch 117, Loss: 0.969788, Accuracy: 67.69%\n",
      "Batch 118, Loss: 1.024172, Accuracy: 67.73%\n",
      "Batch 119, Loss: 1.095860, Accuracy: 67.70%\n",
      "Batch 120, Loss: 0.967370, Accuracy: 67.76%\n",
      "Batch 121, Loss: 1.131838, Accuracy: 67.69%\n",
      "Batch 122, Loss: 1.038100, Accuracy: 67.70%\n",
      "Batch 123, Loss: 1.056146, Accuracy: 67.72%\n",
      "Batch 124, Loss: 1.098357, Accuracy: 67.69%\n",
      "Batch 125, Loss: 1.036703, Accuracy: 67.70%\n",
      "Batch 126, Loss: 1.073467, Accuracy: 67.70%\n",
      "Batch 127, Loss: 1.022991, Accuracy: 67.74%\n",
      "Batch 128, Loss: 1.056100, Accuracy: 67.75%\n",
      "Batch 129, Loss: 1.078760, Accuracy: 67.72%\n",
      "Batch 130, Loss: 0.943157, Accuracy: 67.80%\n",
      "Batch 131, Loss: 0.975930, Accuracy: 67.86%\n",
      "Batch 132, Loss: 1.051089, Accuracy: 67.84%\n",
      "Batch 133, Loss: 1.065139, Accuracy: 67.83%\n",
      "Batch 134, Loss: 1.019671, Accuracy: 67.86%\n",
      "Batch 135, Loss: 1.052615, Accuracy: 67.87%\n",
      "Batch 136, Loss: 1.044338, Accuracy: 67.91%\n",
      "Batch 137, Loss: 1.045924, Accuracy: 67.91%\n",
      "Batch 138, Loss: 1.050473, Accuracy: 67.93%\n",
      "Batch 139, Loss: 1.034670, Accuracy: 67.97%\n",
      "Batch 140, Loss: 1.050095, Accuracy: 67.98%\n",
      "Batch 141, Loss: 1.015993, Accuracy: 68.03%\n",
      "Batch 142, Loss: 0.999702, Accuracy: 68.08%\n",
      "Batch 143, Loss: 1.066904, Accuracy: 68.08%\n",
      "Batch 144, Loss: 0.990437, Accuracy: 68.15%\n",
      "Batch 145, Loss: 1.054806, Accuracy: 68.16%\n",
      "Batch 146, Loss: 1.090085, Accuracy: 68.14%\n",
      "Batch 147, Loss: 1.095957, Accuracy: 68.11%\n",
      "Batch 148, Loss: 0.988608, Accuracy: 68.17%\n",
      "Batch 149, Loss: 1.035801, Accuracy: 68.17%\n",
      "Batch 150, Loss: 0.963290, Accuracy: 68.24%\n",
      "Batch 151, Loss: 1.093930, Accuracy: 68.23%\n",
      "Batch 152, Loss: 1.021716, Accuracy: 68.27%\n",
      "Batch 153, Loss: 1.137607, Accuracy: 68.19%\n",
      "Batch 154, Loss: 0.997792, Accuracy: 68.24%\n",
      "Batch 155, Loss: 1.100938, Accuracy: 68.20%\n",
      "Batch 156, Loss: 1.050525, Accuracy: 68.19%\n",
      "Batch 157, Loss: 1.084705, Accuracy: 68.19%\n",
      "Batch 158, Loss: 1.001952, Accuracy: 68.25%\n",
      "Batch 159, Loss: 1.046414, Accuracy: 68.24%\n",
      "Batch 160, Loss: 1.091255, Accuracy: 68.22%\n",
      "Batch 161, Loss: 1.056641, Accuracy: 68.25%\n",
      "Batch 162, Loss: 1.002448, Accuracy: 68.28%\n",
      "Batch 163, Loss: 0.988784, Accuracy: 68.33%\n",
      "Batch 164, Loss: 1.083418, Accuracy: 68.29%\n",
      "Batch 165, Loss: 1.093516, Accuracy: 68.28%\n",
      "Batch 166, Loss: 1.021487, Accuracy: 68.30%\n",
      "Batch 167, Loss: 1.122463, Accuracy: 68.24%\n",
      "Batch 168, Loss: 1.049766, Accuracy: 68.24%\n",
      "Batch 169, Loss: 1.111043, Accuracy: 68.21%\n",
      "Batch 170, Loss: 1.021144, Accuracy: 68.23%\n",
      "Batch 171, Loss: 1.112284, Accuracy: 68.20%\n",
      "Batch 172, Loss: 1.021045, Accuracy: 68.22%\n",
      "Batch 173, Loss: 1.020073, Accuracy: 68.27%\n",
      "Batch 174, Loss: 1.020164, Accuracy: 68.30%\n",
      "Batch 175, Loss: 1.021484, Accuracy: 68.32%\n",
      "Batch 176, Loss: 1.080287, Accuracy: 68.30%\n",
      "Batch 177, Loss: 1.058181, Accuracy: 68.31%\n",
      "Batch 178, Loss: 1.025707, Accuracy: 68.32%\n",
      "Batch 179, Loss: 1.032322, Accuracy: 68.34%\n",
      "Batch 180, Loss: 1.017998, Accuracy: 68.37%\n",
      "Batch 181, Loss: 1.003067, Accuracy: 68.42%\n",
      "Batch 182, Loss: 1.119204, Accuracy: 68.39%\n",
      "Batch 183, Loss: 1.088763, Accuracy: 68.39%\n",
      "Batch 184, Loss: 1.031121, Accuracy: 68.39%\n",
      "Batch 185, Loss: 1.011843, Accuracy: 68.43%\n",
      "Batch 186, Loss: 1.022361, Accuracy: 68.46%\n",
      "Batch 187, Loss: 1.143613, Accuracy: 68.39%\n",
      "Batch 188, Loss: 1.114356, Accuracy: 68.36%\n",
      "Batch 189, Loss: 1.131326, Accuracy: 68.32%\n",
      "Batch 190, Loss: 0.997237, Accuracy: 68.35%\n",
      "Batch 191, Loss: 0.992441, Accuracy: 68.37%\n",
      "Batch 192, Loss: 1.169108, Accuracy: 68.32%\n",
      "Batch 193, Loss: 0.909648, Accuracy: 68.41%\n",
      "Batch 194, Loss: 1.046624, Accuracy: 68.42%\n",
      "Batch 195, Loss: 1.063166, Accuracy: 68.42%\n",
      "Batch 196, Loss: 1.060680, Accuracy: 68.42%\n",
      "Batch 197, Loss: 1.092067, Accuracy: 68.41%\n",
      "Batch 198, Loss: 1.010990, Accuracy: 68.44%\n",
      "Batch 199, Loss: 1.056409, Accuracy: 68.46%\n",
      "Batch 200, Loss: 1.128735, Accuracy: 68.42%\n",
      "Batch 201, Loss: 1.057558, Accuracy: 68.42%\n",
      "Batch 202, Loss: 1.058161, Accuracy: 68.42%\n",
      "Batch 203, Loss: 1.149016, Accuracy: 68.37%\n",
      "Batch 204, Loss: 1.084233, Accuracy: 68.34%\n",
      "Batch 205, Loss: 1.066058, Accuracy: 68.33%\n",
      "Batch 206, Loss: 0.988417, Accuracy: 68.37%\n",
      "Batch 207, Loss: 1.041545, Accuracy: 68.38%\n",
      "Batch 208, Loss: 1.142334, Accuracy: 68.34%\n",
      "Batch 209, Loss: 1.130007, Accuracy: 68.30%\n",
      "Batch 210, Loss: 1.097133, Accuracy: 68.28%\n",
      "Batch 211, Loss: 1.047953, Accuracy: 68.30%\n",
      "Batch 212, Loss: 1.096539, Accuracy: 68.29%\n",
      "Batch 213, Loss: 1.045327, Accuracy: 68.29%\n",
      "Training - Epoch 29, Loss: 1.057547, Accuracy: 68.29%\n",
      "Validation Batch 1, Loss: 1.087487, Accuracy: 64.06%\n",
      "Validation Batch 2, Loss: 1.204878, Accuracy: 56.25%\n",
      "Validation Batch 3, Loss: 1.222986, Accuracy: 54.69%\n",
      "Validation Batch 4, Loss: 1.087460, Accuracy: 57.81%\n",
      "Validation Batch 5, Loss: 1.176310, Accuracy: 56.88%\n",
      "Validation Batch 6, Loss: 1.132987, Accuracy: 57.29%\n",
      "Validation Batch 7, Loss: 1.151973, Accuracy: 57.59%\n",
      "Validation Batch 8, Loss: 1.167937, Accuracy: 57.62%\n",
      "Validation Batch 9, Loss: 1.191334, Accuracy: 57.47%\n",
      "Validation Batch 10, Loss: 1.155701, Accuracy: 57.50%\n",
      "Validation Batch 11, Loss: 1.121864, Accuracy: 57.81%\n",
      "Validation Batch 12, Loss: 1.091378, Accuracy: 58.20%\n",
      "Validation Batch 13, Loss: 1.216383, Accuracy: 57.69%\n",
      "Validation Batch 14, Loss: 1.126715, Accuracy: 57.92%\n",
      "Validation Batch 15, Loss: 1.171857, Accuracy: 57.50%\n",
      "Validation Batch 16, Loss: 1.149177, Accuracy: 57.62%\n",
      "Validation Batch 17, Loss: 1.243651, Accuracy: 56.89%\n",
      "Validation Batch 18, Loss: 1.125619, Accuracy: 56.94%\n",
      "Validation Batch 19, Loss: 1.214275, Accuracy: 56.58%\n",
      "Validation Batch 20, Loss: 1.152359, Accuracy: 56.72%\n",
      "Validation Batch 21, Loss: 1.156790, Accuracy: 56.77%\n",
      "Validation Batch 22, Loss: 1.205467, Accuracy: 56.46%\n",
      "Validation Batch 23, Loss: 1.231388, Accuracy: 56.11%\n",
      "Validation Batch 24, Loss: 1.157975, Accuracy: 56.25%\n",
      "Validation Batch 25, Loss: 1.141944, Accuracy: 56.38%\n",
      "Validation Batch 26, Loss: 1.105351, Accuracy: 56.49%\n",
      "Validation Batch 27, Loss: 1.164873, Accuracy: 56.43%\n",
      "Validation - Epoch 29, Loss: 1.161338, Accuracy: 56.43%\n",
      "Patienceâ€”0\n",
      "Epoch 30\n",
      "Batch 1, Loss: 0.976191, Accuracy: 76.56%\n",
      "Batch 2, Loss: 1.035643, Accuracy: 73.44%\n",
      "Batch 3, Loss: 1.078604, Accuracy: 70.31%\n",
      "Batch 4, Loss: 0.985579, Accuracy: 71.48%\n",
      "Batch 5, Loss: 1.031660, Accuracy: 70.94%\n",
      "Batch 6, Loss: 1.141468, Accuracy: 69.01%\n",
      "Batch 7, Loss: 1.034214, Accuracy: 69.20%\n",
      "Batch 8, Loss: 1.093803, Accuracy: 68.55%\n",
      "Batch 9, Loss: 0.984087, Accuracy: 69.44%\n",
      "Batch 10, Loss: 0.994502, Accuracy: 70.16%\n",
      "Batch 11, Loss: 0.967396, Accuracy: 71.16%\n",
      "Batch 12, Loss: 0.990673, Accuracy: 71.61%\n",
      "Batch 13, Loss: 1.062749, Accuracy: 71.39%\n",
      "Batch 14, Loss: 1.157997, Accuracy: 70.54%\n",
      "Batch 15, Loss: 1.000751, Accuracy: 70.73%\n",
      "Batch 16, Loss: 0.950772, Accuracy: 71.19%\n",
      "Batch 17, Loss: 1.057894, Accuracy: 71.14%\n",
      "Batch 18, Loss: 1.021548, Accuracy: 71.18%\n",
      "Batch 19, Loss: 1.182612, Accuracy: 70.15%\n",
      "Batch 20, Loss: 1.085215, Accuracy: 69.77%\n",
      "Batch 21, Loss: 1.037350, Accuracy: 69.94%\n",
      "Batch 22, Loss: 1.020673, Accuracy: 70.10%\n",
      "Batch 23, Loss: 0.924612, Accuracy: 70.58%\n",
      "Batch 24, Loss: 1.093724, Accuracy: 70.31%\n",
      "Batch 25, Loss: 1.132406, Accuracy: 69.94%\n",
      "Batch 26, Loss: 1.096358, Accuracy: 69.71%\n",
      "Batch 27, Loss: 1.057073, Accuracy: 69.68%\n",
      "Batch 28, Loss: 1.132264, Accuracy: 69.25%\n",
      "Batch 29, Loss: 1.048840, Accuracy: 69.18%\n",
      "Batch 30, Loss: 1.072877, Accuracy: 69.11%\n",
      "Batch 31, Loss: 1.011091, Accuracy: 69.25%\n",
      "Batch 32, Loss: 1.094960, Accuracy: 69.09%\n",
      "Batch 33, Loss: 1.036962, Accuracy: 69.22%\n",
      "Batch 34, Loss: 1.058336, Accuracy: 69.12%\n",
      "Batch 35, Loss: 1.057953, Accuracy: 69.11%\n",
      "Batch 36, Loss: 1.063820, Accuracy: 69.05%\n",
      "Batch 37, Loss: 1.054368, Accuracy: 69.05%\n",
      "Batch 38, Loss: 1.051165, Accuracy: 69.04%\n",
      "Batch 39, Loss: 1.044667, Accuracy: 69.07%\n",
      "Batch 40, Loss: 1.124189, Accuracy: 68.79%\n",
      "Batch 41, Loss: 1.039452, Accuracy: 68.83%\n",
      "Batch 42, Loss: 0.966907, Accuracy: 69.05%\n",
      "Batch 43, Loss: 1.062004, Accuracy: 68.97%\n",
      "Batch 44, Loss: 1.114931, Accuracy: 68.82%\n",
      "Batch 45, Loss: 1.050226, Accuracy: 68.82%\n",
      "Batch 46, Loss: 0.980063, Accuracy: 68.95%\n",
      "Batch 47, Loss: 1.032297, Accuracy: 68.98%\n",
      "Batch 48, Loss: 1.062298, Accuracy: 68.98%\n",
      "Batch 49, Loss: 1.101326, Accuracy: 68.88%\n",
      "Batch 50, Loss: 0.956172, Accuracy: 69.09%\n",
      "Batch 51, Loss: 1.088570, Accuracy: 68.90%\n",
      "Batch 52, Loss: 0.955040, Accuracy: 69.02%\n",
      "Batch 53, Loss: 1.078309, Accuracy: 69.02%\n",
      "Batch 54, Loss: 1.033831, Accuracy: 69.04%\n",
      "Batch 55, Loss: 1.148012, Accuracy: 68.84%\n",
      "Batch 56, Loss: 1.102561, Accuracy: 68.75%\n",
      "Batch 57, Loss: 1.152734, Accuracy: 68.56%\n",
      "Batch 58, Loss: 1.069031, Accuracy: 68.53%\n",
      "Batch 59, Loss: 1.009966, Accuracy: 68.64%\n",
      "Batch 60, Loss: 1.041498, Accuracy: 68.72%\n",
      "Batch 61, Loss: 1.081681, Accuracy: 68.65%\n",
      "Batch 62, Loss: 1.032055, Accuracy: 68.67%\n",
      "Batch 63, Loss: 1.004669, Accuracy: 68.75%\n",
      "Batch 64, Loss: 1.016338, Accuracy: 68.80%\n",
      "Batch 65, Loss: 0.954436, Accuracy: 68.97%\n",
      "Batch 66, Loss: 1.056290, Accuracy: 68.94%\n",
      "Batch 67, Loss: 1.104465, Accuracy: 68.87%\n",
      "Batch 68, Loss: 1.034580, Accuracy: 68.93%\n",
      "Batch 69, Loss: 1.087536, Accuracy: 68.89%\n",
      "Batch 70, Loss: 1.011644, Accuracy: 68.97%\n",
      "Batch 71, Loss: 0.998936, Accuracy: 69.06%\n",
      "Batch 72, Loss: 1.006106, Accuracy: 69.14%\n",
      "Batch 73, Loss: 1.015035, Accuracy: 69.16%\n",
      "Batch 74, Loss: 1.087443, Accuracy: 69.13%\n",
      "Batch 75, Loss: 1.146579, Accuracy: 69.00%\n",
      "Batch 76, Loss: 1.014020, Accuracy: 69.06%\n",
      "Batch 77, Loss: 1.099570, Accuracy: 68.97%\n",
      "Batch 78, Loss: 1.080043, Accuracy: 68.91%\n",
      "Batch 79, Loss: 1.082693, Accuracy: 68.87%\n",
      "Batch 80, Loss: 1.079634, Accuracy: 68.83%\n",
      "Batch 81, Loss: 0.969323, Accuracy: 68.92%\n",
      "Batch 82, Loss: 1.041252, Accuracy: 68.96%\n",
      "Batch 83, Loss: 0.989560, Accuracy: 68.99%\n",
      "Batch 84, Loss: 1.168740, Accuracy: 68.82%\n",
      "Batch 85, Loss: 1.134896, Accuracy: 68.68%\n",
      "Batch 86, Loss: 1.046117, Accuracy: 68.70%\n",
      "Batch 87, Loss: 1.059867, Accuracy: 68.70%\n",
      "Batch 88, Loss: 1.108478, Accuracy: 68.64%\n",
      "Batch 89, Loss: 1.054247, Accuracy: 68.63%\n",
      "Batch 90, Loss: 1.079818, Accuracy: 68.61%\n",
      "Batch 91, Loss: 1.014473, Accuracy: 68.66%\n",
      "Batch 92, Loss: 0.991992, Accuracy: 68.73%\n",
      "Batch 93, Loss: 1.008464, Accuracy: 68.77%\n",
      "Batch 94, Loss: 1.198021, Accuracy: 68.63%\n",
      "Batch 95, Loss: 1.097323, Accuracy: 68.57%\n",
      "Batch 96, Loss: 1.136368, Accuracy: 68.46%\n",
      "Batch 97, Loss: 1.008842, Accuracy: 68.51%\n",
      "Batch 98, Loss: 1.041591, Accuracy: 68.54%\n",
      "Batch 99, Loss: 1.053829, Accuracy: 68.56%\n",
      "Batch 100, Loss: 1.050977, Accuracy: 68.58%\n",
      "Batch 101, Loss: 1.011629, Accuracy: 68.61%\n",
      "Batch 102, Loss: 1.083188, Accuracy: 68.57%\n",
      "Batch 103, Loss: 1.024861, Accuracy: 68.61%\n",
      "Batch 104, Loss: 1.076949, Accuracy: 68.61%\n",
      "Batch 105, Loss: 1.086965, Accuracy: 68.57%\n",
      "Batch 106, Loss: 1.101103, Accuracy: 68.54%\n",
      "Batch 107, Loss: 0.998651, Accuracy: 68.60%\n",
      "Batch 108, Loss: 1.071726, Accuracy: 68.58%\n",
      "Batch 109, Loss: 1.067386, Accuracy: 68.56%\n",
      "Batch 110, Loss: 1.092268, Accuracy: 68.49%\n",
      "Batch 111, Loss: 1.042446, Accuracy: 68.52%\n",
      "Batch 112, Loss: 1.035929, Accuracy: 68.57%\n",
      "Batch 113, Loss: 1.003891, Accuracy: 68.64%\n",
      "Batch 114, Loss: 1.120577, Accuracy: 68.59%\n",
      "Batch 115, Loss: 1.039382, Accuracy: 68.60%\n",
      "Batch 116, Loss: 1.067253, Accuracy: 68.60%\n",
      "Batch 117, Loss: 1.043192, Accuracy: 68.62%\n",
      "Batch 118, Loss: 1.025482, Accuracy: 68.64%\n",
      "Batch 119, Loss: 1.053249, Accuracy: 68.66%\n",
      "Batch 120, Loss: 1.095811, Accuracy: 68.63%\n",
      "Batch 121, Loss: 0.996719, Accuracy: 68.66%\n",
      "Batch 122, Loss: 1.118428, Accuracy: 68.57%\n",
      "Batch 123, Loss: 1.183888, Accuracy: 68.46%\n",
      "Batch 124, Loss: 0.989335, Accuracy: 68.52%\n",
      "Batch 125, Loss: 1.010316, Accuracy: 68.56%\n",
      "Batch 126, Loss: 0.987291, Accuracy: 68.64%\n",
      "Batch 127, Loss: 1.006547, Accuracy: 68.69%\n",
      "Batch 128, Loss: 1.049645, Accuracy: 68.66%\n",
      "Batch 129, Loss: 1.021011, Accuracy: 68.69%\n",
      "Batch 130, Loss: 0.999324, Accuracy: 68.73%\n",
      "Batch 131, Loss: 1.031180, Accuracy: 68.73%\n",
      "Batch 132, Loss: 1.189879, Accuracy: 68.63%\n",
      "Batch 133, Loss: 0.997742, Accuracy: 68.69%\n",
      "Batch 134, Loss: 1.011404, Accuracy: 68.73%\n",
      "Batch 135, Loss: 1.075492, Accuracy: 68.70%\n",
      "Batch 136, Loss: 0.944109, Accuracy: 68.80%\n",
      "Batch 137, Loss: 1.098376, Accuracy: 68.76%\n",
      "Batch 138, Loss: 1.054064, Accuracy: 68.76%\n",
      "Batch 139, Loss: 0.994940, Accuracy: 68.81%\n",
      "Batch 140, Loss: 1.029556, Accuracy: 68.82%\n",
      "Batch 141, Loss: 1.088375, Accuracy: 68.79%\n",
      "Batch 142, Loss: 1.074069, Accuracy: 68.77%\n",
      "Batch 143, Loss: 1.101540, Accuracy: 68.73%\n",
      "Batch 144, Loss: 1.051385, Accuracy: 68.74%\n",
      "Batch 145, Loss: 1.105695, Accuracy: 68.72%\n",
      "Batch 146, Loss: 1.071478, Accuracy: 68.71%\n",
      "Batch 147, Loss: 1.011910, Accuracy: 68.73%\n",
      "Batch 148, Loss: 1.059494, Accuracy: 68.73%\n",
      "Batch 149, Loss: 1.113126, Accuracy: 68.68%\n",
      "Batch 150, Loss: 0.999930, Accuracy: 68.71%\n",
      "Batch 151, Loss: 0.998293, Accuracy: 68.75%\n",
      "Batch 152, Loss: 1.050383, Accuracy: 68.74%\n",
      "Batch 153, Loss: 1.003660, Accuracy: 68.78%\n",
      "Batch 154, Loss: 1.129512, Accuracy: 68.73%\n",
      "Batch 155, Loss: 1.057361, Accuracy: 68.71%\n",
      "Batch 156, Loss: 1.131311, Accuracy: 68.67%\n",
      "Batch 157, Loss: 1.139770, Accuracy: 68.59%\n",
      "Batch 158, Loss: 1.037615, Accuracy: 68.61%\n",
      "Batch 159, Loss: 1.020547, Accuracy: 68.62%\n",
      "Batch 160, Loss: 1.060495, Accuracy: 68.62%\n",
      "Batch 161, Loss: 1.018385, Accuracy: 68.66%\n",
      "Batch 162, Loss: 1.093103, Accuracy: 68.63%\n",
      "Batch 163, Loss: 1.118559, Accuracy: 68.60%\n",
      "Batch 164, Loss: 0.981171, Accuracy: 68.65%\n",
      "Batch 165, Loss: 1.094665, Accuracy: 68.61%\n",
      "Batch 166, Loss: 1.106267, Accuracy: 68.57%\n",
      "Batch 167, Loss: 0.923370, Accuracy: 68.67%\n",
      "Batch 168, Loss: 1.099513, Accuracy: 68.63%\n",
      "Batch 169, Loss: 0.969124, Accuracy: 68.67%\n",
      "Batch 170, Loss: 1.122729, Accuracy: 68.61%\n",
      "Batch 171, Loss: 1.077542, Accuracy: 68.60%\n",
      "Batch 172, Loss: 1.079130, Accuracy: 68.58%\n",
      "Batch 173, Loss: 1.044563, Accuracy: 68.57%\n",
      "Batch 174, Loss: 1.090350, Accuracy: 68.56%\n",
      "Batch 175, Loss: 1.078963, Accuracy: 68.54%\n",
      "Batch 176, Loss: 1.024120, Accuracy: 68.57%\n",
      "Batch 177, Loss: 1.064548, Accuracy: 68.56%\n",
      "Batch 178, Loss: 0.958617, Accuracy: 68.62%\n",
      "Batch 179, Loss: 1.082043, Accuracy: 68.59%\n",
      "Batch 180, Loss: 1.038306, Accuracy: 68.60%\n",
      "Batch 181, Loss: 1.060690, Accuracy: 68.59%\n",
      "Batch 182, Loss: 1.098904, Accuracy: 68.57%\n",
      "Batch 183, Loss: 1.068696, Accuracy: 68.55%\n",
      "Batch 184, Loss: 1.046407, Accuracy: 68.55%\n",
      "Batch 185, Loss: 1.008023, Accuracy: 68.59%\n",
      "Batch 186, Loss: 1.083607, Accuracy: 68.56%\n",
      "Batch 187, Loss: 1.055773, Accuracy: 68.56%\n",
      "Batch 188, Loss: 1.117086, Accuracy: 68.53%\n",
      "Batch 189, Loss: 1.193488, Accuracy: 68.45%\n",
      "Batch 190, Loss: 1.079171, Accuracy: 68.43%\n",
      "Batch 191, Loss: 1.062461, Accuracy: 68.42%\n",
      "Batch 192, Loss: 1.132459, Accuracy: 68.39%\n",
      "Batch 193, Loss: 1.065699, Accuracy: 68.40%\n",
      "Batch 194, Loss: 1.103807, Accuracy: 68.39%\n",
      "Batch 195, Loss: 1.040989, Accuracy: 68.37%\n",
      "Batch 196, Loss: 1.104861, Accuracy: 68.35%\n",
      "Batch 197, Loss: 1.138386, Accuracy: 68.32%\n",
      "Batch 198, Loss: 1.082292, Accuracy: 68.29%\n",
      "Batch 199, Loss: 1.122730, Accuracy: 68.24%\n",
      "Batch 200, Loss: 1.004248, Accuracy: 68.27%\n",
      "Batch 201, Loss: 0.959713, Accuracy: 68.31%\n",
      "Batch 202, Loss: 1.111282, Accuracy: 68.28%\n",
      "Batch 203, Loss: 1.048440, Accuracy: 68.29%\n",
      "Batch 204, Loss: 1.109560, Accuracy: 68.26%\n",
      "Batch 205, Loss: 1.090031, Accuracy: 68.23%\n",
      "Batch 206, Loss: 0.987015, Accuracy: 68.27%\n",
      "Batch 207, Loss: 1.043649, Accuracy: 68.28%\n",
      "Batch 208, Loss: 1.066381, Accuracy: 68.28%\n",
      "Batch 209, Loss: 1.112529, Accuracy: 68.24%\n",
      "Batch 210, Loss: 1.000426, Accuracy: 68.28%\n",
      "Batch 211, Loss: 1.031406, Accuracy: 68.29%\n",
      "Batch 212, Loss: 0.998080, Accuracy: 68.34%\n",
      "Batch 213, Loss: 1.044887, Accuracy: 68.34%\n",
      "Training - Epoch 30, Loss: 1.055942, Accuracy: 68.34%\n",
      "Validation Batch 1, Loss: 1.088526, Accuracy: 62.50%\n",
      "Validation Batch 2, Loss: 1.208694, Accuracy: 56.25%\n",
      "Validation Batch 3, Loss: 1.225347, Accuracy: 54.17%\n",
      "Validation Batch 4, Loss: 1.089591, Accuracy: 57.42%\n",
      "Validation Batch 5, Loss: 1.177264, Accuracy: 56.88%\n",
      "Validation Batch 6, Loss: 1.134499, Accuracy: 57.55%\n",
      "Validation Batch 7, Loss: 1.152043, Accuracy: 57.81%\n",
      "Validation Batch 8, Loss: 1.166578, Accuracy: 57.62%\n",
      "Validation Batch 9, Loss: 1.192767, Accuracy: 57.47%\n",
      "Validation Batch 10, Loss: 1.159047, Accuracy: 57.50%\n",
      "Validation Batch 11, Loss: 1.124113, Accuracy: 57.81%\n",
      "Validation Batch 12, Loss: 1.099546, Accuracy: 58.20%\n",
      "Validation Batch 13, Loss: 1.214778, Accuracy: 57.81%\n",
      "Validation Batch 14, Loss: 1.133111, Accuracy: 58.04%\n",
      "Validation Batch 15, Loss: 1.169301, Accuracy: 57.60%\n",
      "Validation Batch 16, Loss: 1.149118, Accuracy: 57.71%\n",
      "Validation Batch 17, Loss: 1.244650, Accuracy: 57.08%\n",
      "Validation Batch 18, Loss: 1.120595, Accuracy: 57.29%\n",
      "Validation Batch 19, Loss: 1.213569, Accuracy: 56.91%\n",
      "Validation Batch 20, Loss: 1.152746, Accuracy: 57.11%\n",
      "Validation Batch 21, Loss: 1.161511, Accuracy: 56.99%\n",
      "Validation Batch 22, Loss: 1.206892, Accuracy: 56.75%\n",
      "Validation Batch 23, Loss: 1.236625, Accuracy: 56.32%\n",
      "Validation Batch 24, Loss: 1.161432, Accuracy: 56.45%\n",
      "Validation Batch 25, Loss: 1.144145, Accuracy: 56.56%\n",
      "Validation Batch 26, Loss: 1.104029, Accuracy: 56.67%\n",
      "Validation Batch 27, Loss: 1.170024, Accuracy: 56.66%\n",
      "Validation - Epoch 30, Loss: 1.162983, Accuracy: 56.66%\n",
      "Patienceâ€”1\n",
      "Epoch 31\n",
      "Batch 1, Loss: 1.131929, Accuracy: 59.38%\n",
      "Batch 2, Loss: 1.040056, Accuracy: 66.41%\n",
      "Batch 3, Loss: 1.022728, Accuracy: 68.23%\n",
      "Batch 4, Loss: 0.990033, Accuracy: 69.53%\n",
      "Batch 5, Loss: 1.079211, Accuracy: 69.06%\n",
      "Batch 6, Loss: 1.137922, Accuracy: 67.45%\n",
      "Batch 7, Loss: 1.011481, Accuracy: 68.30%\n",
      "Batch 8, Loss: 0.954988, Accuracy: 69.53%\n",
      "Batch 9, Loss: 1.052039, Accuracy: 69.62%\n",
      "Batch 10, Loss: 1.052658, Accuracy: 69.53%\n",
      "Batch 11, Loss: 0.984805, Accuracy: 70.03%\n",
      "Batch 12, Loss: 1.102454, Accuracy: 69.79%\n",
      "Batch 13, Loss: 1.068592, Accuracy: 69.59%\n",
      "Batch 14, Loss: 1.079500, Accuracy: 69.31%\n",
      "Batch 15, Loss: 1.054369, Accuracy: 69.17%\n",
      "Batch 16, Loss: 1.060954, Accuracy: 69.04%\n",
      "Batch 17, Loss: 1.144369, Accuracy: 68.38%\n",
      "Batch 18, Loss: 1.034887, Accuracy: 68.58%\n",
      "Batch 19, Loss: 1.039250, Accuracy: 68.59%\n",
      "Batch 20, Loss: 1.024997, Accuracy: 68.52%\n",
      "Batch 21, Loss: 1.045266, Accuracy: 68.38%\n",
      "Batch 22, Loss: 1.096179, Accuracy: 68.32%\n",
      "Batch 23, Loss: 1.038980, Accuracy: 68.41%\n",
      "Batch 24, Loss: 1.139528, Accuracy: 68.10%\n",
      "Batch 25, Loss: 1.089389, Accuracy: 67.88%\n",
      "Batch 26, Loss: 0.970886, Accuracy: 68.33%\n",
      "Batch 27, Loss: 1.091268, Accuracy: 68.11%\n",
      "Batch 28, Loss: 0.997491, Accuracy: 68.36%\n",
      "Batch 29, Loss: 1.155491, Accuracy: 67.89%\n",
      "Batch 30, Loss: 1.059668, Accuracy: 67.97%\n",
      "Batch 31, Loss: 1.084427, Accuracy: 67.89%\n",
      "Batch 32, Loss: 1.066982, Accuracy: 67.82%\n",
      "Batch 33, Loss: 1.090447, Accuracy: 67.71%\n",
      "Batch 34, Loss: 1.001445, Accuracy: 67.97%\n",
      "Batch 35, Loss: 1.098154, Accuracy: 67.81%\n",
      "Batch 36, Loss: 0.932434, Accuracy: 68.14%\n",
      "Batch 37, Loss: 1.010151, Accuracy: 68.33%\n",
      "Batch 38, Loss: 1.097680, Accuracy: 68.26%\n",
      "Batch 39, Loss: 1.032123, Accuracy: 68.31%\n",
      "Batch 40, Loss: 1.058471, Accuracy: 68.40%\n",
      "Batch 41, Loss: 1.058309, Accuracy: 68.37%\n",
      "Batch 42, Loss: 0.962338, Accuracy: 68.60%\n",
      "Batch 43, Loss: 1.051797, Accuracy: 68.68%\n",
      "Batch 44, Loss: 1.017971, Accuracy: 68.71%\n",
      "Batch 45, Loss: 0.986300, Accuracy: 68.85%\n",
      "Batch 46, Loss: 1.087557, Accuracy: 68.72%\n",
      "Batch 47, Loss: 0.944030, Accuracy: 68.92%\n",
      "Batch 48, Loss: 1.030601, Accuracy: 68.95%\n",
      "Batch 49, Loss: 1.073566, Accuracy: 68.88%\n",
      "Batch 50, Loss: 1.000332, Accuracy: 69.03%\n",
      "Batch 51, Loss: 1.052391, Accuracy: 69.03%\n",
      "Batch 52, Loss: 1.012545, Accuracy: 69.08%\n",
      "Batch 53, Loss: 1.137573, Accuracy: 68.93%\n",
      "Batch 54, Loss: 1.043069, Accuracy: 68.95%\n",
      "Batch 55, Loss: 1.114593, Accuracy: 68.84%\n",
      "Batch 56, Loss: 1.080650, Accuracy: 68.78%\n",
      "Batch 57, Loss: 1.092098, Accuracy: 68.72%\n",
      "Batch 58, Loss: 1.122722, Accuracy: 68.59%\n",
      "Batch 59, Loss: 1.123270, Accuracy: 68.43%\n",
      "Batch 60, Loss: 0.979870, Accuracy: 68.57%\n",
      "Batch 61, Loss: 1.022676, Accuracy: 68.65%\n",
      "Batch 62, Loss: 1.026856, Accuracy: 68.70%\n",
      "Batch 63, Loss: 0.956579, Accuracy: 68.90%\n",
      "Batch 64, Loss: 1.034801, Accuracy: 68.95%\n",
      "Batch 65, Loss: 1.069412, Accuracy: 68.92%\n",
      "Batch 66, Loss: 1.080206, Accuracy: 68.89%\n",
      "Batch 67, Loss: 0.933369, Accuracy: 69.12%\n",
      "Batch 68, Loss: 1.135711, Accuracy: 68.93%\n",
      "Batch 69, Loss: 1.094743, Accuracy: 68.89%\n",
      "Batch 70, Loss: 1.181549, Accuracy: 68.64%\n",
      "Batch 71, Loss: 1.099841, Accuracy: 68.55%\n",
      "Batch 72, Loss: 1.042484, Accuracy: 68.60%\n",
      "Batch 73, Loss: 1.040225, Accuracy: 68.62%\n",
      "Batch 74, Loss: 1.091677, Accuracy: 68.58%\n",
      "Batch 75, Loss: 0.981759, Accuracy: 68.69%\n",
      "Batch 76, Loss: 1.034139, Accuracy: 68.71%\n",
      "Batch 77, Loss: 1.132827, Accuracy: 68.59%\n",
      "Batch 78, Loss: 1.038252, Accuracy: 68.61%\n",
      "Batch 79, Loss: 1.013706, Accuracy: 68.67%\n",
      "Batch 80, Loss: 1.179325, Accuracy: 68.50%\n",
      "Batch 81, Loss: 1.098823, Accuracy: 68.44%\n",
      "Batch 82, Loss: 1.044854, Accuracy: 68.46%\n",
      "Batch 83, Loss: 1.058163, Accuracy: 68.51%\n",
      "Batch 84, Loss: 1.040166, Accuracy: 68.53%\n",
      "Batch 85, Loss: 1.059929, Accuracy: 68.53%\n",
      "Batch 86, Loss: 1.094955, Accuracy: 68.50%\n",
      "Batch 87, Loss: 1.027506, Accuracy: 68.53%\n",
      "Batch 88, Loss: 0.982571, Accuracy: 68.64%\n",
      "Batch 89, Loss: 1.050287, Accuracy: 68.63%\n",
      "Batch 90, Loss: 0.996136, Accuracy: 68.70%\n",
      "Batch 91, Loss: 1.138545, Accuracy: 68.60%\n",
      "Batch 92, Loss: 0.949974, Accuracy: 68.73%\n",
      "Batch 93, Loss: 1.053151, Accuracy: 68.73%\n",
      "Batch 94, Loss: 1.109210, Accuracy: 68.68%\n",
      "Batch 95, Loss: 1.051920, Accuracy: 68.70%\n",
      "Batch 96, Loss: 1.094187, Accuracy: 68.65%\n",
      "Batch 97, Loss: 1.034663, Accuracy: 68.67%\n",
      "Batch 98, Loss: 1.081037, Accuracy: 68.64%\n",
      "Batch 99, Loss: 1.005316, Accuracy: 68.67%\n",
      "Batch 100, Loss: 1.125179, Accuracy: 68.58%\n",
      "Batch 101, Loss: 0.979462, Accuracy: 68.64%\n",
      "Batch 102, Loss: 1.124360, Accuracy: 68.52%\n",
      "Batch 103, Loss: 0.975911, Accuracy: 68.57%\n",
      "Batch 104, Loss: 1.126091, Accuracy: 68.51%\n",
      "Batch 105, Loss: 1.022849, Accuracy: 68.57%\n",
      "Batch 106, Loss: 1.057365, Accuracy: 68.56%\n",
      "Batch 107, Loss: 1.011195, Accuracy: 68.60%\n",
      "Batch 108, Loss: 1.025265, Accuracy: 68.65%\n",
      "Batch 109, Loss: 1.067267, Accuracy: 68.64%\n",
      "Batch 110, Loss: 1.014084, Accuracy: 68.66%\n",
      "Batch 111, Loss: 1.001142, Accuracy: 68.72%\n",
      "Batch 112, Loss: 1.018109, Accuracy: 68.78%\n",
      "Batch 113, Loss: 1.045862, Accuracy: 68.78%\n",
      "Batch 114, Loss: 1.082626, Accuracy: 68.78%\n",
      "Batch 115, Loss: 1.017858, Accuracy: 68.79%\n",
      "Batch 116, Loss: 1.039469, Accuracy: 68.79%\n",
      "Batch 117, Loss: 1.196723, Accuracy: 68.70%\n",
      "Batch 118, Loss: 0.976536, Accuracy: 68.76%\n",
      "Batch 119, Loss: 1.033112, Accuracy: 68.76%\n",
      "Batch 120, Loss: 1.055445, Accuracy: 68.79%\n",
      "Batch 121, Loss: 1.001482, Accuracy: 68.84%\n",
      "Batch 122, Loss: 0.985705, Accuracy: 68.88%\n",
      "Batch 123, Loss: 1.062034, Accuracy: 68.88%\n",
      "Batch 124, Loss: 1.143308, Accuracy: 68.78%\n",
      "Batch 125, Loss: 1.082544, Accuracy: 68.75%\n",
      "Batch 126, Loss: 0.976123, Accuracy: 68.82%\n",
      "Batch 127, Loss: 1.060903, Accuracy: 68.81%\n",
      "Batch 128, Loss: 1.091388, Accuracy: 68.76%\n",
      "Batch 129, Loss: 0.992871, Accuracy: 68.82%\n",
      "Batch 130, Loss: 1.053793, Accuracy: 68.81%\n",
      "Batch 131, Loss: 1.022985, Accuracy: 68.82%\n",
      "Batch 132, Loss: 1.112913, Accuracy: 68.79%\n",
      "Batch 133, Loss: 0.968397, Accuracy: 68.86%\n",
      "Batch 134, Loss: 1.090895, Accuracy: 68.82%\n",
      "Batch 135, Loss: 1.073428, Accuracy: 68.80%\n",
      "Batch 136, Loss: 1.028454, Accuracy: 68.80%\n",
      "Batch 137, Loss: 0.993319, Accuracy: 68.84%\n",
      "Batch 138, Loss: 1.020458, Accuracy: 68.89%\n",
      "Batch 139, Loss: 1.099799, Accuracy: 68.84%\n",
      "Batch 140, Loss: 0.988483, Accuracy: 68.90%\n",
      "Batch 141, Loss: 1.062591, Accuracy: 68.88%\n",
      "Batch 142, Loss: 1.071088, Accuracy: 68.87%\n",
      "Batch 143, Loss: 1.094636, Accuracy: 68.82%\n",
      "Batch 144, Loss: 1.056898, Accuracy: 68.82%\n",
      "Batch 145, Loss: 1.012719, Accuracy: 68.84%\n",
      "Batch 146, Loss: 1.020061, Accuracy: 68.86%\n",
      "Batch 147, Loss: 1.125154, Accuracy: 68.79%\n",
      "Batch 148, Loss: 1.013451, Accuracy: 68.82%\n",
      "Batch 149, Loss: 0.987077, Accuracy: 68.88%\n",
      "Batch 150, Loss: 1.047184, Accuracy: 68.89%\n",
      "Batch 151, Loss: 1.108283, Accuracy: 68.84%\n",
      "Batch 152, Loss: 1.082697, Accuracy: 68.80%\n",
      "Batch 153, Loss: 1.112032, Accuracy: 68.77%\n",
      "Batch 154, Loss: 1.012140, Accuracy: 68.79%\n",
      "Batch 155, Loss: 0.989242, Accuracy: 68.83%\n",
      "Batch 156, Loss: 1.004849, Accuracy: 68.85%\n",
      "Batch 157, Loss: 1.029073, Accuracy: 68.87%\n",
      "Batch 158, Loss: 1.072886, Accuracy: 68.85%\n",
      "Batch 159, Loss: 1.022056, Accuracy: 68.87%\n",
      "Batch 160, Loss: 1.088019, Accuracy: 68.85%\n",
      "Batch 161, Loss: 1.056820, Accuracy: 68.85%\n",
      "Batch 162, Loss: 0.988812, Accuracy: 68.88%\n",
      "Batch 163, Loss: 0.979834, Accuracy: 68.93%\n",
      "Batch 164, Loss: 1.044844, Accuracy: 68.95%\n",
      "Batch 165, Loss: 1.046032, Accuracy: 68.97%\n",
      "Batch 166, Loss: 1.006808, Accuracy: 69.01%\n",
      "Batch 167, Loss: 1.031646, Accuracy: 69.03%\n",
      "Batch 168, Loss: 1.067583, Accuracy: 69.05%\n",
      "Batch 169, Loss: 1.082187, Accuracy: 69.01%\n",
      "Batch 170, Loss: 0.993697, Accuracy: 69.06%\n",
      "Batch 171, Loss: 1.153432, Accuracy: 69.00%\n",
      "Batch 172, Loss: 1.079991, Accuracy: 68.99%\n",
      "Batch 173, Loss: 0.941776, Accuracy: 69.05%\n",
      "Batch 174, Loss: 1.106361, Accuracy: 69.04%\n",
      "Batch 175, Loss: 1.077680, Accuracy: 69.02%\n",
      "Batch 176, Loss: 1.058581, Accuracy: 69.03%\n",
      "Batch 177, Loss: 1.013610, Accuracy: 69.07%\n",
      "Batch 178, Loss: 1.032062, Accuracy: 69.07%\n",
      "Batch 179, Loss: 1.123968, Accuracy: 69.02%\n",
      "Batch 180, Loss: 1.049276, Accuracy: 69.02%\n",
      "Batch 181, Loss: 1.105324, Accuracy: 68.97%\n",
      "Batch 182, Loss: 1.169648, Accuracy: 68.90%\n",
      "Batch 183, Loss: 1.000359, Accuracy: 68.92%\n",
      "Batch 184, Loss: 1.083076, Accuracy: 68.91%\n",
      "Batch 185, Loss: 1.004328, Accuracy: 68.94%\n",
      "Batch 186, Loss: 1.108228, Accuracy: 68.91%\n",
      "Batch 187, Loss: 1.139220, Accuracy: 68.87%\n",
      "Batch 188, Loss: 1.076576, Accuracy: 68.87%\n",
      "Batch 189, Loss: 1.095884, Accuracy: 68.84%\n",
      "Batch 190, Loss: 1.132182, Accuracy: 68.79%\n",
      "Batch 191, Loss: 1.009843, Accuracy: 68.82%\n",
      "Batch 192, Loss: 1.014320, Accuracy: 68.86%\n",
      "Batch 193, Loss: 1.087893, Accuracy: 68.85%\n",
      "Batch 194, Loss: 1.008135, Accuracy: 68.87%\n",
      "Batch 195, Loss: 0.966569, Accuracy: 68.92%\n",
      "Batch 196, Loss: 0.944855, Accuracy: 68.96%\n",
      "Batch 197, Loss: 1.038462, Accuracy: 68.96%\n",
      "Batch 198, Loss: 1.142591, Accuracy: 68.91%\n",
      "Batch 199, Loss: 1.102799, Accuracy: 68.87%\n",
      "Batch 200, Loss: 1.158056, Accuracy: 68.81%\n",
      "Batch 201, Loss: 1.017198, Accuracy: 68.80%\n",
      "Batch 202, Loss: 0.995609, Accuracy: 68.84%\n",
      "Batch 203, Loss: 1.107038, Accuracy: 68.80%\n",
      "Batch 204, Loss: 1.050319, Accuracy: 68.81%\n",
      "Batch 205, Loss: 1.059528, Accuracy: 68.80%\n",
      "Batch 206, Loss: 0.999138, Accuracy: 68.82%\n",
      "Batch 207, Loss: 1.094654, Accuracy: 68.80%\n",
      "Batch 208, Loss: 1.103749, Accuracy: 68.74%\n",
      "Batch 209, Loss: 1.116496, Accuracy: 68.71%\n",
      "Batch 210, Loss: 0.961518, Accuracy: 68.76%\n",
      "Batch 211, Loss: 1.131742, Accuracy: 68.75%\n",
      "Batch 212, Loss: 1.002454, Accuracy: 68.77%\n",
      "Batch 213, Loss: 1.022174, Accuracy: 68.77%\n",
      "Training - Epoch 31, Loss: 1.052578, Accuracy: 68.77%\n",
      "Validation Batch 1, Loss: 1.066073, Accuracy: 67.19%\n",
      "Validation Batch 2, Loss: 1.190341, Accuracy: 59.38%\n",
      "Validation Batch 3, Loss: 1.205295, Accuracy: 57.29%\n",
      "Validation Batch 4, Loss: 1.071026, Accuracy: 59.77%\n",
      "Validation Batch 5, Loss: 1.147698, Accuracy: 59.38%\n",
      "Validation Batch 6, Loss: 1.105921, Accuracy: 59.90%\n",
      "Validation Batch 7, Loss: 1.133903, Accuracy: 60.04%\n",
      "Validation Batch 8, Loss: 1.152261, Accuracy: 60.16%\n",
      "Validation Batch 9, Loss: 1.174377, Accuracy: 59.72%\n",
      "Validation Batch 10, Loss: 1.132915, Accuracy: 59.84%\n",
      "Validation Batch 11, Loss: 1.100952, Accuracy: 60.23%\n",
      "Validation Batch 12, Loss: 1.080239, Accuracy: 60.42%\n",
      "Validation Batch 13, Loss: 1.193266, Accuracy: 60.10%\n",
      "Validation Batch 14, Loss: 1.118445, Accuracy: 60.16%\n",
      "Validation Batch 15, Loss: 1.151454, Accuracy: 59.79%\n",
      "Validation Batch 16, Loss: 1.116650, Accuracy: 59.86%\n",
      "Validation Batch 17, Loss: 1.221320, Accuracy: 59.10%\n",
      "Validation Batch 18, Loss: 1.091691, Accuracy: 59.20%\n",
      "Validation Batch 19, Loss: 1.187395, Accuracy: 58.88%\n",
      "Validation Batch 20, Loss: 1.133961, Accuracy: 58.98%\n",
      "Validation Batch 21, Loss: 1.137505, Accuracy: 58.85%\n",
      "Validation Batch 22, Loss: 1.180962, Accuracy: 58.66%\n",
      "Validation Batch 23, Loss: 1.217242, Accuracy: 58.36%\n",
      "Validation Batch 24, Loss: 1.144678, Accuracy: 58.46%\n",
      "Validation Batch 25, Loss: 1.122064, Accuracy: 58.56%\n",
      "Validation Batch 26, Loss: 1.092005, Accuracy: 58.77%\n",
      "Validation Batch 27, Loss: 1.144692, Accuracy: 58.72%\n",
      "Validation - Epoch 31, Loss: 1.141272, Accuracy: 58.72%\n",
      "Patienceâ€”0\n",
      "Epoch 32\n",
      "Batch 1, Loss: 1.131974, Accuracy: 62.50%\n",
      "Batch 2, Loss: 0.980074, Accuracy: 69.53%\n",
      "Batch 3, Loss: 1.042557, Accuracy: 70.31%\n",
      "Batch 4, Loss: 1.028527, Accuracy: 70.70%\n",
      "Batch 5, Loss: 1.065188, Accuracy: 70.31%\n",
      "Batch 6, Loss: 1.071760, Accuracy: 69.79%\n",
      "Batch 7, Loss: 1.002478, Accuracy: 70.09%\n",
      "Batch 8, Loss: 0.966023, Accuracy: 71.09%\n",
      "Batch 9, Loss: 1.002530, Accuracy: 71.35%\n",
      "Batch 10, Loss: 0.955101, Accuracy: 72.34%\n",
      "Batch 11, Loss: 1.048560, Accuracy: 71.88%\n",
      "Batch 12, Loss: 1.057112, Accuracy: 71.74%\n",
      "Batch 13, Loss: 1.013014, Accuracy: 71.88%\n",
      "Batch 14, Loss: 0.956687, Accuracy: 72.32%\n",
      "Batch 15, Loss: 1.020794, Accuracy: 72.40%\n",
      "Batch 16, Loss: 0.995374, Accuracy: 72.56%\n",
      "Batch 17, Loss: 1.020887, Accuracy: 72.52%\n",
      "Batch 18, Loss: 1.007418, Accuracy: 72.57%\n",
      "Batch 19, Loss: 1.115241, Accuracy: 71.96%\n",
      "Batch 20, Loss: 1.117021, Accuracy: 71.48%\n",
      "Batch 21, Loss: 1.039407, Accuracy: 71.43%\n",
      "Batch 22, Loss: 1.179176, Accuracy: 70.74%\n",
      "Batch 23, Loss: 1.048307, Accuracy: 70.58%\n",
      "Batch 24, Loss: 1.021237, Accuracy: 70.64%\n",
      "Batch 25, Loss: 1.088634, Accuracy: 70.44%\n",
      "Batch 26, Loss: 1.073481, Accuracy: 70.25%\n",
      "Batch 27, Loss: 1.154774, Accuracy: 69.79%\n",
      "Batch 28, Loss: 1.053688, Accuracy: 69.70%\n",
      "Batch 29, Loss: 0.978143, Accuracy: 69.99%\n",
      "Batch 30, Loss: 1.100322, Accuracy: 69.79%\n",
      "Batch 31, Loss: 1.011698, Accuracy: 69.91%\n",
      "Batch 32, Loss: 1.115501, Accuracy: 69.58%\n",
      "Batch 33, Loss: 1.139585, Accuracy: 69.27%\n",
      "Batch 34, Loss: 1.049210, Accuracy: 69.30%\n",
      "Batch 35, Loss: 1.067686, Accuracy: 69.24%\n",
      "Batch 36, Loss: 1.053854, Accuracy: 69.23%\n",
      "Batch 37, Loss: 0.997464, Accuracy: 69.38%\n",
      "Batch 38, Loss: 1.060107, Accuracy: 69.28%\n",
      "Batch 39, Loss: 1.066703, Accuracy: 69.27%\n",
      "Batch 40, Loss: 1.117488, Accuracy: 69.06%\n",
      "Batch 41, Loss: 1.003121, Accuracy: 69.21%\n",
      "Batch 42, Loss: 1.070100, Accuracy: 69.20%\n",
      "Batch 43, Loss: 1.111583, Accuracy: 69.08%\n",
      "Batch 44, Loss: 1.093261, Accuracy: 68.93%\n",
      "Batch 45, Loss: 1.136292, Accuracy: 68.65%\n",
      "Batch 46, Loss: 1.006557, Accuracy: 68.75%\n",
      "Batch 47, Loss: 0.990076, Accuracy: 68.88%\n",
      "Batch 48, Loss: 1.070907, Accuracy: 68.82%\n",
      "Batch 49, Loss: 1.001260, Accuracy: 68.94%\n",
      "Batch 50, Loss: 1.013220, Accuracy: 69.03%\n",
      "Batch 51, Loss: 1.035301, Accuracy: 69.09%\n",
      "Batch 52, Loss: 1.104894, Accuracy: 68.96%\n",
      "Batch 53, Loss: 1.046718, Accuracy: 68.99%\n",
      "Batch 54, Loss: 1.074553, Accuracy: 68.92%\n",
      "Batch 55, Loss: 1.097982, Accuracy: 68.78%\n",
      "Batch 56, Loss: 0.972616, Accuracy: 68.95%\n",
      "Batch 57, Loss: 1.022047, Accuracy: 69.00%\n",
      "Batch 58, Loss: 1.121684, Accuracy: 68.80%\n",
      "Batch 59, Loss: 0.992149, Accuracy: 68.88%\n",
      "Batch 60, Loss: 0.984818, Accuracy: 69.04%\n",
      "Batch 61, Loss: 1.080959, Accuracy: 68.98%\n",
      "Batch 62, Loss: 1.065603, Accuracy: 68.95%\n",
      "Batch 63, Loss: 1.102980, Accuracy: 68.87%\n",
      "Batch 64, Loss: 1.028434, Accuracy: 68.87%\n",
      "Batch 65, Loss: 1.043563, Accuracy: 68.92%\n",
      "Batch 66, Loss: 1.002924, Accuracy: 68.99%\n",
      "Batch 67, Loss: 1.018077, Accuracy: 69.03%\n",
      "Batch 68, Loss: 1.111109, Accuracy: 68.91%\n",
      "Batch 69, Loss: 0.996814, Accuracy: 69.00%\n",
      "Batch 70, Loss: 1.089156, Accuracy: 68.95%\n",
      "Batch 71, Loss: 1.001026, Accuracy: 69.01%\n",
      "Batch 72, Loss: 0.995677, Accuracy: 69.12%\n",
      "Batch 73, Loss: 1.011739, Accuracy: 69.16%\n",
      "Batch 74, Loss: 1.017665, Accuracy: 69.21%\n",
      "Batch 75, Loss: 1.079668, Accuracy: 69.08%\n",
      "Batch 76, Loss: 0.957816, Accuracy: 69.22%\n",
      "Batch 77, Loss: 0.922030, Accuracy: 69.42%\n",
      "Batch 78, Loss: 1.023660, Accuracy: 69.45%\n",
      "Batch 79, Loss: 0.997144, Accuracy: 69.52%\n",
      "Batch 80, Loss: 1.033970, Accuracy: 69.53%\n",
      "Batch 81, Loss: 1.058937, Accuracy: 69.52%\n",
      "Batch 82, Loss: 1.169085, Accuracy: 69.30%\n",
      "Batch 83, Loss: 1.094655, Accuracy: 69.24%\n",
      "Batch 84, Loss: 1.036196, Accuracy: 69.23%\n",
      "Batch 85, Loss: 1.108923, Accuracy: 69.17%\n",
      "Batch 86, Loss: 1.037343, Accuracy: 69.19%\n",
      "Batch 87, Loss: 1.104558, Accuracy: 69.13%\n",
      "Batch 88, Loss: 0.914951, Accuracy: 69.30%\n",
      "Batch 89, Loss: 1.018255, Accuracy: 69.35%\n",
      "Batch 90, Loss: 1.071091, Accuracy: 69.34%\n",
      "Batch 91, Loss: 1.051106, Accuracy: 69.33%\n",
      "Batch 92, Loss: 1.065232, Accuracy: 69.31%\n",
      "Batch 93, Loss: 0.985409, Accuracy: 69.37%\n",
      "Batch 94, Loss: 1.044220, Accuracy: 69.37%\n",
      "Batch 95, Loss: 1.082449, Accuracy: 69.33%\n",
      "Batch 96, Loss: 1.012074, Accuracy: 69.37%\n",
      "Batch 97, Loss: 1.015321, Accuracy: 69.38%\n",
      "Batch 98, Loss: 1.218064, Accuracy: 69.20%\n",
      "Batch 99, Loss: 1.171431, Accuracy: 69.08%\n",
      "Batch 100, Loss: 1.015306, Accuracy: 69.14%\n",
      "Batch 101, Loss: 1.091268, Accuracy: 69.07%\n",
      "Batch 102, Loss: 1.069209, Accuracy: 69.07%\n",
      "Batch 103, Loss: 1.082282, Accuracy: 69.04%\n",
      "Batch 104, Loss: 1.104141, Accuracy: 69.01%\n",
      "Batch 105, Loss: 1.020975, Accuracy: 69.06%\n",
      "Batch 106, Loss: 1.098394, Accuracy: 68.99%\n",
      "Batch 107, Loss: 1.061170, Accuracy: 68.97%\n",
      "Batch 108, Loss: 1.067933, Accuracy: 68.94%\n",
      "Batch 109, Loss: 1.116677, Accuracy: 68.89%\n",
      "Batch 110, Loss: 1.059001, Accuracy: 68.89%\n",
      "Batch 111, Loss: 1.057765, Accuracy: 68.88%\n",
      "Batch 112, Loss: 1.035333, Accuracy: 68.89%\n",
      "Batch 113, Loss: 1.041004, Accuracy: 68.90%\n",
      "Batch 114, Loss: 1.105533, Accuracy: 68.86%\n",
      "Batch 115, Loss: 1.041580, Accuracy: 68.90%\n",
      "Batch 116, Loss: 0.951620, Accuracy: 69.01%\n",
      "Batch 117, Loss: 1.018209, Accuracy: 69.06%\n",
      "Batch 118, Loss: 1.046781, Accuracy: 69.05%\n",
      "Batch 119, Loss: 1.074509, Accuracy: 69.01%\n",
      "Batch 120, Loss: 0.993164, Accuracy: 69.06%\n",
      "Batch 121, Loss: 1.040146, Accuracy: 69.07%\n",
      "Batch 122, Loss: 1.033581, Accuracy: 69.08%\n",
      "Batch 123, Loss: 1.073266, Accuracy: 69.07%\n",
      "Batch 124, Loss: 1.057054, Accuracy: 69.04%\n",
      "Batch 125, Loss: 1.033492, Accuracy: 69.06%\n",
      "Batch 126, Loss: 1.054949, Accuracy: 69.06%\n",
      "Batch 127, Loss: 1.028267, Accuracy: 69.07%\n",
      "Batch 128, Loss: 1.053267, Accuracy: 69.08%\n",
      "Batch 129, Loss: 1.129670, Accuracy: 69.00%\n",
      "Batch 130, Loss: 1.044958, Accuracy: 69.03%\n",
      "Batch 131, Loss: 1.037000, Accuracy: 69.02%\n",
      "Batch 132, Loss: 1.122566, Accuracy: 68.96%\n",
      "Batch 133, Loss: 0.998180, Accuracy: 69.01%\n",
      "Batch 134, Loss: 1.055008, Accuracy: 68.99%\n",
      "Batch 135, Loss: 1.069987, Accuracy: 68.97%\n",
      "Batch 136, Loss: 1.077430, Accuracy: 68.93%\n",
      "Batch 137, Loss: 1.054841, Accuracy: 68.93%\n",
      "Batch 138, Loss: 1.159056, Accuracy: 68.84%\n",
      "Batch 139, Loss: 1.071157, Accuracy: 68.84%\n",
      "Batch 140, Loss: 1.067478, Accuracy: 68.83%\n",
      "Batch 141, Loss: 1.083335, Accuracy: 68.81%\n",
      "Batch 142, Loss: 0.966336, Accuracy: 68.87%\n",
      "Batch 143, Loss: 1.029808, Accuracy: 68.89%\n",
      "Batch 144, Loss: 1.015995, Accuracy: 68.92%\n",
      "Batch 145, Loss: 1.031225, Accuracy: 68.93%\n",
      "Batch 146, Loss: 1.122015, Accuracy: 68.88%\n",
      "Batch 147, Loss: 0.972533, Accuracy: 68.94%\n",
      "Batch 148, Loss: 1.025180, Accuracy: 68.97%\n",
      "Batch 149, Loss: 1.017514, Accuracy: 69.01%\n",
      "Batch 150, Loss: 1.006621, Accuracy: 69.03%\n",
      "Batch 151, Loss: 0.996734, Accuracy: 69.08%\n",
      "Batch 152, Loss: 1.099679, Accuracy: 69.05%\n",
      "Batch 153, Loss: 1.072897, Accuracy: 69.03%\n",
      "Batch 154, Loss: 1.017769, Accuracy: 69.05%\n",
      "Batch 155, Loss: 1.096364, Accuracy: 69.03%\n",
      "Batch 156, Loss: 1.052021, Accuracy: 69.03%\n",
      "Batch 157, Loss: 1.114666, Accuracy: 68.98%\n",
      "Batch 158, Loss: 0.964349, Accuracy: 69.06%\n",
      "Batch 159, Loss: 1.056303, Accuracy: 69.05%\n",
      "Batch 160, Loss: 1.106648, Accuracy: 69.01%\n",
      "Batch 161, Loss: 1.077097, Accuracy: 68.99%\n",
      "Batch 162, Loss: 0.971631, Accuracy: 69.04%\n",
      "Batch 163, Loss: 1.019790, Accuracy: 69.05%\n",
      "Batch 164, Loss: 1.054875, Accuracy: 69.04%\n",
      "Batch 165, Loss: 1.022983, Accuracy: 69.05%\n",
      "Batch 166, Loss: 0.989686, Accuracy: 69.10%\n",
      "Batch 167, Loss: 1.031955, Accuracy: 69.11%\n",
      "Batch 168, Loss: 1.116625, Accuracy: 69.05%\n",
      "Batch 169, Loss: 0.990657, Accuracy: 69.08%\n",
      "Batch 170, Loss: 1.080368, Accuracy: 69.07%\n",
      "Batch 171, Loss: 1.004300, Accuracy: 69.11%\n",
      "Batch 172, Loss: 1.014527, Accuracy: 69.13%\n",
      "Batch 173, Loss: 1.019872, Accuracy: 69.14%\n",
      "Batch 174, Loss: 1.014805, Accuracy: 69.17%\n",
      "Batch 175, Loss: 1.013863, Accuracy: 69.20%\n",
      "Batch 176, Loss: 1.080266, Accuracy: 69.17%\n",
      "Batch 177, Loss: 1.099190, Accuracy: 69.15%\n",
      "Batch 178, Loss: 0.906351, Accuracy: 69.22%\n",
      "Batch 179, Loss: 1.039055, Accuracy: 69.23%\n",
      "Batch 180, Loss: 0.961989, Accuracy: 69.28%\n",
      "Batch 181, Loss: 1.053494, Accuracy: 69.26%\n",
      "Batch 182, Loss: 1.042169, Accuracy: 69.27%\n",
      "Batch 183, Loss: 1.059562, Accuracy: 69.25%\n",
      "Batch 184, Loss: 1.055873, Accuracy: 69.24%\n",
      "Batch 185, Loss: 1.022136, Accuracy: 69.27%\n",
      "Batch 186, Loss: 1.105837, Accuracy: 69.24%\n",
      "Batch 187, Loss: 1.068720, Accuracy: 69.21%\n",
      "Batch 188, Loss: 1.053161, Accuracy: 69.21%\n",
      "Batch 189, Loss: 1.181941, Accuracy: 69.13%\n",
      "Batch 190, Loss: 1.064784, Accuracy: 69.12%\n",
      "Batch 191, Loss: 1.110326, Accuracy: 69.09%\n",
      "Batch 192, Loss: 1.033456, Accuracy: 69.08%\n",
      "Batch 193, Loss: 1.106164, Accuracy: 69.05%\n",
      "Batch 194, Loss: 1.002807, Accuracy: 69.08%\n",
      "Batch 195, Loss: 0.976542, Accuracy: 69.11%\n",
      "Batch 196, Loss: 1.034108, Accuracy: 69.12%\n",
      "Batch 197, Loss: 1.087363, Accuracy: 69.11%\n",
      "Batch 198, Loss: 0.998816, Accuracy: 69.14%\n",
      "Batch 199, Loss: 1.029028, Accuracy: 69.14%\n",
      "Batch 200, Loss: 0.985602, Accuracy: 69.18%\n",
      "Batch 201, Loss: 1.091142, Accuracy: 69.15%\n",
      "Batch 202, Loss: 1.084347, Accuracy: 69.14%\n",
      "Batch 203, Loss: 1.048646, Accuracy: 69.13%\n",
      "Batch 204, Loss: 0.999725, Accuracy: 69.16%\n",
      "Batch 205, Loss: 1.132208, Accuracy: 69.13%\n",
      "Batch 206, Loss: 1.047916, Accuracy: 69.13%\n",
      "Batch 207, Loss: 1.020469, Accuracy: 69.13%\n",
      "Batch 208, Loss: 1.134336, Accuracy: 69.08%\n",
      "Batch 209, Loss: 1.050126, Accuracy: 69.07%\n",
      "Batch 210, Loss: 1.067466, Accuracy: 69.07%\n",
      "Batch 211, Loss: 1.086295, Accuracy: 69.05%\n",
      "Batch 212, Loss: 1.046349, Accuracy: 69.04%\n",
      "Batch 213, Loss: 1.084834, Accuracy: 69.02%\n",
      "Training - Epoch 32, Loss: 1.049484, Accuracy: 69.02%\n",
      "Validation Batch 1, Loss: 1.091779, Accuracy: 64.06%\n",
      "Validation Batch 2, Loss: 1.214691, Accuracy: 56.25%\n",
      "Validation Batch 3, Loss: 1.234194, Accuracy: 54.17%\n",
      "Validation Batch 4, Loss: 1.093763, Accuracy: 57.42%\n",
      "Validation Batch 5, Loss: 1.177930, Accuracy: 56.56%\n",
      "Validation Batch 6, Loss: 1.135772, Accuracy: 57.03%\n",
      "Validation Batch 7, Loss: 1.154135, Accuracy: 57.37%\n",
      "Validation Batch 8, Loss: 1.165317, Accuracy: 57.23%\n",
      "Validation Batch 9, Loss: 1.191964, Accuracy: 56.77%\n",
      "Validation Batch 10, Loss: 1.168416, Accuracy: 56.88%\n",
      "Validation Batch 11, Loss: 1.133285, Accuracy: 56.96%\n",
      "Validation Batch 12, Loss: 1.099296, Accuracy: 57.55%\n",
      "Validation Batch 13, Loss: 1.216907, Accuracy: 57.21%\n",
      "Validation Batch 14, Loss: 1.139483, Accuracy: 57.25%\n",
      "Validation Batch 15, Loss: 1.173260, Accuracy: 57.08%\n",
      "Validation Batch 16, Loss: 1.146463, Accuracy: 57.03%\n",
      "Validation Batch 17, Loss: 1.248402, Accuracy: 56.34%\n",
      "Validation Batch 18, Loss: 1.125475, Accuracy: 56.60%\n",
      "Validation Batch 19, Loss: 1.218476, Accuracy: 56.25%\n",
      "Validation Batch 20, Loss: 1.161925, Accuracy: 56.17%\n",
      "Validation Batch 21, Loss: 1.158593, Accuracy: 56.18%\n",
      "Validation Batch 22, Loss: 1.211107, Accuracy: 55.97%\n",
      "Validation Batch 23, Loss: 1.242132, Accuracy: 55.50%\n",
      "Validation Batch 24, Loss: 1.163692, Accuracy: 55.66%\n",
      "Validation Batch 25, Loss: 1.147487, Accuracy: 55.81%\n",
      "Validation Batch 26, Loss: 1.111002, Accuracy: 56.01%\n",
      "Validation Batch 27, Loss: 1.177486, Accuracy: 56.02%\n",
      "Validation - Epoch 32, Loss: 1.166757, Accuracy: 56.02%\n",
      "Patienceâ€”1\n",
      "Epoch 33\n",
      "Batch 1, Loss: 1.083233, Accuracy: 67.19%\n",
      "Batch 2, Loss: 1.050366, Accuracy: 68.75%\n",
      "Batch 3, Loss: 1.033785, Accuracy: 70.31%\n",
      "Batch 4, Loss: 1.078188, Accuracy: 69.53%\n",
      "Batch 5, Loss: 1.003468, Accuracy: 70.62%\n",
      "Batch 6, Loss: 1.064116, Accuracy: 70.31%\n",
      "Batch 7, Loss: 1.004826, Accuracy: 71.21%\n",
      "Batch 8, Loss: 1.078933, Accuracy: 70.70%\n",
      "Batch 9, Loss: 1.040664, Accuracy: 70.66%\n",
      "Batch 10, Loss: 1.090397, Accuracy: 70.00%\n",
      "Batch 11, Loss: 1.090877, Accuracy: 69.46%\n",
      "Batch 12, Loss: 1.024124, Accuracy: 69.79%\n",
      "Batch 13, Loss: 0.996218, Accuracy: 70.07%\n",
      "Batch 14, Loss: 1.058386, Accuracy: 69.75%\n",
      "Batch 15, Loss: 1.045991, Accuracy: 69.79%\n",
      "Batch 16, Loss: 1.115054, Accuracy: 69.34%\n",
      "Batch 17, Loss: 1.054090, Accuracy: 69.30%\n",
      "Batch 18, Loss: 1.049212, Accuracy: 69.18%\n",
      "Batch 19, Loss: 1.069293, Accuracy: 69.08%\n",
      "Batch 20, Loss: 1.003277, Accuracy: 69.38%\n",
      "Batch 21, Loss: 1.024766, Accuracy: 69.49%\n",
      "Batch 22, Loss: 1.083986, Accuracy: 69.32%\n",
      "Batch 23, Loss: 1.050197, Accuracy: 69.36%\n",
      "Batch 24, Loss: 1.001761, Accuracy: 69.66%\n",
      "Batch 25, Loss: 1.105175, Accuracy: 69.44%\n",
      "Batch 26, Loss: 1.077484, Accuracy: 69.29%\n",
      "Batch 27, Loss: 1.097988, Accuracy: 69.04%\n",
      "Batch 28, Loss: 0.962765, Accuracy: 69.53%\n",
      "Batch 29, Loss: 1.007232, Accuracy: 69.72%\n",
      "Batch 30, Loss: 1.079185, Accuracy: 69.53%\n",
      "Batch 31, Loss: 1.075366, Accuracy: 69.35%\n",
      "Batch 32, Loss: 1.073775, Accuracy: 69.34%\n",
      "Batch 33, Loss: 1.066949, Accuracy: 69.27%\n",
      "Batch 34, Loss: 1.036310, Accuracy: 69.21%\n",
      "Batch 35, Loss: 1.050290, Accuracy: 69.15%\n",
      "Batch 36, Loss: 1.081182, Accuracy: 69.05%\n",
      "Batch 37, Loss: 1.084980, Accuracy: 68.96%\n",
      "Batch 38, Loss: 1.151481, Accuracy: 68.63%\n",
      "Batch 39, Loss: 1.033160, Accuracy: 68.67%\n",
      "Batch 40, Loss: 1.003925, Accuracy: 68.87%\n",
      "Batch 41, Loss: 1.127956, Accuracy: 68.60%\n",
      "Batch 42, Loss: 1.032634, Accuracy: 68.68%\n",
      "Batch 43, Loss: 1.055090, Accuracy: 68.68%\n",
      "Batch 44, Loss: 1.032175, Accuracy: 68.64%\n",
      "Batch 45, Loss: 1.085500, Accuracy: 68.58%\n",
      "Batch 46, Loss: 1.079492, Accuracy: 68.51%\n",
      "Batch 47, Loss: 1.155606, Accuracy: 68.25%\n",
      "Batch 48, Loss: 1.004070, Accuracy: 68.33%\n",
      "Batch 49, Loss: 1.027504, Accuracy: 68.43%\n",
      "Batch 50, Loss: 0.992977, Accuracy: 68.59%\n",
      "Batch 51, Loss: 1.021885, Accuracy: 68.69%\n",
      "Batch 52, Loss: 1.122441, Accuracy: 68.54%\n",
      "Batch 53, Loss: 1.059043, Accuracy: 68.54%\n",
      "Batch 54, Loss: 1.043930, Accuracy: 68.55%\n",
      "Batch 55, Loss: 1.002093, Accuracy: 68.64%\n",
      "Batch 56, Loss: 1.030449, Accuracy: 68.67%\n",
      "Batch 57, Loss: 0.998688, Accuracy: 68.72%\n",
      "Batch 58, Loss: 1.042865, Accuracy: 68.75%\n",
      "Batch 59, Loss: 1.060347, Accuracy: 68.78%\n",
      "Batch 60, Loss: 1.043778, Accuracy: 68.72%\n",
      "Batch 61, Loss: 1.074569, Accuracy: 68.75%\n",
      "Batch 62, Loss: 1.089008, Accuracy: 68.72%\n",
      "Batch 63, Loss: 1.026818, Accuracy: 68.77%\n",
      "Batch 64, Loss: 1.044583, Accuracy: 68.77%\n",
      "Batch 65, Loss: 1.079630, Accuracy: 68.75%\n",
      "Batch 66, Loss: 1.043596, Accuracy: 68.77%\n",
      "Batch 67, Loss: 1.122311, Accuracy: 68.68%\n",
      "Batch 68, Loss: 1.093385, Accuracy: 68.64%\n",
      "Batch 69, Loss: 1.054044, Accuracy: 68.64%\n",
      "Batch 70, Loss: 1.078697, Accuracy: 68.64%\n",
      "Batch 71, Loss: 1.021981, Accuracy: 68.71%\n",
      "Batch 72, Loss: 0.933537, Accuracy: 68.90%\n",
      "Batch 73, Loss: 1.037249, Accuracy: 68.94%\n",
      "Batch 74, Loss: 1.072046, Accuracy: 68.88%\n",
      "Batch 75, Loss: 1.065226, Accuracy: 68.85%\n",
      "Batch 76, Loss: 1.083683, Accuracy: 68.79%\n",
      "Batch 77, Loss: 1.130438, Accuracy: 68.63%\n",
      "Batch 78, Loss: 1.136990, Accuracy: 68.55%\n",
      "Batch 79, Loss: 1.114078, Accuracy: 68.51%\n",
      "Batch 80, Loss: 1.099947, Accuracy: 68.46%\n",
      "Batch 81, Loss: 1.044474, Accuracy: 68.46%\n",
      "Batch 82, Loss: 0.986503, Accuracy: 68.54%\n",
      "Batch 83, Loss: 0.979508, Accuracy: 68.64%\n",
      "Batch 84, Loss: 0.981011, Accuracy: 68.73%\n",
      "Batch 85, Loss: 1.009483, Accuracy: 68.81%\n",
      "Batch 86, Loss: 1.073569, Accuracy: 68.73%\n",
      "Batch 87, Loss: 0.994329, Accuracy: 68.80%\n",
      "Batch 88, Loss: 1.010244, Accuracy: 68.87%\n",
      "Batch 89, Loss: 1.017264, Accuracy: 68.93%\n",
      "Batch 90, Loss: 1.000449, Accuracy: 68.99%\n",
      "Batch 91, Loss: 1.080099, Accuracy: 68.94%\n",
      "Batch 92, Loss: 1.029393, Accuracy: 68.99%\n",
      "Batch 93, Loss: 1.100860, Accuracy: 68.92%\n",
      "Batch 94, Loss: 1.012795, Accuracy: 68.97%\n",
      "Batch 95, Loss: 1.017321, Accuracy: 69.00%\n",
      "Batch 96, Loss: 1.013557, Accuracy: 69.04%\n",
      "Batch 97, Loss: 0.974274, Accuracy: 69.12%\n",
      "Batch 98, Loss: 1.073173, Accuracy: 69.12%\n",
      "Batch 99, Loss: 1.023823, Accuracy: 69.14%\n",
      "Batch 100, Loss: 1.103939, Accuracy: 69.06%\n",
      "Batch 101, Loss: 1.045336, Accuracy: 69.07%\n",
      "Batch 102, Loss: 1.164845, Accuracy: 68.95%\n",
      "Batch 103, Loss: 1.063795, Accuracy: 68.92%\n",
      "Batch 104, Loss: 0.988741, Accuracy: 68.99%\n",
      "Batch 105, Loss: 0.978595, Accuracy: 69.05%\n",
      "Batch 106, Loss: 1.022445, Accuracy: 69.07%\n",
      "Batch 107, Loss: 1.050799, Accuracy: 69.07%\n",
      "Batch 108, Loss: 1.089610, Accuracy: 69.05%\n",
      "Batch 109, Loss: 1.027842, Accuracy: 69.08%\n",
      "Batch 110, Loss: 1.041209, Accuracy: 69.08%\n",
      "Batch 111, Loss: 1.036009, Accuracy: 69.10%\n",
      "Batch 112, Loss: 1.044645, Accuracy: 69.10%\n",
      "Batch 113, Loss: 1.040261, Accuracy: 69.11%\n",
      "Batch 114, Loss: 1.037263, Accuracy: 69.13%\n",
      "Batch 115, Loss: 1.040840, Accuracy: 69.14%\n",
      "Batch 116, Loss: 1.028790, Accuracy: 69.15%\n",
      "Batch 117, Loss: 1.023700, Accuracy: 69.19%\n",
      "Batch 118, Loss: 1.001606, Accuracy: 69.24%\n",
      "Batch 119, Loss: 1.064639, Accuracy: 69.24%\n",
      "Batch 120, Loss: 1.052622, Accuracy: 69.23%\n",
      "Batch 121, Loss: 1.021729, Accuracy: 69.27%\n",
      "Batch 122, Loss: 1.075520, Accuracy: 69.24%\n",
      "Batch 123, Loss: 0.972849, Accuracy: 69.30%\n",
      "Batch 124, Loss: 1.082934, Accuracy: 69.25%\n",
      "Batch 125, Loss: 0.996905, Accuracy: 69.30%\n",
      "Batch 126, Loss: 1.047670, Accuracy: 69.32%\n",
      "Batch 127, Loss: 1.033812, Accuracy: 69.35%\n",
      "Batch 128, Loss: 1.020180, Accuracy: 69.37%\n",
      "Batch 129, Loss: 1.046319, Accuracy: 69.34%\n",
      "Batch 130, Loss: 1.067001, Accuracy: 69.31%\n",
      "Batch 131, Loss: 1.059165, Accuracy: 69.33%\n",
      "Batch 132, Loss: 0.969394, Accuracy: 69.40%\n",
      "Batch 133, Loss: 1.072697, Accuracy: 69.37%\n",
      "Batch 134, Loss: 1.026021, Accuracy: 69.39%\n",
      "Batch 135, Loss: 1.135905, Accuracy: 69.33%\n",
      "Batch 136, Loss: 1.020922, Accuracy: 69.36%\n",
      "Batch 137, Loss: 1.056045, Accuracy: 69.35%\n",
      "Batch 138, Loss: 0.955654, Accuracy: 69.43%\n",
      "Batch 139, Loss: 1.049954, Accuracy: 69.42%\n",
      "Batch 140, Loss: 1.018152, Accuracy: 69.44%\n",
      "Batch 141, Loss: 0.994376, Accuracy: 69.51%\n",
      "Batch 142, Loss: 0.955604, Accuracy: 69.58%\n",
      "Batch 143, Loss: 1.022465, Accuracy: 69.60%\n",
      "Batch 144, Loss: 1.080380, Accuracy: 69.61%\n",
      "Batch 145, Loss: 1.032456, Accuracy: 69.60%\n",
      "Batch 146, Loss: 1.016225, Accuracy: 69.62%\n",
      "Batch 147, Loss: 0.964954, Accuracy: 69.69%\n",
      "Batch 148, Loss: 1.079623, Accuracy: 69.67%\n",
      "Batch 149, Loss: 1.079471, Accuracy: 69.64%\n",
      "Batch 150, Loss: 1.016968, Accuracy: 69.67%\n",
      "Batch 151, Loss: 1.018017, Accuracy: 69.70%\n",
      "Batch 152, Loss: 1.041988, Accuracy: 69.74%\n",
      "Batch 153, Loss: 1.060658, Accuracy: 69.73%\n",
      "Batch 154, Loss: 1.065637, Accuracy: 69.73%\n",
      "Batch 155, Loss: 1.051878, Accuracy: 69.73%\n",
      "Batch 156, Loss: 0.941630, Accuracy: 69.79%\n",
      "Batch 157, Loss: 1.045733, Accuracy: 69.79%\n",
      "Batch 158, Loss: 1.116922, Accuracy: 69.75%\n",
      "Batch 159, Loss: 1.140684, Accuracy: 69.68%\n",
      "Batch 160, Loss: 1.122191, Accuracy: 69.62%\n",
      "Batch 161, Loss: 1.015472, Accuracy: 69.63%\n",
      "Batch 162, Loss: 1.083932, Accuracy: 69.60%\n",
      "Batch 163, Loss: 1.016457, Accuracy: 69.61%\n",
      "Batch 164, Loss: 1.034252, Accuracy: 69.60%\n",
      "Batch 165, Loss: 0.993971, Accuracy: 69.63%\n",
      "Batch 166, Loss: 1.113019, Accuracy: 69.57%\n",
      "Batch 167, Loss: 1.008618, Accuracy: 69.59%\n",
      "Batch 168, Loss: 1.139240, Accuracy: 69.53%\n",
      "Batch 169, Loss: 1.016329, Accuracy: 69.55%\n",
      "Batch 170, Loss: 1.083174, Accuracy: 69.52%\n",
      "Batch 171, Loss: 1.026357, Accuracy: 69.54%\n",
      "Batch 172, Loss: 1.003328, Accuracy: 69.58%\n",
      "Batch 173, Loss: 0.960763, Accuracy: 69.64%\n",
      "Batch 174, Loss: 1.082232, Accuracy: 69.59%\n",
      "Batch 175, Loss: 1.022786, Accuracy: 69.62%\n",
      "Batch 176, Loss: 1.019736, Accuracy: 69.63%\n",
      "Batch 177, Loss: 1.115690, Accuracy: 69.59%\n",
      "Batch 178, Loss: 1.147210, Accuracy: 69.50%\n",
      "Batch 179, Loss: 1.050402, Accuracy: 69.48%\n",
      "Batch 180, Loss: 0.975798, Accuracy: 69.53%\n",
      "Batch 181, Loss: 1.037688, Accuracy: 69.53%\n",
      "Batch 182, Loss: 1.055930, Accuracy: 69.51%\n",
      "Batch 183, Loss: 1.087949, Accuracy: 69.50%\n",
      "Batch 184, Loss: 1.014440, Accuracy: 69.52%\n",
      "Batch 185, Loss: 1.030950, Accuracy: 69.53%\n",
      "Batch 186, Loss: 1.015655, Accuracy: 69.56%\n",
      "Batch 187, Loss: 1.087360, Accuracy: 69.54%\n",
      "Batch 188, Loss: 1.036032, Accuracy: 69.55%\n",
      "Batch 189, Loss: 1.063233, Accuracy: 69.55%\n",
      "Batch 190, Loss: 1.046629, Accuracy: 69.55%\n",
      "Batch 191, Loss: 0.941493, Accuracy: 69.60%\n",
      "Batch 192, Loss: 0.966349, Accuracy: 69.65%\n",
      "Batch 193, Loss: 1.053252, Accuracy: 69.64%\n",
      "Batch 194, Loss: 1.078636, Accuracy: 69.62%\n",
      "Batch 195, Loss: 1.028810, Accuracy: 69.62%\n",
      "Batch 196, Loss: 1.038272, Accuracy: 69.61%\n",
      "Batch 197, Loss: 1.068354, Accuracy: 69.59%\n",
      "Batch 198, Loss: 1.065724, Accuracy: 69.58%\n",
      "Batch 199, Loss: 1.052402, Accuracy: 69.58%\n",
      "Batch 200, Loss: 1.030709, Accuracy: 69.59%\n",
      "Batch 201, Loss: 1.058941, Accuracy: 69.58%\n",
      "Batch 202, Loss: 1.012913, Accuracy: 69.59%\n",
      "Batch 203, Loss: 1.027588, Accuracy: 69.61%\n",
      "Batch 204, Loss: 1.040515, Accuracy: 69.61%\n",
      "Batch 205, Loss: 1.086218, Accuracy: 69.57%\n",
      "Batch 206, Loss: 1.000087, Accuracy: 69.60%\n",
      "Batch 207, Loss: 1.088592, Accuracy: 69.57%\n",
      "Batch 208, Loss: 1.068033, Accuracy: 69.55%\n",
      "Batch 209, Loss: 1.121246, Accuracy: 69.51%\n",
      "Batch 210, Loss: 1.021805, Accuracy: 69.52%\n",
      "Batch 211, Loss: 1.097109, Accuracy: 69.50%\n",
      "Batch 212, Loss: 0.997524, Accuracy: 69.52%\n",
      "Batch 213, Loss: 1.029628, Accuracy: 69.54%\n",
      "Training - Epoch 33, Loss: 1.046516, Accuracy: 69.54%\n",
      "Validation Batch 1, Loss: 1.056785, Accuracy: 67.19%\n",
      "Validation Batch 2, Loss: 1.186858, Accuracy: 60.16%\n",
      "Validation Batch 3, Loss: 1.191670, Accuracy: 58.33%\n",
      "Validation Batch 4, Loss: 1.066065, Accuracy: 61.33%\n",
      "Validation Batch 5, Loss: 1.135607, Accuracy: 60.62%\n",
      "Validation Batch 6, Loss: 1.091261, Accuracy: 60.94%\n",
      "Validation Batch 7, Loss: 1.126837, Accuracy: 60.94%\n",
      "Validation Batch 8, Loss: 1.137834, Accuracy: 61.33%\n",
      "Validation Batch 9, Loss: 1.160394, Accuracy: 61.11%\n",
      "Validation Batch 10, Loss: 1.123599, Accuracy: 61.25%\n",
      "Validation Batch 11, Loss: 1.096145, Accuracy: 61.22%\n",
      "Validation Batch 12, Loss: 1.057869, Accuracy: 61.85%\n",
      "Validation Batch 13, Loss: 1.182412, Accuracy: 61.54%\n",
      "Validation Batch 14, Loss: 1.113394, Accuracy: 61.50%\n",
      "Validation Batch 15, Loss: 1.140069, Accuracy: 61.25%\n",
      "Validation Batch 16, Loss: 1.101710, Accuracy: 61.52%\n",
      "Validation Batch 17, Loss: 1.204105, Accuracy: 60.85%\n",
      "Validation Batch 18, Loss: 1.087447, Accuracy: 61.11%\n",
      "Validation Batch 19, Loss: 1.180904, Accuracy: 60.69%\n",
      "Validation Batch 20, Loss: 1.127644, Accuracy: 60.86%\n",
      "Validation Batch 21, Loss: 1.121579, Accuracy: 60.71%\n",
      "Validation Batch 22, Loss: 1.177333, Accuracy: 60.51%\n",
      "Validation Batch 23, Loss: 1.208696, Accuracy: 60.19%\n",
      "Validation Batch 24, Loss: 1.139129, Accuracy: 60.22%\n",
      "Validation Batch 25, Loss: 1.114849, Accuracy: 60.25%\n",
      "Validation Batch 26, Loss: 1.081484, Accuracy: 60.46%\n",
      "Validation Batch 27, Loss: 1.129214, Accuracy: 60.42%\n",
      "Validation - Epoch 33, Loss: 1.131144, Accuracy: 60.42%\n",
      "Patienceâ€”0\n",
      "Epoch 34\n",
      "Batch 1, Loss: 1.039181, Accuracy: 70.31%\n",
      "Batch 2, Loss: 1.021973, Accuracy: 71.09%\n",
      "Batch 3, Loss: 1.053323, Accuracy: 69.79%\n",
      "Batch 4, Loss: 1.112239, Accuracy: 68.36%\n",
      "Batch 5, Loss: 1.070017, Accuracy: 68.12%\n",
      "Batch 6, Loss: 1.099126, Accuracy: 67.19%\n",
      "Batch 7, Loss: 1.063963, Accuracy: 67.41%\n",
      "Batch 8, Loss: 1.020112, Accuracy: 67.77%\n",
      "Batch 9, Loss: 1.123857, Accuracy: 67.19%\n",
      "Batch 10, Loss: 1.065664, Accuracy: 67.19%\n",
      "Batch 11, Loss: 1.076522, Accuracy: 67.19%\n",
      "Batch 12, Loss: 0.995830, Accuracy: 67.97%\n",
      "Batch 13, Loss: 0.988105, Accuracy: 68.75%\n",
      "Batch 14, Loss: 0.967770, Accuracy: 69.53%\n",
      "Batch 15, Loss: 0.984134, Accuracy: 70.00%\n",
      "Batch 16, Loss: 1.033640, Accuracy: 70.02%\n",
      "Batch 17, Loss: 0.997397, Accuracy: 70.22%\n",
      "Batch 18, Loss: 1.110856, Accuracy: 69.79%\n",
      "Batch 19, Loss: 1.107997, Accuracy: 69.57%\n",
      "Batch 20, Loss: 1.057235, Accuracy: 69.53%\n",
      "Batch 21, Loss: 1.082330, Accuracy: 69.27%\n",
      "Batch 22, Loss: 1.062326, Accuracy: 69.25%\n",
      "Batch 23, Loss: 1.054027, Accuracy: 69.09%\n",
      "Batch 24, Loss: 1.121440, Accuracy: 68.82%\n",
      "Batch 25, Loss: 1.007903, Accuracy: 69.00%\n",
      "Batch 26, Loss: 0.948908, Accuracy: 69.41%\n",
      "Batch 27, Loss: 1.086809, Accuracy: 69.10%\n",
      "Batch 28, Loss: 1.037976, Accuracy: 69.20%\n",
      "Batch 29, Loss: 1.076250, Accuracy: 69.13%\n",
      "Batch 30, Loss: 1.153388, Accuracy: 68.80%\n",
      "Batch 31, Loss: 1.002332, Accuracy: 69.05%\n",
      "Batch 32, Loss: 1.193915, Accuracy: 68.60%\n",
      "Batch 33, Loss: 1.104367, Accuracy: 68.42%\n",
      "Batch 34, Loss: 1.177432, Accuracy: 68.01%\n",
      "Batch 35, Loss: 1.052487, Accuracy: 68.12%\n",
      "Batch 36, Loss: 1.008609, Accuracy: 68.32%\n",
      "Batch 37, Loss: 1.057183, Accuracy: 68.29%\n",
      "Batch 38, Loss: 1.012305, Accuracy: 68.38%\n",
      "Batch 39, Loss: 1.116276, Accuracy: 68.23%\n",
      "Batch 40, Loss: 1.036217, Accuracy: 68.32%\n",
      "Batch 41, Loss: 1.025752, Accuracy: 68.29%\n",
      "Batch 42, Loss: 1.019318, Accuracy: 68.42%\n",
      "Batch 43, Loss: 0.996943, Accuracy: 68.53%\n",
      "Batch 44, Loss: 1.019114, Accuracy: 68.61%\n",
      "Batch 45, Loss: 1.038270, Accuracy: 68.58%\n",
      "Batch 46, Loss: 1.050742, Accuracy: 68.61%\n",
      "Batch 47, Loss: 0.964599, Accuracy: 68.85%\n",
      "Batch 48, Loss: 1.050770, Accuracy: 68.88%\n",
      "Batch 49, Loss: 0.982427, Accuracy: 69.04%\n",
      "Batch 50, Loss: 1.125661, Accuracy: 68.88%\n",
      "Batch 51, Loss: 1.016130, Accuracy: 68.96%\n",
      "Batch 52, Loss: 1.089948, Accuracy: 68.84%\n",
      "Batch 53, Loss: 0.971998, Accuracy: 69.07%\n",
      "Batch 54, Loss: 1.032098, Accuracy: 69.10%\n",
      "Batch 55, Loss: 1.083148, Accuracy: 68.98%\n",
      "Batch 56, Loss: 1.114243, Accuracy: 68.75%\n",
      "Batch 57, Loss: 1.004420, Accuracy: 68.83%\n",
      "Batch 58, Loss: 1.044866, Accuracy: 68.86%\n",
      "Batch 59, Loss: 1.088433, Accuracy: 68.80%\n",
      "Batch 60, Loss: 1.018598, Accuracy: 68.85%\n",
      "Batch 61, Loss: 1.062955, Accuracy: 68.85%\n",
      "Batch 62, Loss: 0.986528, Accuracy: 68.95%\n",
      "Batch 63, Loss: 1.033443, Accuracy: 68.97%\n",
      "Batch 64, Loss: 0.970217, Accuracy: 69.09%\n",
      "Batch 65, Loss: 1.005994, Accuracy: 69.13%\n",
      "Batch 66, Loss: 1.038776, Accuracy: 69.15%\n",
      "Batch 67, Loss: 1.025063, Accuracy: 69.22%\n",
      "Batch 68, Loss: 1.005223, Accuracy: 69.28%\n",
      "Batch 69, Loss: 1.092432, Accuracy: 69.20%\n",
      "Batch 70, Loss: 0.993259, Accuracy: 69.29%\n",
      "Batch 71, Loss: 1.068929, Accuracy: 69.26%\n",
      "Batch 72, Loss: 1.067888, Accuracy: 69.25%\n",
      "Batch 73, Loss: 0.992931, Accuracy: 69.35%\n",
      "Batch 74, Loss: 0.914558, Accuracy: 69.57%\n",
      "Batch 75, Loss: 1.085443, Accuracy: 69.50%\n",
      "Batch 76, Loss: 1.156046, Accuracy: 69.33%\n",
      "Batch 77, Loss: 1.048203, Accuracy: 69.34%\n",
      "Batch 78, Loss: 1.048117, Accuracy: 69.33%\n",
      "Batch 79, Loss: 1.080418, Accuracy: 69.30%\n",
      "Batch 80, Loss: 1.052064, Accuracy: 69.30%\n",
      "Batch 81, Loss: 1.086934, Accuracy: 69.25%\n",
      "Batch 82, Loss: 1.069000, Accuracy: 69.23%\n",
      "Batch 83, Loss: 1.099036, Accuracy: 69.13%\n",
      "Batch 84, Loss: 1.001756, Accuracy: 69.20%\n",
      "Batch 85, Loss: 0.962193, Accuracy: 69.34%\n",
      "Batch 86, Loss: 0.937866, Accuracy: 69.48%\n",
      "Batch 87, Loss: 1.065895, Accuracy: 69.47%\n",
      "Batch 88, Loss: 1.049364, Accuracy: 69.48%\n",
      "Batch 89, Loss: 1.100175, Accuracy: 69.40%\n",
      "Batch 90, Loss: 1.109812, Accuracy: 69.32%\n",
      "Batch 91, Loss: 0.983931, Accuracy: 69.39%\n",
      "Batch 92, Loss: 1.023528, Accuracy: 69.43%\n",
      "Batch 93, Loss: 1.016419, Accuracy: 69.44%\n",
      "Batch 94, Loss: 0.959277, Accuracy: 69.53%\n",
      "Batch 95, Loss: 1.067432, Accuracy: 69.52%\n",
      "Batch 96, Loss: 1.041571, Accuracy: 69.51%\n",
      "Batch 97, Loss: 0.981810, Accuracy: 69.59%\n",
      "Batch 98, Loss: 1.023663, Accuracy: 69.60%\n",
      "Batch 99, Loss: 1.027509, Accuracy: 69.60%\n",
      "Batch 100, Loss: 1.105069, Accuracy: 69.55%\n",
      "Batch 101, Loss: 1.026007, Accuracy: 69.59%\n",
      "Batch 102, Loss: 1.122851, Accuracy: 69.50%\n",
      "Batch 103, Loss: 1.019416, Accuracy: 69.52%\n",
      "Batch 104, Loss: 1.004846, Accuracy: 69.58%\n",
      "Batch 105, Loss: 1.101904, Accuracy: 69.51%\n",
      "Batch 106, Loss: 1.088397, Accuracy: 69.49%\n",
      "Batch 107, Loss: 1.040679, Accuracy: 69.52%\n",
      "Batch 108, Loss: 1.153757, Accuracy: 69.43%\n",
      "Batch 109, Loss: 0.986602, Accuracy: 69.50%\n",
      "Batch 110, Loss: 1.007270, Accuracy: 69.55%\n",
      "Batch 111, Loss: 0.960777, Accuracy: 69.61%\n",
      "Batch 112, Loss: 0.955253, Accuracy: 69.70%\n",
      "Batch 113, Loss: 0.977696, Accuracy: 69.76%\n",
      "Batch 114, Loss: 1.020647, Accuracy: 69.79%\n",
      "Batch 115, Loss: 1.047724, Accuracy: 69.77%\n",
      "Batch 116, Loss: 1.003677, Accuracy: 69.80%\n",
      "Batch 117, Loss: 0.972227, Accuracy: 69.85%\n",
      "Batch 118, Loss: 0.921713, Accuracy: 69.94%\n",
      "Batch 119, Loss: 1.076222, Accuracy: 69.91%\n",
      "Batch 120, Loss: 1.070794, Accuracy: 69.86%\n",
      "Batch 121, Loss: 0.968523, Accuracy: 69.93%\n",
      "Batch 122, Loss: 1.074908, Accuracy: 69.89%\n",
      "Batch 123, Loss: 1.171132, Accuracy: 69.78%\n",
      "Batch 124, Loss: 1.003999, Accuracy: 69.82%\n",
      "Batch 125, Loss: 1.001437, Accuracy: 69.85%\n",
      "Batch 126, Loss: 1.037580, Accuracy: 69.85%\n",
      "Batch 127, Loss: 1.065453, Accuracy: 69.83%\n",
      "Batch 128, Loss: 1.001733, Accuracy: 69.90%\n",
      "Batch 129, Loss: 1.069442, Accuracy: 69.89%\n",
      "Batch 130, Loss: 1.010210, Accuracy: 69.92%\n",
      "Batch 131, Loss: 1.150331, Accuracy: 69.84%\n",
      "Batch 132, Loss: 1.144855, Accuracy: 69.74%\n",
      "Batch 133, Loss: 1.117230, Accuracy: 69.69%\n",
      "Batch 134, Loss: 1.096289, Accuracy: 69.66%\n",
      "Batch 135, Loss: 1.063656, Accuracy: 69.64%\n",
      "Batch 136, Loss: 1.140050, Accuracy: 69.55%\n",
      "Batch 137, Loss: 0.923219, Accuracy: 69.65%\n",
      "Batch 138, Loss: 1.035796, Accuracy: 69.66%\n",
      "Batch 139, Loss: 1.011858, Accuracy: 69.68%\n",
      "Batch 140, Loss: 1.070008, Accuracy: 69.67%\n",
      "Batch 141, Loss: 1.150200, Accuracy: 69.58%\n",
      "Batch 142, Loss: 1.062545, Accuracy: 69.55%\n",
      "Batch 143, Loss: 1.135530, Accuracy: 69.49%\n",
      "Batch 144, Loss: 0.966535, Accuracy: 69.53%\n",
      "Batch 145, Loss: 1.116102, Accuracy: 69.46%\n",
      "Batch 146, Loss: 1.112198, Accuracy: 69.41%\n",
      "Batch 147, Loss: 1.013962, Accuracy: 69.45%\n",
      "Batch 148, Loss: 0.960036, Accuracy: 69.51%\n",
      "Batch 149, Loss: 1.061476, Accuracy: 69.49%\n",
      "Batch 150, Loss: 1.035510, Accuracy: 69.52%\n",
      "Batch 151, Loss: 1.089417, Accuracy: 69.50%\n",
      "Batch 152, Loss: 1.079687, Accuracy: 69.47%\n",
      "Batch 153, Loss: 1.025394, Accuracy: 69.49%\n",
      "Batch 154, Loss: 1.051036, Accuracy: 69.49%\n",
      "Batch 155, Loss: 1.047033, Accuracy: 69.50%\n",
      "Batch 156, Loss: 1.023355, Accuracy: 69.51%\n",
      "Batch 157, Loss: 1.040628, Accuracy: 69.52%\n",
      "Batch 158, Loss: 0.972006, Accuracy: 69.57%\n",
      "Batch 159, Loss: 0.921510, Accuracy: 69.66%\n",
      "Batch 160, Loss: 1.015617, Accuracy: 69.69%\n",
      "Batch 161, Loss: 0.962590, Accuracy: 69.76%\n",
      "Batch 162, Loss: 1.064506, Accuracy: 69.74%\n",
      "Batch 163, Loss: 1.018196, Accuracy: 69.77%\n",
      "Batch 164, Loss: 0.999085, Accuracy: 69.80%\n",
      "Batch 165, Loss: 1.039140, Accuracy: 69.80%\n",
      "Batch 166, Loss: 1.064345, Accuracy: 69.79%\n",
      "Batch 167, Loss: 1.007509, Accuracy: 69.83%\n",
      "Batch 168, Loss: 1.033431, Accuracy: 69.83%\n",
      "Batch 169, Loss: 1.057833, Accuracy: 69.82%\n",
      "Batch 170, Loss: 1.101614, Accuracy: 69.78%\n",
      "Batch 171, Loss: 1.093146, Accuracy: 69.76%\n",
      "Batch 172, Loss: 1.076064, Accuracy: 69.73%\n",
      "Batch 173, Loss: 1.093992, Accuracy: 69.70%\n",
      "Batch 174, Loss: 1.084891, Accuracy: 69.68%\n",
      "Batch 175, Loss: 1.066875, Accuracy: 69.67%\n",
      "Batch 176, Loss: 1.064548, Accuracy: 69.66%\n",
      "Batch 177, Loss: 1.114364, Accuracy: 69.61%\n",
      "Batch 178, Loss: 1.112854, Accuracy: 69.57%\n",
      "Batch 179, Loss: 1.186087, Accuracy: 69.49%\n",
      "Batch 180, Loss: 0.998555, Accuracy: 69.52%\n",
      "Batch 181, Loss: 1.079380, Accuracy: 69.51%\n",
      "Batch 182, Loss: 1.022653, Accuracy: 69.52%\n",
      "Batch 183, Loss: 1.017601, Accuracy: 69.54%\n",
      "Batch 184, Loss: 1.056791, Accuracy: 69.55%\n",
      "Batch 185, Loss: 1.106794, Accuracy: 69.51%\n",
      "Batch 186, Loss: 1.015066, Accuracy: 69.52%\n",
      "Batch 187, Loss: 1.051658, Accuracy: 69.52%\n",
      "Batch 188, Loss: 1.082486, Accuracy: 69.51%\n",
      "Batch 189, Loss: 1.071726, Accuracy: 69.49%\n",
      "Batch 190, Loss: 1.054430, Accuracy: 69.49%\n",
      "Batch 191, Loss: 1.037753, Accuracy: 69.49%\n",
      "Batch 192, Loss: 1.123894, Accuracy: 69.45%\n",
      "Batch 193, Loss: 1.039960, Accuracy: 69.45%\n",
      "Batch 194, Loss: 1.038321, Accuracy: 69.47%\n",
      "Batch 195, Loss: 1.016165, Accuracy: 69.47%\n",
      "Batch 196, Loss: 1.058603, Accuracy: 69.46%\n",
      "Batch 197, Loss: 1.071422, Accuracy: 69.46%\n",
      "Batch 198, Loss: 1.003384, Accuracy: 69.47%\n",
      "Batch 199, Loss: 1.065817, Accuracy: 69.46%\n",
      "Batch 200, Loss: 1.097672, Accuracy: 69.43%\n",
      "Batch 201, Loss: 1.042469, Accuracy: 69.43%\n",
      "Batch 202, Loss: 1.032388, Accuracy: 69.45%\n",
      "Batch 203, Loss: 0.993647, Accuracy: 69.48%\n",
      "Batch 204, Loss: 1.011894, Accuracy: 69.50%\n",
      "Batch 205, Loss: 1.000430, Accuracy: 69.53%\n",
      "Batch 206, Loss: 1.101435, Accuracy: 69.50%\n",
      "Batch 207, Loss: 1.022633, Accuracy: 69.52%\n",
      "Batch 208, Loss: 0.989764, Accuracy: 69.54%\n",
      "Batch 209, Loss: 0.900056, Accuracy: 69.61%\n",
      "Batch 210, Loss: 1.130895, Accuracy: 69.58%\n",
      "Batch 211, Loss: 0.986772, Accuracy: 69.59%\n",
      "Batch 212, Loss: 1.110298, Accuracy: 69.56%\n",
      "Batch 213, Loss: 1.087898, Accuracy: 69.55%\n",
      "Training - Epoch 34, Loss: 1.046560, Accuracy: 69.55%\n",
      "Validation Batch 1, Loss: 1.063603, Accuracy: 68.75%\n",
      "Validation Batch 2, Loss: 1.191419, Accuracy: 59.38%\n",
      "Validation Batch 3, Loss: 1.194359, Accuracy: 57.81%\n",
      "Validation Batch 4, Loss: 1.071556, Accuracy: 60.16%\n",
      "Validation Batch 5, Loss: 1.151356, Accuracy: 59.38%\n",
      "Validation Batch 6, Loss: 1.100476, Accuracy: 60.16%\n",
      "Validation Batch 7, Loss: 1.126243, Accuracy: 60.27%\n",
      "Validation Batch 8, Loss: 1.138439, Accuracy: 60.16%\n",
      "Validation Batch 9, Loss: 1.165803, Accuracy: 59.72%\n",
      "Validation Batch 10, Loss: 1.128667, Accuracy: 60.16%\n",
      "Validation Batch 11, Loss: 1.100842, Accuracy: 60.51%\n",
      "Validation Batch 12, Loss: 1.061169, Accuracy: 61.20%\n",
      "Validation Batch 13, Loss: 1.185975, Accuracy: 60.70%\n",
      "Validation Batch 14, Loss: 1.121921, Accuracy: 60.71%\n",
      "Validation Batch 15, Loss: 1.142105, Accuracy: 60.52%\n",
      "Validation Batch 16, Loss: 1.115647, Accuracy: 60.64%\n",
      "Validation Batch 17, Loss: 1.215537, Accuracy: 59.93%\n",
      "Validation Batch 18, Loss: 1.095143, Accuracy: 60.16%\n",
      "Validation Batch 19, Loss: 1.191426, Accuracy: 59.70%\n",
      "Validation Batch 20, Loss: 1.128591, Accuracy: 60.00%\n",
      "Validation Batch 21, Loss: 1.125677, Accuracy: 59.90%\n",
      "Validation Batch 22, Loss: 1.187760, Accuracy: 59.66%\n",
      "Validation Batch 23, Loss: 1.219150, Accuracy: 59.31%\n",
      "Validation Batch 24, Loss: 1.140880, Accuracy: 59.38%\n",
      "Validation Batch 25, Loss: 1.123303, Accuracy: 59.44%\n",
      "Validation Batch 26, Loss: 1.082967, Accuracy: 59.68%\n",
      "Validation Batch 27, Loss: 1.142380, Accuracy: 59.66%\n",
      "Validation - Epoch 34, Loss: 1.137496, Accuracy: 59.66%\n",
      "Patienceâ€”1\n",
      "Epoch 35\n",
      "Batch 1, Loss: 1.093926, Accuracy: 65.62%\n",
      "Batch 2, Loss: 1.024957, Accuracy: 67.97%\n",
      "Batch 3, Loss: 1.072072, Accuracy: 68.23%\n",
      "Batch 4, Loss: 0.981852, Accuracy: 70.70%\n",
      "Batch 5, Loss: 1.036613, Accuracy: 70.62%\n",
      "Batch 6, Loss: 1.005470, Accuracy: 71.09%\n",
      "Batch 7, Loss: 1.039972, Accuracy: 70.76%\n",
      "Batch 8, Loss: 1.051167, Accuracy: 70.70%\n",
      "Batch 9, Loss: 0.985313, Accuracy: 71.53%\n",
      "Batch 10, Loss: 0.962529, Accuracy: 72.03%\n",
      "Batch 11, Loss: 0.993391, Accuracy: 72.16%\n",
      "Batch 12, Loss: 1.036871, Accuracy: 72.14%\n",
      "Batch 13, Loss: 0.928422, Accuracy: 73.08%\n",
      "Batch 14, Loss: 1.016773, Accuracy: 72.99%\n",
      "Batch 15, Loss: 0.973991, Accuracy: 73.44%\n",
      "Batch 16, Loss: 1.096781, Accuracy: 72.95%\n",
      "Batch 17, Loss: 1.010464, Accuracy: 73.07%\n",
      "Batch 18, Loss: 1.046802, Accuracy: 72.83%\n",
      "Batch 19, Loss: 1.070654, Accuracy: 72.37%\n",
      "Batch 20, Loss: 1.000097, Accuracy: 72.50%\n",
      "Batch 21, Loss: 0.934637, Accuracy: 72.84%\n",
      "Batch 22, Loss: 1.016437, Accuracy: 72.94%\n",
      "Batch 23, Loss: 1.196737, Accuracy: 72.01%\n",
      "Batch 24, Loss: 1.023306, Accuracy: 71.94%\n",
      "Batch 25, Loss: 0.969325, Accuracy: 72.12%\n",
      "Batch 26, Loss: 1.047554, Accuracy: 72.06%\n",
      "Batch 27, Loss: 1.045488, Accuracy: 71.93%\n",
      "Batch 28, Loss: 1.052895, Accuracy: 71.76%\n",
      "Batch 29, Loss: 1.120715, Accuracy: 71.44%\n",
      "Batch 30, Loss: 1.110459, Accuracy: 71.09%\n",
      "Batch 31, Loss: 1.115293, Accuracy: 70.82%\n",
      "Batch 32, Loss: 1.135651, Accuracy: 70.41%\n",
      "Batch 33, Loss: 1.007084, Accuracy: 70.45%\n",
      "Batch 34, Loss: 1.015731, Accuracy: 70.50%\n",
      "Batch 35, Loss: 0.993450, Accuracy: 70.62%\n",
      "Batch 36, Loss: 1.030283, Accuracy: 70.62%\n",
      "Batch 37, Loss: 1.079217, Accuracy: 70.52%\n",
      "Batch 38, Loss: 0.994627, Accuracy: 70.64%\n",
      "Batch 39, Loss: 0.999393, Accuracy: 70.71%\n",
      "Batch 40, Loss: 1.140688, Accuracy: 70.43%\n",
      "Batch 41, Loss: 1.002169, Accuracy: 70.54%\n",
      "Batch 42, Loss: 1.065146, Accuracy: 70.46%\n",
      "Batch 43, Loss: 0.941870, Accuracy: 70.71%\n",
      "Batch 44, Loss: 1.043237, Accuracy: 70.67%\n",
      "Batch 45, Loss: 1.033281, Accuracy: 70.69%\n",
      "Batch 46, Loss: 0.975090, Accuracy: 70.86%\n",
      "Batch 47, Loss: 1.126037, Accuracy: 70.64%\n",
      "Batch 48, Loss: 1.021806, Accuracy: 70.67%\n",
      "Batch 49, Loss: 0.970869, Accuracy: 70.76%\n",
      "Batch 50, Loss: 1.030743, Accuracy: 70.72%\n",
      "Batch 51, Loss: 0.941803, Accuracy: 70.83%\n",
      "Batch 52, Loss: 1.080221, Accuracy: 70.70%\n",
      "Batch 53, Loss: 1.058006, Accuracy: 70.67%\n",
      "Batch 54, Loss: 1.045247, Accuracy: 70.66%\n",
      "Batch 55, Loss: 1.022135, Accuracy: 70.68%\n",
      "Batch 56, Loss: 0.964059, Accuracy: 70.81%\n",
      "Batch 57, Loss: 1.074384, Accuracy: 70.70%\n",
      "Batch 58, Loss: 1.094990, Accuracy: 70.58%\n",
      "Batch 59, Loss: 0.955681, Accuracy: 70.74%\n",
      "Batch 60, Loss: 1.008511, Accuracy: 70.78%\n",
      "Batch 61, Loss: 1.097640, Accuracy: 70.67%\n",
      "Batch 62, Loss: 1.081425, Accuracy: 70.59%\n",
      "Batch 63, Loss: 1.020548, Accuracy: 70.59%\n",
      "Batch 64, Loss: 0.956137, Accuracy: 70.73%\n",
      "Batch 65, Loss: 1.024465, Accuracy: 70.72%\n",
      "Batch 66, Loss: 0.996771, Accuracy: 70.79%\n",
      "Batch 67, Loss: 1.100833, Accuracy: 70.66%\n",
      "Batch 68, Loss: 1.036525, Accuracy: 70.63%\n",
      "Batch 69, Loss: 1.031339, Accuracy: 70.65%\n",
      "Batch 70, Loss: 1.031909, Accuracy: 70.65%\n",
      "Batch 71, Loss: 1.095541, Accuracy: 70.60%\n",
      "Batch 72, Loss: 1.041551, Accuracy: 70.55%\n",
      "Batch 73, Loss: 0.984642, Accuracy: 70.61%\n",
      "Batch 74, Loss: 1.095821, Accuracy: 70.52%\n",
      "Batch 75, Loss: 1.052814, Accuracy: 70.54%\n",
      "Batch 76, Loss: 1.021607, Accuracy: 70.56%\n",
      "Batch 77, Loss: 1.070975, Accuracy: 70.50%\n",
      "Batch 78, Loss: 1.008253, Accuracy: 70.55%\n",
      "Batch 79, Loss: 1.109866, Accuracy: 70.47%\n",
      "Batch 80, Loss: 1.029964, Accuracy: 70.49%\n",
      "Batch 81, Loss: 1.004214, Accuracy: 70.56%\n",
      "Batch 82, Loss: 1.013211, Accuracy: 70.54%\n",
      "Batch 83, Loss: 1.027436, Accuracy: 70.56%\n",
      "Batch 84, Loss: 1.080825, Accuracy: 70.48%\n",
      "Batch 85, Loss: 1.048247, Accuracy: 70.48%\n",
      "Batch 86, Loss: 1.004290, Accuracy: 70.51%\n",
      "Batch 87, Loss: 1.051405, Accuracy: 70.47%\n",
      "Batch 88, Loss: 1.052225, Accuracy: 70.47%\n",
      "Batch 89, Loss: 1.096144, Accuracy: 70.42%\n",
      "Batch 90, Loss: 1.074392, Accuracy: 70.40%\n",
      "Batch 91, Loss: 1.064137, Accuracy: 70.36%\n",
      "Batch 92, Loss: 1.023138, Accuracy: 70.38%\n",
      "Batch 93, Loss: 1.045036, Accuracy: 70.38%\n",
      "Batch 94, Loss: 1.032255, Accuracy: 70.40%\n",
      "Batch 95, Loss: 1.092693, Accuracy: 70.33%\n",
      "Batch 96, Loss: 1.014154, Accuracy: 70.36%\n",
      "Batch 97, Loss: 1.111058, Accuracy: 70.26%\n",
      "Batch 98, Loss: 1.035631, Accuracy: 70.26%\n",
      "Batch 99, Loss: 1.077277, Accuracy: 70.19%\n",
      "Batch 100, Loss: 1.096261, Accuracy: 70.09%\n",
      "Batch 101, Loss: 1.046906, Accuracy: 70.11%\n",
      "Batch 102, Loss: 1.006753, Accuracy: 70.17%\n",
      "Batch 103, Loss: 1.053846, Accuracy: 70.13%\n",
      "Batch 104, Loss: 1.036588, Accuracy: 70.12%\n",
      "Batch 105, Loss: 0.984231, Accuracy: 70.19%\n",
      "Batch 106, Loss: 1.018753, Accuracy: 70.22%\n",
      "Batch 107, Loss: 1.042299, Accuracy: 70.24%\n",
      "Batch 108, Loss: 1.028313, Accuracy: 70.27%\n",
      "Batch 109, Loss: 1.042986, Accuracy: 70.27%\n",
      "Batch 110, Loss: 1.022033, Accuracy: 70.26%\n",
      "Batch 111, Loss: 1.116186, Accuracy: 70.19%\n",
      "Batch 112, Loss: 1.123355, Accuracy: 70.13%\n",
      "Batch 113, Loss: 1.018733, Accuracy: 70.16%\n",
      "Batch 114, Loss: 1.039260, Accuracy: 70.16%\n",
      "Batch 115, Loss: 1.014363, Accuracy: 70.19%\n",
      "Batch 116, Loss: 1.049274, Accuracy: 70.16%\n",
      "Batch 117, Loss: 1.097060, Accuracy: 70.10%\n",
      "Batch 118, Loss: 1.104383, Accuracy: 70.02%\n",
      "Batch 119, Loss: 1.081162, Accuracy: 69.98%\n",
      "Batch 120, Loss: 0.970638, Accuracy: 70.05%\n",
      "Batch 121, Loss: 1.107884, Accuracy: 70.00%\n",
      "Batch 122, Loss: 1.050988, Accuracy: 69.98%\n",
      "Batch 123, Loss: 1.015980, Accuracy: 69.99%\n",
      "Batch 124, Loss: 0.944594, Accuracy: 70.07%\n",
      "Batch 125, Loss: 1.093072, Accuracy: 70.01%\n",
      "Batch 126, Loss: 1.028257, Accuracy: 70.01%\n",
      "Batch 127, Loss: 1.013399, Accuracy: 70.02%\n",
      "Batch 128, Loss: 1.036223, Accuracy: 70.02%\n",
      "Batch 129, Loss: 1.101657, Accuracy: 69.96%\n",
      "Batch 130, Loss: 1.034900, Accuracy: 69.96%\n",
      "Batch 131, Loss: 1.061641, Accuracy: 69.95%\n",
      "Batch 132, Loss: 1.004877, Accuracy: 69.97%\n",
      "Batch 133, Loss: 1.030105, Accuracy: 69.98%\n",
      "Batch 134, Loss: 1.055149, Accuracy: 69.99%\n",
      "Batch 135, Loss: 1.111861, Accuracy: 69.94%\n",
      "Batch 136, Loss: 1.111108, Accuracy: 69.88%\n",
      "Batch 137, Loss: 1.093890, Accuracy: 69.82%\n",
      "Batch 138, Loss: 1.073495, Accuracy: 69.81%\n",
      "Batch 139, Loss: 1.108676, Accuracy: 69.75%\n",
      "Batch 140, Loss: 1.039598, Accuracy: 69.74%\n",
      "Batch 141, Loss: 1.134726, Accuracy: 69.68%\n",
      "Batch 142, Loss: 0.944058, Accuracy: 69.76%\n",
      "Batch 143, Loss: 1.130260, Accuracy: 69.71%\n",
      "Batch 144, Loss: 1.030202, Accuracy: 69.72%\n",
      "Batch 145, Loss: 1.055850, Accuracy: 69.72%\n",
      "Batch 146, Loss: 0.943927, Accuracy: 69.79%\n",
      "Batch 147, Loss: 1.050704, Accuracy: 69.78%\n",
      "Batch 148, Loss: 1.046328, Accuracy: 69.78%\n",
      "Batch 149, Loss: 1.172165, Accuracy: 69.69%\n",
      "Batch 150, Loss: 1.032843, Accuracy: 69.70%\n",
      "Batch 151, Loss: 1.026923, Accuracy: 69.70%\n",
      "Batch 152, Loss: 1.029151, Accuracy: 69.73%\n",
      "Batch 153, Loss: 1.068224, Accuracy: 69.71%\n",
      "Batch 154, Loss: 1.008456, Accuracy: 69.73%\n",
      "Batch 155, Loss: 0.993518, Accuracy: 69.77%\n",
      "Batch 156, Loss: 1.019604, Accuracy: 69.77%\n",
      "Batch 157, Loss: 1.076150, Accuracy: 69.77%\n",
      "Batch 158, Loss: 1.038793, Accuracy: 69.77%\n",
      "Batch 159, Loss: 1.001651, Accuracy: 69.81%\n",
      "Batch 160, Loss: 1.084222, Accuracy: 69.79%\n",
      "Batch 161, Loss: 1.071348, Accuracy: 69.77%\n",
      "Batch 162, Loss: 0.985406, Accuracy: 69.81%\n",
      "Batch 163, Loss: 1.023942, Accuracy: 69.82%\n",
      "Batch 164, Loss: 1.098474, Accuracy: 69.78%\n",
      "Batch 165, Loss: 1.069376, Accuracy: 69.77%\n",
      "Batch 166, Loss: 1.133368, Accuracy: 69.71%\n",
      "Batch 167, Loss: 1.109008, Accuracy: 69.66%\n",
      "Batch 168, Loss: 1.037062, Accuracy: 69.66%\n",
      "Batch 169, Loss: 1.064343, Accuracy: 69.63%\n",
      "Batch 170, Loss: 1.054263, Accuracy: 69.61%\n",
      "Batch 171, Loss: 1.082903, Accuracy: 69.60%\n",
      "Batch 172, Loss: 1.016628, Accuracy: 69.62%\n",
      "Batch 173, Loss: 0.977652, Accuracy: 69.65%\n",
      "Batch 174, Loss: 1.115905, Accuracy: 69.61%\n",
      "Batch 175, Loss: 1.082569, Accuracy: 69.60%\n",
      "Batch 176, Loss: 1.059576, Accuracy: 69.61%\n",
      "Batch 177, Loss: 0.977075, Accuracy: 69.66%\n",
      "Batch 178, Loss: 1.040790, Accuracy: 69.66%\n",
      "Batch 179, Loss: 1.013561, Accuracy: 69.68%\n",
      "Batch 180, Loss: 1.067239, Accuracy: 69.66%\n",
      "Batch 181, Loss: 0.990244, Accuracy: 69.69%\n",
      "Batch 182, Loss: 1.066374, Accuracy: 69.67%\n",
      "Batch 183, Loss: 0.995758, Accuracy: 69.70%\n",
      "Batch 184, Loss: 1.017353, Accuracy: 69.72%\n",
      "Batch 185, Loss: 0.954721, Accuracy: 69.77%\n",
      "Batch 186, Loss: 1.022291, Accuracy: 69.79%\n",
      "Batch 187, Loss: 1.043754, Accuracy: 69.80%\n",
      "Batch 188, Loss: 1.125321, Accuracy: 69.76%\n",
      "Batch 189, Loss: 0.990265, Accuracy: 69.79%\n",
      "Batch 190, Loss: 1.051569, Accuracy: 69.79%\n",
      "Batch 191, Loss: 1.034635, Accuracy: 69.79%\n",
      "Batch 192, Loss: 1.113952, Accuracy: 69.74%\n",
      "Batch 193, Loss: 1.016506, Accuracy: 69.75%\n",
      "Batch 194, Loss: 1.009101, Accuracy: 69.76%\n",
      "Batch 195, Loss: 1.179366, Accuracy: 69.70%\n",
      "Batch 196, Loss: 1.044307, Accuracy: 69.70%\n",
      "Batch 197, Loss: 1.050288, Accuracy: 69.69%\n",
      "Batch 198, Loss: 1.031916, Accuracy: 69.70%\n",
      "Batch 199, Loss: 1.003753, Accuracy: 69.72%\n",
      "Batch 200, Loss: 1.049801, Accuracy: 69.72%\n",
      "Batch 201, Loss: 1.100365, Accuracy: 69.69%\n",
      "Batch 202, Loss: 0.990784, Accuracy: 69.73%\n",
      "Batch 203, Loss: 1.045433, Accuracy: 69.74%\n",
      "Batch 204, Loss: 1.012183, Accuracy: 69.75%\n",
      "Batch 205, Loss: 1.019847, Accuracy: 69.75%\n",
      "Batch 206, Loss: 1.101735, Accuracy: 69.72%\n",
      "Batch 207, Loss: 1.007607, Accuracy: 69.74%\n",
      "Batch 208, Loss: 1.069089, Accuracy: 69.73%\n",
      "Batch 209, Loss: 0.962712, Accuracy: 69.77%\n",
      "Batch 210, Loss: 1.065702, Accuracy: 69.75%\n",
      "Batch 211, Loss: 1.097212, Accuracy: 69.73%\n",
      "Batch 212, Loss: 1.040430, Accuracy: 69.72%\n",
      "Batch 213, Loss: 1.059963, Accuracy: 69.71%\n",
      "Training - Epoch 35, Loss: 1.043155, Accuracy: 69.71%\n",
      "Validation Batch 1, Loss: 1.086608, Accuracy: 64.06%\n",
      "Validation Batch 2, Loss: 1.211556, Accuracy: 56.25%\n",
      "Validation Batch 3, Loss: 1.213128, Accuracy: 56.25%\n",
      "Validation Batch 4, Loss: 1.084557, Accuracy: 58.59%\n",
      "Validation Batch 5, Loss: 1.164893, Accuracy: 58.12%\n",
      "Validation Batch 6, Loss: 1.119344, Accuracy: 58.85%\n",
      "Validation Batch 7, Loss: 1.141299, Accuracy: 59.15%\n",
      "Validation Batch 8, Loss: 1.149935, Accuracy: 58.79%\n",
      "Validation Batch 9, Loss: 1.181732, Accuracy: 58.51%\n",
      "Validation Batch 10, Loss: 1.158308, Accuracy: 58.44%\n",
      "Validation Batch 11, Loss: 1.126932, Accuracy: 58.38%\n",
      "Validation Batch 12, Loss: 1.096067, Accuracy: 58.85%\n",
      "Validation Batch 13, Loss: 1.208496, Accuracy: 58.41%\n",
      "Validation Batch 14, Loss: 1.148677, Accuracy: 58.37%\n",
      "Validation Batch 15, Loss: 1.159114, Accuracy: 58.23%\n",
      "Validation Batch 16, Loss: 1.135095, Accuracy: 58.50%\n",
      "Validation Batch 17, Loss: 1.242064, Accuracy: 57.81%\n",
      "Validation Batch 18, Loss: 1.117214, Accuracy: 58.16%\n",
      "Validation Batch 19, Loss: 1.217211, Accuracy: 57.73%\n",
      "Validation Batch 20, Loss: 1.147977, Accuracy: 57.89%\n",
      "Validation Batch 21, Loss: 1.147712, Accuracy: 57.74%\n",
      "Validation Batch 22, Loss: 1.215442, Accuracy: 57.46%\n",
      "Validation Batch 23, Loss: 1.240169, Accuracy: 57.00%\n",
      "Validation Batch 24, Loss: 1.160608, Accuracy: 57.10%\n",
      "Validation Batch 25, Loss: 1.138068, Accuracy: 57.12%\n",
      "Validation Batch 26, Loss: 1.096158, Accuracy: 57.39%\n",
      "Validation Batch 27, Loss: 1.170108, Accuracy: 57.43%\n",
      "Validation - Epoch 35, Loss: 1.158462, Accuracy: 57.43%\n",
      "Patienceâ€”2\n",
      "Epoch 36\n",
      "Batch 1, Loss: 0.984119, Accuracy: 76.56%\n",
      "Batch 2, Loss: 1.037349, Accuracy: 73.44%\n",
      "Batch 3, Loss: 1.052274, Accuracy: 71.88%\n",
      "Batch 4, Loss: 1.151118, Accuracy: 67.97%\n",
      "Batch 5, Loss: 1.097206, Accuracy: 66.88%\n",
      "Batch 6, Loss: 1.067801, Accuracy: 67.19%\n",
      "Batch 7, Loss: 1.052845, Accuracy: 67.41%\n",
      "Batch 8, Loss: 0.965077, Accuracy: 68.55%\n",
      "Batch 9, Loss: 1.063096, Accuracy: 68.40%\n",
      "Batch 10, Loss: 1.056037, Accuracy: 68.44%\n",
      "Batch 11, Loss: 0.977633, Accuracy: 69.03%\n",
      "Batch 12, Loss: 1.135698, Accuracy: 68.36%\n",
      "Batch 13, Loss: 0.998373, Accuracy: 68.87%\n",
      "Batch 14, Loss: 1.115078, Accuracy: 68.19%\n",
      "Batch 15, Loss: 0.992000, Accuracy: 68.54%\n",
      "Batch 16, Loss: 1.062256, Accuracy: 68.65%\n",
      "Batch 17, Loss: 1.088212, Accuracy: 68.47%\n",
      "Batch 18, Loss: 1.104370, Accuracy: 68.23%\n",
      "Batch 19, Loss: 1.000241, Accuracy: 68.59%\n",
      "Batch 20, Loss: 1.073149, Accuracy: 68.44%\n",
      "Batch 21, Loss: 1.103346, Accuracy: 68.15%\n",
      "Batch 22, Loss: 1.083092, Accuracy: 68.11%\n",
      "Batch 23, Loss: 1.044972, Accuracy: 68.21%\n",
      "Batch 24, Loss: 1.100603, Accuracy: 67.97%\n",
      "Batch 25, Loss: 0.982163, Accuracy: 68.25%\n",
      "Batch 26, Loss: 1.045254, Accuracy: 68.15%\n",
      "Batch 27, Loss: 1.121062, Accuracy: 67.88%\n",
      "Batch 28, Loss: 1.049261, Accuracy: 68.02%\n",
      "Batch 29, Loss: 1.013576, Accuracy: 68.27%\n",
      "Batch 30, Loss: 1.093699, Accuracy: 68.18%\n",
      "Batch 31, Loss: 1.032089, Accuracy: 68.30%\n",
      "Batch 32, Loss: 1.054516, Accuracy: 68.31%\n",
      "Batch 33, Loss: 0.978204, Accuracy: 68.61%\n",
      "Batch 34, Loss: 0.949508, Accuracy: 68.93%\n",
      "Batch 35, Loss: 0.950220, Accuracy: 69.24%\n",
      "Batch 36, Loss: 1.075474, Accuracy: 69.23%\n",
      "Batch 37, Loss: 1.058523, Accuracy: 69.17%\n",
      "Batch 38, Loss: 1.019353, Accuracy: 69.28%\n",
      "Batch 39, Loss: 0.931855, Accuracy: 69.51%\n",
      "Batch 40, Loss: 1.071991, Accuracy: 69.49%\n",
      "Batch 41, Loss: 1.078114, Accuracy: 69.44%\n",
      "Batch 42, Loss: 1.034787, Accuracy: 69.49%\n",
      "Batch 43, Loss: 1.029698, Accuracy: 69.55%\n",
      "Batch 44, Loss: 1.102165, Accuracy: 69.46%\n",
      "Batch 45, Loss: 1.044964, Accuracy: 69.51%\n",
      "Batch 46, Loss: 1.031131, Accuracy: 69.57%\n",
      "Batch 47, Loss: 0.981356, Accuracy: 69.75%\n",
      "Batch 48, Loss: 1.074689, Accuracy: 69.66%\n",
      "Batch 49, Loss: 0.981845, Accuracy: 69.77%\n",
      "Batch 50, Loss: 1.057595, Accuracy: 69.72%\n",
      "Batch 51, Loss: 1.011296, Accuracy: 69.82%\n",
      "Batch 52, Loss: 1.083007, Accuracy: 69.77%\n",
      "Batch 53, Loss: 1.122188, Accuracy: 69.63%\n",
      "Batch 54, Loss: 1.054309, Accuracy: 69.59%\n",
      "Batch 55, Loss: 1.073278, Accuracy: 69.55%\n",
      "Batch 56, Loss: 1.067852, Accuracy: 69.50%\n",
      "Batch 57, Loss: 1.013939, Accuracy: 69.54%\n",
      "Batch 58, Loss: 1.067463, Accuracy: 69.48%\n",
      "Batch 59, Loss: 1.010506, Accuracy: 69.52%\n",
      "Batch 60, Loss: 0.995358, Accuracy: 69.61%\n",
      "Batch 61, Loss: 1.003442, Accuracy: 69.65%\n",
      "Batch 62, Loss: 0.966142, Accuracy: 69.81%\n",
      "Batch 63, Loss: 1.108361, Accuracy: 69.72%\n",
      "Batch 64, Loss: 1.012900, Accuracy: 69.75%\n",
      "Batch 65, Loss: 1.030244, Accuracy: 69.76%\n",
      "Batch 66, Loss: 0.956123, Accuracy: 69.93%\n",
      "Batch 67, Loss: 1.082178, Accuracy: 69.87%\n",
      "Batch 68, Loss: 1.003647, Accuracy: 69.90%\n",
      "Batch 69, Loss: 1.009628, Accuracy: 69.97%\n",
      "Batch 70, Loss: 1.084136, Accuracy: 69.89%\n",
      "Batch 71, Loss: 1.012300, Accuracy: 69.89%\n",
      "Batch 72, Loss: 1.082220, Accuracy: 69.81%\n",
      "Batch 73, Loss: 1.045182, Accuracy: 69.80%\n",
      "Batch 74, Loss: 0.976974, Accuracy: 69.93%\n",
      "Batch 75, Loss: 1.053789, Accuracy: 69.92%\n",
      "Batch 76, Loss: 0.911445, Accuracy: 70.09%\n",
      "Batch 77, Loss: 1.114780, Accuracy: 69.97%\n",
      "Batch 78, Loss: 1.140113, Accuracy: 69.85%\n",
      "Batch 79, Loss: 1.017290, Accuracy: 69.88%\n",
      "Batch 80, Loss: 1.089874, Accuracy: 69.80%\n",
      "Batch 81, Loss: 1.105536, Accuracy: 69.71%\n",
      "Batch 82, Loss: 1.016861, Accuracy: 69.76%\n",
      "Batch 83, Loss: 1.089586, Accuracy: 69.71%\n",
      "Batch 84, Loss: 1.015466, Accuracy: 69.74%\n",
      "Batch 85, Loss: 1.088828, Accuracy: 69.69%\n",
      "Batch 86, Loss: 0.990059, Accuracy: 69.75%\n",
      "Batch 87, Loss: 1.084904, Accuracy: 69.68%\n",
      "Batch 88, Loss: 1.014758, Accuracy: 69.74%\n",
      "Batch 89, Loss: 1.043764, Accuracy: 69.75%\n",
      "Batch 90, Loss: 1.095642, Accuracy: 69.69%\n",
      "Batch 91, Loss: 1.057004, Accuracy: 69.71%\n",
      "Batch 92, Loss: 1.132687, Accuracy: 69.62%\n",
      "Batch 93, Loss: 1.065336, Accuracy: 69.56%\n",
      "Batch 94, Loss: 0.971942, Accuracy: 69.63%\n",
      "Batch 95, Loss: 1.042842, Accuracy: 69.62%\n",
      "Batch 96, Loss: 1.045367, Accuracy: 69.61%\n",
      "Batch 97, Loss: 1.097899, Accuracy: 69.56%\n",
      "Batch 98, Loss: 1.070116, Accuracy: 69.52%\n",
      "Batch 99, Loss: 1.064533, Accuracy: 69.51%\n",
      "Batch 100, Loss: 1.025596, Accuracy: 69.53%\n",
      "Batch 101, Loss: 1.113483, Accuracy: 69.46%\n",
      "Batch 102, Loss: 1.011956, Accuracy: 69.52%\n",
      "Batch 103, Loss: 0.988162, Accuracy: 69.58%\n",
      "Batch 104, Loss: 0.956075, Accuracy: 69.68%\n",
      "Batch 105, Loss: 1.079994, Accuracy: 69.63%\n",
      "Batch 106, Loss: 1.016771, Accuracy: 69.66%\n",
      "Batch 107, Loss: 1.051040, Accuracy: 69.67%\n",
      "Batch 108, Loss: 0.938525, Accuracy: 69.76%\n",
      "Batch 109, Loss: 1.069670, Accuracy: 69.75%\n",
      "Batch 110, Loss: 1.200184, Accuracy: 69.59%\n",
      "Batch 111, Loss: 1.023803, Accuracy: 69.59%\n",
      "Batch 112, Loss: 1.072479, Accuracy: 69.57%\n",
      "Batch 113, Loss: 1.022030, Accuracy: 69.62%\n",
      "Batch 114, Loss: 1.094965, Accuracy: 69.59%\n",
      "Batch 115, Loss: 0.948884, Accuracy: 69.69%\n",
      "Batch 116, Loss: 1.163158, Accuracy: 69.56%\n",
      "Batch 117, Loss: 1.080377, Accuracy: 69.55%\n",
      "Batch 118, Loss: 1.012124, Accuracy: 69.57%\n",
      "Batch 119, Loss: 1.077695, Accuracy: 69.55%\n",
      "Batch 120, Loss: 0.996612, Accuracy: 69.60%\n",
      "Batch 121, Loss: 1.052900, Accuracy: 69.58%\n",
      "Batch 122, Loss: 1.025227, Accuracy: 69.58%\n",
      "Batch 123, Loss: 1.010656, Accuracy: 69.60%\n",
      "Batch 124, Loss: 0.998059, Accuracy: 69.66%\n",
      "Batch 125, Loss: 0.958759, Accuracy: 69.74%\n",
      "Batch 126, Loss: 1.019156, Accuracy: 69.77%\n",
      "Batch 127, Loss: 1.000868, Accuracy: 69.80%\n",
      "Batch 128, Loss: 1.013161, Accuracy: 69.81%\n",
      "Batch 129, Loss: 1.136205, Accuracy: 69.72%\n",
      "Batch 130, Loss: 1.019901, Accuracy: 69.76%\n",
      "Batch 131, Loss: 1.062912, Accuracy: 69.73%\n",
      "Batch 132, Loss: 1.005128, Accuracy: 69.77%\n",
      "Batch 133, Loss: 1.016247, Accuracy: 69.77%\n",
      "Batch 134, Loss: 0.995005, Accuracy: 69.81%\n",
      "Batch 135, Loss: 1.067613, Accuracy: 69.78%\n",
      "Batch 136, Loss: 1.080239, Accuracy: 69.75%\n",
      "Batch 137, Loss: 0.978725, Accuracy: 69.80%\n",
      "Batch 138, Loss: 0.980959, Accuracy: 69.84%\n",
      "Batch 139, Loss: 1.003139, Accuracy: 69.87%\n",
      "Batch 140, Loss: 1.027995, Accuracy: 69.90%\n",
      "Batch 141, Loss: 1.103741, Accuracy: 69.85%\n",
      "Batch 142, Loss: 1.050654, Accuracy: 69.84%\n",
      "Batch 143, Loss: 1.023198, Accuracy: 69.84%\n",
      "Batch 144, Loss: 0.942161, Accuracy: 69.90%\n",
      "Batch 145, Loss: 0.993132, Accuracy: 69.94%\n",
      "Batch 146, Loss: 1.041802, Accuracy: 69.94%\n",
      "Batch 147, Loss: 1.013816, Accuracy: 69.96%\n",
      "Batch 148, Loss: 1.114005, Accuracy: 69.91%\n",
      "Batch 149, Loss: 0.984715, Accuracy: 69.96%\n",
      "Batch 150, Loss: 1.012501, Accuracy: 69.96%\n",
      "Batch 151, Loss: 1.066383, Accuracy: 69.94%\n",
      "Batch 152, Loss: 0.957552, Accuracy: 69.99%\n",
      "Batch 153, Loss: 1.013603, Accuracy: 70.03%\n",
      "Batch 154, Loss: 1.042811, Accuracy: 70.03%\n",
      "Batch 155, Loss: 1.048292, Accuracy: 70.03%\n",
      "Batch 156, Loss: 0.974319, Accuracy: 70.07%\n",
      "Batch 157, Loss: 0.996534, Accuracy: 70.09%\n",
      "Batch 158, Loss: 1.039880, Accuracy: 70.11%\n",
      "Batch 159, Loss: 0.976764, Accuracy: 70.17%\n",
      "Batch 160, Loss: 1.031950, Accuracy: 70.16%\n",
      "Batch 161, Loss: 1.114623, Accuracy: 70.11%\n",
      "Batch 162, Loss: 0.970324, Accuracy: 70.15%\n",
      "Batch 163, Loss: 0.998768, Accuracy: 70.19%\n",
      "Batch 164, Loss: 1.001857, Accuracy: 70.22%\n",
      "Batch 165, Loss: 1.086581, Accuracy: 70.18%\n",
      "Batch 166, Loss: 1.189553, Accuracy: 70.08%\n",
      "Batch 167, Loss: 1.078690, Accuracy: 70.04%\n",
      "Batch 168, Loss: 1.143731, Accuracy: 69.96%\n",
      "Batch 169, Loss: 1.071406, Accuracy: 69.93%\n",
      "Batch 170, Loss: 1.010766, Accuracy: 69.95%\n",
      "Batch 171, Loss: 0.990900, Accuracy: 69.99%\n",
      "Batch 172, Loss: 1.048869, Accuracy: 69.99%\n",
      "Batch 173, Loss: 1.082509, Accuracy: 69.96%\n",
      "Batch 174, Loss: 1.023326, Accuracy: 69.97%\n",
      "Batch 175, Loss: 1.040559, Accuracy: 69.97%\n",
      "Batch 176, Loss: 1.100162, Accuracy: 69.94%\n",
      "Batch 177, Loss: 1.073870, Accuracy: 69.92%\n",
      "Batch 178, Loss: 1.102840, Accuracy: 69.88%\n",
      "Batch 179, Loss: 1.058114, Accuracy: 69.86%\n",
      "Batch 180, Loss: 1.075586, Accuracy: 69.84%\n",
      "Batch 181, Loss: 1.011939, Accuracy: 69.86%\n",
      "Batch 182, Loss: 1.088547, Accuracy: 69.83%\n",
      "Batch 183, Loss: 1.077543, Accuracy: 69.82%\n",
      "Batch 184, Loss: 0.989960, Accuracy: 69.84%\n",
      "Batch 185, Loss: 1.111588, Accuracy: 69.82%\n",
      "Batch 186, Loss: 1.012626, Accuracy: 69.85%\n",
      "Batch 187, Loss: 1.052744, Accuracy: 69.83%\n",
      "Batch 188, Loss: 1.058582, Accuracy: 69.82%\n",
      "Batch 189, Loss: 0.943986, Accuracy: 69.87%\n",
      "Batch 190, Loss: 1.027633, Accuracy: 69.88%\n",
      "Batch 191, Loss: 1.007085, Accuracy: 69.90%\n",
      "Batch 192, Loss: 1.038049, Accuracy: 69.91%\n",
      "Batch 193, Loss: 1.006007, Accuracy: 69.94%\n",
      "Batch 194, Loss: 1.015350, Accuracy: 69.96%\n",
      "Batch 195, Loss: 1.045656, Accuracy: 69.98%\n",
      "Batch 196, Loss: 0.987793, Accuracy: 70.00%\n",
      "Batch 197, Loss: 1.040654, Accuracy: 70.00%\n",
      "Batch 198, Loss: 1.066745, Accuracy: 69.97%\n",
      "Batch 199, Loss: 1.082952, Accuracy: 69.94%\n",
      "Batch 200, Loss: 1.109793, Accuracy: 69.90%\n",
      "Batch 201, Loss: 1.044922, Accuracy: 69.90%\n",
      "Batch 202, Loss: 1.118557, Accuracy: 69.86%\n",
      "Batch 203, Loss: 0.928889, Accuracy: 69.92%\n",
      "Batch 204, Loss: 0.996748, Accuracy: 69.94%\n",
      "Batch 205, Loss: 1.136989, Accuracy: 69.89%\n",
      "Batch 206, Loss: 1.003864, Accuracy: 69.91%\n",
      "Batch 207, Loss: 1.086607, Accuracy: 69.88%\n",
      "Batch 208, Loss: 1.034717, Accuracy: 69.88%\n",
      "Batch 209, Loss: 1.015009, Accuracy: 69.88%\n",
      "Batch 210, Loss: 1.054749, Accuracy: 69.87%\n",
      "Batch 211, Loss: 1.086731, Accuracy: 69.85%\n",
      "Batch 212, Loss: 1.079524, Accuracy: 69.84%\n",
      "Batch 213, Loss: 1.142237, Accuracy: 69.77%\n",
      "Training - Epoch 36, Loss: 1.042884, Accuracy: 69.77%\n",
      "Validation Batch 1, Loss: 1.068058, Accuracy: 64.06%\n",
      "Validation Batch 2, Loss: 1.198155, Accuracy: 57.81%\n",
      "Validation Batch 3, Loss: 1.193493, Accuracy: 56.77%\n",
      "Validation Batch 4, Loss: 1.069260, Accuracy: 60.16%\n",
      "Validation Batch 5, Loss: 1.147549, Accuracy: 59.38%\n",
      "Validation Batch 6, Loss: 1.100775, Accuracy: 60.16%\n",
      "Validation Batch 7, Loss: 1.128819, Accuracy: 60.27%\n",
      "Validation Batch 8, Loss: 1.137766, Accuracy: 60.74%\n",
      "Validation Batch 9, Loss: 1.176145, Accuracy: 60.42%\n",
      "Validation Batch 10, Loss: 1.138744, Accuracy: 60.31%\n",
      "Validation Batch 11, Loss: 1.106681, Accuracy: 60.65%\n",
      "Validation Batch 12, Loss: 1.081962, Accuracy: 60.94%\n",
      "Validation Batch 13, Loss: 1.194374, Accuracy: 60.46%\n",
      "Validation Batch 14, Loss: 1.134840, Accuracy: 60.27%\n",
      "Validation Batch 15, Loss: 1.146838, Accuracy: 60.10%\n",
      "Validation Batch 16, Loss: 1.112716, Accuracy: 60.25%\n",
      "Validation Batch 17, Loss: 1.221657, Accuracy: 59.56%\n",
      "Validation Batch 18, Loss: 1.102838, Accuracy: 59.81%\n",
      "Validation Batch 19, Loss: 1.201793, Accuracy: 59.38%\n",
      "Validation Batch 20, Loss: 1.139065, Accuracy: 59.45%\n",
      "Validation Batch 21, Loss: 1.130396, Accuracy: 59.30%\n",
      "Validation Batch 22, Loss: 1.200137, Accuracy: 58.95%\n",
      "Validation Batch 23, Loss: 1.221944, Accuracy: 58.42%\n",
      "Validation Batch 24, Loss: 1.148299, Accuracy: 58.53%\n",
      "Validation Batch 25, Loss: 1.121630, Accuracy: 58.56%\n",
      "Validation Batch 26, Loss: 1.087764, Accuracy: 58.71%\n",
      "Validation Batch 27, Loss: 1.143019, Accuracy: 58.72%\n",
      "Validation - Epoch 36, Loss: 1.142767, Accuracy: 58.72%\n",
      "Patienceâ€”3\n",
      "Epoch 37\n",
      "Batch 1, Loss: 1.072281, Accuracy: 64.06%\n",
      "Batch 2, Loss: 1.094007, Accuracy: 62.50%\n",
      "Batch 3, Loss: 1.064055, Accuracy: 65.10%\n",
      "Batch 4, Loss: 1.058526, Accuracy: 65.23%\n",
      "Batch 5, Loss: 1.094389, Accuracy: 65.31%\n",
      "Batch 6, Loss: 1.032143, Accuracy: 66.41%\n",
      "Batch 7, Loss: 1.115310, Accuracy: 65.85%\n",
      "Batch 8, Loss: 0.989036, Accuracy: 67.19%\n",
      "Batch 9, Loss: 1.092334, Accuracy: 66.84%\n",
      "Batch 10, Loss: 1.062909, Accuracy: 67.03%\n",
      "Batch 11, Loss: 0.995626, Accuracy: 67.61%\n",
      "Batch 12, Loss: 1.009416, Accuracy: 67.97%\n",
      "Batch 13, Loss: 1.082946, Accuracy: 67.91%\n",
      "Batch 14, Loss: 0.986906, Accuracy: 68.30%\n",
      "Batch 15, Loss: 1.109891, Accuracy: 67.92%\n",
      "Batch 16, Loss: 1.028223, Accuracy: 68.16%\n",
      "Batch 17, Loss: 0.997089, Accuracy: 68.57%\n",
      "Batch 18, Loss: 1.055116, Accuracy: 68.49%\n",
      "Batch 19, Loss: 1.064127, Accuracy: 68.34%\n",
      "Batch 20, Loss: 1.020552, Accuracy: 68.52%\n",
      "Batch 21, Loss: 0.910615, Accuracy: 69.35%\n",
      "Batch 22, Loss: 1.040086, Accuracy: 69.39%\n",
      "Batch 23, Loss: 1.049529, Accuracy: 69.36%\n",
      "Batch 24, Loss: 1.006356, Accuracy: 69.53%\n",
      "Batch 25, Loss: 1.095260, Accuracy: 69.38%\n",
      "Batch 26, Loss: 1.022094, Accuracy: 69.41%\n",
      "Batch 27, Loss: 1.073232, Accuracy: 69.27%\n",
      "Batch 28, Loss: 0.953169, Accuracy: 69.64%\n",
      "Batch 29, Loss: 1.055292, Accuracy: 69.67%\n",
      "Batch 30, Loss: 1.085878, Accuracy: 69.48%\n",
      "Batch 31, Loss: 1.100632, Accuracy: 69.35%\n",
      "Batch 32, Loss: 1.036158, Accuracy: 69.38%\n",
      "Batch 33, Loss: 1.058217, Accuracy: 69.37%\n",
      "Batch 34, Loss: 1.086012, Accuracy: 69.26%\n",
      "Batch 35, Loss: 1.035917, Accuracy: 69.33%\n",
      "Batch 36, Loss: 1.003844, Accuracy: 69.49%\n",
      "Batch 37, Loss: 1.039531, Accuracy: 69.55%\n",
      "Batch 38, Loss: 1.099834, Accuracy: 69.41%\n",
      "Batch 39, Loss: 1.103588, Accuracy: 69.31%\n",
      "Batch 40, Loss: 0.988146, Accuracy: 69.49%\n",
      "Batch 41, Loss: 1.068518, Accuracy: 69.44%\n",
      "Batch 42, Loss: 0.980965, Accuracy: 69.61%\n",
      "Batch 43, Loss: 1.083539, Accuracy: 69.51%\n",
      "Batch 44, Loss: 1.016530, Accuracy: 69.60%\n",
      "Batch 45, Loss: 0.947573, Accuracy: 69.83%\n",
      "Batch 46, Loss: 0.980942, Accuracy: 69.97%\n",
      "Batch 47, Loss: 1.010830, Accuracy: 70.01%\n",
      "Batch 48, Loss: 0.994012, Accuracy: 70.12%\n",
      "Batch 49, Loss: 1.070330, Accuracy: 70.06%\n",
      "Batch 50, Loss: 1.051146, Accuracy: 70.06%\n",
      "Batch 51, Loss: 0.982581, Accuracy: 70.22%\n",
      "Batch 52, Loss: 1.005310, Accuracy: 70.31%\n",
      "Batch 53, Loss: 1.028829, Accuracy: 70.34%\n",
      "Batch 54, Loss: 1.114735, Accuracy: 70.20%\n",
      "Batch 55, Loss: 1.017129, Accuracy: 70.20%\n",
      "Batch 56, Loss: 1.012393, Accuracy: 70.23%\n",
      "Batch 57, Loss: 1.086007, Accuracy: 70.15%\n",
      "Batch 58, Loss: 1.099521, Accuracy: 70.04%\n",
      "Batch 59, Loss: 0.972863, Accuracy: 70.18%\n",
      "Batch 60, Loss: 0.992916, Accuracy: 70.29%\n",
      "Batch 61, Loss: 1.059809, Accuracy: 70.24%\n",
      "Batch 62, Loss: 1.028437, Accuracy: 70.24%\n",
      "Batch 63, Loss: 1.020661, Accuracy: 70.29%\n",
      "Batch 64, Loss: 0.992248, Accuracy: 70.31%\n",
      "Batch 65, Loss: 0.965101, Accuracy: 70.41%\n",
      "Batch 66, Loss: 1.014251, Accuracy: 70.41%\n",
      "Batch 67, Loss: 1.130345, Accuracy: 70.20%\n",
      "Batch 68, Loss: 1.040093, Accuracy: 70.17%\n",
      "Batch 69, Loss: 0.997886, Accuracy: 70.22%\n",
      "Batch 70, Loss: 1.073266, Accuracy: 70.16%\n",
      "Batch 71, Loss: 0.999525, Accuracy: 70.22%\n",
      "Batch 72, Loss: 1.050607, Accuracy: 70.27%\n",
      "Batch 73, Loss: 1.002367, Accuracy: 70.33%\n",
      "Batch 74, Loss: 1.035345, Accuracy: 70.33%\n",
      "Batch 75, Loss: 0.962070, Accuracy: 70.42%\n",
      "Batch 76, Loss: 1.039956, Accuracy: 70.42%\n",
      "Batch 77, Loss: 1.089511, Accuracy: 70.35%\n",
      "Batch 78, Loss: 1.085116, Accuracy: 70.27%\n",
      "Batch 79, Loss: 0.994248, Accuracy: 70.35%\n",
      "Batch 80, Loss: 1.038835, Accuracy: 70.35%\n",
      "Batch 81, Loss: 1.017577, Accuracy: 70.41%\n",
      "Batch 82, Loss: 1.087051, Accuracy: 70.31%\n",
      "Batch 83, Loss: 1.018200, Accuracy: 70.35%\n",
      "Batch 84, Loss: 0.996812, Accuracy: 70.37%\n",
      "Batch 85, Loss: 1.152667, Accuracy: 70.20%\n",
      "Batch 86, Loss: 1.010963, Accuracy: 70.22%\n",
      "Batch 87, Loss: 0.991705, Accuracy: 70.29%\n",
      "Batch 88, Loss: 1.067195, Accuracy: 70.26%\n",
      "Batch 89, Loss: 1.027333, Accuracy: 70.26%\n",
      "Batch 90, Loss: 1.039330, Accuracy: 70.28%\n",
      "Batch 91, Loss: 0.978853, Accuracy: 70.30%\n",
      "Batch 92, Loss: 0.991776, Accuracy: 70.35%\n",
      "Batch 93, Loss: 1.145024, Accuracy: 70.21%\n",
      "Batch 94, Loss: 0.960838, Accuracy: 70.31%\n",
      "Batch 95, Loss: 1.097778, Accuracy: 70.28%\n",
      "Batch 96, Loss: 0.981925, Accuracy: 70.38%\n",
      "Batch 97, Loss: 1.154478, Accuracy: 70.20%\n",
      "Batch 98, Loss: 1.044078, Accuracy: 70.22%\n",
      "Batch 99, Loss: 1.094146, Accuracy: 70.15%\n",
      "Batch 100, Loss: 1.081827, Accuracy: 70.09%\n",
      "Batch 101, Loss: 1.123089, Accuracy: 70.02%\n",
      "Batch 102, Loss: 1.033482, Accuracy: 70.01%\n",
      "Batch 103, Loss: 0.914190, Accuracy: 70.15%\n",
      "Batch 104, Loss: 1.076132, Accuracy: 70.13%\n",
      "Batch 105, Loss: 1.002302, Accuracy: 70.16%\n",
      "Batch 106, Loss: 1.065463, Accuracy: 70.14%\n",
      "Batch 107, Loss: 1.073606, Accuracy: 70.09%\n",
      "Batch 108, Loss: 1.021662, Accuracy: 70.12%\n",
      "Batch 109, Loss: 0.993100, Accuracy: 70.17%\n",
      "Batch 110, Loss: 1.046924, Accuracy: 70.16%\n",
      "Batch 111, Loss: 1.062951, Accuracy: 70.12%\n",
      "Batch 112, Loss: 1.046581, Accuracy: 70.12%\n",
      "Batch 113, Loss: 1.046901, Accuracy: 70.13%\n",
      "Batch 114, Loss: 1.026711, Accuracy: 70.13%\n",
      "Batch 115, Loss: 0.993959, Accuracy: 70.16%\n",
      "Batch 116, Loss: 1.078053, Accuracy: 70.12%\n",
      "Batch 117, Loss: 1.019567, Accuracy: 70.15%\n",
      "Batch 118, Loss: 1.047402, Accuracy: 70.14%\n",
      "Batch 119, Loss: 1.046378, Accuracy: 70.14%\n",
      "Batch 120, Loss: 0.982082, Accuracy: 70.20%\n",
      "Batch 121, Loss: 0.996306, Accuracy: 70.25%\n",
      "Batch 122, Loss: 1.064985, Accuracy: 70.24%\n",
      "Batch 123, Loss: 1.047899, Accuracy: 70.22%\n",
      "Batch 124, Loss: 1.080157, Accuracy: 70.20%\n",
      "Batch 125, Loss: 1.131778, Accuracy: 70.12%\n",
      "Batch 126, Loss: 1.024300, Accuracy: 70.14%\n",
      "Batch 127, Loss: 0.993210, Accuracy: 70.18%\n",
      "Batch 128, Loss: 0.989960, Accuracy: 70.21%\n",
      "Batch 129, Loss: 1.049780, Accuracy: 70.18%\n",
      "Batch 130, Loss: 1.072689, Accuracy: 70.14%\n",
      "Batch 131, Loss: 1.037701, Accuracy: 70.13%\n",
      "Batch 132, Loss: 1.053228, Accuracy: 70.11%\n",
      "Batch 133, Loss: 1.005787, Accuracy: 70.15%\n",
      "Batch 134, Loss: 1.034956, Accuracy: 70.17%\n",
      "Batch 135, Loss: 1.016330, Accuracy: 70.19%\n",
      "Batch 136, Loss: 1.032383, Accuracy: 70.19%\n",
      "Batch 137, Loss: 0.985502, Accuracy: 70.22%\n",
      "Batch 138, Loss: 0.990493, Accuracy: 70.26%\n",
      "Batch 139, Loss: 1.026001, Accuracy: 70.28%\n",
      "Batch 140, Loss: 1.081592, Accuracy: 70.23%\n",
      "Batch 141, Loss: 0.955389, Accuracy: 70.31%\n",
      "Batch 142, Loss: 1.081437, Accuracy: 70.26%\n",
      "Batch 143, Loss: 1.055962, Accuracy: 70.24%\n",
      "Batch 144, Loss: 1.026531, Accuracy: 70.26%\n",
      "Batch 145, Loss: 1.032455, Accuracy: 70.27%\n",
      "Batch 146, Loss: 1.008901, Accuracy: 70.29%\n",
      "Batch 147, Loss: 1.099335, Accuracy: 70.26%\n",
      "Batch 148, Loss: 1.091543, Accuracy: 70.22%\n",
      "Batch 149, Loss: 1.067490, Accuracy: 70.22%\n",
      "Batch 150, Loss: 1.031821, Accuracy: 70.21%\n",
      "Batch 151, Loss: 1.033549, Accuracy: 70.22%\n",
      "Batch 152, Loss: 1.051630, Accuracy: 70.21%\n",
      "Batch 153, Loss: 0.985670, Accuracy: 70.25%\n",
      "Batch 154, Loss: 1.045422, Accuracy: 70.25%\n",
      "Batch 155, Loss: 1.008445, Accuracy: 70.28%\n",
      "Batch 156, Loss: 1.017137, Accuracy: 70.30%\n",
      "Batch 157, Loss: 1.141608, Accuracy: 70.22%\n",
      "Batch 158, Loss: 0.943519, Accuracy: 70.29%\n",
      "Batch 159, Loss: 0.956018, Accuracy: 70.35%\n",
      "Batch 160, Loss: 1.055352, Accuracy: 70.34%\n",
      "Batch 161, Loss: 1.016338, Accuracy: 70.36%\n",
      "Batch 162, Loss: 1.006724, Accuracy: 70.36%\n",
      "Batch 163, Loss: 1.015283, Accuracy: 70.38%\n",
      "Batch 164, Loss: 1.064227, Accuracy: 70.35%\n",
      "Batch 165, Loss: 1.037056, Accuracy: 70.35%\n",
      "Batch 166, Loss: 1.057256, Accuracy: 70.35%\n",
      "Batch 167, Loss: 1.094367, Accuracy: 70.32%\n",
      "Batch 168, Loss: 0.967902, Accuracy: 70.36%\n",
      "Batch 169, Loss: 0.941698, Accuracy: 70.43%\n",
      "Batch 170, Loss: 1.045581, Accuracy: 70.42%\n",
      "Batch 171, Loss: 1.020546, Accuracy: 70.42%\n",
      "Batch 172, Loss: 1.119467, Accuracy: 70.38%\n",
      "Batch 173, Loss: 1.060380, Accuracy: 70.38%\n",
      "Batch 174, Loss: 0.991549, Accuracy: 70.42%\n",
      "Batch 175, Loss: 1.035025, Accuracy: 70.42%\n",
      "Batch 176, Loss: 1.062712, Accuracy: 70.40%\n",
      "Batch 177, Loss: 1.042189, Accuracy: 70.38%\n",
      "Batch 178, Loss: 1.027763, Accuracy: 70.39%\n",
      "Batch 179, Loss: 0.958956, Accuracy: 70.43%\n",
      "Batch 180, Loss: 1.049065, Accuracy: 70.43%\n",
      "Batch 181, Loss: 1.010881, Accuracy: 70.42%\n",
      "Batch 182, Loss: 0.978764, Accuracy: 70.47%\n",
      "Batch 183, Loss: 1.056297, Accuracy: 70.46%\n",
      "Batch 184, Loss: 1.099408, Accuracy: 70.42%\n",
      "Batch 185, Loss: 1.096808, Accuracy: 70.38%\n",
      "Batch 186, Loss: 0.996434, Accuracy: 70.40%\n",
      "Batch 187, Loss: 1.000042, Accuracy: 70.41%\n",
      "Batch 188, Loss: 1.083026, Accuracy: 70.38%\n",
      "Batch 189, Loss: 1.003309, Accuracy: 70.40%\n",
      "Batch 190, Loss: 1.135754, Accuracy: 70.35%\n",
      "Batch 191, Loss: 1.007772, Accuracy: 70.35%\n",
      "Batch 192, Loss: 0.962288, Accuracy: 70.39%\n",
      "Batch 193, Loss: 0.999353, Accuracy: 70.40%\n",
      "Batch 194, Loss: 0.994345, Accuracy: 70.43%\n",
      "Batch 195, Loss: 1.066030, Accuracy: 70.41%\n",
      "Batch 196, Loss: 0.927638, Accuracy: 70.47%\n",
      "Batch 197, Loss: 1.004025, Accuracy: 70.49%\n",
      "Batch 198, Loss: 1.026992, Accuracy: 70.49%\n",
      "Batch 199, Loss: 1.038661, Accuracy: 70.49%\n",
      "Batch 200, Loss: 1.081384, Accuracy: 70.47%\n",
      "Batch 201, Loss: 1.065559, Accuracy: 70.45%\n",
      "Batch 202, Loss: 1.064563, Accuracy: 70.44%\n",
      "Batch 203, Loss: 1.063885, Accuracy: 70.41%\n",
      "Batch 204, Loss: 1.060621, Accuracy: 70.40%\n",
      "Batch 205, Loss: 1.033879, Accuracy: 70.39%\n",
      "Batch 206, Loss: 1.005774, Accuracy: 70.40%\n",
      "Batch 207, Loss: 0.961478, Accuracy: 70.45%\n",
      "Batch 208, Loss: 0.986635, Accuracy: 70.46%\n",
      "Batch 209, Loss: 1.013706, Accuracy: 70.48%\n",
      "Batch 210, Loss: 1.017612, Accuracy: 70.49%\n",
      "Batch 211, Loss: 1.088715, Accuracy: 70.47%\n",
      "Batch 212, Loss: 1.021452, Accuracy: 70.47%\n",
      "Batch 213, Loss: 0.956124, Accuracy: 70.51%\n",
      "Training - Epoch 37, Loss: 1.035551, Accuracy: 70.51%\n",
      "Validation Batch 1, Loss: 1.052236, Accuracy: 67.19%\n",
      "Validation Batch 2, Loss: 1.184092, Accuracy: 60.94%\n",
      "Validation Batch 3, Loss: 1.174629, Accuracy: 59.90%\n",
      "Validation Batch 4, Loss: 1.058288, Accuracy: 62.50%\n",
      "Validation Batch 5, Loss: 1.129249, Accuracy: 61.56%\n",
      "Validation Batch 6, Loss: 1.080587, Accuracy: 61.98%\n",
      "Validation Batch 7, Loss: 1.113369, Accuracy: 62.50%\n",
      "Validation Batch 8, Loss: 1.128416, Accuracy: 62.70%\n",
      "Validation Batch 9, Loss: 1.155427, Accuracy: 62.15%\n",
      "Validation Batch 10, Loss: 1.117652, Accuracy: 62.34%\n",
      "Validation Batch 11, Loss: 1.085146, Accuracy: 62.64%\n",
      "Validation Batch 12, Loss: 1.052997, Accuracy: 63.28%\n",
      "Validation Batch 13, Loss: 1.177058, Accuracy: 62.74%\n",
      "Validation Batch 14, Loss: 1.118136, Accuracy: 62.61%\n",
      "Validation Batch 15, Loss: 1.126720, Accuracy: 62.50%\n",
      "Validation Batch 16, Loss: 1.092033, Accuracy: 62.60%\n",
      "Validation Batch 17, Loss: 1.195266, Accuracy: 61.95%\n",
      "Validation Batch 18, Loss: 1.080111, Accuracy: 62.15%\n",
      "Validation Batch 19, Loss: 1.177436, Accuracy: 61.68%\n",
      "Validation Batch 20, Loss: 1.114589, Accuracy: 61.80%\n",
      "Validation Batch 21, Loss: 1.110913, Accuracy: 61.76%\n",
      "Validation Batch 22, Loss: 1.176072, Accuracy: 61.36%\n",
      "Validation Batch 23, Loss: 1.207657, Accuracy: 61.01%\n",
      "Validation Batch 24, Loss: 1.137938, Accuracy: 61.00%\n",
      "Validation Batch 25, Loss: 1.111303, Accuracy: 61.06%\n",
      "Validation Batch 26, Loss: 1.073484, Accuracy: 61.24%\n",
      "Validation Batch 27, Loss: 1.122763, Accuracy: 61.30%\n",
      "Validation - Epoch 37, Loss: 1.124206, Accuracy: 61.30%\n",
      "Patienceâ€”0\n",
      "Epoch 38\n",
      "Batch 1, Loss: 1.073237, Accuracy: 67.19%\n",
      "Batch 2, Loss: 0.977280, Accuracy: 71.88%\n",
      "Batch 3, Loss: 1.064837, Accuracy: 70.31%\n",
      "Batch 4, Loss: 1.141895, Accuracy: 66.80%\n",
      "Batch 5, Loss: 0.919180, Accuracy: 70.31%\n",
      "Batch 6, Loss: 1.091733, Accuracy: 69.27%\n",
      "Batch 7, Loss: 0.992201, Accuracy: 70.31%\n",
      "Batch 8, Loss: 1.003690, Accuracy: 70.70%\n",
      "Batch 9, Loss: 1.040889, Accuracy: 70.83%\n",
      "Batch 10, Loss: 1.055325, Accuracy: 70.62%\n",
      "Batch 11, Loss: 1.006544, Accuracy: 70.88%\n",
      "Batch 12, Loss: 1.059560, Accuracy: 70.70%\n",
      "Batch 13, Loss: 0.989852, Accuracy: 71.15%\n",
      "Batch 14, Loss: 1.032468, Accuracy: 71.09%\n",
      "Batch 15, Loss: 1.034450, Accuracy: 71.15%\n",
      "Batch 16, Loss: 1.018348, Accuracy: 71.00%\n",
      "Batch 17, Loss: 0.993519, Accuracy: 71.23%\n",
      "Batch 18, Loss: 1.048522, Accuracy: 71.09%\n",
      "Batch 19, Loss: 1.062146, Accuracy: 70.81%\n",
      "Batch 20, Loss: 1.033877, Accuracy: 70.86%\n",
      "Batch 21, Loss: 1.067943, Accuracy: 70.61%\n",
      "Batch 22, Loss: 0.974352, Accuracy: 70.95%\n",
      "Batch 23, Loss: 1.006970, Accuracy: 70.99%\n",
      "Batch 24, Loss: 0.982245, Accuracy: 71.29%\n",
      "Batch 25, Loss: 1.101907, Accuracy: 70.75%\n",
      "Batch 26, Loss: 1.048489, Accuracy: 70.73%\n",
      "Batch 27, Loss: 1.064158, Accuracy: 70.60%\n",
      "Batch 28, Loss: 1.043198, Accuracy: 70.54%\n",
      "Batch 29, Loss: 1.178231, Accuracy: 70.10%\n",
      "Batch 30, Loss: 1.132761, Accuracy: 69.74%\n",
      "Batch 31, Loss: 1.048621, Accuracy: 69.71%\n",
      "Batch 32, Loss: 1.073500, Accuracy: 69.63%\n",
      "Batch 33, Loss: 1.036694, Accuracy: 69.65%\n",
      "Batch 34, Loss: 0.926632, Accuracy: 70.04%\n",
      "Batch 35, Loss: 0.940249, Accuracy: 70.31%\n",
      "Batch 36, Loss: 1.011586, Accuracy: 70.40%\n",
      "Batch 37, Loss: 1.073498, Accuracy: 70.27%\n",
      "Batch 38, Loss: 0.955242, Accuracy: 70.44%\n",
      "Batch 39, Loss: 1.012994, Accuracy: 70.47%\n",
      "Batch 40, Loss: 1.110999, Accuracy: 70.27%\n",
      "Batch 41, Loss: 0.941682, Accuracy: 70.54%\n",
      "Batch 42, Loss: 0.988983, Accuracy: 70.68%\n",
      "Batch 43, Loss: 0.985006, Accuracy: 70.86%\n",
      "Batch 44, Loss: 1.035392, Accuracy: 70.88%\n",
      "Batch 45, Loss: 1.016383, Accuracy: 70.87%\n",
      "Batch 46, Loss: 0.948324, Accuracy: 71.03%\n",
      "Batch 47, Loss: 1.160258, Accuracy: 70.71%\n",
      "Batch 48, Loss: 1.101395, Accuracy: 70.57%\n",
      "Batch 49, Loss: 1.075755, Accuracy: 70.47%\n",
      "Batch 50, Loss: 1.137662, Accuracy: 70.28%\n",
      "Batch 51, Loss: 1.055220, Accuracy: 70.22%\n",
      "Batch 52, Loss: 1.025112, Accuracy: 70.25%\n",
      "Batch 53, Loss: 1.120892, Accuracy: 70.11%\n",
      "Batch 54, Loss: 1.017886, Accuracy: 70.14%\n",
      "Batch 55, Loss: 1.085676, Accuracy: 70.09%\n",
      "Batch 56, Loss: 1.086923, Accuracy: 70.03%\n",
      "Batch 57, Loss: 0.989184, Accuracy: 70.12%\n",
      "Batch 58, Loss: 0.987256, Accuracy: 70.23%\n",
      "Batch 59, Loss: 1.058545, Accuracy: 70.23%\n",
      "Batch 60, Loss: 1.050395, Accuracy: 70.21%\n",
      "Batch 61, Loss: 0.970317, Accuracy: 70.36%\n",
      "Batch 62, Loss: 0.951312, Accuracy: 70.51%\n",
      "Batch 63, Loss: 0.906446, Accuracy: 70.71%\n",
      "Batch 64, Loss: 1.025928, Accuracy: 70.70%\n",
      "Batch 65, Loss: 1.043289, Accuracy: 70.72%\n",
      "Batch 66, Loss: 1.143098, Accuracy: 70.55%\n",
      "Batch 67, Loss: 0.995915, Accuracy: 70.59%\n",
      "Batch 68, Loss: 1.107415, Accuracy: 70.47%\n",
      "Batch 69, Loss: 1.033916, Accuracy: 70.47%\n",
      "Batch 70, Loss: 1.024509, Accuracy: 70.47%\n",
      "Batch 71, Loss: 1.036012, Accuracy: 70.51%\n",
      "Batch 72, Loss: 1.055390, Accuracy: 70.51%\n",
      "Batch 73, Loss: 1.046926, Accuracy: 70.51%\n",
      "Batch 74, Loss: 1.044421, Accuracy: 70.50%\n",
      "Batch 75, Loss: 1.015085, Accuracy: 70.52%\n",
      "Batch 76, Loss: 1.042988, Accuracy: 70.54%\n",
      "Batch 77, Loss: 0.940764, Accuracy: 70.68%\n",
      "Batch 78, Loss: 1.004896, Accuracy: 70.73%\n",
      "Batch 79, Loss: 1.001686, Accuracy: 70.79%\n",
      "Batch 80, Loss: 0.991771, Accuracy: 70.82%\n",
      "Batch 81, Loss: 1.015648, Accuracy: 70.81%\n",
      "Batch 82, Loss: 1.006598, Accuracy: 70.85%\n",
      "Batch 83, Loss: 1.060493, Accuracy: 70.80%\n",
      "Batch 84, Loss: 1.031001, Accuracy: 70.80%\n",
      "Batch 85, Loss: 1.061250, Accuracy: 70.81%\n",
      "Batch 86, Loss: 1.057709, Accuracy: 70.78%\n",
      "Batch 87, Loss: 1.080678, Accuracy: 70.76%\n",
      "Batch 88, Loss: 0.990719, Accuracy: 70.81%\n",
      "Batch 89, Loss: 1.002150, Accuracy: 70.82%\n",
      "Batch 90, Loss: 1.054648, Accuracy: 70.80%\n",
      "Batch 91, Loss: 1.025957, Accuracy: 70.83%\n",
      "Batch 92, Loss: 1.065506, Accuracy: 70.81%\n",
      "Batch 93, Loss: 0.940151, Accuracy: 70.92%\n",
      "Batch 94, Loss: 1.039436, Accuracy: 70.93%\n",
      "Batch 95, Loss: 1.109030, Accuracy: 70.82%\n",
      "Batch 96, Loss: 1.108993, Accuracy: 70.74%\n",
      "Batch 97, Loss: 1.069378, Accuracy: 70.70%\n",
      "Batch 98, Loss: 0.984198, Accuracy: 70.73%\n",
      "Batch 99, Loss: 1.085165, Accuracy: 70.69%\n",
      "Batch 100, Loss: 0.985599, Accuracy: 70.77%\n",
      "Batch 101, Loss: 1.061870, Accuracy: 70.75%\n",
      "Batch 102, Loss: 1.032547, Accuracy: 70.74%\n",
      "Batch 103, Loss: 1.019957, Accuracy: 70.71%\n",
      "Batch 104, Loss: 0.967469, Accuracy: 70.76%\n",
      "Batch 105, Loss: 1.046960, Accuracy: 70.73%\n",
      "Batch 106, Loss: 1.042856, Accuracy: 70.74%\n",
      "Batch 107, Loss: 1.041930, Accuracy: 70.72%\n",
      "Batch 108, Loss: 0.977723, Accuracy: 70.79%\n",
      "Batch 109, Loss: 1.050568, Accuracy: 70.79%\n",
      "Batch 110, Loss: 1.061323, Accuracy: 70.77%\n",
      "Batch 111, Loss: 1.056344, Accuracy: 70.75%\n",
      "Batch 112, Loss: 1.031217, Accuracy: 70.74%\n",
      "Batch 113, Loss: 1.064960, Accuracy: 70.70%\n",
      "Batch 114, Loss: 1.071426, Accuracy: 70.66%\n",
      "Batch 115, Loss: 1.029485, Accuracy: 70.67%\n",
      "Batch 116, Loss: 1.160097, Accuracy: 70.54%\n",
      "Batch 117, Loss: 0.972808, Accuracy: 70.61%\n",
      "Batch 118, Loss: 0.980239, Accuracy: 70.66%\n",
      "Batch 119, Loss: 0.932505, Accuracy: 70.76%\n",
      "Batch 120, Loss: 1.057240, Accuracy: 70.73%\n",
      "Batch 121, Loss: 0.983738, Accuracy: 70.76%\n",
      "Batch 122, Loss: 0.994631, Accuracy: 70.79%\n",
      "Batch 123, Loss: 1.034092, Accuracy: 70.77%\n",
      "Batch 124, Loss: 1.056513, Accuracy: 70.74%\n",
      "Batch 125, Loss: 1.038771, Accuracy: 70.76%\n",
      "Batch 126, Loss: 0.983418, Accuracy: 70.81%\n",
      "Batch 127, Loss: 1.001691, Accuracy: 70.82%\n",
      "Batch 128, Loss: 0.960085, Accuracy: 70.90%\n",
      "Batch 129, Loss: 1.021608, Accuracy: 70.91%\n",
      "Batch 130, Loss: 0.963838, Accuracy: 70.97%\n",
      "Batch 131, Loss: 1.086128, Accuracy: 70.92%\n",
      "Batch 132, Loss: 1.017754, Accuracy: 70.93%\n",
      "Batch 133, Loss: 1.007048, Accuracy: 70.92%\n",
      "Batch 134, Loss: 1.061750, Accuracy: 70.88%\n",
      "Batch 135, Loss: 0.973960, Accuracy: 70.94%\n",
      "Batch 136, Loss: 1.083222, Accuracy: 70.91%\n",
      "Batch 137, Loss: 1.044591, Accuracy: 70.92%\n",
      "Batch 138, Loss: 1.106327, Accuracy: 70.87%\n",
      "Batch 139, Loss: 1.072309, Accuracy: 70.83%\n",
      "Batch 140, Loss: 1.039384, Accuracy: 70.81%\n",
      "Batch 141, Loss: 1.027457, Accuracy: 70.82%\n",
      "Batch 142, Loss: 1.148648, Accuracy: 70.72%\n",
      "Batch 143, Loss: 1.034261, Accuracy: 70.73%\n",
      "Batch 144, Loss: 0.993236, Accuracy: 70.77%\n",
      "Batch 145, Loss: 0.935965, Accuracy: 70.83%\n",
      "Batch 146, Loss: 0.993920, Accuracy: 70.86%\n",
      "Batch 147, Loss: 1.005414, Accuracy: 70.89%\n",
      "Batch 148, Loss: 1.080601, Accuracy: 70.85%\n",
      "Batch 149, Loss: 1.044094, Accuracy: 70.84%\n",
      "Batch 150, Loss: 1.175814, Accuracy: 70.74%\n",
      "Batch 151, Loss: 1.064375, Accuracy: 70.72%\n",
      "Batch 152, Loss: 1.038382, Accuracy: 70.72%\n",
      "Batch 153, Loss: 1.046058, Accuracy: 70.72%\n",
      "Batch 154, Loss: 1.080899, Accuracy: 70.69%\n",
      "Batch 155, Loss: 1.026176, Accuracy: 70.69%\n",
      "Batch 156, Loss: 1.080432, Accuracy: 70.66%\n",
      "Batch 157, Loss: 0.997794, Accuracy: 70.69%\n",
      "Batch 158, Loss: 1.069819, Accuracy: 70.67%\n",
      "Batch 159, Loss: 0.991512, Accuracy: 70.70%\n",
      "Batch 160, Loss: 0.974478, Accuracy: 70.73%\n",
      "Batch 161, Loss: 1.037977, Accuracy: 70.73%\n",
      "Batch 162, Loss: 0.983837, Accuracy: 70.77%\n",
      "Batch 163, Loss: 1.108804, Accuracy: 70.72%\n",
      "Batch 164, Loss: 1.050517, Accuracy: 70.69%\n",
      "Batch 165, Loss: 1.121286, Accuracy: 70.62%\n",
      "Batch 166, Loss: 0.964599, Accuracy: 70.67%\n",
      "Batch 167, Loss: 0.970195, Accuracy: 70.71%\n",
      "Batch 168, Loss: 1.003599, Accuracy: 70.74%\n",
      "Batch 169, Loss: 1.056799, Accuracy: 70.72%\n",
      "Batch 170, Loss: 1.008418, Accuracy: 70.74%\n",
      "Batch 171, Loss: 1.128304, Accuracy: 70.67%\n",
      "Batch 172, Loss: 0.980452, Accuracy: 70.71%\n",
      "Batch 173, Loss: 1.016805, Accuracy: 70.73%\n",
      "Batch 174, Loss: 1.092515, Accuracy: 70.69%\n",
      "Batch 175, Loss: 1.023909, Accuracy: 70.68%\n",
      "Batch 176, Loss: 1.077684, Accuracy: 70.65%\n",
      "Batch 177, Loss: 1.077843, Accuracy: 70.63%\n",
      "Batch 178, Loss: 1.114286, Accuracy: 70.58%\n",
      "Batch 179, Loss: 1.039774, Accuracy: 70.58%\n",
      "Batch 180, Loss: 0.987860, Accuracy: 70.61%\n",
      "Batch 181, Loss: 1.045891, Accuracy: 70.60%\n",
      "Batch 182, Loss: 1.081764, Accuracy: 70.55%\n",
      "Batch 183, Loss: 1.017445, Accuracy: 70.57%\n",
      "Batch 184, Loss: 0.992436, Accuracy: 70.59%\n",
      "Batch 185, Loss: 0.965379, Accuracy: 70.64%\n",
      "Batch 186, Loss: 1.062838, Accuracy: 70.63%\n",
      "Batch 187, Loss: 1.024418, Accuracy: 70.62%\n",
      "Batch 188, Loss: 1.048965, Accuracy: 70.62%\n",
      "Batch 189, Loss: 0.995991, Accuracy: 70.65%\n",
      "Batch 190, Loss: 0.906501, Accuracy: 70.72%\n",
      "Batch 191, Loss: 1.043861, Accuracy: 70.73%\n",
      "Batch 192, Loss: 1.069595, Accuracy: 70.71%\n",
      "Batch 193, Loss: 1.098745, Accuracy: 70.68%\n",
      "Batch 194, Loss: 0.960730, Accuracy: 70.72%\n",
      "Batch 195, Loss: 1.010066, Accuracy: 70.71%\n",
      "Batch 196, Loss: 0.959535, Accuracy: 70.76%\n",
      "Batch 197, Loss: 1.090573, Accuracy: 70.73%\n",
      "Batch 198, Loss: 0.970442, Accuracy: 70.77%\n",
      "Batch 199, Loss: 0.962968, Accuracy: 70.80%\n",
      "Batch 200, Loss: 1.041907, Accuracy: 70.79%\n",
      "Batch 201, Loss: 1.018496, Accuracy: 70.79%\n",
      "Batch 202, Loss: 1.064422, Accuracy: 70.78%\n",
      "Batch 203, Loss: 0.963389, Accuracy: 70.82%\n",
      "Batch 204, Loss: 1.029720, Accuracy: 70.83%\n",
      "Batch 205, Loss: 1.080962, Accuracy: 70.80%\n",
      "Batch 206, Loss: 1.004963, Accuracy: 70.81%\n",
      "Batch 207, Loss: 0.988395, Accuracy: 70.83%\n",
      "Batch 208, Loss: 1.061165, Accuracy: 70.81%\n",
      "Batch 209, Loss: 1.013459, Accuracy: 70.82%\n",
      "Batch 210, Loss: 1.029173, Accuracy: 70.83%\n",
      "Batch 211, Loss: 1.147454, Accuracy: 70.77%\n",
      "Batch 212, Loss: 1.012043, Accuracy: 70.77%\n",
      "Batch 213, Loss: 1.008508, Accuracy: 70.78%\n",
      "Training - Epoch 38, Loss: 1.033200, Accuracy: 70.78%\n",
      "Validation Batch 1, Loss: 1.071549, Accuracy: 67.19%\n",
      "Validation Batch 2, Loss: 1.199159, Accuracy: 59.38%\n",
      "Validation Batch 3, Loss: 1.187302, Accuracy: 58.33%\n",
      "Validation Batch 4, Loss: 1.071844, Accuracy: 60.94%\n",
      "Validation Batch 5, Loss: 1.149262, Accuracy: 60.31%\n",
      "Validation Batch 6, Loss: 1.098233, Accuracy: 60.94%\n",
      "Validation Batch 7, Loss: 1.124911, Accuracy: 60.94%\n",
      "Validation Batch 8, Loss: 1.135349, Accuracy: 61.13%\n",
      "Validation Batch 9, Loss: 1.167923, Accuracy: 60.59%\n",
      "Validation Batch 10, Loss: 1.138718, Accuracy: 60.62%\n",
      "Validation Batch 11, Loss: 1.103323, Accuracy: 60.80%\n",
      "Validation Batch 12, Loss: 1.073316, Accuracy: 61.33%\n",
      "Validation Batch 13, Loss: 1.189967, Accuracy: 61.06%\n",
      "Validation Batch 14, Loss: 1.133221, Accuracy: 60.83%\n",
      "Validation Batch 15, Loss: 1.138345, Accuracy: 60.52%\n",
      "Validation Batch 16, Loss: 1.111372, Accuracy: 60.64%\n",
      "Validation Batch 17, Loss: 1.219172, Accuracy: 59.93%\n",
      "Validation Batch 18, Loss: 1.097026, Accuracy: 60.24%\n",
      "Validation Batch 19, Loss: 1.201760, Accuracy: 59.79%\n",
      "Validation Batch 20, Loss: 1.131078, Accuracy: 59.92%\n",
      "Validation Batch 21, Loss: 1.124510, Accuracy: 59.97%\n",
      "Validation Batch 22, Loss: 1.196501, Accuracy: 59.66%\n",
      "Validation Batch 23, Loss: 1.225347, Accuracy: 59.10%\n",
      "Validation Batch 24, Loss: 1.149578, Accuracy: 59.18%\n",
      "Validation Batch 25, Loss: 1.129156, Accuracy: 59.25%\n",
      "Validation Batch 26, Loss: 1.081235, Accuracy: 59.50%\n",
      "Validation Batch 27, Loss: 1.143473, Accuracy: 59.48%\n",
      "Validation - Epoch 38, Loss: 1.140468, Accuracy: 59.48%\n",
      "Patienceâ€”1\n",
      "Epoch 39\n",
      "Batch 1, Loss: 0.953964, Accuracy: 78.12%\n",
      "Batch 2, Loss: 1.083433, Accuracy: 71.88%\n",
      "Batch 3, Loss: 1.077648, Accuracy: 69.27%\n",
      "Batch 4, Loss: 1.039446, Accuracy: 68.36%\n",
      "Batch 5, Loss: 1.050534, Accuracy: 68.12%\n",
      "Batch 6, Loss: 1.100447, Accuracy: 67.45%\n",
      "Batch 7, Loss: 1.004327, Accuracy: 68.53%\n",
      "Batch 8, Loss: 0.998591, Accuracy: 69.53%\n",
      "Batch 9, Loss: 1.034442, Accuracy: 69.62%\n",
      "Batch 10, Loss: 1.058767, Accuracy: 69.38%\n",
      "Batch 11, Loss: 1.072880, Accuracy: 69.18%\n",
      "Batch 12, Loss: 1.022356, Accuracy: 69.27%\n",
      "Batch 13, Loss: 1.016052, Accuracy: 69.59%\n",
      "Batch 14, Loss: 1.074714, Accuracy: 69.53%\n",
      "Batch 15, Loss: 0.980703, Accuracy: 70.00%\n",
      "Batch 16, Loss: 1.057664, Accuracy: 70.02%\n",
      "Batch 17, Loss: 1.018370, Accuracy: 70.13%\n",
      "Batch 18, Loss: 0.952576, Accuracy: 70.57%\n",
      "Batch 19, Loss: 1.134659, Accuracy: 69.98%\n",
      "Batch 20, Loss: 0.955617, Accuracy: 70.39%\n",
      "Batch 21, Loss: 1.034718, Accuracy: 70.46%\n",
      "Batch 22, Loss: 1.034692, Accuracy: 70.60%\n",
      "Batch 23, Loss: 1.009341, Accuracy: 70.65%\n",
      "Batch 24, Loss: 0.966075, Accuracy: 70.96%\n",
      "Batch 25, Loss: 1.036720, Accuracy: 70.94%\n",
      "Batch 26, Loss: 0.979680, Accuracy: 71.15%\n",
      "Batch 27, Loss: 0.895094, Accuracy: 71.76%\n",
      "Batch 28, Loss: 1.008271, Accuracy: 71.82%\n",
      "Batch 29, Loss: 1.055682, Accuracy: 71.71%\n",
      "Batch 30, Loss: 1.110340, Accuracy: 71.46%\n",
      "Batch 31, Loss: 1.056124, Accuracy: 71.42%\n",
      "Batch 32, Loss: 1.043249, Accuracy: 71.44%\n",
      "Batch 33, Loss: 1.138356, Accuracy: 71.07%\n",
      "Batch 34, Loss: 1.046739, Accuracy: 71.00%\n",
      "Batch 35, Loss: 1.134493, Accuracy: 70.71%\n",
      "Batch 36, Loss: 1.067580, Accuracy: 70.53%\n",
      "Batch 37, Loss: 1.021341, Accuracy: 70.57%\n",
      "Batch 38, Loss: 1.070534, Accuracy: 70.44%\n",
      "Batch 39, Loss: 1.026344, Accuracy: 70.47%\n",
      "Batch 40, Loss: 0.987172, Accuracy: 70.59%\n",
      "Batch 41, Loss: 0.947639, Accuracy: 70.85%\n",
      "Batch 42, Loss: 1.029102, Accuracy: 70.83%\n",
      "Batch 43, Loss: 1.091671, Accuracy: 70.64%\n",
      "Batch 44, Loss: 1.108829, Accuracy: 70.45%\n",
      "Batch 45, Loss: 1.085536, Accuracy: 70.31%\n",
      "Batch 46, Loss: 1.095558, Accuracy: 70.21%\n",
      "Batch 47, Loss: 0.979319, Accuracy: 70.38%\n",
      "Batch 48, Loss: 0.969706, Accuracy: 70.54%\n",
      "Batch 49, Loss: 0.950235, Accuracy: 70.73%\n",
      "Batch 50, Loss: 0.981853, Accuracy: 70.81%\n",
      "Batch 51, Loss: 1.078091, Accuracy: 70.71%\n",
      "Batch 52, Loss: 1.057646, Accuracy: 70.64%\n",
      "Batch 53, Loss: 1.029210, Accuracy: 70.67%\n",
      "Batch 54, Loss: 0.944504, Accuracy: 70.86%\n",
      "Batch 55, Loss: 0.992373, Accuracy: 70.91%\n",
      "Batch 56, Loss: 0.954779, Accuracy: 71.07%\n",
      "Batch 57, Loss: 1.126762, Accuracy: 70.92%\n",
      "Batch 58, Loss: 1.043820, Accuracy: 70.88%\n",
      "Batch 59, Loss: 1.028921, Accuracy: 70.90%\n",
      "Batch 60, Loss: 1.008858, Accuracy: 70.94%\n",
      "Batch 61, Loss: 1.004668, Accuracy: 70.95%\n",
      "Batch 62, Loss: 1.042364, Accuracy: 70.89%\n",
      "Batch 63, Loss: 1.060549, Accuracy: 70.83%\n",
      "Batch 64, Loss: 1.029521, Accuracy: 70.85%\n",
      "Batch 65, Loss: 1.021177, Accuracy: 70.84%\n",
      "Batch 66, Loss: 0.983679, Accuracy: 70.93%\n",
      "Batch 67, Loss: 1.021353, Accuracy: 70.90%\n",
      "Batch 68, Loss: 1.008345, Accuracy: 70.91%\n",
      "Batch 69, Loss: 1.080229, Accuracy: 70.81%\n",
      "Batch 70, Loss: 1.046272, Accuracy: 70.80%\n",
      "Batch 71, Loss: 0.975329, Accuracy: 70.86%\n",
      "Batch 72, Loss: 1.065688, Accuracy: 70.81%\n",
      "Batch 73, Loss: 1.168793, Accuracy: 70.63%\n",
      "Batch 74, Loss: 1.087332, Accuracy: 70.57%\n",
      "Batch 75, Loss: 1.057109, Accuracy: 70.56%\n",
      "Batch 76, Loss: 1.049415, Accuracy: 70.56%\n",
      "Batch 77, Loss: 1.014984, Accuracy: 70.60%\n",
      "Batch 78, Loss: 0.968144, Accuracy: 70.69%\n",
      "Batch 79, Loss: 1.041052, Accuracy: 70.69%\n",
      "Batch 80, Loss: 1.028534, Accuracy: 70.66%\n",
      "Batch 81, Loss: 1.050403, Accuracy: 70.64%\n",
      "Batch 82, Loss: 0.994918, Accuracy: 70.69%\n",
      "Batch 83, Loss: 1.075322, Accuracy: 70.65%\n",
      "Batch 84, Loss: 1.074516, Accuracy: 70.61%\n",
      "Batch 85, Loss: 1.122112, Accuracy: 70.51%\n",
      "Batch 86, Loss: 1.014829, Accuracy: 70.53%\n",
      "Batch 87, Loss: 0.940701, Accuracy: 70.65%\n",
      "Batch 88, Loss: 0.980876, Accuracy: 70.70%\n",
      "Batch 89, Loss: 1.012529, Accuracy: 70.75%\n",
      "Batch 90, Loss: 1.013968, Accuracy: 70.78%\n",
      "Batch 91, Loss: 0.938771, Accuracy: 70.91%\n",
      "Batch 92, Loss: 0.973063, Accuracy: 70.99%\n",
      "Batch 93, Loss: 1.047218, Accuracy: 70.98%\n",
      "Batch 94, Loss: 0.997378, Accuracy: 70.98%\n",
      "Batch 95, Loss: 1.056970, Accuracy: 70.95%\n",
      "Batch 96, Loss: 1.017696, Accuracy: 70.98%\n",
      "Batch 97, Loss: 1.039646, Accuracy: 70.96%\n",
      "Batch 98, Loss: 0.946625, Accuracy: 71.05%\n",
      "Batch 99, Loss: 1.110448, Accuracy: 70.94%\n",
      "Batch 100, Loss: 1.016378, Accuracy: 70.95%\n",
      "Batch 101, Loss: 0.991453, Accuracy: 71.02%\n",
      "Batch 102, Loss: 1.073964, Accuracy: 70.97%\n",
      "Batch 103, Loss: 0.958993, Accuracy: 71.06%\n",
      "Batch 104, Loss: 1.028651, Accuracy: 71.06%\n",
      "Batch 105, Loss: 0.969229, Accuracy: 71.12%\n",
      "Batch 106, Loss: 0.953662, Accuracy: 71.20%\n",
      "Batch 107, Loss: 1.065172, Accuracy: 71.17%\n",
      "Batch 108, Loss: 1.020819, Accuracy: 71.17%\n",
      "Batch 109, Loss: 0.998618, Accuracy: 71.23%\n",
      "Batch 110, Loss: 1.058797, Accuracy: 71.19%\n",
      "Batch 111, Loss: 1.040582, Accuracy: 71.17%\n",
      "Batch 112, Loss: 0.996956, Accuracy: 71.19%\n",
      "Batch 113, Loss: 1.039143, Accuracy: 71.18%\n",
      "Batch 114, Loss: 1.056413, Accuracy: 71.18%\n",
      "Batch 115, Loss: 1.107880, Accuracy: 71.11%\n",
      "Batch 116, Loss: 1.152249, Accuracy: 70.99%\n",
      "Batch 117, Loss: 1.062125, Accuracy: 70.97%\n",
      "Batch 118, Loss: 1.131492, Accuracy: 70.88%\n",
      "Batch 119, Loss: 0.952295, Accuracy: 70.94%\n",
      "Batch 120, Loss: 0.996342, Accuracy: 70.96%\n",
      "Batch 121, Loss: 1.133821, Accuracy: 70.87%\n",
      "Batch 122, Loss: 0.988650, Accuracy: 70.91%\n",
      "Batch 123, Loss: 0.978698, Accuracy: 70.95%\n",
      "Batch 124, Loss: 1.126552, Accuracy: 70.87%\n",
      "Batch 125, Loss: 1.078312, Accuracy: 70.84%\n",
      "Batch 126, Loss: 1.115416, Accuracy: 70.76%\n",
      "Batch 127, Loss: 0.955143, Accuracy: 70.84%\n",
      "Batch 128, Loss: 0.978499, Accuracy: 70.91%\n",
      "Batch 129, Loss: 1.079665, Accuracy: 70.87%\n",
      "Batch 130, Loss: 0.941311, Accuracy: 70.95%\n",
      "Batch 131, Loss: 1.015863, Accuracy: 70.94%\n",
      "Batch 132, Loss: 0.994170, Accuracy: 70.98%\n",
      "Batch 133, Loss: 1.022863, Accuracy: 70.99%\n",
      "Batch 134, Loss: 1.059616, Accuracy: 70.97%\n",
      "Batch 135, Loss: 1.037204, Accuracy: 70.96%\n",
      "Batch 136, Loss: 1.049980, Accuracy: 70.93%\n",
      "Batch 137, Loss: 1.005779, Accuracy: 70.94%\n",
      "Batch 138, Loss: 1.157067, Accuracy: 70.84%\n",
      "Batch 139, Loss: 1.024050, Accuracy: 70.85%\n",
      "Batch 140, Loss: 1.049856, Accuracy: 70.83%\n",
      "Batch 141, Loss: 0.969215, Accuracy: 70.88%\n",
      "Batch 142, Loss: 1.009286, Accuracy: 70.90%\n",
      "Batch 143, Loss: 0.942276, Accuracy: 70.97%\n",
      "Batch 144, Loss: 1.010288, Accuracy: 70.99%\n",
      "Batch 145, Loss: 1.021743, Accuracy: 70.99%\n",
      "Batch 146, Loss: 1.011664, Accuracy: 71.00%\n",
      "Batch 147, Loss: 1.062146, Accuracy: 70.97%\n",
      "Batch 148, Loss: 1.057677, Accuracy: 70.95%\n",
      "Batch 149, Loss: 1.064923, Accuracy: 70.91%\n",
      "Batch 150, Loss: 1.012280, Accuracy: 70.92%\n",
      "Batch 151, Loss: 0.938545, Accuracy: 70.99%\n",
      "Batch 152, Loss: 1.078172, Accuracy: 70.94%\n",
      "Batch 153, Loss: 1.079492, Accuracy: 70.92%\n",
      "Batch 154, Loss: 1.031821, Accuracy: 70.91%\n",
      "Batch 155, Loss: 0.972123, Accuracy: 70.95%\n",
      "Batch 156, Loss: 1.124445, Accuracy: 70.89%\n",
      "Batch 157, Loss: 1.110461, Accuracy: 70.84%\n",
      "Batch 158, Loss: 1.066074, Accuracy: 70.80%\n",
      "Batch 159, Loss: 1.084773, Accuracy: 70.76%\n",
      "Batch 160, Loss: 1.024810, Accuracy: 70.76%\n",
      "Batch 161, Loss: 1.120208, Accuracy: 70.72%\n",
      "Batch 162, Loss: 1.057207, Accuracy: 70.70%\n",
      "Batch 163, Loss: 0.990333, Accuracy: 70.72%\n",
      "Batch 164, Loss: 1.005433, Accuracy: 70.73%\n",
      "Batch 165, Loss: 1.071483, Accuracy: 70.68%\n",
      "Batch 166, Loss: 1.042775, Accuracy: 70.67%\n",
      "Batch 167, Loss: 1.021865, Accuracy: 70.68%\n",
      "Batch 168, Loss: 1.004872, Accuracy: 70.70%\n",
      "Batch 169, Loss: 1.003845, Accuracy: 70.73%\n",
      "Batch 170, Loss: 0.955248, Accuracy: 70.77%\n",
      "Batch 171, Loss: 1.181553, Accuracy: 70.67%\n",
      "Batch 172, Loss: 1.061384, Accuracy: 70.64%\n",
      "Batch 173, Loss: 1.085432, Accuracy: 70.59%\n",
      "Batch 174, Loss: 1.082007, Accuracy: 70.56%\n",
      "Batch 175, Loss: 0.938060, Accuracy: 70.63%\n",
      "Batch 176, Loss: 1.013419, Accuracy: 70.65%\n",
      "Batch 177, Loss: 1.021210, Accuracy: 70.67%\n",
      "Batch 178, Loss: 0.958888, Accuracy: 70.71%\n",
      "Batch 179, Loss: 1.023165, Accuracy: 70.71%\n",
      "Batch 180, Loss: 1.117060, Accuracy: 70.67%\n",
      "Batch 181, Loss: 0.989661, Accuracy: 70.70%\n",
      "Batch 182, Loss: 1.026770, Accuracy: 70.71%\n",
      "Batch 183, Loss: 1.021097, Accuracy: 70.72%\n",
      "Batch 184, Loss: 1.051202, Accuracy: 70.70%\n",
      "Batch 185, Loss: 1.049831, Accuracy: 70.70%\n",
      "Batch 186, Loss: 1.048551, Accuracy: 70.69%\n",
      "Batch 187, Loss: 1.043764, Accuracy: 70.69%\n",
      "Batch 188, Loss: 1.027687, Accuracy: 70.70%\n",
      "Batch 189, Loss: 0.974089, Accuracy: 70.73%\n",
      "Batch 190, Loss: 1.035185, Accuracy: 70.74%\n",
      "Batch 191, Loss: 1.054443, Accuracy: 70.73%\n",
      "Batch 192, Loss: 1.068024, Accuracy: 70.70%\n",
      "Batch 193, Loss: 1.033955, Accuracy: 70.69%\n",
      "Batch 194, Loss: 0.979538, Accuracy: 70.72%\n",
      "Batch 195, Loss: 1.009970, Accuracy: 70.73%\n",
      "Batch 196, Loss: 0.973499, Accuracy: 70.77%\n",
      "Batch 197, Loss: 1.002390, Accuracy: 70.79%\n",
      "Batch 198, Loss: 1.003953, Accuracy: 70.80%\n",
      "Batch 199, Loss: 1.075101, Accuracy: 70.78%\n",
      "Batch 200, Loss: 1.076113, Accuracy: 70.77%\n",
      "Batch 201, Loss: 1.085298, Accuracy: 70.74%\n",
      "Batch 202, Loss: 1.036205, Accuracy: 70.74%\n",
      "Batch 203, Loss: 1.026128, Accuracy: 70.74%\n",
      "Batch 204, Loss: 1.026389, Accuracy: 70.75%\n",
      "Batch 205, Loss: 1.003330, Accuracy: 70.77%\n",
      "Batch 206, Loss: 1.120862, Accuracy: 70.72%\n",
      "Batch 207, Loss: 1.060865, Accuracy: 70.71%\n",
      "Batch 208, Loss: 1.078706, Accuracy: 70.69%\n",
      "Batch 209, Loss: 0.976267, Accuracy: 70.72%\n",
      "Batch 210, Loss: 0.990260, Accuracy: 70.74%\n",
      "Batch 211, Loss: 1.018267, Accuracy: 70.75%\n",
      "Batch 212, Loss: 1.071689, Accuracy: 70.73%\n",
      "Batch 213, Loss: 1.120563, Accuracy: 70.68%\n",
      "Training - Epoch 39, Loss: 1.033463, Accuracy: 70.68%\n",
      "Validation Batch 1, Loss: 1.061278, Accuracy: 64.06%\n",
      "Validation Batch 2, Loss: 1.181934, Accuracy: 59.38%\n",
      "Validation Batch 3, Loss: 1.184826, Accuracy: 58.85%\n",
      "Validation Batch 4, Loss: 1.064534, Accuracy: 61.33%\n",
      "Validation Batch 5, Loss: 1.129742, Accuracy: 60.00%\n",
      "Validation Batch 6, Loss: 1.090107, Accuracy: 60.94%\n",
      "Validation Batch 7, Loss: 1.119394, Accuracy: 60.94%\n",
      "Validation Batch 8, Loss: 1.134889, Accuracy: 60.94%\n",
      "Validation Batch 9, Loss: 1.159168, Accuracy: 60.76%\n",
      "Validation Batch 10, Loss: 1.133881, Accuracy: 60.78%\n",
      "Validation Batch 11, Loss: 1.101925, Accuracy: 61.08%\n",
      "Validation Batch 12, Loss: 1.070108, Accuracy: 61.72%\n",
      "Validation Batch 13, Loss: 1.184486, Accuracy: 61.30%\n",
      "Validation Batch 14, Loss: 1.132810, Accuracy: 61.05%\n",
      "Validation Batch 15, Loss: 1.130226, Accuracy: 60.94%\n",
      "Validation Batch 16, Loss: 1.107945, Accuracy: 61.04%\n",
      "Validation Batch 17, Loss: 1.218652, Accuracy: 60.20%\n",
      "Validation Batch 18, Loss: 1.089843, Accuracy: 60.50%\n",
      "Validation Batch 19, Loss: 1.190618, Accuracy: 60.03%\n",
      "Validation Batch 20, Loss: 1.123260, Accuracy: 60.23%\n",
      "Validation Batch 21, Loss: 1.122271, Accuracy: 60.27%\n",
      "Validation Batch 22, Loss: 1.174325, Accuracy: 60.01%\n",
      "Validation Batch 23, Loss: 1.213852, Accuracy: 59.58%\n",
      "Validation Batch 24, Loss: 1.145678, Accuracy: 59.64%\n",
      "Validation Batch 25, Loss: 1.105567, Accuracy: 59.69%\n",
      "Validation Batch 26, Loss: 1.088819, Accuracy: 59.86%\n",
      "Validation Batch 27, Loss: 1.139968, Accuracy: 59.84%\n",
      "Validation - Epoch 39, Loss: 1.133337, Accuracy: 59.84%\n",
      "Patienceâ€”2\n",
      "Epoch 40\n",
      "Batch 1, Loss: 0.959628, Accuracy: 79.69%\n",
      "Batch 2, Loss: 0.968398, Accuracy: 78.12%\n",
      "Batch 3, Loss: 0.965587, Accuracy: 78.12%\n",
      "Batch 4, Loss: 0.999124, Accuracy: 76.95%\n",
      "Batch 5, Loss: 0.983623, Accuracy: 76.88%\n",
      "Batch 6, Loss: 1.077096, Accuracy: 74.74%\n",
      "Batch 7, Loss: 0.960761, Accuracy: 75.00%\n",
      "Batch 8, Loss: 1.082242, Accuracy: 73.63%\n",
      "Batch 9, Loss: 1.030904, Accuracy: 73.44%\n",
      "Batch 10, Loss: 0.944904, Accuracy: 74.22%\n",
      "Batch 11, Loss: 1.015132, Accuracy: 74.01%\n",
      "Batch 12, Loss: 0.979860, Accuracy: 74.22%\n",
      "Batch 13, Loss: 0.963840, Accuracy: 74.52%\n",
      "Batch 14, Loss: 1.021591, Accuracy: 74.33%\n",
      "Batch 15, Loss: 1.037164, Accuracy: 74.17%\n",
      "Batch 16, Loss: 0.989086, Accuracy: 74.32%\n",
      "Batch 17, Loss: 1.108650, Accuracy: 73.62%\n",
      "Batch 18, Loss: 1.017250, Accuracy: 73.35%\n",
      "Batch 19, Loss: 1.045062, Accuracy: 73.03%\n",
      "Batch 20, Loss: 1.065400, Accuracy: 72.73%\n",
      "Batch 21, Loss: 1.041545, Accuracy: 72.54%\n",
      "Batch 22, Loss: 1.017152, Accuracy: 72.51%\n",
      "Batch 23, Loss: 0.920325, Accuracy: 73.03%\n",
      "Batch 24, Loss: 1.060490, Accuracy: 72.85%\n",
      "Batch 25, Loss: 0.956578, Accuracy: 73.06%\n",
      "Batch 26, Loss: 1.087692, Accuracy: 72.72%\n",
      "Batch 27, Loss: 1.013888, Accuracy: 72.74%\n",
      "Batch 28, Loss: 1.026240, Accuracy: 72.71%\n",
      "Batch 29, Loss: 1.030846, Accuracy: 72.68%\n",
      "Batch 30, Loss: 1.027441, Accuracy: 72.66%\n",
      "Batch 31, Loss: 1.071854, Accuracy: 72.48%\n",
      "Batch 32, Loss: 1.125712, Accuracy: 72.12%\n",
      "Batch 33, Loss: 1.020216, Accuracy: 72.06%\n",
      "Batch 34, Loss: 1.054775, Accuracy: 71.97%\n",
      "Batch 35, Loss: 0.995318, Accuracy: 71.96%\n",
      "Batch 36, Loss: 1.012565, Accuracy: 72.01%\n",
      "Batch 37, Loss: 1.034663, Accuracy: 71.96%\n",
      "Batch 38, Loss: 0.952267, Accuracy: 72.16%\n",
      "Batch 39, Loss: 1.031758, Accuracy: 72.12%\n",
      "Batch 40, Loss: 1.025535, Accuracy: 72.11%\n",
      "Batch 41, Loss: 1.071537, Accuracy: 71.99%\n",
      "Batch 42, Loss: 1.052839, Accuracy: 71.91%\n",
      "Batch 43, Loss: 0.988581, Accuracy: 72.02%\n",
      "Batch 44, Loss: 1.012813, Accuracy: 72.02%\n",
      "Batch 45, Loss: 0.959262, Accuracy: 72.15%\n",
      "Batch 46, Loss: 1.035065, Accuracy: 72.08%\n",
      "Batch 47, Loss: 0.945143, Accuracy: 72.21%\n",
      "Batch 48, Loss: 1.018742, Accuracy: 72.23%\n",
      "Batch 49, Loss: 1.021670, Accuracy: 72.26%\n",
      "Batch 50, Loss: 0.943621, Accuracy: 72.44%\n",
      "Batch 51, Loss: 1.029856, Accuracy: 72.49%\n",
      "Batch 52, Loss: 1.130127, Accuracy: 72.24%\n",
      "Batch 53, Loss: 0.990501, Accuracy: 72.26%\n",
      "Batch 54, Loss: 1.031589, Accuracy: 72.28%\n",
      "Batch 55, Loss: 1.155281, Accuracy: 71.99%\n",
      "Batch 56, Loss: 0.964103, Accuracy: 72.15%\n",
      "Batch 57, Loss: 1.092990, Accuracy: 72.04%\n",
      "Batch 58, Loss: 1.020035, Accuracy: 72.06%\n",
      "Batch 59, Loss: 1.035627, Accuracy: 72.03%\n",
      "Batch 60, Loss: 1.028362, Accuracy: 72.03%\n",
      "Batch 61, Loss: 1.055831, Accuracy: 71.98%\n",
      "Batch 62, Loss: 1.092361, Accuracy: 71.85%\n",
      "Batch 63, Loss: 1.022328, Accuracy: 71.88%\n",
      "Batch 64, Loss: 1.096136, Accuracy: 71.75%\n",
      "Batch 65, Loss: 1.074267, Accuracy: 71.66%\n",
      "Batch 66, Loss: 1.006139, Accuracy: 71.71%\n",
      "Batch 67, Loss: 1.121415, Accuracy: 71.53%\n",
      "Batch 68, Loss: 1.047231, Accuracy: 71.51%\n",
      "Batch 69, Loss: 1.000271, Accuracy: 71.58%\n",
      "Batch 70, Loss: 0.999159, Accuracy: 71.58%\n",
      "Batch 71, Loss: 0.998529, Accuracy: 71.63%\n",
      "Batch 72, Loss: 1.024130, Accuracy: 71.61%\n",
      "Batch 73, Loss: 1.014234, Accuracy: 71.62%\n",
      "Batch 74, Loss: 1.022495, Accuracy: 71.62%\n",
      "Batch 75, Loss: 1.071841, Accuracy: 71.54%\n",
      "Batch 76, Loss: 1.143427, Accuracy: 71.36%\n",
      "Batch 77, Loss: 0.978931, Accuracy: 71.41%\n",
      "Batch 78, Loss: 0.991657, Accuracy: 71.43%\n",
      "Batch 79, Loss: 0.890820, Accuracy: 71.62%\n",
      "Batch 80, Loss: 1.038796, Accuracy: 71.58%\n",
      "Batch 81, Loss: 1.021487, Accuracy: 71.55%\n",
      "Batch 82, Loss: 1.020287, Accuracy: 71.55%\n",
      "Batch 83, Loss: 1.096387, Accuracy: 71.46%\n",
      "Batch 84, Loss: 1.017431, Accuracy: 71.48%\n",
      "Batch 85, Loss: 0.981180, Accuracy: 71.56%\n",
      "Batch 86, Loss: 1.092344, Accuracy: 71.51%\n",
      "Batch 87, Loss: 1.101334, Accuracy: 71.41%\n",
      "Batch 88, Loss: 1.022302, Accuracy: 71.43%\n",
      "Batch 89, Loss: 1.091782, Accuracy: 71.35%\n",
      "Batch 90, Loss: 1.002456, Accuracy: 71.37%\n",
      "Batch 91, Loss: 1.030248, Accuracy: 71.39%\n",
      "Batch 92, Loss: 1.048335, Accuracy: 71.37%\n",
      "Batch 93, Loss: 1.021108, Accuracy: 71.37%\n",
      "Batch 94, Loss: 1.079160, Accuracy: 71.31%\n",
      "Batch 95, Loss: 1.011542, Accuracy: 71.33%\n",
      "Batch 96, Loss: 1.114030, Accuracy: 71.26%\n",
      "Batch 97, Loss: 0.984569, Accuracy: 71.33%\n",
      "Batch 98, Loss: 1.035618, Accuracy: 71.33%\n",
      "Batch 99, Loss: 1.036395, Accuracy: 71.32%\n",
      "Batch 100, Loss: 0.947796, Accuracy: 71.39%\n",
      "Batch 101, Loss: 1.002753, Accuracy: 71.44%\n",
      "Batch 102, Loss: 0.900328, Accuracy: 71.57%\n",
      "Batch 103, Loss: 1.017320, Accuracy: 71.59%\n",
      "Batch 104, Loss: 1.077614, Accuracy: 71.54%\n",
      "Batch 105, Loss: 1.094196, Accuracy: 71.49%\n",
      "Batch 106, Loss: 1.007613, Accuracy: 71.51%\n",
      "Batch 107, Loss: 1.047397, Accuracy: 71.47%\n",
      "Batch 108, Loss: 1.040019, Accuracy: 71.46%\n",
      "Batch 109, Loss: 0.938384, Accuracy: 71.55%\n",
      "Batch 110, Loss: 1.123087, Accuracy: 71.46%\n",
      "Batch 111, Loss: 1.036476, Accuracy: 71.44%\n",
      "Batch 112, Loss: 1.015595, Accuracy: 71.46%\n",
      "Batch 113, Loss: 1.095515, Accuracy: 71.39%\n",
      "Batch 114, Loss: 1.083533, Accuracy: 71.30%\n",
      "Batch 115, Loss: 0.925758, Accuracy: 71.40%\n",
      "Batch 116, Loss: 1.041305, Accuracy: 71.39%\n",
      "Batch 117, Loss: 1.077879, Accuracy: 71.35%\n",
      "Batch 118, Loss: 0.968187, Accuracy: 71.39%\n",
      "Batch 119, Loss: 0.967754, Accuracy: 71.44%\n",
      "Batch 120, Loss: 1.065127, Accuracy: 71.38%\n",
      "Batch 121, Loss: 1.012508, Accuracy: 71.38%\n",
      "Batch 122, Loss: 1.022398, Accuracy: 71.39%\n",
      "Batch 123, Loss: 1.054085, Accuracy: 71.35%\n",
      "Batch 124, Loss: 0.972947, Accuracy: 71.41%\n",
      "Batch 125, Loss: 0.938940, Accuracy: 71.46%\n",
      "Batch 126, Loss: 1.007021, Accuracy: 71.49%\n",
      "Batch 127, Loss: 1.006937, Accuracy: 71.51%\n",
      "Batch 128, Loss: 1.179667, Accuracy: 71.36%\n",
      "Batch 129, Loss: 0.984828, Accuracy: 71.40%\n",
      "Batch 130, Loss: 1.009871, Accuracy: 71.43%\n",
      "Batch 131, Loss: 0.998995, Accuracy: 71.46%\n",
      "Batch 132, Loss: 0.987042, Accuracy: 71.51%\n",
      "Batch 133, Loss: 1.034243, Accuracy: 71.50%\n",
      "Batch 134, Loss: 1.105330, Accuracy: 71.48%\n",
      "Batch 135, Loss: 1.069544, Accuracy: 71.45%\n",
      "Batch 136, Loss: 1.025655, Accuracy: 71.45%\n",
      "Batch 137, Loss: 1.055134, Accuracy: 71.41%\n",
      "Batch 138, Loss: 1.046822, Accuracy: 71.40%\n",
      "Batch 139, Loss: 1.074384, Accuracy: 71.35%\n",
      "Batch 140, Loss: 1.039005, Accuracy: 71.36%\n",
      "Batch 141, Loss: 1.026834, Accuracy: 71.37%\n",
      "Batch 142, Loss: 0.911884, Accuracy: 71.45%\n",
      "Batch 143, Loss: 1.036839, Accuracy: 71.44%\n",
      "Batch 144, Loss: 0.992764, Accuracy: 71.46%\n",
      "Batch 145, Loss: 1.035205, Accuracy: 71.43%\n",
      "Batch 146, Loss: 0.995145, Accuracy: 71.45%\n",
      "Batch 147, Loss: 1.024370, Accuracy: 71.45%\n",
      "Batch 148, Loss: 1.071066, Accuracy: 71.41%\n",
      "Batch 149, Loss: 1.076226, Accuracy: 71.36%\n",
      "Batch 150, Loss: 1.015474, Accuracy: 71.38%\n",
      "Batch 151, Loss: 1.006690, Accuracy: 71.41%\n",
      "Batch 152, Loss: 0.999711, Accuracy: 71.42%\n",
      "Batch 153, Loss: 0.995781, Accuracy: 71.44%\n",
      "Batch 154, Loss: 1.043358, Accuracy: 71.42%\n",
      "Batch 155, Loss: 1.055383, Accuracy: 71.39%\n",
      "Batch 156, Loss: 1.089905, Accuracy: 71.33%\n",
      "Batch 157, Loss: 1.052092, Accuracy: 71.32%\n",
      "Batch 158, Loss: 1.111162, Accuracy: 71.27%\n",
      "Batch 159, Loss: 1.034889, Accuracy: 71.27%\n",
      "Batch 160, Loss: 1.011601, Accuracy: 71.27%\n",
      "Batch 161, Loss: 1.010046, Accuracy: 71.29%\n",
      "Batch 162, Loss: 1.017583, Accuracy: 71.31%\n",
      "Batch 163, Loss: 1.090824, Accuracy: 71.26%\n",
      "Batch 164, Loss: 1.022669, Accuracy: 71.27%\n",
      "Batch 165, Loss: 0.977053, Accuracy: 71.31%\n",
      "Batch 166, Loss: 1.028747, Accuracy: 71.30%\n",
      "Batch 167, Loss: 1.013736, Accuracy: 71.31%\n",
      "Batch 168, Loss: 1.033689, Accuracy: 71.31%\n",
      "Batch 169, Loss: 1.052211, Accuracy: 71.29%\n",
      "Batch 170, Loss: 0.976832, Accuracy: 71.31%\n",
      "Batch 171, Loss: 0.995562, Accuracy: 71.32%\n",
      "Batch 172, Loss: 1.078664, Accuracy: 71.29%\n",
      "Batch 173, Loss: 1.104017, Accuracy: 71.23%\n",
      "Batch 174, Loss: 1.046474, Accuracy: 71.23%\n",
      "Batch 175, Loss: 0.994272, Accuracy: 71.25%\n",
      "Batch 176, Loss: 1.062342, Accuracy: 71.24%\n",
      "Batch 177, Loss: 1.065457, Accuracy: 71.21%\n",
      "Batch 178, Loss: 1.083870, Accuracy: 71.18%\n",
      "Batch 179, Loss: 1.000620, Accuracy: 71.20%\n",
      "Batch 180, Loss: 1.108967, Accuracy: 71.15%\n",
      "Batch 181, Loss: 1.085016, Accuracy: 71.12%\n",
      "Batch 182, Loss: 0.962685, Accuracy: 71.16%\n",
      "Batch 183, Loss: 1.033774, Accuracy: 71.15%\n",
      "Batch 184, Loss: 1.029744, Accuracy: 71.14%\n",
      "Batch 185, Loss: 1.007028, Accuracy: 71.16%\n",
      "Batch 186, Loss: 0.966986, Accuracy: 71.19%\n",
      "Batch 187, Loss: 1.112359, Accuracy: 71.13%\n",
      "Batch 188, Loss: 1.020464, Accuracy: 71.14%\n",
      "Batch 189, Loss: 0.981272, Accuracy: 71.17%\n",
      "Batch 190, Loss: 0.959641, Accuracy: 71.21%\n",
      "Batch 191, Loss: 1.067499, Accuracy: 71.19%\n",
      "Batch 192, Loss: 0.974668, Accuracy: 71.22%\n",
      "Batch 193, Loss: 1.057271, Accuracy: 71.19%\n",
      "Batch 194, Loss: 1.035971, Accuracy: 71.19%\n",
      "Batch 195, Loss: 0.974320, Accuracy: 71.22%\n",
      "Batch 196, Loss: 1.011143, Accuracy: 71.21%\n",
      "Batch 197, Loss: 0.967082, Accuracy: 71.26%\n",
      "Batch 198, Loss: 1.026302, Accuracy: 71.24%\n",
      "Batch 199, Loss: 1.024862, Accuracy: 71.24%\n",
      "Batch 200, Loss: 1.067637, Accuracy: 71.23%\n",
      "Batch 201, Loss: 1.023734, Accuracy: 71.23%\n",
      "Batch 202, Loss: 1.066357, Accuracy: 71.22%\n",
      "Batch 203, Loss: 0.985846, Accuracy: 71.24%\n",
      "Batch 204, Loss: 1.032868, Accuracy: 71.24%\n",
      "Batch 205, Loss: 1.030759, Accuracy: 71.24%\n",
      "Batch 206, Loss: 1.049760, Accuracy: 71.22%\n",
      "Batch 207, Loss: 1.039167, Accuracy: 71.23%\n",
      "Batch 208, Loss: 0.953339, Accuracy: 71.26%\n",
      "Batch 209, Loss: 0.996109, Accuracy: 71.26%\n",
      "Batch 210, Loss: 1.070203, Accuracy: 71.24%\n",
      "Batch 211, Loss: 0.996044, Accuracy: 71.26%\n",
      "Batch 212, Loss: 1.025905, Accuracy: 71.27%\n",
      "Batch 213, Loss: 1.128418, Accuracy: 71.21%\n",
      "Training - Epoch 40, Loss: 1.027987, Accuracy: 71.21%\n",
      "Validation Batch 1, Loss: 1.053948, Accuracy: 67.19%\n",
      "Validation Batch 2, Loss: 1.179988, Accuracy: 60.16%\n",
      "Validation Batch 3, Loss: 1.174881, Accuracy: 59.90%\n",
      "Validation Batch 4, Loss: 1.058259, Accuracy: 62.50%\n",
      "Validation Batch 5, Loss: 1.127306, Accuracy: 61.56%\n",
      "Validation Batch 6, Loss: 1.075784, Accuracy: 62.76%\n",
      "Validation Batch 7, Loss: 1.108333, Accuracy: 62.95%\n",
      "Validation Batch 8, Loss: 1.122772, Accuracy: 62.89%\n",
      "Validation Batch 9, Loss: 1.148102, Accuracy: 62.67%\n",
      "Validation Batch 10, Loss: 1.120457, Accuracy: 62.50%\n",
      "Validation Batch 11, Loss: 1.089842, Accuracy: 62.78%\n",
      "Validation Batch 12, Loss: 1.050106, Accuracy: 63.41%\n",
      "Validation Batch 13, Loss: 1.173371, Accuracy: 63.10%\n",
      "Validation Batch 14, Loss: 1.118530, Accuracy: 62.83%\n",
      "Validation Batch 15, Loss: 1.118978, Accuracy: 62.71%\n",
      "Validation Batch 16, Loss: 1.097463, Accuracy: 62.79%\n",
      "Validation Batch 17, Loss: 1.203403, Accuracy: 61.86%\n",
      "Validation Batch 18, Loss: 1.074629, Accuracy: 62.15%\n",
      "Validation Batch 19, Loss: 1.181060, Accuracy: 61.68%\n",
      "Validation Batch 20, Loss: 1.112162, Accuracy: 61.88%\n",
      "Validation Batch 21, Loss: 1.109359, Accuracy: 61.83%\n",
      "Validation Batch 22, Loss: 1.169588, Accuracy: 61.51%\n",
      "Validation Batch 23, Loss: 1.210895, Accuracy: 61.01%\n",
      "Validation Batch 24, Loss: 1.136996, Accuracy: 61.00%\n",
      "Validation Batch 25, Loss: 1.109878, Accuracy: 61.00%\n",
      "Validation Batch 26, Loss: 1.073055, Accuracy: 61.18%\n",
      "Validation Batch 27, Loss: 1.129920, Accuracy: 61.19%\n",
      "Validation - Epoch 40, Loss: 1.123299, Accuracy: 61.19%\n",
      "Patienceâ€”0\n",
      "Epoch 41\n",
      "Batch 1, Loss: 0.988368, Accuracy: 73.44%\n",
      "Batch 2, Loss: 1.038797, Accuracy: 71.88%\n",
      "Batch 3, Loss: 0.970212, Accuracy: 73.96%\n",
      "Batch 4, Loss: 0.994510, Accuracy: 74.22%\n",
      "Batch 5, Loss: 0.987730, Accuracy: 75.00%\n",
      "Batch 6, Loss: 1.040600, Accuracy: 73.70%\n",
      "Batch 7, Loss: 0.989020, Accuracy: 73.88%\n",
      "Batch 8, Loss: 1.027856, Accuracy: 73.24%\n",
      "Batch 9, Loss: 1.010771, Accuracy: 73.09%\n",
      "Batch 10, Loss: 0.967541, Accuracy: 73.44%\n",
      "Batch 11, Loss: 1.108325, Accuracy: 72.44%\n",
      "Batch 12, Loss: 1.021371, Accuracy: 72.27%\n",
      "Batch 13, Loss: 0.968836, Accuracy: 72.72%\n",
      "Batch 14, Loss: 1.044832, Accuracy: 72.43%\n",
      "Batch 15, Loss: 1.081449, Accuracy: 72.08%\n",
      "Batch 16, Loss: 1.081481, Accuracy: 71.78%\n",
      "Batch 17, Loss: 1.007416, Accuracy: 71.97%\n",
      "Batch 18, Loss: 1.022785, Accuracy: 71.88%\n",
      "Batch 19, Loss: 1.053485, Accuracy: 71.55%\n",
      "Batch 20, Loss: 1.057398, Accuracy: 71.33%\n",
      "Batch 21, Loss: 1.130593, Accuracy: 70.91%\n",
      "Batch 22, Loss: 1.090539, Accuracy: 70.45%\n",
      "Batch 23, Loss: 1.104581, Accuracy: 70.11%\n",
      "Batch 24, Loss: 1.002445, Accuracy: 70.25%\n",
      "Batch 25, Loss: 1.054736, Accuracy: 70.12%\n",
      "Batch 26, Loss: 1.020583, Accuracy: 70.19%\n",
      "Batch 27, Loss: 1.010431, Accuracy: 70.37%\n",
      "Batch 28, Loss: 1.057837, Accuracy: 70.31%\n",
      "Batch 29, Loss: 1.106677, Accuracy: 70.10%\n",
      "Batch 30, Loss: 1.025754, Accuracy: 70.10%\n",
      "Batch 31, Loss: 0.996121, Accuracy: 70.16%\n",
      "Batch 32, Loss: 0.919983, Accuracy: 70.51%\n",
      "Batch 33, Loss: 1.059618, Accuracy: 70.41%\n",
      "Batch 34, Loss: 1.050741, Accuracy: 70.36%\n",
      "Batch 35, Loss: 1.087708, Accuracy: 70.27%\n",
      "Batch 36, Loss: 1.017764, Accuracy: 70.31%\n",
      "Batch 37, Loss: 0.951468, Accuracy: 70.61%\n",
      "Batch 38, Loss: 1.066117, Accuracy: 70.44%\n",
      "Batch 39, Loss: 1.071432, Accuracy: 70.39%\n",
      "Batch 40, Loss: 0.939723, Accuracy: 70.70%\n",
      "Batch 41, Loss: 0.953274, Accuracy: 70.88%\n",
      "Batch 42, Loss: 0.968376, Accuracy: 70.94%\n",
      "Batch 43, Loss: 0.975199, Accuracy: 71.08%\n",
      "Batch 44, Loss: 1.057190, Accuracy: 71.02%\n",
      "Batch 45, Loss: 0.964353, Accuracy: 71.28%\n",
      "Batch 46, Loss: 1.121553, Accuracy: 71.09%\n",
      "Batch 47, Loss: 1.025699, Accuracy: 71.11%\n",
      "Batch 48, Loss: 1.043360, Accuracy: 71.06%\n",
      "Batch 49, Loss: 1.104959, Accuracy: 70.92%\n",
      "Batch 50, Loss: 1.064193, Accuracy: 70.81%\n",
      "Batch 51, Loss: 1.053787, Accuracy: 70.71%\n",
      "Batch 52, Loss: 1.091422, Accuracy: 70.58%\n",
      "Batch 53, Loss: 1.068095, Accuracy: 70.55%\n",
      "Batch 54, Loss: 1.061361, Accuracy: 70.52%\n",
      "Batch 55, Loss: 0.954618, Accuracy: 70.71%\n",
      "Batch 56, Loss: 1.073646, Accuracy: 70.62%\n",
      "Batch 57, Loss: 0.971665, Accuracy: 70.72%\n",
      "Batch 58, Loss: 1.055771, Accuracy: 70.69%\n",
      "Batch 59, Loss: 1.169827, Accuracy: 70.42%\n",
      "Batch 60, Loss: 0.943132, Accuracy: 70.60%\n",
      "Batch 61, Loss: 1.089707, Accuracy: 70.49%\n",
      "Batch 62, Loss: 1.030221, Accuracy: 70.49%\n",
      "Batch 63, Loss: 1.114508, Accuracy: 70.29%\n",
      "Batch 64, Loss: 1.097788, Accuracy: 70.14%\n",
      "Batch 65, Loss: 1.047174, Accuracy: 70.14%\n",
      "Batch 66, Loss: 0.930648, Accuracy: 70.34%\n",
      "Batch 67, Loss: 1.029788, Accuracy: 70.31%\n",
      "Batch 68, Loss: 1.039989, Accuracy: 70.29%\n",
      "Batch 69, Loss: 1.001558, Accuracy: 70.36%\n",
      "Batch 70, Loss: 1.045408, Accuracy: 70.33%\n",
      "Batch 71, Loss: 0.990769, Accuracy: 70.42%\n",
      "Batch 72, Loss: 1.024922, Accuracy: 70.46%\n",
      "Batch 73, Loss: 0.955023, Accuracy: 70.57%\n",
      "Batch 74, Loss: 0.896691, Accuracy: 70.80%\n",
      "Batch 75, Loss: 1.047332, Accuracy: 70.81%\n",
      "Batch 76, Loss: 1.062681, Accuracy: 70.79%\n",
      "Batch 77, Loss: 1.102227, Accuracy: 70.70%\n",
      "Batch 78, Loss: 1.028882, Accuracy: 70.69%\n",
      "Batch 79, Loss: 1.019503, Accuracy: 70.71%\n",
      "Batch 80, Loss: 1.088924, Accuracy: 70.62%\n",
      "Batch 81, Loss: 1.043655, Accuracy: 70.64%\n",
      "Batch 82, Loss: 1.071819, Accuracy: 70.60%\n",
      "Batch 83, Loss: 1.061563, Accuracy: 70.59%\n",
      "Batch 84, Loss: 1.030073, Accuracy: 70.63%\n",
      "Batch 85, Loss: 1.033655, Accuracy: 70.59%\n",
      "Batch 86, Loss: 1.023328, Accuracy: 70.62%\n",
      "Batch 87, Loss: 1.090577, Accuracy: 70.53%\n",
      "Batch 88, Loss: 1.053704, Accuracy: 70.51%\n",
      "Batch 89, Loss: 1.010451, Accuracy: 70.52%\n",
      "Batch 90, Loss: 0.984363, Accuracy: 70.59%\n",
      "Batch 91, Loss: 0.937534, Accuracy: 70.69%\n",
      "Batch 92, Loss: 1.021896, Accuracy: 70.69%\n",
      "Batch 93, Loss: 1.005981, Accuracy: 70.70%\n",
      "Batch 94, Loss: 1.075493, Accuracy: 70.64%\n",
      "Batch 95, Loss: 1.128664, Accuracy: 70.54%\n",
      "Batch 96, Loss: 1.071491, Accuracy: 70.51%\n",
      "Batch 97, Loss: 1.054166, Accuracy: 70.51%\n",
      "Batch 98, Loss: 1.101638, Accuracy: 70.41%\n",
      "Batch 99, Loss: 0.994920, Accuracy: 70.47%\n",
      "Batch 100, Loss: 0.965454, Accuracy: 70.55%\n",
      "Batch 101, Loss: 1.016993, Accuracy: 70.54%\n",
      "Batch 102, Loss: 1.074503, Accuracy: 70.51%\n",
      "Batch 103, Loss: 1.012353, Accuracy: 70.54%\n",
      "Batch 104, Loss: 1.054917, Accuracy: 70.54%\n",
      "Batch 105, Loss: 0.992326, Accuracy: 70.60%\n",
      "Batch 106, Loss: 0.938465, Accuracy: 70.71%\n",
      "Batch 107, Loss: 1.046430, Accuracy: 70.69%\n",
      "Batch 108, Loss: 1.049753, Accuracy: 70.69%\n",
      "Batch 109, Loss: 1.049950, Accuracy: 70.70%\n",
      "Batch 110, Loss: 1.041953, Accuracy: 70.68%\n",
      "Batch 111, Loss: 0.920993, Accuracy: 70.79%\n",
      "Batch 112, Loss: 1.001816, Accuracy: 70.81%\n",
      "Batch 113, Loss: 1.040847, Accuracy: 70.80%\n",
      "Batch 114, Loss: 1.070302, Accuracy: 70.75%\n",
      "Batch 115, Loss: 1.017758, Accuracy: 70.77%\n",
      "Batch 116, Loss: 0.965119, Accuracy: 70.84%\n",
      "Batch 117, Loss: 1.042619, Accuracy: 70.82%\n",
      "Batch 118, Loss: 1.067524, Accuracy: 70.79%\n",
      "Batch 119, Loss: 1.013187, Accuracy: 70.81%\n",
      "Batch 120, Loss: 0.901366, Accuracy: 70.90%\n",
      "Batch 121, Loss: 1.043430, Accuracy: 70.85%\n",
      "Batch 122, Loss: 1.077061, Accuracy: 70.81%\n",
      "Batch 123, Loss: 1.008550, Accuracy: 70.85%\n",
      "Batch 124, Loss: 0.934820, Accuracy: 70.93%\n",
      "Batch 125, Loss: 1.074111, Accuracy: 70.90%\n",
      "Batch 126, Loss: 1.119325, Accuracy: 70.83%\n",
      "Batch 127, Loss: 1.003802, Accuracy: 70.85%\n",
      "Batch 128, Loss: 0.969809, Accuracy: 70.90%\n",
      "Batch 129, Loss: 0.944206, Accuracy: 70.98%\n",
      "Batch 130, Loss: 1.039781, Accuracy: 70.97%\n",
      "Batch 131, Loss: 1.021948, Accuracy: 70.98%\n",
      "Batch 132, Loss: 0.981121, Accuracy: 71.02%\n",
      "Batch 133, Loss: 1.133547, Accuracy: 70.94%\n",
      "Batch 134, Loss: 1.083988, Accuracy: 70.90%\n",
      "Batch 135, Loss: 1.023102, Accuracy: 70.90%\n",
      "Batch 136, Loss: 1.025684, Accuracy: 70.92%\n",
      "Batch 137, Loss: 0.964533, Accuracy: 70.99%\n",
      "Batch 138, Loss: 1.041650, Accuracy: 70.99%\n",
      "Batch 139, Loss: 0.992355, Accuracy: 71.01%\n",
      "Batch 140, Loss: 1.001718, Accuracy: 71.03%\n",
      "Batch 141, Loss: 1.043926, Accuracy: 71.00%\n",
      "Batch 142, Loss: 1.062470, Accuracy: 70.98%\n",
      "Batch 143, Loss: 0.941871, Accuracy: 71.06%\n",
      "Batch 144, Loss: 1.090433, Accuracy: 71.01%\n",
      "Batch 145, Loss: 1.009880, Accuracy: 71.02%\n",
      "Batch 146, Loss: 0.993295, Accuracy: 71.06%\n",
      "Batch 147, Loss: 1.017493, Accuracy: 71.05%\n",
      "Batch 148, Loss: 1.008117, Accuracy: 71.06%\n",
      "Batch 149, Loss: 1.064030, Accuracy: 71.03%\n",
      "Batch 150, Loss: 1.107636, Accuracy: 70.96%\n",
      "Batch 151, Loss: 1.041155, Accuracy: 70.93%\n",
      "Batch 152, Loss: 1.004706, Accuracy: 70.96%\n",
      "Batch 153, Loss: 1.012559, Accuracy: 70.97%\n",
      "Batch 154, Loss: 1.021238, Accuracy: 70.96%\n",
      "Batch 155, Loss: 0.975371, Accuracy: 70.99%\n",
      "Batch 156, Loss: 1.127881, Accuracy: 70.92%\n",
      "Batch 157, Loss: 1.025988, Accuracy: 70.93%\n",
      "Batch 158, Loss: 0.964900, Accuracy: 70.98%\n",
      "Batch 159, Loss: 1.004671, Accuracy: 71.00%\n",
      "Batch 160, Loss: 1.003864, Accuracy: 71.03%\n",
      "Batch 161, Loss: 0.973020, Accuracy: 71.06%\n",
      "Batch 162, Loss: 0.990961, Accuracy: 71.08%\n",
      "Batch 163, Loss: 1.021088, Accuracy: 71.09%\n",
      "Batch 164, Loss: 0.999561, Accuracy: 71.11%\n",
      "Batch 165, Loss: 1.020426, Accuracy: 71.12%\n",
      "Batch 166, Loss: 1.011746, Accuracy: 71.12%\n",
      "Batch 167, Loss: 1.064264, Accuracy: 71.11%\n",
      "Batch 168, Loss: 1.008146, Accuracy: 71.13%\n",
      "Batch 169, Loss: 0.994621, Accuracy: 71.15%\n",
      "Batch 170, Loss: 1.048972, Accuracy: 71.13%\n",
      "Batch 171, Loss: 1.023388, Accuracy: 71.13%\n",
      "Batch 172, Loss: 1.068041, Accuracy: 71.11%\n",
      "Batch 173, Loss: 0.958726, Accuracy: 71.16%\n",
      "Batch 174, Loss: 1.006589, Accuracy: 71.18%\n",
      "Batch 175, Loss: 1.014328, Accuracy: 71.20%\n",
      "Batch 176, Loss: 1.031815, Accuracy: 71.19%\n",
      "Batch 177, Loss: 0.939128, Accuracy: 71.25%\n",
      "Batch 178, Loss: 1.039722, Accuracy: 71.26%\n",
      "Batch 179, Loss: 0.969260, Accuracy: 71.29%\n",
      "Batch 180, Loss: 1.044505, Accuracy: 71.28%\n",
      "Batch 181, Loss: 0.978966, Accuracy: 71.31%\n",
      "Batch 182, Loss: 1.014303, Accuracy: 71.32%\n",
      "Batch 183, Loss: 1.006688, Accuracy: 71.33%\n",
      "Batch 184, Loss: 1.039113, Accuracy: 71.32%\n",
      "Batch 185, Loss: 1.068788, Accuracy: 71.29%\n",
      "Batch 186, Loss: 1.071863, Accuracy: 71.26%\n",
      "Batch 187, Loss: 1.092157, Accuracy: 71.22%\n",
      "Batch 188, Loss: 0.960216, Accuracy: 71.26%\n",
      "Batch 189, Loss: 1.033066, Accuracy: 71.26%\n",
      "Batch 190, Loss: 1.032739, Accuracy: 71.28%\n",
      "Batch 191, Loss: 0.949365, Accuracy: 71.32%\n",
      "Batch 192, Loss: 1.032501, Accuracy: 71.32%\n",
      "Batch 193, Loss: 0.999683, Accuracy: 71.34%\n",
      "Batch 194, Loss: 0.974014, Accuracy: 71.38%\n",
      "Batch 195, Loss: 1.016676, Accuracy: 71.39%\n",
      "Batch 196, Loss: 1.034404, Accuracy: 71.39%\n",
      "Batch 197, Loss: 0.972824, Accuracy: 71.42%\n",
      "Batch 198, Loss: 1.059139, Accuracy: 71.40%\n",
      "Batch 199, Loss: 1.062528, Accuracy: 71.37%\n",
      "Batch 200, Loss: 1.034492, Accuracy: 71.36%\n",
      "Batch 201, Loss: 0.978102, Accuracy: 71.39%\n",
      "Batch 202, Loss: 0.976961, Accuracy: 71.41%\n",
      "Batch 203, Loss: 1.037693, Accuracy: 71.40%\n",
      "Batch 204, Loss: 1.046101, Accuracy: 71.38%\n",
      "Batch 205, Loss: 1.090346, Accuracy: 71.34%\n",
      "Batch 206, Loss: 1.028538, Accuracy: 71.34%\n",
      "Batch 207, Loss: 1.087971, Accuracy: 71.30%\n",
      "Batch 208, Loss: 1.093295, Accuracy: 71.27%\n",
      "Batch 209, Loss: 1.044398, Accuracy: 71.26%\n",
      "Batch 210, Loss: 1.106305, Accuracy: 71.23%\n",
      "Batch 211, Loss: 0.980166, Accuracy: 71.26%\n",
      "Batch 212, Loss: 0.986520, Accuracy: 71.29%\n",
      "Batch 213, Loss: 1.014872, Accuracy: 71.30%\n",
      "Training - Epoch 41, Loss: 1.027284, Accuracy: 71.30%\n",
      "Validation Batch 1, Loss: 1.052692, Accuracy: 67.19%\n",
      "Validation Batch 2, Loss: 1.176708, Accuracy: 60.16%\n",
      "Validation Batch 3, Loss: 1.171750, Accuracy: 59.38%\n",
      "Validation Batch 4, Loss: 1.056487, Accuracy: 62.11%\n",
      "Validation Batch 5, Loss: 1.118609, Accuracy: 61.56%\n",
      "Validation Batch 6, Loss: 1.067750, Accuracy: 63.02%\n",
      "Validation Batch 7, Loss: 1.108864, Accuracy: 62.95%\n",
      "Validation Batch 8, Loss: 1.121512, Accuracy: 63.09%\n",
      "Validation Batch 9, Loss: 1.150417, Accuracy: 62.67%\n",
      "Validation Batch 10, Loss: 1.125295, Accuracy: 62.50%\n",
      "Validation Batch 11, Loss: 1.087202, Accuracy: 62.64%\n",
      "Validation Batch 12, Loss: 1.055983, Accuracy: 63.28%\n",
      "Validation Batch 13, Loss: 1.178313, Accuracy: 62.74%\n",
      "Validation Batch 14, Loss: 1.118745, Accuracy: 62.50%\n",
      "Validation Batch 15, Loss: 1.115842, Accuracy: 62.40%\n",
      "Validation Batch 16, Loss: 1.088154, Accuracy: 62.60%\n",
      "Validation Batch 17, Loss: 1.200541, Accuracy: 61.86%\n",
      "Validation Batch 18, Loss: 1.076494, Accuracy: 62.07%\n",
      "Validation Batch 19, Loss: 1.183874, Accuracy: 61.51%\n",
      "Validation Batch 20, Loss: 1.118553, Accuracy: 61.56%\n",
      "Validation Batch 21, Loss: 1.106884, Accuracy: 61.53%\n",
      "Validation Batch 22, Loss: 1.164819, Accuracy: 61.29%\n",
      "Validation Batch 23, Loss: 1.206228, Accuracy: 60.80%\n",
      "Validation Batch 24, Loss: 1.141010, Accuracy: 60.81%\n",
      "Validation Batch 25, Loss: 1.103722, Accuracy: 60.75%\n",
      "Validation Batch 26, Loss: 1.077721, Accuracy: 60.94%\n",
      "Validation Batch 27, Loss: 1.120656, Accuracy: 60.95%\n",
      "Validation - Epoch 41, Loss: 1.122031, Accuracy: 60.95%\n",
      "Patienceâ€”0\n",
      "Epoch 42\n",
      "Batch 1, Loss: 1.003697, Accuracy: 73.44%\n",
      "Batch 2, Loss: 1.044432, Accuracy: 70.31%\n",
      "Batch 3, Loss: 1.102538, Accuracy: 68.75%\n",
      "Batch 4, Loss: 0.963882, Accuracy: 71.48%\n",
      "Batch 5, Loss: 1.025571, Accuracy: 71.56%\n",
      "Batch 6, Loss: 1.061867, Accuracy: 70.83%\n",
      "Batch 7, Loss: 1.014310, Accuracy: 70.98%\n",
      "Batch 8, Loss: 0.937678, Accuracy: 72.27%\n",
      "Batch 9, Loss: 1.140165, Accuracy: 70.31%\n",
      "Batch 10, Loss: 0.959069, Accuracy: 71.09%\n",
      "Batch 11, Loss: 1.040482, Accuracy: 70.74%\n",
      "Batch 12, Loss: 0.996046, Accuracy: 70.96%\n",
      "Batch 13, Loss: 1.033741, Accuracy: 71.15%\n",
      "Batch 14, Loss: 1.091215, Accuracy: 70.65%\n",
      "Batch 15, Loss: 1.009924, Accuracy: 70.83%\n",
      "Batch 16, Loss: 1.028457, Accuracy: 71.00%\n",
      "Batch 17, Loss: 1.076391, Accuracy: 70.50%\n",
      "Batch 18, Loss: 1.041290, Accuracy: 70.57%\n",
      "Batch 19, Loss: 1.062924, Accuracy: 70.31%\n",
      "Batch 20, Loss: 0.968417, Accuracy: 70.62%\n",
      "Batch 21, Loss: 1.011414, Accuracy: 70.83%\n",
      "Batch 22, Loss: 0.965005, Accuracy: 71.24%\n",
      "Batch 23, Loss: 1.026753, Accuracy: 71.26%\n",
      "Batch 24, Loss: 1.034783, Accuracy: 71.29%\n",
      "Batch 25, Loss: 1.031454, Accuracy: 71.38%\n",
      "Batch 26, Loss: 1.083690, Accuracy: 71.15%\n",
      "Batch 27, Loss: 1.079208, Accuracy: 70.95%\n",
      "Batch 28, Loss: 0.986000, Accuracy: 71.15%\n",
      "Batch 29, Loss: 0.937946, Accuracy: 71.50%\n",
      "Batch 30, Loss: 1.023265, Accuracy: 71.51%\n",
      "Batch 31, Loss: 1.001340, Accuracy: 71.57%\n",
      "Batch 32, Loss: 1.053598, Accuracy: 71.48%\n",
      "Batch 33, Loss: 0.985999, Accuracy: 71.59%\n",
      "Batch 34, Loss: 1.007391, Accuracy: 71.65%\n",
      "Batch 35, Loss: 1.037662, Accuracy: 71.52%\n",
      "Batch 36, Loss: 0.993015, Accuracy: 71.61%\n",
      "Batch 37, Loss: 1.052664, Accuracy: 71.54%\n",
      "Batch 38, Loss: 1.153292, Accuracy: 71.18%\n",
      "Batch 39, Loss: 1.011701, Accuracy: 71.23%\n",
      "Batch 40, Loss: 1.109088, Accuracy: 71.02%\n",
      "Batch 41, Loss: 1.046787, Accuracy: 70.96%\n",
      "Batch 42, Loss: 1.058643, Accuracy: 70.91%\n",
      "Batch 43, Loss: 1.001193, Accuracy: 71.00%\n",
      "Batch 44, Loss: 0.950032, Accuracy: 71.16%\n",
      "Batch 45, Loss: 0.988779, Accuracy: 71.25%\n",
      "Batch 46, Loss: 0.955824, Accuracy: 71.43%\n",
      "Batch 47, Loss: 0.988787, Accuracy: 71.51%\n",
      "Batch 48, Loss: 0.929398, Accuracy: 71.68%\n",
      "Batch 49, Loss: 0.979379, Accuracy: 71.75%\n",
      "Batch 50, Loss: 1.045452, Accuracy: 71.72%\n",
      "Batch 51, Loss: 1.084754, Accuracy: 71.60%\n",
      "Batch 52, Loss: 0.960708, Accuracy: 71.72%\n",
      "Batch 53, Loss: 0.978933, Accuracy: 71.85%\n",
      "Batch 54, Loss: 0.957474, Accuracy: 71.96%\n",
      "Batch 55, Loss: 1.011947, Accuracy: 71.99%\n",
      "Batch 56, Loss: 1.061511, Accuracy: 71.88%\n",
      "Batch 57, Loss: 0.955186, Accuracy: 71.98%\n",
      "Batch 58, Loss: 1.070901, Accuracy: 71.85%\n",
      "Batch 59, Loss: 1.060695, Accuracy: 71.77%\n",
      "Batch 60, Loss: 1.087964, Accuracy: 71.67%\n",
      "Batch 61, Loss: 1.034555, Accuracy: 71.64%\n",
      "Batch 62, Loss: 1.027686, Accuracy: 71.60%\n",
      "Batch 63, Loss: 1.028172, Accuracy: 71.63%\n",
      "Batch 64, Loss: 0.972941, Accuracy: 71.75%\n",
      "Batch 65, Loss: 1.023068, Accuracy: 71.80%\n",
      "Batch 66, Loss: 1.022945, Accuracy: 71.83%\n",
      "Batch 67, Loss: 1.049124, Accuracy: 71.78%\n",
      "Batch 68, Loss: 1.027410, Accuracy: 71.81%\n",
      "Batch 69, Loss: 0.982723, Accuracy: 71.90%\n",
      "Batch 70, Loss: 0.991367, Accuracy: 71.96%\n",
      "Batch 71, Loss: 1.055035, Accuracy: 71.90%\n",
      "Batch 72, Loss: 1.095412, Accuracy: 71.79%\n",
      "Batch 73, Loss: 0.994118, Accuracy: 71.83%\n",
      "Batch 74, Loss: 1.039434, Accuracy: 71.83%\n",
      "Batch 75, Loss: 1.029108, Accuracy: 71.85%\n",
      "Batch 76, Loss: 0.993790, Accuracy: 71.92%\n",
      "Batch 77, Loss: 1.047124, Accuracy: 71.88%\n",
      "Batch 78, Loss: 1.023721, Accuracy: 71.88%\n",
      "Batch 79, Loss: 1.125838, Accuracy: 71.72%\n",
      "Batch 80, Loss: 1.021979, Accuracy: 71.74%\n",
      "Batch 81, Loss: 1.029259, Accuracy: 71.74%\n",
      "Batch 82, Loss: 1.059593, Accuracy: 71.70%\n",
      "Batch 83, Loss: 1.015186, Accuracy: 71.74%\n",
      "Batch 84, Loss: 1.048290, Accuracy: 71.69%\n",
      "Batch 85, Loss: 0.942470, Accuracy: 71.78%\n",
      "Batch 86, Loss: 1.009780, Accuracy: 71.80%\n",
      "Batch 87, Loss: 0.977439, Accuracy: 71.86%\n",
      "Batch 88, Loss: 1.066175, Accuracy: 71.79%\n",
      "Batch 89, Loss: 1.027421, Accuracy: 71.80%\n",
      "Batch 90, Loss: 1.058976, Accuracy: 71.75%\n",
      "Batch 91, Loss: 1.104219, Accuracy: 71.65%\n",
      "Batch 92, Loss: 1.116166, Accuracy: 71.52%\n",
      "Batch 93, Loss: 0.984349, Accuracy: 71.56%\n",
      "Batch 94, Loss: 1.085756, Accuracy: 71.49%\n",
      "Batch 95, Loss: 1.012128, Accuracy: 71.48%\n",
      "Batch 96, Loss: 1.052810, Accuracy: 71.47%\n",
      "Batch 97, Loss: 1.037057, Accuracy: 71.47%\n",
      "Batch 98, Loss: 1.133657, Accuracy: 71.36%\n",
      "Batch 99, Loss: 0.997610, Accuracy: 71.40%\n",
      "Batch 100, Loss: 0.997299, Accuracy: 71.44%\n",
      "Batch 101, Loss: 0.980907, Accuracy: 71.47%\n",
      "Batch 102, Loss: 0.958603, Accuracy: 71.54%\n",
      "Batch 103, Loss: 1.083857, Accuracy: 71.47%\n",
      "Batch 104, Loss: 1.040454, Accuracy: 71.45%\n",
      "Batch 105, Loss: 1.077689, Accuracy: 71.40%\n",
      "Batch 106, Loss: 1.004618, Accuracy: 71.40%\n",
      "Batch 107, Loss: 1.037719, Accuracy: 71.38%\n",
      "Batch 108, Loss: 1.042981, Accuracy: 71.34%\n",
      "Batch 109, Loss: 1.055977, Accuracy: 71.30%\n",
      "Batch 110, Loss: 1.116454, Accuracy: 71.22%\n",
      "Batch 111, Loss: 1.000708, Accuracy: 71.26%\n",
      "Batch 112, Loss: 1.056969, Accuracy: 71.23%\n",
      "Batch 113, Loss: 1.000252, Accuracy: 71.25%\n",
      "Batch 114, Loss: 1.038792, Accuracy: 71.24%\n",
      "Batch 115, Loss: 0.987657, Accuracy: 71.28%\n",
      "Batch 116, Loss: 1.032825, Accuracy: 71.27%\n",
      "Batch 117, Loss: 1.062723, Accuracy: 71.23%\n",
      "Batch 118, Loss: 1.030955, Accuracy: 71.23%\n",
      "Batch 119, Loss: 1.089565, Accuracy: 71.17%\n",
      "Batch 120, Loss: 0.989210, Accuracy: 71.21%\n",
      "Batch 121, Loss: 0.946220, Accuracy: 71.28%\n",
      "Batch 122, Loss: 1.059306, Accuracy: 71.26%\n",
      "Batch 123, Loss: 1.041636, Accuracy: 71.25%\n",
      "Batch 124, Loss: 1.001527, Accuracy: 71.26%\n",
      "Batch 125, Loss: 1.082608, Accuracy: 71.21%\n",
      "Batch 126, Loss: 1.022472, Accuracy: 71.22%\n",
      "Batch 127, Loss: 1.047587, Accuracy: 71.21%\n",
      "Batch 128, Loss: 0.996047, Accuracy: 71.25%\n",
      "Batch 129, Loss: 1.034253, Accuracy: 71.25%\n",
      "Batch 130, Loss: 1.043599, Accuracy: 71.23%\n",
      "Batch 131, Loss: 1.073084, Accuracy: 71.20%\n",
      "Batch 132, Loss: 0.984532, Accuracy: 71.22%\n",
      "Batch 133, Loss: 1.022443, Accuracy: 71.22%\n",
      "Batch 134, Loss: 1.043009, Accuracy: 71.21%\n",
      "Batch 135, Loss: 0.999531, Accuracy: 71.23%\n",
      "Batch 136, Loss: 1.085197, Accuracy: 71.17%\n",
      "Batch 137, Loss: 1.022702, Accuracy: 71.18%\n",
      "Batch 138, Loss: 1.050414, Accuracy: 71.16%\n",
      "Batch 139, Loss: 1.104493, Accuracy: 71.10%\n",
      "Batch 140, Loss: 1.032991, Accuracy: 71.09%\n",
      "Batch 141, Loss: 1.021802, Accuracy: 71.09%\n",
      "Batch 142, Loss: 1.048298, Accuracy: 71.06%\n",
      "Batch 143, Loss: 1.061061, Accuracy: 71.02%\n",
      "Batch 144, Loss: 0.977698, Accuracy: 71.06%\n",
      "Batch 145, Loss: 1.042461, Accuracy: 71.05%\n",
      "Batch 146, Loss: 1.024044, Accuracy: 71.04%\n",
      "Batch 147, Loss: 0.992172, Accuracy: 71.08%\n",
      "Batch 148, Loss: 1.080720, Accuracy: 71.02%\n",
      "Batch 149, Loss: 0.977252, Accuracy: 71.07%\n",
      "Batch 150, Loss: 0.992443, Accuracy: 71.10%\n",
      "Batch 151, Loss: 1.056252, Accuracy: 71.08%\n",
      "Batch 152, Loss: 1.072737, Accuracy: 71.03%\n",
      "Batch 153, Loss: 1.020052, Accuracy: 71.05%\n",
      "Batch 154, Loss: 1.139958, Accuracy: 70.97%\n",
      "Batch 155, Loss: 0.995119, Accuracy: 71.00%\n",
      "Batch 156, Loss: 1.066445, Accuracy: 70.95%\n",
      "Batch 157, Loss: 0.961619, Accuracy: 71.00%\n",
      "Batch 158, Loss: 1.126995, Accuracy: 70.93%\n",
      "Batch 159, Loss: 1.103830, Accuracy: 70.87%\n",
      "Batch 160, Loss: 0.941944, Accuracy: 70.95%\n",
      "Batch 161, Loss: 1.086694, Accuracy: 70.91%\n",
      "Batch 162, Loss: 1.027421, Accuracy: 70.92%\n",
      "Batch 163, Loss: 0.986995, Accuracy: 70.95%\n",
      "Batch 164, Loss: 1.071835, Accuracy: 70.91%\n",
      "Batch 165, Loss: 0.958101, Accuracy: 70.94%\n",
      "Batch 166, Loss: 1.033436, Accuracy: 70.94%\n",
      "Batch 167, Loss: 0.978670, Accuracy: 70.98%\n",
      "Batch 168, Loss: 0.997441, Accuracy: 71.00%\n",
      "Batch 169, Loss: 0.909403, Accuracy: 71.10%\n",
      "Batch 170, Loss: 0.976141, Accuracy: 71.12%\n",
      "Batch 171, Loss: 1.002526, Accuracy: 71.15%\n",
      "Batch 172, Loss: 1.101041, Accuracy: 71.10%\n",
      "Batch 173, Loss: 1.038602, Accuracy: 71.10%\n",
      "Batch 174, Loss: 1.023051, Accuracy: 71.11%\n",
      "Batch 175, Loss: 0.956658, Accuracy: 71.17%\n",
      "Batch 176, Loss: 1.155910, Accuracy: 71.08%\n",
      "Batch 177, Loss: 1.060488, Accuracy: 71.08%\n",
      "Batch 178, Loss: 1.009107, Accuracy: 71.09%\n",
      "Batch 179, Loss: 1.048445, Accuracy: 71.08%\n",
      "Batch 180, Loss: 0.975698, Accuracy: 71.12%\n",
      "Batch 181, Loss: 1.058778, Accuracy: 71.11%\n",
      "Batch 182, Loss: 0.969482, Accuracy: 71.15%\n",
      "Batch 183, Loss: 1.007573, Accuracy: 71.16%\n",
      "Batch 184, Loss: 1.027112, Accuracy: 71.14%\n",
      "Batch 185, Loss: 1.034922, Accuracy: 71.13%\n",
      "Batch 186, Loss: 1.029288, Accuracy: 71.14%\n",
      "Batch 187, Loss: 1.010163, Accuracy: 71.15%\n",
      "Batch 188, Loss: 0.950212, Accuracy: 71.20%\n",
      "Batch 189, Loss: 1.001895, Accuracy: 71.21%\n",
      "Batch 190, Loss: 1.024742, Accuracy: 71.21%\n",
      "Batch 191, Loss: 0.979932, Accuracy: 71.24%\n",
      "Batch 192, Loss: 1.015800, Accuracy: 71.26%\n",
      "Batch 193, Loss: 1.097090, Accuracy: 71.22%\n",
      "Batch 194, Loss: 0.966080, Accuracy: 71.25%\n",
      "Batch 195, Loss: 1.022614, Accuracy: 71.24%\n",
      "Batch 196, Loss: 0.967677, Accuracy: 71.28%\n",
      "Batch 197, Loss: 0.933128, Accuracy: 71.33%\n",
      "Batch 198, Loss: 1.065752, Accuracy: 71.31%\n",
      "Batch 199, Loss: 0.994319, Accuracy: 71.35%\n",
      "Batch 200, Loss: 0.973686, Accuracy: 71.38%\n",
      "Batch 201, Loss: 0.993695, Accuracy: 71.39%\n",
      "Batch 202, Loss: 1.036103, Accuracy: 71.40%\n",
      "Batch 203, Loss: 0.986274, Accuracy: 71.41%\n",
      "Batch 204, Loss: 1.095485, Accuracy: 71.38%\n",
      "Batch 205, Loss: 1.017472, Accuracy: 71.39%\n",
      "Batch 206, Loss: 1.040238, Accuracy: 71.39%\n",
      "Batch 207, Loss: 1.024582, Accuracy: 71.38%\n",
      "Batch 208, Loss: 0.985983, Accuracy: 71.40%\n",
      "Batch 209, Loss: 0.958730, Accuracy: 71.42%\n",
      "Batch 210, Loss: 1.035189, Accuracy: 71.41%\n",
      "Batch 211, Loss: 1.047506, Accuracy: 71.39%\n",
      "Batch 212, Loss: 1.016058, Accuracy: 71.40%\n",
      "Batch 213, Loss: 1.054131, Accuracy: 71.39%\n",
      "Training - Epoch 42, Loss: 1.026327, Accuracy: 71.39%\n",
      "Validation Batch 1, Loss: 1.051802, Accuracy: 67.19%\n",
      "Validation Batch 2, Loss: 1.172149, Accuracy: 61.72%\n",
      "Validation Batch 3, Loss: 1.168550, Accuracy: 60.42%\n",
      "Validation Batch 4, Loss: 1.056116, Accuracy: 62.89%\n",
      "Validation Batch 5, Loss: 1.120225, Accuracy: 61.88%\n",
      "Validation Batch 6, Loss: 1.064337, Accuracy: 63.02%\n",
      "Validation Batch 7, Loss: 1.102256, Accuracy: 63.17%\n",
      "Validation Batch 8, Loss: 1.118886, Accuracy: 63.09%\n",
      "Validation Batch 9, Loss: 1.145167, Accuracy: 62.67%\n",
      "Validation Batch 10, Loss: 1.119481, Accuracy: 62.50%\n",
      "Validation Batch 11, Loss: 1.090815, Accuracy: 62.50%\n",
      "Validation Batch 12, Loss: 1.050582, Accuracy: 63.15%\n",
      "Validation Batch 13, Loss: 1.168808, Accuracy: 62.86%\n",
      "Validation Batch 14, Loss: 1.116866, Accuracy: 62.72%\n",
      "Validation Batch 15, Loss: 1.112841, Accuracy: 62.71%\n",
      "Validation Batch 16, Loss: 1.088841, Accuracy: 62.89%\n",
      "Validation Batch 17, Loss: 1.197409, Accuracy: 62.04%\n",
      "Validation Batch 18, Loss: 1.068850, Accuracy: 62.24%\n",
      "Validation Batch 19, Loss: 1.180269, Accuracy: 61.68%\n",
      "Validation Batch 20, Loss: 1.111169, Accuracy: 61.88%\n",
      "Validation Batch 21, Loss: 1.107229, Accuracy: 61.83%\n",
      "Validation Batch 22, Loss: 1.164588, Accuracy: 61.65%\n",
      "Validation Batch 23, Loss: 1.205878, Accuracy: 61.21%\n",
      "Validation Batch 24, Loss: 1.134738, Accuracy: 61.26%\n",
      "Validation Batch 25, Loss: 1.102780, Accuracy: 61.19%\n",
      "Validation Batch 26, Loss: 1.072854, Accuracy: 61.42%\n",
      "Validation Batch 27, Loss: 1.122833, Accuracy: 61.42%\n",
      "Validation - Epoch 42, Loss: 1.119123, Accuracy: 61.42%\n",
      "Patienceâ€”0\n",
      "Epoch 43\n",
      "Batch 1, Loss: 1.044702, Accuracy: 67.19%\n",
      "Batch 2, Loss: 1.061524, Accuracy: 67.97%\n",
      "Batch 3, Loss: 1.048695, Accuracy: 67.19%\n",
      "Batch 4, Loss: 0.948593, Accuracy: 70.31%\n",
      "Batch 5, Loss: 1.022588, Accuracy: 70.62%\n",
      "Batch 6, Loss: 1.008597, Accuracy: 71.35%\n",
      "Batch 7, Loss: 0.953463, Accuracy: 72.99%\n",
      "Batch 8, Loss: 1.025374, Accuracy: 72.85%\n",
      "Batch 9, Loss: 1.098884, Accuracy: 71.70%\n",
      "Batch 10, Loss: 1.064557, Accuracy: 71.09%\n",
      "Batch 11, Loss: 1.088624, Accuracy: 70.31%\n",
      "Batch 12, Loss: 0.967226, Accuracy: 71.09%\n",
      "Batch 13, Loss: 1.016574, Accuracy: 71.03%\n",
      "Batch 14, Loss: 1.070041, Accuracy: 70.76%\n",
      "Batch 15, Loss: 1.079514, Accuracy: 70.31%\n",
      "Batch 16, Loss: 1.083202, Accuracy: 70.02%\n",
      "Batch 17, Loss: 1.083751, Accuracy: 69.76%\n",
      "Batch 18, Loss: 1.066291, Accuracy: 69.62%\n",
      "Batch 19, Loss: 0.994354, Accuracy: 69.90%\n",
      "Batch 20, Loss: 0.986368, Accuracy: 70.23%\n",
      "Batch 21, Loss: 1.026226, Accuracy: 70.31%\n",
      "Batch 22, Loss: 0.978266, Accuracy: 70.67%\n",
      "Batch 23, Loss: 0.998570, Accuracy: 70.92%\n",
      "Batch 24, Loss: 1.015662, Accuracy: 70.83%\n",
      "Batch 25, Loss: 0.939241, Accuracy: 71.12%\n",
      "Batch 26, Loss: 1.051227, Accuracy: 71.03%\n",
      "Batch 27, Loss: 0.939392, Accuracy: 71.47%\n",
      "Batch 28, Loss: 1.087698, Accuracy: 71.32%\n",
      "Batch 29, Loss: 1.078416, Accuracy: 71.12%\n",
      "Batch 30, Loss: 1.004676, Accuracy: 71.25%\n",
      "Batch 31, Loss: 1.097843, Accuracy: 71.02%\n",
      "Batch 32, Loss: 1.073471, Accuracy: 70.90%\n",
      "Batch 33, Loss: 1.092984, Accuracy: 70.69%\n",
      "Batch 34, Loss: 0.964857, Accuracy: 70.91%\n",
      "Batch 35, Loss: 1.063838, Accuracy: 70.80%\n",
      "Batch 36, Loss: 1.029798, Accuracy: 70.83%\n",
      "Batch 37, Loss: 1.056465, Accuracy: 70.78%\n",
      "Batch 38, Loss: 1.046166, Accuracy: 70.76%\n",
      "Batch 39, Loss: 1.076151, Accuracy: 70.63%\n",
      "Batch 40, Loss: 1.090269, Accuracy: 70.55%\n",
      "Batch 41, Loss: 0.969513, Accuracy: 70.69%\n",
      "Batch 42, Loss: 0.993342, Accuracy: 70.80%\n",
      "Batch 43, Loss: 1.007737, Accuracy: 70.89%\n",
      "Batch 44, Loss: 1.088484, Accuracy: 70.70%\n",
      "Batch 45, Loss: 1.030429, Accuracy: 70.69%\n",
      "Batch 46, Loss: 0.988945, Accuracy: 70.86%\n",
      "Batch 47, Loss: 0.933333, Accuracy: 71.08%\n",
      "Batch 48, Loss: 0.983748, Accuracy: 71.19%\n",
      "Batch 49, Loss: 1.100368, Accuracy: 71.08%\n",
      "Batch 50, Loss: 1.025700, Accuracy: 71.12%\n",
      "Batch 51, Loss: 1.026159, Accuracy: 71.08%\n",
      "Batch 52, Loss: 1.026929, Accuracy: 71.06%\n",
      "Batch 53, Loss: 1.064675, Accuracy: 70.99%\n",
      "Batch 54, Loss: 0.985240, Accuracy: 71.09%\n",
      "Batch 55, Loss: 0.929327, Accuracy: 71.28%\n",
      "Batch 56, Loss: 0.945239, Accuracy: 71.43%\n",
      "Batch 57, Loss: 0.990278, Accuracy: 71.49%\n",
      "Batch 58, Loss: 1.061481, Accuracy: 71.42%\n",
      "Batch 59, Loss: 1.096998, Accuracy: 71.32%\n",
      "Batch 60, Loss: 0.953714, Accuracy: 71.46%\n",
      "Batch 61, Loss: 1.115561, Accuracy: 71.29%\n",
      "Batch 62, Loss: 1.077363, Accuracy: 71.19%\n",
      "Batch 63, Loss: 1.063103, Accuracy: 71.13%\n",
      "Batch 64, Loss: 0.985712, Accuracy: 71.22%\n",
      "Batch 65, Loss: 0.950912, Accuracy: 71.35%\n",
      "Batch 66, Loss: 0.979940, Accuracy: 71.43%\n",
      "Batch 67, Loss: 1.051917, Accuracy: 71.36%\n",
      "Batch 68, Loss: 1.033088, Accuracy: 71.37%\n",
      "Batch 69, Loss: 1.018294, Accuracy: 71.40%\n",
      "Batch 70, Loss: 1.019194, Accuracy: 71.43%\n",
      "Batch 71, Loss: 1.040316, Accuracy: 71.37%\n",
      "Batch 72, Loss: 1.014537, Accuracy: 71.42%\n",
      "Batch 73, Loss: 0.994080, Accuracy: 71.47%\n",
      "Batch 74, Loss: 0.954415, Accuracy: 71.58%\n",
      "Batch 75, Loss: 1.068621, Accuracy: 71.50%\n",
      "Batch 76, Loss: 0.928575, Accuracy: 71.61%\n",
      "Batch 77, Loss: 1.103350, Accuracy: 71.51%\n",
      "Batch 78, Loss: 0.984411, Accuracy: 71.57%\n",
      "Batch 79, Loss: 0.954105, Accuracy: 71.66%\n",
      "Batch 80, Loss: 0.993887, Accuracy: 71.70%\n",
      "Batch 81, Loss: 0.981570, Accuracy: 71.76%\n",
      "Batch 82, Loss: 1.011690, Accuracy: 71.78%\n",
      "Batch 83, Loss: 1.064493, Accuracy: 71.72%\n",
      "Batch 84, Loss: 1.090895, Accuracy: 71.61%\n",
      "Batch 85, Loss: 1.013229, Accuracy: 71.64%\n",
      "Batch 86, Loss: 1.057998, Accuracy: 71.57%\n",
      "Batch 87, Loss: 1.084755, Accuracy: 71.50%\n",
      "Batch 88, Loss: 0.997141, Accuracy: 71.54%\n",
      "Batch 89, Loss: 1.024146, Accuracy: 71.56%\n",
      "Batch 90, Loss: 1.024530, Accuracy: 71.58%\n",
      "Batch 91, Loss: 1.062314, Accuracy: 71.51%\n",
      "Batch 92, Loss: 1.062335, Accuracy: 71.45%\n",
      "Batch 93, Loss: 1.067224, Accuracy: 71.40%\n",
      "Batch 94, Loss: 1.071775, Accuracy: 71.36%\n",
      "Batch 95, Loss: 1.051610, Accuracy: 71.33%\n",
      "Batch 96, Loss: 0.991290, Accuracy: 71.39%\n",
      "Batch 97, Loss: 1.012562, Accuracy: 71.46%\n",
      "Batch 98, Loss: 1.038806, Accuracy: 71.43%\n",
      "Batch 99, Loss: 0.983027, Accuracy: 71.48%\n",
      "Batch 100, Loss: 1.045573, Accuracy: 71.47%\n",
      "Batch 101, Loss: 0.966375, Accuracy: 71.52%\n",
      "Batch 102, Loss: 1.091037, Accuracy: 71.46%\n",
      "Batch 103, Loss: 1.097263, Accuracy: 71.39%\n",
      "Batch 104, Loss: 1.078967, Accuracy: 71.33%\n",
      "Batch 105, Loss: 1.008689, Accuracy: 71.34%\n",
      "Batch 106, Loss: 1.045954, Accuracy: 71.31%\n",
      "Batch 107, Loss: 0.943319, Accuracy: 71.36%\n",
      "Batch 108, Loss: 0.988906, Accuracy: 71.37%\n",
      "Batch 109, Loss: 1.066630, Accuracy: 71.33%\n",
      "Batch 110, Loss: 1.041581, Accuracy: 71.32%\n",
      "Batch 111, Loss: 1.083282, Accuracy: 71.24%\n",
      "Batch 112, Loss: 1.035208, Accuracy: 71.22%\n",
      "Batch 113, Loss: 0.999224, Accuracy: 71.25%\n",
      "Batch 114, Loss: 1.000033, Accuracy: 71.29%\n",
      "Batch 115, Loss: 1.015928, Accuracy: 71.32%\n",
      "Batch 116, Loss: 0.941119, Accuracy: 71.40%\n",
      "Batch 117, Loss: 0.947646, Accuracy: 71.49%\n",
      "Batch 118, Loss: 1.012811, Accuracy: 71.50%\n",
      "Batch 119, Loss: 0.984269, Accuracy: 71.56%\n",
      "Batch 120, Loss: 1.029050, Accuracy: 71.56%\n",
      "Batch 121, Loss: 0.970585, Accuracy: 71.62%\n",
      "Batch 122, Loss: 1.047559, Accuracy: 71.59%\n",
      "Batch 123, Loss: 1.022965, Accuracy: 71.60%\n",
      "Batch 124, Loss: 1.018735, Accuracy: 71.60%\n",
      "Batch 125, Loss: 0.992803, Accuracy: 71.64%\n",
      "Batch 126, Loss: 1.050475, Accuracy: 71.61%\n",
      "Batch 127, Loss: 1.026337, Accuracy: 71.63%\n",
      "Batch 128, Loss: 0.999884, Accuracy: 71.66%\n",
      "Batch 129, Loss: 1.012136, Accuracy: 71.67%\n",
      "Batch 130, Loss: 1.050242, Accuracy: 71.62%\n",
      "Batch 131, Loss: 1.010890, Accuracy: 71.64%\n",
      "Batch 132, Loss: 1.027898, Accuracy: 71.64%\n",
      "Batch 133, Loss: 1.090072, Accuracy: 71.59%\n",
      "Batch 134, Loss: 1.037763, Accuracy: 71.58%\n",
      "Batch 135, Loss: 1.079404, Accuracy: 71.52%\n",
      "Batch 136, Loss: 0.966567, Accuracy: 71.55%\n",
      "Batch 137, Loss: 0.953819, Accuracy: 71.60%\n",
      "Batch 138, Loss: 1.079431, Accuracy: 71.55%\n",
      "Batch 139, Loss: 1.048348, Accuracy: 71.53%\n",
      "Batch 140, Loss: 0.975171, Accuracy: 71.55%\n",
      "Batch 141, Loss: 1.094816, Accuracy: 71.49%\n",
      "Batch 142, Loss: 1.005418, Accuracy: 71.51%\n",
      "Batch 143, Loss: 1.023798, Accuracy: 71.51%\n",
      "Batch 144, Loss: 0.978653, Accuracy: 71.56%\n",
      "Batch 145, Loss: 1.068347, Accuracy: 71.53%\n",
      "Batch 146, Loss: 0.914137, Accuracy: 71.60%\n",
      "Batch 147, Loss: 0.939475, Accuracy: 71.67%\n",
      "Batch 148, Loss: 1.015680, Accuracy: 71.68%\n",
      "Batch 149, Loss: 1.005901, Accuracy: 71.70%\n",
      "Batch 150, Loss: 1.017862, Accuracy: 71.70%\n",
      "Batch 151, Loss: 0.994377, Accuracy: 71.71%\n",
      "Batch 152, Loss: 1.060885, Accuracy: 71.68%\n",
      "Batch 153, Loss: 1.087197, Accuracy: 71.61%\n",
      "Batch 154, Loss: 0.937540, Accuracy: 71.66%\n",
      "Batch 155, Loss: 1.024976, Accuracy: 71.66%\n",
      "Batch 156, Loss: 1.047944, Accuracy: 71.63%\n",
      "Batch 157, Loss: 1.026408, Accuracy: 71.64%\n",
      "Batch 158, Loss: 1.001731, Accuracy: 71.65%\n",
      "Batch 159, Loss: 0.980446, Accuracy: 71.68%\n",
      "Batch 160, Loss: 0.992837, Accuracy: 71.70%\n",
      "Batch 161, Loss: 1.064030, Accuracy: 71.69%\n",
      "Batch 162, Loss: 1.048897, Accuracy: 71.68%\n",
      "Batch 163, Loss: 1.137333, Accuracy: 71.62%\n",
      "Batch 164, Loss: 0.959167, Accuracy: 71.66%\n",
      "Batch 165, Loss: 1.006704, Accuracy: 71.66%\n",
      "Batch 166, Loss: 1.056463, Accuracy: 71.64%\n",
      "Batch 167, Loss: 1.073012, Accuracy: 71.61%\n",
      "Batch 168, Loss: 1.009465, Accuracy: 71.62%\n",
      "Batch 169, Loss: 1.118515, Accuracy: 71.55%\n",
      "Batch 170, Loss: 1.007749, Accuracy: 71.56%\n",
      "Batch 171, Loss: 1.008140, Accuracy: 71.58%\n",
      "Batch 172, Loss: 0.996006, Accuracy: 71.60%\n",
      "Batch 173, Loss: 1.021585, Accuracy: 71.61%\n",
      "Batch 174, Loss: 1.002897, Accuracy: 71.63%\n",
      "Batch 175, Loss: 0.973965, Accuracy: 71.67%\n",
      "Batch 176, Loss: 1.104222, Accuracy: 71.60%\n",
      "Batch 177, Loss: 1.006845, Accuracy: 71.60%\n",
      "Batch 178, Loss: 1.024271, Accuracy: 71.59%\n",
      "Batch 179, Loss: 1.082151, Accuracy: 71.55%\n",
      "Batch 180, Loss: 0.942536, Accuracy: 71.59%\n",
      "Batch 181, Loss: 1.061928, Accuracy: 71.56%\n",
      "Batch 182, Loss: 0.930807, Accuracy: 71.60%\n",
      "Batch 183, Loss: 1.033867, Accuracy: 71.60%\n",
      "Batch 184, Loss: 1.020933, Accuracy: 71.59%\n",
      "Batch 185, Loss: 1.067769, Accuracy: 71.57%\n",
      "Batch 186, Loss: 1.004617, Accuracy: 71.59%\n",
      "Batch 187, Loss: 1.051467, Accuracy: 71.57%\n",
      "Batch 188, Loss: 1.100804, Accuracy: 71.53%\n",
      "Batch 189, Loss: 1.053725, Accuracy: 71.52%\n",
      "Batch 190, Loss: 0.979249, Accuracy: 71.55%\n",
      "Batch 191, Loss: 1.004257, Accuracy: 71.56%\n",
      "Batch 192, Loss: 1.048568, Accuracy: 71.53%\n",
      "Batch 193, Loss: 1.035822, Accuracy: 71.53%\n",
      "Batch 194, Loss: 0.953708, Accuracy: 71.57%\n",
      "Batch 195, Loss: 1.040410, Accuracy: 71.57%\n",
      "Batch 196, Loss: 1.047317, Accuracy: 71.56%\n",
      "Batch 197, Loss: 0.970201, Accuracy: 71.60%\n",
      "Batch 198, Loss: 0.955481, Accuracy: 71.64%\n",
      "Batch 199, Loss: 0.977202, Accuracy: 71.66%\n",
      "Batch 200, Loss: 1.060717, Accuracy: 71.64%\n",
      "Batch 201, Loss: 1.022241, Accuracy: 71.64%\n",
      "Batch 202, Loss: 1.064860, Accuracy: 71.62%\n",
      "Batch 203, Loss: 1.016446, Accuracy: 71.63%\n",
      "Batch 204, Loss: 1.158443, Accuracy: 71.56%\n",
      "Batch 205, Loss: 0.934306, Accuracy: 71.59%\n",
      "Batch 206, Loss: 0.933289, Accuracy: 71.64%\n",
      "Batch 207, Loss: 0.991957, Accuracy: 71.66%\n",
      "Batch 208, Loss: 0.996709, Accuracy: 71.68%\n",
      "Batch 209, Loss: 1.070031, Accuracy: 71.67%\n",
      "Batch 210, Loss: 1.080387, Accuracy: 71.64%\n",
      "Batch 211, Loss: 0.926215, Accuracy: 71.69%\n",
      "Batch 212, Loss: 1.027912, Accuracy: 71.70%\n",
      "Batch 213, Loss: 1.136137, Accuracy: 71.64%\n",
      "Training - Epoch 43, Loss: 1.024191, Accuracy: 71.64%\n",
      "Validation Batch 1, Loss: 1.025194, Accuracy: 70.31%\n",
      "Validation Batch 2, Loss: 1.136565, Accuracy: 64.06%\n",
      "Validation Batch 3, Loss: 1.135697, Accuracy: 63.02%\n",
      "Validation Batch 4, Loss: 1.034097, Accuracy: 65.23%\n",
      "Validation Batch 5, Loss: 1.080826, Accuracy: 64.38%\n",
      "Validation Batch 6, Loss: 1.031883, Accuracy: 65.62%\n",
      "Validation Batch 7, Loss: 1.077547, Accuracy: 66.07%\n",
      "Validation Batch 8, Loss: 1.105447, Accuracy: 65.62%\n",
      "Validation Batch 9, Loss: 1.126614, Accuracy: 65.28%\n",
      "Validation Batch 10, Loss: 1.083356, Accuracy: 65.62%\n",
      "Validation Batch 11, Loss: 1.053517, Accuracy: 65.91%\n",
      "Validation Batch 12, Loss: 1.023106, Accuracy: 66.41%\n",
      "Validation Batch 13, Loss: 1.149351, Accuracy: 65.99%\n",
      "Validation Batch 14, Loss: 1.085702, Accuracy: 65.85%\n",
      "Validation Batch 15, Loss: 1.086783, Accuracy: 65.73%\n",
      "Validation Batch 16, Loss: 1.055340, Accuracy: 65.72%\n",
      "Validation Batch 17, Loss: 1.156509, Accuracy: 65.26%\n",
      "Validation Batch 18, Loss: 1.043207, Accuracy: 65.62%\n",
      "Validation Batch 19, Loss: 1.146882, Accuracy: 65.05%\n",
      "Validation Batch 20, Loss: 1.082368, Accuracy: 65.23%\n",
      "Validation Batch 21, Loss: 1.078172, Accuracy: 65.25%\n",
      "Validation Batch 22, Loss: 1.131180, Accuracy: 65.06%\n",
      "Validation Batch 23, Loss: 1.182205, Accuracy: 64.67%\n",
      "Validation Batch 24, Loss: 1.117402, Accuracy: 64.65%\n",
      "Validation Batch 25, Loss: 1.067067, Accuracy: 64.69%\n",
      "Validation Batch 26, Loss: 1.061172, Accuracy: 64.84%\n",
      "Validation Batch 27, Loss: 1.076875, Accuracy: 64.94%\n",
      "Validation - Epoch 43, Loss: 1.090151, Accuracy: 64.94%\n",
      "Patienceâ€”0\n",
      "Epoch 44\n",
      "Batch 1, Loss: 1.061669, Accuracy: 64.06%\n",
      "Batch 2, Loss: 0.998298, Accuracy: 69.53%\n",
      "Batch 3, Loss: 1.008945, Accuracy: 70.83%\n",
      "Batch 4, Loss: 0.940077, Accuracy: 73.05%\n",
      "Batch 5, Loss: 0.960585, Accuracy: 74.38%\n",
      "Batch 6, Loss: 1.016988, Accuracy: 74.22%\n",
      "Batch 7, Loss: 1.067654, Accuracy: 72.99%\n",
      "Batch 8, Loss: 1.000338, Accuracy: 73.44%\n",
      "Batch 9, Loss: 0.908783, Accuracy: 74.65%\n",
      "Batch 10, Loss: 1.048674, Accuracy: 73.91%\n",
      "Batch 11, Loss: 1.058986, Accuracy: 73.72%\n",
      "Batch 12, Loss: 0.994491, Accuracy: 73.70%\n",
      "Batch 13, Loss: 1.001165, Accuracy: 73.68%\n",
      "Batch 14, Loss: 1.041639, Accuracy: 73.55%\n",
      "Batch 15, Loss: 0.981343, Accuracy: 73.54%\n",
      "Batch 16, Loss: 0.953220, Accuracy: 73.83%\n",
      "Batch 17, Loss: 1.074169, Accuracy: 73.53%\n",
      "Batch 18, Loss: 1.112273, Accuracy: 72.92%\n",
      "Batch 19, Loss: 1.025672, Accuracy: 72.86%\n",
      "Batch 20, Loss: 1.042594, Accuracy: 72.73%\n",
      "Batch 21, Loss: 1.098292, Accuracy: 72.17%\n",
      "Batch 22, Loss: 1.111160, Accuracy: 71.73%\n",
      "Batch 23, Loss: 0.988541, Accuracy: 72.01%\n",
      "Batch 24, Loss: 1.057607, Accuracy: 71.81%\n",
      "Batch 25, Loss: 1.050821, Accuracy: 71.75%\n",
      "Batch 26, Loss: 1.014515, Accuracy: 71.81%\n",
      "Batch 27, Loss: 1.036772, Accuracy: 71.76%\n",
      "Batch 28, Loss: 1.023293, Accuracy: 71.76%\n",
      "Batch 29, Loss: 1.114640, Accuracy: 71.39%\n",
      "Batch 30, Loss: 1.005476, Accuracy: 71.51%\n",
      "Batch 31, Loss: 0.961555, Accuracy: 71.67%\n",
      "Batch 32, Loss: 0.965886, Accuracy: 71.88%\n",
      "Batch 33, Loss: 0.959438, Accuracy: 72.02%\n",
      "Batch 34, Loss: 1.119118, Accuracy: 71.74%\n",
      "Batch 35, Loss: 0.991076, Accuracy: 71.88%\n",
      "Batch 36, Loss: 1.037058, Accuracy: 71.79%\n",
      "Batch 37, Loss: 1.002877, Accuracy: 71.88%\n",
      "Batch 38, Loss: 0.935681, Accuracy: 72.08%\n",
      "Batch 39, Loss: 1.013484, Accuracy: 72.08%\n",
      "Batch 40, Loss: 1.046201, Accuracy: 72.07%\n",
      "Batch 41, Loss: 0.983834, Accuracy: 72.14%\n",
      "Batch 42, Loss: 1.019401, Accuracy: 72.21%\n",
      "Batch 43, Loss: 0.967241, Accuracy: 72.31%\n",
      "Batch 44, Loss: 0.916835, Accuracy: 72.51%\n",
      "Batch 45, Loss: 0.994319, Accuracy: 72.60%\n",
      "Batch 46, Loss: 1.135920, Accuracy: 72.38%\n",
      "Batch 47, Loss: 1.004875, Accuracy: 72.37%\n",
      "Batch 48, Loss: 1.048382, Accuracy: 72.30%\n",
      "Batch 49, Loss: 1.023899, Accuracy: 72.29%\n",
      "Batch 50, Loss: 1.102371, Accuracy: 72.03%\n",
      "Batch 51, Loss: 0.980230, Accuracy: 72.09%\n",
      "Batch 52, Loss: 1.111508, Accuracy: 71.91%\n",
      "Batch 53, Loss: 0.980211, Accuracy: 71.99%\n",
      "Batch 54, Loss: 1.020009, Accuracy: 71.96%\n",
      "Batch 55, Loss: 0.964664, Accuracy: 72.13%\n",
      "Batch 56, Loss: 0.977376, Accuracy: 72.24%\n",
      "Batch 57, Loss: 0.949293, Accuracy: 72.40%\n",
      "Batch 58, Loss: 0.986111, Accuracy: 72.44%\n",
      "Batch 59, Loss: 1.045923, Accuracy: 72.43%\n",
      "Batch 60, Loss: 0.971566, Accuracy: 72.47%\n",
      "Batch 61, Loss: 1.118035, Accuracy: 72.23%\n",
      "Batch 62, Loss: 1.051886, Accuracy: 72.15%\n",
      "Batch 63, Loss: 1.097800, Accuracy: 72.02%\n",
      "Batch 64, Loss: 1.020330, Accuracy: 72.02%\n",
      "Batch 65, Loss: 1.002980, Accuracy: 72.02%\n",
      "Batch 66, Loss: 1.073779, Accuracy: 71.92%\n",
      "Batch 67, Loss: 1.050787, Accuracy: 71.90%\n",
      "Batch 68, Loss: 1.065455, Accuracy: 71.81%\n",
      "Batch 69, Loss: 0.975548, Accuracy: 71.85%\n",
      "Batch 70, Loss: 1.084961, Accuracy: 71.79%\n",
      "Batch 71, Loss: 1.074323, Accuracy: 71.74%\n",
      "Batch 72, Loss: 1.039773, Accuracy: 71.70%\n",
      "Batch 73, Loss: 1.065994, Accuracy: 71.64%\n",
      "Batch 74, Loss: 0.904555, Accuracy: 71.81%\n",
      "Batch 75, Loss: 0.974274, Accuracy: 71.85%\n",
      "Batch 76, Loss: 1.081275, Accuracy: 71.81%\n",
      "Batch 77, Loss: 0.933184, Accuracy: 71.96%\n",
      "Batch 78, Loss: 1.041614, Accuracy: 71.92%\n",
      "Batch 79, Loss: 0.981058, Accuracy: 71.95%\n",
      "Batch 80, Loss: 1.040297, Accuracy: 71.93%\n",
      "Batch 81, Loss: 0.968624, Accuracy: 72.01%\n",
      "Batch 82, Loss: 0.966065, Accuracy: 72.07%\n",
      "Batch 83, Loss: 1.002271, Accuracy: 72.08%\n",
      "Batch 84, Loss: 1.042325, Accuracy: 72.06%\n",
      "Batch 85, Loss: 1.002930, Accuracy: 72.04%\n",
      "Batch 86, Loss: 1.021530, Accuracy: 72.06%\n",
      "Batch 87, Loss: 1.016385, Accuracy: 72.05%\n",
      "Batch 88, Loss: 0.992584, Accuracy: 72.09%\n",
      "Batch 89, Loss: 0.959811, Accuracy: 72.16%\n",
      "Batch 90, Loss: 0.953662, Accuracy: 72.24%\n",
      "Batch 91, Loss: 1.039955, Accuracy: 72.18%\n",
      "Batch 92, Loss: 1.023324, Accuracy: 72.20%\n",
      "Batch 93, Loss: 0.927420, Accuracy: 72.30%\n",
      "Batch 94, Loss: 1.041610, Accuracy: 72.26%\n",
      "Batch 95, Loss: 0.985884, Accuracy: 72.30%\n",
      "Batch 96, Loss: 0.967899, Accuracy: 72.36%\n",
      "Batch 97, Loss: 1.033069, Accuracy: 72.33%\n",
      "Batch 98, Loss: 1.070617, Accuracy: 72.27%\n",
      "Batch 99, Loss: 1.092428, Accuracy: 72.21%\n",
      "Batch 100, Loss: 1.082075, Accuracy: 72.12%\n",
      "Batch 101, Loss: 1.000492, Accuracy: 72.14%\n",
      "Batch 102, Loss: 1.225645, Accuracy: 71.92%\n",
      "Batch 103, Loss: 1.098266, Accuracy: 71.84%\n",
      "Batch 104, Loss: 0.992310, Accuracy: 71.86%\n",
      "Batch 105, Loss: 1.010820, Accuracy: 71.88%\n",
      "Batch 106, Loss: 1.108953, Accuracy: 71.77%\n",
      "Batch 107, Loss: 1.074662, Accuracy: 71.76%\n",
      "Batch 108, Loss: 0.947024, Accuracy: 71.85%\n",
      "Batch 109, Loss: 0.971126, Accuracy: 71.92%\n",
      "Batch 110, Loss: 1.047430, Accuracy: 71.90%\n",
      "Batch 111, Loss: 1.027121, Accuracy: 71.89%\n",
      "Batch 112, Loss: 1.037344, Accuracy: 71.88%\n",
      "Batch 113, Loss: 0.879169, Accuracy: 72.01%\n",
      "Batch 114, Loss: 1.079171, Accuracy: 71.97%\n",
      "Batch 115, Loss: 1.070806, Accuracy: 71.92%\n",
      "Batch 116, Loss: 1.041271, Accuracy: 71.92%\n",
      "Batch 117, Loss: 1.056194, Accuracy: 71.86%\n",
      "Batch 118, Loss: 0.977474, Accuracy: 71.90%\n",
      "Batch 119, Loss: 1.106317, Accuracy: 71.78%\n",
      "Batch 120, Loss: 1.092153, Accuracy: 71.72%\n",
      "Batch 121, Loss: 1.070064, Accuracy: 71.67%\n",
      "Batch 122, Loss: 1.118764, Accuracy: 71.57%\n",
      "Batch 123, Loss: 1.069296, Accuracy: 71.54%\n",
      "Batch 124, Loss: 1.033865, Accuracy: 71.55%\n",
      "Batch 125, Loss: 0.968779, Accuracy: 71.60%\n",
      "Batch 126, Loss: 1.075898, Accuracy: 71.58%\n",
      "Batch 127, Loss: 1.094735, Accuracy: 71.53%\n",
      "Batch 128, Loss: 0.944199, Accuracy: 71.61%\n",
      "Batch 129, Loss: 0.944879, Accuracy: 71.69%\n",
      "Batch 130, Loss: 1.013844, Accuracy: 71.69%\n",
      "Batch 131, Loss: 1.023501, Accuracy: 71.70%\n",
      "Batch 132, Loss: 0.982299, Accuracy: 71.73%\n",
      "Batch 133, Loss: 1.125296, Accuracy: 71.66%\n",
      "Batch 134, Loss: 1.125896, Accuracy: 71.57%\n",
      "Batch 135, Loss: 1.008159, Accuracy: 71.59%\n",
      "Batch 136, Loss: 1.016909, Accuracy: 71.59%\n",
      "Batch 137, Loss: 1.042969, Accuracy: 71.57%\n",
      "Batch 138, Loss: 1.028768, Accuracy: 71.57%\n",
      "Batch 139, Loss: 1.115049, Accuracy: 71.48%\n",
      "Batch 140, Loss: 0.952244, Accuracy: 71.54%\n",
      "Batch 141, Loss: 0.983934, Accuracy: 71.58%\n",
      "Batch 142, Loss: 1.014044, Accuracy: 71.58%\n",
      "Batch 143, Loss: 0.977892, Accuracy: 71.61%\n",
      "Batch 144, Loss: 0.975410, Accuracy: 71.66%\n",
      "Batch 145, Loss: 1.001464, Accuracy: 71.68%\n",
      "Batch 146, Loss: 1.128416, Accuracy: 71.59%\n",
      "Batch 147, Loss: 0.966475, Accuracy: 71.64%\n",
      "Batch 148, Loss: 1.137342, Accuracy: 71.58%\n",
      "Batch 149, Loss: 1.020647, Accuracy: 71.58%\n",
      "Batch 150, Loss: 1.077819, Accuracy: 71.57%\n",
      "Batch 151, Loss: 0.982372, Accuracy: 71.61%\n",
      "Batch 152, Loss: 0.993327, Accuracy: 71.62%\n",
      "Batch 153, Loss: 1.046704, Accuracy: 71.59%\n",
      "Batch 154, Loss: 0.987098, Accuracy: 71.62%\n",
      "Batch 155, Loss: 0.931832, Accuracy: 71.67%\n",
      "Batch 156, Loss: 1.047175, Accuracy: 71.65%\n",
      "Batch 157, Loss: 1.036786, Accuracy: 71.65%\n",
      "Batch 158, Loss: 0.962471, Accuracy: 71.68%\n",
      "Batch 159, Loss: 1.092511, Accuracy: 71.63%\n",
      "Batch 160, Loss: 1.046023, Accuracy: 71.61%\n",
      "Batch 161, Loss: 1.072921, Accuracy: 71.57%\n",
      "Batch 162, Loss: 1.061405, Accuracy: 71.54%\n",
      "Batch 163, Loss: 1.092383, Accuracy: 71.48%\n",
      "Batch 164, Loss: 0.969545, Accuracy: 71.52%\n",
      "Batch 165, Loss: 0.982941, Accuracy: 71.56%\n",
      "Batch 166, Loss: 1.072340, Accuracy: 71.55%\n",
      "Batch 167, Loss: 1.043678, Accuracy: 71.55%\n",
      "Batch 168, Loss: 0.916975, Accuracy: 71.62%\n",
      "Batch 169, Loss: 0.925043, Accuracy: 71.69%\n",
      "Batch 170, Loss: 0.935022, Accuracy: 71.75%\n",
      "Batch 171, Loss: 0.986635, Accuracy: 71.77%\n",
      "Batch 172, Loss: 1.034080, Accuracy: 71.77%\n",
      "Batch 173, Loss: 1.058161, Accuracy: 71.75%\n",
      "Batch 174, Loss: 1.053325, Accuracy: 71.72%\n",
      "Batch 175, Loss: 0.943979, Accuracy: 71.79%\n",
      "Batch 176, Loss: 0.975225, Accuracy: 71.81%\n",
      "Batch 177, Loss: 1.067795, Accuracy: 71.80%\n",
      "Batch 178, Loss: 1.029511, Accuracy: 71.78%\n",
      "Batch 179, Loss: 0.993428, Accuracy: 71.80%\n",
      "Batch 180, Loss: 1.102099, Accuracy: 71.75%\n",
      "Batch 181, Loss: 1.018205, Accuracy: 71.76%\n",
      "Batch 182, Loss: 1.019670, Accuracy: 71.76%\n",
      "Batch 183, Loss: 0.938551, Accuracy: 71.82%\n",
      "Batch 184, Loss: 1.029874, Accuracy: 71.82%\n",
      "Batch 185, Loss: 1.091793, Accuracy: 71.79%\n",
      "Batch 186, Loss: 0.995071, Accuracy: 71.80%\n",
      "Batch 187, Loss: 1.009027, Accuracy: 71.80%\n",
      "Batch 188, Loss: 1.003693, Accuracy: 71.81%\n",
      "Batch 189, Loss: 0.996312, Accuracy: 71.82%\n",
      "Batch 190, Loss: 1.027261, Accuracy: 71.82%\n",
      "Batch 191, Loss: 0.875146, Accuracy: 71.90%\n",
      "Batch 192, Loss: 1.095401, Accuracy: 71.87%\n",
      "Batch 193, Loss: 1.001553, Accuracy: 71.88%\n",
      "Batch 194, Loss: 0.984371, Accuracy: 71.91%\n",
      "Batch 195, Loss: 1.069996, Accuracy: 71.88%\n",
      "Batch 196, Loss: 1.070302, Accuracy: 71.86%\n",
      "Batch 197, Loss: 1.039211, Accuracy: 71.84%\n",
      "Batch 198, Loss: 0.957611, Accuracy: 71.88%\n",
      "Batch 199, Loss: 0.911674, Accuracy: 71.94%\n",
      "Batch 200, Loss: 0.943756, Accuracy: 71.98%\n",
      "Batch 201, Loss: 1.050205, Accuracy: 71.96%\n",
      "Batch 202, Loss: 1.030819, Accuracy: 71.95%\n",
      "Batch 203, Loss: 0.905613, Accuracy: 72.02%\n",
      "Batch 204, Loss: 1.059087, Accuracy: 71.98%\n",
      "Batch 205, Loss: 1.072133, Accuracy: 71.96%\n",
      "Batch 206, Loss: 0.996687, Accuracy: 71.97%\n",
      "Batch 207, Loss: 0.986342, Accuracy: 72.01%\n",
      "Batch 208, Loss: 0.983501, Accuracy: 72.04%\n",
      "Batch 209, Loss: 1.046946, Accuracy: 72.05%\n",
      "Batch 210, Loss: 1.039263, Accuracy: 72.04%\n",
      "Batch 211, Loss: 1.080685, Accuracy: 72.02%\n",
      "Batch 212, Loss: 1.091113, Accuracy: 71.99%\n",
      "Batch 213, Loss: 1.051359, Accuracy: 71.97%\n",
      "Training - Epoch 44, Loss: 1.021977, Accuracy: 71.97%\n",
      "Validation Batch 1, Loss: 1.032043, Accuracy: 68.75%\n",
      "Validation Batch 2, Loss: 1.141293, Accuracy: 63.28%\n",
      "Validation Batch 3, Loss: 1.145074, Accuracy: 62.50%\n",
      "Validation Batch 4, Loss: 1.039556, Accuracy: 64.45%\n",
      "Validation Batch 5, Loss: 1.093618, Accuracy: 63.75%\n",
      "Validation Batch 6, Loss: 1.042606, Accuracy: 64.84%\n",
      "Validation Batch 7, Loss: 1.083613, Accuracy: 65.18%\n",
      "Validation Batch 8, Loss: 1.106873, Accuracy: 64.65%\n",
      "Validation Batch 9, Loss: 1.129249, Accuracy: 64.24%\n",
      "Validation Batch 10, Loss: 1.088814, Accuracy: 64.38%\n",
      "Validation Batch 11, Loss: 1.066397, Accuracy: 64.77%\n",
      "Validation Batch 12, Loss: 1.027258, Accuracy: 65.36%\n",
      "Validation Batch 13, Loss: 1.152059, Accuracy: 65.02%\n",
      "Validation Batch 14, Loss: 1.098006, Accuracy: 65.07%\n",
      "Validation Batch 15, Loss: 1.088338, Accuracy: 65.10%\n",
      "Validation Batch 16, Loss: 1.060837, Accuracy: 65.14%\n",
      "Validation Batch 17, Loss: 1.170868, Accuracy: 64.43%\n",
      "Validation Batch 18, Loss: 1.049180, Accuracy: 64.84%\n",
      "Validation Batch 19, Loss: 1.139992, Accuracy: 64.47%\n",
      "Validation Batch 20, Loss: 1.081136, Accuracy: 64.61%\n",
      "Validation Batch 21, Loss: 1.088500, Accuracy: 64.51%\n",
      "Validation Batch 22, Loss: 1.137467, Accuracy: 64.28%\n",
      "Validation Batch 23, Loss: 1.186905, Accuracy: 63.86%\n",
      "Validation Batch 24, Loss: 1.118902, Accuracy: 63.87%\n",
      "Validation Batch 25, Loss: 1.077216, Accuracy: 63.75%\n",
      "Validation Batch 26, Loss: 1.065482, Accuracy: 63.94%\n",
      "Validation Batch 27, Loss: 1.095927, Accuracy: 63.95%\n",
      "Validation - Epoch 44, Loss: 1.096563, Accuracy: 63.95%\n",
      "Patienceâ€”1\n",
      "Epoch 45\n",
      "Batch 1, Loss: 1.058931, Accuracy: 65.62%\n",
      "Batch 2, Loss: 1.056122, Accuracy: 65.62%\n",
      "Batch 3, Loss: 0.932952, Accuracy: 70.83%\n",
      "Batch 4, Loss: 1.024849, Accuracy: 70.70%\n",
      "Batch 5, Loss: 0.991677, Accuracy: 71.88%\n",
      "Batch 6, Loss: 1.070384, Accuracy: 71.35%\n",
      "Batch 7, Loss: 1.050151, Accuracy: 70.98%\n",
      "Batch 8, Loss: 1.015898, Accuracy: 71.09%\n",
      "Batch 9, Loss: 1.003124, Accuracy: 71.53%\n",
      "Batch 10, Loss: 0.933486, Accuracy: 72.66%\n",
      "Batch 11, Loss: 1.036918, Accuracy: 72.30%\n",
      "Batch 12, Loss: 1.028273, Accuracy: 72.27%\n",
      "Batch 13, Loss: 1.049857, Accuracy: 72.12%\n",
      "Batch 14, Loss: 0.979597, Accuracy: 72.54%\n",
      "Batch 15, Loss: 0.949722, Accuracy: 72.92%\n",
      "Batch 16, Loss: 0.965680, Accuracy: 73.24%\n",
      "Batch 17, Loss: 1.106591, Accuracy: 72.61%\n",
      "Batch 18, Loss: 1.053668, Accuracy: 72.22%\n",
      "Batch 19, Loss: 1.071127, Accuracy: 71.96%\n",
      "Batch 20, Loss: 1.033486, Accuracy: 71.80%\n",
      "Batch 21, Loss: 0.946189, Accuracy: 72.17%\n",
      "Batch 22, Loss: 1.111997, Accuracy: 71.88%\n",
      "Batch 23, Loss: 0.973358, Accuracy: 72.15%\n",
      "Batch 24, Loss: 1.072434, Accuracy: 71.88%\n",
      "Batch 25, Loss: 1.070985, Accuracy: 71.56%\n",
      "Batch 26, Loss: 1.008052, Accuracy: 71.69%\n",
      "Batch 27, Loss: 0.938455, Accuracy: 71.99%\n",
      "Batch 28, Loss: 0.970558, Accuracy: 72.21%\n",
      "Batch 29, Loss: 1.046831, Accuracy: 72.04%\n",
      "Batch 30, Loss: 0.960036, Accuracy: 72.24%\n",
      "Batch 31, Loss: 0.994422, Accuracy: 72.38%\n",
      "Batch 32, Loss: 0.943024, Accuracy: 72.61%\n",
      "Batch 33, Loss: 1.083915, Accuracy: 72.44%\n",
      "Batch 34, Loss: 1.053771, Accuracy: 72.38%\n",
      "Batch 35, Loss: 0.949020, Accuracy: 72.63%\n",
      "Batch 36, Loss: 0.961207, Accuracy: 72.74%\n",
      "Batch 37, Loss: 1.004834, Accuracy: 72.76%\n",
      "Batch 38, Loss: 1.058899, Accuracy: 72.57%\n",
      "Batch 39, Loss: 1.006632, Accuracy: 72.56%\n",
      "Batch 40, Loss: 1.058441, Accuracy: 72.42%\n",
      "Batch 41, Loss: 0.983026, Accuracy: 72.48%\n",
      "Batch 42, Loss: 1.072005, Accuracy: 72.32%\n",
      "Batch 43, Loss: 1.083700, Accuracy: 72.17%\n",
      "Batch 44, Loss: 1.010062, Accuracy: 72.23%\n",
      "Batch 45, Loss: 1.048687, Accuracy: 72.15%\n",
      "Batch 46, Loss: 0.993651, Accuracy: 72.18%\n",
      "Batch 47, Loss: 0.989731, Accuracy: 72.27%\n",
      "Batch 48, Loss: 1.057154, Accuracy: 72.17%\n",
      "Batch 49, Loss: 1.041566, Accuracy: 72.13%\n",
      "Batch 50, Loss: 1.044104, Accuracy: 72.09%\n",
      "Batch 51, Loss: 0.996315, Accuracy: 72.15%\n",
      "Batch 52, Loss: 1.077795, Accuracy: 72.06%\n",
      "Batch 53, Loss: 0.993870, Accuracy: 72.11%\n",
      "Batch 54, Loss: 1.053315, Accuracy: 72.05%\n",
      "Batch 55, Loss: 0.985666, Accuracy: 72.10%\n",
      "Batch 56, Loss: 0.998582, Accuracy: 72.15%\n",
      "Batch 57, Loss: 1.061745, Accuracy: 72.09%\n",
      "Batch 58, Loss: 1.057156, Accuracy: 72.04%\n",
      "Batch 59, Loss: 0.992177, Accuracy: 72.09%\n",
      "Batch 60, Loss: 1.018906, Accuracy: 72.14%\n",
      "Batch 61, Loss: 1.012706, Accuracy: 72.13%\n",
      "Batch 62, Loss: 0.984104, Accuracy: 72.18%\n",
      "Batch 63, Loss: 0.996896, Accuracy: 72.22%\n",
      "Batch 64, Loss: 0.906905, Accuracy: 72.39%\n",
      "Batch 65, Loss: 1.000031, Accuracy: 72.43%\n",
      "Batch 66, Loss: 0.968986, Accuracy: 72.51%\n",
      "Batch 67, Loss: 1.005104, Accuracy: 72.53%\n",
      "Batch 68, Loss: 1.092442, Accuracy: 72.38%\n",
      "Batch 69, Loss: 0.974740, Accuracy: 72.46%\n",
      "Batch 70, Loss: 0.989134, Accuracy: 72.52%\n",
      "Batch 71, Loss: 1.000173, Accuracy: 72.54%\n",
      "Batch 72, Loss: 1.097366, Accuracy: 72.40%\n",
      "Batch 73, Loss: 1.111701, Accuracy: 72.28%\n",
      "Batch 74, Loss: 1.109074, Accuracy: 72.17%\n",
      "Batch 75, Loss: 1.071709, Accuracy: 72.06%\n",
      "Batch 76, Loss: 1.031335, Accuracy: 72.06%\n",
      "Batch 77, Loss: 1.115318, Accuracy: 71.92%\n",
      "Batch 78, Loss: 1.102367, Accuracy: 71.79%\n",
      "Batch 79, Loss: 0.976908, Accuracy: 71.86%\n",
      "Batch 80, Loss: 1.047850, Accuracy: 71.84%\n",
      "Batch 81, Loss: 1.017481, Accuracy: 71.84%\n",
      "Batch 82, Loss: 0.922314, Accuracy: 71.99%\n",
      "Batch 83, Loss: 1.007835, Accuracy: 72.01%\n",
      "Batch 84, Loss: 1.032915, Accuracy: 72.01%\n",
      "Batch 85, Loss: 1.023016, Accuracy: 72.02%\n",
      "Batch 86, Loss: 1.018926, Accuracy: 72.04%\n",
      "Batch 87, Loss: 1.010514, Accuracy: 72.05%\n",
      "Batch 88, Loss: 1.071721, Accuracy: 71.98%\n",
      "Batch 89, Loss: 1.089181, Accuracy: 71.89%\n",
      "Batch 90, Loss: 1.050929, Accuracy: 71.86%\n",
      "Batch 91, Loss: 1.033240, Accuracy: 71.82%\n",
      "Batch 92, Loss: 0.992418, Accuracy: 71.86%\n",
      "Batch 93, Loss: 0.969210, Accuracy: 71.93%\n",
      "Batch 94, Loss: 1.078312, Accuracy: 71.86%\n",
      "Batch 95, Loss: 1.022118, Accuracy: 71.86%\n",
      "Batch 96, Loss: 1.009952, Accuracy: 71.88%\n",
      "Batch 97, Loss: 1.076969, Accuracy: 71.81%\n",
      "Batch 98, Loss: 1.012798, Accuracy: 71.81%\n",
      "Batch 99, Loss: 1.054843, Accuracy: 71.76%\n",
      "Batch 100, Loss: 1.036704, Accuracy: 71.75%\n",
      "Batch 101, Loss: 0.935029, Accuracy: 71.84%\n",
      "Batch 102, Loss: 0.964830, Accuracy: 71.89%\n",
      "Batch 103, Loss: 1.000997, Accuracy: 71.91%\n",
      "Batch 104, Loss: 1.039126, Accuracy: 71.88%\n",
      "Batch 105, Loss: 1.039128, Accuracy: 71.88%\n",
      "Batch 106, Loss: 1.013969, Accuracy: 71.89%\n",
      "Batch 107, Loss: 1.035225, Accuracy: 71.88%\n",
      "Batch 108, Loss: 1.012897, Accuracy: 71.89%\n",
      "Batch 109, Loss: 1.011275, Accuracy: 71.90%\n",
      "Batch 110, Loss: 0.978334, Accuracy: 71.95%\n",
      "Batch 111, Loss: 1.124471, Accuracy: 71.88%\n",
      "Batch 112, Loss: 0.949428, Accuracy: 71.94%\n",
      "Batch 113, Loss: 1.012505, Accuracy: 71.94%\n",
      "Batch 114, Loss: 1.123865, Accuracy: 71.83%\n",
      "Batch 115, Loss: 1.032841, Accuracy: 71.82%\n",
      "Batch 116, Loss: 1.026193, Accuracy: 71.82%\n",
      "Batch 117, Loss: 0.973313, Accuracy: 71.85%\n",
      "Batch 118, Loss: 1.035425, Accuracy: 71.85%\n",
      "Batch 119, Loss: 1.051410, Accuracy: 71.82%\n",
      "Batch 120, Loss: 1.000932, Accuracy: 71.84%\n",
      "Batch 121, Loss: 1.027760, Accuracy: 71.85%\n",
      "Batch 122, Loss: 0.996095, Accuracy: 71.86%\n",
      "Batch 123, Loss: 1.017575, Accuracy: 71.88%\n",
      "Batch 124, Loss: 1.082864, Accuracy: 71.84%\n",
      "Batch 125, Loss: 1.091982, Accuracy: 71.78%\n",
      "Batch 126, Loss: 1.071230, Accuracy: 71.75%\n",
      "Batch 127, Loss: 1.035108, Accuracy: 71.74%\n",
      "Batch 128, Loss: 1.031533, Accuracy: 71.74%\n",
      "Batch 129, Loss: 1.094435, Accuracy: 71.66%\n",
      "Batch 130, Loss: 0.966185, Accuracy: 71.72%\n",
      "Batch 131, Loss: 0.987377, Accuracy: 71.74%\n",
      "Batch 132, Loss: 0.966901, Accuracy: 71.78%\n",
      "Batch 133, Loss: 0.974064, Accuracy: 71.83%\n",
      "Batch 134, Loss: 0.999229, Accuracy: 71.83%\n",
      "Batch 135, Loss: 1.057892, Accuracy: 71.79%\n",
      "Batch 136, Loss: 0.992070, Accuracy: 71.81%\n",
      "Batch 137, Loss: 1.036366, Accuracy: 71.78%\n",
      "Batch 138, Loss: 1.032789, Accuracy: 71.77%\n",
      "Batch 139, Loss: 1.146610, Accuracy: 71.66%\n",
      "Batch 140, Loss: 1.071990, Accuracy: 71.64%\n",
      "Batch 141, Loss: 1.035660, Accuracy: 71.63%\n",
      "Batch 142, Loss: 1.060884, Accuracy: 71.60%\n",
      "Batch 143, Loss: 1.024947, Accuracy: 71.60%\n",
      "Batch 144, Loss: 0.983451, Accuracy: 71.63%\n",
      "Batch 145, Loss: 0.940040, Accuracy: 71.70%\n",
      "Batch 146, Loss: 0.988022, Accuracy: 71.74%\n",
      "Batch 147, Loss: 0.999612, Accuracy: 71.75%\n",
      "Batch 148, Loss: 1.112847, Accuracy: 71.68%\n",
      "Batch 149, Loss: 0.985944, Accuracy: 71.71%\n",
      "Batch 150, Loss: 0.931551, Accuracy: 71.78%\n",
      "Batch 151, Loss: 0.977435, Accuracy: 71.82%\n",
      "Batch 152, Loss: 1.024897, Accuracy: 71.82%\n",
      "Batch 153, Loss: 1.090626, Accuracy: 71.78%\n",
      "Batch 154, Loss: 1.029725, Accuracy: 71.77%\n",
      "Batch 155, Loss: 1.058076, Accuracy: 71.76%\n",
      "Batch 156, Loss: 0.941509, Accuracy: 71.82%\n",
      "Batch 157, Loss: 1.042290, Accuracy: 71.82%\n",
      "Batch 158, Loss: 1.020786, Accuracy: 71.81%\n",
      "Batch 159, Loss: 1.010904, Accuracy: 71.83%\n",
      "Batch 160, Loss: 1.057050, Accuracy: 71.81%\n",
      "Batch 161, Loss: 1.064134, Accuracy: 71.78%\n",
      "Batch 162, Loss: 0.986455, Accuracy: 71.80%\n",
      "Batch 163, Loss: 1.057127, Accuracy: 71.77%\n",
      "Batch 164, Loss: 1.068188, Accuracy: 71.75%\n",
      "Batch 165, Loss: 1.137459, Accuracy: 71.67%\n",
      "Batch 166, Loss: 1.013260, Accuracy: 71.67%\n",
      "Batch 167, Loss: 0.924109, Accuracy: 71.73%\n",
      "Batch 168, Loss: 0.984284, Accuracy: 71.76%\n",
      "Batch 169, Loss: 0.985692, Accuracy: 71.79%\n",
      "Batch 170, Loss: 0.993526, Accuracy: 71.81%\n",
      "Batch 171, Loss: 1.010172, Accuracy: 71.83%\n",
      "Batch 172, Loss: 1.037118, Accuracy: 71.83%\n",
      "Batch 173, Loss: 0.966915, Accuracy: 71.86%\n",
      "Batch 174, Loss: 1.057019, Accuracy: 71.83%\n",
      "Batch 175, Loss: 0.968046, Accuracy: 71.87%\n",
      "Batch 176, Loss: 1.066329, Accuracy: 71.82%\n",
      "Batch 177, Loss: 1.067798, Accuracy: 71.80%\n",
      "Batch 178, Loss: 0.973756, Accuracy: 71.83%\n",
      "Batch 179, Loss: 1.058261, Accuracy: 71.81%\n",
      "Batch 180, Loss: 1.049502, Accuracy: 71.79%\n",
      "Batch 181, Loss: 1.049484, Accuracy: 71.78%\n",
      "Batch 182, Loss: 0.915042, Accuracy: 71.85%\n",
      "Batch 183, Loss: 1.068094, Accuracy: 71.82%\n",
      "Batch 184, Loss: 0.996755, Accuracy: 71.83%\n",
      "Batch 185, Loss: 1.037415, Accuracy: 71.83%\n",
      "Batch 186, Loss: 0.957614, Accuracy: 71.87%\n",
      "Batch 187, Loss: 1.052264, Accuracy: 71.84%\n",
      "Batch 188, Loss: 1.052266, Accuracy: 71.83%\n",
      "Batch 189, Loss: 0.981102, Accuracy: 71.85%\n",
      "Batch 190, Loss: 0.998477, Accuracy: 71.86%\n",
      "Batch 191, Loss: 1.002633, Accuracy: 71.87%\n",
      "Batch 192, Loss: 1.011705, Accuracy: 71.88%\n",
      "Batch 193, Loss: 1.014403, Accuracy: 71.90%\n",
      "Batch 194, Loss: 0.987951, Accuracy: 71.91%\n",
      "Batch 195, Loss: 1.039139, Accuracy: 71.90%\n",
      "Batch 196, Loss: 1.040153, Accuracy: 71.89%\n",
      "Batch 197, Loss: 1.051361, Accuracy: 71.88%\n",
      "Batch 198, Loss: 1.055300, Accuracy: 71.85%\n",
      "Batch 199, Loss: 0.943581, Accuracy: 71.90%\n",
      "Batch 200, Loss: 0.999934, Accuracy: 71.91%\n",
      "Batch 201, Loss: 1.019450, Accuracy: 71.92%\n",
      "Batch 202, Loss: 1.051259, Accuracy: 71.91%\n",
      "Batch 203, Loss: 1.012640, Accuracy: 71.91%\n",
      "Batch 204, Loss: 1.071597, Accuracy: 71.88%\n",
      "Batch 205, Loss: 1.026619, Accuracy: 71.88%\n",
      "Batch 206, Loss: 0.919619, Accuracy: 71.94%\n",
      "Batch 207, Loss: 1.010551, Accuracy: 71.94%\n",
      "Batch 208, Loss: 1.006769, Accuracy: 71.95%\n",
      "Batch 209, Loss: 1.024273, Accuracy: 71.96%\n",
      "Batch 210, Loss: 0.921719, Accuracy: 72.01%\n",
      "Batch 211, Loss: 0.977832, Accuracy: 72.02%\n",
      "Batch 212, Loss: 1.020753, Accuracy: 72.04%\n",
      "Batch 213, Loss: 0.995798, Accuracy: 72.05%\n",
      "Training - Epoch 45, Loss: 1.020898, Accuracy: 72.05%\n",
      "Validation Batch 1, Loss: 1.033586, Accuracy: 70.31%\n",
      "Validation Batch 2, Loss: 1.143920, Accuracy: 64.06%\n",
      "Validation Batch 3, Loss: 1.151260, Accuracy: 62.50%\n",
      "Validation Batch 4, Loss: 1.045215, Accuracy: 64.06%\n",
      "Validation Batch 5, Loss: 1.091997, Accuracy: 63.12%\n",
      "Validation Batch 6, Loss: 1.041150, Accuracy: 64.58%\n",
      "Validation Batch 7, Loss: 1.085557, Accuracy: 64.96%\n",
      "Validation Batch 8, Loss: 1.108206, Accuracy: 64.45%\n",
      "Validation Batch 9, Loss: 1.129571, Accuracy: 64.41%\n",
      "Validation Batch 10, Loss: 1.097337, Accuracy: 64.22%\n",
      "Validation Batch 11, Loss: 1.071696, Accuracy: 64.35%\n",
      "Validation Batch 12, Loss: 1.035241, Accuracy: 64.97%\n",
      "Validation Batch 13, Loss: 1.156851, Accuracy: 64.54%\n",
      "Validation Batch 14, Loss: 1.102069, Accuracy: 64.51%\n",
      "Validation Batch 15, Loss: 1.090986, Accuracy: 64.58%\n",
      "Validation Batch 16, Loss: 1.058715, Accuracy: 64.65%\n",
      "Validation Batch 17, Loss: 1.173353, Accuracy: 64.06%\n",
      "Validation Batch 18, Loss: 1.048993, Accuracy: 64.50%\n",
      "Validation Batch 19, Loss: 1.139917, Accuracy: 64.23%\n",
      "Validation Batch 20, Loss: 1.086612, Accuracy: 64.38%\n",
      "Validation Batch 21, Loss: 1.094042, Accuracy: 64.29%\n",
      "Validation Batch 22, Loss: 1.134406, Accuracy: 64.13%\n",
      "Validation Batch 23, Loss: 1.189651, Accuracy: 63.65%\n",
      "Validation Batch 24, Loss: 1.121378, Accuracy: 63.67%\n",
      "Validation Batch 25, Loss: 1.078346, Accuracy: 63.56%\n",
      "Validation Batch 26, Loss: 1.072654, Accuracy: 63.70%\n",
      "Validation Batch 27, Loss: 1.104529, Accuracy: 63.71%\n",
      "Validation - Epoch 45, Loss: 1.099527, Accuracy: 63.71%\n",
      "Patienceâ€”2\n",
      "Epoch 46\n",
      "Batch 1, Loss: 1.009434, Accuracy: 71.88%\n",
      "Batch 2, Loss: 0.993244, Accuracy: 74.22%\n",
      "Batch 3, Loss: 0.921619, Accuracy: 77.08%\n",
      "Batch 4, Loss: 0.941839, Accuracy: 77.73%\n",
      "Batch 5, Loss: 0.983840, Accuracy: 77.19%\n",
      "Batch 6, Loss: 1.089457, Accuracy: 75.00%\n",
      "Batch 7, Loss: 0.972729, Accuracy: 75.45%\n",
      "Batch 8, Loss: 1.016804, Accuracy: 75.20%\n",
      "Batch 9, Loss: 0.951770, Accuracy: 75.69%\n",
      "Batch 10, Loss: 1.071156, Accuracy: 75.00%\n",
      "Batch 11, Loss: 1.081830, Accuracy: 74.29%\n",
      "Batch 12, Loss: 0.981712, Accuracy: 74.22%\n",
      "Batch 13, Loss: 0.947467, Accuracy: 74.64%\n",
      "Batch 14, Loss: 1.052314, Accuracy: 74.22%\n",
      "Batch 15, Loss: 1.006983, Accuracy: 74.17%\n",
      "Batch 16, Loss: 1.042309, Accuracy: 73.83%\n",
      "Batch 17, Loss: 1.035908, Accuracy: 73.62%\n",
      "Batch 18, Loss: 1.112206, Accuracy: 72.92%\n",
      "Batch 19, Loss: 1.038954, Accuracy: 72.94%\n",
      "Batch 20, Loss: 0.944089, Accuracy: 73.28%\n",
      "Batch 21, Loss: 0.981308, Accuracy: 73.44%\n",
      "Batch 22, Loss: 1.068968, Accuracy: 73.15%\n",
      "Batch 23, Loss: 1.030194, Accuracy: 73.03%\n",
      "Batch 24, Loss: 0.992362, Accuracy: 73.11%\n",
      "Batch 25, Loss: 0.974725, Accuracy: 73.19%\n",
      "Batch 26, Loss: 0.950403, Accuracy: 73.50%\n",
      "Batch 27, Loss: 0.977460, Accuracy: 73.61%\n",
      "Batch 28, Loss: 1.028571, Accuracy: 73.55%\n",
      "Batch 29, Loss: 0.995781, Accuracy: 73.60%\n",
      "Batch 30, Loss: 0.911259, Accuracy: 73.91%\n",
      "Batch 31, Loss: 1.034445, Accuracy: 73.79%\n",
      "Batch 32, Loss: 0.997986, Accuracy: 73.78%\n",
      "Batch 33, Loss: 0.999189, Accuracy: 73.82%\n",
      "Batch 34, Loss: 0.960826, Accuracy: 73.99%\n",
      "Batch 35, Loss: 1.153729, Accuracy: 73.57%\n",
      "Batch 36, Loss: 1.025653, Accuracy: 73.57%\n",
      "Batch 37, Loss: 1.017161, Accuracy: 73.52%\n",
      "Batch 38, Loss: 1.123318, Accuracy: 73.23%\n",
      "Batch 39, Loss: 1.046752, Accuracy: 73.12%\n",
      "Batch 40, Loss: 0.937890, Accuracy: 73.28%\n",
      "Batch 41, Loss: 0.979227, Accuracy: 73.40%\n",
      "Batch 42, Loss: 0.976501, Accuracy: 73.51%\n",
      "Batch 43, Loss: 1.022399, Accuracy: 73.44%\n",
      "Batch 44, Loss: 0.936466, Accuracy: 73.58%\n",
      "Batch 45, Loss: 0.986098, Accuracy: 73.65%\n",
      "Batch 46, Loss: 1.029523, Accuracy: 73.54%\n",
      "Batch 47, Loss: 1.079594, Accuracy: 73.37%\n",
      "Batch 48, Loss: 0.965888, Accuracy: 73.50%\n",
      "Batch 49, Loss: 1.082047, Accuracy: 73.34%\n",
      "Batch 50, Loss: 0.999162, Accuracy: 73.34%\n",
      "Batch 51, Loss: 1.076394, Accuracy: 73.13%\n",
      "Batch 52, Loss: 1.061417, Accuracy: 73.02%\n",
      "Batch 53, Loss: 1.023840, Accuracy: 73.00%\n",
      "Batch 54, Loss: 1.001484, Accuracy: 73.00%\n",
      "Batch 55, Loss: 0.986477, Accuracy: 73.07%\n",
      "Batch 56, Loss: 0.946139, Accuracy: 73.13%\n",
      "Batch 57, Loss: 1.004287, Accuracy: 73.16%\n",
      "Batch 58, Loss: 0.971228, Accuracy: 73.25%\n",
      "Batch 59, Loss: 0.949812, Accuracy: 73.36%\n",
      "Batch 60, Loss: 0.977525, Accuracy: 73.41%\n",
      "Batch 61, Loss: 1.032755, Accuracy: 73.34%\n",
      "Batch 62, Loss: 0.984710, Accuracy: 73.39%\n",
      "Batch 63, Loss: 1.063625, Accuracy: 73.31%\n",
      "Batch 64, Loss: 1.070791, Accuracy: 73.22%\n",
      "Batch 65, Loss: 0.999818, Accuracy: 73.22%\n",
      "Batch 66, Loss: 1.078492, Accuracy: 73.11%\n",
      "Batch 67, Loss: 0.965681, Accuracy: 73.16%\n",
      "Batch 68, Loss: 0.950476, Accuracy: 73.23%\n",
      "Batch 69, Loss: 1.066737, Accuracy: 73.19%\n",
      "Batch 70, Loss: 1.032892, Accuracy: 73.17%\n",
      "Batch 71, Loss: 1.005334, Accuracy: 73.20%\n",
      "Batch 72, Loss: 1.117312, Accuracy: 73.03%\n",
      "Batch 73, Loss: 1.016745, Accuracy: 73.03%\n",
      "Batch 74, Loss: 0.987815, Accuracy: 73.06%\n",
      "Batch 75, Loss: 0.947025, Accuracy: 73.15%\n",
      "Batch 76, Loss: 1.090827, Accuracy: 73.03%\n",
      "Batch 77, Loss: 1.062344, Accuracy: 72.93%\n",
      "Batch 78, Loss: 1.061583, Accuracy: 72.90%\n",
      "Batch 79, Loss: 1.076242, Accuracy: 72.82%\n",
      "Batch 80, Loss: 1.000197, Accuracy: 72.83%\n",
      "Batch 81, Loss: 1.059944, Accuracy: 72.76%\n",
      "Batch 82, Loss: 1.036049, Accuracy: 72.75%\n",
      "Batch 83, Loss: 0.941556, Accuracy: 72.85%\n",
      "Batch 84, Loss: 1.093047, Accuracy: 72.73%\n",
      "Batch 85, Loss: 0.970260, Accuracy: 72.79%\n",
      "Batch 86, Loss: 1.043729, Accuracy: 72.78%\n",
      "Batch 87, Loss: 1.084069, Accuracy: 72.68%\n",
      "Batch 88, Loss: 1.042682, Accuracy: 72.67%\n",
      "Batch 89, Loss: 1.088720, Accuracy: 72.59%\n",
      "Batch 90, Loss: 0.997268, Accuracy: 72.64%\n",
      "Batch 91, Loss: 0.926731, Accuracy: 72.75%\n",
      "Batch 92, Loss: 1.076924, Accuracy: 72.67%\n",
      "Batch 93, Loss: 0.964660, Accuracy: 72.72%\n",
      "Batch 94, Loss: 1.105083, Accuracy: 72.62%\n",
      "Batch 95, Loss: 1.030114, Accuracy: 72.62%\n",
      "Batch 96, Loss: 1.012863, Accuracy: 72.62%\n",
      "Batch 97, Loss: 1.023502, Accuracy: 72.60%\n",
      "Batch 98, Loss: 0.967149, Accuracy: 72.64%\n",
      "Batch 99, Loss: 1.033040, Accuracy: 72.62%\n",
      "Batch 100, Loss: 1.110924, Accuracy: 72.53%\n",
      "Batch 101, Loss: 1.042675, Accuracy: 72.52%\n",
      "Batch 102, Loss: 1.056763, Accuracy: 72.47%\n",
      "Batch 103, Loss: 0.926486, Accuracy: 72.56%\n",
      "Batch 104, Loss: 1.125075, Accuracy: 72.46%\n",
      "Batch 105, Loss: 0.980352, Accuracy: 72.50%\n",
      "Batch 106, Loss: 1.070801, Accuracy: 72.41%\n",
      "Batch 107, Loss: 0.954645, Accuracy: 72.49%\n",
      "Batch 108, Loss: 1.132199, Accuracy: 72.35%\n",
      "Batch 109, Loss: 0.991348, Accuracy: 72.39%\n",
      "Batch 110, Loss: 1.036525, Accuracy: 72.39%\n",
      "Batch 111, Loss: 1.017092, Accuracy: 72.38%\n",
      "Batch 112, Loss: 0.984696, Accuracy: 72.41%\n",
      "Batch 113, Loss: 1.034768, Accuracy: 72.36%\n",
      "Batch 114, Loss: 1.008854, Accuracy: 72.37%\n",
      "Batch 115, Loss: 0.987199, Accuracy: 72.39%\n",
      "Batch 116, Loss: 1.041597, Accuracy: 72.36%\n",
      "Batch 117, Loss: 0.969259, Accuracy: 72.40%\n",
      "Batch 118, Loss: 0.946241, Accuracy: 72.46%\n",
      "Batch 119, Loss: 0.972325, Accuracy: 72.48%\n",
      "Batch 120, Loss: 1.041537, Accuracy: 72.46%\n",
      "Batch 121, Loss: 1.012801, Accuracy: 72.47%\n",
      "Batch 122, Loss: 0.934261, Accuracy: 72.54%\n",
      "Batch 123, Loss: 1.080578, Accuracy: 72.47%\n",
      "Batch 124, Loss: 0.941292, Accuracy: 72.52%\n",
      "Batch 125, Loss: 0.921505, Accuracy: 72.60%\n",
      "Batch 126, Loss: 1.027591, Accuracy: 72.61%\n",
      "Batch 127, Loss: 1.055113, Accuracy: 72.60%\n",
      "Batch 128, Loss: 1.111338, Accuracy: 72.51%\n",
      "Batch 129, Loss: 1.013484, Accuracy: 72.50%\n",
      "Batch 130, Loss: 0.895937, Accuracy: 72.61%\n",
      "Batch 131, Loss: 0.991756, Accuracy: 72.64%\n",
      "Batch 132, Loss: 1.063447, Accuracy: 72.61%\n",
      "Batch 133, Loss: 1.035666, Accuracy: 72.58%\n",
      "Batch 134, Loss: 1.054049, Accuracy: 72.55%\n",
      "Batch 135, Loss: 0.976004, Accuracy: 72.57%\n",
      "Batch 136, Loss: 0.953811, Accuracy: 72.61%\n",
      "Batch 137, Loss: 0.925343, Accuracy: 72.67%\n",
      "Batch 138, Loss: 0.998899, Accuracy: 72.70%\n",
      "Batch 139, Loss: 0.938768, Accuracy: 72.77%\n",
      "Batch 140, Loss: 1.075609, Accuracy: 72.71%\n",
      "Batch 141, Loss: 1.047543, Accuracy: 72.70%\n",
      "Batch 142, Loss: 1.080785, Accuracy: 72.63%\n",
      "Batch 143, Loss: 0.956744, Accuracy: 72.68%\n",
      "Batch 144, Loss: 0.998769, Accuracy: 72.69%\n",
      "Batch 145, Loss: 1.016537, Accuracy: 72.68%\n",
      "Batch 146, Loss: 1.059998, Accuracy: 72.67%\n",
      "Batch 147, Loss: 1.014935, Accuracy: 72.65%\n",
      "Batch 148, Loss: 1.067386, Accuracy: 72.60%\n",
      "Batch 149, Loss: 0.959156, Accuracy: 72.64%\n",
      "Batch 150, Loss: 0.987671, Accuracy: 72.66%\n",
      "Batch 151, Loss: 1.097790, Accuracy: 72.59%\n",
      "Batch 152, Loss: 1.092754, Accuracy: 72.53%\n",
      "Batch 153, Loss: 1.033594, Accuracy: 72.52%\n",
      "Batch 154, Loss: 1.027255, Accuracy: 72.52%\n",
      "Batch 155, Loss: 1.072694, Accuracy: 72.47%\n",
      "Batch 156, Loss: 0.988859, Accuracy: 72.50%\n",
      "Batch 157, Loss: 0.982376, Accuracy: 72.52%\n",
      "Batch 158, Loss: 0.921020, Accuracy: 72.59%\n",
      "Batch 159, Loss: 0.955377, Accuracy: 72.62%\n",
      "Batch 160, Loss: 1.056511, Accuracy: 72.61%\n",
      "Batch 161, Loss: 1.097572, Accuracy: 72.54%\n",
      "Batch 162, Loss: 1.058774, Accuracy: 72.51%\n",
      "Batch 163, Loss: 0.995934, Accuracy: 72.53%\n",
      "Batch 164, Loss: 1.079318, Accuracy: 72.48%\n",
      "Batch 165, Loss: 1.034682, Accuracy: 72.48%\n",
      "Batch 166, Loss: 1.093898, Accuracy: 72.43%\n",
      "Batch 167, Loss: 1.025928, Accuracy: 72.42%\n",
      "Batch 168, Loss: 0.999424, Accuracy: 72.43%\n",
      "Batch 169, Loss: 1.012298, Accuracy: 72.45%\n",
      "Batch 170, Loss: 1.046636, Accuracy: 72.44%\n",
      "Batch 171, Loss: 1.107291, Accuracy: 72.39%\n",
      "Batch 172, Loss: 0.961684, Accuracy: 72.42%\n",
      "Batch 173, Loss: 0.997536, Accuracy: 72.43%\n",
      "Batch 174, Loss: 0.948687, Accuracy: 72.49%\n",
      "Batch 175, Loss: 1.039768, Accuracy: 72.48%\n",
      "Batch 176, Loss: 1.018944, Accuracy: 72.49%\n",
      "Batch 177, Loss: 0.976645, Accuracy: 72.52%\n",
      "Batch 178, Loss: 0.974925, Accuracy: 72.56%\n",
      "Batch 179, Loss: 1.072606, Accuracy: 72.53%\n",
      "Batch 180, Loss: 1.002428, Accuracy: 72.54%\n",
      "Batch 181, Loss: 0.999712, Accuracy: 72.56%\n",
      "Batch 182, Loss: 1.093436, Accuracy: 72.50%\n",
      "Batch 183, Loss: 1.096440, Accuracy: 72.46%\n",
      "Batch 184, Loss: 0.994131, Accuracy: 72.48%\n",
      "Batch 185, Loss: 1.053531, Accuracy: 72.46%\n",
      "Batch 186, Loss: 0.935230, Accuracy: 72.50%\n",
      "Batch 187, Loss: 1.108791, Accuracy: 72.44%\n",
      "Batch 188, Loss: 1.041984, Accuracy: 72.44%\n",
      "Batch 189, Loss: 1.055042, Accuracy: 72.42%\n",
      "Batch 190, Loss: 1.017106, Accuracy: 72.43%\n",
      "Batch 191, Loss: 1.040370, Accuracy: 72.41%\n",
      "Batch 192, Loss: 0.991847, Accuracy: 72.43%\n",
      "Batch 193, Loss: 1.068210, Accuracy: 72.42%\n",
      "Batch 194, Loss: 0.954423, Accuracy: 72.46%\n",
      "Batch 195, Loss: 0.979950, Accuracy: 72.48%\n",
      "Batch 196, Loss: 1.008236, Accuracy: 72.48%\n",
      "Batch 197, Loss: 1.023457, Accuracy: 72.48%\n",
      "Batch 198, Loss: 1.110680, Accuracy: 72.44%\n",
      "Batch 199, Loss: 0.983280, Accuracy: 72.46%\n",
      "Batch 200, Loss: 0.991995, Accuracy: 72.48%\n",
      "Batch 201, Loss: 1.046646, Accuracy: 72.45%\n",
      "Batch 202, Loss: 0.945912, Accuracy: 72.49%\n",
      "Batch 203, Loss: 1.026933, Accuracy: 72.49%\n",
      "Batch 204, Loss: 1.088074, Accuracy: 72.45%\n",
      "Batch 205, Loss: 0.936172, Accuracy: 72.50%\n",
      "Batch 206, Loss: 0.986200, Accuracy: 72.52%\n",
      "Batch 207, Loss: 1.142547, Accuracy: 72.47%\n",
      "Batch 208, Loss: 0.986753, Accuracy: 72.48%\n",
      "Batch 209, Loss: 1.070387, Accuracy: 72.46%\n",
      "Batch 210, Loss: 1.115407, Accuracy: 72.40%\n",
      "Batch 211, Loss: 1.076670, Accuracy: 72.36%\n",
      "Batch 212, Loss: 1.172610, Accuracy: 72.28%\n",
      "Batch 213, Loss: 1.032260, Accuracy: 72.26%\n",
      "Training - Epoch 46, Loss: 1.019454, Accuracy: 72.26%\n",
      "Validation Batch 1, Loss: 1.062510, Accuracy: 65.62%\n",
      "Validation Batch 2, Loss: 1.181218, Accuracy: 58.59%\n",
      "Validation Batch 3, Loss: 1.177870, Accuracy: 57.81%\n",
      "Validation Batch 4, Loss: 1.066756, Accuracy: 60.55%\n",
      "Validation Batch 5, Loss: 1.127011, Accuracy: 60.00%\n",
      "Validation Batch 6, Loss: 1.074019, Accuracy: 61.46%\n",
      "Validation Batch 7, Loss: 1.108459, Accuracy: 62.05%\n",
      "Validation Batch 8, Loss: 1.119409, Accuracy: 62.11%\n",
      "Validation Batch 9, Loss: 1.152264, Accuracy: 61.46%\n",
      "Validation Batch 10, Loss: 1.133281, Accuracy: 61.41%\n",
      "Validation Batch 11, Loss: 1.102330, Accuracy: 61.51%\n",
      "Validation Batch 12, Loss: 1.062474, Accuracy: 61.98%\n",
      "Validation Batch 13, Loss: 1.177983, Accuracy: 61.66%\n",
      "Validation Batch 14, Loss: 1.130216, Accuracy: 61.50%\n",
      "Validation Batch 15, Loss: 1.115536, Accuracy: 61.46%\n",
      "Validation Batch 16, Loss: 1.093340, Accuracy: 61.72%\n",
      "Validation Batch 17, Loss: 1.211887, Accuracy: 60.94%\n",
      "Validation Batch 18, Loss: 1.080691, Accuracy: 61.11%\n",
      "Validation Batch 19, Loss: 1.177344, Accuracy: 60.69%\n",
      "Validation Batch 20, Loss: 1.116306, Accuracy: 60.78%\n",
      "Validation Batch 21, Loss: 1.116860, Accuracy: 60.79%\n",
      "Validation Batch 22, Loss: 1.174784, Accuracy: 60.44%\n",
      "Validation Batch 23, Loss: 1.215490, Accuracy: 59.92%\n",
      "Validation Batch 24, Loss: 1.142408, Accuracy: 59.96%\n",
      "Validation Batch 25, Loss: 1.110840, Accuracy: 59.94%\n",
      "Validation Batch 26, Loss: 1.085139, Accuracy: 60.10%\n",
      "Validation Batch 27, Loss: 1.141369, Accuracy: 59.95%\n",
      "Validation - Epoch 46, Loss: 1.128066, Accuracy: 59.95%\n",
      "Patienceâ€”3\n",
      "Epoch 47\n",
      "Batch 1, Loss: 1.047765, Accuracy: 67.19%\n",
      "Batch 2, Loss: 1.034659, Accuracy: 68.75%\n",
      "Batch 3, Loss: 0.944958, Accuracy: 72.40%\n",
      "Batch 4, Loss: 1.050104, Accuracy: 71.88%\n",
      "Batch 5, Loss: 0.998797, Accuracy: 72.50%\n",
      "Batch 6, Loss: 1.043833, Accuracy: 71.88%\n",
      "Batch 7, Loss: 1.039365, Accuracy: 71.88%\n",
      "Batch 8, Loss: 1.020239, Accuracy: 72.07%\n",
      "Batch 9, Loss: 1.100741, Accuracy: 71.18%\n",
      "Batch 10, Loss: 0.955143, Accuracy: 71.88%\n",
      "Batch 11, Loss: 0.914351, Accuracy: 72.87%\n",
      "Batch 12, Loss: 0.999067, Accuracy: 72.79%\n",
      "Batch 13, Loss: 0.982711, Accuracy: 73.20%\n",
      "Batch 14, Loss: 0.976691, Accuracy: 73.33%\n",
      "Batch 15, Loss: 1.021358, Accuracy: 73.23%\n",
      "Batch 16, Loss: 1.013273, Accuracy: 73.24%\n",
      "Batch 17, Loss: 1.005990, Accuracy: 73.25%\n",
      "Batch 18, Loss: 0.967880, Accuracy: 73.52%\n",
      "Batch 19, Loss: 0.990850, Accuracy: 73.60%\n",
      "Batch 20, Loss: 1.003559, Accuracy: 73.59%\n",
      "Batch 21, Loss: 1.043752, Accuracy: 73.44%\n",
      "Batch 22, Loss: 0.977873, Accuracy: 73.58%\n",
      "Batch 23, Loss: 1.001883, Accuracy: 73.64%\n",
      "Batch 24, Loss: 1.035249, Accuracy: 73.50%\n",
      "Batch 25, Loss: 0.964699, Accuracy: 73.69%\n",
      "Batch 26, Loss: 1.089075, Accuracy: 73.44%\n",
      "Batch 27, Loss: 0.962692, Accuracy: 73.61%\n",
      "Batch 28, Loss: 0.960564, Accuracy: 73.88%\n",
      "Batch 29, Loss: 1.024766, Accuracy: 73.81%\n",
      "Batch 30, Loss: 1.099439, Accuracy: 73.49%\n",
      "Batch 31, Loss: 0.885623, Accuracy: 73.84%\n",
      "Batch 32, Loss: 0.975619, Accuracy: 73.93%\n",
      "Batch 33, Loss: 1.109788, Accuracy: 73.63%\n",
      "Batch 34, Loss: 1.060516, Accuracy: 73.48%\n",
      "Batch 35, Loss: 1.040338, Accuracy: 73.39%\n",
      "Batch 36, Loss: 1.007253, Accuracy: 73.39%\n",
      "Batch 37, Loss: 1.041544, Accuracy: 73.31%\n",
      "Batch 38, Loss: 1.079038, Accuracy: 73.11%\n",
      "Batch 39, Loss: 0.948571, Accuracy: 73.28%\n",
      "Batch 40, Loss: 1.016965, Accuracy: 73.24%\n",
      "Batch 41, Loss: 1.109053, Accuracy: 73.02%\n",
      "Batch 42, Loss: 1.077357, Accuracy: 72.88%\n",
      "Batch 43, Loss: 1.074390, Accuracy: 72.75%\n",
      "Batch 44, Loss: 0.978768, Accuracy: 72.80%\n",
      "Batch 45, Loss: 1.014797, Accuracy: 72.81%\n",
      "Batch 46, Loss: 0.964055, Accuracy: 72.89%\n",
      "Batch 47, Loss: 1.081201, Accuracy: 72.74%\n",
      "Batch 48, Loss: 1.087360, Accuracy: 72.59%\n",
      "Batch 49, Loss: 1.143768, Accuracy: 72.29%\n",
      "Batch 50, Loss: 1.032323, Accuracy: 72.25%\n",
      "Batch 51, Loss: 1.074404, Accuracy: 72.09%\n",
      "Batch 52, Loss: 1.033038, Accuracy: 72.03%\n",
      "Batch 53, Loss: 0.984625, Accuracy: 72.11%\n",
      "Batch 54, Loss: 0.906378, Accuracy: 72.34%\n",
      "Batch 55, Loss: 1.036519, Accuracy: 72.36%\n",
      "Batch 56, Loss: 0.997506, Accuracy: 72.41%\n",
      "Batch 57, Loss: 0.999543, Accuracy: 72.48%\n",
      "Batch 58, Loss: 1.014898, Accuracy: 72.52%\n",
      "Batch 59, Loss: 0.954681, Accuracy: 72.62%\n",
      "Batch 60, Loss: 1.099939, Accuracy: 72.50%\n",
      "Batch 61, Loss: 1.064130, Accuracy: 72.41%\n",
      "Batch 62, Loss: 0.991128, Accuracy: 72.45%\n",
      "Batch 63, Loss: 1.075579, Accuracy: 72.35%\n",
      "Batch 64, Loss: 1.010312, Accuracy: 72.34%\n",
      "Batch 65, Loss: 1.021666, Accuracy: 72.33%\n",
      "Batch 66, Loss: 1.045518, Accuracy: 72.32%\n",
      "Batch 67, Loss: 1.055550, Accuracy: 72.27%\n",
      "Batch 68, Loss: 0.967459, Accuracy: 72.33%\n",
      "Batch 69, Loss: 0.963869, Accuracy: 72.44%\n",
      "Batch 70, Loss: 0.981755, Accuracy: 72.52%\n",
      "Batch 71, Loss: 1.049372, Accuracy: 72.47%\n",
      "Batch 72, Loss: 1.009342, Accuracy: 72.48%\n",
      "Batch 73, Loss: 0.994853, Accuracy: 72.54%\n",
      "Batch 74, Loss: 0.972559, Accuracy: 72.57%\n",
      "Batch 75, Loss: 1.031053, Accuracy: 72.56%\n",
      "Batch 76, Loss: 1.122432, Accuracy: 72.43%\n",
      "Batch 77, Loss: 1.025800, Accuracy: 72.40%\n",
      "Batch 78, Loss: 1.028802, Accuracy: 72.40%\n",
      "Batch 79, Loss: 1.052773, Accuracy: 72.37%\n",
      "Batch 80, Loss: 1.119642, Accuracy: 72.23%\n",
      "Batch 81, Loss: 0.995547, Accuracy: 72.28%\n",
      "Batch 82, Loss: 0.985087, Accuracy: 72.37%\n",
      "Batch 83, Loss: 1.043208, Accuracy: 72.35%\n",
      "Batch 84, Loss: 1.016887, Accuracy: 72.30%\n",
      "Batch 85, Loss: 1.058230, Accuracy: 72.26%\n",
      "Batch 86, Loss: 0.964012, Accuracy: 72.35%\n",
      "Batch 87, Loss: 0.913738, Accuracy: 72.49%\n",
      "Batch 88, Loss: 0.964271, Accuracy: 72.57%\n",
      "Batch 89, Loss: 1.183087, Accuracy: 72.38%\n",
      "Batch 90, Loss: 1.186427, Accuracy: 72.19%\n",
      "Batch 91, Loss: 1.018850, Accuracy: 72.22%\n",
      "Batch 92, Loss: 1.078526, Accuracy: 72.16%\n",
      "Batch 93, Loss: 1.001196, Accuracy: 72.16%\n",
      "Batch 94, Loss: 1.049695, Accuracy: 72.16%\n",
      "Batch 95, Loss: 0.926495, Accuracy: 72.25%\n",
      "Batch 96, Loss: 1.058975, Accuracy: 72.22%\n",
      "Batch 97, Loss: 1.009175, Accuracy: 72.23%\n",
      "Batch 98, Loss: 1.001245, Accuracy: 72.26%\n",
      "Batch 99, Loss: 1.099412, Accuracy: 72.16%\n",
      "Batch 100, Loss: 1.041707, Accuracy: 72.14%\n",
      "Batch 101, Loss: 0.971480, Accuracy: 72.17%\n",
      "Batch 102, Loss: 1.091954, Accuracy: 72.09%\n",
      "Batch 103, Loss: 0.989007, Accuracy: 72.16%\n",
      "Batch 104, Loss: 1.009737, Accuracy: 72.18%\n",
      "Batch 105, Loss: 1.106961, Accuracy: 72.08%\n",
      "Batch 106, Loss: 1.017955, Accuracy: 72.08%\n",
      "Batch 107, Loss: 0.994780, Accuracy: 72.09%\n",
      "Batch 108, Loss: 0.971880, Accuracy: 72.16%\n",
      "Batch 109, Loss: 1.090510, Accuracy: 72.09%\n",
      "Batch 110, Loss: 1.005652, Accuracy: 72.07%\n",
      "Batch 111, Loss: 0.986901, Accuracy: 72.09%\n",
      "Batch 112, Loss: 0.911950, Accuracy: 72.17%\n",
      "Batch 113, Loss: 1.013049, Accuracy: 72.18%\n",
      "Batch 114, Loss: 0.978722, Accuracy: 72.20%\n",
      "Batch 115, Loss: 1.027719, Accuracy: 72.20%\n",
      "Batch 116, Loss: 1.003648, Accuracy: 72.20%\n",
      "Batch 117, Loss: 1.009092, Accuracy: 72.21%\n",
      "Batch 118, Loss: 1.048340, Accuracy: 72.21%\n",
      "Batch 119, Loss: 0.981437, Accuracy: 72.23%\n",
      "Batch 120, Loss: 1.133808, Accuracy: 72.12%\n",
      "Batch 121, Loss: 1.015552, Accuracy: 72.13%\n",
      "Batch 122, Loss: 0.960739, Accuracy: 72.17%\n",
      "Batch 123, Loss: 0.948730, Accuracy: 72.23%\n",
      "Batch 124, Loss: 1.063150, Accuracy: 72.18%\n",
      "Batch 125, Loss: 1.026823, Accuracy: 72.20%\n",
      "Batch 126, Loss: 1.038758, Accuracy: 72.19%\n",
      "Batch 127, Loss: 1.014443, Accuracy: 72.17%\n",
      "Batch 128, Loss: 1.051093, Accuracy: 72.14%\n",
      "Batch 129, Loss: 0.943111, Accuracy: 72.21%\n",
      "Batch 130, Loss: 1.054321, Accuracy: 72.19%\n",
      "Batch 131, Loss: 1.012409, Accuracy: 72.19%\n",
      "Batch 132, Loss: 0.883127, Accuracy: 72.29%\n",
      "Batch 133, Loss: 1.056718, Accuracy: 72.26%\n",
      "Batch 134, Loss: 1.025291, Accuracy: 72.26%\n",
      "Batch 135, Loss: 1.006817, Accuracy: 72.28%\n",
      "Batch 136, Loss: 1.020944, Accuracy: 72.28%\n",
      "Batch 137, Loss: 1.007796, Accuracy: 72.30%\n",
      "Batch 138, Loss: 0.991161, Accuracy: 72.32%\n",
      "Batch 139, Loss: 1.077360, Accuracy: 72.26%\n",
      "Batch 140, Loss: 1.074991, Accuracy: 72.22%\n",
      "Batch 141, Loss: 1.011641, Accuracy: 72.23%\n",
      "Batch 142, Loss: 1.035828, Accuracy: 72.21%\n",
      "Batch 143, Loss: 1.018046, Accuracy: 72.20%\n",
      "Batch 144, Loss: 1.021850, Accuracy: 72.19%\n",
      "Batch 145, Loss: 0.973651, Accuracy: 72.22%\n",
      "Batch 146, Loss: 1.031734, Accuracy: 72.23%\n",
      "Batch 147, Loss: 1.004084, Accuracy: 72.24%\n",
      "Batch 148, Loss: 1.004817, Accuracy: 72.24%\n",
      "Batch 149, Loss: 0.970655, Accuracy: 72.27%\n",
      "Batch 150, Loss: 1.018167, Accuracy: 72.29%\n",
      "Batch 151, Loss: 1.005904, Accuracy: 72.30%\n",
      "Batch 152, Loss: 1.042742, Accuracy: 72.29%\n",
      "Batch 153, Loss: 1.028296, Accuracy: 72.26%\n",
      "Batch 154, Loss: 1.046204, Accuracy: 72.25%\n",
      "Batch 155, Loss: 1.059112, Accuracy: 72.22%\n",
      "Batch 156, Loss: 0.969560, Accuracy: 72.25%\n",
      "Batch 157, Loss: 0.994527, Accuracy: 72.28%\n",
      "Batch 158, Loss: 1.027380, Accuracy: 72.27%\n",
      "Batch 159, Loss: 1.070157, Accuracy: 72.24%\n",
      "Batch 160, Loss: 1.014772, Accuracy: 72.25%\n",
      "Batch 161, Loss: 1.060102, Accuracy: 72.21%\n",
      "Batch 162, Loss: 0.992448, Accuracy: 72.23%\n",
      "Batch 163, Loss: 1.033904, Accuracy: 72.23%\n",
      "Batch 164, Loss: 1.050257, Accuracy: 72.22%\n",
      "Batch 165, Loss: 0.989802, Accuracy: 72.23%\n",
      "Batch 166, Loss: 1.012733, Accuracy: 72.24%\n",
      "Batch 167, Loss: 0.978091, Accuracy: 72.29%\n",
      "Batch 168, Loss: 1.042908, Accuracy: 72.27%\n",
      "Batch 169, Loss: 0.955169, Accuracy: 72.31%\n",
      "Batch 170, Loss: 1.037434, Accuracy: 72.29%\n",
      "Batch 171, Loss: 0.950014, Accuracy: 72.32%\n",
      "Batch 172, Loss: 0.957428, Accuracy: 72.35%\n",
      "Batch 173, Loss: 0.959033, Accuracy: 72.40%\n",
      "Batch 174, Loss: 1.024889, Accuracy: 72.39%\n",
      "Batch 175, Loss: 0.957466, Accuracy: 72.42%\n",
      "Batch 176, Loss: 1.005225, Accuracy: 72.43%\n",
      "Batch 177, Loss: 1.048547, Accuracy: 72.41%\n",
      "Batch 178, Loss: 1.047319, Accuracy: 72.40%\n",
      "Batch 179, Loss: 1.062853, Accuracy: 72.37%\n",
      "Batch 180, Loss: 1.075093, Accuracy: 72.34%\n",
      "Batch 181, Loss: 0.983980, Accuracy: 72.38%\n",
      "Batch 182, Loss: 1.105533, Accuracy: 72.32%\n",
      "Batch 183, Loss: 1.040283, Accuracy: 72.31%\n",
      "Batch 184, Loss: 1.009489, Accuracy: 72.32%\n",
      "Batch 185, Loss: 0.938552, Accuracy: 72.37%\n",
      "Batch 186, Loss: 0.987123, Accuracy: 72.39%\n",
      "Batch 187, Loss: 1.046416, Accuracy: 72.37%\n",
      "Batch 188, Loss: 0.975377, Accuracy: 72.39%\n",
      "Batch 189, Loss: 1.100339, Accuracy: 72.35%\n",
      "Batch 190, Loss: 1.013132, Accuracy: 72.34%\n",
      "Batch 191, Loss: 0.994057, Accuracy: 72.34%\n",
      "Batch 192, Loss: 0.984749, Accuracy: 72.35%\n",
      "Batch 193, Loss: 0.954775, Accuracy: 72.38%\n",
      "Batch 194, Loss: 1.037563, Accuracy: 72.36%\n",
      "Batch 195, Loss: 1.042490, Accuracy: 72.34%\n",
      "Batch 196, Loss: 1.115927, Accuracy: 72.28%\n",
      "Batch 197, Loss: 1.054176, Accuracy: 72.26%\n",
      "Batch 198, Loss: 0.951208, Accuracy: 72.29%\n",
      "Batch 199, Loss: 0.991993, Accuracy: 72.30%\n",
      "Batch 200, Loss: 0.952327, Accuracy: 72.34%\n",
      "Batch 201, Loss: 1.000202, Accuracy: 72.35%\n",
      "Batch 202, Loss: 1.026376, Accuracy: 72.34%\n",
      "Batch 203, Loss: 0.989607, Accuracy: 72.36%\n",
      "Batch 204, Loss: 1.032085, Accuracy: 72.35%\n",
      "Batch 205, Loss: 0.990581, Accuracy: 72.37%\n",
      "Batch 206, Loss: 0.983533, Accuracy: 72.38%\n",
      "Batch 207, Loss: 0.987894, Accuracy: 72.40%\n",
      "Batch 208, Loss: 1.056379, Accuracy: 72.38%\n",
      "Batch 209, Loss: 0.998853, Accuracy: 72.39%\n",
      "Batch 210, Loss: 0.946400, Accuracy: 72.43%\n",
      "Batch 211, Loss: 0.990545, Accuracy: 72.45%\n",
      "Batch 212, Loss: 1.047809, Accuracy: 72.43%\n",
      "Batch 213, Loss: 0.985245, Accuracy: 72.46%\n",
      "Training - Epoch 47, Loss: 1.017856, Accuracy: 72.46%\n",
      "Validation Batch 1, Loss: 1.044054, Accuracy: 70.31%\n",
      "Validation Batch 2, Loss: 1.164842, Accuracy: 62.50%\n",
      "Validation Batch 3, Loss: 1.154403, Accuracy: 61.46%\n",
      "Validation Batch 4, Loss: 1.055397, Accuracy: 62.89%\n",
      "Validation Batch 5, Loss: 1.104228, Accuracy: 62.19%\n",
      "Validation Batch 6, Loss: 1.052569, Accuracy: 63.80%\n",
      "Validation Batch 7, Loss: 1.095389, Accuracy: 64.29%\n",
      "Validation Batch 8, Loss: 1.113686, Accuracy: 63.87%\n",
      "Validation Batch 9, Loss: 1.139523, Accuracy: 63.19%\n",
      "Validation Batch 10, Loss: 1.110571, Accuracy: 63.28%\n",
      "Validation Batch 11, Loss: 1.086490, Accuracy: 63.21%\n",
      "Validation Batch 12, Loss: 1.046127, Accuracy: 63.93%\n",
      "Validation Batch 13, Loss: 1.167065, Accuracy: 63.46%\n",
      "Validation Batch 14, Loss: 1.115634, Accuracy: 63.28%\n",
      "Validation Batch 15, Loss: 1.100824, Accuracy: 63.33%\n",
      "Validation Batch 16, Loss: 1.073447, Accuracy: 63.48%\n",
      "Validation Batch 17, Loss: 1.187692, Accuracy: 62.78%\n",
      "Validation Batch 18, Loss: 1.068119, Accuracy: 62.85%\n",
      "Validation Batch 19, Loss: 1.164414, Accuracy: 62.34%\n",
      "Validation Batch 20, Loss: 1.100241, Accuracy: 62.58%\n",
      "Validation Batch 21, Loss: 1.100012, Accuracy: 62.57%\n",
      "Validation Batch 22, Loss: 1.161628, Accuracy: 62.14%\n",
      "Validation Batch 23, Loss: 1.200762, Accuracy: 61.68%\n",
      "Validation Batch 24, Loss: 1.131363, Accuracy: 61.72%\n",
      "Validation Batch 25, Loss: 1.095559, Accuracy: 61.62%\n",
      "Validation Batch 26, Loss: 1.071229, Accuracy: 61.84%\n",
      "Validation Batch 27, Loss: 1.112449, Accuracy: 61.89%\n",
      "Validation - Epoch 47, Loss: 1.111767, Accuracy: 61.89%\n",
      "Patienceâ€”4\n",
      "Epoch 48\n",
      "Batch 1, Loss: 0.985004, Accuracy: 78.12%\n",
      "Batch 2, Loss: 0.974669, Accuracy: 76.56%\n",
      "Batch 3, Loss: 1.001576, Accuracy: 75.52%\n",
      "Batch 4, Loss: 0.920650, Accuracy: 76.56%\n",
      "Batch 5, Loss: 1.010989, Accuracy: 75.94%\n",
      "Batch 6, Loss: 1.030017, Accuracy: 75.52%\n",
      "Batch 7, Loss: 0.993085, Accuracy: 75.00%\n",
      "Batch 8, Loss: 1.171641, Accuracy: 72.66%\n",
      "Batch 9, Loss: 0.917892, Accuracy: 73.96%\n",
      "Batch 10, Loss: 1.044043, Accuracy: 73.28%\n",
      "Batch 11, Loss: 0.972119, Accuracy: 73.58%\n",
      "Batch 12, Loss: 1.021099, Accuracy: 73.44%\n",
      "Batch 13, Loss: 0.957648, Accuracy: 73.92%\n",
      "Batch 14, Loss: 0.953491, Accuracy: 74.22%\n",
      "Batch 15, Loss: 1.041455, Accuracy: 73.85%\n",
      "Batch 16, Loss: 1.055256, Accuracy: 73.44%\n",
      "Batch 17, Loss: 0.978987, Accuracy: 73.62%\n",
      "Batch 18, Loss: 1.001698, Accuracy: 73.70%\n",
      "Batch 19, Loss: 0.937232, Accuracy: 74.10%\n",
      "Batch 20, Loss: 1.081990, Accuracy: 73.75%\n",
      "Batch 21, Loss: 1.015692, Accuracy: 73.81%\n",
      "Batch 22, Loss: 1.029678, Accuracy: 73.65%\n",
      "Batch 23, Loss: 1.020642, Accuracy: 73.51%\n",
      "Batch 24, Loss: 0.963055, Accuracy: 73.83%\n",
      "Batch 25, Loss: 0.977781, Accuracy: 74.00%\n",
      "Batch 26, Loss: 1.104376, Accuracy: 73.56%\n",
      "Batch 27, Loss: 0.959988, Accuracy: 73.78%\n",
      "Batch 28, Loss: 0.964927, Accuracy: 73.88%\n",
      "Batch 29, Loss: 0.967836, Accuracy: 74.03%\n",
      "Batch 30, Loss: 1.010535, Accuracy: 73.96%\n",
      "Batch 31, Loss: 1.052387, Accuracy: 73.79%\n",
      "Batch 32, Loss: 0.978995, Accuracy: 73.88%\n",
      "Batch 33, Loss: 0.907831, Accuracy: 74.20%\n",
      "Batch 34, Loss: 1.011833, Accuracy: 74.17%\n",
      "Batch 35, Loss: 1.090958, Accuracy: 73.93%\n",
      "Batch 36, Loss: 0.999717, Accuracy: 73.96%\n",
      "Batch 37, Loss: 1.012235, Accuracy: 73.94%\n",
      "Batch 38, Loss: 1.043349, Accuracy: 73.85%\n",
      "Batch 39, Loss: 1.048711, Accuracy: 73.68%\n",
      "Batch 40, Loss: 1.047656, Accuracy: 73.52%\n",
      "Batch 41, Loss: 1.012680, Accuracy: 73.48%\n",
      "Batch 42, Loss: 1.020611, Accuracy: 73.36%\n",
      "Batch 43, Loss: 1.040294, Accuracy: 73.26%\n",
      "Batch 44, Loss: 0.896189, Accuracy: 73.47%\n",
      "Batch 45, Loss: 1.020965, Accuracy: 73.37%\n",
      "Batch 46, Loss: 0.960555, Accuracy: 73.54%\n",
      "Batch 47, Loss: 1.032475, Accuracy: 73.50%\n",
      "Batch 48, Loss: 1.038650, Accuracy: 73.40%\n",
      "Batch 49, Loss: 1.076992, Accuracy: 73.18%\n",
      "Batch 50, Loss: 1.012039, Accuracy: 73.16%\n",
      "Batch 51, Loss: 0.984804, Accuracy: 73.19%\n",
      "Batch 52, Loss: 1.014042, Accuracy: 73.20%\n",
      "Batch 53, Loss: 1.085162, Accuracy: 73.05%\n",
      "Batch 54, Loss: 0.977866, Accuracy: 73.09%\n",
      "Batch 55, Loss: 0.974550, Accuracy: 73.18%\n",
      "Batch 56, Loss: 0.990499, Accuracy: 73.21%\n",
      "Batch 57, Loss: 1.003007, Accuracy: 73.27%\n",
      "Batch 58, Loss: 0.973336, Accuracy: 73.41%\n",
      "Batch 59, Loss: 0.986950, Accuracy: 73.44%\n",
      "Batch 60, Loss: 1.095180, Accuracy: 73.28%\n",
      "Batch 61, Loss: 1.006566, Accuracy: 73.23%\n",
      "Batch 62, Loss: 1.027089, Accuracy: 73.21%\n",
      "Batch 63, Loss: 1.108763, Accuracy: 73.09%\n",
      "Batch 64, Loss: 0.958586, Accuracy: 73.17%\n",
      "Batch 65, Loss: 0.959527, Accuracy: 73.25%\n",
      "Batch 66, Loss: 1.037020, Accuracy: 73.20%\n",
      "Batch 67, Loss: 0.994049, Accuracy: 73.25%\n",
      "Batch 68, Loss: 0.993989, Accuracy: 73.28%\n",
      "Batch 69, Loss: 1.095803, Accuracy: 73.19%\n",
      "Batch 70, Loss: 1.045217, Accuracy: 73.10%\n",
      "Batch 71, Loss: 0.959767, Accuracy: 73.22%\n",
      "Batch 72, Loss: 1.034347, Accuracy: 73.18%\n",
      "Batch 73, Loss: 1.058930, Accuracy: 73.10%\n",
      "Batch 74, Loss: 1.071700, Accuracy: 72.99%\n",
      "Batch 75, Loss: 1.027669, Accuracy: 72.98%\n",
      "Batch 76, Loss: 1.023510, Accuracy: 72.96%\n",
      "Batch 77, Loss: 1.036766, Accuracy: 72.93%\n",
      "Batch 78, Loss: 0.960178, Accuracy: 73.00%\n",
      "Batch 79, Loss: 0.992969, Accuracy: 73.02%\n",
      "Batch 80, Loss: 1.017464, Accuracy: 73.03%\n",
      "Batch 81, Loss: 0.993047, Accuracy: 73.05%\n",
      "Batch 82, Loss: 1.160380, Accuracy: 72.85%\n",
      "Batch 83, Loss: 1.001526, Accuracy: 72.84%\n",
      "Batch 84, Loss: 0.917890, Accuracy: 72.97%\n",
      "Batch 85, Loss: 0.974832, Accuracy: 73.01%\n",
      "Batch 86, Loss: 1.032601, Accuracy: 73.00%\n",
      "Batch 87, Loss: 1.047214, Accuracy: 72.97%\n",
      "Batch 88, Loss: 1.038968, Accuracy: 72.92%\n",
      "Batch 89, Loss: 1.025281, Accuracy: 72.93%\n",
      "Batch 90, Loss: 1.047552, Accuracy: 72.88%\n",
      "Batch 91, Loss: 1.045380, Accuracy: 72.87%\n",
      "Batch 92, Loss: 0.939659, Accuracy: 72.98%\n",
      "Batch 93, Loss: 0.992848, Accuracy: 72.98%\n",
      "Batch 94, Loss: 1.017586, Accuracy: 72.97%\n",
      "Batch 95, Loss: 0.951701, Accuracy: 73.03%\n",
      "Batch 96, Loss: 1.022663, Accuracy: 73.00%\n",
      "Batch 97, Loss: 0.931245, Accuracy: 73.07%\n",
      "Batch 98, Loss: 0.977722, Accuracy: 73.10%\n",
      "Batch 99, Loss: 1.006211, Accuracy: 73.09%\n",
      "Batch 100, Loss: 0.952637, Accuracy: 73.16%\n",
      "Batch 101, Loss: 1.145577, Accuracy: 73.02%\n",
      "Batch 102, Loss: 1.045173, Accuracy: 72.99%\n",
      "Batch 103, Loss: 1.024174, Accuracy: 73.00%\n",
      "Batch 104, Loss: 0.905670, Accuracy: 73.11%\n",
      "Batch 105, Loss: 1.019696, Accuracy: 73.12%\n",
      "Batch 106, Loss: 0.996289, Accuracy: 73.14%\n",
      "Batch 107, Loss: 0.979049, Accuracy: 73.17%\n",
      "Batch 108, Loss: 0.906873, Accuracy: 73.28%\n",
      "Batch 109, Loss: 0.957647, Accuracy: 73.32%\n",
      "Batch 110, Loss: 0.991208, Accuracy: 73.32%\n",
      "Batch 111, Loss: 0.931611, Accuracy: 73.40%\n",
      "Batch 112, Loss: 1.121922, Accuracy: 73.30%\n",
      "Batch 113, Loss: 1.047798, Accuracy: 73.27%\n",
      "Batch 114, Loss: 1.067547, Accuracy: 73.22%\n",
      "Batch 115, Loss: 1.000042, Accuracy: 73.23%\n",
      "Batch 116, Loss: 0.971529, Accuracy: 73.28%\n",
      "Batch 117, Loss: 1.001394, Accuracy: 73.28%\n",
      "Batch 118, Loss: 1.081373, Accuracy: 73.19%\n",
      "Batch 119, Loss: 1.081066, Accuracy: 73.11%\n",
      "Batch 120, Loss: 0.965905, Accuracy: 73.15%\n",
      "Batch 121, Loss: 1.060288, Accuracy: 73.13%\n",
      "Batch 122, Loss: 1.015815, Accuracy: 73.12%\n",
      "Batch 123, Loss: 1.086660, Accuracy: 73.06%\n",
      "Batch 124, Loss: 0.988109, Accuracy: 73.07%\n",
      "Batch 125, Loss: 0.962318, Accuracy: 73.11%\n",
      "Batch 126, Loss: 1.003869, Accuracy: 73.10%\n",
      "Batch 127, Loss: 1.012708, Accuracy: 73.11%\n",
      "Batch 128, Loss: 0.945688, Accuracy: 73.16%\n",
      "Batch 129, Loss: 1.049819, Accuracy: 73.11%\n",
      "Batch 130, Loss: 1.055392, Accuracy: 73.05%\n",
      "Batch 131, Loss: 0.931808, Accuracy: 73.12%\n",
      "Batch 132, Loss: 1.016444, Accuracy: 73.12%\n",
      "Batch 133, Loss: 0.973493, Accuracy: 73.16%\n",
      "Batch 134, Loss: 1.142613, Accuracy: 73.05%\n",
      "Batch 135, Loss: 0.879243, Accuracy: 73.16%\n",
      "Batch 136, Loss: 1.010300, Accuracy: 73.15%\n",
      "Batch 137, Loss: 0.979633, Accuracy: 73.19%\n",
      "Batch 138, Loss: 0.986875, Accuracy: 73.20%\n",
      "Batch 139, Loss: 1.036137, Accuracy: 73.18%\n",
      "Batch 140, Loss: 1.047150, Accuracy: 73.15%\n",
      "Batch 141, Loss: 0.969742, Accuracy: 73.18%\n",
      "Batch 142, Loss: 0.956443, Accuracy: 73.22%\n",
      "Batch 143, Loss: 0.992995, Accuracy: 73.23%\n",
      "Batch 144, Loss: 1.021560, Accuracy: 73.23%\n",
      "Batch 145, Loss: 0.978241, Accuracy: 73.24%\n",
      "Batch 146, Loss: 1.035487, Accuracy: 73.22%\n",
      "Batch 147, Loss: 1.061696, Accuracy: 73.19%\n",
      "Batch 148, Loss: 0.937518, Accuracy: 73.24%\n",
      "Batch 149, Loss: 1.031929, Accuracy: 73.23%\n",
      "Batch 150, Loss: 1.019570, Accuracy: 73.21%\n",
      "Batch 151, Loss: 0.932725, Accuracy: 73.25%\n",
      "Batch 152, Loss: 1.053462, Accuracy: 73.22%\n",
      "Batch 153, Loss: 1.112269, Accuracy: 73.14%\n",
      "Batch 154, Loss: 1.050940, Accuracy: 73.12%\n",
      "Batch 155, Loss: 1.051278, Accuracy: 73.09%\n",
      "Batch 156, Loss: 0.994939, Accuracy: 73.12%\n",
      "Batch 157, Loss: 0.964356, Accuracy: 73.14%\n",
      "Batch 158, Loss: 1.049649, Accuracy: 73.11%\n",
      "Batch 159, Loss: 1.058051, Accuracy: 73.07%\n",
      "Batch 160, Loss: 1.002929, Accuracy: 73.08%\n",
      "Batch 161, Loss: 1.050396, Accuracy: 73.05%\n",
      "Batch 162, Loss: 0.947881, Accuracy: 73.10%\n",
      "Batch 163, Loss: 0.985075, Accuracy: 73.10%\n",
      "Batch 164, Loss: 0.982032, Accuracy: 73.12%\n",
      "Batch 165, Loss: 0.968484, Accuracy: 73.14%\n",
      "Batch 166, Loss: 1.083189, Accuracy: 73.10%\n",
      "Batch 167, Loss: 1.008039, Accuracy: 73.10%\n",
      "Batch 168, Loss: 1.101147, Accuracy: 73.04%\n",
      "Batch 169, Loss: 1.063407, Accuracy: 73.00%\n",
      "Batch 170, Loss: 1.044682, Accuracy: 72.98%\n",
      "Batch 171, Loss: 0.968069, Accuracy: 73.00%\n",
      "Batch 172, Loss: 1.041265, Accuracy: 72.98%\n",
      "Batch 173, Loss: 0.999801, Accuracy: 72.99%\n",
      "Batch 174, Loss: 1.019212, Accuracy: 73.00%\n",
      "Batch 175, Loss: 0.901545, Accuracy: 73.06%\n",
      "Batch 176, Loss: 1.088257, Accuracy: 73.01%\n",
      "Batch 177, Loss: 1.026587, Accuracy: 73.01%\n",
      "Batch 178, Loss: 0.994887, Accuracy: 73.02%\n",
      "Batch 179, Loss: 1.066983, Accuracy: 72.99%\n",
      "Batch 180, Loss: 1.063554, Accuracy: 72.95%\n",
      "Batch 181, Loss: 0.941500, Accuracy: 73.01%\n",
      "Batch 182, Loss: 0.966730, Accuracy: 73.03%\n",
      "Batch 183, Loss: 1.030182, Accuracy: 73.00%\n",
      "Batch 184, Loss: 1.015974, Accuracy: 73.00%\n",
      "Batch 185, Loss: 1.049720, Accuracy: 72.97%\n",
      "Batch 186, Loss: 1.101105, Accuracy: 72.93%\n",
      "Batch 187, Loss: 0.915896, Accuracy: 72.99%\n",
      "Batch 188, Loss: 1.130280, Accuracy: 72.91%\n",
      "Batch 189, Loss: 1.045887, Accuracy: 72.90%\n",
      "Batch 190, Loss: 0.939511, Accuracy: 72.95%\n",
      "Batch 191, Loss: 0.993294, Accuracy: 72.96%\n",
      "Batch 192, Loss: 1.004015, Accuracy: 72.97%\n",
      "Batch 193, Loss: 0.958283, Accuracy: 73.01%\n",
      "Batch 194, Loss: 0.986197, Accuracy: 73.03%\n",
      "Batch 195, Loss: 1.005548, Accuracy: 73.02%\n",
      "Batch 196, Loss: 0.989533, Accuracy: 73.04%\n",
      "Batch 197, Loss: 0.976602, Accuracy: 73.06%\n",
      "Batch 198, Loss: 1.005333, Accuracy: 73.07%\n",
      "Batch 199, Loss: 0.981093, Accuracy: 73.08%\n",
      "Batch 200, Loss: 1.010189, Accuracy: 73.07%\n",
      "Batch 201, Loss: 1.000960, Accuracy: 73.07%\n",
      "Batch 202, Loss: 1.207814, Accuracy: 72.97%\n",
      "Batch 203, Loss: 1.047073, Accuracy: 72.95%\n",
      "Batch 204, Loss: 1.048953, Accuracy: 72.92%\n",
      "Batch 205, Loss: 1.024685, Accuracy: 72.93%\n",
      "Batch 206, Loss: 1.008947, Accuracy: 72.92%\n",
      "Batch 207, Loss: 1.046926, Accuracy: 72.91%\n",
      "Batch 208, Loss: 0.956969, Accuracy: 72.93%\n",
      "Batch 209, Loss: 1.075473, Accuracy: 72.90%\n",
      "Batch 210, Loss: 1.033509, Accuracy: 72.89%\n",
      "Batch 211, Loss: 0.949431, Accuracy: 72.92%\n",
      "Batch 212, Loss: 0.979193, Accuracy: 72.94%\n",
      "Batch 213, Loss: 1.042807, Accuracy: 72.93%\n",
      "Training - Epoch 48, Loss: 1.011823, Accuracy: 72.93%\n",
      "Validation Batch 1, Loss: 1.015408, Accuracy: 71.88%\n",
      "Validation Batch 2, Loss: 1.131083, Accuracy: 65.62%\n",
      "Validation Batch 3, Loss: 1.123536, Accuracy: 64.58%\n",
      "Validation Batch 4, Loss: 1.037072, Accuracy: 66.02%\n",
      "Validation Batch 5, Loss: 1.061552, Accuracy: 66.88%\n",
      "Validation Batch 6, Loss: 1.011887, Accuracy: 68.23%\n",
      "Validation Batch 7, Loss: 1.069281, Accuracy: 68.30%\n",
      "Validation Batch 8, Loss: 1.095533, Accuracy: 67.97%\n",
      "Validation Batch 9, Loss: 1.118618, Accuracy: 67.19%\n",
      "Validation Batch 10, Loss: 1.075306, Accuracy: 67.19%\n",
      "Validation Batch 11, Loss: 1.052309, Accuracy: 67.19%\n",
      "Validation Batch 12, Loss: 1.011400, Accuracy: 67.84%\n",
      "Validation Batch 13, Loss: 1.141401, Accuracy: 67.43%\n",
      "Validation Batch 14, Loss: 1.084207, Accuracy: 67.19%\n",
      "Validation Batch 15, Loss: 1.063736, Accuracy: 67.19%\n",
      "Validation Batch 16, Loss: 1.038931, Accuracy: 67.48%\n",
      "Validation Batch 17, Loss: 1.135847, Accuracy: 66.91%\n",
      "Validation Batch 18, Loss: 1.037872, Accuracy: 67.10%\n",
      "Validation Batch 19, Loss: 1.118408, Accuracy: 66.86%\n",
      "Validation Batch 20, Loss: 1.066616, Accuracy: 67.03%\n",
      "Validation Batch 21, Loss: 1.068718, Accuracy: 66.96%\n",
      "Validation Batch 22, Loss: 1.117692, Accuracy: 66.69%\n",
      "Validation Batch 23, Loss: 1.173045, Accuracy: 66.24%\n",
      "Validation Batch 24, Loss: 1.106995, Accuracy: 66.15%\n",
      "Validation Batch 25, Loss: 1.063725, Accuracy: 66.12%\n",
      "Validation Batch 26, Loss: 1.051331, Accuracy: 66.29%\n",
      "Validation Batch 27, Loss: 1.074751, Accuracy: 66.29%\n",
      "Validation - Epoch 48, Loss: 1.079491, Accuracy: 66.29%\n",
      "Patienceâ€”0\n",
      "Epoch 49\n",
      "Batch 1, Loss: 1.038011, Accuracy: 68.75%\n",
      "Batch 2, Loss: 1.094367, Accuracy: 67.19%\n",
      "Batch 3, Loss: 0.960817, Accuracy: 71.88%\n",
      "Batch 4, Loss: 1.100363, Accuracy: 70.31%\n",
      "Batch 5, Loss: 1.000161, Accuracy: 70.62%\n",
      "Batch 6, Loss: 1.034899, Accuracy: 70.05%\n",
      "Batch 7, Loss: 0.991485, Accuracy: 70.98%\n",
      "Batch 8, Loss: 1.023952, Accuracy: 70.90%\n",
      "Batch 9, Loss: 1.013530, Accuracy: 71.18%\n",
      "Batch 10, Loss: 1.024719, Accuracy: 71.09%\n",
      "Batch 11, Loss: 0.992686, Accuracy: 71.45%\n",
      "Batch 12, Loss: 0.981049, Accuracy: 71.74%\n",
      "Batch 13, Loss: 1.045925, Accuracy: 71.63%\n",
      "Batch 14, Loss: 1.024708, Accuracy: 71.54%\n",
      "Batch 15, Loss: 1.120826, Accuracy: 70.62%\n",
      "Batch 16, Loss: 0.975408, Accuracy: 71.00%\n",
      "Batch 17, Loss: 1.024010, Accuracy: 71.14%\n",
      "Batch 18, Loss: 1.010721, Accuracy: 71.35%\n",
      "Batch 19, Loss: 0.965759, Accuracy: 71.63%\n",
      "Batch 20, Loss: 1.012616, Accuracy: 71.80%\n",
      "Batch 21, Loss: 1.057005, Accuracy: 71.58%\n",
      "Batch 22, Loss: 0.960465, Accuracy: 72.09%\n",
      "Batch 23, Loss: 1.074286, Accuracy: 71.81%\n",
      "Batch 24, Loss: 0.955133, Accuracy: 72.14%\n",
      "Batch 25, Loss: 0.963174, Accuracy: 72.38%\n",
      "Batch 26, Loss: 1.014122, Accuracy: 72.42%\n",
      "Batch 27, Loss: 0.931867, Accuracy: 72.69%\n",
      "Batch 28, Loss: 0.995773, Accuracy: 72.82%\n",
      "Batch 29, Loss: 0.983450, Accuracy: 73.01%\n",
      "Batch 30, Loss: 0.954884, Accuracy: 73.23%\n",
      "Batch 31, Loss: 1.029157, Accuracy: 73.08%\n",
      "Batch 32, Loss: 0.994226, Accuracy: 73.14%\n",
      "Batch 33, Loss: 1.018049, Accuracy: 73.11%\n",
      "Batch 34, Loss: 1.082016, Accuracy: 72.89%\n",
      "Batch 35, Loss: 1.046885, Accuracy: 72.77%\n",
      "Batch 36, Loss: 0.946495, Accuracy: 72.92%\n",
      "Batch 37, Loss: 1.002238, Accuracy: 72.93%\n",
      "Batch 38, Loss: 1.082485, Accuracy: 72.74%\n",
      "Batch 39, Loss: 1.070081, Accuracy: 72.56%\n",
      "Batch 40, Loss: 1.023984, Accuracy: 72.50%\n",
      "Batch 41, Loss: 1.024005, Accuracy: 72.52%\n",
      "Batch 42, Loss: 0.962432, Accuracy: 72.66%\n",
      "Batch 43, Loss: 1.007881, Accuracy: 72.67%\n",
      "Batch 44, Loss: 1.005419, Accuracy: 72.69%\n",
      "Batch 45, Loss: 1.014730, Accuracy: 72.71%\n",
      "Batch 46, Loss: 1.033759, Accuracy: 72.69%\n",
      "Batch 47, Loss: 1.032693, Accuracy: 72.74%\n",
      "Batch 48, Loss: 0.983834, Accuracy: 72.75%\n",
      "Batch 49, Loss: 0.934596, Accuracy: 72.99%\n",
      "Batch 50, Loss: 1.005027, Accuracy: 72.97%\n",
      "Batch 51, Loss: 1.062531, Accuracy: 72.86%\n",
      "Batch 52, Loss: 0.992287, Accuracy: 72.90%\n",
      "Batch 53, Loss: 1.029870, Accuracy: 72.85%\n",
      "Batch 54, Loss: 1.055142, Accuracy: 72.74%\n",
      "Batch 55, Loss: 0.977626, Accuracy: 72.84%\n",
      "Batch 56, Loss: 1.075504, Accuracy: 72.71%\n",
      "Batch 57, Loss: 0.995951, Accuracy: 72.75%\n",
      "Batch 58, Loss: 1.084204, Accuracy: 72.63%\n",
      "Batch 59, Loss: 1.017839, Accuracy: 72.59%\n",
      "Batch 60, Loss: 0.988951, Accuracy: 72.66%\n",
      "Batch 61, Loss: 1.051223, Accuracy: 72.57%\n",
      "Batch 62, Loss: 0.948837, Accuracy: 72.66%\n",
      "Batch 63, Loss: 1.035957, Accuracy: 72.62%\n",
      "Batch 64, Loss: 1.042816, Accuracy: 72.53%\n",
      "Batch 65, Loss: 0.969694, Accuracy: 72.60%\n",
      "Batch 66, Loss: 0.981951, Accuracy: 72.68%\n",
      "Batch 67, Loss: 0.986958, Accuracy: 72.71%\n",
      "Batch 68, Loss: 1.019282, Accuracy: 72.70%\n",
      "Batch 69, Loss: 1.154142, Accuracy: 72.51%\n",
      "Batch 70, Loss: 1.003341, Accuracy: 72.52%\n",
      "Batch 71, Loss: 1.090463, Accuracy: 72.40%\n",
      "Batch 72, Loss: 1.018197, Accuracy: 72.40%\n",
      "Batch 73, Loss: 0.926779, Accuracy: 72.52%\n",
      "Batch 74, Loss: 0.971439, Accuracy: 72.57%\n",
      "Batch 75, Loss: 1.121049, Accuracy: 72.44%\n",
      "Batch 76, Loss: 0.907992, Accuracy: 72.59%\n",
      "Batch 77, Loss: 1.030850, Accuracy: 72.61%\n",
      "Batch 78, Loss: 1.068216, Accuracy: 72.56%\n",
      "Batch 79, Loss: 1.073440, Accuracy: 72.47%\n",
      "Batch 80, Loss: 1.012083, Accuracy: 72.48%\n",
      "Batch 81, Loss: 1.020892, Accuracy: 72.47%\n",
      "Batch 82, Loss: 1.068512, Accuracy: 72.39%\n",
      "Batch 83, Loss: 1.037739, Accuracy: 72.36%\n",
      "Batch 84, Loss: 0.910496, Accuracy: 72.49%\n",
      "Batch 85, Loss: 0.943412, Accuracy: 72.59%\n",
      "Batch 86, Loss: 1.070995, Accuracy: 72.53%\n",
      "Batch 87, Loss: 1.041493, Accuracy: 72.49%\n",
      "Batch 88, Loss: 1.003433, Accuracy: 72.51%\n",
      "Batch 89, Loss: 1.026299, Accuracy: 72.51%\n",
      "Batch 90, Loss: 0.978800, Accuracy: 72.53%\n",
      "Batch 91, Loss: 1.029503, Accuracy: 72.51%\n",
      "Batch 92, Loss: 0.983875, Accuracy: 72.55%\n",
      "Batch 93, Loss: 1.070496, Accuracy: 72.50%\n",
      "Batch 94, Loss: 1.017555, Accuracy: 72.47%\n",
      "Batch 95, Loss: 1.012941, Accuracy: 72.48%\n",
      "Batch 96, Loss: 1.038373, Accuracy: 72.43%\n",
      "Batch 97, Loss: 0.969062, Accuracy: 72.50%\n",
      "Batch 98, Loss: 1.007021, Accuracy: 72.53%\n",
      "Batch 99, Loss: 0.988958, Accuracy: 72.55%\n",
      "Batch 100, Loss: 1.021360, Accuracy: 72.53%\n",
      "Batch 101, Loss: 1.008475, Accuracy: 72.54%\n",
      "Batch 102, Loss: 1.036333, Accuracy: 72.52%\n",
      "Batch 103, Loss: 0.984462, Accuracy: 72.56%\n",
      "Batch 104, Loss: 1.111146, Accuracy: 72.46%\n",
      "Batch 105, Loss: 1.103174, Accuracy: 72.37%\n",
      "Batch 106, Loss: 1.047498, Accuracy: 72.35%\n",
      "Batch 107, Loss: 0.957614, Accuracy: 72.40%\n",
      "Batch 108, Loss: 1.001340, Accuracy: 72.44%\n",
      "Batch 109, Loss: 1.004424, Accuracy: 72.46%\n",
      "Batch 110, Loss: 1.014252, Accuracy: 72.49%\n",
      "Batch 111, Loss: 1.036083, Accuracy: 72.48%\n",
      "Batch 112, Loss: 1.066012, Accuracy: 72.45%\n",
      "Batch 113, Loss: 0.978068, Accuracy: 72.48%\n",
      "Batch 114, Loss: 0.918540, Accuracy: 72.59%\n",
      "Batch 115, Loss: 1.037372, Accuracy: 72.58%\n",
      "Batch 116, Loss: 1.061895, Accuracy: 72.54%\n",
      "Batch 117, Loss: 0.920713, Accuracy: 72.61%\n",
      "Batch 118, Loss: 1.021865, Accuracy: 72.60%\n",
      "Batch 119, Loss: 1.037103, Accuracy: 72.60%\n",
      "Batch 120, Loss: 1.026568, Accuracy: 72.58%\n",
      "Batch 121, Loss: 1.131254, Accuracy: 72.48%\n",
      "Batch 122, Loss: 0.977310, Accuracy: 72.52%\n",
      "Batch 123, Loss: 0.976848, Accuracy: 72.55%\n",
      "Batch 124, Loss: 1.007318, Accuracy: 72.56%\n",
      "Batch 125, Loss: 0.966709, Accuracy: 72.60%\n",
      "Batch 126, Loss: 0.979759, Accuracy: 72.63%\n",
      "Batch 127, Loss: 1.010158, Accuracy: 72.65%\n",
      "Batch 128, Loss: 1.023219, Accuracy: 72.63%\n",
      "Batch 129, Loss: 1.026411, Accuracy: 72.61%\n",
      "Batch 130, Loss: 1.061754, Accuracy: 72.57%\n",
      "Batch 131, Loss: 0.995110, Accuracy: 72.59%\n",
      "Batch 132, Loss: 0.991823, Accuracy: 72.61%\n",
      "Batch 133, Loss: 0.971417, Accuracy: 72.65%\n",
      "Batch 134, Loss: 1.035080, Accuracy: 72.62%\n",
      "Batch 135, Loss: 0.953722, Accuracy: 72.65%\n",
      "Batch 136, Loss: 1.061479, Accuracy: 72.62%\n",
      "Batch 137, Loss: 0.947637, Accuracy: 72.70%\n",
      "Batch 138, Loss: 1.006798, Accuracy: 72.69%\n",
      "Batch 139, Loss: 1.040993, Accuracy: 72.67%\n",
      "Batch 140, Loss: 1.040709, Accuracy: 72.65%\n",
      "Batch 141, Loss: 1.059876, Accuracy: 72.62%\n",
      "Batch 142, Loss: 1.008102, Accuracy: 72.62%\n",
      "Batch 143, Loss: 1.061507, Accuracy: 72.59%\n",
      "Batch 144, Loss: 1.052664, Accuracy: 72.56%\n",
      "Batch 145, Loss: 1.027646, Accuracy: 72.55%\n",
      "Batch 146, Loss: 1.047448, Accuracy: 72.53%\n",
      "Batch 147, Loss: 1.045102, Accuracy: 72.50%\n",
      "Batch 148, Loss: 1.020599, Accuracy: 72.50%\n",
      "Batch 149, Loss: 1.074098, Accuracy: 72.46%\n",
      "Batch 150, Loss: 1.048610, Accuracy: 72.46%\n",
      "Batch 151, Loss: 1.007141, Accuracy: 72.45%\n",
      "Batch 152, Loss: 0.977712, Accuracy: 72.49%\n",
      "Batch 153, Loss: 0.983342, Accuracy: 72.51%\n",
      "Batch 154, Loss: 1.026137, Accuracy: 72.49%\n",
      "Batch 155, Loss: 0.980193, Accuracy: 72.52%\n",
      "Batch 156, Loss: 1.041154, Accuracy: 72.51%\n",
      "Batch 157, Loss: 1.037237, Accuracy: 72.48%\n",
      "Batch 158, Loss: 0.971293, Accuracy: 72.51%\n",
      "Batch 159, Loss: 1.024848, Accuracy: 72.51%\n",
      "Batch 160, Loss: 0.979032, Accuracy: 72.53%\n",
      "Batch 161, Loss: 0.994519, Accuracy: 72.55%\n",
      "Batch 162, Loss: 0.970398, Accuracy: 72.58%\n",
      "Batch 163, Loss: 0.966910, Accuracy: 72.61%\n",
      "Batch 164, Loss: 1.009183, Accuracy: 72.62%\n",
      "Batch 165, Loss: 1.052901, Accuracy: 72.59%\n",
      "Batch 166, Loss: 0.991068, Accuracy: 72.61%\n",
      "Batch 167, Loss: 1.007163, Accuracy: 72.62%\n",
      "Batch 168, Loss: 0.982087, Accuracy: 72.66%\n",
      "Batch 169, Loss: 1.039897, Accuracy: 72.64%\n",
      "Batch 170, Loss: 0.987100, Accuracy: 72.67%\n",
      "Batch 171, Loss: 0.957735, Accuracy: 72.71%\n",
      "Batch 172, Loss: 1.054130, Accuracy: 72.68%\n",
      "Batch 173, Loss: 0.995439, Accuracy: 72.71%\n",
      "Batch 174, Loss: 0.978317, Accuracy: 72.73%\n",
      "Batch 175, Loss: 0.923024, Accuracy: 72.78%\n",
      "Batch 176, Loss: 0.953394, Accuracy: 72.81%\n",
      "Batch 177, Loss: 0.992032, Accuracy: 72.83%\n",
      "Batch 178, Loss: 0.935435, Accuracy: 72.88%\n",
      "Batch 179, Loss: 1.026036, Accuracy: 72.87%\n",
      "Batch 180, Loss: 1.031016, Accuracy: 72.86%\n",
      "Batch 181, Loss: 0.983543, Accuracy: 72.89%\n",
      "Batch 182, Loss: 0.989257, Accuracy: 72.90%\n",
      "Batch 183, Loss: 1.011826, Accuracy: 72.90%\n",
      "Batch 184, Loss: 0.916205, Accuracy: 72.94%\n",
      "Batch 185, Loss: 0.994368, Accuracy: 72.94%\n",
      "Batch 186, Loss: 0.975866, Accuracy: 72.96%\n",
      "Batch 187, Loss: 0.964003, Accuracy: 72.99%\n",
      "Batch 188, Loss: 0.967283, Accuracy: 73.01%\n",
      "Batch 189, Loss: 0.997885, Accuracy: 73.02%\n",
      "Batch 190, Loss: 1.134610, Accuracy: 72.95%\n",
      "Batch 191, Loss: 1.040710, Accuracy: 72.92%\n",
      "Batch 192, Loss: 0.950995, Accuracy: 72.96%\n",
      "Batch 193, Loss: 1.025833, Accuracy: 72.95%\n",
      "Batch 194, Loss: 1.032383, Accuracy: 72.94%\n",
      "Batch 195, Loss: 1.073488, Accuracy: 72.90%\n",
      "Batch 196, Loss: 0.994057, Accuracy: 72.91%\n",
      "Batch 197, Loss: 0.992301, Accuracy: 72.93%\n",
      "Batch 198, Loss: 1.024154, Accuracy: 72.92%\n",
      "Batch 199, Loss: 0.991643, Accuracy: 72.93%\n",
      "Batch 200, Loss: 1.103475, Accuracy: 72.88%\n",
      "Batch 201, Loss: 0.979136, Accuracy: 72.89%\n",
      "Batch 202, Loss: 1.072832, Accuracy: 72.86%\n",
      "Batch 203, Loss: 1.070865, Accuracy: 72.83%\n",
      "Batch 204, Loss: 0.977459, Accuracy: 72.86%\n",
      "Batch 205, Loss: 1.037042, Accuracy: 72.86%\n",
      "Batch 206, Loss: 1.076827, Accuracy: 72.82%\n",
      "Batch 207, Loss: 1.038038, Accuracy: 72.82%\n",
      "Batch 208, Loss: 1.054646, Accuracy: 72.81%\n",
      "Batch 209, Loss: 1.053206, Accuracy: 72.79%\n",
      "Batch 210, Loss: 1.055621, Accuracy: 72.78%\n",
      "Batch 211, Loss: 0.944482, Accuracy: 72.81%\n",
      "Batch 212, Loss: 0.941852, Accuracy: 72.84%\n",
      "Batch 213, Loss: 0.974637, Accuracy: 72.86%\n",
      "Training - Epoch 49, Loss: 1.013627, Accuracy: 72.86%\n",
      "Validation Batch 1, Loss: 1.028980, Accuracy: 70.31%\n",
      "Validation Batch 2, Loss: 1.142833, Accuracy: 64.06%\n",
      "Validation Batch 3, Loss: 1.132813, Accuracy: 63.02%\n",
      "Validation Batch 4, Loss: 1.041100, Accuracy: 64.84%\n",
      "Validation Batch 5, Loss: 1.083638, Accuracy: 64.69%\n",
      "Validation Batch 6, Loss: 1.025366, Accuracy: 66.15%\n",
      "Validation Batch 7, Loss: 1.074587, Accuracy: 66.29%\n",
      "Validation Batch 8, Loss: 1.097589, Accuracy: 66.02%\n",
      "Validation Batch 9, Loss: 1.123452, Accuracy: 65.45%\n",
      "Validation Batch 10, Loss: 1.087940, Accuracy: 65.62%\n",
      "Validation Batch 11, Loss: 1.066448, Accuracy: 65.62%\n",
      "Validation Batch 12, Loss: 1.014023, Accuracy: 66.41%\n",
      "Validation Batch 13, Loss: 1.144782, Accuracy: 66.11%\n",
      "Validation Batch 14, Loss: 1.092607, Accuracy: 65.96%\n",
      "Validation Batch 15, Loss: 1.071227, Accuracy: 66.04%\n",
      "Validation Batch 16, Loss: 1.048936, Accuracy: 66.21%\n",
      "Validation Batch 17, Loss: 1.152071, Accuracy: 65.62%\n",
      "Validation Batch 18, Loss: 1.037930, Accuracy: 65.89%\n",
      "Validation Batch 19, Loss: 1.124037, Accuracy: 65.62%\n",
      "Validation Batch 20, Loss: 1.074623, Accuracy: 65.70%\n",
      "Validation Batch 21, Loss: 1.079529, Accuracy: 65.70%\n",
      "Validation Batch 22, Loss: 1.122791, Accuracy: 65.48%\n",
      "Validation Batch 23, Loss: 1.179820, Accuracy: 65.08%\n",
      "Validation Batch 24, Loss: 1.112408, Accuracy: 65.04%\n",
      "Validation Batch 25, Loss: 1.069932, Accuracy: 65.00%\n",
      "Validation Batch 26, Loss: 1.054840, Accuracy: 65.14%\n",
      "Validation Batch 27, Loss: 1.086969, Accuracy: 65.12%\n",
      "Validation - Epoch 49, Loss: 1.087825, Accuracy: 65.12%\n",
      "Patienceâ€”1\n",
      "Epoch 50\n",
      "Batch 1, Loss: 1.121256, Accuracy: 64.06%\n",
      "Batch 2, Loss: 1.042777, Accuracy: 68.75%\n",
      "Batch 3, Loss: 1.024623, Accuracy: 69.27%\n",
      "Batch 4, Loss: 1.048462, Accuracy: 69.14%\n",
      "Batch 5, Loss: 0.978872, Accuracy: 70.31%\n",
      "Batch 6, Loss: 1.026461, Accuracy: 70.31%\n",
      "Batch 7, Loss: 1.069239, Accuracy: 70.09%\n",
      "Batch 8, Loss: 0.948749, Accuracy: 71.29%\n",
      "Batch 9, Loss: 0.986051, Accuracy: 71.70%\n",
      "Batch 10, Loss: 1.064938, Accuracy: 71.09%\n",
      "Batch 11, Loss: 1.020322, Accuracy: 71.31%\n",
      "Batch 12, Loss: 0.921606, Accuracy: 72.27%\n",
      "Batch 13, Loss: 1.040264, Accuracy: 72.12%\n",
      "Batch 14, Loss: 0.990435, Accuracy: 72.32%\n",
      "Batch 15, Loss: 1.063351, Accuracy: 71.98%\n",
      "Batch 16, Loss: 0.988118, Accuracy: 72.27%\n",
      "Batch 17, Loss: 1.009869, Accuracy: 72.24%\n",
      "Batch 18, Loss: 1.025647, Accuracy: 72.22%\n",
      "Batch 19, Loss: 1.018177, Accuracy: 72.20%\n",
      "Batch 20, Loss: 1.054351, Accuracy: 71.95%\n",
      "Batch 21, Loss: 0.953287, Accuracy: 72.25%\n",
      "Batch 22, Loss: 0.995596, Accuracy: 72.30%\n",
      "Batch 23, Loss: 1.048627, Accuracy: 72.08%\n",
      "Batch 24, Loss: 0.991039, Accuracy: 72.20%\n",
      "Batch 25, Loss: 0.930281, Accuracy: 72.56%\n",
      "Batch 26, Loss: 0.999106, Accuracy: 72.60%\n",
      "Batch 27, Loss: 0.975410, Accuracy: 72.74%\n",
      "Batch 28, Loss: 1.100562, Accuracy: 72.38%\n",
      "Batch 29, Loss: 1.016201, Accuracy: 72.36%\n",
      "Batch 30, Loss: 1.017035, Accuracy: 72.34%\n",
      "Batch 31, Loss: 1.025470, Accuracy: 72.28%\n",
      "Batch 32, Loss: 1.032920, Accuracy: 72.22%\n",
      "Batch 33, Loss: 1.109173, Accuracy: 71.88%\n",
      "Batch 34, Loss: 0.983750, Accuracy: 71.97%\n",
      "Batch 35, Loss: 0.997229, Accuracy: 72.05%\n",
      "Batch 36, Loss: 0.964144, Accuracy: 72.22%\n",
      "Batch 37, Loss: 0.968928, Accuracy: 72.42%\n",
      "Batch 38, Loss: 1.022295, Accuracy: 72.41%\n",
      "Batch 39, Loss: 0.869652, Accuracy: 72.84%\n",
      "Batch 40, Loss: 1.000805, Accuracy: 72.85%\n",
      "Batch 41, Loss: 0.982406, Accuracy: 72.98%\n",
      "Batch 42, Loss: 0.956341, Accuracy: 73.18%\n",
      "Batch 43, Loss: 1.043483, Accuracy: 73.11%\n",
      "Batch 44, Loss: 0.977696, Accuracy: 73.22%\n",
      "Batch 45, Loss: 1.122338, Accuracy: 72.99%\n",
      "Batch 46, Loss: 1.109056, Accuracy: 72.79%\n",
      "Batch 47, Loss: 0.964757, Accuracy: 72.91%\n",
      "Batch 48, Loss: 1.026645, Accuracy: 72.88%\n",
      "Batch 49, Loss: 0.939178, Accuracy: 73.05%\n",
      "Batch 50, Loss: 1.039890, Accuracy: 73.00%\n",
      "Batch 51, Loss: 0.991220, Accuracy: 73.07%\n",
      "Batch 52, Loss: 1.068074, Accuracy: 72.96%\n",
      "Batch 53, Loss: 1.005743, Accuracy: 72.97%\n",
      "Batch 54, Loss: 1.067168, Accuracy: 72.86%\n",
      "Batch 55, Loss: 1.075530, Accuracy: 72.73%\n",
      "Batch 56, Loss: 0.964672, Accuracy: 72.82%\n",
      "Batch 57, Loss: 0.997813, Accuracy: 72.86%\n",
      "Batch 58, Loss: 0.926549, Accuracy: 72.98%\n",
      "Batch 59, Loss: 1.077133, Accuracy: 72.88%\n",
      "Batch 60, Loss: 1.042435, Accuracy: 72.84%\n",
      "Batch 61, Loss: 0.991494, Accuracy: 72.87%\n",
      "Batch 62, Loss: 0.950737, Accuracy: 72.98%\n",
      "Batch 63, Loss: 0.985938, Accuracy: 73.04%\n",
      "Batch 64, Loss: 0.933923, Accuracy: 73.22%\n",
      "Batch 65, Loss: 0.895877, Accuracy: 73.39%\n",
      "Batch 66, Loss: 1.000749, Accuracy: 73.39%\n",
      "Batch 67, Loss: 0.986009, Accuracy: 73.41%\n",
      "Batch 68, Loss: 1.030174, Accuracy: 73.37%\n",
      "Batch 69, Loss: 1.092270, Accuracy: 73.21%\n",
      "Batch 70, Loss: 1.067938, Accuracy: 73.15%\n",
      "Batch 71, Loss: 0.983452, Accuracy: 73.17%\n",
      "Batch 72, Loss: 1.000579, Accuracy: 73.20%\n",
      "Batch 73, Loss: 1.076360, Accuracy: 73.12%\n",
      "Batch 74, Loss: 1.038955, Accuracy: 73.12%\n",
      "Batch 75, Loss: 1.050277, Accuracy: 73.08%\n",
      "Batch 76, Loss: 1.192039, Accuracy: 72.82%\n",
      "Batch 77, Loss: 1.074017, Accuracy: 72.71%\n",
      "Batch 78, Loss: 1.107426, Accuracy: 72.60%\n",
      "Batch 79, Loss: 0.992541, Accuracy: 72.65%\n",
      "Batch 80, Loss: 1.058752, Accuracy: 72.60%\n",
      "Batch 81, Loss: 1.039688, Accuracy: 72.55%\n",
      "Batch 82, Loss: 0.985950, Accuracy: 72.62%\n",
      "Batch 83, Loss: 1.102056, Accuracy: 72.50%\n",
      "Batch 84, Loss: 0.999102, Accuracy: 72.54%\n",
      "Batch 85, Loss: 1.001548, Accuracy: 72.56%\n",
      "Batch 86, Loss: 0.960373, Accuracy: 72.62%\n",
      "Batch 87, Loss: 1.053227, Accuracy: 72.56%\n",
      "Batch 88, Loss: 0.991762, Accuracy: 72.57%\n",
      "Batch 89, Loss: 0.927437, Accuracy: 72.68%\n",
      "Batch 90, Loss: 0.967164, Accuracy: 72.74%\n",
      "Batch 91, Loss: 0.969695, Accuracy: 72.80%\n",
      "Batch 92, Loss: 0.983123, Accuracy: 72.79%\n",
      "Batch 93, Loss: 0.996420, Accuracy: 72.82%\n",
      "Batch 94, Loss: 1.014397, Accuracy: 72.84%\n",
      "Batch 95, Loss: 0.999903, Accuracy: 72.86%\n",
      "Batch 96, Loss: 0.944008, Accuracy: 72.97%\n",
      "Batch 97, Loss: 0.983507, Accuracy: 72.99%\n",
      "Batch 98, Loss: 1.036382, Accuracy: 72.96%\n",
      "Batch 99, Loss: 0.929283, Accuracy: 73.04%\n",
      "Batch 100, Loss: 1.103189, Accuracy: 72.92%\n",
      "Batch 101, Loss: 1.089366, Accuracy: 72.85%\n",
      "Batch 102, Loss: 0.979751, Accuracy: 72.89%\n",
      "Batch 103, Loss: 0.945421, Accuracy: 72.97%\n",
      "Batch 104, Loss: 1.029898, Accuracy: 72.93%\n",
      "Batch 105, Loss: 0.995135, Accuracy: 72.92%\n",
      "Batch 106, Loss: 0.955489, Accuracy: 72.98%\n",
      "Batch 107, Loss: 0.929304, Accuracy: 73.09%\n",
      "Batch 108, Loss: 1.065250, Accuracy: 73.02%\n",
      "Batch 109, Loss: 1.018253, Accuracy: 73.01%\n",
      "Batch 110, Loss: 0.986034, Accuracy: 73.03%\n",
      "Batch 111, Loss: 1.004939, Accuracy: 73.03%\n",
      "Batch 112, Loss: 0.956457, Accuracy: 73.09%\n",
      "Batch 113, Loss: 0.982545, Accuracy: 73.12%\n",
      "Batch 114, Loss: 0.977360, Accuracy: 73.15%\n",
      "Batch 115, Loss: 1.028879, Accuracy: 73.14%\n",
      "Batch 116, Loss: 1.029415, Accuracy: 73.13%\n",
      "Batch 117, Loss: 1.075831, Accuracy: 73.05%\n",
      "Batch 118, Loss: 0.998502, Accuracy: 73.05%\n",
      "Batch 119, Loss: 0.957317, Accuracy: 73.11%\n",
      "Batch 120, Loss: 1.023490, Accuracy: 73.09%\n",
      "Batch 121, Loss: 0.933132, Accuracy: 73.15%\n",
      "Batch 122, Loss: 1.068662, Accuracy: 73.10%\n",
      "Batch 123, Loss: 1.038281, Accuracy: 73.09%\n",
      "Batch 124, Loss: 0.914058, Accuracy: 73.16%\n",
      "Batch 125, Loss: 1.007307, Accuracy: 73.15%\n",
      "Batch 126, Loss: 0.954019, Accuracy: 73.19%\n",
      "Batch 127, Loss: 0.971999, Accuracy: 73.23%\n",
      "Batch 128, Loss: 1.056736, Accuracy: 73.17%\n",
      "Batch 129, Loss: 0.931274, Accuracy: 73.22%\n",
      "Batch 130, Loss: 0.975547, Accuracy: 73.23%\n",
      "Batch 131, Loss: 0.930451, Accuracy: 73.29%\n",
      "Batch 132, Loss: 0.968175, Accuracy: 73.31%\n",
      "Batch 133, Loss: 1.029429, Accuracy: 73.27%\n",
      "Batch 134, Loss: 1.037169, Accuracy: 73.25%\n",
      "Batch 135, Loss: 0.977529, Accuracy: 73.26%\n",
      "Batch 136, Loss: 1.074470, Accuracy: 73.22%\n",
      "Batch 137, Loss: 1.038581, Accuracy: 73.20%\n",
      "Batch 138, Loss: 0.983800, Accuracy: 73.21%\n",
      "Batch 139, Loss: 0.968128, Accuracy: 73.25%\n",
      "Batch 140, Loss: 1.022286, Accuracy: 73.24%\n",
      "Batch 141, Loss: 1.012536, Accuracy: 73.24%\n",
      "Batch 142, Loss: 1.053588, Accuracy: 73.20%\n",
      "Batch 143, Loss: 1.064103, Accuracy: 73.18%\n",
      "Batch 144, Loss: 1.006431, Accuracy: 73.19%\n",
      "Batch 145, Loss: 1.006257, Accuracy: 73.19%\n",
      "Batch 146, Loss: 1.012527, Accuracy: 73.16%\n",
      "Batch 147, Loss: 1.046376, Accuracy: 73.14%\n",
      "Batch 148, Loss: 1.050309, Accuracy: 73.12%\n",
      "Batch 149, Loss: 1.015404, Accuracy: 73.09%\n",
      "Batch 150, Loss: 0.981127, Accuracy: 73.10%\n",
      "Batch 151, Loss: 1.008630, Accuracy: 73.11%\n",
      "Batch 152, Loss: 0.983995, Accuracy: 73.13%\n",
      "Batch 153, Loss: 0.966256, Accuracy: 73.17%\n",
      "Batch 154, Loss: 0.960622, Accuracy: 73.21%\n",
      "Batch 155, Loss: 1.010818, Accuracy: 73.23%\n",
      "Batch 156, Loss: 1.081027, Accuracy: 73.16%\n",
      "Batch 157, Loss: 0.945201, Accuracy: 73.20%\n",
      "Batch 158, Loss: 1.032706, Accuracy: 73.18%\n",
      "Batch 159, Loss: 1.000250, Accuracy: 73.19%\n",
      "Batch 160, Loss: 1.017758, Accuracy: 73.19%\n",
      "Batch 161, Loss: 0.984513, Accuracy: 73.20%\n",
      "Batch 162, Loss: 1.049896, Accuracy: 73.18%\n",
      "Batch 163, Loss: 1.017502, Accuracy: 73.17%\n",
      "Batch 164, Loss: 1.063202, Accuracy: 73.13%\n",
      "Batch 165, Loss: 0.941344, Accuracy: 73.17%\n",
      "Batch 166, Loss: 1.068009, Accuracy: 73.13%\n",
      "Batch 167, Loss: 1.065472, Accuracy: 73.09%\n",
      "Batch 168, Loss: 1.028888, Accuracy: 73.07%\n",
      "Batch 169, Loss: 1.069397, Accuracy: 73.04%\n",
      "Batch 170, Loss: 0.973271, Accuracy: 73.06%\n",
      "Batch 171, Loss: 1.036654, Accuracy: 73.04%\n",
      "Batch 172, Loss: 1.001381, Accuracy: 73.07%\n",
      "Batch 173, Loss: 1.003605, Accuracy: 73.08%\n",
      "Batch 174, Loss: 1.139025, Accuracy: 73.01%\n",
      "Batch 175, Loss: 0.956085, Accuracy: 73.04%\n",
      "Batch 176, Loss: 0.962551, Accuracy: 73.06%\n",
      "Batch 177, Loss: 0.968130, Accuracy: 73.09%\n",
      "Batch 178, Loss: 1.127054, Accuracy: 73.02%\n",
      "Batch 179, Loss: 1.074374, Accuracy: 72.99%\n",
      "Batch 180, Loss: 0.958356, Accuracy: 73.02%\n",
      "Batch 181, Loss: 1.039619, Accuracy: 73.00%\n",
      "Batch 182, Loss: 1.025924, Accuracy: 72.98%\n",
      "Batch 183, Loss: 1.013476, Accuracy: 72.97%\n",
      "Batch 184, Loss: 1.095639, Accuracy: 72.90%\n",
      "Batch 185, Loss: 1.012368, Accuracy: 72.91%\n",
      "Batch 186, Loss: 1.017799, Accuracy: 72.90%\n",
      "Batch 187, Loss: 1.033459, Accuracy: 72.87%\n",
      "Batch 188, Loss: 0.947116, Accuracy: 72.90%\n",
      "Batch 189, Loss: 1.000931, Accuracy: 72.91%\n",
      "Batch 190, Loss: 1.011693, Accuracy: 72.90%\n",
      "Batch 191, Loss: 0.968154, Accuracy: 72.92%\n",
      "Batch 192, Loss: 1.043294, Accuracy: 72.90%\n",
      "Batch 193, Loss: 1.072391, Accuracy: 72.86%\n",
      "Batch 194, Loss: 1.012251, Accuracy: 72.86%\n",
      "Batch 195, Loss: 1.028873, Accuracy: 72.86%\n",
      "Batch 196, Loss: 1.005675, Accuracy: 72.86%\n",
      "Batch 197, Loss: 0.923687, Accuracy: 72.91%\n",
      "Batch 198, Loss: 0.985063, Accuracy: 72.93%\n",
      "Batch 199, Loss: 0.962181, Accuracy: 72.95%\n",
      "Batch 200, Loss: 1.038948, Accuracy: 72.94%\n",
      "Batch 201, Loss: 1.033532, Accuracy: 72.93%\n",
      "Batch 202, Loss: 0.981996, Accuracy: 72.95%\n",
      "Batch 203, Loss: 0.949321, Accuracy: 72.99%\n",
      "Batch 204, Loss: 1.052094, Accuracy: 72.98%\n",
      "Batch 205, Loss: 1.004159, Accuracy: 72.98%\n",
      "Batch 206, Loss: 0.982810, Accuracy: 72.99%\n",
      "Batch 207, Loss: 1.118594, Accuracy: 72.94%\n",
      "Batch 208, Loss: 0.977379, Accuracy: 72.96%\n",
      "Batch 209, Loss: 1.014637, Accuracy: 72.95%\n",
      "Batch 210, Loss: 1.078667, Accuracy: 72.91%\n",
      "Batch 211, Loss: 1.005545, Accuracy: 72.90%\n",
      "Batch 212, Loss: 0.951622, Accuracy: 72.94%\n",
      "Batch 213, Loss: 0.976529, Accuracy: 72.96%\n",
      "Training - Epoch 50, Loss: 1.010989, Accuracy: 72.96%\n",
      "Validation Batch 1, Loss: 1.028343, Accuracy: 70.31%\n",
      "Validation Batch 2, Loss: 1.142903, Accuracy: 64.06%\n",
      "Validation Batch 3, Loss: 1.128502, Accuracy: 63.54%\n",
      "Validation Batch 4, Loss: 1.040869, Accuracy: 65.23%\n",
      "Validation Batch 5, Loss: 1.077124, Accuracy: 65.31%\n",
      "Validation Batch 6, Loss: 1.023952, Accuracy: 66.93%\n",
      "Validation Batch 7, Loss: 1.076323, Accuracy: 67.19%\n",
      "Validation Batch 8, Loss: 1.094477, Accuracy: 66.99%\n",
      "Validation Batch 9, Loss: 1.128414, Accuracy: 66.32%\n",
      "Validation Batch 10, Loss: 1.087359, Accuracy: 66.41%\n",
      "Validation Batch 11, Loss: 1.065576, Accuracy: 66.19%\n",
      "Validation Batch 12, Loss: 1.026801, Accuracy: 66.67%\n",
      "Validation Batch 13, Loss: 1.146652, Accuracy: 66.11%\n",
      "Validation Batch 14, Loss: 1.098310, Accuracy: 65.96%\n",
      "Validation Batch 15, Loss: 1.070591, Accuracy: 65.94%\n",
      "Validation Batch 16, Loss: 1.047091, Accuracy: 66.21%\n",
      "Validation Batch 17, Loss: 1.155385, Accuracy: 65.62%\n",
      "Validation Batch 18, Loss: 1.044150, Accuracy: 65.97%\n",
      "Validation Batch 19, Loss: 1.134650, Accuracy: 65.54%\n",
      "Validation Batch 20, Loss: 1.079260, Accuracy: 65.62%\n",
      "Validation Batch 21, Loss: 1.078312, Accuracy: 65.62%\n",
      "Validation Batch 22, Loss: 1.123454, Accuracy: 65.48%\n",
      "Validation Batch 23, Loss: 1.181520, Accuracy: 64.95%\n",
      "Validation Batch 24, Loss: 1.118750, Accuracy: 64.91%\n",
      "Validation Batch 25, Loss: 1.066767, Accuracy: 64.94%\n",
      "Validation Batch 26, Loss: 1.061997, Accuracy: 64.96%\n",
      "Validation Batch 27, Loss: 1.085250, Accuracy: 65.00%\n",
      "Validation - Epoch 50, Loss: 1.089362, Accuracy: 65.00%\n",
      "Patienceâ€”2\n",
      "Epoch 51\n",
      "Batch 1, Loss: 1.112223, Accuracy: 62.50%\n",
      "Batch 2, Loss: 0.993231, Accuracy: 67.97%\n",
      "Batch 3, Loss: 1.023743, Accuracy: 69.79%\n",
      "Batch 4, Loss: 1.021260, Accuracy: 70.70%\n",
      "Batch 5, Loss: 0.967287, Accuracy: 71.88%\n",
      "Batch 6, Loss: 0.918750, Accuracy: 73.96%\n",
      "Batch 7, Loss: 0.977281, Accuracy: 74.55%\n",
      "Batch 8, Loss: 1.034503, Accuracy: 73.83%\n",
      "Batch 9, Loss: 0.945843, Accuracy: 74.48%\n",
      "Batch 10, Loss: 0.931220, Accuracy: 75.31%\n",
      "Batch 11, Loss: 1.030078, Accuracy: 74.86%\n",
      "Batch 12, Loss: 0.981038, Accuracy: 75.00%\n",
      "Batch 13, Loss: 1.106167, Accuracy: 74.16%\n",
      "Batch 14, Loss: 1.098374, Accuracy: 73.44%\n",
      "Batch 15, Loss: 1.025672, Accuracy: 73.33%\n",
      "Batch 16, Loss: 1.026586, Accuracy: 73.34%\n",
      "Batch 17, Loss: 1.045305, Accuracy: 73.07%\n",
      "Batch 18, Loss: 0.966754, Accuracy: 73.35%\n",
      "Batch 19, Loss: 0.991116, Accuracy: 73.36%\n",
      "Batch 20, Loss: 1.039652, Accuracy: 73.20%\n",
      "Batch 21, Loss: 0.998043, Accuracy: 73.14%\n",
      "Batch 22, Loss: 0.998681, Accuracy: 73.22%\n",
      "Batch 23, Loss: 1.075264, Accuracy: 73.03%\n",
      "Batch 24, Loss: 1.020492, Accuracy: 72.98%\n",
      "Batch 25, Loss: 1.090218, Accuracy: 72.62%\n",
      "Batch 26, Loss: 1.068146, Accuracy: 72.36%\n",
      "Batch 27, Loss: 1.022730, Accuracy: 72.34%\n",
      "Batch 28, Loss: 0.975061, Accuracy: 72.43%\n",
      "Batch 29, Loss: 1.064228, Accuracy: 72.25%\n",
      "Batch 30, Loss: 1.018485, Accuracy: 72.24%\n",
      "Batch 31, Loss: 1.059556, Accuracy: 72.13%\n",
      "Batch 32, Loss: 0.972174, Accuracy: 72.31%\n",
      "Batch 33, Loss: 1.082674, Accuracy: 72.11%\n",
      "Batch 34, Loss: 0.986026, Accuracy: 72.24%\n",
      "Batch 35, Loss: 0.978576, Accuracy: 72.41%\n",
      "Batch 36, Loss: 1.000917, Accuracy: 72.40%\n",
      "Batch 37, Loss: 0.878384, Accuracy: 72.72%\n",
      "Batch 38, Loss: 0.991405, Accuracy: 72.82%\n",
      "Batch 39, Loss: 1.052570, Accuracy: 72.72%\n",
      "Batch 40, Loss: 1.124728, Accuracy: 72.38%\n",
      "Batch 41, Loss: 1.001481, Accuracy: 72.41%\n",
      "Batch 42, Loss: 0.968657, Accuracy: 72.51%\n",
      "Batch 43, Loss: 1.019125, Accuracy: 72.49%\n",
      "Batch 44, Loss: 0.981011, Accuracy: 72.55%\n",
      "Batch 45, Loss: 1.106043, Accuracy: 72.29%\n",
      "Batch 46, Loss: 0.980246, Accuracy: 72.38%\n",
      "Batch 47, Loss: 1.001147, Accuracy: 72.34%\n",
      "Batch 48, Loss: 1.038681, Accuracy: 72.33%\n",
      "Batch 49, Loss: 1.100108, Accuracy: 72.16%\n",
      "Batch 50, Loss: 1.036725, Accuracy: 72.12%\n",
      "Batch 51, Loss: 1.067271, Accuracy: 72.09%\n",
      "Batch 52, Loss: 0.954210, Accuracy: 72.24%\n",
      "Batch 53, Loss: 1.037657, Accuracy: 72.20%\n",
      "Batch 54, Loss: 0.948461, Accuracy: 72.34%\n",
      "Batch 55, Loss: 1.014199, Accuracy: 72.39%\n",
      "Batch 56, Loss: 0.952574, Accuracy: 72.49%\n",
      "Batch 57, Loss: 1.091784, Accuracy: 72.34%\n",
      "Batch 58, Loss: 0.970990, Accuracy: 72.39%\n",
      "Batch 59, Loss: 0.989462, Accuracy: 72.46%\n",
      "Batch 60, Loss: 1.079746, Accuracy: 72.34%\n",
      "Batch 61, Loss: 0.956532, Accuracy: 72.46%\n",
      "Batch 62, Loss: 0.963864, Accuracy: 72.58%\n",
      "Batch 63, Loss: 1.017945, Accuracy: 72.57%\n",
      "Batch 64, Loss: 1.022605, Accuracy: 72.53%\n",
      "Batch 65, Loss: 1.003000, Accuracy: 72.57%\n",
      "Batch 66, Loss: 1.062676, Accuracy: 72.51%\n",
      "Batch 67, Loss: 0.995268, Accuracy: 72.53%\n",
      "Batch 68, Loss: 1.057546, Accuracy: 72.43%\n",
      "Batch 69, Loss: 1.001892, Accuracy: 72.42%\n",
      "Batch 70, Loss: 1.026222, Accuracy: 72.43%\n",
      "Batch 71, Loss: 1.007088, Accuracy: 72.47%\n",
      "Batch 72, Loss: 1.090438, Accuracy: 72.37%\n",
      "Batch 73, Loss: 1.068614, Accuracy: 72.26%\n",
      "Batch 74, Loss: 0.991590, Accuracy: 72.28%\n",
      "Batch 75, Loss: 1.024266, Accuracy: 72.29%\n",
      "Batch 76, Loss: 1.021443, Accuracy: 72.27%\n",
      "Batch 77, Loss: 1.009201, Accuracy: 72.28%\n",
      "Batch 78, Loss: 0.971344, Accuracy: 72.36%\n",
      "Batch 79, Loss: 1.039877, Accuracy: 72.33%\n",
      "Batch 80, Loss: 1.036615, Accuracy: 72.32%\n",
      "Batch 81, Loss: 1.052777, Accuracy: 72.30%\n",
      "Batch 82, Loss: 0.938661, Accuracy: 72.39%\n",
      "Batch 83, Loss: 1.061890, Accuracy: 72.33%\n",
      "Batch 84, Loss: 1.028354, Accuracy: 72.32%\n",
      "Batch 85, Loss: 0.976549, Accuracy: 72.37%\n",
      "Batch 86, Loss: 0.932474, Accuracy: 72.46%\n",
      "Batch 87, Loss: 1.002498, Accuracy: 72.47%\n",
      "Batch 88, Loss: 1.038016, Accuracy: 72.46%\n",
      "Batch 89, Loss: 0.927349, Accuracy: 72.56%\n",
      "Batch 90, Loss: 0.966963, Accuracy: 72.64%\n",
      "Batch 91, Loss: 0.975403, Accuracy: 72.70%\n",
      "Batch 92, Loss: 1.033097, Accuracy: 72.66%\n",
      "Batch 93, Loss: 0.998090, Accuracy: 72.66%\n",
      "Batch 94, Loss: 1.116809, Accuracy: 72.54%\n",
      "Batch 95, Loss: 0.965210, Accuracy: 72.60%\n",
      "Batch 96, Loss: 0.941178, Accuracy: 72.67%\n",
      "Batch 97, Loss: 1.044499, Accuracy: 72.62%\n",
      "Batch 98, Loss: 1.019057, Accuracy: 72.58%\n",
      "Batch 99, Loss: 0.989656, Accuracy: 72.62%\n",
      "Batch 100, Loss: 0.938999, Accuracy: 72.70%\n",
      "Batch 101, Loss: 1.075742, Accuracy: 72.63%\n",
      "Batch 102, Loss: 0.925434, Accuracy: 72.72%\n",
      "Batch 103, Loss: 0.996977, Accuracy: 72.72%\n",
      "Batch 104, Loss: 1.052205, Accuracy: 72.69%\n",
      "Batch 105, Loss: 0.903703, Accuracy: 72.80%\n",
      "Batch 106, Loss: 0.967296, Accuracy: 72.86%\n",
      "Batch 107, Loss: 0.991493, Accuracy: 72.87%\n",
      "Batch 108, Loss: 0.883139, Accuracy: 73.02%\n",
      "Batch 109, Loss: 0.957215, Accuracy: 73.05%\n",
      "Batch 110, Loss: 0.996793, Accuracy: 73.05%\n",
      "Batch 111, Loss: 0.975964, Accuracy: 73.07%\n",
      "Batch 112, Loss: 1.044103, Accuracy: 73.03%\n",
      "Batch 113, Loss: 0.977457, Accuracy: 73.06%\n",
      "Batch 114, Loss: 1.066346, Accuracy: 73.00%\n",
      "Batch 115, Loss: 0.979330, Accuracy: 73.04%\n",
      "Batch 116, Loss: 1.050130, Accuracy: 73.02%\n",
      "Batch 117, Loss: 0.992461, Accuracy: 73.04%\n",
      "Batch 118, Loss: 0.925186, Accuracy: 73.11%\n",
      "Batch 119, Loss: 1.029181, Accuracy: 73.07%\n",
      "Batch 120, Loss: 1.002568, Accuracy: 73.07%\n",
      "Batch 121, Loss: 1.103311, Accuracy: 73.01%\n",
      "Batch 122, Loss: 1.000245, Accuracy: 73.03%\n",
      "Batch 123, Loss: 0.977541, Accuracy: 73.07%\n",
      "Batch 124, Loss: 1.108020, Accuracy: 72.98%\n",
      "Batch 125, Loss: 0.941457, Accuracy: 73.05%\n",
      "Batch 126, Loss: 0.987438, Accuracy: 73.07%\n",
      "Batch 127, Loss: 1.101708, Accuracy: 73.01%\n",
      "Batch 128, Loss: 1.024777, Accuracy: 73.01%\n",
      "Batch 129, Loss: 0.966132, Accuracy: 73.05%\n",
      "Batch 130, Loss: 1.077427, Accuracy: 73.00%\n",
      "Batch 131, Loss: 1.070973, Accuracy: 72.95%\n",
      "Batch 132, Loss: 1.012109, Accuracy: 72.94%\n",
      "Batch 133, Loss: 0.895249, Accuracy: 73.05%\n",
      "Batch 134, Loss: 1.006233, Accuracy: 73.04%\n",
      "Batch 135, Loss: 0.983453, Accuracy: 73.04%\n",
      "Batch 136, Loss: 0.988600, Accuracy: 73.07%\n",
      "Batch 137, Loss: 1.061761, Accuracy: 73.05%\n",
      "Batch 138, Loss: 0.952326, Accuracy: 73.10%\n",
      "Batch 139, Loss: 1.121432, Accuracy: 73.02%\n",
      "Batch 140, Loss: 1.048091, Accuracy: 73.00%\n",
      "Batch 141, Loss: 1.040913, Accuracy: 72.98%\n",
      "Batch 142, Loss: 1.114590, Accuracy: 72.92%\n",
      "Batch 143, Loss: 1.033483, Accuracy: 72.92%\n",
      "Batch 144, Loss: 1.030524, Accuracy: 72.91%\n",
      "Batch 145, Loss: 1.054815, Accuracy: 72.87%\n",
      "Batch 146, Loss: 0.984400, Accuracy: 72.88%\n",
      "Batch 147, Loss: 1.000857, Accuracy: 72.90%\n",
      "Batch 148, Loss: 1.033067, Accuracy: 72.88%\n",
      "Batch 149, Loss: 1.049582, Accuracy: 72.85%\n",
      "Batch 150, Loss: 0.933371, Accuracy: 72.91%\n",
      "Batch 151, Loss: 1.019886, Accuracy: 72.90%\n",
      "Batch 152, Loss: 1.059478, Accuracy: 72.87%\n",
      "Batch 153, Loss: 0.849413, Accuracy: 72.99%\n",
      "Batch 154, Loss: 1.088269, Accuracy: 72.94%\n",
      "Batch 155, Loss: 1.023718, Accuracy: 72.92%\n",
      "Batch 156, Loss: 0.951293, Accuracy: 72.96%\n",
      "Batch 157, Loss: 1.066148, Accuracy: 72.92%\n",
      "Batch 158, Loss: 1.003890, Accuracy: 72.93%\n",
      "Batch 159, Loss: 1.030103, Accuracy: 72.93%\n",
      "Batch 160, Loss: 0.993719, Accuracy: 72.96%\n",
      "Batch 161, Loss: 1.032472, Accuracy: 72.94%\n",
      "Batch 162, Loss: 1.012884, Accuracy: 72.95%\n",
      "Batch 163, Loss: 1.036332, Accuracy: 72.94%\n",
      "Batch 164, Loss: 1.034150, Accuracy: 72.93%\n",
      "Batch 165, Loss: 1.026859, Accuracy: 72.94%\n",
      "Batch 166, Loss: 1.004706, Accuracy: 72.96%\n",
      "Batch 167, Loss: 0.936502, Accuracy: 73.01%\n",
      "Batch 168, Loss: 0.971342, Accuracy: 73.03%\n",
      "Batch 169, Loss: 1.006670, Accuracy: 73.03%\n",
      "Batch 170, Loss: 1.036814, Accuracy: 73.03%\n",
      "Batch 171, Loss: 0.985753, Accuracy: 73.04%\n",
      "Batch 172, Loss: 1.065874, Accuracy: 73.01%\n",
      "Batch 173, Loss: 1.018762, Accuracy: 73.01%\n",
      "Batch 174, Loss: 1.009105, Accuracy: 73.02%\n",
      "Batch 175, Loss: 0.992985, Accuracy: 73.04%\n",
      "Batch 176, Loss: 1.070205, Accuracy: 72.99%\n",
      "Batch 177, Loss: 0.974554, Accuracy: 73.02%\n",
      "Batch 178, Loss: 0.959954, Accuracy: 73.06%\n",
      "Batch 179, Loss: 1.005277, Accuracy: 73.06%\n",
      "Batch 180, Loss: 0.982980, Accuracy: 73.09%\n",
      "Batch 181, Loss: 0.976739, Accuracy: 73.10%\n",
      "Batch 182, Loss: 1.028595, Accuracy: 73.09%\n",
      "Batch 183, Loss: 1.018557, Accuracy: 73.07%\n",
      "Batch 184, Loss: 1.110492, Accuracy: 73.01%\n",
      "Batch 185, Loss: 1.058700, Accuracy: 72.98%\n",
      "Batch 186, Loss: 0.905236, Accuracy: 73.04%\n",
      "Batch 187, Loss: 0.941143, Accuracy: 73.09%\n",
      "Batch 188, Loss: 1.037573, Accuracy: 73.08%\n",
      "Batch 189, Loss: 1.030058, Accuracy: 73.07%\n",
      "Batch 190, Loss: 0.992189, Accuracy: 73.08%\n",
      "Batch 191, Loss: 0.950054, Accuracy: 73.12%\n",
      "Batch 192, Loss: 1.030005, Accuracy: 73.12%\n",
      "Batch 193, Loss: 1.020793, Accuracy: 73.11%\n",
      "Batch 194, Loss: 0.929845, Accuracy: 73.16%\n",
      "Batch 195, Loss: 1.007622, Accuracy: 73.17%\n",
      "Batch 196, Loss: 0.966381, Accuracy: 73.19%\n",
      "Batch 197, Loss: 1.075588, Accuracy: 73.15%\n",
      "Batch 198, Loss: 1.000180, Accuracy: 73.17%\n",
      "Batch 199, Loss: 1.033524, Accuracy: 73.15%\n",
      "Batch 200, Loss: 0.990480, Accuracy: 73.16%\n",
      "Batch 201, Loss: 1.054357, Accuracy: 73.13%\n",
      "Batch 202, Loss: 1.010855, Accuracy: 73.14%\n",
      "Batch 203, Loss: 1.055866, Accuracy: 73.11%\n",
      "Batch 204, Loss: 1.043270, Accuracy: 73.09%\n",
      "Batch 205, Loss: 1.000470, Accuracy: 73.10%\n",
      "Batch 206, Loss: 0.985690, Accuracy: 73.12%\n",
      "Batch 207, Loss: 1.007305, Accuracy: 73.12%\n",
      "Batch 208, Loss: 0.985193, Accuracy: 73.14%\n",
      "Batch 209, Loss: 1.059435, Accuracy: 73.12%\n",
      "Batch 210, Loss: 1.037778, Accuracy: 73.10%\n",
      "Batch 211, Loss: 1.022774, Accuracy: 73.10%\n",
      "Batch 212, Loss: 0.947027, Accuracy: 73.14%\n",
      "Batch 213, Loss: 1.035953, Accuracy: 73.12%\n",
      "Training - Epoch 51, Loss: 1.011484, Accuracy: 73.12%\n",
      "Validation Batch 1, Loss: 1.010446, Accuracy: 71.88%\n",
      "Validation Batch 2, Loss: 1.128108, Accuracy: 65.62%\n",
      "Validation Batch 3, Loss: 1.119246, Accuracy: 65.10%\n",
      "Validation Batch 4, Loss: 1.033942, Accuracy: 66.41%\n",
      "Validation Batch 5, Loss: 1.058865, Accuracy: 66.88%\n",
      "Validation Batch 6, Loss: 1.008357, Accuracy: 68.23%\n",
      "Validation Batch 7, Loss: 1.069509, Accuracy: 68.30%\n",
      "Validation Batch 8, Loss: 1.093040, Accuracy: 67.97%\n",
      "Validation Batch 9, Loss: 1.116869, Accuracy: 67.36%\n",
      "Validation Batch 10, Loss: 1.077319, Accuracy: 67.34%\n",
      "Validation Batch 11, Loss: 1.056134, Accuracy: 67.33%\n",
      "Validation Batch 12, Loss: 1.005118, Accuracy: 67.97%\n",
      "Validation Batch 13, Loss: 1.136256, Accuracy: 67.55%\n",
      "Validation Batch 14, Loss: 1.085325, Accuracy: 67.41%\n",
      "Validation Batch 15, Loss: 1.059592, Accuracy: 67.40%\n",
      "Validation Batch 16, Loss: 1.035064, Accuracy: 67.58%\n",
      "Validation Batch 17, Loss: 1.136465, Accuracy: 67.10%\n",
      "Validation Batch 18, Loss: 1.033414, Accuracy: 67.27%\n",
      "Validation Batch 19, Loss: 1.118108, Accuracy: 67.02%\n",
      "Validation Batch 20, Loss: 1.063592, Accuracy: 67.11%\n",
      "Validation Batch 21, Loss: 1.073599, Accuracy: 66.96%\n",
      "Validation Batch 22, Loss: 1.107928, Accuracy: 66.90%\n",
      "Validation Batch 23, Loss: 1.167993, Accuracy: 66.58%\n",
      "Validation Batch 24, Loss: 1.107642, Accuracy: 66.47%\n",
      "Validation Batch 25, Loss: 1.053593, Accuracy: 66.50%\n",
      "Validation Batch 26, Loss: 1.050018, Accuracy: 66.65%\n",
      "Validation Batch 27, Loss: 1.075321, Accuracy: 66.71%\n",
      "Validation - Epoch 51, Loss: 1.077069, Accuracy: 66.71%\n",
      "Patienceâ€”0\n",
      "Epoch 52\n",
      "Batch 1, Loss: 1.013131, Accuracy: 73.44%\n",
      "Batch 2, Loss: 1.010971, Accuracy: 71.88%\n",
      "Batch 3, Loss: 1.027471, Accuracy: 71.88%\n",
      "Batch 4, Loss: 1.019944, Accuracy: 71.48%\n",
      "Batch 5, Loss: 0.954797, Accuracy: 73.44%\n",
      "Batch 6, Loss: 1.040770, Accuracy: 72.66%\n",
      "Batch 7, Loss: 0.988922, Accuracy: 72.99%\n",
      "Batch 8, Loss: 1.073731, Accuracy: 72.27%\n",
      "Batch 9, Loss: 0.972699, Accuracy: 72.92%\n",
      "Batch 10, Loss: 1.064572, Accuracy: 72.50%\n",
      "Batch 11, Loss: 0.923710, Accuracy: 73.58%\n",
      "Batch 12, Loss: 1.005687, Accuracy: 73.70%\n",
      "Batch 13, Loss: 0.978068, Accuracy: 74.04%\n",
      "Batch 14, Loss: 1.026696, Accuracy: 73.77%\n",
      "Batch 15, Loss: 1.053866, Accuracy: 73.54%\n",
      "Batch 16, Loss: 1.074428, Accuracy: 72.95%\n",
      "Batch 17, Loss: 0.935690, Accuracy: 73.44%\n",
      "Batch 18, Loss: 1.031990, Accuracy: 73.44%\n",
      "Batch 19, Loss: 0.967643, Accuracy: 73.60%\n",
      "Batch 20, Loss: 0.978821, Accuracy: 73.83%\n",
      "Batch 21, Loss: 1.071625, Accuracy: 73.44%\n",
      "Batch 22, Loss: 1.082102, Accuracy: 73.22%\n",
      "Batch 23, Loss: 1.000774, Accuracy: 73.23%\n",
      "Batch 24, Loss: 0.984583, Accuracy: 73.37%\n",
      "Batch 25, Loss: 1.025738, Accuracy: 73.31%\n",
      "Batch 26, Loss: 1.008662, Accuracy: 73.32%\n",
      "Batch 27, Loss: 1.031438, Accuracy: 73.21%\n",
      "Batch 28, Loss: 0.995707, Accuracy: 73.38%\n",
      "Batch 29, Loss: 1.000338, Accuracy: 73.49%\n",
      "Batch 30, Loss: 1.088531, Accuracy: 73.18%\n",
      "Batch 31, Loss: 1.037434, Accuracy: 73.08%\n",
      "Batch 32, Loss: 0.943277, Accuracy: 73.19%\n",
      "Batch 33, Loss: 1.063700, Accuracy: 73.01%\n",
      "Batch 34, Loss: 0.976155, Accuracy: 73.12%\n",
      "Batch 35, Loss: 0.969455, Accuracy: 73.21%\n",
      "Batch 36, Loss: 1.045059, Accuracy: 73.13%\n",
      "Batch 37, Loss: 1.000471, Accuracy: 73.18%\n",
      "Batch 38, Loss: 0.983383, Accuracy: 73.23%\n",
      "Batch 39, Loss: 0.996921, Accuracy: 73.20%\n",
      "Batch 40, Loss: 0.990738, Accuracy: 73.24%\n",
      "Batch 41, Loss: 0.979113, Accuracy: 73.32%\n",
      "Batch 42, Loss: 1.054423, Accuracy: 73.21%\n",
      "Batch 43, Loss: 0.967569, Accuracy: 73.29%\n",
      "Batch 44, Loss: 0.935099, Accuracy: 73.47%\n",
      "Batch 45, Loss: 0.968723, Accuracy: 73.58%\n",
      "Batch 46, Loss: 0.998837, Accuracy: 73.61%\n",
      "Batch 47, Loss: 1.031952, Accuracy: 73.54%\n",
      "Batch 48, Loss: 1.015212, Accuracy: 73.54%\n",
      "Batch 49, Loss: 1.054342, Accuracy: 73.41%\n",
      "Batch 50, Loss: 1.021480, Accuracy: 73.41%\n",
      "Batch 51, Loss: 0.980752, Accuracy: 73.47%\n",
      "Batch 52, Loss: 0.986226, Accuracy: 73.53%\n",
      "Batch 53, Loss: 1.098038, Accuracy: 73.35%\n",
      "Batch 54, Loss: 0.968285, Accuracy: 73.41%\n",
      "Batch 55, Loss: 1.016600, Accuracy: 73.38%\n",
      "Batch 56, Loss: 1.002868, Accuracy: 73.44%\n",
      "Batch 57, Loss: 1.036518, Accuracy: 73.41%\n",
      "Batch 58, Loss: 0.948491, Accuracy: 73.46%\n",
      "Batch 59, Loss: 1.010177, Accuracy: 73.44%\n",
      "Batch 60, Loss: 1.012077, Accuracy: 73.46%\n",
      "Batch 61, Loss: 1.029674, Accuracy: 73.44%\n",
      "Batch 62, Loss: 1.004088, Accuracy: 73.46%\n",
      "Batch 63, Loss: 0.974110, Accuracy: 73.51%\n",
      "Batch 64, Loss: 0.983954, Accuracy: 73.51%\n",
      "Batch 65, Loss: 0.967153, Accuracy: 73.56%\n",
      "Batch 66, Loss: 1.020372, Accuracy: 73.53%\n",
      "Batch 67, Loss: 1.112711, Accuracy: 73.34%\n",
      "Batch 68, Loss: 1.017283, Accuracy: 73.30%\n",
      "Batch 69, Loss: 1.027903, Accuracy: 73.28%\n",
      "Batch 70, Loss: 1.026999, Accuracy: 73.24%\n",
      "Batch 71, Loss: 1.058025, Accuracy: 73.13%\n",
      "Batch 72, Loss: 1.041883, Accuracy: 73.07%\n",
      "Batch 73, Loss: 1.042636, Accuracy: 73.01%\n",
      "Batch 74, Loss: 1.045576, Accuracy: 72.97%\n",
      "Batch 75, Loss: 0.950757, Accuracy: 73.04%\n",
      "Batch 76, Loss: 1.040546, Accuracy: 72.96%\n",
      "Batch 77, Loss: 1.006388, Accuracy: 72.99%\n",
      "Batch 78, Loss: 0.979717, Accuracy: 73.04%\n",
      "Batch 79, Loss: 1.052314, Accuracy: 72.96%\n",
      "Batch 80, Loss: 1.015718, Accuracy: 72.95%\n",
      "Batch 81, Loss: 1.083959, Accuracy: 72.86%\n",
      "Batch 82, Loss: 0.983807, Accuracy: 72.88%\n",
      "Batch 83, Loss: 1.034911, Accuracy: 72.84%\n",
      "Batch 84, Loss: 0.961837, Accuracy: 72.92%\n",
      "Batch 85, Loss: 1.039870, Accuracy: 72.89%\n",
      "Batch 86, Loss: 1.015556, Accuracy: 72.91%\n",
      "Batch 87, Loss: 1.040644, Accuracy: 72.90%\n",
      "Batch 88, Loss: 1.031376, Accuracy: 72.89%\n",
      "Batch 89, Loss: 1.031439, Accuracy: 72.86%\n",
      "Batch 90, Loss: 1.068214, Accuracy: 72.78%\n",
      "Batch 91, Loss: 0.972203, Accuracy: 72.82%\n",
      "Batch 92, Loss: 0.985514, Accuracy: 72.84%\n",
      "Batch 93, Loss: 0.970584, Accuracy: 72.90%\n",
      "Batch 94, Loss: 1.021661, Accuracy: 72.91%\n",
      "Batch 95, Loss: 1.092438, Accuracy: 72.80%\n",
      "Batch 96, Loss: 1.021745, Accuracy: 72.80%\n",
      "Batch 97, Loss: 1.095671, Accuracy: 72.70%\n",
      "Batch 98, Loss: 0.972081, Accuracy: 72.75%\n",
      "Batch 99, Loss: 0.965016, Accuracy: 72.82%\n",
      "Batch 100, Loss: 1.018205, Accuracy: 72.81%\n",
      "Batch 101, Loss: 1.054136, Accuracy: 72.77%\n",
      "Batch 102, Loss: 1.039711, Accuracy: 72.73%\n",
      "Batch 103, Loss: 0.974187, Accuracy: 72.77%\n",
      "Batch 104, Loss: 0.949286, Accuracy: 72.85%\n",
      "Batch 105, Loss: 1.029070, Accuracy: 72.84%\n",
      "Batch 106, Loss: 1.086368, Accuracy: 72.76%\n",
      "Batch 107, Loss: 1.005813, Accuracy: 72.78%\n",
      "Batch 108, Loss: 1.047159, Accuracy: 72.76%\n",
      "Batch 109, Loss: 0.976631, Accuracy: 72.81%\n",
      "Batch 110, Loss: 0.977214, Accuracy: 72.83%\n",
      "Batch 111, Loss: 0.975645, Accuracy: 72.89%\n",
      "Batch 112, Loss: 1.006354, Accuracy: 72.89%\n",
      "Batch 113, Loss: 0.991152, Accuracy: 72.90%\n",
      "Batch 114, Loss: 0.993925, Accuracy: 72.90%\n",
      "Batch 115, Loss: 1.024030, Accuracy: 72.89%\n",
      "Batch 116, Loss: 1.045718, Accuracy: 72.87%\n",
      "Batch 117, Loss: 0.926145, Accuracy: 72.96%\n",
      "Batch 118, Loss: 0.996331, Accuracy: 72.99%\n",
      "Batch 119, Loss: 1.054916, Accuracy: 72.94%\n",
      "Batch 120, Loss: 1.007178, Accuracy: 72.93%\n",
      "Batch 121, Loss: 0.964298, Accuracy: 72.96%\n",
      "Batch 122, Loss: 1.005137, Accuracy: 73.00%\n",
      "Batch 123, Loss: 1.060327, Accuracy: 72.95%\n",
      "Batch 124, Loss: 1.094011, Accuracy: 72.90%\n",
      "Batch 125, Loss: 0.992984, Accuracy: 72.91%\n",
      "Batch 126, Loss: 1.017905, Accuracy: 72.90%\n",
      "Batch 127, Loss: 1.046000, Accuracy: 72.87%\n",
      "Batch 128, Loss: 0.925729, Accuracy: 72.95%\n",
      "Batch 129, Loss: 0.967357, Accuracy: 72.98%\n",
      "Batch 130, Loss: 0.957921, Accuracy: 73.02%\n",
      "Batch 131, Loss: 0.970546, Accuracy: 73.06%\n",
      "Batch 132, Loss: 0.933879, Accuracy: 73.12%\n",
      "Batch 133, Loss: 0.962806, Accuracy: 73.16%\n",
      "Batch 134, Loss: 0.964130, Accuracy: 73.19%\n",
      "Batch 135, Loss: 1.153876, Accuracy: 73.08%\n",
      "Batch 136, Loss: 1.007948, Accuracy: 73.09%\n",
      "Batch 137, Loss: 1.072365, Accuracy: 73.05%\n",
      "Batch 138, Loss: 0.994975, Accuracy: 73.08%\n",
      "Batch 139, Loss: 0.987672, Accuracy: 73.11%\n",
      "Batch 140, Loss: 0.994549, Accuracy: 73.11%\n",
      "Batch 141, Loss: 1.001703, Accuracy: 73.12%\n",
      "Batch 142, Loss: 1.046642, Accuracy: 73.10%\n",
      "Batch 143, Loss: 1.047673, Accuracy: 73.08%\n",
      "Batch 144, Loss: 0.996234, Accuracy: 73.08%\n",
      "Batch 145, Loss: 0.996377, Accuracy: 73.06%\n",
      "Batch 146, Loss: 1.056417, Accuracy: 73.03%\n",
      "Batch 147, Loss: 0.982442, Accuracy: 73.04%\n",
      "Batch 148, Loss: 1.003433, Accuracy: 73.05%\n",
      "Batch 149, Loss: 0.944896, Accuracy: 73.10%\n",
      "Batch 150, Loss: 0.983988, Accuracy: 73.12%\n",
      "Batch 151, Loss: 1.081274, Accuracy: 73.08%\n",
      "Batch 152, Loss: 1.027214, Accuracy: 73.07%\n",
      "Batch 153, Loss: 0.947672, Accuracy: 73.11%\n",
      "Batch 154, Loss: 1.061885, Accuracy: 73.07%\n",
      "Batch 155, Loss: 0.953913, Accuracy: 73.11%\n",
      "Batch 156, Loss: 0.979837, Accuracy: 73.13%\n",
      "Batch 157, Loss: 0.998700, Accuracy: 73.14%\n",
      "Batch 158, Loss: 1.003745, Accuracy: 73.14%\n",
      "Batch 159, Loss: 1.004108, Accuracy: 73.14%\n",
      "Batch 160, Loss: 0.981221, Accuracy: 73.15%\n",
      "Batch 161, Loss: 0.993985, Accuracy: 73.17%\n",
      "Batch 162, Loss: 0.977375, Accuracy: 73.19%\n",
      "Batch 163, Loss: 0.909924, Accuracy: 73.25%\n",
      "Batch 164, Loss: 1.043370, Accuracy: 73.23%\n",
      "Batch 165, Loss: 1.014728, Accuracy: 73.21%\n",
      "Batch 166, Loss: 1.033734, Accuracy: 73.19%\n",
      "Batch 167, Loss: 0.922308, Accuracy: 73.25%\n",
      "Batch 168, Loss: 0.917019, Accuracy: 73.31%\n",
      "Batch 169, Loss: 1.023366, Accuracy: 73.30%\n",
      "Batch 170, Loss: 1.007904, Accuracy: 73.29%\n",
      "Batch 171, Loss: 0.934209, Accuracy: 73.33%\n",
      "Batch 172, Loss: 1.082829, Accuracy: 73.28%\n",
      "Batch 173, Loss: 1.011108, Accuracy: 73.29%\n",
      "Batch 174, Loss: 1.051002, Accuracy: 73.26%\n",
      "Batch 175, Loss: 0.943830, Accuracy: 73.29%\n",
      "Batch 176, Loss: 0.949765, Accuracy: 73.34%\n",
      "Batch 177, Loss: 1.108312, Accuracy: 73.28%\n",
      "Batch 178, Loss: 0.950012, Accuracy: 73.31%\n",
      "Batch 179, Loss: 1.048756, Accuracy: 73.28%\n",
      "Batch 180, Loss: 0.995565, Accuracy: 73.29%\n",
      "Batch 181, Loss: 0.945138, Accuracy: 73.33%\n",
      "Batch 182, Loss: 0.980857, Accuracy: 73.34%\n",
      "Batch 183, Loss: 1.002133, Accuracy: 73.34%\n",
      "Batch 184, Loss: 0.907407, Accuracy: 73.40%\n",
      "Batch 185, Loss: 1.006995, Accuracy: 73.40%\n",
      "Batch 186, Loss: 0.995295, Accuracy: 73.40%\n",
      "Batch 187, Loss: 0.983970, Accuracy: 73.41%\n",
      "Batch 188, Loss: 1.061967, Accuracy: 73.37%\n",
      "Batch 189, Loss: 1.010997, Accuracy: 73.37%\n",
      "Batch 190, Loss: 0.970700, Accuracy: 73.40%\n",
      "Batch 191, Loss: 1.029697, Accuracy: 73.39%\n",
      "Batch 192, Loss: 1.027936, Accuracy: 73.38%\n",
      "Batch 193, Loss: 1.047842, Accuracy: 73.36%\n",
      "Batch 194, Loss: 1.025609, Accuracy: 73.34%\n",
      "Batch 195, Loss: 1.112391, Accuracy: 73.29%\n",
      "Batch 196, Loss: 0.943578, Accuracy: 73.32%\n",
      "Batch 197, Loss: 0.952201, Accuracy: 73.35%\n",
      "Batch 198, Loss: 1.053976, Accuracy: 73.33%\n",
      "Batch 199, Loss: 0.966041, Accuracy: 73.34%\n",
      "Batch 200, Loss: 1.001344, Accuracy: 73.35%\n",
      "Batch 201, Loss: 0.960667, Accuracy: 73.37%\n",
      "Batch 202, Loss: 0.979163, Accuracy: 73.38%\n",
      "Batch 203, Loss: 0.990411, Accuracy: 73.40%\n",
      "Batch 204, Loss: 0.946409, Accuracy: 73.43%\n",
      "Batch 205, Loss: 1.026799, Accuracy: 73.42%\n",
      "Batch 206, Loss: 1.046251, Accuracy: 73.40%\n",
      "Batch 207, Loss: 0.998169, Accuracy: 73.41%\n",
      "Batch 208, Loss: 1.025138, Accuracy: 73.40%\n",
      "Batch 209, Loss: 1.006006, Accuracy: 73.41%\n",
      "Batch 210, Loss: 1.065081, Accuracy: 73.38%\n",
      "Batch 211, Loss: 1.037487, Accuracy: 73.36%\n",
      "Batch 212, Loss: 1.039258, Accuracy: 73.34%\n",
      "Batch 213, Loss: 0.986568, Accuracy: 73.36%\n",
      "Training - Epoch 52, Loss: 1.008067, Accuracy: 73.36%\n",
      "Validation Batch 1, Loss: 0.999796, Accuracy: 75.00%\n",
      "Validation Batch 2, Loss: 1.117210, Accuracy: 68.75%\n",
      "Validation Batch 3, Loss: 1.110976, Accuracy: 67.19%\n",
      "Validation Batch 4, Loss: 1.030992, Accuracy: 67.97%\n",
      "Validation Batch 5, Loss: 1.047315, Accuracy: 68.12%\n",
      "Validation Batch 6, Loss: 0.999660, Accuracy: 69.01%\n",
      "Validation Batch 7, Loss: 1.066751, Accuracy: 69.20%\n",
      "Validation Batch 8, Loss: 1.091899, Accuracy: 68.55%\n",
      "Validation Batch 9, Loss: 1.110533, Accuracy: 67.88%\n",
      "Validation Batch 10, Loss: 1.069220, Accuracy: 67.81%\n",
      "Validation Batch 11, Loss: 1.050595, Accuracy: 67.90%\n",
      "Validation Batch 12, Loss: 1.001944, Accuracy: 68.36%\n",
      "Validation Batch 13, Loss: 1.136218, Accuracy: 67.67%\n",
      "Validation Batch 14, Loss: 1.076885, Accuracy: 67.52%\n",
      "Validation Batch 15, Loss: 1.056912, Accuracy: 67.60%\n",
      "Validation Batch 16, Loss: 1.025568, Accuracy: 67.97%\n",
      "Validation Batch 17, Loss: 1.129871, Accuracy: 67.46%\n",
      "Validation Batch 18, Loss: 1.031099, Accuracy: 67.80%\n",
      "Validation Batch 19, Loss: 1.118411, Accuracy: 67.35%\n",
      "Validation Batch 20, Loss: 1.056166, Accuracy: 67.50%\n",
      "Validation Batch 21, Loss: 1.063405, Accuracy: 67.49%\n",
      "Validation Batch 22, Loss: 1.102236, Accuracy: 67.33%\n",
      "Validation Batch 23, Loss: 1.158466, Accuracy: 67.05%\n",
      "Validation Batch 24, Loss: 1.105581, Accuracy: 66.93%\n",
      "Validation Batch 25, Loss: 1.043385, Accuracy: 67.06%\n",
      "Validation Batch 26, Loss: 1.047087, Accuracy: 67.19%\n",
      "Validation Batch 27, Loss: 1.054296, Accuracy: 67.29%\n",
      "Validation - Epoch 52, Loss: 1.070462, Accuracy: 67.29%\n",
      "Patienceâ€”0\n",
      "Epoch 53\n",
      "Batch 1, Loss: 1.006055, Accuracy: 73.44%\n",
      "Batch 2, Loss: 0.982357, Accuracy: 74.22%\n",
      "Batch 3, Loss: 0.958850, Accuracy: 75.52%\n",
      "Batch 4, Loss: 0.994392, Accuracy: 75.39%\n",
      "Batch 5, Loss: 0.999601, Accuracy: 75.62%\n",
      "Batch 6, Loss: 0.984921, Accuracy: 75.78%\n",
      "Batch 7, Loss: 0.972602, Accuracy: 76.12%\n",
      "Batch 8, Loss: 1.003970, Accuracy: 75.78%\n",
      "Batch 9, Loss: 1.102150, Accuracy: 74.65%\n",
      "Batch 10, Loss: 1.046131, Accuracy: 74.22%\n",
      "Batch 11, Loss: 0.948991, Accuracy: 74.72%\n",
      "Batch 12, Loss: 0.946944, Accuracy: 75.26%\n",
      "Batch 13, Loss: 0.951342, Accuracy: 75.60%\n",
      "Batch 14, Loss: 1.088540, Accuracy: 74.89%\n",
      "Batch 15, Loss: 1.185660, Accuracy: 73.54%\n",
      "Batch 16, Loss: 0.901391, Accuracy: 74.12%\n",
      "Batch 17, Loss: 0.998206, Accuracy: 74.08%\n",
      "Batch 18, Loss: 1.011598, Accuracy: 74.05%\n",
      "Batch 19, Loss: 0.977959, Accuracy: 74.18%\n",
      "Batch 20, Loss: 0.988338, Accuracy: 74.06%\n",
      "Batch 21, Loss: 1.076322, Accuracy: 73.74%\n",
      "Batch 22, Loss: 0.943693, Accuracy: 74.08%\n",
      "Batch 23, Loss: 1.000345, Accuracy: 73.91%\n",
      "Batch 24, Loss: 1.044349, Accuracy: 73.70%\n",
      "Batch 25, Loss: 1.063700, Accuracy: 73.44%\n",
      "Batch 26, Loss: 0.976127, Accuracy: 73.50%\n",
      "Batch 27, Loss: 1.003696, Accuracy: 73.55%\n",
      "Batch 28, Loss: 1.029099, Accuracy: 73.44%\n",
      "Batch 29, Loss: 1.068455, Accuracy: 73.28%\n",
      "Batch 30, Loss: 1.036440, Accuracy: 73.12%\n",
      "Batch 31, Loss: 0.975591, Accuracy: 73.24%\n",
      "Batch 32, Loss: 0.966267, Accuracy: 73.39%\n",
      "Batch 33, Loss: 1.124462, Accuracy: 72.92%\n",
      "Batch 34, Loss: 1.006387, Accuracy: 72.93%\n",
      "Batch 35, Loss: 1.091012, Accuracy: 72.68%\n",
      "Batch 36, Loss: 0.969100, Accuracy: 72.87%\n",
      "Batch 37, Loss: 0.969230, Accuracy: 72.97%\n",
      "Batch 38, Loss: 1.074021, Accuracy: 72.82%\n",
      "Batch 39, Loss: 1.042318, Accuracy: 72.76%\n",
      "Batch 40, Loss: 0.997345, Accuracy: 72.77%\n",
      "Batch 41, Loss: 0.931804, Accuracy: 73.02%\n",
      "Batch 42, Loss: 0.971306, Accuracy: 73.18%\n",
      "Batch 43, Loss: 1.044789, Accuracy: 73.04%\n",
      "Batch 44, Loss: 1.056769, Accuracy: 72.83%\n",
      "Batch 45, Loss: 0.951782, Accuracy: 73.06%\n",
      "Batch 46, Loss: 1.018192, Accuracy: 73.00%\n",
      "Batch 47, Loss: 1.005258, Accuracy: 73.01%\n",
      "Batch 48, Loss: 1.065224, Accuracy: 72.85%\n",
      "Batch 49, Loss: 1.030396, Accuracy: 72.80%\n",
      "Batch 50, Loss: 0.993558, Accuracy: 72.88%\n",
      "Batch 51, Loss: 1.032062, Accuracy: 72.82%\n",
      "Batch 52, Loss: 1.004435, Accuracy: 72.81%\n",
      "Batch 53, Loss: 0.998716, Accuracy: 72.85%\n",
      "Batch 54, Loss: 1.023664, Accuracy: 72.83%\n",
      "Batch 55, Loss: 1.065686, Accuracy: 72.73%\n",
      "Batch 56, Loss: 0.975666, Accuracy: 72.80%\n",
      "Batch 57, Loss: 0.979321, Accuracy: 72.83%\n",
      "Batch 58, Loss: 1.026918, Accuracy: 72.84%\n",
      "Batch 59, Loss: 1.115946, Accuracy: 72.67%\n",
      "Batch 60, Loss: 1.134151, Accuracy: 72.42%\n",
      "Batch 61, Loss: 1.025383, Accuracy: 72.41%\n",
      "Batch 62, Loss: 0.993231, Accuracy: 72.45%\n",
      "Batch 63, Loss: 0.972107, Accuracy: 72.57%\n",
      "Batch 64, Loss: 1.037206, Accuracy: 72.51%\n",
      "Batch 65, Loss: 0.944880, Accuracy: 72.62%\n",
      "Batch 66, Loss: 0.989825, Accuracy: 72.68%\n",
      "Batch 67, Loss: 1.023064, Accuracy: 72.67%\n",
      "Batch 68, Loss: 1.077395, Accuracy: 72.61%\n",
      "Batch 69, Loss: 1.058028, Accuracy: 72.51%\n",
      "Batch 70, Loss: 1.045620, Accuracy: 72.48%\n",
      "Batch 71, Loss: 1.058814, Accuracy: 72.40%\n",
      "Batch 72, Loss: 0.990337, Accuracy: 72.44%\n",
      "Batch 73, Loss: 0.960085, Accuracy: 72.54%\n",
      "Batch 74, Loss: 1.047148, Accuracy: 72.47%\n",
      "Batch 75, Loss: 0.964193, Accuracy: 72.56%\n",
      "Batch 76, Loss: 0.988516, Accuracy: 72.62%\n",
      "Batch 77, Loss: 0.953383, Accuracy: 72.69%\n",
      "Batch 78, Loss: 1.030305, Accuracy: 72.70%\n",
      "Batch 79, Loss: 0.988623, Accuracy: 72.73%\n",
      "Batch 80, Loss: 1.009546, Accuracy: 72.73%\n",
      "Batch 81, Loss: 1.057297, Accuracy: 72.69%\n",
      "Batch 82, Loss: 0.977785, Accuracy: 72.75%\n",
      "Batch 83, Loss: 1.027077, Accuracy: 72.74%\n",
      "Batch 84, Loss: 0.991182, Accuracy: 72.77%\n",
      "Batch 85, Loss: 0.976086, Accuracy: 72.83%\n",
      "Batch 86, Loss: 0.976676, Accuracy: 72.87%\n",
      "Batch 87, Loss: 1.003034, Accuracy: 72.88%\n",
      "Batch 88, Loss: 0.964874, Accuracy: 72.94%\n",
      "Batch 89, Loss: 1.026100, Accuracy: 72.93%\n",
      "Batch 90, Loss: 1.025820, Accuracy: 72.92%\n",
      "Batch 91, Loss: 1.012097, Accuracy: 72.92%\n",
      "Batch 92, Loss: 0.961890, Accuracy: 72.98%\n",
      "Batch 93, Loss: 0.933864, Accuracy: 73.07%\n",
      "Batch 94, Loss: 0.960312, Accuracy: 73.14%\n",
      "Batch 95, Loss: 0.958559, Accuracy: 73.19%\n",
      "Batch 96, Loss: 0.991850, Accuracy: 73.21%\n",
      "Batch 97, Loss: 1.034317, Accuracy: 73.18%\n",
      "Batch 98, Loss: 0.947057, Accuracy: 73.23%\n",
      "Batch 99, Loss: 1.040070, Accuracy: 73.20%\n",
      "Batch 100, Loss: 1.002573, Accuracy: 73.22%\n",
      "Batch 101, Loss: 0.998819, Accuracy: 73.24%\n",
      "Batch 102, Loss: 1.002466, Accuracy: 73.24%\n",
      "Batch 103, Loss: 0.982540, Accuracy: 73.26%\n",
      "Batch 104, Loss: 1.043430, Accuracy: 73.23%\n",
      "Batch 105, Loss: 1.118571, Accuracy: 73.11%\n",
      "Batch 106, Loss: 0.923601, Accuracy: 73.20%\n",
      "Batch 107, Loss: 0.996970, Accuracy: 73.22%\n",
      "Batch 108, Loss: 1.022244, Accuracy: 73.19%\n",
      "Batch 109, Loss: 0.943272, Accuracy: 73.25%\n",
      "Batch 110, Loss: 0.965913, Accuracy: 73.28%\n",
      "Batch 111, Loss: 1.048912, Accuracy: 73.25%\n",
      "Batch 112, Loss: 0.984065, Accuracy: 73.27%\n",
      "Batch 113, Loss: 0.998379, Accuracy: 73.30%\n",
      "Batch 114, Loss: 0.971900, Accuracy: 73.33%\n",
      "Batch 115, Loss: 1.008336, Accuracy: 73.34%\n",
      "Batch 116, Loss: 0.997429, Accuracy: 73.34%\n",
      "Batch 117, Loss: 0.940864, Accuracy: 73.41%\n",
      "Batch 118, Loss: 0.994334, Accuracy: 73.42%\n",
      "Batch 119, Loss: 0.987223, Accuracy: 73.44%\n",
      "Batch 120, Loss: 0.977796, Accuracy: 73.46%\n",
      "Batch 121, Loss: 1.001921, Accuracy: 73.46%\n",
      "Batch 122, Loss: 1.013584, Accuracy: 73.48%\n",
      "Batch 123, Loss: 1.036553, Accuracy: 73.48%\n",
      "Batch 124, Loss: 0.994038, Accuracy: 73.49%\n",
      "Batch 125, Loss: 1.022547, Accuracy: 73.47%\n",
      "Batch 126, Loss: 1.058185, Accuracy: 73.41%\n",
      "Batch 127, Loss: 0.991667, Accuracy: 73.44%\n",
      "Batch 128, Loss: 1.068808, Accuracy: 73.38%\n",
      "Batch 129, Loss: 1.060033, Accuracy: 73.34%\n",
      "Batch 130, Loss: 1.001982, Accuracy: 73.33%\n",
      "Batch 131, Loss: 1.025288, Accuracy: 73.32%\n",
      "Batch 132, Loss: 0.967147, Accuracy: 73.34%\n",
      "Batch 133, Loss: 1.007804, Accuracy: 73.34%\n",
      "Batch 134, Loss: 0.911757, Accuracy: 73.40%\n",
      "Batch 135, Loss: 0.977653, Accuracy: 73.40%\n",
      "Batch 136, Loss: 1.013306, Accuracy: 73.40%\n",
      "Batch 137, Loss: 0.955339, Accuracy: 73.44%\n",
      "Batch 138, Loss: 1.001006, Accuracy: 73.43%\n",
      "Batch 139, Loss: 0.977381, Accuracy: 73.45%\n",
      "Batch 140, Loss: 1.041595, Accuracy: 73.43%\n",
      "Batch 141, Loss: 1.035258, Accuracy: 73.40%\n",
      "Batch 142, Loss: 0.970966, Accuracy: 73.44%\n",
      "Batch 143, Loss: 1.032632, Accuracy: 73.42%\n",
      "Batch 144, Loss: 1.012832, Accuracy: 73.40%\n",
      "Batch 145, Loss: 0.992516, Accuracy: 73.41%\n",
      "Batch 146, Loss: 1.002093, Accuracy: 73.41%\n",
      "Batch 147, Loss: 1.032752, Accuracy: 73.41%\n",
      "Batch 148, Loss: 1.009884, Accuracy: 73.42%\n",
      "Batch 149, Loss: 0.959049, Accuracy: 73.45%\n",
      "Batch 150, Loss: 1.090203, Accuracy: 73.40%\n",
      "Batch 151, Loss: 1.049588, Accuracy: 73.34%\n",
      "Batch 152, Loss: 1.069532, Accuracy: 73.30%\n",
      "Batch 153, Loss: 1.034836, Accuracy: 73.27%\n",
      "Batch 154, Loss: 0.951082, Accuracy: 73.32%\n",
      "Batch 155, Loss: 0.884299, Accuracy: 73.40%\n",
      "Batch 156, Loss: 0.978951, Accuracy: 73.41%\n",
      "Batch 157, Loss: 1.041646, Accuracy: 73.38%\n",
      "Batch 158, Loss: 0.937673, Accuracy: 73.44%\n",
      "Batch 159, Loss: 1.024159, Accuracy: 73.43%\n",
      "Batch 160, Loss: 1.057111, Accuracy: 73.41%\n",
      "Batch 161, Loss: 0.955997, Accuracy: 73.44%\n",
      "Batch 162, Loss: 1.060472, Accuracy: 73.40%\n",
      "Batch 163, Loss: 0.929120, Accuracy: 73.45%\n",
      "Batch 164, Loss: 0.930375, Accuracy: 73.49%\n",
      "Batch 165, Loss: 1.058831, Accuracy: 73.45%\n",
      "Batch 166, Loss: 1.101839, Accuracy: 73.39%\n",
      "Batch 167, Loss: 1.002949, Accuracy: 73.38%\n",
      "Batch 168, Loss: 0.982437, Accuracy: 73.39%\n",
      "Batch 169, Loss: 1.059411, Accuracy: 73.35%\n",
      "Batch 170, Loss: 1.071217, Accuracy: 73.31%\n",
      "Batch 171, Loss: 0.993291, Accuracy: 73.33%\n",
      "Batch 172, Loss: 0.997444, Accuracy: 73.34%\n",
      "Batch 173, Loss: 1.012769, Accuracy: 73.34%\n",
      "Batch 174, Loss: 0.984058, Accuracy: 73.36%\n",
      "Batch 175, Loss: 0.938633, Accuracy: 73.39%\n",
      "Batch 176, Loss: 0.958417, Accuracy: 73.41%\n",
      "Batch 177, Loss: 1.020620, Accuracy: 73.39%\n",
      "Batch 178, Loss: 1.004939, Accuracy: 73.39%\n",
      "Batch 179, Loss: 1.029893, Accuracy: 73.38%\n",
      "Batch 180, Loss: 1.060168, Accuracy: 73.34%\n",
      "Batch 181, Loss: 0.947999, Accuracy: 73.38%\n",
      "Batch 182, Loss: 0.970380, Accuracy: 73.40%\n",
      "Batch 183, Loss: 1.039924, Accuracy: 73.38%\n",
      "Batch 184, Loss: 0.968990, Accuracy: 73.40%\n",
      "Batch 185, Loss: 1.035040, Accuracy: 73.40%\n",
      "Batch 186, Loss: 1.094160, Accuracy: 73.35%\n",
      "Batch 187, Loss: 0.952631, Accuracy: 73.39%\n",
      "Batch 188, Loss: 1.015219, Accuracy: 73.38%\n",
      "Batch 189, Loss: 1.030495, Accuracy: 73.35%\n",
      "Batch 190, Loss: 1.002695, Accuracy: 73.35%\n",
      "Batch 191, Loss: 1.045020, Accuracy: 73.33%\n",
      "Batch 192, Loss: 0.972355, Accuracy: 73.36%\n",
      "Batch 193, Loss: 0.976729, Accuracy: 73.39%\n",
      "Batch 194, Loss: 0.999868, Accuracy: 73.40%\n",
      "Batch 195, Loss: 1.020501, Accuracy: 73.40%\n",
      "Batch 196, Loss: 0.951415, Accuracy: 73.41%\n",
      "Batch 197, Loss: 0.970046, Accuracy: 73.41%\n",
      "Batch 198, Loss: 1.082625, Accuracy: 73.37%\n",
      "Batch 199, Loss: 1.022961, Accuracy: 73.36%\n",
      "Batch 200, Loss: 0.878604, Accuracy: 73.43%\n",
      "Batch 201, Loss: 1.053627, Accuracy: 73.41%\n",
      "Batch 202, Loss: 1.008254, Accuracy: 73.40%\n",
      "Batch 203, Loss: 0.989228, Accuracy: 73.41%\n",
      "Batch 204, Loss: 1.016428, Accuracy: 73.40%\n",
      "Batch 205, Loss: 1.060927, Accuracy: 73.36%\n",
      "Batch 206, Loss: 1.037359, Accuracy: 73.35%\n",
      "Batch 207, Loss: 1.025899, Accuracy: 73.34%\n",
      "Batch 208, Loss: 1.038594, Accuracy: 73.32%\n",
      "Batch 209, Loss: 1.010725, Accuracy: 73.33%\n",
      "Batch 210, Loss: 0.958083, Accuracy: 73.36%\n",
      "Batch 211, Loss: 0.975709, Accuracy: 73.39%\n",
      "Batch 212, Loss: 0.956682, Accuracy: 73.42%\n",
      "Batch 213, Loss: 0.972445, Accuracy: 73.42%\n",
      "Training - Epoch 53, Loss: 1.006554, Accuracy: 73.42%\n",
      "Validation Batch 1, Loss: 1.025963, Accuracy: 70.31%\n",
      "Validation Batch 2, Loss: 1.146078, Accuracy: 64.06%\n",
      "Validation Batch 3, Loss: 1.136063, Accuracy: 62.50%\n",
      "Validation Batch 4, Loss: 1.050372, Accuracy: 64.06%\n",
      "Validation Batch 5, Loss: 1.096673, Accuracy: 62.81%\n",
      "Validation Batch 6, Loss: 1.034956, Accuracy: 64.58%\n",
      "Validation Batch 7, Loss: 1.084431, Accuracy: 64.96%\n",
      "Validation Batch 8, Loss: 1.095964, Accuracy: 65.04%\n",
      "Validation Batch 9, Loss: 1.126619, Accuracy: 64.41%\n",
      "Validation Batch 10, Loss: 1.092482, Accuracy: 64.69%\n",
      "Validation Batch 11, Loss: 1.077403, Accuracy: 64.49%\n",
      "Validation Batch 12, Loss: 1.011124, Accuracy: 65.23%\n",
      "Validation Batch 13, Loss: 1.145410, Accuracy: 65.02%\n",
      "Validation Batch 14, Loss: 1.097507, Accuracy: 64.96%\n",
      "Validation Batch 15, Loss: 1.073160, Accuracy: 65.31%\n",
      "Validation Batch 16, Loss: 1.059497, Accuracy: 65.33%\n",
      "Validation Batch 17, Loss: 1.170347, Accuracy: 64.61%\n",
      "Validation Batch 18, Loss: 1.049641, Accuracy: 64.84%\n",
      "Validation Batch 19, Loss: 1.150870, Accuracy: 64.31%\n",
      "Validation Batch 20, Loss: 1.083021, Accuracy: 64.38%\n",
      "Validation Batch 21, Loss: 1.084892, Accuracy: 64.36%\n",
      "Validation Batch 22, Loss: 1.133069, Accuracy: 64.13%\n",
      "Validation Batch 23, Loss: 1.179714, Accuracy: 63.79%\n",
      "Validation Batch 24, Loss: 1.113668, Accuracy: 63.80%\n",
      "Validation Batch 25, Loss: 1.084856, Accuracy: 63.69%\n",
      "Validation Batch 26, Loss: 1.047717, Accuracy: 63.94%\n",
      "Validation Batch 27, Loss: 1.091070, Accuracy: 64.00%\n",
      "Validation - Epoch 53, Loss: 1.094169, Accuracy: 64.00%\n",
      "Patienceâ€”1\n",
      "Epoch 54\n",
      "Batch 1, Loss: 1.099289, Accuracy: 62.50%\n",
      "Batch 2, Loss: 0.984013, Accuracy: 68.75%\n",
      "Batch 3, Loss: 1.047456, Accuracy: 68.75%\n",
      "Batch 4, Loss: 0.968083, Accuracy: 71.09%\n",
      "Batch 5, Loss: 1.075919, Accuracy: 70.00%\n",
      "Batch 6, Loss: 1.043507, Accuracy: 70.05%\n",
      "Batch 7, Loss: 0.976133, Accuracy: 71.21%\n",
      "Batch 8, Loss: 1.070558, Accuracy: 70.70%\n",
      "Batch 9, Loss: 1.049501, Accuracy: 70.49%\n",
      "Batch 10, Loss: 0.963818, Accuracy: 71.25%\n",
      "Batch 11, Loss: 0.990332, Accuracy: 71.73%\n",
      "Batch 12, Loss: 0.947957, Accuracy: 72.40%\n",
      "Batch 13, Loss: 1.006882, Accuracy: 72.36%\n",
      "Batch 14, Loss: 1.081591, Accuracy: 71.88%\n",
      "Batch 15, Loss: 0.949333, Accuracy: 72.40%\n",
      "Batch 16, Loss: 1.016472, Accuracy: 72.46%\n",
      "Batch 17, Loss: 1.103206, Accuracy: 71.97%\n",
      "Batch 18, Loss: 1.037161, Accuracy: 71.88%\n",
      "Batch 19, Loss: 0.982059, Accuracy: 72.12%\n",
      "Batch 20, Loss: 1.005793, Accuracy: 72.19%\n",
      "Batch 21, Loss: 0.970143, Accuracy: 72.47%\n",
      "Batch 22, Loss: 1.026691, Accuracy: 72.30%\n",
      "Batch 23, Loss: 0.985624, Accuracy: 72.55%\n",
      "Batch 24, Loss: 1.021803, Accuracy: 72.53%\n",
      "Batch 25, Loss: 0.997691, Accuracy: 72.56%\n",
      "Batch 26, Loss: 1.025613, Accuracy: 72.60%\n",
      "Batch 27, Loss: 1.018166, Accuracy: 72.63%\n",
      "Batch 28, Loss: 1.055098, Accuracy: 72.43%\n",
      "Batch 29, Loss: 0.996208, Accuracy: 72.58%\n",
      "Batch 30, Loss: 0.931176, Accuracy: 72.81%\n",
      "Batch 31, Loss: 1.030839, Accuracy: 72.68%\n",
      "Batch 32, Loss: 0.924076, Accuracy: 73.00%\n",
      "Batch 33, Loss: 1.057413, Accuracy: 72.77%\n",
      "Batch 34, Loss: 1.049091, Accuracy: 72.66%\n",
      "Batch 35, Loss: 1.070556, Accuracy: 72.50%\n",
      "Batch 36, Loss: 0.990926, Accuracy: 72.66%\n",
      "Batch 37, Loss: 0.987253, Accuracy: 72.72%\n",
      "Batch 38, Loss: 1.053661, Accuracy: 72.57%\n",
      "Batch 39, Loss: 0.943225, Accuracy: 72.76%\n",
      "Batch 40, Loss: 0.954547, Accuracy: 72.97%\n",
      "Batch 41, Loss: 0.949337, Accuracy: 73.09%\n",
      "Batch 42, Loss: 0.909534, Accuracy: 73.40%\n",
      "Batch 43, Loss: 1.008730, Accuracy: 73.40%\n",
      "Batch 44, Loss: 1.113069, Accuracy: 73.19%\n",
      "Batch 45, Loss: 1.099088, Accuracy: 72.95%\n",
      "Batch 46, Loss: 1.026647, Accuracy: 72.89%\n",
      "Batch 47, Loss: 1.019477, Accuracy: 72.91%\n",
      "Batch 48, Loss: 0.971587, Accuracy: 72.92%\n",
      "Batch 49, Loss: 1.053879, Accuracy: 72.80%\n",
      "Batch 50, Loss: 1.032436, Accuracy: 72.78%\n",
      "Batch 51, Loss: 0.969686, Accuracy: 72.89%\n",
      "Batch 52, Loss: 0.965524, Accuracy: 72.96%\n",
      "Batch 53, Loss: 1.000964, Accuracy: 72.94%\n",
      "Batch 54, Loss: 0.949117, Accuracy: 73.06%\n",
      "Batch 55, Loss: 1.032274, Accuracy: 73.01%\n",
      "Batch 56, Loss: 1.039058, Accuracy: 72.94%\n",
      "Batch 57, Loss: 1.082416, Accuracy: 72.81%\n",
      "Batch 58, Loss: 1.017549, Accuracy: 72.82%\n",
      "Batch 59, Loss: 1.034011, Accuracy: 72.72%\n",
      "Batch 60, Loss: 1.011819, Accuracy: 72.71%\n",
      "Batch 61, Loss: 0.994787, Accuracy: 72.77%\n",
      "Batch 62, Loss: 0.974620, Accuracy: 72.86%\n",
      "Batch 63, Loss: 0.977669, Accuracy: 72.92%\n",
      "Batch 64, Loss: 1.091810, Accuracy: 72.83%\n",
      "Batch 65, Loss: 1.051664, Accuracy: 72.76%\n",
      "Batch 66, Loss: 1.024386, Accuracy: 72.75%\n",
      "Batch 67, Loss: 0.972961, Accuracy: 72.81%\n",
      "Batch 68, Loss: 0.963788, Accuracy: 72.89%\n",
      "Batch 69, Loss: 0.943444, Accuracy: 72.98%\n",
      "Batch 70, Loss: 0.980374, Accuracy: 73.04%\n",
      "Batch 71, Loss: 1.027368, Accuracy: 73.02%\n",
      "Batch 72, Loss: 0.959064, Accuracy: 73.09%\n",
      "Batch 73, Loss: 0.928423, Accuracy: 73.20%\n",
      "Batch 74, Loss: 1.003475, Accuracy: 73.23%\n",
      "Batch 75, Loss: 0.986400, Accuracy: 73.27%\n",
      "Batch 76, Loss: 0.942666, Accuracy: 73.38%\n",
      "Batch 77, Loss: 1.053832, Accuracy: 73.30%\n",
      "Batch 78, Loss: 0.991128, Accuracy: 73.32%\n",
      "Batch 79, Loss: 0.940600, Accuracy: 73.40%\n",
      "Batch 80, Loss: 1.057293, Accuracy: 73.36%\n",
      "Batch 81, Loss: 0.867910, Accuracy: 73.53%\n",
      "Batch 82, Loss: 0.985258, Accuracy: 73.53%\n",
      "Batch 83, Loss: 0.942389, Accuracy: 73.61%\n",
      "Batch 84, Loss: 0.960979, Accuracy: 73.66%\n",
      "Batch 85, Loss: 1.068562, Accuracy: 73.60%\n",
      "Batch 86, Loss: 0.974140, Accuracy: 73.62%\n",
      "Batch 87, Loss: 1.016115, Accuracy: 73.60%\n",
      "Batch 88, Loss: 1.044845, Accuracy: 73.56%\n",
      "Batch 89, Loss: 1.037250, Accuracy: 73.53%\n",
      "Batch 90, Loss: 1.022421, Accuracy: 73.52%\n",
      "Batch 91, Loss: 0.891678, Accuracy: 73.66%\n",
      "Batch 92, Loss: 1.017002, Accuracy: 73.66%\n",
      "Batch 93, Loss: 1.028498, Accuracy: 73.61%\n",
      "Batch 94, Loss: 1.041991, Accuracy: 73.55%\n",
      "Batch 95, Loss: 1.003543, Accuracy: 73.54%\n",
      "Batch 96, Loss: 1.006063, Accuracy: 73.55%\n",
      "Batch 97, Loss: 0.964419, Accuracy: 73.60%\n",
      "Batch 98, Loss: 1.026810, Accuracy: 73.58%\n",
      "Batch 99, Loss: 1.011444, Accuracy: 73.58%\n",
      "Batch 100, Loss: 1.038048, Accuracy: 73.53%\n",
      "Batch 101, Loss: 1.030608, Accuracy: 73.51%\n",
      "Batch 102, Loss: 1.056620, Accuracy: 73.45%\n",
      "Batch 103, Loss: 1.122936, Accuracy: 73.35%\n",
      "Batch 104, Loss: 1.000416, Accuracy: 73.35%\n",
      "Batch 105, Loss: 1.047217, Accuracy: 73.30%\n",
      "Batch 106, Loss: 1.035670, Accuracy: 73.29%\n",
      "Batch 107, Loss: 1.003847, Accuracy: 73.29%\n",
      "Batch 108, Loss: 1.098481, Accuracy: 73.21%\n",
      "Batch 109, Loss: 1.003900, Accuracy: 73.22%\n",
      "Batch 110, Loss: 1.013718, Accuracy: 73.25%\n",
      "Batch 111, Loss: 0.966156, Accuracy: 73.31%\n",
      "Batch 112, Loss: 0.993552, Accuracy: 73.33%\n",
      "Batch 113, Loss: 1.005906, Accuracy: 73.33%\n",
      "Batch 114, Loss: 1.005596, Accuracy: 73.33%\n",
      "Batch 115, Loss: 1.005983, Accuracy: 73.32%\n",
      "Batch 116, Loss: 0.961228, Accuracy: 73.36%\n",
      "Batch 117, Loss: 1.049144, Accuracy: 73.32%\n",
      "Batch 118, Loss: 1.011064, Accuracy: 73.32%\n",
      "Batch 119, Loss: 1.020282, Accuracy: 73.32%\n",
      "Batch 120, Loss: 0.993321, Accuracy: 73.33%\n",
      "Batch 121, Loss: 0.952731, Accuracy: 73.39%\n",
      "Batch 122, Loss: 1.107597, Accuracy: 73.28%\n",
      "Batch 123, Loss: 1.003876, Accuracy: 73.27%\n",
      "Batch 124, Loss: 0.916859, Accuracy: 73.36%\n",
      "Batch 125, Loss: 1.002810, Accuracy: 73.36%\n",
      "Batch 126, Loss: 1.010984, Accuracy: 73.36%\n",
      "Batch 127, Loss: 1.012175, Accuracy: 73.35%\n",
      "Batch 128, Loss: 0.928630, Accuracy: 73.43%\n",
      "Batch 129, Loss: 1.035340, Accuracy: 73.40%\n",
      "Batch 130, Loss: 0.875808, Accuracy: 73.52%\n",
      "Batch 131, Loss: 0.989165, Accuracy: 73.52%\n",
      "Batch 132, Loss: 0.971379, Accuracy: 73.56%\n",
      "Batch 133, Loss: 0.949153, Accuracy: 73.59%\n",
      "Batch 134, Loss: 0.946840, Accuracy: 73.62%\n",
      "Batch 135, Loss: 0.976758, Accuracy: 73.65%\n",
      "Batch 136, Loss: 1.001869, Accuracy: 73.66%\n",
      "Batch 137, Loss: 0.978968, Accuracy: 73.67%\n",
      "Batch 138, Loss: 1.010846, Accuracy: 73.66%\n",
      "Batch 139, Loss: 1.074474, Accuracy: 73.62%\n",
      "Batch 140, Loss: 0.967231, Accuracy: 73.64%\n",
      "Batch 141, Loss: 0.980673, Accuracy: 73.66%\n",
      "Batch 142, Loss: 1.073537, Accuracy: 73.60%\n",
      "Batch 143, Loss: 0.915712, Accuracy: 73.69%\n",
      "Batch 144, Loss: 1.012402, Accuracy: 73.69%\n",
      "Batch 145, Loss: 1.078392, Accuracy: 73.63%\n",
      "Batch 146, Loss: 1.047200, Accuracy: 73.59%\n",
      "Batch 147, Loss: 1.062119, Accuracy: 73.54%\n",
      "Batch 148, Loss: 0.995105, Accuracy: 73.55%\n",
      "Batch 149, Loss: 0.932883, Accuracy: 73.61%\n",
      "Batch 150, Loss: 0.866302, Accuracy: 73.71%\n",
      "Batch 151, Loss: 0.964374, Accuracy: 73.72%\n",
      "Batch 152, Loss: 1.035375, Accuracy: 73.69%\n",
      "Batch 153, Loss: 0.979946, Accuracy: 73.71%\n",
      "Batch 154, Loss: 0.996835, Accuracy: 73.71%\n",
      "Batch 155, Loss: 0.997573, Accuracy: 73.72%\n",
      "Batch 156, Loss: 1.007709, Accuracy: 73.72%\n",
      "Batch 157, Loss: 1.017413, Accuracy: 73.72%\n",
      "Batch 158, Loss: 0.954844, Accuracy: 73.74%\n",
      "Batch 159, Loss: 0.988676, Accuracy: 73.74%\n",
      "Batch 160, Loss: 0.992580, Accuracy: 73.75%\n",
      "Batch 161, Loss: 0.948006, Accuracy: 73.80%\n",
      "Batch 162, Loss: 1.005511, Accuracy: 73.79%\n",
      "Batch 163, Loss: 0.932202, Accuracy: 73.84%\n",
      "Batch 164, Loss: 0.970369, Accuracy: 73.86%\n",
      "Batch 165, Loss: 1.045582, Accuracy: 73.84%\n",
      "Batch 166, Loss: 1.058784, Accuracy: 73.79%\n",
      "Batch 167, Loss: 0.998406, Accuracy: 73.78%\n",
      "Batch 168, Loss: 1.063522, Accuracy: 73.75%\n",
      "Batch 169, Loss: 0.981887, Accuracy: 73.78%\n",
      "Batch 170, Loss: 1.035309, Accuracy: 73.77%\n",
      "Batch 171, Loss: 0.931059, Accuracy: 73.80%\n",
      "Batch 172, Loss: 1.011682, Accuracy: 73.81%\n",
      "Batch 173, Loss: 0.958495, Accuracy: 73.83%\n",
      "Batch 174, Loss: 1.079378, Accuracy: 73.79%\n",
      "Batch 175, Loss: 0.977331, Accuracy: 73.80%\n",
      "Batch 176, Loss: 1.105211, Accuracy: 73.74%\n",
      "Batch 177, Loss: 1.009245, Accuracy: 73.75%\n",
      "Batch 178, Loss: 0.961467, Accuracy: 73.78%\n",
      "Batch 179, Loss: 1.006601, Accuracy: 73.78%\n",
      "Batch 180, Loss: 1.125407, Accuracy: 73.71%\n",
      "Batch 181, Loss: 0.993626, Accuracy: 73.71%\n",
      "Batch 182, Loss: 0.997831, Accuracy: 73.71%\n",
      "Batch 183, Loss: 0.999016, Accuracy: 73.73%\n",
      "Batch 184, Loss: 0.996164, Accuracy: 73.73%\n",
      "Batch 185, Loss: 1.040284, Accuracy: 73.70%\n",
      "Batch 186, Loss: 1.061675, Accuracy: 73.67%\n",
      "Batch 187, Loss: 0.986902, Accuracy: 73.67%\n",
      "Batch 188, Loss: 0.969865, Accuracy: 73.69%\n",
      "Batch 189, Loss: 1.067318, Accuracy: 73.65%\n",
      "Batch 190, Loss: 1.029289, Accuracy: 73.64%\n",
      "Batch 191, Loss: 1.101975, Accuracy: 73.58%\n",
      "Batch 192, Loss: 0.956598, Accuracy: 73.60%\n",
      "Batch 193, Loss: 1.056380, Accuracy: 73.57%\n",
      "Batch 194, Loss: 1.013451, Accuracy: 73.57%\n",
      "Batch 195, Loss: 1.000751, Accuracy: 73.57%\n",
      "Batch 196, Loss: 0.995830, Accuracy: 73.58%\n",
      "Batch 197, Loss: 0.988471, Accuracy: 73.59%\n",
      "Batch 198, Loss: 1.019157, Accuracy: 73.58%\n",
      "Batch 199, Loss: 1.010468, Accuracy: 73.58%\n",
      "Batch 200, Loss: 0.988538, Accuracy: 73.59%\n",
      "Batch 201, Loss: 1.038777, Accuracy: 73.57%\n",
      "Batch 202, Loss: 0.950468, Accuracy: 73.59%\n",
      "Batch 203, Loss: 0.926345, Accuracy: 73.62%\n",
      "Batch 204, Loss: 1.078241, Accuracy: 73.58%\n",
      "Batch 205, Loss: 1.002436, Accuracy: 73.57%\n",
      "Batch 206, Loss: 1.144264, Accuracy: 73.51%\n",
      "Batch 207, Loss: 1.032654, Accuracy: 73.49%\n",
      "Batch 208, Loss: 1.043567, Accuracy: 73.48%\n",
      "Batch 209, Loss: 0.953466, Accuracy: 73.48%\n",
      "Batch 210, Loss: 0.952298, Accuracy: 73.50%\n",
      "Batch 211, Loss: 0.911931, Accuracy: 73.55%\n",
      "Batch 212, Loss: 1.057435, Accuracy: 73.53%\n",
      "Batch 213, Loss: 0.973345, Accuracy: 73.55%\n",
      "Training - Epoch 54, Loss: 1.005508, Accuracy: 73.55%\n",
      "Validation Batch 1, Loss: 1.013992, Accuracy: 73.44%\n",
      "Validation Batch 2, Loss: 1.132637, Accuracy: 65.62%\n",
      "Validation Batch 3, Loss: 1.122842, Accuracy: 65.10%\n",
      "Validation Batch 4, Loss: 1.041002, Accuracy: 66.02%\n",
      "Validation Batch 5, Loss: 1.064594, Accuracy: 65.94%\n",
      "Validation Batch 6, Loss: 1.015156, Accuracy: 67.71%\n",
      "Validation Batch 7, Loss: 1.074551, Accuracy: 67.86%\n",
      "Validation Batch 8, Loss: 1.089609, Accuracy: 67.58%\n",
      "Validation Batch 9, Loss: 1.119169, Accuracy: 66.84%\n",
      "Validation Batch 10, Loss: 1.084478, Accuracy: 66.88%\n",
      "Validation Batch 11, Loss: 1.060433, Accuracy: 66.76%\n",
      "Validation Batch 12, Loss: 1.005906, Accuracy: 67.45%\n",
      "Validation Batch 13, Loss: 1.140507, Accuracy: 66.95%\n",
      "Validation Batch 14, Loss: 1.088307, Accuracy: 66.85%\n",
      "Validation Batch 15, Loss: 1.055247, Accuracy: 67.08%\n",
      "Validation Batch 16, Loss: 1.039323, Accuracy: 67.29%\n",
      "Validation Batch 17, Loss: 1.142667, Accuracy: 66.73%\n",
      "Validation Batch 18, Loss: 1.043262, Accuracy: 66.75%\n",
      "Validation Batch 19, Loss: 1.133060, Accuracy: 66.28%\n",
      "Validation Batch 20, Loss: 1.072578, Accuracy: 66.48%\n",
      "Validation Batch 21, Loss: 1.066901, Accuracy: 66.52%\n",
      "Validation Batch 22, Loss: 1.112113, Accuracy: 66.48%\n",
      "Validation Batch 23, Loss: 1.167583, Accuracy: 66.17%\n",
      "Validation Batch 24, Loss: 1.111510, Accuracy: 66.08%\n",
      "Validation Batch 25, Loss: 1.064348, Accuracy: 66.06%\n",
      "Validation Batch 26, Loss: 1.045549, Accuracy: 66.23%\n",
      "Validation Batch 27, Loss: 1.071568, Accuracy: 66.29%\n",
      "Validation - Epoch 54, Loss: 1.080700, Accuracy: 66.29%\n",
      "Patienceâ€”2\n",
      "Epoch 55\n",
      "Batch 1, Loss: 1.082173, Accuracy: 64.06%\n",
      "Batch 2, Loss: 0.982074, Accuracy: 70.31%\n",
      "Batch 3, Loss: 0.965241, Accuracy: 72.40%\n",
      "Batch 4, Loss: 0.994183, Accuracy: 72.27%\n",
      "Batch 5, Loss: 0.950599, Accuracy: 73.75%\n",
      "Batch 6, Loss: 0.917648, Accuracy: 75.26%\n",
      "Batch 7, Loss: 1.093295, Accuracy: 73.44%\n",
      "Batch 8, Loss: 1.001180, Accuracy: 73.24%\n",
      "Batch 9, Loss: 0.998030, Accuracy: 73.44%\n",
      "Batch 10, Loss: 1.045926, Accuracy: 73.12%\n",
      "Batch 11, Loss: 1.037994, Accuracy: 73.01%\n",
      "Batch 12, Loss: 1.027860, Accuracy: 73.05%\n",
      "Batch 13, Loss: 0.955481, Accuracy: 73.44%\n",
      "Batch 14, Loss: 1.013473, Accuracy: 73.44%\n",
      "Batch 15, Loss: 1.040417, Accuracy: 73.23%\n",
      "Batch 16, Loss: 1.014323, Accuracy: 73.14%\n",
      "Batch 17, Loss: 0.939264, Accuracy: 73.53%\n",
      "Batch 18, Loss: 0.966339, Accuracy: 73.78%\n",
      "Batch 19, Loss: 1.044241, Accuracy: 73.52%\n",
      "Batch 20, Loss: 0.996773, Accuracy: 73.52%\n",
      "Batch 21, Loss: 1.158094, Accuracy: 72.77%\n",
      "Batch 22, Loss: 0.988967, Accuracy: 72.87%\n",
      "Batch 23, Loss: 0.985477, Accuracy: 73.03%\n",
      "Batch 24, Loss: 1.016201, Accuracy: 72.85%\n",
      "Batch 25, Loss: 0.971518, Accuracy: 73.06%\n",
      "Batch 26, Loss: 0.916062, Accuracy: 73.44%\n",
      "Batch 27, Loss: 0.914744, Accuracy: 73.84%\n",
      "Batch 28, Loss: 1.015625, Accuracy: 73.83%\n",
      "Batch 29, Loss: 0.944791, Accuracy: 74.03%\n",
      "Batch 30, Loss: 1.020452, Accuracy: 74.01%\n",
      "Batch 31, Loss: 1.083434, Accuracy: 73.74%\n",
      "Batch 32, Loss: 0.952010, Accuracy: 74.02%\n",
      "Batch 33, Loss: 1.016758, Accuracy: 74.05%\n",
      "Batch 34, Loss: 0.951610, Accuracy: 74.22%\n",
      "Batch 35, Loss: 0.971853, Accuracy: 74.29%\n",
      "Batch 36, Loss: 1.062122, Accuracy: 74.13%\n",
      "Batch 37, Loss: 0.967499, Accuracy: 74.24%\n",
      "Batch 38, Loss: 0.979527, Accuracy: 74.26%\n",
      "Batch 39, Loss: 1.063404, Accuracy: 74.08%\n",
      "Batch 40, Loss: 0.989735, Accuracy: 74.06%\n",
      "Batch 41, Loss: 1.029565, Accuracy: 74.01%\n",
      "Batch 42, Loss: 0.966076, Accuracy: 74.07%\n",
      "Batch 43, Loss: 0.978516, Accuracy: 74.13%\n",
      "Batch 44, Loss: 0.992554, Accuracy: 74.11%\n",
      "Batch 45, Loss: 1.108326, Accuracy: 73.82%\n",
      "Batch 46, Loss: 0.999270, Accuracy: 73.81%\n",
      "Batch 47, Loss: 1.059808, Accuracy: 73.70%\n",
      "Batch 48, Loss: 1.026267, Accuracy: 73.67%\n",
      "Batch 49, Loss: 0.939437, Accuracy: 73.82%\n",
      "Batch 50, Loss: 0.926793, Accuracy: 74.03%\n",
      "Batch 51, Loss: 0.950618, Accuracy: 74.17%\n",
      "Batch 52, Loss: 1.131934, Accuracy: 73.86%\n",
      "Batch 53, Loss: 0.976396, Accuracy: 73.94%\n",
      "Batch 54, Loss: 1.023765, Accuracy: 73.90%\n",
      "Batch 55, Loss: 0.987940, Accuracy: 73.89%\n",
      "Batch 56, Loss: 0.962989, Accuracy: 73.97%\n",
      "Batch 57, Loss: 0.985603, Accuracy: 73.96%\n",
      "Batch 58, Loss: 0.903129, Accuracy: 74.16%\n",
      "Batch 59, Loss: 0.995176, Accuracy: 74.15%\n",
      "Batch 60, Loss: 1.074485, Accuracy: 74.04%\n",
      "Batch 61, Loss: 1.062091, Accuracy: 73.92%\n",
      "Batch 62, Loss: 0.965749, Accuracy: 74.02%\n",
      "Batch 63, Loss: 1.030797, Accuracy: 73.98%\n",
      "Batch 64, Loss: 0.981739, Accuracy: 74.00%\n",
      "Batch 65, Loss: 1.065482, Accuracy: 73.87%\n",
      "Batch 66, Loss: 0.981509, Accuracy: 73.93%\n",
      "Batch 67, Loss: 0.985410, Accuracy: 73.97%\n",
      "Batch 68, Loss: 1.056970, Accuracy: 73.87%\n",
      "Batch 69, Loss: 0.979957, Accuracy: 73.89%\n",
      "Batch 70, Loss: 0.983082, Accuracy: 73.88%\n",
      "Batch 71, Loss: 0.963849, Accuracy: 73.94%\n",
      "Batch 72, Loss: 1.050087, Accuracy: 73.87%\n",
      "Batch 73, Loss: 0.977220, Accuracy: 73.91%\n",
      "Batch 74, Loss: 0.987173, Accuracy: 73.94%\n",
      "Batch 75, Loss: 1.063227, Accuracy: 73.85%\n",
      "Batch 76, Loss: 1.028663, Accuracy: 73.81%\n",
      "Batch 77, Loss: 0.990443, Accuracy: 73.80%\n",
      "Batch 78, Loss: 0.919837, Accuracy: 73.94%\n",
      "Batch 79, Loss: 0.994158, Accuracy: 73.97%\n",
      "Batch 80, Loss: 1.001361, Accuracy: 73.98%\n",
      "Batch 81, Loss: 1.090105, Accuracy: 73.86%\n",
      "Batch 82, Loss: 1.096867, Accuracy: 73.74%\n",
      "Batch 83, Loss: 1.041987, Accuracy: 73.68%\n",
      "Batch 84, Loss: 0.969164, Accuracy: 73.74%\n",
      "Batch 85, Loss: 1.016577, Accuracy: 73.71%\n",
      "Batch 86, Loss: 0.954267, Accuracy: 73.76%\n",
      "Batch 87, Loss: 1.018348, Accuracy: 73.72%\n",
      "Batch 88, Loss: 0.967744, Accuracy: 73.79%\n",
      "Batch 89, Loss: 1.076675, Accuracy: 73.70%\n",
      "Batch 90, Loss: 0.940181, Accuracy: 73.78%\n",
      "Batch 91, Loss: 0.987072, Accuracy: 73.83%\n",
      "Batch 92, Loss: 0.986171, Accuracy: 73.86%\n",
      "Batch 93, Loss: 0.989849, Accuracy: 73.87%\n",
      "Batch 94, Loss: 1.107358, Accuracy: 73.77%\n",
      "Batch 95, Loss: 0.980155, Accuracy: 73.78%\n",
      "Batch 96, Loss: 1.098182, Accuracy: 73.68%\n",
      "Batch 97, Loss: 1.029706, Accuracy: 73.66%\n",
      "Batch 98, Loss: 0.974690, Accuracy: 73.72%\n",
      "Batch 99, Loss: 1.000195, Accuracy: 73.74%\n",
      "Batch 100, Loss: 0.948975, Accuracy: 73.78%\n",
      "Batch 101, Loss: 0.952550, Accuracy: 73.86%\n",
      "Batch 102, Loss: 1.040043, Accuracy: 73.82%\n",
      "Batch 103, Loss: 0.949369, Accuracy: 73.85%\n",
      "Batch 104, Loss: 1.005352, Accuracy: 73.83%\n",
      "Batch 105, Loss: 1.076578, Accuracy: 73.76%\n",
      "Batch 106, Loss: 0.975130, Accuracy: 73.81%\n",
      "Batch 107, Loss: 1.019521, Accuracy: 73.79%\n",
      "Batch 108, Loss: 1.012121, Accuracy: 73.78%\n",
      "Batch 109, Loss: 0.991470, Accuracy: 73.81%\n",
      "Batch 110, Loss: 0.940101, Accuracy: 73.86%\n",
      "Batch 111, Loss: 1.052307, Accuracy: 73.80%\n",
      "Batch 112, Loss: 0.975272, Accuracy: 73.86%\n",
      "Batch 113, Loss: 1.016463, Accuracy: 73.85%\n",
      "Batch 114, Loss: 1.004142, Accuracy: 73.85%\n",
      "Batch 115, Loss: 1.043519, Accuracy: 73.82%\n",
      "Batch 116, Loss: 1.048594, Accuracy: 73.77%\n",
      "Batch 117, Loss: 1.010249, Accuracy: 73.76%\n",
      "Batch 118, Loss: 0.974021, Accuracy: 73.80%\n",
      "Batch 119, Loss: 1.004713, Accuracy: 73.78%\n",
      "Batch 120, Loss: 1.066243, Accuracy: 73.72%\n",
      "Batch 121, Loss: 0.930156, Accuracy: 73.80%\n",
      "Batch 122, Loss: 0.916653, Accuracy: 73.87%\n",
      "Batch 123, Loss: 0.976359, Accuracy: 73.88%\n",
      "Batch 124, Loss: 1.026382, Accuracy: 73.85%\n",
      "Batch 125, Loss: 0.993014, Accuracy: 73.86%\n",
      "Batch 126, Loss: 1.049827, Accuracy: 73.83%\n",
      "Batch 127, Loss: 1.081366, Accuracy: 73.78%\n",
      "Batch 128, Loss: 0.998700, Accuracy: 73.80%\n",
      "Batch 129, Loss: 1.008609, Accuracy: 73.80%\n",
      "Batch 130, Loss: 1.014580, Accuracy: 73.77%\n",
      "Batch 131, Loss: 1.071453, Accuracy: 73.72%\n",
      "Batch 132, Loss: 1.016134, Accuracy: 73.72%\n",
      "Batch 133, Loss: 1.041625, Accuracy: 73.71%\n",
      "Batch 134, Loss: 0.979473, Accuracy: 73.72%\n",
      "Batch 135, Loss: 1.127971, Accuracy: 73.61%\n",
      "Batch 136, Loss: 1.008497, Accuracy: 73.61%\n",
      "Batch 137, Loss: 1.010657, Accuracy: 73.60%\n",
      "Batch 138, Loss: 0.983505, Accuracy: 73.60%\n",
      "Batch 139, Loss: 0.936397, Accuracy: 73.64%\n",
      "Batch 140, Loss: 1.014627, Accuracy: 73.63%\n",
      "Batch 141, Loss: 0.976148, Accuracy: 73.65%\n",
      "Batch 142, Loss: 1.069074, Accuracy: 73.59%\n",
      "Batch 143, Loss: 1.023369, Accuracy: 73.57%\n",
      "Batch 144, Loss: 1.012863, Accuracy: 73.55%\n",
      "Batch 145, Loss: 1.054228, Accuracy: 73.50%\n",
      "Batch 146, Loss: 0.974427, Accuracy: 73.51%\n",
      "Batch 147, Loss: 1.085290, Accuracy: 73.45%\n",
      "Batch 148, Loss: 1.038804, Accuracy: 73.42%\n",
      "Batch 149, Loss: 1.085550, Accuracy: 73.35%\n",
      "Batch 150, Loss: 1.038447, Accuracy: 73.33%\n",
      "Batch 151, Loss: 0.900395, Accuracy: 73.40%\n",
      "Batch 152, Loss: 1.025105, Accuracy: 73.39%\n",
      "Batch 153, Loss: 0.984022, Accuracy: 73.40%\n",
      "Batch 154, Loss: 1.096167, Accuracy: 73.33%\n",
      "Batch 155, Loss: 1.006061, Accuracy: 73.34%\n",
      "Batch 156, Loss: 1.011210, Accuracy: 73.34%\n",
      "Batch 157, Loss: 0.934731, Accuracy: 73.38%\n",
      "Batch 158, Loss: 1.036533, Accuracy: 73.36%\n",
      "Batch 159, Loss: 0.997831, Accuracy: 73.38%\n",
      "Batch 160, Loss: 1.175727, Accuracy: 73.28%\n",
      "Batch 161, Loss: 0.962120, Accuracy: 73.31%\n",
      "Batch 162, Loss: 1.006181, Accuracy: 73.31%\n",
      "Batch 163, Loss: 0.968641, Accuracy: 73.33%\n",
      "Batch 164, Loss: 1.028939, Accuracy: 73.33%\n",
      "Batch 165, Loss: 1.005112, Accuracy: 73.34%\n",
      "Batch 166, Loss: 0.967382, Accuracy: 73.36%\n",
      "Batch 167, Loss: 0.958836, Accuracy: 73.40%\n",
      "Batch 168, Loss: 1.016790, Accuracy: 73.42%\n",
      "Batch 169, Loss: 1.071203, Accuracy: 73.40%\n",
      "Batch 170, Loss: 0.959070, Accuracy: 73.44%\n",
      "Batch 171, Loss: 0.987999, Accuracy: 73.46%\n",
      "Batch 172, Loss: 0.960741, Accuracy: 73.49%\n",
      "Batch 173, Loss: 1.040882, Accuracy: 73.46%\n",
      "Batch 174, Loss: 0.996206, Accuracy: 73.46%\n",
      "Batch 175, Loss: 1.006467, Accuracy: 73.46%\n",
      "Batch 176, Loss: 1.026462, Accuracy: 73.46%\n",
      "Batch 177, Loss: 1.029378, Accuracy: 73.45%\n",
      "Batch 178, Loss: 0.987831, Accuracy: 73.46%\n",
      "Batch 179, Loss: 1.049066, Accuracy: 73.45%\n",
      "Batch 180, Loss: 1.040786, Accuracy: 73.42%\n",
      "Batch 181, Loss: 0.965222, Accuracy: 73.45%\n",
      "Batch 182, Loss: 1.073110, Accuracy: 73.41%\n",
      "Batch 183, Loss: 1.013918, Accuracy: 73.40%\n",
      "Batch 184, Loss: 0.942497, Accuracy: 73.45%\n",
      "Batch 185, Loss: 0.956289, Accuracy: 73.47%\n",
      "Batch 186, Loss: 0.917491, Accuracy: 73.54%\n",
      "Batch 187, Loss: 0.980204, Accuracy: 73.55%\n",
      "Batch 188, Loss: 1.059499, Accuracy: 73.53%\n",
      "Batch 189, Loss: 0.923020, Accuracy: 73.57%\n",
      "Batch 190, Loss: 0.990142, Accuracy: 73.57%\n",
      "Batch 191, Loss: 0.979411, Accuracy: 73.58%\n",
      "Batch 192, Loss: 0.992639, Accuracy: 73.58%\n",
      "Batch 193, Loss: 0.894428, Accuracy: 73.65%\n",
      "Batch 194, Loss: 0.935180, Accuracy: 73.69%\n",
      "Batch 195, Loss: 0.997164, Accuracy: 73.69%\n",
      "Batch 196, Loss: 1.040456, Accuracy: 73.68%\n",
      "Batch 197, Loss: 1.036053, Accuracy: 73.67%\n",
      "Batch 198, Loss: 1.023545, Accuracy: 73.66%\n",
      "Batch 199, Loss: 1.049747, Accuracy: 73.63%\n",
      "Batch 200, Loss: 1.056472, Accuracy: 73.60%\n",
      "Batch 201, Loss: 1.067549, Accuracy: 73.57%\n",
      "Batch 202, Loss: 0.938851, Accuracy: 73.61%\n",
      "Batch 203, Loss: 0.984649, Accuracy: 73.61%\n",
      "Batch 204, Loss: 0.972794, Accuracy: 73.62%\n",
      "Batch 205, Loss: 1.027037, Accuracy: 73.63%\n",
      "Batch 206, Loss: 1.046865, Accuracy: 73.59%\n",
      "Batch 207, Loss: 0.976199, Accuracy: 73.61%\n",
      "Batch 208, Loss: 0.995692, Accuracy: 73.60%\n",
      "Batch 209, Loss: 1.023048, Accuracy: 73.60%\n",
      "Batch 210, Loss: 1.072440, Accuracy: 73.56%\n",
      "Batch 211, Loss: 0.994655, Accuracy: 73.57%\n",
      "Batch 212, Loss: 0.935356, Accuracy: 73.61%\n",
      "Batch 213, Loss: 1.016358, Accuracy: 73.60%\n",
      "Training - Epoch 55, Loss: 1.005709, Accuracy: 73.60%\n",
      "Validation Batch 1, Loss: 1.010317, Accuracy: 75.00%\n",
      "Validation Batch 2, Loss: 1.128185, Accuracy: 66.41%\n",
      "Validation Batch 3, Loss: 1.117113, Accuracy: 65.62%\n",
      "Validation Batch 4, Loss: 1.039585, Accuracy: 66.02%\n",
      "Validation Batch 5, Loss: 1.055001, Accuracy: 66.88%\n",
      "Validation Batch 6, Loss: 1.004255, Accuracy: 68.49%\n",
      "Validation Batch 7, Loss: 1.071731, Accuracy: 68.53%\n",
      "Validation Batch 8, Loss: 1.085530, Accuracy: 67.97%\n",
      "Validation Batch 9, Loss: 1.114981, Accuracy: 67.01%\n",
      "Validation Batch 10, Loss: 1.080671, Accuracy: 67.03%\n",
      "Validation Batch 11, Loss: 1.055016, Accuracy: 66.90%\n",
      "Validation Batch 12, Loss: 0.999467, Accuracy: 67.58%\n",
      "Validation Batch 13, Loss: 1.136251, Accuracy: 67.19%\n",
      "Validation Batch 14, Loss: 1.083635, Accuracy: 67.08%\n",
      "Validation Batch 15, Loss: 1.046598, Accuracy: 67.50%\n",
      "Validation Batch 16, Loss: 1.032522, Accuracy: 67.68%\n",
      "Validation Batch 17, Loss: 1.129516, Accuracy: 67.10%\n",
      "Validation Batch 18, Loss: 1.036370, Accuracy: 67.10%\n",
      "Validation Batch 19, Loss: 1.124601, Accuracy: 66.61%\n",
      "Validation Batch 20, Loss: 1.069333, Accuracy: 66.80%\n",
      "Validation Batch 21, Loss: 1.064519, Accuracy: 66.82%\n",
      "Validation Batch 22, Loss: 1.108687, Accuracy: 66.69%\n",
      "Validation Batch 23, Loss: 1.160899, Accuracy: 66.30%\n",
      "Validation Batch 24, Loss: 1.109033, Accuracy: 66.21%\n",
      "Validation Batch 25, Loss: 1.055918, Accuracy: 66.25%\n",
      "Validation Batch 26, Loss: 1.041306, Accuracy: 66.47%\n",
      "Validation Batch 27, Loss: 1.072583, Accuracy: 66.53%\n",
      "Validation - Epoch 55, Loss: 1.075319, Accuracy: 66.53%\n",
      "Patienceâ€”3\n",
      "Epoch 56\n",
      "Batch 1, Loss: 0.998643, Accuracy: 75.00%\n",
      "Batch 2, Loss: 0.964709, Accuracy: 77.34%\n",
      "Batch 3, Loss: 0.988709, Accuracy: 76.04%\n",
      "Batch 4, Loss: 1.043152, Accuracy: 74.22%\n",
      "Batch 5, Loss: 0.926892, Accuracy: 75.94%\n",
      "Batch 6, Loss: 0.923611, Accuracy: 77.08%\n",
      "Batch 7, Loss: 1.033552, Accuracy: 76.12%\n",
      "Batch 8, Loss: 1.069473, Accuracy: 75.00%\n",
      "Batch 9, Loss: 1.036932, Accuracy: 74.65%\n",
      "Batch 10, Loss: 0.998816, Accuracy: 74.53%\n",
      "Batch 11, Loss: 1.025438, Accuracy: 74.29%\n",
      "Batch 12, Loss: 0.929958, Accuracy: 74.87%\n",
      "Batch 13, Loss: 1.020270, Accuracy: 74.76%\n",
      "Batch 14, Loss: 0.957662, Accuracy: 75.00%\n",
      "Batch 15, Loss: 1.071406, Accuracy: 74.58%\n",
      "Batch 16, Loss: 1.013311, Accuracy: 74.51%\n",
      "Batch 17, Loss: 1.027307, Accuracy: 74.26%\n",
      "Batch 18, Loss: 1.016212, Accuracy: 74.22%\n",
      "Batch 19, Loss: 0.970965, Accuracy: 74.51%\n",
      "Batch 20, Loss: 1.044608, Accuracy: 74.14%\n",
      "Batch 21, Loss: 1.026563, Accuracy: 73.88%\n",
      "Batch 22, Loss: 1.102951, Accuracy: 73.30%\n",
      "Batch 23, Loss: 1.104498, Accuracy: 72.89%\n",
      "Batch 24, Loss: 1.061196, Accuracy: 72.72%\n",
      "Batch 25, Loss: 1.058688, Accuracy: 72.50%\n",
      "Batch 26, Loss: 1.018668, Accuracy: 72.42%\n",
      "Batch 27, Loss: 1.011953, Accuracy: 72.34%\n",
      "Batch 28, Loss: 0.973767, Accuracy: 72.49%\n",
      "Batch 29, Loss: 0.988544, Accuracy: 72.68%\n",
      "Batch 30, Loss: 1.040291, Accuracy: 72.60%\n",
      "Batch 31, Loss: 0.924650, Accuracy: 72.88%\n",
      "Batch 32, Loss: 1.022311, Accuracy: 72.85%\n",
      "Batch 33, Loss: 1.005995, Accuracy: 72.87%\n",
      "Batch 34, Loss: 0.993516, Accuracy: 72.98%\n",
      "Batch 35, Loss: 0.995805, Accuracy: 73.08%\n",
      "Batch 36, Loss: 0.962839, Accuracy: 73.22%\n",
      "Batch 37, Loss: 0.962087, Accuracy: 73.31%\n",
      "Batch 38, Loss: 1.033602, Accuracy: 73.27%\n",
      "Batch 39, Loss: 0.988391, Accuracy: 73.36%\n",
      "Batch 40, Loss: 0.993605, Accuracy: 73.40%\n",
      "Batch 41, Loss: 0.991276, Accuracy: 73.48%\n",
      "Batch 42, Loss: 1.018568, Accuracy: 73.44%\n",
      "Batch 43, Loss: 0.925682, Accuracy: 73.69%\n",
      "Batch 44, Loss: 0.952089, Accuracy: 73.90%\n",
      "Batch 45, Loss: 1.041088, Accuracy: 73.75%\n",
      "Batch 46, Loss: 0.956937, Accuracy: 73.88%\n",
      "Batch 47, Loss: 1.064195, Accuracy: 73.74%\n",
      "Batch 48, Loss: 1.024009, Accuracy: 73.70%\n",
      "Batch 49, Loss: 0.952238, Accuracy: 73.85%\n",
      "Batch 50, Loss: 1.080182, Accuracy: 73.69%\n",
      "Batch 51, Loss: 0.977656, Accuracy: 73.81%\n",
      "Batch 52, Loss: 1.006837, Accuracy: 73.77%\n",
      "Batch 53, Loss: 0.935283, Accuracy: 73.88%\n",
      "Batch 54, Loss: 1.026353, Accuracy: 73.81%\n",
      "Batch 55, Loss: 1.026018, Accuracy: 73.75%\n",
      "Batch 56, Loss: 1.000675, Accuracy: 73.72%\n",
      "Batch 57, Loss: 1.078145, Accuracy: 73.57%\n",
      "Batch 58, Loss: 0.957508, Accuracy: 73.68%\n",
      "Batch 59, Loss: 0.968397, Accuracy: 73.70%\n",
      "Batch 60, Loss: 1.016688, Accuracy: 73.67%\n",
      "Batch 61, Loss: 0.927964, Accuracy: 73.77%\n",
      "Batch 62, Loss: 1.037996, Accuracy: 73.71%\n",
      "Batch 63, Loss: 1.071055, Accuracy: 73.64%\n",
      "Batch 64, Loss: 1.027563, Accuracy: 73.61%\n",
      "Batch 65, Loss: 1.072798, Accuracy: 73.53%\n",
      "Batch 66, Loss: 0.983957, Accuracy: 73.56%\n",
      "Batch 67, Loss: 1.031235, Accuracy: 73.51%\n",
      "Batch 68, Loss: 1.027371, Accuracy: 73.46%\n",
      "Batch 69, Loss: 1.009308, Accuracy: 73.48%\n",
      "Batch 70, Loss: 1.036346, Accuracy: 73.44%\n",
      "Batch 71, Loss: 1.082846, Accuracy: 73.33%\n",
      "Batch 72, Loss: 1.063483, Accuracy: 73.26%\n",
      "Batch 73, Loss: 1.029233, Accuracy: 73.24%\n",
      "Batch 74, Loss: 1.035855, Accuracy: 73.21%\n",
      "Batch 75, Loss: 0.986840, Accuracy: 73.25%\n",
      "Batch 76, Loss: 0.960416, Accuracy: 73.31%\n",
      "Batch 77, Loss: 0.966584, Accuracy: 73.38%\n",
      "Batch 78, Loss: 1.021146, Accuracy: 73.36%\n",
      "Batch 79, Loss: 1.031117, Accuracy: 73.32%\n",
      "Batch 80, Loss: 1.006662, Accuracy: 73.34%\n",
      "Batch 81, Loss: 1.139990, Accuracy: 73.17%\n",
      "Batch 82, Loss: 0.954287, Accuracy: 73.21%\n",
      "Batch 83, Loss: 0.911815, Accuracy: 73.31%\n",
      "Batch 84, Loss: 0.995548, Accuracy: 73.34%\n",
      "Batch 85, Loss: 1.117515, Accuracy: 73.22%\n",
      "Batch 86, Loss: 1.058239, Accuracy: 73.16%\n",
      "Batch 87, Loss: 1.052615, Accuracy: 73.15%\n",
      "Batch 88, Loss: 1.018954, Accuracy: 73.15%\n",
      "Batch 89, Loss: 1.029601, Accuracy: 73.14%\n",
      "Batch 90, Loss: 0.940094, Accuracy: 73.23%\n",
      "Batch 91, Loss: 0.979334, Accuracy: 73.25%\n",
      "Batch 92, Loss: 1.019169, Accuracy: 73.23%\n",
      "Batch 93, Loss: 1.048759, Accuracy: 73.14%\n",
      "Batch 94, Loss: 0.951853, Accuracy: 73.20%\n",
      "Batch 95, Loss: 0.956247, Accuracy: 73.26%\n",
      "Batch 96, Loss: 0.987782, Accuracy: 73.27%\n",
      "Batch 97, Loss: 1.010426, Accuracy: 73.28%\n",
      "Batch 98, Loss: 0.959243, Accuracy: 73.31%\n",
      "Batch 99, Loss: 0.959447, Accuracy: 73.37%\n",
      "Batch 100, Loss: 1.027201, Accuracy: 73.36%\n",
      "Batch 101, Loss: 0.967870, Accuracy: 73.41%\n",
      "Batch 102, Loss: 0.984572, Accuracy: 73.44%\n",
      "Batch 103, Loss: 0.989684, Accuracy: 73.44%\n",
      "Batch 104, Loss: 1.012979, Accuracy: 73.41%\n",
      "Batch 105, Loss: 0.928734, Accuracy: 73.47%\n",
      "Batch 106, Loss: 0.980322, Accuracy: 73.48%\n",
      "Batch 107, Loss: 0.971605, Accuracy: 73.53%\n",
      "Batch 108, Loss: 0.990761, Accuracy: 73.52%\n",
      "Batch 109, Loss: 1.039516, Accuracy: 73.49%\n",
      "Batch 110, Loss: 0.896274, Accuracy: 73.61%\n",
      "Batch 111, Loss: 1.019252, Accuracy: 73.59%\n",
      "Batch 112, Loss: 1.035816, Accuracy: 73.56%\n",
      "Batch 113, Loss: 0.968191, Accuracy: 73.59%\n",
      "Batch 114, Loss: 0.956643, Accuracy: 73.63%\n",
      "Batch 115, Loss: 1.021312, Accuracy: 73.61%\n",
      "Batch 116, Loss: 0.950603, Accuracy: 73.65%\n",
      "Batch 117, Loss: 1.054253, Accuracy: 73.61%\n",
      "Batch 118, Loss: 0.954984, Accuracy: 73.65%\n",
      "Batch 119, Loss: 0.998037, Accuracy: 73.66%\n",
      "Batch 120, Loss: 1.033033, Accuracy: 73.65%\n",
      "Batch 121, Loss: 1.072239, Accuracy: 73.57%\n",
      "Batch 122, Loss: 0.958904, Accuracy: 73.60%\n",
      "Batch 123, Loss: 1.012912, Accuracy: 73.58%\n",
      "Batch 124, Loss: 0.973910, Accuracy: 73.61%\n",
      "Batch 125, Loss: 0.959568, Accuracy: 73.66%\n",
      "Batch 126, Loss: 0.935789, Accuracy: 73.72%\n",
      "Batch 127, Loss: 1.035492, Accuracy: 73.68%\n",
      "Batch 128, Loss: 0.974693, Accuracy: 73.72%\n",
      "Batch 129, Loss: 0.937510, Accuracy: 73.76%\n",
      "Batch 130, Loss: 1.014230, Accuracy: 73.75%\n",
      "Batch 131, Loss: 0.977729, Accuracy: 73.77%\n",
      "Batch 132, Loss: 1.044830, Accuracy: 73.76%\n",
      "Batch 133, Loss: 1.062129, Accuracy: 73.71%\n",
      "Batch 134, Loss: 1.005590, Accuracy: 73.71%\n",
      "Batch 135, Loss: 1.002498, Accuracy: 73.70%\n",
      "Batch 136, Loss: 1.017714, Accuracy: 73.70%\n",
      "Batch 137, Loss: 1.045994, Accuracy: 73.67%\n",
      "Batch 138, Loss: 1.007550, Accuracy: 73.66%\n",
      "Batch 139, Loss: 0.992995, Accuracy: 73.66%\n",
      "Batch 140, Loss: 0.954679, Accuracy: 73.72%\n",
      "Batch 141, Loss: 1.007939, Accuracy: 73.70%\n",
      "Batch 142, Loss: 1.017067, Accuracy: 73.69%\n",
      "Batch 143, Loss: 1.010852, Accuracy: 73.69%\n",
      "Batch 144, Loss: 0.917298, Accuracy: 73.75%\n",
      "Batch 145, Loss: 1.076783, Accuracy: 73.71%\n",
      "Batch 146, Loss: 0.946721, Accuracy: 73.75%\n",
      "Batch 147, Loss: 0.979279, Accuracy: 73.76%\n",
      "Batch 148, Loss: 1.037412, Accuracy: 73.75%\n",
      "Batch 149, Loss: 0.975970, Accuracy: 73.77%\n",
      "Batch 150, Loss: 0.959056, Accuracy: 73.80%\n",
      "Batch 151, Loss: 0.934313, Accuracy: 73.85%\n",
      "Batch 152, Loss: 0.991322, Accuracy: 73.87%\n",
      "Batch 153, Loss: 1.034950, Accuracy: 73.85%\n",
      "Batch 154, Loss: 1.010295, Accuracy: 73.84%\n",
      "Batch 155, Loss: 1.043434, Accuracy: 73.81%\n",
      "Batch 156, Loss: 1.017903, Accuracy: 73.80%\n",
      "Batch 157, Loss: 1.001140, Accuracy: 73.81%\n",
      "Batch 158, Loss: 1.185712, Accuracy: 73.67%\n",
      "Batch 159, Loss: 1.110807, Accuracy: 73.60%\n",
      "Batch 160, Loss: 0.930559, Accuracy: 73.66%\n",
      "Batch 161, Loss: 1.062617, Accuracy: 73.62%\n",
      "Batch 162, Loss: 0.950979, Accuracy: 73.65%\n",
      "Batch 163, Loss: 1.020859, Accuracy: 73.64%\n",
      "Batch 164, Loss: 1.027696, Accuracy: 73.63%\n",
      "Batch 165, Loss: 0.985820, Accuracy: 73.65%\n",
      "Batch 166, Loss: 0.916223, Accuracy: 73.69%\n",
      "Batch 167, Loss: 0.979827, Accuracy: 73.72%\n",
      "Batch 168, Loss: 0.968635, Accuracy: 73.74%\n",
      "Batch 169, Loss: 1.061275, Accuracy: 73.72%\n",
      "Batch 170, Loss: 1.029199, Accuracy: 73.72%\n",
      "Batch 171, Loss: 0.990669, Accuracy: 73.74%\n",
      "Batch 172, Loss: 1.003694, Accuracy: 73.74%\n",
      "Batch 173, Loss: 1.013935, Accuracy: 73.72%\n",
      "Batch 174, Loss: 0.913528, Accuracy: 73.77%\n",
      "Batch 175, Loss: 1.020635, Accuracy: 73.76%\n",
      "Batch 176, Loss: 1.026883, Accuracy: 73.75%\n",
      "Batch 177, Loss: 1.053417, Accuracy: 73.71%\n",
      "Batch 178, Loss: 0.960435, Accuracy: 73.73%\n",
      "Batch 179, Loss: 1.095764, Accuracy: 73.66%\n",
      "Batch 180, Loss: 0.975922, Accuracy: 73.67%\n",
      "Batch 181, Loss: 0.936432, Accuracy: 73.72%\n",
      "Batch 182, Loss: 0.955974, Accuracy: 73.74%\n",
      "Batch 183, Loss: 1.036178, Accuracy: 73.71%\n",
      "Batch 184, Loss: 0.997945, Accuracy: 73.73%\n",
      "Batch 185, Loss: 0.940112, Accuracy: 73.77%\n",
      "Batch 186, Loss: 0.976806, Accuracy: 73.79%\n",
      "Batch 187, Loss: 1.091415, Accuracy: 73.76%\n",
      "Batch 188, Loss: 0.925454, Accuracy: 73.79%\n",
      "Batch 189, Loss: 1.083204, Accuracy: 73.75%\n",
      "Batch 190, Loss: 1.013801, Accuracy: 73.74%\n",
      "Batch 191, Loss: 0.964285, Accuracy: 73.76%\n",
      "Batch 192, Loss: 1.036965, Accuracy: 73.74%\n",
      "Batch 193, Loss: 0.978449, Accuracy: 73.75%\n",
      "Batch 194, Loss: 0.919699, Accuracy: 73.80%\n",
      "Batch 195, Loss: 1.006847, Accuracy: 73.80%\n",
      "Batch 196, Loss: 1.007871, Accuracy: 73.80%\n",
      "Batch 197, Loss: 1.062173, Accuracy: 73.76%\n",
      "Batch 198, Loss: 0.976789, Accuracy: 73.78%\n",
      "Batch 199, Loss: 1.099167, Accuracy: 73.72%\n",
      "Batch 200, Loss: 0.979342, Accuracy: 73.75%\n",
      "Batch 201, Loss: 0.940187, Accuracy: 73.79%\n",
      "Batch 202, Loss: 0.983092, Accuracy: 73.80%\n",
      "Batch 203, Loss: 0.936307, Accuracy: 73.83%\n",
      "Batch 204, Loss: 1.022672, Accuracy: 73.82%\n",
      "Batch 205, Loss: 0.983606, Accuracy: 73.83%\n",
      "Batch 206, Loss: 1.017452, Accuracy: 73.82%\n",
      "Batch 207, Loss: 0.970136, Accuracy: 73.84%\n",
      "Batch 208, Loss: 1.014100, Accuracy: 73.84%\n",
      "Batch 209, Loss: 0.923200, Accuracy: 73.88%\n",
      "Batch 210, Loss: 0.995986, Accuracy: 73.88%\n",
      "Batch 211, Loss: 1.035655, Accuracy: 73.87%\n",
      "Batch 212, Loss: 0.994539, Accuracy: 73.87%\n",
      "Batch 213, Loss: 1.005954, Accuracy: 73.87%\n",
      "Training - Epoch 56, Loss: 1.003095, Accuracy: 73.87%\n",
      "Validation Batch 1, Loss: 1.025174, Accuracy: 70.31%\n",
      "Validation Batch 2, Loss: 1.131199, Accuracy: 64.84%\n",
      "Validation Batch 3, Loss: 1.130217, Accuracy: 64.58%\n",
      "Validation Batch 4, Loss: 1.042253, Accuracy: 65.23%\n",
      "Validation Batch 5, Loss: 1.054784, Accuracy: 65.62%\n",
      "Validation Batch 6, Loss: 1.008755, Accuracy: 67.45%\n",
      "Validation Batch 7, Loss: 1.076878, Accuracy: 67.63%\n",
      "Validation Batch 8, Loss: 1.092575, Accuracy: 66.99%\n",
      "Validation Batch 9, Loss: 1.121344, Accuracy: 66.15%\n",
      "Validation Batch 10, Loss: 1.092237, Accuracy: 65.78%\n",
      "Validation Batch 11, Loss: 1.070410, Accuracy: 65.62%\n",
      "Validation Batch 12, Loss: 1.014748, Accuracy: 66.41%\n",
      "Validation Batch 13, Loss: 1.140977, Accuracy: 65.99%\n",
      "Validation Batch 14, Loss: 1.098816, Accuracy: 65.74%\n",
      "Validation Batch 15, Loss: 1.052432, Accuracy: 66.15%\n",
      "Validation Batch 16, Loss: 1.042708, Accuracy: 66.31%\n",
      "Validation Batch 17, Loss: 1.147267, Accuracy: 65.53%\n",
      "Validation Batch 18, Loss: 1.037810, Accuracy: 65.89%\n",
      "Validation Batch 19, Loss: 1.132299, Accuracy: 65.46%\n",
      "Validation Batch 20, Loss: 1.078687, Accuracy: 65.47%\n",
      "Validation Batch 21, Loss: 1.078651, Accuracy: 65.40%\n",
      "Validation Batch 22, Loss: 1.112042, Accuracy: 65.41%\n",
      "Validation Batch 23, Loss: 1.178896, Accuracy: 65.01%\n",
      "Validation Batch 24, Loss: 1.117086, Accuracy: 64.97%\n",
      "Validation Batch 25, Loss: 1.062152, Accuracy: 65.00%\n",
      "Validation Batch 26, Loss: 1.056586, Accuracy: 65.14%\n",
      "Validation Batch 27, Loss: 1.094939, Accuracy: 65.00%\n",
      "Validation - Epoch 56, Loss: 1.084886, Accuracy: 65.00%\n",
      "Patienceâ€”4\n",
      "Epoch 57\n",
      "Batch 1, Loss: 0.987784, Accuracy: 75.00%\n",
      "Batch 2, Loss: 1.038514, Accuracy: 71.88%\n",
      "Batch 3, Loss: 0.969579, Accuracy: 73.96%\n",
      "Batch 4, Loss: 0.939163, Accuracy: 75.78%\n",
      "Batch 5, Loss: 0.913781, Accuracy: 77.50%\n",
      "Batch 6, Loss: 1.011373, Accuracy: 76.82%\n",
      "Batch 7, Loss: 0.994344, Accuracy: 76.79%\n",
      "Batch 8, Loss: 1.093880, Accuracy: 75.20%\n",
      "Batch 9, Loss: 0.930478, Accuracy: 75.87%\n",
      "Batch 10, Loss: 0.954918, Accuracy: 76.09%\n",
      "Batch 11, Loss: 0.923089, Accuracy: 76.85%\n",
      "Batch 12, Loss: 0.975801, Accuracy: 76.56%\n",
      "Batch 13, Loss: 0.972364, Accuracy: 76.80%\n",
      "Batch 14, Loss: 1.050664, Accuracy: 76.34%\n",
      "Batch 15, Loss: 0.992659, Accuracy: 76.25%\n",
      "Batch 16, Loss: 1.003296, Accuracy: 76.17%\n",
      "Batch 17, Loss: 1.019512, Accuracy: 75.92%\n",
      "Batch 18, Loss: 1.018015, Accuracy: 75.61%\n",
      "Batch 19, Loss: 1.009995, Accuracy: 75.33%\n",
      "Batch 20, Loss: 1.043784, Accuracy: 75.00%\n",
      "Batch 21, Loss: 1.021497, Accuracy: 74.85%\n",
      "Batch 22, Loss: 0.946698, Accuracy: 75.00%\n",
      "Batch 23, Loss: 0.958021, Accuracy: 75.14%\n",
      "Batch 24, Loss: 0.995921, Accuracy: 74.93%\n",
      "Batch 25, Loss: 1.052361, Accuracy: 74.69%\n",
      "Batch 26, Loss: 0.933465, Accuracy: 74.94%\n",
      "Batch 27, Loss: 1.005447, Accuracy: 74.88%\n",
      "Batch 28, Loss: 0.984002, Accuracy: 74.94%\n",
      "Batch 29, Loss: 1.009498, Accuracy: 75.00%\n",
      "Batch 30, Loss: 0.940842, Accuracy: 75.21%\n",
      "Batch 31, Loss: 0.963651, Accuracy: 75.30%\n",
      "Batch 32, Loss: 0.984898, Accuracy: 75.24%\n",
      "Batch 33, Loss: 0.959497, Accuracy: 75.33%\n",
      "Batch 34, Loss: 0.975826, Accuracy: 75.32%\n",
      "Batch 35, Loss: 0.942172, Accuracy: 75.54%\n",
      "Batch 36, Loss: 0.983601, Accuracy: 75.48%\n",
      "Batch 37, Loss: 1.057222, Accuracy: 75.30%\n",
      "Batch 38, Loss: 1.023648, Accuracy: 75.16%\n",
      "Batch 39, Loss: 1.024963, Accuracy: 75.12%\n",
      "Batch 40, Loss: 1.032238, Accuracy: 75.00%\n",
      "Batch 41, Loss: 1.029590, Accuracy: 74.92%\n",
      "Batch 42, Loss: 0.965484, Accuracy: 75.00%\n",
      "Batch 43, Loss: 1.016518, Accuracy: 74.85%\n",
      "Batch 44, Loss: 1.054623, Accuracy: 74.72%\n",
      "Batch 45, Loss: 1.027288, Accuracy: 74.62%\n",
      "Batch 46, Loss: 1.048348, Accuracy: 74.52%\n",
      "Batch 47, Loss: 1.059096, Accuracy: 74.37%\n",
      "Batch 48, Loss: 1.025751, Accuracy: 74.32%\n",
      "Batch 49, Loss: 0.992350, Accuracy: 74.36%\n",
      "Batch 50, Loss: 1.052362, Accuracy: 74.19%\n",
      "Batch 51, Loss: 1.000704, Accuracy: 74.17%\n",
      "Batch 52, Loss: 1.026754, Accuracy: 74.13%\n",
      "Batch 53, Loss: 0.927222, Accuracy: 74.26%\n",
      "Batch 54, Loss: 1.013431, Accuracy: 74.22%\n",
      "Batch 55, Loss: 0.930371, Accuracy: 74.35%\n",
      "Batch 56, Loss: 1.013046, Accuracy: 74.33%\n",
      "Batch 57, Loss: 1.032248, Accuracy: 74.29%\n",
      "Batch 58, Loss: 1.061598, Accuracy: 74.19%\n",
      "Batch 59, Loss: 1.061340, Accuracy: 74.10%\n",
      "Batch 60, Loss: 1.006072, Accuracy: 74.04%\n",
      "Batch 61, Loss: 1.048265, Accuracy: 73.98%\n",
      "Batch 62, Loss: 1.010431, Accuracy: 73.97%\n",
      "Batch 63, Loss: 0.995708, Accuracy: 73.98%\n",
      "Batch 64, Loss: 0.975805, Accuracy: 74.02%\n",
      "Batch 65, Loss: 0.957175, Accuracy: 74.13%\n",
      "Batch 66, Loss: 1.007658, Accuracy: 74.12%\n",
      "Batch 67, Loss: 0.991184, Accuracy: 74.16%\n",
      "Batch 68, Loss: 1.076556, Accuracy: 74.01%\n",
      "Batch 69, Loss: 0.985298, Accuracy: 74.03%\n",
      "Batch 70, Loss: 0.976006, Accuracy: 74.04%\n",
      "Batch 71, Loss: 0.993744, Accuracy: 74.08%\n",
      "Batch 72, Loss: 0.968647, Accuracy: 74.15%\n",
      "Batch 73, Loss: 1.087142, Accuracy: 74.06%\n",
      "Batch 74, Loss: 0.938720, Accuracy: 74.16%\n",
      "Batch 75, Loss: 1.002496, Accuracy: 74.12%\n",
      "Batch 76, Loss: 1.136128, Accuracy: 73.99%\n",
      "Batch 77, Loss: 0.915622, Accuracy: 74.11%\n",
      "Batch 78, Loss: 1.017274, Accuracy: 74.04%\n",
      "Batch 79, Loss: 1.003281, Accuracy: 74.05%\n",
      "Batch 80, Loss: 0.961453, Accuracy: 74.08%\n",
      "Batch 81, Loss: 1.137532, Accuracy: 73.90%\n",
      "Batch 82, Loss: 0.978161, Accuracy: 73.91%\n",
      "Batch 83, Loss: 0.961657, Accuracy: 73.95%\n",
      "Batch 84, Loss: 0.971757, Accuracy: 73.98%\n",
      "Batch 85, Loss: 1.050164, Accuracy: 73.92%\n",
      "Batch 86, Loss: 0.938169, Accuracy: 74.00%\n",
      "Batch 87, Loss: 1.007561, Accuracy: 73.99%\n",
      "Batch 88, Loss: 1.008037, Accuracy: 73.99%\n",
      "Batch 89, Loss: 0.992247, Accuracy: 73.96%\n",
      "Batch 90, Loss: 0.947579, Accuracy: 74.01%\n",
      "Batch 91, Loss: 1.013164, Accuracy: 73.99%\n",
      "Batch 92, Loss: 1.010998, Accuracy: 73.96%\n",
      "Batch 93, Loss: 0.975125, Accuracy: 74.01%\n",
      "Batch 94, Loss: 1.056860, Accuracy: 73.95%\n",
      "Batch 95, Loss: 0.946115, Accuracy: 74.05%\n",
      "Batch 96, Loss: 1.017908, Accuracy: 74.04%\n",
      "Batch 97, Loss: 0.993034, Accuracy: 74.05%\n",
      "Batch 98, Loss: 0.958627, Accuracy: 74.12%\n",
      "Batch 99, Loss: 0.999126, Accuracy: 74.12%\n",
      "Batch 100, Loss: 0.988594, Accuracy: 74.12%\n",
      "Batch 101, Loss: 0.931055, Accuracy: 74.21%\n",
      "Batch 102, Loss: 0.993649, Accuracy: 74.20%\n",
      "Batch 103, Loss: 0.990348, Accuracy: 74.24%\n",
      "Batch 104, Loss: 1.021035, Accuracy: 74.22%\n",
      "Batch 105, Loss: 0.927791, Accuracy: 74.29%\n",
      "Batch 106, Loss: 0.949242, Accuracy: 74.34%\n",
      "Batch 107, Loss: 0.994468, Accuracy: 74.34%\n",
      "Batch 108, Loss: 0.965716, Accuracy: 74.35%\n",
      "Batch 109, Loss: 1.027093, Accuracy: 74.31%\n",
      "Batch 110, Loss: 0.966793, Accuracy: 74.35%\n",
      "Batch 111, Loss: 1.012580, Accuracy: 74.34%\n",
      "Batch 112, Loss: 0.991940, Accuracy: 74.33%\n",
      "Batch 113, Loss: 0.998976, Accuracy: 74.35%\n",
      "Batch 114, Loss: 1.030158, Accuracy: 74.31%\n",
      "Batch 115, Loss: 1.092889, Accuracy: 74.24%\n",
      "Batch 116, Loss: 1.039769, Accuracy: 74.19%\n",
      "Batch 117, Loss: 1.108472, Accuracy: 74.11%\n",
      "Batch 118, Loss: 1.071724, Accuracy: 74.02%\n",
      "Batch 119, Loss: 1.004386, Accuracy: 74.02%\n",
      "Batch 120, Loss: 0.984598, Accuracy: 74.01%\n",
      "Batch 121, Loss: 0.969672, Accuracy: 74.02%\n",
      "Batch 122, Loss: 1.015737, Accuracy: 74.01%\n",
      "Batch 123, Loss: 1.048599, Accuracy: 73.97%\n",
      "Batch 124, Loss: 1.119461, Accuracy: 73.85%\n",
      "Batch 125, Loss: 0.974250, Accuracy: 73.90%\n",
      "Batch 126, Loss: 0.972346, Accuracy: 73.92%\n",
      "Batch 127, Loss: 1.012630, Accuracy: 73.92%\n",
      "Batch 128, Loss: 0.925344, Accuracy: 73.97%\n",
      "Batch 129, Loss: 1.079634, Accuracy: 73.92%\n",
      "Batch 130, Loss: 1.044926, Accuracy: 73.88%\n",
      "Batch 131, Loss: 0.950522, Accuracy: 73.93%\n",
      "Batch 132, Loss: 0.958784, Accuracy: 73.97%\n",
      "Batch 133, Loss: 1.032992, Accuracy: 73.94%\n",
      "Batch 134, Loss: 1.019710, Accuracy: 73.93%\n",
      "Batch 135, Loss: 0.991689, Accuracy: 73.94%\n",
      "Batch 136, Loss: 0.972607, Accuracy: 73.95%\n",
      "Batch 137, Loss: 1.015174, Accuracy: 73.93%\n",
      "Batch 138, Loss: 1.094603, Accuracy: 73.87%\n",
      "Batch 139, Loss: 0.960829, Accuracy: 73.90%\n",
      "Batch 140, Loss: 1.005105, Accuracy: 73.90%\n",
      "Batch 141, Loss: 1.026804, Accuracy: 73.86%\n",
      "Batch 142, Loss: 1.016278, Accuracy: 73.83%\n",
      "Batch 143, Loss: 1.073542, Accuracy: 73.77%\n",
      "Batch 144, Loss: 0.912982, Accuracy: 73.83%\n",
      "Batch 145, Loss: 0.979050, Accuracy: 73.84%\n",
      "Batch 146, Loss: 1.001870, Accuracy: 73.84%\n",
      "Batch 147, Loss: 1.140241, Accuracy: 73.75%\n",
      "Batch 148, Loss: 0.911730, Accuracy: 73.83%\n",
      "Batch 149, Loss: 0.972085, Accuracy: 73.85%\n",
      "Batch 150, Loss: 0.951320, Accuracy: 73.88%\n",
      "Batch 151, Loss: 0.934921, Accuracy: 73.92%\n",
      "Batch 152, Loss: 0.970796, Accuracy: 73.95%\n",
      "Batch 153, Loss: 1.000390, Accuracy: 73.96%\n",
      "Batch 154, Loss: 0.961213, Accuracy: 73.98%\n",
      "Batch 155, Loss: 1.010093, Accuracy: 73.98%\n",
      "Batch 156, Loss: 1.012706, Accuracy: 73.97%\n",
      "Batch 157, Loss: 1.033878, Accuracy: 73.95%\n",
      "Batch 158, Loss: 1.108830, Accuracy: 73.87%\n",
      "Batch 159, Loss: 1.037334, Accuracy: 73.84%\n",
      "Batch 160, Loss: 1.038208, Accuracy: 73.82%\n",
      "Batch 161, Loss: 0.971680, Accuracy: 73.84%\n",
      "Batch 162, Loss: 0.945665, Accuracy: 73.88%\n",
      "Batch 163, Loss: 0.972777, Accuracy: 73.92%\n",
      "Batch 164, Loss: 1.005226, Accuracy: 73.91%\n",
      "Batch 165, Loss: 1.041522, Accuracy: 73.88%\n",
      "Batch 166, Loss: 1.037107, Accuracy: 73.84%\n",
      "Batch 167, Loss: 0.905917, Accuracy: 73.91%\n",
      "Batch 168, Loss: 0.933248, Accuracy: 73.95%\n",
      "Batch 169, Loss: 1.025782, Accuracy: 73.93%\n",
      "Batch 170, Loss: 1.001317, Accuracy: 73.92%\n",
      "Batch 171, Loss: 0.951691, Accuracy: 73.93%\n",
      "Batch 172, Loss: 1.079126, Accuracy: 73.88%\n",
      "Batch 173, Loss: 1.083412, Accuracy: 73.83%\n",
      "Batch 174, Loss: 0.991572, Accuracy: 73.84%\n",
      "Batch 175, Loss: 1.058356, Accuracy: 73.81%\n",
      "Batch 176, Loss: 1.022455, Accuracy: 73.80%\n",
      "Batch 177, Loss: 1.021301, Accuracy: 73.78%\n",
      "Batch 178, Loss: 0.984400, Accuracy: 73.79%\n",
      "Batch 179, Loss: 1.002232, Accuracy: 73.79%\n",
      "Batch 180, Loss: 1.040201, Accuracy: 73.76%\n",
      "Batch 181, Loss: 0.992863, Accuracy: 73.77%\n",
      "Batch 182, Loss: 0.945538, Accuracy: 73.80%\n",
      "Batch 183, Loss: 1.095135, Accuracy: 73.74%\n",
      "Batch 184, Loss: 1.021661, Accuracy: 73.73%\n",
      "Batch 185, Loss: 1.039603, Accuracy: 73.72%\n",
      "Batch 186, Loss: 0.893787, Accuracy: 73.77%\n",
      "Batch 187, Loss: 0.957995, Accuracy: 73.80%\n",
      "Batch 188, Loss: 1.018595, Accuracy: 73.78%\n",
      "Batch 189, Loss: 0.885709, Accuracy: 73.85%\n",
      "Batch 190, Loss: 0.995483, Accuracy: 73.85%\n",
      "Batch 191, Loss: 1.043639, Accuracy: 73.84%\n",
      "Batch 192, Loss: 0.947171, Accuracy: 73.87%\n",
      "Batch 193, Loss: 1.058279, Accuracy: 73.84%\n",
      "Batch 194, Loss: 0.977390, Accuracy: 73.86%\n",
      "Batch 195, Loss: 0.984503, Accuracy: 73.87%\n",
      "Batch 196, Loss: 1.095639, Accuracy: 73.82%\n",
      "Batch 197, Loss: 0.933983, Accuracy: 73.84%\n",
      "Batch 198, Loss: 0.975179, Accuracy: 73.85%\n",
      "Batch 199, Loss: 1.010535, Accuracy: 73.84%\n",
      "Batch 200, Loss: 0.908741, Accuracy: 73.90%\n",
      "Batch 201, Loss: 1.007019, Accuracy: 73.88%\n",
      "Batch 202, Loss: 1.062570, Accuracy: 73.85%\n",
      "Batch 203, Loss: 0.963673, Accuracy: 73.86%\n",
      "Batch 204, Loss: 0.951559, Accuracy: 73.88%\n",
      "Batch 205, Loss: 0.975467, Accuracy: 73.90%\n",
      "Batch 206, Loss: 1.026849, Accuracy: 73.89%\n",
      "Batch 207, Loss: 1.039571, Accuracy: 73.88%\n",
      "Batch 208, Loss: 0.996085, Accuracy: 73.88%\n",
      "Batch 209, Loss: 1.011885, Accuracy: 73.87%\n",
      "Batch 210, Loss: 0.904523, Accuracy: 73.92%\n",
      "Batch 211, Loss: 0.972734, Accuracy: 73.94%\n",
      "Batch 212, Loss: 1.026565, Accuracy: 73.92%\n",
      "Batch 213, Loss: 0.966014, Accuracy: 73.94%\n",
      "Training - Epoch 57, Loss: 1.000498, Accuracy: 73.94%\n",
      "Validation Batch 1, Loss: 1.017556, Accuracy: 73.44%\n",
      "Validation Batch 2, Loss: 1.127337, Accuracy: 67.97%\n",
      "Validation Batch 3, Loss: 1.126946, Accuracy: 66.67%\n",
      "Validation Batch 4, Loss: 1.042886, Accuracy: 67.58%\n",
      "Validation Batch 5, Loss: 1.052312, Accuracy: 67.50%\n",
      "Validation Batch 6, Loss: 1.002668, Accuracy: 68.75%\n",
      "Validation Batch 7, Loss: 1.074332, Accuracy: 68.97%\n",
      "Validation Batch 8, Loss: 1.090135, Accuracy: 68.36%\n",
      "Validation Batch 9, Loss: 1.121921, Accuracy: 67.36%\n",
      "Validation Batch 10, Loss: 1.085227, Accuracy: 67.34%\n",
      "Validation Batch 11, Loss: 1.062299, Accuracy: 67.19%\n",
      "Validation Batch 12, Loss: 1.008850, Accuracy: 67.84%\n",
      "Validation Batch 13, Loss: 1.142047, Accuracy: 67.19%\n",
      "Validation Batch 14, Loss: 1.095077, Accuracy: 66.96%\n",
      "Validation Batch 15, Loss: 1.048225, Accuracy: 67.40%\n",
      "Validation Batch 16, Loss: 1.042594, Accuracy: 67.48%\n",
      "Validation Batch 17, Loss: 1.139145, Accuracy: 66.82%\n",
      "Validation Batch 18, Loss: 1.037793, Accuracy: 67.01%\n",
      "Validation Batch 19, Loss: 1.134192, Accuracy: 66.53%\n",
      "Validation Batch 20, Loss: 1.082273, Accuracy: 66.48%\n",
      "Validation Batch 21, Loss: 1.075809, Accuracy: 66.37%\n",
      "Validation Batch 22, Loss: 1.110401, Accuracy: 66.41%\n",
      "Validation Batch 23, Loss: 1.175000, Accuracy: 65.96%\n",
      "Validation Batch 24, Loss: 1.117035, Accuracy: 65.89%\n",
      "Validation Batch 25, Loss: 1.058803, Accuracy: 66.00%\n",
      "Validation Batch 26, Loss: 1.054929, Accuracy: 66.17%\n",
      "Validation Batch 27, Loss: 1.084317, Accuracy: 66.29%\n",
      "Validation - Epoch 57, Loss: 1.081856, Accuracy: 66.29%\n",
      "Patienceâ€”5\n",
      "Epoch 58\n",
      "Batch 1, Loss: 0.971824, Accuracy: 76.56%\n",
      "Batch 2, Loss: 0.969440, Accuracy: 77.34%\n",
      "Batch 3, Loss: 0.927592, Accuracy: 79.17%\n",
      "Batch 4, Loss: 1.063623, Accuracy: 76.17%\n",
      "Batch 5, Loss: 0.998658, Accuracy: 75.31%\n",
      "Batch 6, Loss: 1.072336, Accuracy: 73.96%\n",
      "Batch 7, Loss: 1.021356, Accuracy: 73.66%\n",
      "Batch 8, Loss: 0.968883, Accuracy: 74.22%\n",
      "Batch 9, Loss: 1.043640, Accuracy: 73.61%\n",
      "Batch 10, Loss: 1.001397, Accuracy: 73.59%\n",
      "Batch 11, Loss: 1.025515, Accuracy: 73.30%\n",
      "Batch 12, Loss: 1.022787, Accuracy: 73.18%\n",
      "Batch 13, Loss: 0.991798, Accuracy: 73.20%\n",
      "Batch 14, Loss: 1.052394, Accuracy: 72.77%\n",
      "Batch 15, Loss: 0.974321, Accuracy: 72.92%\n",
      "Batch 16, Loss: 0.924372, Accuracy: 73.44%\n",
      "Batch 17, Loss: 1.002689, Accuracy: 73.53%\n",
      "Batch 18, Loss: 0.913438, Accuracy: 74.13%\n",
      "Batch 19, Loss: 0.896225, Accuracy: 74.67%\n",
      "Batch 20, Loss: 1.074738, Accuracy: 74.30%\n",
      "Batch 21, Loss: 1.049561, Accuracy: 73.96%\n",
      "Batch 22, Loss: 0.992964, Accuracy: 74.08%\n",
      "Batch 23, Loss: 1.030812, Accuracy: 73.91%\n",
      "Batch 24, Loss: 1.038273, Accuracy: 73.76%\n",
      "Batch 25, Loss: 1.001926, Accuracy: 73.75%\n",
      "Batch 26, Loss: 1.070191, Accuracy: 73.44%\n",
      "Batch 27, Loss: 0.991374, Accuracy: 73.50%\n",
      "Batch 28, Loss: 1.069831, Accuracy: 73.27%\n",
      "Batch 29, Loss: 0.942353, Accuracy: 73.55%\n",
      "Batch 30, Loss: 1.041786, Accuracy: 73.44%\n",
      "Batch 31, Loss: 1.061850, Accuracy: 73.29%\n",
      "Batch 32, Loss: 1.062976, Accuracy: 73.05%\n",
      "Batch 33, Loss: 1.021910, Accuracy: 73.01%\n",
      "Batch 34, Loss: 1.028988, Accuracy: 72.93%\n",
      "Batch 35, Loss: 0.989630, Accuracy: 72.99%\n",
      "Batch 36, Loss: 1.131142, Accuracy: 72.66%\n",
      "Batch 37, Loss: 0.921602, Accuracy: 72.93%\n",
      "Batch 38, Loss: 0.920169, Accuracy: 73.19%\n",
      "Batch 39, Loss: 0.949771, Accuracy: 73.40%\n",
      "Batch 40, Loss: 1.085519, Accuracy: 73.20%\n",
      "Batch 41, Loss: 0.960322, Accuracy: 73.29%\n",
      "Batch 42, Loss: 0.904897, Accuracy: 73.51%\n",
      "Batch 43, Loss: 0.973424, Accuracy: 73.66%\n",
      "Batch 44, Loss: 1.010160, Accuracy: 73.65%\n",
      "Batch 45, Loss: 1.030866, Accuracy: 73.58%\n",
      "Batch 46, Loss: 0.927175, Accuracy: 73.78%\n",
      "Batch 47, Loss: 0.976919, Accuracy: 73.84%\n",
      "Batch 48, Loss: 1.056624, Accuracy: 73.73%\n",
      "Batch 49, Loss: 0.970265, Accuracy: 73.82%\n",
      "Batch 50, Loss: 1.078742, Accuracy: 73.62%\n",
      "Batch 51, Loss: 1.000179, Accuracy: 73.62%\n",
      "Batch 52, Loss: 1.002371, Accuracy: 73.59%\n",
      "Batch 53, Loss: 1.002462, Accuracy: 73.58%\n",
      "Batch 54, Loss: 0.932450, Accuracy: 73.73%\n",
      "Batch 55, Loss: 1.014966, Accuracy: 73.72%\n",
      "Batch 56, Loss: 1.040662, Accuracy: 73.58%\n",
      "Batch 57, Loss: 1.031757, Accuracy: 73.55%\n",
      "Batch 58, Loss: 1.042604, Accuracy: 73.46%\n",
      "Batch 59, Loss: 1.029862, Accuracy: 73.38%\n",
      "Batch 60, Loss: 1.021414, Accuracy: 73.36%\n",
      "Batch 61, Loss: 0.898847, Accuracy: 73.54%\n",
      "Batch 62, Loss: 1.015637, Accuracy: 73.54%\n",
      "Batch 63, Loss: 1.081356, Accuracy: 73.41%\n",
      "Batch 64, Loss: 0.960258, Accuracy: 73.51%\n",
      "Batch 65, Loss: 0.953363, Accuracy: 73.58%\n",
      "Batch 66, Loss: 1.047310, Accuracy: 73.48%\n",
      "Batch 67, Loss: 0.952429, Accuracy: 73.60%\n",
      "Batch 68, Loss: 0.969717, Accuracy: 73.64%\n",
      "Batch 69, Loss: 0.985778, Accuracy: 73.66%\n",
      "Batch 70, Loss: 0.935596, Accuracy: 73.77%\n",
      "Batch 71, Loss: 0.907470, Accuracy: 73.92%\n",
      "Batch 72, Loss: 1.032227, Accuracy: 73.87%\n",
      "Batch 73, Loss: 0.937894, Accuracy: 73.95%\n",
      "Batch 74, Loss: 1.031098, Accuracy: 73.90%\n",
      "Batch 75, Loss: 1.022929, Accuracy: 73.85%\n",
      "Batch 76, Loss: 1.047413, Accuracy: 73.81%\n",
      "Batch 77, Loss: 1.016166, Accuracy: 73.78%\n",
      "Batch 78, Loss: 1.032382, Accuracy: 73.74%\n",
      "Batch 79, Loss: 0.975744, Accuracy: 73.77%\n",
      "Batch 80, Loss: 0.999456, Accuracy: 73.81%\n",
      "Batch 81, Loss: 0.983217, Accuracy: 73.84%\n",
      "Batch 82, Loss: 1.029889, Accuracy: 73.78%\n",
      "Batch 83, Loss: 1.029438, Accuracy: 73.76%\n",
      "Batch 84, Loss: 0.901201, Accuracy: 73.90%\n",
      "Batch 85, Loss: 1.049348, Accuracy: 73.82%\n",
      "Batch 86, Loss: 0.973136, Accuracy: 73.86%\n",
      "Batch 87, Loss: 1.023760, Accuracy: 73.81%\n",
      "Batch 88, Loss: 0.931497, Accuracy: 73.92%\n",
      "Batch 89, Loss: 1.039131, Accuracy: 73.88%\n",
      "Batch 90, Loss: 0.891026, Accuracy: 74.03%\n",
      "Batch 91, Loss: 0.962009, Accuracy: 74.09%\n",
      "Batch 92, Loss: 0.903374, Accuracy: 74.18%\n",
      "Batch 93, Loss: 1.020564, Accuracy: 74.16%\n",
      "Batch 94, Loss: 0.974849, Accuracy: 74.15%\n",
      "Batch 95, Loss: 0.966160, Accuracy: 74.19%\n",
      "Batch 96, Loss: 1.008269, Accuracy: 74.19%\n",
      "Batch 97, Loss: 0.958882, Accuracy: 74.23%\n",
      "Batch 98, Loss: 0.942901, Accuracy: 74.30%\n",
      "Batch 99, Loss: 1.096103, Accuracy: 74.23%\n",
      "Batch 100, Loss: 1.048573, Accuracy: 74.17%\n",
      "Batch 101, Loss: 1.019880, Accuracy: 74.16%\n",
      "Batch 102, Loss: 1.026400, Accuracy: 74.13%\n",
      "Batch 103, Loss: 0.972216, Accuracy: 74.17%\n",
      "Batch 104, Loss: 1.032012, Accuracy: 74.11%\n",
      "Batch 105, Loss: 1.023610, Accuracy: 74.09%\n",
      "Batch 106, Loss: 0.963706, Accuracy: 74.13%\n",
      "Batch 107, Loss: 1.028394, Accuracy: 74.08%\n",
      "Batch 108, Loss: 1.049078, Accuracy: 74.03%\n",
      "Batch 109, Loss: 0.898841, Accuracy: 74.13%\n",
      "Batch 110, Loss: 1.132645, Accuracy: 74.01%\n",
      "Batch 111, Loss: 0.907311, Accuracy: 74.11%\n",
      "Batch 112, Loss: 1.030112, Accuracy: 74.07%\n",
      "Batch 113, Loss: 0.988432, Accuracy: 74.07%\n",
      "Batch 114, Loss: 0.958808, Accuracy: 74.12%\n",
      "Batch 115, Loss: 0.949645, Accuracy: 74.17%\n",
      "Batch 116, Loss: 0.959737, Accuracy: 74.21%\n",
      "Batch 117, Loss: 0.904805, Accuracy: 74.31%\n",
      "Batch 118, Loss: 1.029510, Accuracy: 74.28%\n",
      "Batch 119, Loss: 0.935072, Accuracy: 74.36%\n",
      "Batch 120, Loss: 0.949698, Accuracy: 74.39%\n",
      "Batch 121, Loss: 0.972257, Accuracy: 74.41%\n",
      "Batch 122, Loss: 0.917559, Accuracy: 74.47%\n",
      "Batch 123, Loss: 1.085824, Accuracy: 74.42%\n",
      "Batch 124, Loss: 0.990742, Accuracy: 74.43%\n",
      "Batch 125, Loss: 0.976243, Accuracy: 74.46%\n",
      "Batch 126, Loss: 1.018768, Accuracy: 74.45%\n",
      "Batch 127, Loss: 0.968529, Accuracy: 74.48%\n",
      "Batch 128, Loss: 0.987508, Accuracy: 74.50%\n",
      "Batch 129, Loss: 0.951759, Accuracy: 74.54%\n",
      "Batch 130, Loss: 0.950407, Accuracy: 74.59%\n",
      "Batch 131, Loss: 0.979405, Accuracy: 74.62%\n",
      "Batch 132, Loss: 1.102592, Accuracy: 74.53%\n",
      "Batch 133, Loss: 1.026159, Accuracy: 74.51%\n",
      "Batch 134, Loss: 0.911826, Accuracy: 74.58%\n",
      "Batch 135, Loss: 0.946418, Accuracy: 74.63%\n",
      "Batch 136, Loss: 0.992140, Accuracy: 74.62%\n",
      "Batch 137, Loss: 0.983557, Accuracy: 74.64%\n",
      "Batch 138, Loss: 1.026338, Accuracy: 74.62%\n",
      "Batch 139, Loss: 0.981915, Accuracy: 74.62%\n",
      "Batch 140, Loss: 1.006618, Accuracy: 74.60%\n",
      "Batch 141, Loss: 0.985520, Accuracy: 74.61%\n",
      "Batch 142, Loss: 1.068399, Accuracy: 74.56%\n",
      "Batch 143, Loss: 1.012101, Accuracy: 74.56%\n",
      "Batch 144, Loss: 1.006927, Accuracy: 74.57%\n",
      "Batch 145, Loss: 1.045375, Accuracy: 74.53%\n",
      "Batch 146, Loss: 1.044367, Accuracy: 74.49%\n",
      "Batch 147, Loss: 1.089342, Accuracy: 74.42%\n",
      "Batch 148, Loss: 1.027904, Accuracy: 74.40%\n",
      "Batch 149, Loss: 1.077608, Accuracy: 74.33%\n",
      "Batch 150, Loss: 0.984916, Accuracy: 74.34%\n",
      "Batch 151, Loss: 1.059040, Accuracy: 74.30%\n",
      "Batch 152, Loss: 1.013463, Accuracy: 74.27%\n",
      "Batch 153, Loss: 1.009117, Accuracy: 74.25%\n",
      "Batch 154, Loss: 0.952379, Accuracy: 74.28%\n",
      "Batch 155, Loss: 0.986076, Accuracy: 74.28%\n",
      "Batch 156, Loss: 0.990667, Accuracy: 74.29%\n",
      "Batch 157, Loss: 1.049178, Accuracy: 74.24%\n",
      "Batch 158, Loss: 0.917229, Accuracy: 74.31%\n",
      "Batch 159, Loss: 1.098495, Accuracy: 74.24%\n",
      "Batch 160, Loss: 0.974791, Accuracy: 74.26%\n",
      "Batch 161, Loss: 1.003404, Accuracy: 74.25%\n",
      "Batch 162, Loss: 0.875511, Accuracy: 74.34%\n",
      "Batch 163, Loss: 1.014392, Accuracy: 74.33%\n",
      "Batch 164, Loss: 0.968124, Accuracy: 74.33%\n",
      "Batch 165, Loss: 1.046823, Accuracy: 74.30%\n",
      "Batch 166, Loss: 1.052082, Accuracy: 74.26%\n",
      "Batch 167, Loss: 1.018723, Accuracy: 74.25%\n",
      "Batch 168, Loss: 1.003187, Accuracy: 74.25%\n",
      "Batch 169, Loss: 1.075368, Accuracy: 74.20%\n",
      "Batch 170, Loss: 0.936952, Accuracy: 74.23%\n",
      "Batch 171, Loss: 0.969796, Accuracy: 74.25%\n",
      "Batch 172, Loss: 0.936213, Accuracy: 74.27%\n",
      "Batch 173, Loss: 0.969325, Accuracy: 74.30%\n",
      "Batch 174, Loss: 0.936883, Accuracy: 74.33%\n",
      "Batch 175, Loss: 1.057488, Accuracy: 74.28%\n",
      "Batch 176, Loss: 0.940645, Accuracy: 74.32%\n",
      "Batch 177, Loss: 1.008302, Accuracy: 74.32%\n",
      "Batch 178, Loss: 0.961549, Accuracy: 74.33%\n",
      "Batch 179, Loss: 0.942448, Accuracy: 74.37%\n",
      "Batch 180, Loss: 1.027993, Accuracy: 74.36%\n",
      "Batch 181, Loss: 1.080436, Accuracy: 74.31%\n",
      "Batch 182, Loss: 1.010230, Accuracy: 74.28%\n",
      "Batch 183, Loss: 0.976665, Accuracy: 74.28%\n",
      "Batch 184, Loss: 0.965004, Accuracy: 74.30%\n",
      "Batch 185, Loss: 1.054034, Accuracy: 74.27%\n",
      "Batch 186, Loss: 0.943943, Accuracy: 74.30%\n",
      "Batch 187, Loss: 1.024704, Accuracy: 74.30%\n",
      "Batch 188, Loss: 0.982318, Accuracy: 74.32%\n",
      "Batch 189, Loss: 0.923798, Accuracy: 74.36%\n",
      "Batch 190, Loss: 0.925715, Accuracy: 74.41%\n",
      "Batch 191, Loss: 1.066124, Accuracy: 74.36%\n",
      "Batch 192, Loss: 1.038188, Accuracy: 74.33%\n",
      "Batch 193, Loss: 1.025764, Accuracy: 74.33%\n",
      "Batch 194, Loss: 0.911897, Accuracy: 74.37%\n",
      "Batch 195, Loss: 0.930637, Accuracy: 74.42%\n",
      "Batch 196, Loss: 0.932693, Accuracy: 74.46%\n",
      "Batch 197, Loss: 1.005478, Accuracy: 74.44%\n",
      "Batch 198, Loss: 1.096378, Accuracy: 74.38%\n",
      "Batch 199, Loss: 1.115337, Accuracy: 74.32%\n",
      "Batch 200, Loss: 0.971987, Accuracy: 74.34%\n",
      "Batch 201, Loss: 0.994332, Accuracy: 74.35%\n",
      "Batch 202, Loss: 1.024323, Accuracy: 74.33%\n",
      "Batch 203, Loss: 1.056088, Accuracy: 74.30%\n",
      "Batch 204, Loss: 0.998622, Accuracy: 74.30%\n",
      "Batch 205, Loss: 0.986668, Accuracy: 74.31%\n",
      "Batch 206, Loss: 1.027846, Accuracy: 74.29%\n",
      "Batch 207, Loss: 0.946001, Accuracy: 74.31%\n",
      "Batch 208, Loss: 1.021175, Accuracy: 74.29%\n",
      "Batch 209, Loss: 0.914696, Accuracy: 74.34%\n",
      "Batch 210, Loss: 0.967541, Accuracy: 74.36%\n",
      "Batch 211, Loss: 0.942837, Accuracy: 74.39%\n",
      "Batch 212, Loss: 0.967592, Accuracy: 74.40%\n",
      "Batch 213, Loss: 0.991946, Accuracy: 74.40%\n",
      "Training - Epoch 58, Loss: 0.996994, Accuracy: 74.40%\n",
      "Validation Batch 1, Loss: 0.978247, Accuracy: 78.12%\n",
      "Validation Batch 2, Loss: 1.076820, Accuracy: 72.66%\n",
      "Validation Batch 3, Loss: 1.086448, Accuracy: 70.31%\n",
      "Validation Batch 4, Loss: 1.018519, Accuracy: 70.70%\n",
      "Validation Batch 5, Loss: 1.005894, Accuracy: 71.56%\n",
      "Validation Batch 6, Loss: 0.965630, Accuracy: 72.40%\n",
      "Validation Batch 7, Loss: 1.045257, Accuracy: 72.10%\n",
      "Validation Batch 8, Loss: 1.065314, Accuracy: 71.29%\n",
      "Validation Batch 9, Loss: 1.095698, Accuracy: 70.14%\n",
      "Validation Batch 10, Loss: 1.045087, Accuracy: 70.16%\n",
      "Validation Batch 11, Loss: 1.010562, Accuracy: 70.45%\n",
      "Validation Batch 12, Loss: 0.968091, Accuracy: 70.83%\n",
      "Validation Batch 13, Loss: 1.112505, Accuracy: 70.19%\n",
      "Validation Batch 14, Loss: 1.049385, Accuracy: 70.20%\n",
      "Validation Batch 15, Loss: 1.017792, Accuracy: 70.42%\n",
      "Validation Batch 16, Loss: 0.995097, Accuracy: 70.90%\n",
      "Validation Batch 17, Loss: 1.066280, Accuracy: 70.96%\n",
      "Validation Batch 18, Loss: 1.006052, Accuracy: 71.18%\n",
      "Validation Batch 19, Loss: 1.072898, Accuracy: 70.97%\n",
      "Validation Batch 20, Loss: 1.032105, Accuracy: 71.02%\n",
      "Validation Batch 21, Loss: 1.027918, Accuracy: 71.06%\n",
      "Validation Batch 22, Loss: 1.058623, Accuracy: 70.95%\n",
      "Validation Batch 23, Loss: 1.136696, Accuracy: 70.58%\n",
      "Validation Batch 24, Loss: 1.091855, Accuracy: 70.31%\n",
      "Validation Batch 25, Loss: 1.015315, Accuracy: 70.50%\n",
      "Validation Batch 26, Loss: 1.029295, Accuracy: 70.49%\n",
      "Validation Batch 27, Loss: 1.030673, Accuracy: 70.52%\n",
      "Validation - Epoch 58, Loss: 1.040891, Accuracy: 70.52%\n",
      "Patienceâ€”0\n",
      "Epoch 59\n",
      "Batch 1, Loss: 0.997008, Accuracy: 75.00%\n",
      "Batch 2, Loss: 1.016626, Accuracy: 73.44%\n",
      "Batch 3, Loss: 0.884563, Accuracy: 77.60%\n",
      "Batch 4, Loss: 0.962092, Accuracy: 77.73%\n",
      "Batch 5, Loss: 1.046151, Accuracy: 75.94%\n",
      "Batch 6, Loss: 1.045186, Accuracy: 74.74%\n",
      "Batch 7, Loss: 1.086731, Accuracy: 73.44%\n",
      "Batch 8, Loss: 0.989818, Accuracy: 73.63%\n",
      "Batch 9, Loss: 1.121304, Accuracy: 72.22%\n",
      "Batch 10, Loss: 0.943027, Accuracy: 73.12%\n",
      "Batch 11, Loss: 1.049877, Accuracy: 72.87%\n",
      "Batch 12, Loss: 1.032362, Accuracy: 72.66%\n",
      "Batch 13, Loss: 0.988809, Accuracy: 72.60%\n",
      "Batch 14, Loss: 0.991152, Accuracy: 72.77%\n",
      "Batch 15, Loss: 0.944564, Accuracy: 73.23%\n",
      "Batch 16, Loss: 1.026818, Accuracy: 73.05%\n",
      "Batch 17, Loss: 1.035581, Accuracy: 72.89%\n",
      "Batch 18, Loss: 0.946257, Accuracy: 73.26%\n",
      "Batch 19, Loss: 1.074457, Accuracy: 72.94%\n",
      "Batch 20, Loss: 1.006343, Accuracy: 72.97%\n",
      "Batch 21, Loss: 0.897979, Accuracy: 73.59%\n",
      "Batch 22, Loss: 1.060576, Accuracy: 73.37%\n",
      "Batch 23, Loss: 0.940274, Accuracy: 73.57%\n",
      "Batch 24, Loss: 1.038716, Accuracy: 73.37%\n",
      "Batch 25, Loss: 0.991598, Accuracy: 73.44%\n",
      "Batch 26, Loss: 1.011917, Accuracy: 73.44%\n",
      "Batch 27, Loss: 1.014940, Accuracy: 73.38%\n",
      "Batch 28, Loss: 0.962718, Accuracy: 73.60%\n",
      "Batch 29, Loss: 1.033521, Accuracy: 73.55%\n",
      "Batch 30, Loss: 0.998043, Accuracy: 73.54%\n",
      "Batch 31, Loss: 0.945091, Accuracy: 73.69%\n",
      "Batch 32, Loss: 0.985880, Accuracy: 73.83%\n",
      "Batch 33, Loss: 0.983017, Accuracy: 73.91%\n",
      "Batch 34, Loss: 0.966987, Accuracy: 74.03%\n",
      "Batch 35, Loss: 1.096792, Accuracy: 73.75%\n",
      "Batch 36, Loss: 0.994377, Accuracy: 73.74%\n",
      "Batch 37, Loss: 0.990772, Accuracy: 73.73%\n",
      "Batch 38, Loss: 1.058860, Accuracy: 73.60%\n",
      "Batch 39, Loss: 1.030309, Accuracy: 73.56%\n",
      "Batch 40, Loss: 0.977011, Accuracy: 73.63%\n",
      "Batch 41, Loss: 1.013221, Accuracy: 73.59%\n",
      "Batch 42, Loss: 0.966448, Accuracy: 73.62%\n",
      "Batch 43, Loss: 1.063537, Accuracy: 73.47%\n",
      "Batch 44, Loss: 1.029687, Accuracy: 73.37%\n",
      "Batch 45, Loss: 1.025789, Accuracy: 73.33%\n",
      "Batch 46, Loss: 0.986386, Accuracy: 73.40%\n",
      "Batch 47, Loss: 0.963064, Accuracy: 73.54%\n",
      "Batch 48, Loss: 1.078135, Accuracy: 73.37%\n",
      "Batch 49, Loss: 0.988480, Accuracy: 73.34%\n",
      "Batch 50, Loss: 1.048351, Accuracy: 73.25%\n",
      "Batch 51, Loss: 1.043733, Accuracy: 73.16%\n",
      "Batch 52, Loss: 1.087403, Accuracy: 72.99%\n",
      "Batch 53, Loss: 0.955493, Accuracy: 73.11%\n",
      "Batch 54, Loss: 0.920291, Accuracy: 73.29%\n",
      "Batch 55, Loss: 0.914270, Accuracy: 73.49%\n",
      "Batch 56, Loss: 0.977366, Accuracy: 73.55%\n",
      "Batch 57, Loss: 0.972574, Accuracy: 73.63%\n",
      "Batch 58, Loss: 0.859419, Accuracy: 73.90%\n",
      "Batch 59, Loss: 0.980609, Accuracy: 73.94%\n",
      "Batch 60, Loss: 0.972961, Accuracy: 73.98%\n",
      "Batch 61, Loss: 1.007991, Accuracy: 74.00%\n",
      "Batch 62, Loss: 0.887175, Accuracy: 74.19%\n",
      "Batch 63, Loss: 1.113898, Accuracy: 73.96%\n",
      "Batch 64, Loss: 0.958788, Accuracy: 74.05%\n",
      "Batch 65, Loss: 1.074225, Accuracy: 73.92%\n",
      "Batch 66, Loss: 0.953750, Accuracy: 73.96%\n",
      "Batch 67, Loss: 1.053112, Accuracy: 73.88%\n",
      "Batch 68, Loss: 0.993461, Accuracy: 73.92%\n",
      "Batch 69, Loss: 1.161619, Accuracy: 73.66%\n",
      "Batch 70, Loss: 1.019139, Accuracy: 73.68%\n",
      "Batch 71, Loss: 1.021727, Accuracy: 73.68%\n",
      "Batch 72, Loss: 1.014805, Accuracy: 73.68%\n",
      "Batch 73, Loss: 0.995775, Accuracy: 73.69%\n",
      "Batch 74, Loss: 1.025865, Accuracy: 73.71%\n",
      "Batch 75, Loss: 1.073062, Accuracy: 73.62%\n",
      "Batch 76, Loss: 1.004270, Accuracy: 73.64%\n",
      "Batch 77, Loss: 0.944358, Accuracy: 73.72%\n",
      "Batch 78, Loss: 1.103399, Accuracy: 73.60%\n",
      "Batch 79, Loss: 0.970010, Accuracy: 73.66%\n",
      "Batch 80, Loss: 0.986934, Accuracy: 73.67%\n",
      "Batch 81, Loss: 0.993407, Accuracy: 73.69%\n",
      "Batch 82, Loss: 1.044212, Accuracy: 73.65%\n",
      "Batch 83, Loss: 1.005795, Accuracy: 73.66%\n",
      "Batch 84, Loss: 1.027459, Accuracy: 73.60%\n",
      "Batch 85, Loss: 1.039978, Accuracy: 73.60%\n",
      "Batch 86, Loss: 1.028269, Accuracy: 73.58%\n",
      "Batch 87, Loss: 1.086826, Accuracy: 73.46%\n",
      "Batch 88, Loss: 1.000517, Accuracy: 73.47%\n",
      "Batch 89, Loss: 1.060543, Accuracy: 73.42%\n",
      "Batch 90, Loss: 0.954175, Accuracy: 73.47%\n",
      "Batch 91, Loss: 1.052211, Accuracy: 73.40%\n",
      "Batch 92, Loss: 0.994121, Accuracy: 73.42%\n",
      "Batch 93, Loss: 1.059792, Accuracy: 73.34%\n",
      "Batch 94, Loss: 0.963831, Accuracy: 73.39%\n",
      "Batch 95, Loss: 1.036065, Accuracy: 73.36%\n",
      "Batch 96, Loss: 0.921290, Accuracy: 73.45%\n",
      "Batch 97, Loss: 1.024151, Accuracy: 73.45%\n",
      "Batch 98, Loss: 1.096771, Accuracy: 73.34%\n",
      "Batch 99, Loss: 0.871315, Accuracy: 73.48%\n",
      "Batch 100, Loss: 1.015754, Accuracy: 73.48%\n",
      "Batch 101, Loss: 1.086191, Accuracy: 73.38%\n",
      "Batch 102, Loss: 0.971199, Accuracy: 73.42%\n",
      "Batch 103, Loss: 0.971583, Accuracy: 73.47%\n",
      "Batch 104, Loss: 1.032943, Accuracy: 73.44%\n",
      "Batch 105, Loss: 0.963241, Accuracy: 73.48%\n",
      "Batch 106, Loss: 0.891240, Accuracy: 73.60%\n",
      "Batch 107, Loss: 0.903379, Accuracy: 73.70%\n",
      "Batch 108, Loss: 1.015699, Accuracy: 73.68%\n",
      "Batch 109, Loss: 1.033925, Accuracy: 73.65%\n",
      "Batch 110, Loss: 0.924031, Accuracy: 73.76%\n",
      "Batch 111, Loss: 1.015360, Accuracy: 73.78%\n",
      "Batch 112, Loss: 0.985782, Accuracy: 73.80%\n",
      "Batch 113, Loss: 1.139332, Accuracy: 73.69%\n",
      "Batch 114, Loss: 0.908144, Accuracy: 73.75%\n",
      "Batch 115, Loss: 1.076707, Accuracy: 73.70%\n",
      "Batch 116, Loss: 1.055017, Accuracy: 73.63%\n",
      "Batch 117, Loss: 0.966954, Accuracy: 73.66%\n",
      "Batch 118, Loss: 0.951577, Accuracy: 73.72%\n",
      "Batch 119, Loss: 1.017140, Accuracy: 73.70%\n",
      "Batch 120, Loss: 1.072271, Accuracy: 73.63%\n",
      "Batch 121, Loss: 0.928400, Accuracy: 73.68%\n",
      "Batch 122, Loss: 0.995733, Accuracy: 73.71%\n",
      "Batch 123, Loss: 1.015992, Accuracy: 73.70%\n",
      "Batch 124, Loss: 1.015541, Accuracy: 73.69%\n",
      "Batch 125, Loss: 0.939083, Accuracy: 73.75%\n",
      "Batch 126, Loss: 1.057532, Accuracy: 73.71%\n",
      "Batch 127, Loss: 1.018218, Accuracy: 73.70%\n",
      "Batch 128, Loss: 1.000954, Accuracy: 73.71%\n",
      "Batch 129, Loss: 1.035352, Accuracy: 73.68%\n",
      "Batch 130, Loss: 1.084613, Accuracy: 73.61%\n",
      "Batch 131, Loss: 0.958521, Accuracy: 73.65%\n",
      "Batch 132, Loss: 0.985037, Accuracy: 73.66%\n",
      "Batch 133, Loss: 0.983524, Accuracy: 73.67%\n",
      "Batch 134, Loss: 0.997658, Accuracy: 73.66%\n",
      "Batch 135, Loss: 1.010810, Accuracy: 73.66%\n",
      "Batch 136, Loss: 0.949438, Accuracy: 73.69%\n",
      "Batch 137, Loss: 1.019160, Accuracy: 73.67%\n",
      "Batch 138, Loss: 1.060212, Accuracy: 73.64%\n",
      "Batch 139, Loss: 0.995784, Accuracy: 73.65%\n",
      "Batch 140, Loss: 0.980460, Accuracy: 73.67%\n",
      "Batch 141, Loss: 0.932230, Accuracy: 73.71%\n",
      "Batch 142, Loss: 1.037939, Accuracy: 73.68%\n",
      "Batch 143, Loss: 0.914006, Accuracy: 73.75%\n",
      "Batch 144, Loss: 0.906304, Accuracy: 73.83%\n",
      "Batch 145, Loss: 1.034180, Accuracy: 73.80%\n",
      "Batch 146, Loss: 0.978418, Accuracy: 73.83%\n",
      "Batch 147, Loss: 1.004864, Accuracy: 73.83%\n",
      "Batch 148, Loss: 0.998971, Accuracy: 73.83%\n",
      "Batch 149, Loss: 0.954615, Accuracy: 73.86%\n",
      "Batch 150, Loss: 0.991581, Accuracy: 73.86%\n",
      "Batch 151, Loss: 0.991637, Accuracy: 73.87%\n",
      "Batch 152, Loss: 0.964026, Accuracy: 73.91%\n",
      "Batch 153, Loss: 0.954280, Accuracy: 73.94%\n",
      "Batch 154, Loss: 0.943746, Accuracy: 73.98%\n",
      "Batch 155, Loss: 0.956013, Accuracy: 73.99%\n",
      "Batch 156, Loss: 1.006834, Accuracy: 74.00%\n",
      "Batch 157, Loss: 1.034391, Accuracy: 73.99%\n",
      "Batch 158, Loss: 1.037229, Accuracy: 73.96%\n",
      "Batch 159, Loss: 0.896939, Accuracy: 74.04%\n",
      "Batch 160, Loss: 1.051370, Accuracy: 74.01%\n",
      "Batch 161, Loss: 1.116120, Accuracy: 73.93%\n",
      "Batch 162, Loss: 0.943070, Accuracy: 73.98%\n",
      "Batch 163, Loss: 0.912396, Accuracy: 74.04%\n",
      "Batch 164, Loss: 0.998229, Accuracy: 74.05%\n",
      "Batch 165, Loss: 0.918878, Accuracy: 74.11%\n",
      "Batch 166, Loss: 1.027933, Accuracy: 74.09%\n",
      "Batch 167, Loss: 0.989027, Accuracy: 74.11%\n",
      "Batch 168, Loss: 1.006598, Accuracy: 74.11%\n",
      "Batch 169, Loss: 0.949869, Accuracy: 74.14%\n",
      "Batch 170, Loss: 1.007416, Accuracy: 74.14%\n",
      "Batch 171, Loss: 0.937905, Accuracy: 74.17%\n",
      "Batch 172, Loss: 0.966911, Accuracy: 74.17%\n",
      "Batch 173, Loss: 1.035176, Accuracy: 74.16%\n",
      "Batch 174, Loss: 1.013886, Accuracy: 74.16%\n",
      "Batch 175, Loss: 0.955771, Accuracy: 74.20%\n",
      "Batch 176, Loss: 0.937070, Accuracy: 74.24%\n",
      "Batch 177, Loss: 0.967541, Accuracy: 74.26%\n",
      "Batch 178, Loss: 1.064824, Accuracy: 74.23%\n",
      "Batch 179, Loss: 0.963705, Accuracy: 74.23%\n",
      "Batch 180, Loss: 0.965145, Accuracy: 74.25%\n",
      "Batch 181, Loss: 1.025100, Accuracy: 74.22%\n",
      "Batch 182, Loss: 1.047980, Accuracy: 74.19%\n",
      "Batch 183, Loss: 1.041909, Accuracy: 74.17%\n",
      "Batch 184, Loss: 1.016001, Accuracy: 74.16%\n",
      "Batch 185, Loss: 0.931710, Accuracy: 74.20%\n",
      "Batch 186, Loss: 0.963577, Accuracy: 74.22%\n",
      "Batch 187, Loss: 1.011506, Accuracy: 74.21%\n",
      "Batch 188, Loss: 0.949970, Accuracy: 74.24%\n",
      "Batch 189, Loss: 0.967431, Accuracy: 74.26%\n",
      "Batch 190, Loss: 0.975143, Accuracy: 74.29%\n",
      "Batch 191, Loss: 0.939750, Accuracy: 74.31%\n",
      "Batch 192, Loss: 0.986554, Accuracy: 74.32%\n",
      "Batch 193, Loss: 0.956861, Accuracy: 74.34%\n",
      "Batch 194, Loss: 0.967891, Accuracy: 74.36%\n",
      "Batch 195, Loss: 0.961232, Accuracy: 74.36%\n",
      "Batch 196, Loss: 0.934559, Accuracy: 74.39%\n",
      "Batch 197, Loss: 0.975852, Accuracy: 74.40%\n",
      "Batch 198, Loss: 1.122207, Accuracy: 74.33%\n",
      "Batch 199, Loss: 0.926226, Accuracy: 74.37%\n",
      "Batch 200, Loss: 1.007463, Accuracy: 74.37%\n",
      "Batch 201, Loss: 1.009087, Accuracy: 74.36%\n",
      "Batch 202, Loss: 1.022153, Accuracy: 74.34%\n",
      "Batch 203, Loss: 0.948505, Accuracy: 74.36%\n",
      "Batch 204, Loss: 1.044258, Accuracy: 74.33%\n",
      "Batch 205, Loss: 1.048013, Accuracy: 74.31%\n",
      "Batch 206, Loss: 0.961841, Accuracy: 74.33%\n",
      "Batch 207, Loss: 0.995088, Accuracy: 74.34%\n",
      "Batch 208, Loss: 0.965827, Accuracy: 74.37%\n",
      "Batch 209, Loss: 1.043776, Accuracy: 74.35%\n",
      "Batch 210, Loss: 0.996726, Accuracy: 74.35%\n",
      "Batch 211, Loss: 0.947757, Accuracy: 74.37%\n",
      "Batch 212, Loss: 0.970508, Accuracy: 74.39%\n",
      "Batch 213, Loss: 0.983878, Accuracy: 74.39%\n",
      "Training - Epoch 59, Loss: 0.998039, Accuracy: 74.39%\n",
      "Validation Batch 1, Loss: 0.995948, Accuracy: 76.56%\n",
      "Validation Batch 2, Loss: 1.087709, Accuracy: 71.09%\n",
      "Validation Batch 3, Loss: 1.098718, Accuracy: 68.75%\n",
      "Validation Batch 4, Loss: 1.027940, Accuracy: 69.14%\n",
      "Validation Batch 5, Loss: 1.029685, Accuracy: 69.69%\n",
      "Validation Batch 6, Loss: 0.980310, Accuracy: 70.83%\n",
      "Validation Batch 7, Loss: 1.058789, Accuracy: 70.54%\n",
      "Validation Batch 8, Loss: 1.073510, Accuracy: 70.12%\n",
      "Validation Batch 9, Loss: 1.100870, Accuracy: 69.10%\n",
      "Validation Batch 10, Loss: 1.062737, Accuracy: 68.91%\n",
      "Validation Batch 11, Loss: 1.029015, Accuracy: 68.89%\n",
      "Validation Batch 12, Loss: 0.977006, Accuracy: 69.53%\n",
      "Validation Batch 13, Loss: 1.117376, Accuracy: 68.99%\n",
      "Validation Batch 14, Loss: 1.064816, Accuracy: 68.86%\n",
      "Validation Batch 15, Loss: 1.028431, Accuracy: 69.17%\n",
      "Validation Batch 16, Loss: 1.009715, Accuracy: 69.53%\n",
      "Validation Batch 17, Loss: 1.081696, Accuracy: 69.21%\n",
      "Validation Batch 18, Loss: 1.013930, Accuracy: 69.44%\n",
      "Validation Batch 19, Loss: 1.086330, Accuracy: 69.41%\n",
      "Validation Batch 20, Loss: 1.050083, Accuracy: 69.53%\n",
      "Validation Batch 21, Loss: 1.045141, Accuracy: 69.49%\n",
      "Validation Batch 22, Loss: 1.073134, Accuracy: 69.39%\n",
      "Validation Batch 23, Loss: 1.143380, Accuracy: 68.95%\n",
      "Validation Batch 24, Loss: 1.101132, Accuracy: 68.75%\n",
      "Validation Batch 25, Loss: 1.027266, Accuracy: 68.81%\n",
      "Validation Batch 26, Loss: 1.037996, Accuracy: 68.87%\n",
      "Validation Batch 27, Loss: 1.046037, Accuracy: 68.94%\n",
      "Validation - Epoch 59, Loss: 1.053656, Accuracy: 68.94%\n",
      "Patienceâ€”1\n",
      "Epoch 60\n",
      "Batch 1, Loss: 0.970671, Accuracy: 76.56%\n",
      "Batch 2, Loss: 1.037269, Accuracy: 72.66%\n",
      "Batch 3, Loss: 0.947255, Accuracy: 75.00%\n",
      "Batch 4, Loss: 0.898012, Accuracy: 77.34%\n",
      "Batch 5, Loss: 0.936235, Accuracy: 77.81%\n",
      "Batch 6, Loss: 1.071518, Accuracy: 76.04%\n",
      "Batch 7, Loss: 1.065525, Accuracy: 74.78%\n",
      "Batch 8, Loss: 0.959851, Accuracy: 75.39%\n",
      "Batch 9, Loss: 0.987399, Accuracy: 75.52%\n",
      "Batch 10, Loss: 1.031716, Accuracy: 75.31%\n",
      "Batch 11, Loss: 0.953729, Accuracy: 75.85%\n",
      "Batch 12, Loss: 1.035973, Accuracy: 75.39%\n",
      "Batch 13, Loss: 0.988705, Accuracy: 75.48%\n",
      "Batch 14, Loss: 1.039773, Accuracy: 75.11%\n",
      "Batch 15, Loss: 0.915151, Accuracy: 75.73%\n",
      "Batch 16, Loss: 1.082141, Accuracy: 75.10%\n",
      "Batch 17, Loss: 0.942171, Accuracy: 75.55%\n",
      "Batch 18, Loss: 1.046838, Accuracy: 75.17%\n",
      "Batch 19, Loss: 1.031908, Accuracy: 74.84%\n",
      "Batch 20, Loss: 0.931271, Accuracy: 75.16%\n",
      "Batch 21, Loss: 0.889736, Accuracy: 75.67%\n",
      "Batch 22, Loss: 0.970655, Accuracy: 75.85%\n",
      "Batch 23, Loss: 0.954393, Accuracy: 76.09%\n",
      "Batch 24, Loss: 0.976893, Accuracy: 76.04%\n",
      "Batch 25, Loss: 0.980786, Accuracy: 76.06%\n",
      "Batch 26, Loss: 0.989430, Accuracy: 76.08%\n",
      "Batch 27, Loss: 0.979641, Accuracy: 76.16%\n",
      "Batch 28, Loss: 1.030391, Accuracy: 76.00%\n",
      "Batch 29, Loss: 1.009014, Accuracy: 75.86%\n",
      "Batch 30, Loss: 1.120766, Accuracy: 75.36%\n",
      "Batch 31, Loss: 0.990986, Accuracy: 75.30%\n",
      "Batch 32, Loss: 0.994219, Accuracy: 75.24%\n",
      "Batch 33, Loss: 1.038578, Accuracy: 75.05%\n",
      "Batch 34, Loss: 1.020078, Accuracy: 74.95%\n",
      "Batch 35, Loss: 0.897361, Accuracy: 75.18%\n",
      "Batch 36, Loss: 0.978031, Accuracy: 75.22%\n",
      "Batch 37, Loss: 1.024805, Accuracy: 75.13%\n",
      "Batch 38, Loss: 1.062825, Accuracy: 74.88%\n",
      "Batch 39, Loss: 0.990700, Accuracy: 74.88%\n",
      "Batch 40, Loss: 1.077867, Accuracy: 74.61%\n",
      "Batch 41, Loss: 0.988639, Accuracy: 74.62%\n",
      "Batch 42, Loss: 0.979004, Accuracy: 74.63%\n",
      "Batch 43, Loss: 0.986051, Accuracy: 74.67%\n",
      "Batch 44, Loss: 0.955759, Accuracy: 74.79%\n",
      "Batch 45, Loss: 0.939797, Accuracy: 74.90%\n",
      "Batch 46, Loss: 1.015344, Accuracy: 74.83%\n",
      "Batch 47, Loss: 1.052171, Accuracy: 74.73%\n",
      "Batch 48, Loss: 1.068548, Accuracy: 74.54%\n",
      "Batch 49, Loss: 0.993757, Accuracy: 74.59%\n",
      "Batch 50, Loss: 1.025195, Accuracy: 74.50%\n",
      "Batch 51, Loss: 0.973732, Accuracy: 74.54%\n",
      "Batch 52, Loss: 1.065084, Accuracy: 74.40%\n",
      "Batch 53, Loss: 1.036431, Accuracy: 74.32%\n",
      "Batch 54, Loss: 0.971693, Accuracy: 74.33%\n",
      "Batch 55, Loss: 1.025284, Accuracy: 74.29%\n",
      "Batch 56, Loss: 1.019621, Accuracy: 74.25%\n",
      "Batch 57, Loss: 1.036452, Accuracy: 74.18%\n",
      "Batch 58, Loss: 1.020031, Accuracy: 74.14%\n",
      "Batch 59, Loss: 1.028177, Accuracy: 74.07%\n",
      "Batch 60, Loss: 0.961843, Accuracy: 74.11%\n",
      "Batch 61, Loss: 0.988698, Accuracy: 74.13%\n",
      "Batch 62, Loss: 0.999189, Accuracy: 74.09%\n",
      "Batch 63, Loss: 1.035371, Accuracy: 74.06%\n",
      "Batch 64, Loss: 0.989997, Accuracy: 74.07%\n",
      "Batch 65, Loss: 1.014963, Accuracy: 74.06%\n",
      "Batch 66, Loss: 0.898112, Accuracy: 74.29%\n",
      "Batch 67, Loss: 1.000313, Accuracy: 74.25%\n",
      "Batch 68, Loss: 1.017129, Accuracy: 74.24%\n",
      "Batch 69, Loss: 0.928804, Accuracy: 74.34%\n",
      "Batch 70, Loss: 0.954689, Accuracy: 74.40%\n",
      "Batch 71, Loss: 1.006592, Accuracy: 74.41%\n",
      "Batch 72, Loss: 0.979118, Accuracy: 74.44%\n",
      "Batch 73, Loss: 0.936998, Accuracy: 74.53%\n",
      "Batch 74, Loss: 0.955961, Accuracy: 74.58%\n",
      "Batch 75, Loss: 0.980462, Accuracy: 74.60%\n",
      "Batch 76, Loss: 1.002494, Accuracy: 74.63%\n",
      "Batch 77, Loss: 0.965303, Accuracy: 74.66%\n",
      "Batch 78, Loss: 1.013564, Accuracy: 74.64%\n",
      "Batch 79, Loss: 1.030866, Accuracy: 74.56%\n",
      "Batch 80, Loss: 1.005630, Accuracy: 74.53%\n",
      "Batch 81, Loss: 0.989304, Accuracy: 74.58%\n",
      "Batch 82, Loss: 0.978107, Accuracy: 74.60%\n",
      "Batch 83, Loss: 1.032151, Accuracy: 74.51%\n",
      "Batch 84, Loss: 1.027863, Accuracy: 74.46%\n",
      "Batch 85, Loss: 0.993324, Accuracy: 74.50%\n",
      "Batch 86, Loss: 0.931343, Accuracy: 74.60%\n",
      "Batch 87, Loss: 1.041873, Accuracy: 74.53%\n",
      "Batch 88, Loss: 1.105172, Accuracy: 74.41%\n",
      "Batch 89, Loss: 1.055310, Accuracy: 74.33%\n",
      "Batch 90, Loss: 1.065688, Accuracy: 74.25%\n",
      "Batch 91, Loss: 1.085950, Accuracy: 74.14%\n",
      "Batch 92, Loss: 1.008907, Accuracy: 74.12%\n",
      "Batch 93, Loss: 1.070168, Accuracy: 74.03%\n",
      "Batch 94, Loss: 1.002849, Accuracy: 74.04%\n",
      "Batch 95, Loss: 0.961639, Accuracy: 74.08%\n",
      "Batch 96, Loss: 0.956640, Accuracy: 74.10%\n",
      "Batch 97, Loss: 1.021619, Accuracy: 74.10%\n",
      "Batch 98, Loss: 1.025671, Accuracy: 74.08%\n",
      "Batch 99, Loss: 0.925946, Accuracy: 74.16%\n",
      "Batch 100, Loss: 0.977541, Accuracy: 74.19%\n",
      "Batch 101, Loss: 0.999739, Accuracy: 74.20%\n",
      "Batch 102, Loss: 1.092761, Accuracy: 74.08%\n",
      "Batch 103, Loss: 0.955451, Accuracy: 74.12%\n",
      "Batch 104, Loss: 1.003119, Accuracy: 74.11%\n",
      "Batch 105, Loss: 1.014779, Accuracy: 74.09%\n",
      "Batch 106, Loss: 0.985913, Accuracy: 74.12%\n",
      "Batch 107, Loss: 0.886624, Accuracy: 74.23%\n",
      "Batch 108, Loss: 1.043958, Accuracy: 74.19%\n",
      "Batch 109, Loss: 1.034060, Accuracy: 74.14%\n",
      "Batch 110, Loss: 0.987956, Accuracy: 74.15%\n",
      "Batch 111, Loss: 1.079902, Accuracy: 74.06%\n",
      "Batch 112, Loss: 1.083179, Accuracy: 73.98%\n",
      "Batch 113, Loss: 0.935625, Accuracy: 74.03%\n",
      "Batch 114, Loss: 0.957412, Accuracy: 74.07%\n",
      "Batch 115, Loss: 0.926146, Accuracy: 74.14%\n",
      "Batch 116, Loss: 0.999349, Accuracy: 74.14%\n",
      "Batch 117, Loss: 0.982174, Accuracy: 74.16%\n",
      "Batch 118, Loss: 1.038371, Accuracy: 74.14%\n",
      "Batch 119, Loss: 1.033977, Accuracy: 74.13%\n",
      "Batch 120, Loss: 0.929074, Accuracy: 74.19%\n",
      "Batch 121, Loss: 1.023449, Accuracy: 74.17%\n",
      "Batch 122, Loss: 0.982510, Accuracy: 74.19%\n",
      "Batch 123, Loss: 1.053096, Accuracy: 74.15%\n",
      "Batch 124, Loss: 1.081496, Accuracy: 74.07%\n",
      "Batch 125, Loss: 1.012704, Accuracy: 74.06%\n",
      "Batch 126, Loss: 0.918477, Accuracy: 74.14%\n",
      "Batch 127, Loss: 0.986816, Accuracy: 74.15%\n",
      "Batch 128, Loss: 1.018546, Accuracy: 74.11%\n",
      "Batch 129, Loss: 0.941606, Accuracy: 74.16%\n",
      "Batch 130, Loss: 0.981294, Accuracy: 74.17%\n",
      "Batch 131, Loss: 0.934438, Accuracy: 74.22%\n",
      "Batch 132, Loss: 1.044574, Accuracy: 74.18%\n",
      "Batch 133, Loss: 0.969832, Accuracy: 74.20%\n",
      "Batch 134, Loss: 0.931207, Accuracy: 74.25%\n",
      "Batch 135, Loss: 1.000302, Accuracy: 74.25%\n",
      "Batch 136, Loss: 0.927664, Accuracy: 74.31%\n",
      "Batch 137, Loss: 1.080722, Accuracy: 74.26%\n",
      "Batch 138, Loss: 1.016798, Accuracy: 74.24%\n",
      "Batch 139, Loss: 0.952268, Accuracy: 74.28%\n",
      "Batch 140, Loss: 0.947256, Accuracy: 74.32%\n",
      "Batch 141, Loss: 1.060174, Accuracy: 74.27%\n",
      "Batch 142, Loss: 1.069037, Accuracy: 74.23%\n",
      "Batch 143, Loss: 0.983927, Accuracy: 74.25%\n",
      "Batch 144, Loss: 1.006984, Accuracy: 74.23%\n",
      "Batch 145, Loss: 0.973063, Accuracy: 74.25%\n",
      "Batch 146, Loss: 1.026249, Accuracy: 74.23%\n",
      "Batch 147, Loss: 0.951781, Accuracy: 74.27%\n",
      "Batch 148, Loss: 0.999088, Accuracy: 74.26%\n",
      "Batch 149, Loss: 1.070077, Accuracy: 74.21%\n",
      "Batch 150, Loss: 0.871080, Accuracy: 74.31%\n",
      "Batch 151, Loss: 1.019278, Accuracy: 74.29%\n",
      "Batch 152, Loss: 0.906302, Accuracy: 74.35%\n",
      "Batch 153, Loss: 0.941705, Accuracy: 74.39%\n",
      "Batch 154, Loss: 0.981765, Accuracy: 74.40%\n",
      "Batch 155, Loss: 0.946388, Accuracy: 74.44%\n",
      "Batch 156, Loss: 0.996014, Accuracy: 74.44%\n",
      "Batch 157, Loss: 0.958310, Accuracy: 74.46%\n",
      "Batch 158, Loss: 0.894421, Accuracy: 74.54%\n",
      "Batch 159, Loss: 1.021798, Accuracy: 74.52%\n",
      "Batch 160, Loss: 1.043648, Accuracy: 74.48%\n",
      "Batch 161, Loss: 1.047940, Accuracy: 74.46%\n",
      "Batch 162, Loss: 0.961413, Accuracy: 74.48%\n",
      "Batch 163, Loss: 0.981434, Accuracy: 74.48%\n",
      "Batch 164, Loss: 0.942263, Accuracy: 74.51%\n",
      "Batch 165, Loss: 0.952134, Accuracy: 74.54%\n",
      "Batch 166, Loss: 1.094094, Accuracy: 74.46%\n",
      "Batch 167, Loss: 0.917290, Accuracy: 74.50%\n",
      "Batch 168, Loss: 1.011834, Accuracy: 74.50%\n",
      "Batch 169, Loss: 0.970215, Accuracy: 74.51%\n",
      "Batch 170, Loss: 1.029957, Accuracy: 74.49%\n",
      "Batch 171, Loss: 1.097910, Accuracy: 74.43%\n",
      "Batch 172, Loss: 1.001188, Accuracy: 74.43%\n",
      "Batch 173, Loss: 1.193553, Accuracy: 74.31%\n",
      "Batch 174, Loss: 0.960699, Accuracy: 74.34%\n",
      "Batch 175, Loss: 1.008259, Accuracy: 74.32%\n",
      "Batch 176, Loss: 0.990313, Accuracy: 74.33%\n",
      "Batch 177, Loss: 1.017518, Accuracy: 74.32%\n",
      "Batch 178, Loss: 1.000657, Accuracy: 74.32%\n",
      "Batch 179, Loss: 1.046965, Accuracy: 74.28%\n",
      "Batch 180, Loss: 1.003149, Accuracy: 74.28%\n",
      "Batch 181, Loss: 0.943123, Accuracy: 74.30%\n",
      "Batch 182, Loss: 0.982577, Accuracy: 74.30%\n",
      "Batch 183, Loss: 1.036018, Accuracy: 74.29%\n",
      "Batch 184, Loss: 1.015478, Accuracy: 74.28%\n",
      "Batch 185, Loss: 1.038412, Accuracy: 74.25%\n",
      "Batch 186, Loss: 0.952885, Accuracy: 74.27%\n",
      "Batch 187, Loss: 0.992213, Accuracy: 74.27%\n",
      "Batch 188, Loss: 1.007686, Accuracy: 74.27%\n",
      "Batch 189, Loss: 0.983884, Accuracy: 74.27%\n",
      "Batch 190, Loss: 0.983948, Accuracy: 74.28%\n",
      "Batch 191, Loss: 1.100708, Accuracy: 74.22%\n",
      "Batch 192, Loss: 0.983737, Accuracy: 74.24%\n",
      "Batch 193, Loss: 0.969900, Accuracy: 74.26%\n",
      "Batch 194, Loss: 1.017716, Accuracy: 74.24%\n",
      "Batch 195, Loss: 1.031603, Accuracy: 74.22%\n",
      "Batch 196, Loss: 1.007951, Accuracy: 74.22%\n",
      "Batch 197, Loss: 1.057287, Accuracy: 74.19%\n",
      "Batch 198, Loss: 0.964807, Accuracy: 74.20%\n",
      "Batch 199, Loss: 0.995434, Accuracy: 74.21%\n",
      "Batch 200, Loss: 1.050472, Accuracy: 74.19%\n",
      "Batch 201, Loss: 0.991706, Accuracy: 74.19%\n",
      "Batch 202, Loss: 1.028272, Accuracy: 74.17%\n",
      "Batch 203, Loss: 1.003760, Accuracy: 74.17%\n",
      "Batch 204, Loss: 0.966831, Accuracy: 74.18%\n",
      "Batch 205, Loss: 0.989399, Accuracy: 74.18%\n",
      "Batch 206, Loss: 0.996183, Accuracy: 74.19%\n",
      "Batch 207, Loss: 0.932409, Accuracy: 74.21%\n",
      "Batch 208, Loss: 0.931648, Accuracy: 74.24%\n",
      "Batch 209, Loss: 1.016036, Accuracy: 74.23%\n",
      "Batch 210, Loss: 1.014438, Accuracy: 74.23%\n",
      "Batch 211, Loss: 0.972111, Accuracy: 74.24%\n",
      "Batch 212, Loss: 1.030879, Accuracy: 74.23%\n",
      "Batch 213, Loss: 0.967386, Accuracy: 74.25%\n",
      "Training - Epoch 60, Loss: 0.998289, Accuracy: 74.25%\n",
      "Validation Batch 1, Loss: 1.013966, Accuracy: 73.44%\n",
      "Validation Batch 2, Loss: 1.119494, Accuracy: 67.97%\n",
      "Validation Batch 3, Loss: 1.123382, Accuracy: 66.15%\n",
      "Validation Batch 4, Loss: 1.047870, Accuracy: 66.41%\n",
      "Validation Batch 5, Loss: 1.061209, Accuracy: 66.88%\n",
      "Validation Batch 6, Loss: 1.006398, Accuracy: 68.23%\n",
      "Validation Batch 7, Loss: 1.071748, Accuracy: 68.08%\n",
      "Validation Batch 8, Loss: 1.091358, Accuracy: 67.77%\n",
      "Validation Batch 9, Loss: 1.115719, Accuracy: 67.01%\n",
      "Validation Batch 10, Loss: 1.081978, Accuracy: 67.03%\n",
      "Validation Batch 11, Loss: 1.058476, Accuracy: 66.76%\n",
      "Validation Batch 12, Loss: 1.001979, Accuracy: 67.45%\n",
      "Validation Batch 13, Loss: 1.134341, Accuracy: 66.95%\n",
      "Validation Batch 14, Loss: 1.086888, Accuracy: 66.85%\n",
      "Validation Batch 15, Loss: 1.044638, Accuracy: 67.29%\n",
      "Validation Batch 16, Loss: 1.043364, Accuracy: 67.38%\n",
      "Validation Batch 17, Loss: 1.127560, Accuracy: 66.73%\n",
      "Validation Batch 18, Loss: 1.025233, Accuracy: 67.10%\n",
      "Validation Batch 19, Loss: 1.124545, Accuracy: 66.69%\n",
      "Validation Batch 20, Loss: 1.072627, Accuracy: 66.64%\n",
      "Validation Batch 21, Loss: 1.070714, Accuracy: 66.59%\n",
      "Validation Batch 22, Loss: 1.106367, Accuracy: 66.48%\n",
      "Validation Batch 23, Loss: 1.163567, Accuracy: 66.10%\n",
      "Validation Batch 24, Loss: 1.109129, Accuracy: 66.02%\n",
      "Validation Batch 25, Loss: 1.061637, Accuracy: 65.94%\n",
      "Validation Batch 26, Loss: 1.045027, Accuracy: 66.17%\n",
      "Validation Batch 27, Loss: 1.069314, Accuracy: 66.24%\n",
      "Validation - Epoch 60, Loss: 1.076982, Accuracy: 66.24%\n",
      "Patienceâ€”2\n",
      "Epoch 61\n",
      "Batch 1, Loss: 0.974896, Accuracy: 75.00%\n",
      "Batch 2, Loss: 1.077284, Accuracy: 71.09%\n",
      "Batch 3, Loss: 0.933884, Accuracy: 74.48%\n",
      "Batch 4, Loss: 1.012668, Accuracy: 73.83%\n",
      "Batch 5, Loss: 1.110000, Accuracy: 72.19%\n",
      "Batch 6, Loss: 0.922642, Accuracy: 73.44%\n",
      "Batch 7, Loss: 1.068369, Accuracy: 72.77%\n",
      "Batch 8, Loss: 1.013362, Accuracy: 72.66%\n",
      "Batch 9, Loss: 1.005532, Accuracy: 72.74%\n",
      "Batch 10, Loss: 1.024320, Accuracy: 72.81%\n",
      "Batch 11, Loss: 0.909206, Accuracy: 73.86%\n",
      "Batch 12, Loss: 0.972891, Accuracy: 74.22%\n",
      "Batch 13, Loss: 0.969727, Accuracy: 74.52%\n",
      "Batch 14, Loss: 0.958295, Accuracy: 74.89%\n",
      "Batch 15, Loss: 1.089864, Accuracy: 74.27%\n",
      "Batch 16, Loss: 0.951167, Accuracy: 74.61%\n",
      "Batch 17, Loss: 0.939381, Accuracy: 75.00%\n",
      "Batch 18, Loss: 0.956059, Accuracy: 75.26%\n",
      "Batch 19, Loss: 1.046664, Accuracy: 74.84%\n",
      "Batch 20, Loss: 0.991342, Accuracy: 74.92%\n",
      "Batch 21, Loss: 1.027493, Accuracy: 74.70%\n",
      "Batch 22, Loss: 1.037075, Accuracy: 74.57%\n",
      "Batch 23, Loss: 0.985029, Accuracy: 74.66%\n",
      "Batch 24, Loss: 0.993055, Accuracy: 74.74%\n",
      "Batch 25, Loss: 1.006500, Accuracy: 74.56%\n",
      "Batch 26, Loss: 0.985963, Accuracy: 74.64%\n",
      "Batch 27, Loss: 0.971736, Accuracy: 74.77%\n",
      "Batch 28, Loss: 0.927771, Accuracy: 74.94%\n",
      "Batch 29, Loss: 0.974392, Accuracy: 74.95%\n",
      "Batch 30, Loss: 1.036345, Accuracy: 74.74%\n",
      "Batch 31, Loss: 0.992649, Accuracy: 74.70%\n",
      "Batch 32, Loss: 0.999604, Accuracy: 74.66%\n",
      "Batch 33, Loss: 0.950431, Accuracy: 74.76%\n",
      "Batch 34, Loss: 0.951314, Accuracy: 74.91%\n",
      "Batch 35, Loss: 1.083286, Accuracy: 74.64%\n",
      "Batch 36, Loss: 0.947404, Accuracy: 74.70%\n",
      "Batch 37, Loss: 1.007120, Accuracy: 74.70%\n",
      "Batch 38, Loss: 0.986717, Accuracy: 74.75%\n",
      "Batch 39, Loss: 1.054547, Accuracy: 74.56%\n",
      "Batch 40, Loss: 1.034681, Accuracy: 74.45%\n",
      "Batch 41, Loss: 0.911451, Accuracy: 74.62%\n",
      "Batch 42, Loss: 0.959541, Accuracy: 74.67%\n",
      "Batch 43, Loss: 1.017888, Accuracy: 74.60%\n",
      "Batch 44, Loss: 0.971295, Accuracy: 74.68%\n",
      "Batch 45, Loss: 1.010225, Accuracy: 74.69%\n",
      "Batch 46, Loss: 1.004631, Accuracy: 74.63%\n",
      "Batch 47, Loss: 1.034586, Accuracy: 74.53%\n",
      "Batch 48, Loss: 1.014686, Accuracy: 74.51%\n",
      "Batch 49, Loss: 0.945861, Accuracy: 74.65%\n",
      "Batch 50, Loss: 1.007945, Accuracy: 74.59%\n",
      "Batch 51, Loss: 0.990223, Accuracy: 74.60%\n",
      "Batch 52, Loss: 1.034615, Accuracy: 74.55%\n",
      "Batch 53, Loss: 0.962088, Accuracy: 74.65%\n",
      "Batch 54, Loss: 0.933810, Accuracy: 74.80%\n",
      "Batch 55, Loss: 1.034826, Accuracy: 74.69%\n",
      "Batch 56, Loss: 1.002255, Accuracy: 74.61%\n",
      "Batch 57, Loss: 0.916410, Accuracy: 74.75%\n",
      "Batch 58, Loss: 1.005468, Accuracy: 74.73%\n",
      "Batch 59, Loss: 1.053184, Accuracy: 74.66%\n",
      "Batch 60, Loss: 0.953236, Accuracy: 74.71%\n",
      "Batch 61, Loss: 1.035180, Accuracy: 74.64%\n",
      "Batch 62, Loss: 0.959512, Accuracy: 74.72%\n",
      "Batch 63, Loss: 1.004229, Accuracy: 74.70%\n",
      "Batch 64, Loss: 0.997335, Accuracy: 74.68%\n",
      "Batch 65, Loss: 0.918594, Accuracy: 74.81%\n",
      "Batch 66, Loss: 1.002295, Accuracy: 74.81%\n",
      "Batch 67, Loss: 0.992526, Accuracy: 74.81%\n",
      "Batch 68, Loss: 0.938498, Accuracy: 74.86%\n",
      "Batch 69, Loss: 0.995386, Accuracy: 74.86%\n",
      "Batch 70, Loss: 1.042625, Accuracy: 74.75%\n",
      "Batch 71, Loss: 1.098336, Accuracy: 74.60%\n",
      "Batch 72, Loss: 0.973837, Accuracy: 74.67%\n",
      "Batch 73, Loss: 0.944261, Accuracy: 74.76%\n",
      "Batch 74, Loss: 1.040112, Accuracy: 74.70%\n",
      "Batch 75, Loss: 0.903160, Accuracy: 74.85%\n",
      "Batch 76, Loss: 0.981990, Accuracy: 74.86%\n",
      "Batch 77, Loss: 1.034549, Accuracy: 74.80%\n",
      "Batch 78, Loss: 1.028054, Accuracy: 74.78%\n",
      "Batch 79, Loss: 0.911795, Accuracy: 74.88%\n",
      "Batch 80, Loss: 1.061074, Accuracy: 74.79%\n",
      "Batch 81, Loss: 1.009957, Accuracy: 74.77%\n",
      "Batch 82, Loss: 1.009003, Accuracy: 74.71%\n",
      "Batch 83, Loss: 1.036618, Accuracy: 74.64%\n",
      "Batch 84, Loss: 0.924850, Accuracy: 74.70%\n",
      "Batch 85, Loss: 1.011084, Accuracy: 74.67%\n",
      "Batch 86, Loss: 0.952170, Accuracy: 74.71%\n",
      "Batch 87, Loss: 0.900837, Accuracy: 74.82%\n",
      "Batch 88, Loss: 1.065690, Accuracy: 74.73%\n",
      "Batch 89, Loss: 1.014956, Accuracy: 74.68%\n",
      "Batch 90, Loss: 0.978843, Accuracy: 74.70%\n",
      "Batch 91, Loss: 1.006099, Accuracy: 74.69%\n",
      "Batch 92, Loss: 0.955569, Accuracy: 74.75%\n",
      "Batch 93, Loss: 0.917620, Accuracy: 74.85%\n",
      "Batch 94, Loss: 0.956594, Accuracy: 74.90%\n",
      "Batch 95, Loss: 1.048252, Accuracy: 74.85%\n",
      "Batch 96, Loss: 1.060144, Accuracy: 74.79%\n",
      "Batch 97, Loss: 0.920039, Accuracy: 74.87%\n",
      "Batch 98, Loss: 1.007138, Accuracy: 74.86%\n",
      "Batch 99, Loss: 1.109371, Accuracy: 74.75%\n",
      "Batch 100, Loss: 0.987275, Accuracy: 74.73%\n",
      "Batch 101, Loss: 0.953510, Accuracy: 74.78%\n",
      "Batch 102, Loss: 1.035759, Accuracy: 74.75%\n",
      "Batch 103, Loss: 0.980964, Accuracy: 74.76%\n",
      "Batch 104, Loss: 0.976553, Accuracy: 74.77%\n",
      "Batch 105, Loss: 1.023203, Accuracy: 74.73%\n",
      "Batch 106, Loss: 0.974382, Accuracy: 74.73%\n",
      "Batch 107, Loss: 0.955792, Accuracy: 74.78%\n",
      "Batch 108, Loss: 0.975168, Accuracy: 74.78%\n",
      "Batch 109, Loss: 0.994537, Accuracy: 74.77%\n",
      "Batch 110, Loss: 0.932915, Accuracy: 74.83%\n",
      "Batch 111, Loss: 1.014617, Accuracy: 74.82%\n",
      "Batch 112, Loss: 1.065362, Accuracy: 74.75%\n",
      "Batch 113, Loss: 0.965281, Accuracy: 74.79%\n",
      "Batch 114, Loss: 0.998631, Accuracy: 74.79%\n",
      "Batch 115, Loss: 0.979339, Accuracy: 74.81%\n",
      "Batch 116, Loss: 0.971821, Accuracy: 74.84%\n",
      "Batch 117, Loss: 0.975826, Accuracy: 74.83%\n",
      "Batch 118, Loss: 1.061023, Accuracy: 74.77%\n",
      "Batch 119, Loss: 0.988775, Accuracy: 74.76%\n",
      "Batch 120, Loss: 1.051634, Accuracy: 74.71%\n",
      "Batch 121, Loss: 0.982032, Accuracy: 74.74%\n",
      "Batch 122, Loss: 1.012849, Accuracy: 74.73%\n",
      "Batch 123, Loss: 0.992532, Accuracy: 74.75%\n",
      "Batch 124, Loss: 0.978393, Accuracy: 74.76%\n",
      "Batch 125, Loss: 0.989882, Accuracy: 74.79%\n",
      "Batch 126, Loss: 0.963400, Accuracy: 74.83%\n",
      "Batch 127, Loss: 1.070743, Accuracy: 74.74%\n",
      "Batch 128, Loss: 0.981807, Accuracy: 74.74%\n",
      "Batch 129, Loss: 1.102620, Accuracy: 74.66%\n",
      "Batch 130, Loss: 0.973895, Accuracy: 74.68%\n",
      "Batch 131, Loss: 1.084098, Accuracy: 74.58%\n",
      "Batch 132, Loss: 1.027510, Accuracy: 74.53%\n",
      "Batch 133, Loss: 0.983139, Accuracy: 74.54%\n",
      "Batch 134, Loss: 0.969538, Accuracy: 74.56%\n",
      "Batch 135, Loss: 1.046116, Accuracy: 74.51%\n",
      "Batch 136, Loss: 0.903109, Accuracy: 74.59%\n",
      "Batch 137, Loss: 0.987742, Accuracy: 74.59%\n",
      "Batch 138, Loss: 1.032278, Accuracy: 74.55%\n",
      "Batch 139, Loss: 0.930800, Accuracy: 74.61%\n",
      "Batch 140, Loss: 1.014026, Accuracy: 74.59%\n",
      "Batch 141, Loss: 1.000712, Accuracy: 74.60%\n",
      "Batch 142, Loss: 1.024360, Accuracy: 74.56%\n",
      "Batch 143, Loss: 0.957077, Accuracy: 74.58%\n",
      "Batch 144, Loss: 0.999373, Accuracy: 74.59%\n",
      "Batch 145, Loss: 0.914431, Accuracy: 74.63%\n",
      "Batch 146, Loss: 0.946374, Accuracy: 74.67%\n",
      "Batch 147, Loss: 0.956268, Accuracy: 74.69%\n",
      "Batch 148, Loss: 0.996821, Accuracy: 74.68%\n",
      "Batch 149, Loss: 0.944378, Accuracy: 74.74%\n",
      "Batch 150, Loss: 0.933331, Accuracy: 74.77%\n",
      "Batch 151, Loss: 0.947406, Accuracy: 74.79%\n",
      "Batch 152, Loss: 0.928201, Accuracy: 74.85%\n",
      "Batch 153, Loss: 1.058092, Accuracy: 74.82%\n",
      "Batch 154, Loss: 1.008445, Accuracy: 74.81%\n",
      "Batch 155, Loss: 0.949193, Accuracy: 74.85%\n",
      "Batch 156, Loss: 0.949920, Accuracy: 74.89%\n",
      "Batch 157, Loss: 0.886527, Accuracy: 74.97%\n",
      "Batch 158, Loss: 1.024625, Accuracy: 74.95%\n",
      "Batch 159, Loss: 1.027291, Accuracy: 74.93%\n",
      "Batch 160, Loss: 1.006631, Accuracy: 74.91%\n",
      "Batch 161, Loss: 1.022683, Accuracy: 74.89%\n",
      "Batch 162, Loss: 0.956443, Accuracy: 74.92%\n",
      "Batch 163, Loss: 0.966725, Accuracy: 74.93%\n",
      "Batch 164, Loss: 0.991649, Accuracy: 74.93%\n",
      "Batch 165, Loss: 0.989780, Accuracy: 74.93%\n",
      "Batch 166, Loss: 0.999824, Accuracy: 74.93%\n",
      "Batch 167, Loss: 1.022956, Accuracy: 74.92%\n",
      "Batch 168, Loss: 0.994802, Accuracy: 74.91%\n",
      "Batch 169, Loss: 0.909287, Accuracy: 74.96%\n",
      "Batch 170, Loss: 1.021499, Accuracy: 74.94%\n",
      "Batch 171, Loss: 1.025747, Accuracy: 74.92%\n",
      "Batch 172, Loss: 1.012235, Accuracy: 74.91%\n",
      "Batch 173, Loss: 1.107260, Accuracy: 74.84%\n",
      "Batch 174, Loss: 1.016383, Accuracy: 74.83%\n",
      "Batch 175, Loss: 0.979726, Accuracy: 74.84%\n",
      "Batch 176, Loss: 0.924785, Accuracy: 74.88%\n",
      "Batch 177, Loss: 1.087884, Accuracy: 74.82%\n",
      "Batch 178, Loss: 1.053163, Accuracy: 74.80%\n",
      "Batch 179, Loss: 1.000199, Accuracy: 74.79%\n",
      "Batch 180, Loss: 0.993448, Accuracy: 74.79%\n",
      "Batch 181, Loss: 0.941398, Accuracy: 74.83%\n",
      "Batch 182, Loss: 1.114788, Accuracy: 74.77%\n",
      "Batch 183, Loss: 0.965412, Accuracy: 74.79%\n",
      "Batch 184, Loss: 1.051586, Accuracy: 74.77%\n",
      "Batch 185, Loss: 0.973088, Accuracy: 74.78%\n",
      "Batch 186, Loss: 0.957734, Accuracy: 74.81%\n",
      "Batch 187, Loss: 1.013853, Accuracy: 74.78%\n",
      "Batch 188, Loss: 0.971943, Accuracy: 74.80%\n",
      "Batch 189, Loss: 0.945594, Accuracy: 74.82%\n",
      "Batch 190, Loss: 0.923401, Accuracy: 74.85%\n",
      "Batch 191, Loss: 0.996348, Accuracy: 74.84%\n",
      "Batch 192, Loss: 1.049090, Accuracy: 74.82%\n",
      "Batch 193, Loss: 0.937023, Accuracy: 74.86%\n",
      "Batch 194, Loss: 1.019475, Accuracy: 74.85%\n",
      "Batch 195, Loss: 0.952555, Accuracy: 74.88%\n",
      "Batch 196, Loss: 1.042045, Accuracy: 74.86%\n",
      "Batch 197, Loss: 0.941909, Accuracy: 74.87%\n",
      "Batch 198, Loss: 1.068553, Accuracy: 74.83%\n",
      "Batch 199, Loss: 0.896772, Accuracy: 74.87%\n",
      "Batch 200, Loss: 1.010805, Accuracy: 74.87%\n",
      "Batch 201, Loss: 0.926205, Accuracy: 74.90%\n",
      "Batch 202, Loss: 1.061138, Accuracy: 74.86%\n",
      "Batch 203, Loss: 0.934744, Accuracy: 74.89%\n",
      "Batch 204, Loss: 0.966410, Accuracy: 74.91%\n",
      "Batch 205, Loss: 1.014600, Accuracy: 74.91%\n",
      "Batch 206, Loss: 0.992881, Accuracy: 74.91%\n",
      "Batch 207, Loss: 1.022722, Accuracy: 74.89%\n",
      "Batch 208, Loss: 1.044511, Accuracy: 74.86%\n",
      "Batch 209, Loss: 1.065060, Accuracy: 74.83%\n",
      "Batch 210, Loss: 1.021093, Accuracy: 74.81%\n",
      "Batch 211, Loss: 1.071553, Accuracy: 74.77%\n",
      "Batch 212, Loss: 0.987768, Accuracy: 74.76%\n",
      "Batch 213, Loss: 0.930561, Accuracy: 74.79%\n",
      "Training - Epoch 61, Loss: 0.993570, Accuracy: 74.79%\n",
      "Validation Batch 1, Loss: 1.007255, Accuracy: 75.00%\n",
      "Validation Batch 2, Loss: 1.109483, Accuracy: 68.75%\n",
      "Validation Batch 3, Loss: 1.113180, Accuracy: 67.19%\n",
      "Validation Batch 4, Loss: 1.043246, Accuracy: 67.97%\n",
      "Validation Batch 5, Loss: 1.049239, Accuracy: 68.44%\n",
      "Validation Batch 6, Loss: 0.994254, Accuracy: 69.79%\n",
      "Validation Batch 7, Loss: 1.063954, Accuracy: 69.64%\n",
      "Validation Batch 8, Loss: 1.084281, Accuracy: 68.95%\n",
      "Validation Batch 9, Loss: 1.109079, Accuracy: 68.23%\n",
      "Validation Batch 10, Loss: 1.075634, Accuracy: 68.12%\n",
      "Validation Batch 11, Loss: 1.056462, Accuracy: 67.90%\n",
      "Validation Batch 12, Loss: 1.003043, Accuracy: 68.49%\n",
      "Validation Batch 13, Loss: 1.132726, Accuracy: 68.03%\n",
      "Validation Batch 14, Loss: 1.084017, Accuracy: 67.86%\n",
      "Validation Batch 15, Loss: 1.040278, Accuracy: 68.12%\n",
      "Validation Batch 16, Loss: 1.040274, Accuracy: 68.26%\n",
      "Validation Batch 17, Loss: 1.116844, Accuracy: 67.74%\n",
      "Validation Batch 18, Loss: 1.030889, Accuracy: 67.88%\n",
      "Validation Batch 19, Loss: 1.125157, Accuracy: 67.35%\n",
      "Validation Batch 20, Loss: 1.067908, Accuracy: 67.42%\n",
      "Validation Batch 21, Loss: 1.062245, Accuracy: 67.49%\n",
      "Validation Batch 22, Loss: 1.105881, Accuracy: 67.47%\n",
      "Validation Batch 23, Loss: 1.157418, Accuracy: 67.05%\n",
      "Validation Batch 24, Loss: 1.105488, Accuracy: 66.93%\n",
      "Validation Batch 25, Loss: 1.052997, Accuracy: 66.94%\n",
      "Validation Batch 26, Loss: 1.039553, Accuracy: 67.13%\n",
      "Validation Batch 27, Loss: 1.058699, Accuracy: 67.29%\n",
      "Validation - Epoch 61, Loss: 1.071462, Accuracy: 67.29%\n",
      "Patienceâ€”3\n",
      "Epoch 62\n",
      "Batch 1, Loss: 1.012289, Accuracy: 73.44%\n",
      "Batch 2, Loss: 1.079001, Accuracy: 68.75%\n",
      "Batch 3, Loss: 1.012212, Accuracy: 70.83%\n",
      "Batch 4, Loss: 0.980717, Accuracy: 72.27%\n",
      "Batch 5, Loss: 1.046697, Accuracy: 71.88%\n",
      "Batch 6, Loss: 1.066315, Accuracy: 70.57%\n",
      "Batch 7, Loss: 1.063411, Accuracy: 70.09%\n",
      "Batch 8, Loss: 1.050446, Accuracy: 69.92%\n",
      "Batch 9, Loss: 0.993469, Accuracy: 70.49%\n",
      "Batch 10, Loss: 1.003372, Accuracy: 70.62%\n",
      "Batch 11, Loss: 0.986592, Accuracy: 71.16%\n",
      "Batch 12, Loss: 0.925715, Accuracy: 72.01%\n",
      "Batch 13, Loss: 1.011542, Accuracy: 72.12%\n",
      "Batch 14, Loss: 0.888682, Accuracy: 73.21%\n",
      "Batch 15, Loss: 0.943070, Accuracy: 73.75%\n",
      "Batch 16, Loss: 0.973980, Accuracy: 73.93%\n",
      "Batch 17, Loss: 1.017047, Accuracy: 73.90%\n",
      "Batch 18, Loss: 1.063061, Accuracy: 73.44%\n",
      "Batch 19, Loss: 0.973578, Accuracy: 73.68%\n",
      "Batch 20, Loss: 1.026640, Accuracy: 73.52%\n",
      "Batch 21, Loss: 1.078891, Accuracy: 73.21%\n",
      "Batch 22, Loss: 0.984596, Accuracy: 73.37%\n",
      "Batch 23, Loss: 1.031605, Accuracy: 73.30%\n",
      "Batch 24, Loss: 0.918862, Accuracy: 73.70%\n",
      "Batch 25, Loss: 0.980066, Accuracy: 73.81%\n",
      "Batch 26, Loss: 1.073689, Accuracy: 73.50%\n",
      "Batch 27, Loss: 1.060479, Accuracy: 73.26%\n",
      "Batch 28, Loss: 0.913153, Accuracy: 73.55%\n",
      "Batch 29, Loss: 1.046020, Accuracy: 73.44%\n",
      "Batch 30, Loss: 1.038568, Accuracy: 73.28%\n",
      "Batch 31, Loss: 1.043073, Accuracy: 73.19%\n",
      "Batch 32, Loss: 1.056285, Accuracy: 73.00%\n",
      "Batch 33, Loss: 1.054432, Accuracy: 72.92%\n",
      "Batch 34, Loss: 0.957819, Accuracy: 73.07%\n",
      "Batch 35, Loss: 1.047013, Accuracy: 72.95%\n",
      "Batch 36, Loss: 0.934231, Accuracy: 73.22%\n",
      "Batch 37, Loss: 0.959910, Accuracy: 73.27%\n",
      "Batch 38, Loss: 1.015041, Accuracy: 73.27%\n",
      "Batch 39, Loss: 1.038833, Accuracy: 73.16%\n",
      "Batch 40, Loss: 1.046719, Accuracy: 73.09%\n",
      "Batch 41, Loss: 0.870533, Accuracy: 73.44%\n",
      "Batch 42, Loss: 0.970369, Accuracy: 73.51%\n",
      "Batch 43, Loss: 1.014558, Accuracy: 73.51%\n",
      "Batch 44, Loss: 0.894952, Accuracy: 73.79%\n",
      "Batch 45, Loss: 0.885263, Accuracy: 74.10%\n",
      "Batch 46, Loss: 1.041531, Accuracy: 73.95%\n",
      "Batch 47, Loss: 0.926004, Accuracy: 74.14%\n",
      "Batch 48, Loss: 1.024514, Accuracy: 74.12%\n",
      "Batch 49, Loss: 1.075692, Accuracy: 73.95%\n",
      "Batch 50, Loss: 0.995352, Accuracy: 73.91%\n",
      "Batch 51, Loss: 0.885097, Accuracy: 74.17%\n",
      "Batch 52, Loss: 0.954074, Accuracy: 74.28%\n",
      "Batch 53, Loss: 0.973573, Accuracy: 74.32%\n",
      "Batch 54, Loss: 1.067665, Accuracy: 74.19%\n",
      "Batch 55, Loss: 1.019895, Accuracy: 74.15%\n",
      "Batch 56, Loss: 0.971172, Accuracy: 74.19%\n",
      "Batch 57, Loss: 1.020394, Accuracy: 74.18%\n",
      "Batch 58, Loss: 0.988159, Accuracy: 74.19%\n",
      "Batch 59, Loss: 0.999684, Accuracy: 74.18%\n",
      "Batch 60, Loss: 0.980953, Accuracy: 74.22%\n",
      "Batch 61, Loss: 0.996918, Accuracy: 74.21%\n",
      "Batch 62, Loss: 1.027717, Accuracy: 74.17%\n",
      "Batch 63, Loss: 1.043047, Accuracy: 74.13%\n",
      "Batch 64, Loss: 0.948914, Accuracy: 74.22%\n",
      "Batch 65, Loss: 0.947848, Accuracy: 74.30%\n",
      "Batch 66, Loss: 0.942920, Accuracy: 74.41%\n",
      "Batch 67, Loss: 1.023557, Accuracy: 74.37%\n",
      "Batch 68, Loss: 0.945404, Accuracy: 74.47%\n",
      "Batch 69, Loss: 1.010293, Accuracy: 74.46%\n",
      "Batch 70, Loss: 0.943320, Accuracy: 74.51%\n",
      "Batch 71, Loss: 1.005484, Accuracy: 74.52%\n",
      "Batch 72, Loss: 1.056332, Accuracy: 74.39%\n",
      "Batch 73, Loss: 1.065793, Accuracy: 74.27%\n",
      "Batch 74, Loss: 0.981643, Accuracy: 74.32%\n",
      "Batch 75, Loss: 1.065885, Accuracy: 74.19%\n",
      "Batch 76, Loss: 1.032657, Accuracy: 74.12%\n",
      "Batch 77, Loss: 1.013255, Accuracy: 74.09%\n",
      "Batch 78, Loss: 1.022563, Accuracy: 74.06%\n",
      "Batch 79, Loss: 1.016311, Accuracy: 74.07%\n",
      "Batch 80, Loss: 0.994604, Accuracy: 74.06%\n",
      "Batch 81, Loss: 0.997937, Accuracy: 74.04%\n",
      "Batch 82, Loss: 1.028855, Accuracy: 74.01%\n",
      "Batch 83, Loss: 1.008166, Accuracy: 74.00%\n",
      "Batch 84, Loss: 1.010017, Accuracy: 74.00%\n",
      "Batch 85, Loss: 1.018869, Accuracy: 73.97%\n",
      "Batch 86, Loss: 0.964221, Accuracy: 74.00%\n",
      "Batch 87, Loss: 0.887262, Accuracy: 74.14%\n",
      "Batch 88, Loss: 1.016356, Accuracy: 74.11%\n",
      "Batch 89, Loss: 0.934872, Accuracy: 74.19%\n",
      "Batch 90, Loss: 1.052082, Accuracy: 74.10%\n",
      "Batch 91, Loss: 1.092389, Accuracy: 73.99%\n",
      "Batch 92, Loss: 1.063344, Accuracy: 73.91%\n",
      "Batch 93, Loss: 0.953986, Accuracy: 73.98%\n",
      "Batch 94, Loss: 0.931282, Accuracy: 74.05%\n",
      "Batch 95, Loss: 1.078820, Accuracy: 74.01%\n",
      "Batch 96, Loss: 1.019301, Accuracy: 74.01%\n",
      "Batch 97, Loss: 0.965831, Accuracy: 74.03%\n",
      "Batch 98, Loss: 0.977806, Accuracy: 74.08%\n",
      "Batch 99, Loss: 0.957812, Accuracy: 74.12%\n",
      "Batch 100, Loss: 1.061959, Accuracy: 74.06%\n",
      "Batch 101, Loss: 0.995185, Accuracy: 74.06%\n",
      "Batch 102, Loss: 0.981353, Accuracy: 74.08%\n",
      "Batch 103, Loss: 1.006627, Accuracy: 74.07%\n",
      "Batch 104, Loss: 0.955432, Accuracy: 74.11%\n",
      "Batch 105, Loss: 0.990934, Accuracy: 74.12%\n",
      "Batch 106, Loss: 1.030245, Accuracy: 74.09%\n",
      "Batch 107, Loss: 1.046664, Accuracy: 74.02%\n",
      "Batch 108, Loss: 1.021294, Accuracy: 73.99%\n",
      "Batch 109, Loss: 0.983155, Accuracy: 74.01%\n",
      "Batch 110, Loss: 0.967378, Accuracy: 74.05%\n",
      "Batch 111, Loss: 0.984468, Accuracy: 74.07%\n",
      "Batch 112, Loss: 0.987916, Accuracy: 74.08%\n",
      "Batch 113, Loss: 1.015241, Accuracy: 74.06%\n",
      "Batch 114, Loss: 1.058597, Accuracy: 74.00%\n",
      "Batch 115, Loss: 0.871351, Accuracy: 74.12%\n",
      "Batch 116, Loss: 1.012680, Accuracy: 74.08%\n",
      "Batch 117, Loss: 1.017292, Accuracy: 74.05%\n",
      "Batch 118, Loss: 1.031840, Accuracy: 74.03%\n",
      "Batch 119, Loss: 0.945227, Accuracy: 74.07%\n",
      "Batch 120, Loss: 0.941417, Accuracy: 74.13%\n",
      "Batch 121, Loss: 1.012278, Accuracy: 74.11%\n",
      "Batch 122, Loss: 0.969970, Accuracy: 74.12%\n",
      "Batch 123, Loss: 0.994373, Accuracy: 74.11%\n",
      "Batch 124, Loss: 1.034602, Accuracy: 74.09%\n",
      "Batch 125, Loss: 1.045309, Accuracy: 74.05%\n",
      "Batch 126, Loss: 0.929816, Accuracy: 74.12%\n",
      "Batch 127, Loss: 1.062977, Accuracy: 74.05%\n",
      "Batch 128, Loss: 1.063867, Accuracy: 74.00%\n",
      "Batch 129, Loss: 1.009305, Accuracy: 73.99%\n",
      "Batch 130, Loss: 0.954169, Accuracy: 74.03%\n",
      "Batch 131, Loss: 0.984679, Accuracy: 74.03%\n",
      "Batch 132, Loss: 0.986045, Accuracy: 74.06%\n",
      "Batch 133, Loss: 0.959309, Accuracy: 74.10%\n",
      "Batch 134, Loss: 1.049725, Accuracy: 74.04%\n",
      "Batch 135, Loss: 0.991240, Accuracy: 74.05%\n",
      "Batch 136, Loss: 0.957612, Accuracy: 74.10%\n",
      "Batch 137, Loss: 0.977183, Accuracy: 74.12%\n",
      "Batch 138, Loss: 0.963009, Accuracy: 74.15%\n",
      "Batch 139, Loss: 0.959907, Accuracy: 74.19%\n",
      "Batch 140, Loss: 0.951848, Accuracy: 74.23%\n",
      "Batch 141, Loss: 0.978316, Accuracy: 74.25%\n",
      "Batch 142, Loss: 1.074314, Accuracy: 74.17%\n",
      "Batch 143, Loss: 1.008021, Accuracy: 74.15%\n",
      "Batch 144, Loss: 0.970129, Accuracy: 74.16%\n",
      "Batch 145, Loss: 1.005955, Accuracy: 74.15%\n",
      "Batch 146, Loss: 0.959215, Accuracy: 74.18%\n",
      "Batch 147, Loss: 0.998338, Accuracy: 74.17%\n",
      "Batch 148, Loss: 1.050243, Accuracy: 74.13%\n",
      "Batch 149, Loss: 0.968794, Accuracy: 74.16%\n",
      "Batch 150, Loss: 0.993175, Accuracy: 74.17%\n",
      "Batch 151, Loss: 0.991836, Accuracy: 74.17%\n",
      "Batch 152, Loss: 0.897825, Accuracy: 74.26%\n",
      "Batch 153, Loss: 0.996182, Accuracy: 74.27%\n",
      "Batch 154, Loss: 0.942244, Accuracy: 74.32%\n",
      "Batch 155, Loss: 0.939875, Accuracy: 74.35%\n",
      "Batch 156, Loss: 0.953085, Accuracy: 74.38%\n",
      "Batch 157, Loss: 0.993163, Accuracy: 74.37%\n",
      "Batch 158, Loss: 1.010730, Accuracy: 74.36%\n",
      "Batch 159, Loss: 0.937387, Accuracy: 74.40%\n",
      "Batch 160, Loss: 1.032917, Accuracy: 74.38%\n",
      "Batch 161, Loss: 1.024016, Accuracy: 74.36%\n",
      "Batch 162, Loss: 0.953032, Accuracy: 74.38%\n",
      "Batch 163, Loss: 0.991572, Accuracy: 74.38%\n",
      "Batch 164, Loss: 1.028499, Accuracy: 74.34%\n",
      "Batch 165, Loss: 0.980152, Accuracy: 74.35%\n",
      "Batch 166, Loss: 0.843251, Accuracy: 74.44%\n",
      "Batch 167, Loss: 0.997727, Accuracy: 74.45%\n",
      "Batch 168, Loss: 0.914733, Accuracy: 74.50%\n",
      "Batch 169, Loss: 0.853774, Accuracy: 74.58%\n",
      "Batch 170, Loss: 0.921873, Accuracy: 74.63%\n",
      "Batch 171, Loss: 1.016145, Accuracy: 74.62%\n",
      "Batch 172, Loss: 0.965112, Accuracy: 74.64%\n",
      "Batch 173, Loss: 1.060412, Accuracy: 74.58%\n",
      "Batch 174, Loss: 1.060180, Accuracy: 74.54%\n",
      "Batch 175, Loss: 1.009710, Accuracy: 74.52%\n",
      "Batch 176, Loss: 0.940649, Accuracy: 74.56%\n",
      "Batch 177, Loss: 1.004845, Accuracy: 74.55%\n",
      "Batch 178, Loss: 1.071559, Accuracy: 74.52%\n",
      "Batch 179, Loss: 1.094247, Accuracy: 74.44%\n",
      "Batch 180, Loss: 0.837515, Accuracy: 74.54%\n",
      "Batch 181, Loss: 1.014056, Accuracy: 74.53%\n",
      "Batch 182, Loss: 0.969124, Accuracy: 74.54%\n",
      "Batch 183, Loss: 1.087875, Accuracy: 74.50%\n",
      "Batch 184, Loss: 1.002787, Accuracy: 74.50%\n",
      "Batch 185, Loss: 1.077860, Accuracy: 74.46%\n",
      "Batch 186, Loss: 0.968645, Accuracy: 74.48%\n",
      "Batch 187, Loss: 1.077238, Accuracy: 74.42%\n",
      "Batch 188, Loss: 0.920119, Accuracy: 74.47%\n",
      "Batch 189, Loss: 0.995419, Accuracy: 74.48%\n",
      "Batch 190, Loss: 1.003874, Accuracy: 74.47%\n",
      "Batch 191, Loss: 1.075989, Accuracy: 74.43%\n",
      "Batch 192, Loss: 1.055364, Accuracy: 74.39%\n",
      "Batch 193, Loss: 0.983019, Accuracy: 74.39%\n",
      "Batch 194, Loss: 1.065958, Accuracy: 74.34%\n",
      "Batch 195, Loss: 1.000522, Accuracy: 74.34%\n",
      "Batch 196, Loss: 0.907478, Accuracy: 74.39%\n",
      "Batch 197, Loss: 0.928263, Accuracy: 74.44%\n",
      "Batch 198, Loss: 0.898770, Accuracy: 74.49%\n",
      "Batch 199, Loss: 1.040754, Accuracy: 74.47%\n",
      "Batch 200, Loss: 0.993461, Accuracy: 74.48%\n",
      "Batch 201, Loss: 1.030655, Accuracy: 74.47%\n",
      "Batch 202, Loss: 1.014325, Accuracy: 74.46%\n",
      "Batch 203, Loss: 0.916513, Accuracy: 74.50%\n",
      "Batch 204, Loss: 0.892245, Accuracy: 74.56%\n",
      "Batch 205, Loss: 0.938055, Accuracy: 74.60%\n",
      "Batch 206, Loss: 1.000639, Accuracy: 74.59%\n",
      "Batch 207, Loss: 0.977545, Accuracy: 74.60%\n",
      "Batch 208, Loss: 1.062642, Accuracy: 74.57%\n",
      "Batch 209, Loss: 1.088001, Accuracy: 74.53%\n",
      "Batch 210, Loss: 1.009292, Accuracy: 74.52%\n",
      "Batch 211, Loss: 0.977071, Accuracy: 74.52%\n",
      "Batch 212, Loss: 0.927773, Accuracy: 74.57%\n",
      "Batch 213, Loss: 1.122751, Accuracy: 74.50%\n",
      "Training - Epoch 62, Loss: 0.995580, Accuracy: 74.50%\n",
      "Validation Batch 1, Loss: 0.994431, Accuracy: 75.00%\n",
      "Validation Batch 2, Loss: 1.092297, Accuracy: 70.31%\n",
      "Validation Batch 3, Loss: 1.097652, Accuracy: 68.75%\n",
      "Validation Batch 4, Loss: 1.036395, Accuracy: 68.36%\n",
      "Validation Batch 5, Loss: 1.029259, Accuracy: 69.06%\n",
      "Validation Batch 6, Loss: 0.984875, Accuracy: 70.31%\n",
      "Validation Batch 7, Loss: 1.049621, Accuracy: 70.31%\n",
      "Validation Batch 8, Loss: 1.074486, Accuracy: 69.92%\n",
      "Validation Batch 9, Loss: 1.097653, Accuracy: 69.10%\n",
      "Validation Batch 10, Loss: 1.060734, Accuracy: 69.06%\n",
      "Validation Batch 11, Loss: 1.037556, Accuracy: 68.89%\n",
      "Validation Batch 12, Loss: 0.988652, Accuracy: 69.40%\n",
      "Validation Batch 13, Loss: 1.119816, Accuracy: 68.99%\n",
      "Validation Batch 14, Loss: 1.063304, Accuracy: 68.97%\n",
      "Validation Batch 15, Loss: 1.025151, Accuracy: 69.38%\n",
      "Validation Batch 16, Loss: 1.018107, Accuracy: 69.63%\n",
      "Validation Batch 17, Loss: 1.085006, Accuracy: 69.49%\n",
      "Validation Batch 18, Loss: 1.016395, Accuracy: 69.70%\n",
      "Validation Batch 19, Loss: 1.100674, Accuracy: 69.41%\n",
      "Validation Batch 20, Loss: 1.051623, Accuracy: 69.45%\n",
      "Validation Batch 21, Loss: 1.041803, Accuracy: 69.42%\n",
      "Validation Batch 22, Loss: 1.084349, Accuracy: 69.32%\n",
      "Validation Batch 23, Loss: 1.135717, Accuracy: 68.95%\n",
      "Validation Batch 24, Loss: 1.096069, Accuracy: 68.75%\n",
      "Validation Batch 25, Loss: 1.035261, Accuracy: 68.69%\n",
      "Validation Batch 26, Loss: 1.028820, Accuracy: 68.81%\n",
      "Validation Batch 27, Loss: 1.031666, Accuracy: 68.94%\n",
      "Validation - Epoch 62, Loss: 1.054717, Accuracy: 68.94%\n",
      "Patienceâ€”4\n",
      "Epoch 63\n",
      "Batch 1, Loss: 0.952894, Accuracy: 79.69%\n",
      "Batch 2, Loss: 1.049555, Accuracy: 73.44%\n",
      "Batch 3, Loss: 1.002629, Accuracy: 73.44%\n",
      "Batch 4, Loss: 1.002265, Accuracy: 73.83%\n",
      "Batch 5, Loss: 0.987756, Accuracy: 74.06%\n",
      "Batch 6, Loss: 0.912837, Accuracy: 75.78%\n",
      "Batch 7, Loss: 0.997258, Accuracy: 75.67%\n",
      "Batch 8, Loss: 0.901156, Accuracy: 76.95%\n",
      "Batch 9, Loss: 1.007757, Accuracy: 76.39%\n",
      "Batch 10, Loss: 1.040733, Accuracy: 75.31%\n",
      "Batch 11, Loss: 1.004591, Accuracy: 75.14%\n",
      "Batch 12, Loss: 1.045956, Accuracy: 74.61%\n",
      "Batch 13, Loss: 1.087777, Accuracy: 73.68%\n",
      "Batch 14, Loss: 1.013929, Accuracy: 73.66%\n",
      "Batch 15, Loss: 0.902791, Accuracy: 74.48%\n",
      "Batch 16, Loss: 1.092793, Accuracy: 73.73%\n",
      "Batch 17, Loss: 0.952150, Accuracy: 74.08%\n",
      "Batch 18, Loss: 0.966750, Accuracy: 74.31%\n",
      "Batch 19, Loss: 0.936348, Accuracy: 74.67%\n",
      "Batch 20, Loss: 0.997748, Accuracy: 74.69%\n",
      "Batch 21, Loss: 1.069007, Accuracy: 74.33%\n",
      "Batch 22, Loss: 0.941653, Accuracy: 74.64%\n",
      "Batch 23, Loss: 0.993857, Accuracy: 74.73%\n",
      "Batch 24, Loss: 1.022503, Accuracy: 74.61%\n",
      "Batch 25, Loss: 0.960878, Accuracy: 74.81%\n",
      "Batch 26, Loss: 0.950650, Accuracy: 75.00%\n",
      "Batch 27, Loss: 1.009603, Accuracy: 74.88%\n",
      "Batch 28, Loss: 0.987082, Accuracy: 75.00%\n",
      "Batch 29, Loss: 0.927695, Accuracy: 75.32%\n",
      "Batch 30, Loss: 1.019932, Accuracy: 75.16%\n",
      "Batch 31, Loss: 0.980060, Accuracy: 75.15%\n",
      "Batch 32, Loss: 1.031456, Accuracy: 75.00%\n",
      "Batch 33, Loss: 0.911787, Accuracy: 75.24%\n",
      "Batch 34, Loss: 0.948328, Accuracy: 75.37%\n",
      "Batch 35, Loss: 0.994470, Accuracy: 75.40%\n",
      "Batch 36, Loss: 0.935831, Accuracy: 75.56%\n",
      "Batch 37, Loss: 0.987451, Accuracy: 75.55%\n",
      "Batch 38, Loss: 0.981542, Accuracy: 75.53%\n",
      "Batch 39, Loss: 1.009587, Accuracy: 75.48%\n",
      "Batch 40, Loss: 1.083983, Accuracy: 75.20%\n",
      "Batch 41, Loss: 1.031595, Accuracy: 75.11%\n",
      "Batch 42, Loss: 0.876285, Accuracy: 75.41%\n",
      "Batch 43, Loss: 0.969122, Accuracy: 75.44%\n",
      "Batch 44, Loss: 1.001909, Accuracy: 75.39%\n",
      "Batch 45, Loss: 1.061247, Accuracy: 75.17%\n",
      "Batch 46, Loss: 1.075995, Accuracy: 74.97%\n",
      "Batch 47, Loss: 1.023476, Accuracy: 74.90%\n",
      "Batch 48, Loss: 1.037846, Accuracy: 74.77%\n",
      "Batch 49, Loss: 1.036225, Accuracy: 74.68%\n",
      "Batch 50, Loss: 1.062039, Accuracy: 74.50%\n",
      "Batch 51, Loss: 1.050786, Accuracy: 74.42%\n",
      "Batch 52, Loss: 0.974260, Accuracy: 74.46%\n",
      "Batch 53, Loss: 0.963623, Accuracy: 74.56%\n",
      "Batch 54, Loss: 1.084031, Accuracy: 74.39%\n",
      "Batch 55, Loss: 0.923855, Accuracy: 74.52%\n",
      "Batch 56, Loss: 0.946350, Accuracy: 74.58%\n",
      "Batch 57, Loss: 0.932418, Accuracy: 74.70%\n",
      "Batch 58, Loss: 0.979284, Accuracy: 74.73%\n",
      "Batch 59, Loss: 0.922819, Accuracy: 74.87%\n",
      "Batch 60, Loss: 0.938145, Accuracy: 75.00%\n",
      "Batch 61, Loss: 0.996062, Accuracy: 74.97%\n",
      "Batch 62, Loss: 0.930835, Accuracy: 75.10%\n",
      "Batch 63, Loss: 1.070261, Accuracy: 74.98%\n",
      "Batch 64, Loss: 0.964894, Accuracy: 75.00%\n",
      "Batch 65, Loss: 1.028975, Accuracy: 74.93%\n",
      "Batch 66, Loss: 0.894240, Accuracy: 75.07%\n",
      "Batch 67, Loss: 0.961327, Accuracy: 75.14%\n",
      "Batch 68, Loss: 1.098886, Accuracy: 75.00%\n",
      "Batch 69, Loss: 0.985178, Accuracy: 75.00%\n",
      "Batch 70, Loss: 0.990263, Accuracy: 75.00%\n",
      "Batch 71, Loss: 0.917845, Accuracy: 75.09%\n",
      "Batch 72, Loss: 0.953855, Accuracy: 75.13%\n",
      "Batch 73, Loss: 1.023639, Accuracy: 75.11%\n",
      "Batch 74, Loss: 1.021341, Accuracy: 75.06%\n",
      "Batch 75, Loss: 0.952549, Accuracy: 75.10%\n",
      "Batch 76, Loss: 0.935503, Accuracy: 75.19%\n",
      "Batch 77, Loss: 1.023382, Accuracy: 75.14%\n",
      "Batch 78, Loss: 0.906275, Accuracy: 75.26%\n",
      "Batch 79, Loss: 0.949829, Accuracy: 75.32%\n",
      "Batch 80, Loss: 0.973239, Accuracy: 75.31%\n",
      "Batch 81, Loss: 0.987831, Accuracy: 75.33%\n",
      "Batch 82, Loss: 0.956173, Accuracy: 75.36%\n",
      "Batch 83, Loss: 1.029831, Accuracy: 75.30%\n",
      "Batch 84, Loss: 0.984719, Accuracy: 75.28%\n",
      "Batch 85, Loss: 0.954992, Accuracy: 75.31%\n",
      "Batch 86, Loss: 1.016687, Accuracy: 75.27%\n",
      "Batch 87, Loss: 0.984298, Accuracy: 75.29%\n",
      "Batch 88, Loss: 1.019141, Accuracy: 75.23%\n",
      "Batch 89, Loss: 0.993190, Accuracy: 75.25%\n",
      "Batch 90, Loss: 0.928503, Accuracy: 75.33%\n",
      "Batch 91, Loss: 0.976306, Accuracy: 75.34%\n",
      "Batch 92, Loss: 0.953147, Accuracy: 75.37%\n",
      "Batch 93, Loss: 1.058813, Accuracy: 75.29%\n",
      "Batch 94, Loss: 0.981432, Accuracy: 75.28%\n",
      "Batch 95, Loss: 1.002599, Accuracy: 75.26%\n",
      "Batch 96, Loss: 1.024991, Accuracy: 75.23%\n",
      "Batch 97, Loss: 0.966411, Accuracy: 75.26%\n",
      "Batch 98, Loss: 1.023205, Accuracy: 75.24%\n",
      "Batch 99, Loss: 0.973247, Accuracy: 75.25%\n",
      "Batch 100, Loss: 0.935227, Accuracy: 75.33%\n",
      "Batch 101, Loss: 1.019397, Accuracy: 75.29%\n",
      "Batch 102, Loss: 0.975221, Accuracy: 75.32%\n",
      "Batch 103, Loss: 1.040695, Accuracy: 75.27%\n",
      "Batch 104, Loss: 1.077039, Accuracy: 75.20%\n",
      "Batch 105, Loss: 1.059914, Accuracy: 75.10%\n",
      "Batch 106, Loss: 0.929632, Accuracy: 75.19%\n",
      "Batch 107, Loss: 0.949746, Accuracy: 75.23%\n",
      "Batch 108, Loss: 1.033384, Accuracy: 75.19%\n",
      "Batch 109, Loss: 0.988042, Accuracy: 75.20%\n",
      "Batch 110, Loss: 1.117719, Accuracy: 75.10%\n",
      "Batch 111, Loss: 0.978011, Accuracy: 75.13%\n",
      "Batch 112, Loss: 1.114930, Accuracy: 75.00%\n",
      "Batch 113, Loss: 0.983400, Accuracy: 75.00%\n",
      "Batch 114, Loss: 1.095221, Accuracy: 74.89%\n",
      "Batch 115, Loss: 1.039795, Accuracy: 74.85%\n",
      "Batch 116, Loss: 0.989862, Accuracy: 74.85%\n",
      "Batch 117, Loss: 0.921073, Accuracy: 74.92%\n",
      "Batch 118, Loss: 1.023260, Accuracy: 74.89%\n",
      "Batch 119, Loss: 1.038738, Accuracy: 74.86%\n",
      "Batch 120, Loss: 0.963097, Accuracy: 74.88%\n",
      "Batch 121, Loss: 0.974134, Accuracy: 74.88%\n",
      "Batch 122, Loss: 0.986285, Accuracy: 74.88%\n",
      "Batch 123, Loss: 0.973680, Accuracy: 74.90%\n",
      "Batch 124, Loss: 1.011495, Accuracy: 74.89%\n",
      "Batch 125, Loss: 1.038188, Accuracy: 74.85%\n",
      "Batch 126, Loss: 1.050893, Accuracy: 74.80%\n",
      "Batch 127, Loss: 0.962893, Accuracy: 74.83%\n",
      "Batch 128, Loss: 0.970923, Accuracy: 74.84%\n",
      "Batch 129, Loss: 1.056764, Accuracy: 74.78%\n",
      "Batch 130, Loss: 0.898844, Accuracy: 74.86%\n",
      "Batch 131, Loss: 1.083205, Accuracy: 74.79%\n",
      "Batch 132, Loss: 0.994521, Accuracy: 74.80%\n",
      "Batch 133, Loss: 0.990504, Accuracy: 74.79%\n",
      "Batch 134, Loss: 0.992402, Accuracy: 74.78%\n",
      "Batch 135, Loss: 0.951275, Accuracy: 74.80%\n",
      "Batch 136, Loss: 1.055755, Accuracy: 74.76%\n",
      "Batch 137, Loss: 0.961681, Accuracy: 74.79%\n",
      "Batch 138, Loss: 0.971472, Accuracy: 74.81%\n",
      "Batch 139, Loss: 1.029329, Accuracy: 74.78%\n",
      "Batch 140, Loss: 0.996911, Accuracy: 74.78%\n",
      "Batch 141, Loss: 1.129500, Accuracy: 74.67%\n",
      "Batch 142, Loss: 0.959261, Accuracy: 74.69%\n",
      "Batch 143, Loss: 0.976669, Accuracy: 74.72%\n",
      "Batch 144, Loss: 0.966529, Accuracy: 74.73%\n",
      "Batch 145, Loss: 1.021911, Accuracy: 74.70%\n",
      "Batch 146, Loss: 0.941692, Accuracy: 74.74%\n",
      "Batch 147, Loss: 0.983645, Accuracy: 74.76%\n",
      "Batch 148, Loss: 1.063856, Accuracy: 74.70%\n",
      "Batch 149, Loss: 1.052433, Accuracy: 74.65%\n",
      "Batch 150, Loss: 0.927868, Accuracy: 74.70%\n",
      "Batch 151, Loss: 0.986261, Accuracy: 74.71%\n",
      "Batch 152, Loss: 1.018269, Accuracy: 74.68%\n",
      "Batch 153, Loss: 1.004749, Accuracy: 74.67%\n",
      "Batch 154, Loss: 0.979338, Accuracy: 74.69%\n",
      "Batch 155, Loss: 1.056261, Accuracy: 74.65%\n",
      "Batch 156, Loss: 0.981171, Accuracy: 74.64%\n",
      "Batch 157, Loss: 1.036410, Accuracy: 74.60%\n",
      "Batch 158, Loss: 0.928398, Accuracy: 74.63%\n",
      "Batch 159, Loss: 1.048093, Accuracy: 74.60%\n",
      "Batch 160, Loss: 1.041200, Accuracy: 74.55%\n",
      "Batch 161, Loss: 0.998881, Accuracy: 74.55%\n",
      "Batch 162, Loss: 0.963681, Accuracy: 74.57%\n",
      "Batch 163, Loss: 1.003927, Accuracy: 74.57%\n",
      "Batch 164, Loss: 1.007222, Accuracy: 74.56%\n",
      "Batch 165, Loss: 0.959553, Accuracy: 74.58%\n",
      "Batch 166, Loss: 1.098873, Accuracy: 74.50%\n",
      "Batch 167, Loss: 0.988356, Accuracy: 74.50%\n",
      "Batch 168, Loss: 1.081904, Accuracy: 74.44%\n",
      "Batch 169, Loss: 0.957824, Accuracy: 74.46%\n",
      "Batch 170, Loss: 0.969446, Accuracy: 74.48%\n",
      "Batch 171, Loss: 0.935261, Accuracy: 74.52%\n",
      "Batch 172, Loss: 0.966653, Accuracy: 74.54%\n",
      "Batch 173, Loss: 0.895006, Accuracy: 74.59%\n",
      "Batch 174, Loss: 0.953221, Accuracy: 74.63%\n",
      "Batch 175, Loss: 1.039365, Accuracy: 74.60%\n",
      "Batch 176, Loss: 0.996442, Accuracy: 74.58%\n",
      "Batch 177, Loss: 1.011273, Accuracy: 74.57%\n",
      "Batch 178, Loss: 0.977304, Accuracy: 74.59%\n",
      "Batch 179, Loss: 1.017804, Accuracy: 74.58%\n",
      "Batch 180, Loss: 0.968819, Accuracy: 74.60%\n",
      "Batch 181, Loss: 0.994447, Accuracy: 74.59%\n",
      "Batch 182, Loss: 0.908595, Accuracy: 74.65%\n",
      "Batch 183, Loss: 0.981763, Accuracy: 74.65%\n",
      "Batch 184, Loss: 0.937541, Accuracy: 74.68%\n",
      "Batch 185, Loss: 0.901066, Accuracy: 74.74%\n",
      "Batch 186, Loss: 0.919222, Accuracy: 74.79%\n",
      "Batch 187, Loss: 1.042606, Accuracy: 74.77%\n",
      "Batch 188, Loss: 0.921204, Accuracy: 74.81%\n",
      "Batch 189, Loss: 1.026477, Accuracy: 74.79%\n",
      "Batch 190, Loss: 1.045512, Accuracy: 74.76%\n",
      "Batch 191, Loss: 1.045572, Accuracy: 74.74%\n",
      "Batch 192, Loss: 1.011135, Accuracy: 74.72%\n",
      "Batch 193, Loss: 1.008992, Accuracy: 74.72%\n",
      "Batch 194, Loss: 0.987253, Accuracy: 74.73%\n",
      "Batch 195, Loss: 0.949370, Accuracy: 74.75%\n",
      "Batch 196, Loss: 0.987967, Accuracy: 74.76%\n",
      "Batch 197, Loss: 0.972572, Accuracy: 74.76%\n",
      "Batch 198, Loss: 1.000366, Accuracy: 74.75%\n",
      "Batch 199, Loss: 0.953569, Accuracy: 74.76%\n",
      "Batch 200, Loss: 0.912084, Accuracy: 74.80%\n",
      "Batch 201, Loss: 0.955363, Accuracy: 74.83%\n",
      "Batch 202, Loss: 0.994210, Accuracy: 74.83%\n",
      "Batch 203, Loss: 0.957741, Accuracy: 74.85%\n",
      "Batch 204, Loss: 1.009522, Accuracy: 74.85%\n",
      "Batch 205, Loss: 0.945694, Accuracy: 74.87%\n",
      "Batch 206, Loss: 0.997690, Accuracy: 74.88%\n",
      "Batch 207, Loss: 0.991568, Accuracy: 74.88%\n",
      "Batch 208, Loss: 0.949317, Accuracy: 74.91%\n",
      "Batch 209, Loss: 0.995577, Accuracy: 74.91%\n",
      "Batch 210, Loss: 0.984626, Accuracy: 74.91%\n",
      "Batch 211, Loss: 0.992658, Accuracy: 74.91%\n",
      "Batch 212, Loss: 1.024015, Accuracy: 74.90%\n",
      "Batch 213, Loss: 1.075323, Accuracy: 74.84%\n",
      "Training - Epoch 63, Loss: 0.992431, Accuracy: 74.84%\n",
      "Validation Batch 1, Loss: 1.012517, Accuracy: 71.88%\n",
      "Validation Batch 2, Loss: 1.117812, Accuracy: 67.97%\n",
      "Validation Batch 3, Loss: 1.114393, Accuracy: 66.15%\n",
      "Validation Batch 4, Loss: 1.050988, Accuracy: 66.41%\n",
      "Validation Batch 5, Loss: 1.069013, Accuracy: 65.94%\n",
      "Validation Batch 6, Loss: 1.011709, Accuracy: 66.93%\n",
      "Validation Batch 7, Loss: 1.069661, Accuracy: 67.19%\n",
      "Validation Batch 8, Loss: 1.088497, Accuracy: 66.80%\n",
      "Validation Batch 9, Loss: 1.113600, Accuracy: 66.15%\n",
      "Validation Batch 10, Loss: 1.081006, Accuracy: 66.25%\n",
      "Validation Batch 11, Loss: 1.064335, Accuracy: 65.91%\n",
      "Validation Batch 12, Loss: 1.009370, Accuracy: 66.54%\n",
      "Validation Batch 13, Loss: 1.139836, Accuracy: 66.11%\n",
      "Validation Batch 14, Loss: 1.082423, Accuracy: 65.96%\n",
      "Validation Batch 15, Loss: 1.046757, Accuracy: 66.35%\n",
      "Validation Batch 16, Loss: 1.050811, Accuracy: 66.41%\n",
      "Validation Batch 17, Loss: 1.124053, Accuracy: 66.08%\n",
      "Validation Batch 18, Loss: 1.033265, Accuracy: 66.32%\n",
      "Validation Batch 19, Loss: 1.141321, Accuracy: 65.79%\n",
      "Validation Batch 20, Loss: 1.078369, Accuracy: 65.78%\n",
      "Validation Batch 21, Loss: 1.066687, Accuracy: 65.92%\n",
      "Validation Batch 22, Loss: 1.116618, Accuracy: 65.70%\n",
      "Validation Batch 23, Loss: 1.163799, Accuracy: 65.35%\n",
      "Validation Batch 24, Loss: 1.104075, Accuracy: 65.30%\n",
      "Validation Batch 25, Loss: 1.068509, Accuracy: 65.25%\n",
      "Validation Batch 26, Loss: 1.032516, Accuracy: 65.50%\n",
      "Validation Batch 27, Loss: 1.056834, Accuracy: 65.59%\n",
      "Validation - Epoch 63, Loss: 1.078103, Accuracy: 65.59%\n",
      "Patienceâ€”5\n",
      "Epoch 64\n",
      "Batch 1, Loss: 0.997497, Accuracy: 73.44%\n",
      "Batch 2, Loss: 0.981153, Accuracy: 75.00%\n",
      "Batch 3, Loss: 0.992795, Accuracy: 73.96%\n",
      "Batch 4, Loss: 0.951966, Accuracy: 75.00%\n",
      "Batch 5, Loss: 0.979489, Accuracy: 75.00%\n",
      "Batch 6, Loss: 0.995471, Accuracy: 75.00%\n",
      "Batch 7, Loss: 0.898966, Accuracy: 76.12%\n",
      "Batch 8, Loss: 1.070422, Accuracy: 75.00%\n",
      "Batch 9, Loss: 0.998371, Accuracy: 75.00%\n",
      "Batch 10, Loss: 0.997165, Accuracy: 75.00%\n",
      "Batch 11, Loss: 0.985976, Accuracy: 75.00%\n",
      "Batch 12, Loss: 1.020548, Accuracy: 74.74%\n",
      "Batch 13, Loss: 0.915463, Accuracy: 75.36%\n",
      "Batch 14, Loss: 0.980488, Accuracy: 75.33%\n",
      "Batch 15, Loss: 0.890606, Accuracy: 76.04%\n",
      "Batch 16, Loss: 0.993097, Accuracy: 75.88%\n",
      "Batch 17, Loss: 0.965923, Accuracy: 76.01%\n",
      "Batch 18, Loss: 1.048856, Accuracy: 75.61%\n",
      "Batch 19, Loss: 1.082289, Accuracy: 74.92%\n",
      "Batch 20, Loss: 1.000305, Accuracy: 74.92%\n",
      "Batch 21, Loss: 0.948305, Accuracy: 75.22%\n",
      "Batch 22, Loss: 1.025584, Accuracy: 75.00%\n",
      "Batch 23, Loss: 1.009399, Accuracy: 74.93%\n",
      "Batch 24, Loss: 0.909221, Accuracy: 75.33%\n",
      "Batch 25, Loss: 0.872970, Accuracy: 75.81%\n",
      "Batch 26, Loss: 1.021297, Accuracy: 75.60%\n",
      "Batch 27, Loss: 0.983874, Accuracy: 75.52%\n",
      "Batch 28, Loss: 0.915940, Accuracy: 75.78%\n",
      "Batch 29, Loss: 0.991150, Accuracy: 75.70%\n",
      "Batch 30, Loss: 1.028226, Accuracy: 75.52%\n",
      "Batch 31, Loss: 0.956363, Accuracy: 75.60%\n",
      "Batch 32, Loss: 1.012208, Accuracy: 75.49%\n",
      "Batch 33, Loss: 1.066520, Accuracy: 75.24%\n",
      "Batch 34, Loss: 1.006621, Accuracy: 75.14%\n",
      "Batch 35, Loss: 0.969619, Accuracy: 75.22%\n",
      "Batch 36, Loss: 1.009399, Accuracy: 75.13%\n",
      "Batch 37, Loss: 1.067508, Accuracy: 74.79%\n",
      "Batch 38, Loss: 1.017872, Accuracy: 74.71%\n",
      "Batch 39, Loss: 0.986918, Accuracy: 74.76%\n",
      "Batch 40, Loss: 1.138047, Accuracy: 74.41%\n",
      "Batch 41, Loss: 1.022037, Accuracy: 74.39%\n",
      "Batch 42, Loss: 1.033512, Accuracy: 74.29%\n",
      "Batch 43, Loss: 0.979638, Accuracy: 74.38%\n",
      "Batch 44, Loss: 0.957133, Accuracy: 74.47%\n",
      "Batch 45, Loss: 1.009611, Accuracy: 74.48%\n",
      "Batch 46, Loss: 0.974622, Accuracy: 74.52%\n",
      "Batch 47, Loss: 0.959084, Accuracy: 74.63%\n",
      "Batch 48, Loss: 1.080685, Accuracy: 74.38%\n",
      "Batch 49, Loss: 1.044157, Accuracy: 74.30%\n",
      "Batch 50, Loss: 0.960214, Accuracy: 74.34%\n",
      "Batch 51, Loss: 0.988042, Accuracy: 74.33%\n",
      "Batch 52, Loss: 0.933933, Accuracy: 74.43%\n",
      "Batch 53, Loss: 0.947721, Accuracy: 74.53%\n",
      "Batch 54, Loss: 0.893108, Accuracy: 74.74%\n",
      "Batch 55, Loss: 1.035921, Accuracy: 74.66%\n",
      "Batch 56, Loss: 1.106887, Accuracy: 74.47%\n",
      "Batch 57, Loss: 1.031425, Accuracy: 74.42%\n",
      "Batch 58, Loss: 1.054922, Accuracy: 74.30%\n",
      "Batch 59, Loss: 1.078086, Accuracy: 74.18%\n",
      "Batch 60, Loss: 0.964314, Accuracy: 74.24%\n",
      "Batch 61, Loss: 0.893661, Accuracy: 74.44%\n",
      "Batch 62, Loss: 0.963619, Accuracy: 74.50%\n",
      "Batch 63, Loss: 1.000280, Accuracy: 74.48%\n",
      "Batch 64, Loss: 1.030019, Accuracy: 74.44%\n",
      "Batch 65, Loss: 0.903377, Accuracy: 74.59%\n",
      "Batch 66, Loss: 1.028350, Accuracy: 74.55%\n",
      "Batch 67, Loss: 0.989324, Accuracy: 74.56%\n",
      "Batch 68, Loss: 0.994147, Accuracy: 74.59%\n",
      "Batch 69, Loss: 1.019212, Accuracy: 74.57%\n",
      "Batch 70, Loss: 1.011649, Accuracy: 74.55%\n",
      "Batch 71, Loss: 0.966846, Accuracy: 74.58%\n",
      "Batch 72, Loss: 1.010800, Accuracy: 74.59%\n",
      "Batch 73, Loss: 0.964425, Accuracy: 74.64%\n",
      "Batch 74, Loss: 0.882020, Accuracy: 74.79%\n",
      "Batch 75, Loss: 0.906735, Accuracy: 74.92%\n",
      "Batch 76, Loss: 1.006570, Accuracy: 74.92%\n",
      "Batch 77, Loss: 1.027495, Accuracy: 74.84%\n",
      "Batch 78, Loss: 0.945916, Accuracy: 74.90%\n",
      "Batch 79, Loss: 0.996107, Accuracy: 74.92%\n",
      "Batch 80, Loss: 0.942774, Accuracy: 75.00%\n",
      "Batch 81, Loss: 1.125806, Accuracy: 74.81%\n",
      "Batch 82, Loss: 0.955268, Accuracy: 74.85%\n",
      "Batch 83, Loss: 0.990198, Accuracy: 74.85%\n",
      "Batch 84, Loss: 0.931594, Accuracy: 74.94%\n",
      "Batch 85, Loss: 0.923418, Accuracy: 75.04%\n",
      "Batch 86, Loss: 0.942169, Accuracy: 75.11%\n",
      "Batch 87, Loss: 1.093832, Accuracy: 75.00%\n",
      "Batch 88, Loss: 0.999921, Accuracy: 74.98%\n",
      "Batch 89, Loss: 1.017741, Accuracy: 74.95%\n",
      "Batch 90, Loss: 1.004928, Accuracy: 74.93%\n",
      "Batch 91, Loss: 0.955104, Accuracy: 74.98%\n",
      "Batch 92, Loss: 1.111182, Accuracy: 74.81%\n",
      "Batch 93, Loss: 1.079659, Accuracy: 74.71%\n",
      "Batch 94, Loss: 1.065369, Accuracy: 74.62%\n",
      "Batch 95, Loss: 1.002046, Accuracy: 74.61%\n",
      "Batch 96, Loss: 1.083319, Accuracy: 74.51%\n",
      "Batch 97, Loss: 1.011119, Accuracy: 74.53%\n",
      "Batch 98, Loss: 0.997410, Accuracy: 74.52%\n",
      "Batch 99, Loss: 0.882738, Accuracy: 74.65%\n",
      "Batch 100, Loss: 0.970895, Accuracy: 74.67%\n",
      "Batch 101, Loss: 1.006778, Accuracy: 74.68%\n",
      "Batch 102, Loss: 0.978312, Accuracy: 74.66%\n",
      "Batch 103, Loss: 0.985118, Accuracy: 74.68%\n",
      "Batch 104, Loss: 0.949747, Accuracy: 74.73%\n",
      "Batch 105, Loss: 1.034179, Accuracy: 74.66%\n",
      "Batch 106, Loss: 0.987992, Accuracy: 74.66%\n",
      "Batch 107, Loss: 1.020707, Accuracy: 74.62%\n",
      "Batch 108, Loss: 0.931184, Accuracy: 74.65%\n",
      "Batch 109, Loss: 0.996475, Accuracy: 74.64%\n",
      "Batch 110, Loss: 1.014454, Accuracy: 74.62%\n",
      "Batch 111, Loss: 1.014305, Accuracy: 74.61%\n",
      "Batch 112, Loss: 1.007157, Accuracy: 74.60%\n",
      "Batch 113, Loss: 0.971172, Accuracy: 74.63%\n",
      "Batch 114, Loss: 1.015895, Accuracy: 74.62%\n",
      "Batch 115, Loss: 1.020657, Accuracy: 74.59%\n",
      "Batch 116, Loss: 1.099357, Accuracy: 74.49%\n",
      "Batch 117, Loss: 0.974629, Accuracy: 74.49%\n",
      "Batch 118, Loss: 0.902316, Accuracy: 74.56%\n",
      "Batch 119, Loss: 0.939131, Accuracy: 74.61%\n",
      "Batch 120, Loss: 1.025037, Accuracy: 74.57%\n",
      "Batch 121, Loss: 0.987542, Accuracy: 74.59%\n",
      "Batch 122, Loss: 0.976919, Accuracy: 74.60%\n",
      "Batch 123, Loss: 0.956626, Accuracy: 74.64%\n",
      "Batch 124, Loss: 1.028116, Accuracy: 74.61%\n",
      "Batch 125, Loss: 1.043545, Accuracy: 74.56%\n",
      "Batch 126, Loss: 0.898699, Accuracy: 74.65%\n",
      "Batch 127, Loss: 1.020629, Accuracy: 74.62%\n",
      "Batch 128, Loss: 1.011932, Accuracy: 74.61%\n",
      "Batch 129, Loss: 0.995546, Accuracy: 74.61%\n",
      "Batch 130, Loss: 1.008056, Accuracy: 74.62%\n",
      "Batch 131, Loss: 1.025575, Accuracy: 74.61%\n",
      "Batch 132, Loss: 0.943941, Accuracy: 74.64%\n",
      "Batch 133, Loss: 1.004266, Accuracy: 74.64%\n",
      "Batch 134, Loss: 0.921061, Accuracy: 74.69%\n",
      "Batch 135, Loss: 0.975678, Accuracy: 74.70%\n",
      "Batch 136, Loss: 0.953619, Accuracy: 74.74%\n",
      "Batch 137, Loss: 0.995292, Accuracy: 74.73%\n",
      "Batch 138, Loss: 0.959767, Accuracy: 74.73%\n",
      "Batch 139, Loss: 0.901555, Accuracy: 74.80%\n",
      "Batch 140, Loss: 0.942176, Accuracy: 74.83%\n",
      "Batch 141, Loss: 0.870530, Accuracy: 74.92%\n",
      "Batch 142, Loss: 0.948368, Accuracy: 74.96%\n",
      "Batch 143, Loss: 0.944223, Accuracy: 75.00%\n",
      "Batch 144, Loss: 1.097633, Accuracy: 74.91%\n",
      "Batch 145, Loss: 0.984251, Accuracy: 74.91%\n",
      "Batch 146, Loss: 1.016869, Accuracy: 74.90%\n",
      "Batch 147, Loss: 0.934896, Accuracy: 74.95%\n",
      "Batch 148, Loss: 0.928551, Accuracy: 74.99%\n",
      "Batch 149, Loss: 0.987963, Accuracy: 74.99%\n",
      "Batch 150, Loss: 0.984876, Accuracy: 74.99%\n",
      "Batch 151, Loss: 1.029104, Accuracy: 74.95%\n",
      "Batch 152, Loss: 1.002339, Accuracy: 74.95%\n",
      "Batch 153, Loss: 1.026705, Accuracy: 74.94%\n",
      "Batch 154, Loss: 0.997424, Accuracy: 74.92%\n",
      "Batch 155, Loss: 1.010294, Accuracy: 74.90%\n",
      "Batch 156, Loss: 1.030003, Accuracy: 74.88%\n",
      "Batch 157, Loss: 0.990425, Accuracy: 74.89%\n",
      "Batch 158, Loss: 0.941855, Accuracy: 74.92%\n",
      "Batch 159, Loss: 1.008473, Accuracy: 74.91%\n",
      "Batch 160, Loss: 0.919133, Accuracy: 74.96%\n",
      "Batch 161, Loss: 1.083360, Accuracy: 74.91%\n",
      "Batch 162, Loss: 1.097924, Accuracy: 74.85%\n",
      "Batch 163, Loss: 1.015035, Accuracy: 74.84%\n",
      "Batch 164, Loss: 1.012934, Accuracy: 74.83%\n",
      "Batch 165, Loss: 0.959233, Accuracy: 74.85%\n",
      "Batch 166, Loss: 1.007539, Accuracy: 74.84%\n",
      "Batch 167, Loss: 1.019303, Accuracy: 74.82%\n",
      "Batch 168, Loss: 0.958530, Accuracy: 74.84%\n",
      "Batch 169, Loss: 0.968549, Accuracy: 74.85%\n",
      "Batch 170, Loss: 0.972510, Accuracy: 74.85%\n",
      "Batch 171, Loss: 1.048605, Accuracy: 74.82%\n",
      "Batch 172, Loss: 0.999632, Accuracy: 74.81%\n",
      "Batch 173, Loss: 1.038534, Accuracy: 74.78%\n",
      "Batch 174, Loss: 0.935582, Accuracy: 74.81%\n",
      "Batch 175, Loss: 0.852463, Accuracy: 74.89%\n",
      "Batch 176, Loss: 1.023293, Accuracy: 74.87%\n",
      "Batch 177, Loss: 1.046656, Accuracy: 74.82%\n",
      "Batch 178, Loss: 1.023169, Accuracy: 74.80%\n",
      "Batch 179, Loss: 0.953664, Accuracy: 74.81%\n",
      "Batch 180, Loss: 1.032252, Accuracy: 74.78%\n",
      "Batch 181, Loss: 0.965477, Accuracy: 74.79%\n",
      "Batch 182, Loss: 1.033708, Accuracy: 74.77%\n",
      "Batch 183, Loss: 0.974591, Accuracy: 74.79%\n",
      "Batch 184, Loss: 1.036591, Accuracy: 74.75%\n",
      "Batch 185, Loss: 0.953902, Accuracy: 74.78%\n",
      "Batch 186, Loss: 1.026869, Accuracy: 74.76%\n",
      "Batch 187, Loss: 1.037325, Accuracy: 74.73%\n",
      "Batch 188, Loss: 0.870748, Accuracy: 74.80%\n",
      "Batch 189, Loss: 0.964405, Accuracy: 74.82%\n",
      "Batch 190, Loss: 0.936584, Accuracy: 74.86%\n",
      "Batch 191, Loss: 0.983685, Accuracy: 74.86%\n",
      "Batch 192, Loss: 1.068028, Accuracy: 74.82%\n",
      "Batch 193, Loss: 0.966969, Accuracy: 74.84%\n",
      "Batch 194, Loss: 1.003280, Accuracy: 74.82%\n",
      "Batch 195, Loss: 1.057507, Accuracy: 74.78%\n",
      "Batch 196, Loss: 1.002924, Accuracy: 74.78%\n",
      "Batch 197, Loss: 1.012628, Accuracy: 74.76%\n",
      "Batch 198, Loss: 1.018167, Accuracy: 74.75%\n",
      "Batch 199, Loss: 0.926505, Accuracy: 74.78%\n",
      "Batch 200, Loss: 1.080711, Accuracy: 74.73%\n",
      "Batch 201, Loss: 0.917111, Accuracy: 74.78%\n",
      "Batch 202, Loss: 0.905348, Accuracy: 74.83%\n",
      "Batch 203, Loss: 1.044001, Accuracy: 74.80%\n",
      "Batch 204, Loss: 1.003946, Accuracy: 74.80%\n",
      "Batch 205, Loss: 1.054215, Accuracy: 74.77%\n",
      "Batch 206, Loss: 1.005795, Accuracy: 74.77%\n",
      "Batch 207, Loss: 0.982837, Accuracy: 74.78%\n",
      "Batch 208, Loss: 0.996765, Accuracy: 74.78%\n",
      "Batch 209, Loss: 0.960665, Accuracy: 74.80%\n",
      "Batch 210, Loss: 0.979192, Accuracy: 74.81%\n",
      "Batch 211, Loss: 1.034368, Accuracy: 74.80%\n",
      "Batch 212, Loss: 1.055052, Accuracy: 74.78%\n",
      "Batch 213, Loss: 0.916288, Accuracy: 74.81%\n",
      "Training - Epoch 64, Loss: 0.991878, Accuracy: 74.81%\n",
      "Validation Batch 1, Loss: 0.988085, Accuracy: 75.00%\n",
      "Validation Batch 2, Loss: 1.076203, Accuracy: 71.88%\n",
      "Validation Batch 3, Loss: 1.087855, Accuracy: 69.79%\n",
      "Validation Batch 4, Loss: 1.033094, Accuracy: 69.53%\n",
      "Validation Batch 5, Loss: 1.012868, Accuracy: 70.00%\n",
      "Validation Batch 6, Loss: 0.973351, Accuracy: 70.83%\n",
      "Validation Batch 7, Loss: 1.041079, Accuracy: 70.76%\n",
      "Validation Batch 8, Loss: 1.068593, Accuracy: 70.12%\n",
      "Validation Batch 9, Loss: 1.089568, Accuracy: 69.27%\n",
      "Validation Batch 10, Loss: 1.055358, Accuracy: 69.22%\n",
      "Validation Batch 11, Loss: 1.027863, Accuracy: 68.89%\n",
      "Validation Batch 12, Loss: 0.983790, Accuracy: 69.40%\n",
      "Validation Batch 13, Loss: 1.113760, Accuracy: 68.99%\n",
      "Validation Batch 14, Loss: 1.062207, Accuracy: 68.97%\n",
      "Validation Batch 15, Loss: 1.017308, Accuracy: 69.27%\n",
      "Validation Batch 16, Loss: 1.007572, Accuracy: 69.63%\n",
      "Validation Batch 17, Loss: 1.079654, Accuracy: 69.49%\n",
      "Validation Batch 18, Loss: 1.010403, Accuracy: 69.79%\n",
      "Validation Batch 19, Loss: 1.091563, Accuracy: 69.41%\n",
      "Validation Batch 20, Loss: 1.049557, Accuracy: 69.45%\n",
      "Validation Batch 21, Loss: 1.037369, Accuracy: 69.49%\n",
      "Validation Batch 22, Loss: 1.074589, Accuracy: 69.39%\n",
      "Validation Batch 23, Loss: 1.128603, Accuracy: 68.95%\n",
      "Validation Batch 24, Loss: 1.094539, Accuracy: 68.75%\n",
      "Validation Batch 25, Loss: 1.022136, Accuracy: 68.88%\n",
      "Validation Batch 26, Loss: 1.024846, Accuracy: 68.99%\n",
      "Validation Batch 27, Loss: 1.028792, Accuracy: 69.11%\n",
      "Validation - Epoch 64, Loss: 1.047430, Accuracy: 69.11%\n",
      "Patienceâ€”6\n",
      "Epoch 65\n",
      "Batch 1, Loss: 0.953763, Accuracy: 79.69%\n",
      "Batch 2, Loss: 0.979302, Accuracy: 77.34%\n",
      "Batch 3, Loss: 0.981212, Accuracy: 77.08%\n",
      "Batch 4, Loss: 0.987181, Accuracy: 76.56%\n",
      "Batch 5, Loss: 0.949696, Accuracy: 77.19%\n",
      "Batch 6, Loss: 0.981102, Accuracy: 77.08%\n",
      "Batch 7, Loss: 1.053093, Accuracy: 75.89%\n",
      "Batch 8, Loss: 0.995870, Accuracy: 75.98%\n",
      "Batch 9, Loss: 1.022787, Accuracy: 75.69%\n",
      "Batch 10, Loss: 0.966795, Accuracy: 75.78%\n",
      "Batch 11, Loss: 0.959607, Accuracy: 76.14%\n",
      "Batch 12, Loss: 0.983135, Accuracy: 76.17%\n",
      "Batch 13, Loss: 0.977962, Accuracy: 76.08%\n",
      "Batch 14, Loss: 1.022135, Accuracy: 75.78%\n",
      "Batch 15, Loss: 0.950343, Accuracy: 75.94%\n",
      "Batch 16, Loss: 0.967031, Accuracy: 75.98%\n",
      "Batch 17, Loss: 1.018812, Accuracy: 75.83%\n",
      "Batch 18, Loss: 0.953073, Accuracy: 75.87%\n",
      "Batch 19, Loss: 0.989948, Accuracy: 75.90%\n",
      "Batch 20, Loss: 1.006669, Accuracy: 75.70%\n",
      "Batch 21, Loss: 0.927961, Accuracy: 75.89%\n",
      "Batch 22, Loss: 0.992744, Accuracy: 75.85%\n",
      "Batch 23, Loss: 1.027465, Accuracy: 75.61%\n",
      "Batch 24, Loss: 0.875832, Accuracy: 76.04%\n",
      "Batch 25, Loss: 0.966668, Accuracy: 76.06%\n",
      "Batch 26, Loss: 0.988354, Accuracy: 76.02%\n",
      "Batch 27, Loss: 1.100382, Accuracy: 75.46%\n",
      "Batch 28, Loss: 0.917414, Accuracy: 75.67%\n",
      "Batch 29, Loss: 0.938941, Accuracy: 75.86%\n",
      "Batch 30, Loss: 0.991997, Accuracy: 75.78%\n",
      "Batch 31, Loss: 1.026814, Accuracy: 75.60%\n",
      "Batch 32, Loss: 0.936478, Accuracy: 75.83%\n",
      "Batch 33, Loss: 1.048770, Accuracy: 75.62%\n",
      "Batch 34, Loss: 1.024330, Accuracy: 75.41%\n",
      "Batch 35, Loss: 1.027091, Accuracy: 75.27%\n",
      "Batch 36, Loss: 0.951973, Accuracy: 75.39%\n",
      "Batch 37, Loss: 0.944731, Accuracy: 75.51%\n",
      "Batch 38, Loss: 1.055088, Accuracy: 75.29%\n",
      "Batch 39, Loss: 0.952581, Accuracy: 75.44%\n",
      "Batch 40, Loss: 1.033539, Accuracy: 75.27%\n",
      "Batch 41, Loss: 0.976094, Accuracy: 75.30%\n",
      "Batch 42, Loss: 0.979909, Accuracy: 75.30%\n",
      "Batch 43, Loss: 0.922167, Accuracy: 75.44%\n",
      "Batch 44, Loss: 0.957011, Accuracy: 75.50%\n",
      "Batch 45, Loss: 1.009375, Accuracy: 75.42%\n",
      "Batch 46, Loss: 1.004402, Accuracy: 75.41%\n",
      "Batch 47, Loss: 0.930032, Accuracy: 75.60%\n",
      "Batch 48, Loss: 1.012693, Accuracy: 75.52%\n",
      "Batch 49, Loss: 0.960531, Accuracy: 75.57%\n",
      "Batch 50, Loss: 0.983147, Accuracy: 75.56%\n",
      "Batch 51, Loss: 0.927192, Accuracy: 75.70%\n",
      "Batch 52, Loss: 0.994349, Accuracy: 75.72%\n",
      "Batch 53, Loss: 0.887240, Accuracy: 75.88%\n",
      "Batch 54, Loss: 0.973338, Accuracy: 75.93%\n",
      "Batch 55, Loss: 0.958747, Accuracy: 75.94%\n",
      "Batch 56, Loss: 0.981137, Accuracy: 75.92%\n",
      "Batch 57, Loss: 0.995064, Accuracy: 75.90%\n",
      "Batch 58, Loss: 0.993661, Accuracy: 75.89%\n",
      "Batch 59, Loss: 0.962699, Accuracy: 75.93%\n",
      "Batch 60, Loss: 1.045654, Accuracy: 75.78%\n",
      "Batch 61, Loss: 0.924792, Accuracy: 75.87%\n",
      "Batch 62, Loss: 1.007960, Accuracy: 75.83%\n",
      "Batch 63, Loss: 0.954023, Accuracy: 75.89%\n",
      "Batch 64, Loss: 0.990372, Accuracy: 75.90%\n",
      "Batch 65, Loss: 0.994891, Accuracy: 75.87%\n",
      "Batch 66, Loss: 0.929069, Accuracy: 75.95%\n",
      "Batch 67, Loss: 1.018031, Accuracy: 75.89%\n",
      "Batch 68, Loss: 1.035753, Accuracy: 75.83%\n",
      "Batch 69, Loss: 1.019390, Accuracy: 75.75%\n",
      "Batch 70, Loss: 0.957773, Accuracy: 75.83%\n",
      "Batch 71, Loss: 1.052983, Accuracy: 75.73%\n",
      "Batch 72, Loss: 1.004835, Accuracy: 75.69%\n",
      "Batch 73, Loss: 1.044323, Accuracy: 75.62%\n",
      "Batch 74, Loss: 1.030881, Accuracy: 75.59%\n",
      "Batch 75, Loss: 0.912144, Accuracy: 75.69%\n",
      "Batch 76, Loss: 0.947914, Accuracy: 75.74%\n",
      "Batch 77, Loss: 0.998380, Accuracy: 75.71%\n",
      "Batch 78, Loss: 1.114801, Accuracy: 75.50%\n",
      "Batch 79, Loss: 0.975288, Accuracy: 75.51%\n",
      "Batch 80, Loss: 1.020567, Accuracy: 75.47%\n",
      "Batch 81, Loss: 1.021045, Accuracy: 75.42%\n",
      "Batch 82, Loss: 0.993186, Accuracy: 75.42%\n",
      "Batch 83, Loss: 1.047066, Accuracy: 75.34%\n",
      "Batch 84, Loss: 1.014982, Accuracy: 75.32%\n",
      "Batch 85, Loss: 1.031950, Accuracy: 75.24%\n",
      "Batch 86, Loss: 1.051494, Accuracy: 75.15%\n",
      "Batch 87, Loss: 0.923999, Accuracy: 75.22%\n",
      "Batch 88, Loss: 1.016090, Accuracy: 75.16%\n",
      "Batch 89, Loss: 0.918630, Accuracy: 75.23%\n",
      "Batch 90, Loss: 0.968692, Accuracy: 75.26%\n",
      "Batch 91, Loss: 0.985325, Accuracy: 75.26%\n",
      "Batch 92, Loss: 1.055415, Accuracy: 75.19%\n",
      "Batch 93, Loss: 0.931266, Accuracy: 75.25%\n",
      "Batch 94, Loss: 0.939779, Accuracy: 75.28%\n",
      "Batch 95, Loss: 0.971550, Accuracy: 75.31%\n",
      "Batch 96, Loss: 0.935363, Accuracy: 75.37%\n",
      "Batch 97, Loss: 0.970312, Accuracy: 75.39%\n",
      "Batch 98, Loss: 0.945508, Accuracy: 75.43%\n",
      "Batch 99, Loss: 1.007754, Accuracy: 75.39%\n",
      "Batch 100, Loss: 0.995719, Accuracy: 75.41%\n",
      "Batch 101, Loss: 0.984034, Accuracy: 75.42%\n",
      "Batch 102, Loss: 1.032022, Accuracy: 75.35%\n",
      "Batch 103, Loss: 1.002123, Accuracy: 75.33%\n",
      "Batch 104, Loss: 0.998169, Accuracy: 75.33%\n",
      "Batch 105, Loss: 0.985039, Accuracy: 75.34%\n",
      "Batch 106, Loss: 0.967648, Accuracy: 75.35%\n",
      "Batch 107, Loss: 1.076890, Accuracy: 75.28%\n",
      "Batch 108, Loss: 0.914982, Accuracy: 75.36%\n",
      "Batch 109, Loss: 0.999862, Accuracy: 75.34%\n",
      "Batch 110, Loss: 0.969948, Accuracy: 75.37%\n",
      "Batch 111, Loss: 0.987942, Accuracy: 75.37%\n",
      "Batch 112, Loss: 1.012702, Accuracy: 75.35%\n",
      "Batch 113, Loss: 0.996707, Accuracy: 75.36%\n",
      "Batch 114, Loss: 0.947744, Accuracy: 75.40%\n",
      "Batch 115, Loss: 1.034363, Accuracy: 75.35%\n",
      "Batch 116, Loss: 1.054428, Accuracy: 75.28%\n",
      "Batch 117, Loss: 0.927819, Accuracy: 75.33%\n",
      "Batch 118, Loss: 0.956832, Accuracy: 75.37%\n",
      "Batch 119, Loss: 0.978629, Accuracy: 75.38%\n",
      "Batch 120, Loss: 1.068197, Accuracy: 75.30%\n",
      "Batch 121, Loss: 1.038787, Accuracy: 75.27%\n",
      "Batch 122, Loss: 1.029262, Accuracy: 75.26%\n",
      "Batch 123, Loss: 0.987884, Accuracy: 75.24%\n",
      "Batch 124, Loss: 0.989050, Accuracy: 75.26%\n",
      "Batch 125, Loss: 1.008795, Accuracy: 75.25%\n",
      "Batch 126, Loss: 1.054872, Accuracy: 75.20%\n",
      "Batch 127, Loss: 1.014731, Accuracy: 75.18%\n",
      "Batch 128, Loss: 1.048819, Accuracy: 75.15%\n",
      "Batch 129, Loss: 0.955817, Accuracy: 75.16%\n",
      "Batch 130, Loss: 0.993593, Accuracy: 75.16%\n",
      "Batch 131, Loss: 0.940768, Accuracy: 75.19%\n",
      "Batch 132, Loss: 1.044802, Accuracy: 75.13%\n",
      "Batch 133, Loss: 1.020150, Accuracy: 75.11%\n",
      "Batch 134, Loss: 1.024426, Accuracy: 75.08%\n",
      "Batch 135, Loss: 0.937060, Accuracy: 75.12%\n",
      "Batch 136, Loss: 0.991912, Accuracy: 75.10%\n",
      "Batch 137, Loss: 0.969419, Accuracy: 75.11%\n",
      "Batch 138, Loss: 1.015252, Accuracy: 75.09%\n",
      "Batch 139, Loss: 1.072522, Accuracy: 75.01%\n",
      "Batch 140, Loss: 0.947415, Accuracy: 75.04%\n",
      "Batch 141, Loss: 0.936737, Accuracy: 75.09%\n",
      "Batch 142, Loss: 0.974726, Accuracy: 75.10%\n",
      "Batch 143, Loss: 0.974319, Accuracy: 75.10%\n",
      "Batch 144, Loss: 0.929641, Accuracy: 75.13%\n",
      "Batch 145, Loss: 1.018751, Accuracy: 75.12%\n",
      "Batch 146, Loss: 1.060612, Accuracy: 75.05%\n",
      "Batch 147, Loss: 1.044221, Accuracy: 75.01%\n",
      "Batch 148, Loss: 0.930935, Accuracy: 75.05%\n",
      "Batch 149, Loss: 0.944677, Accuracy: 75.09%\n",
      "Batch 150, Loss: 1.047632, Accuracy: 75.06%\n",
      "Batch 151, Loss: 1.093657, Accuracy: 74.99%\n",
      "Batch 152, Loss: 0.945104, Accuracy: 75.02%\n",
      "Batch 153, Loss: 0.925864, Accuracy: 75.08%\n",
      "Batch 154, Loss: 1.010496, Accuracy: 75.05%\n",
      "Batch 155, Loss: 0.948852, Accuracy: 75.08%\n",
      "Batch 156, Loss: 0.911035, Accuracy: 75.14%\n",
      "Batch 157, Loss: 1.004629, Accuracy: 75.14%\n",
      "Batch 158, Loss: 1.057075, Accuracy: 75.09%\n",
      "Batch 159, Loss: 1.008748, Accuracy: 75.07%\n",
      "Batch 160, Loss: 1.132667, Accuracy: 74.96%\n",
      "Batch 161, Loss: 1.046564, Accuracy: 74.91%\n",
      "Batch 162, Loss: 0.942017, Accuracy: 74.94%\n",
      "Batch 163, Loss: 0.971504, Accuracy: 74.95%\n",
      "Batch 164, Loss: 1.011499, Accuracy: 74.93%\n",
      "Batch 165, Loss: 1.025241, Accuracy: 74.92%\n",
      "Batch 166, Loss: 1.051104, Accuracy: 74.90%\n",
      "Batch 167, Loss: 0.996892, Accuracy: 74.90%\n",
      "Batch 168, Loss: 1.096223, Accuracy: 74.82%\n",
      "Batch 169, Loss: 0.985254, Accuracy: 74.83%\n",
      "Batch 170, Loss: 0.926876, Accuracy: 74.87%\n",
      "Batch 171, Loss: 0.960195, Accuracy: 74.90%\n",
      "Batch 172, Loss: 1.071748, Accuracy: 74.86%\n",
      "Batch 173, Loss: 1.004426, Accuracy: 74.85%\n",
      "Batch 174, Loss: 0.968598, Accuracy: 74.87%\n",
      "Batch 175, Loss: 0.994437, Accuracy: 74.87%\n",
      "Batch 176, Loss: 0.953829, Accuracy: 74.88%\n",
      "Batch 177, Loss: 0.984100, Accuracy: 74.89%\n",
      "Batch 178, Loss: 0.953666, Accuracy: 74.91%\n",
      "Batch 179, Loss: 1.095858, Accuracy: 74.85%\n",
      "Batch 180, Loss: 1.063996, Accuracy: 74.82%\n",
      "Batch 181, Loss: 0.977424, Accuracy: 74.83%\n",
      "Batch 182, Loss: 0.942007, Accuracy: 74.85%\n",
      "Batch 183, Loss: 1.035595, Accuracy: 74.83%\n",
      "Batch 184, Loss: 1.049906, Accuracy: 74.80%\n",
      "Batch 185, Loss: 0.997309, Accuracy: 74.80%\n",
      "Batch 186, Loss: 0.975191, Accuracy: 74.79%\n",
      "Batch 187, Loss: 1.025097, Accuracy: 74.77%\n",
      "Batch 188, Loss: 0.972457, Accuracy: 74.79%\n",
      "Batch 189, Loss: 0.989014, Accuracy: 74.80%\n",
      "Batch 190, Loss: 1.009368, Accuracy: 74.79%\n",
      "Batch 191, Loss: 0.910804, Accuracy: 74.84%\n",
      "Batch 192, Loss: 0.987667, Accuracy: 74.83%\n",
      "Batch 193, Loss: 0.920603, Accuracy: 74.88%\n",
      "Batch 194, Loss: 0.898069, Accuracy: 74.94%\n",
      "Batch 195, Loss: 0.937930, Accuracy: 74.98%\n",
      "Batch 196, Loss: 0.970798, Accuracy: 74.98%\n",
      "Batch 197, Loss: 1.030535, Accuracy: 74.97%\n",
      "Batch 198, Loss: 1.028763, Accuracy: 74.94%\n",
      "Batch 199, Loss: 1.011485, Accuracy: 74.94%\n",
      "Batch 200, Loss: 1.070847, Accuracy: 74.91%\n",
      "Batch 201, Loss: 1.068674, Accuracy: 74.88%\n",
      "Batch 202, Loss: 1.024254, Accuracy: 74.85%\n",
      "Batch 203, Loss: 1.077273, Accuracy: 74.80%\n",
      "Batch 204, Loss: 1.001812, Accuracy: 74.79%\n",
      "Batch 205, Loss: 1.009936, Accuracy: 74.79%\n",
      "Batch 206, Loss: 0.961177, Accuracy: 74.82%\n",
      "Batch 207, Loss: 0.996380, Accuracy: 74.81%\n",
      "Batch 208, Loss: 0.958638, Accuracy: 74.82%\n",
      "Batch 209, Loss: 1.011261, Accuracy: 74.81%\n",
      "Batch 210, Loss: 0.938633, Accuracy: 74.84%\n",
      "Batch 211, Loss: 0.881394, Accuracy: 74.89%\n",
      "Batch 212, Loss: 1.005836, Accuracy: 74.88%\n",
      "Batch 213, Loss: 0.956274, Accuracy: 74.91%\n",
      "Training - Epoch 65, Loss: 0.991364, Accuracy: 74.91%\n",
      "Validation Batch 1, Loss: 0.985951, Accuracy: 75.00%\n",
      "Validation Batch 2, Loss: 1.061970, Accuracy: 72.66%\n",
      "Validation Batch 3, Loss: 1.080182, Accuracy: 70.31%\n",
      "Validation Batch 4, Loss: 1.026214, Accuracy: 69.92%\n",
      "Validation Batch 5, Loss: 1.002393, Accuracy: 70.62%\n",
      "Validation Batch 6, Loss: 0.960106, Accuracy: 71.88%\n",
      "Validation Batch 7, Loss: 1.034403, Accuracy: 71.88%\n",
      "Validation Batch 8, Loss: 1.059358, Accuracy: 71.29%\n",
      "Validation Batch 9, Loss: 1.080400, Accuracy: 70.66%\n",
      "Validation Batch 10, Loss: 1.045939, Accuracy: 70.47%\n",
      "Validation Batch 11, Loss: 1.010256, Accuracy: 70.60%\n",
      "Validation Batch 12, Loss: 0.968398, Accuracy: 71.22%\n",
      "Validation Batch 13, Loss: 1.106291, Accuracy: 70.67%\n",
      "Validation Batch 14, Loss: 1.048166, Accuracy: 70.65%\n",
      "Validation Batch 15, Loss: 1.011082, Accuracy: 70.83%\n",
      "Validation Batch 16, Loss: 0.996421, Accuracy: 71.19%\n",
      "Validation Batch 17, Loss: 1.055794, Accuracy: 71.23%\n",
      "Validation Batch 18, Loss: 1.007859, Accuracy: 71.44%\n",
      "Validation Batch 19, Loss: 1.074276, Accuracy: 71.30%\n",
      "Validation Batch 20, Loss: 1.039057, Accuracy: 71.25%\n",
      "Validation Batch 21, Loss: 1.029041, Accuracy: 71.21%\n",
      "Validation Batch 22, Loss: 1.067754, Accuracy: 71.02%\n",
      "Validation Batch 23, Loss: 1.121090, Accuracy: 70.58%\n",
      "Validation Batch 24, Loss: 1.087651, Accuracy: 70.31%\n",
      "Validation Batch 25, Loss: 1.011661, Accuracy: 70.50%\n",
      "Validation Batch 26, Loss: 1.016196, Accuracy: 70.61%\n",
      "Validation Batch 27, Loss: 1.022171, Accuracy: 70.64%\n",
      "Validation - Epoch 65, Loss: 1.037410, Accuracy: 70.64%\n",
      "Patienceâ€”0\n",
      "Epoch 66\n",
      "Batch 1, Loss: 1.043967, Accuracy: 70.31%\n",
      "Batch 2, Loss: 0.937775, Accuracy: 75.78%\n",
      "Batch 3, Loss: 1.024696, Accuracy: 74.48%\n",
      "Batch 4, Loss: 1.153226, Accuracy: 70.70%\n",
      "Batch 5, Loss: 0.983883, Accuracy: 71.88%\n",
      "Batch 6, Loss: 0.903740, Accuracy: 73.70%\n",
      "Batch 7, Loss: 0.913980, Accuracy: 75.00%\n",
      "Batch 8, Loss: 1.033401, Accuracy: 74.41%\n",
      "Batch 9, Loss: 1.048809, Accuracy: 73.44%\n",
      "Batch 10, Loss: 1.016631, Accuracy: 73.28%\n",
      "Batch 11, Loss: 0.922611, Accuracy: 74.29%\n",
      "Batch 12, Loss: 0.957895, Accuracy: 74.61%\n",
      "Batch 13, Loss: 1.000524, Accuracy: 74.64%\n",
      "Batch 14, Loss: 1.050835, Accuracy: 74.22%\n",
      "Batch 15, Loss: 1.011544, Accuracy: 74.06%\n",
      "Batch 16, Loss: 0.909230, Accuracy: 74.71%\n",
      "Batch 17, Loss: 1.081939, Accuracy: 74.17%\n",
      "Batch 18, Loss: 1.042872, Accuracy: 73.87%\n",
      "Batch 19, Loss: 0.903276, Accuracy: 74.42%\n",
      "Batch 20, Loss: 1.005649, Accuracy: 74.53%\n",
      "Batch 21, Loss: 0.897815, Accuracy: 74.93%\n",
      "Batch 22, Loss: 1.004375, Accuracy: 74.86%\n",
      "Batch 23, Loss: 1.015694, Accuracy: 74.59%\n",
      "Batch 24, Loss: 1.071044, Accuracy: 74.28%\n",
      "Batch 25, Loss: 1.072495, Accuracy: 73.88%\n",
      "Batch 26, Loss: 0.964006, Accuracy: 73.98%\n",
      "Batch 27, Loss: 0.912667, Accuracy: 74.36%\n",
      "Batch 28, Loss: 1.105121, Accuracy: 73.94%\n",
      "Batch 29, Loss: 0.972609, Accuracy: 74.03%\n",
      "Batch 30, Loss: 0.934826, Accuracy: 74.27%\n",
      "Batch 31, Loss: 0.997011, Accuracy: 74.29%\n",
      "Batch 32, Loss: 0.949410, Accuracy: 74.41%\n",
      "Batch 33, Loss: 0.952711, Accuracy: 74.53%\n",
      "Batch 34, Loss: 1.084897, Accuracy: 74.22%\n",
      "Batch 35, Loss: 0.916403, Accuracy: 74.46%\n",
      "Batch 36, Loss: 1.029447, Accuracy: 74.35%\n",
      "Batch 37, Loss: 1.046477, Accuracy: 74.16%\n",
      "Batch 38, Loss: 0.959029, Accuracy: 74.26%\n",
      "Batch 39, Loss: 1.038864, Accuracy: 74.20%\n",
      "Batch 40, Loss: 0.961340, Accuracy: 74.30%\n",
      "Batch 41, Loss: 0.951692, Accuracy: 74.39%\n",
      "Batch 42, Loss: 1.037990, Accuracy: 74.29%\n",
      "Batch 43, Loss: 0.902876, Accuracy: 74.53%\n",
      "Batch 44, Loss: 0.958115, Accuracy: 74.57%\n",
      "Batch 45, Loss: 1.028000, Accuracy: 74.51%\n",
      "Batch 46, Loss: 0.977188, Accuracy: 74.56%\n",
      "Batch 47, Loss: 0.953895, Accuracy: 74.63%\n",
      "Batch 48, Loss: 1.014602, Accuracy: 74.61%\n",
      "Batch 49, Loss: 0.975225, Accuracy: 74.65%\n",
      "Batch 50, Loss: 0.973165, Accuracy: 74.75%\n",
      "Batch 51, Loss: 1.018755, Accuracy: 74.69%\n",
      "Batch 52, Loss: 0.957791, Accuracy: 74.79%\n",
      "Batch 53, Loss: 0.947151, Accuracy: 74.88%\n",
      "Batch 54, Loss: 0.954152, Accuracy: 74.97%\n",
      "Batch 55, Loss: 1.013517, Accuracy: 74.89%\n",
      "Batch 56, Loss: 1.052507, Accuracy: 74.78%\n",
      "Batch 57, Loss: 0.913591, Accuracy: 74.92%\n",
      "Batch 58, Loss: 0.940369, Accuracy: 75.00%\n",
      "Batch 59, Loss: 1.025635, Accuracy: 74.92%\n",
      "Batch 60, Loss: 1.037383, Accuracy: 74.87%\n",
      "Batch 61, Loss: 1.058392, Accuracy: 74.72%\n",
      "Batch 62, Loss: 1.024605, Accuracy: 74.62%\n",
      "Batch 63, Loss: 0.916832, Accuracy: 74.75%\n",
      "Batch 64, Loss: 1.011489, Accuracy: 74.71%\n",
      "Batch 65, Loss: 0.913283, Accuracy: 74.83%\n",
      "Batch 66, Loss: 0.995180, Accuracy: 74.81%\n",
      "Batch 67, Loss: 1.092751, Accuracy: 74.65%\n",
      "Batch 68, Loss: 1.043233, Accuracy: 74.56%\n",
      "Batch 69, Loss: 1.011665, Accuracy: 74.55%\n",
      "Batch 70, Loss: 0.902340, Accuracy: 74.69%\n",
      "Batch 71, Loss: 0.973657, Accuracy: 74.74%\n",
      "Batch 72, Loss: 1.036730, Accuracy: 74.67%\n",
      "Batch 73, Loss: 0.867708, Accuracy: 74.87%\n",
      "Batch 74, Loss: 0.850671, Accuracy: 75.11%\n",
      "Batch 75, Loss: 0.987883, Accuracy: 75.10%\n",
      "Batch 76, Loss: 0.887319, Accuracy: 75.25%\n",
      "Batch 77, Loss: 0.982640, Accuracy: 75.26%\n",
      "Batch 78, Loss: 0.987504, Accuracy: 75.28%\n",
      "Batch 79, Loss: 0.944952, Accuracy: 75.36%\n",
      "Batch 80, Loss: 1.019676, Accuracy: 75.31%\n",
      "Batch 81, Loss: 0.923070, Accuracy: 75.41%\n",
      "Batch 82, Loss: 0.982534, Accuracy: 75.38%\n",
      "Batch 83, Loss: 1.021799, Accuracy: 75.34%\n",
      "Batch 84, Loss: 1.025406, Accuracy: 75.32%\n",
      "Batch 85, Loss: 0.887927, Accuracy: 75.44%\n",
      "Batch 86, Loss: 0.961405, Accuracy: 75.47%\n",
      "Batch 87, Loss: 0.916939, Accuracy: 75.59%\n",
      "Batch 88, Loss: 1.012012, Accuracy: 75.57%\n",
      "Batch 89, Loss: 0.969751, Accuracy: 75.58%\n",
      "Batch 90, Loss: 0.871355, Accuracy: 75.71%\n",
      "Batch 91, Loss: 1.013785, Accuracy: 75.69%\n",
      "Batch 92, Loss: 1.017361, Accuracy: 75.68%\n",
      "Batch 93, Loss: 0.929348, Accuracy: 75.74%\n",
      "Batch 94, Loss: 0.941673, Accuracy: 75.78%\n",
      "Batch 95, Loss: 0.962320, Accuracy: 75.79%\n",
      "Batch 96, Loss: 1.046551, Accuracy: 75.73%\n",
      "Batch 97, Loss: 0.946718, Accuracy: 75.77%\n",
      "Batch 98, Loss: 1.035741, Accuracy: 75.70%\n",
      "Batch 99, Loss: 0.916137, Accuracy: 75.79%\n",
      "Batch 100, Loss: 1.067248, Accuracy: 75.69%\n",
      "Batch 101, Loss: 0.996993, Accuracy: 75.68%\n",
      "Batch 102, Loss: 0.993536, Accuracy: 75.64%\n",
      "Batch 103, Loss: 0.968439, Accuracy: 75.68%\n",
      "Batch 104, Loss: 1.026500, Accuracy: 75.66%\n",
      "Batch 105, Loss: 0.988208, Accuracy: 75.65%\n",
      "Batch 106, Loss: 0.944370, Accuracy: 75.66%\n",
      "Batch 107, Loss: 0.968433, Accuracy: 75.67%\n",
      "Batch 108, Loss: 0.923443, Accuracy: 75.75%\n",
      "Batch 109, Loss: 0.948429, Accuracy: 75.79%\n",
      "Batch 110, Loss: 0.978410, Accuracy: 75.80%\n",
      "Batch 111, Loss: 1.026164, Accuracy: 75.76%\n",
      "Batch 112, Loss: 0.966181, Accuracy: 75.77%\n",
      "Batch 113, Loss: 0.942870, Accuracy: 75.80%\n",
      "Batch 114, Loss: 1.051237, Accuracy: 75.73%\n",
      "Batch 115, Loss: 1.016003, Accuracy: 75.68%\n",
      "Batch 116, Loss: 0.958666, Accuracy: 75.71%\n",
      "Batch 117, Loss: 0.922242, Accuracy: 75.77%\n",
      "Batch 118, Loss: 0.956951, Accuracy: 75.81%\n",
      "Batch 119, Loss: 0.916936, Accuracy: 75.88%\n",
      "Batch 120, Loss: 1.003396, Accuracy: 75.86%\n",
      "Batch 121, Loss: 0.931388, Accuracy: 75.93%\n",
      "Batch 122, Loss: 1.017718, Accuracy: 75.91%\n",
      "Batch 123, Loss: 1.090741, Accuracy: 75.83%\n",
      "Batch 124, Loss: 0.988750, Accuracy: 75.82%\n",
      "Batch 125, Loss: 0.976563, Accuracy: 75.83%\n",
      "Batch 126, Loss: 0.905624, Accuracy: 75.91%\n",
      "Batch 127, Loss: 0.911940, Accuracy: 75.97%\n",
      "Batch 128, Loss: 1.047810, Accuracy: 75.94%\n",
      "Batch 129, Loss: 0.983699, Accuracy: 75.96%\n",
      "Batch 130, Loss: 1.067206, Accuracy: 75.90%\n",
      "Batch 131, Loss: 0.921857, Accuracy: 75.95%\n",
      "Batch 132, Loss: 1.003932, Accuracy: 75.94%\n",
      "Batch 133, Loss: 1.011419, Accuracy: 75.90%\n",
      "Batch 134, Loss: 0.961092, Accuracy: 75.92%\n",
      "Batch 135, Loss: 0.964725, Accuracy: 75.94%\n",
      "Batch 136, Loss: 0.932152, Accuracy: 75.98%\n",
      "Batch 137, Loss: 0.983270, Accuracy: 75.98%\n",
      "Batch 138, Loss: 0.936139, Accuracy: 76.02%\n",
      "Batch 139, Loss: 1.011393, Accuracy: 76.00%\n",
      "Batch 140, Loss: 1.065557, Accuracy: 75.94%\n",
      "Batch 141, Loss: 0.904747, Accuracy: 76.00%\n",
      "Batch 142, Loss: 0.990781, Accuracy: 75.99%\n",
      "Batch 143, Loss: 0.965348, Accuracy: 76.01%\n",
      "Batch 144, Loss: 0.966362, Accuracy: 76.02%\n",
      "Batch 145, Loss: 0.950530, Accuracy: 76.06%\n",
      "Batch 146, Loss: 1.045783, Accuracy: 76.01%\n",
      "Batch 147, Loss: 0.954447, Accuracy: 76.02%\n",
      "Batch 148, Loss: 1.037230, Accuracy: 75.97%\n",
      "Batch 149, Loss: 0.982382, Accuracy: 75.98%\n",
      "Batch 150, Loss: 0.923795, Accuracy: 76.02%\n",
      "Batch 151, Loss: 0.975554, Accuracy: 76.01%\n",
      "Batch 152, Loss: 1.059532, Accuracy: 75.96%\n",
      "Batch 153, Loss: 0.985497, Accuracy: 75.95%\n",
      "Batch 154, Loss: 0.954736, Accuracy: 75.97%\n",
      "Batch 155, Loss: 1.035089, Accuracy: 75.93%\n",
      "Batch 156, Loss: 0.970023, Accuracy: 75.94%\n",
      "Batch 157, Loss: 0.959709, Accuracy: 75.97%\n",
      "Batch 158, Loss: 0.940122, Accuracy: 75.99%\n",
      "Batch 159, Loss: 0.986835, Accuracy: 75.99%\n",
      "Batch 160, Loss: 1.021247, Accuracy: 75.98%\n",
      "Batch 161, Loss: 0.964032, Accuracy: 75.98%\n",
      "Batch 162, Loss: 1.014530, Accuracy: 75.96%\n",
      "Batch 163, Loss: 1.019928, Accuracy: 75.94%\n",
      "Batch 164, Loss: 0.924796, Accuracy: 75.97%\n",
      "Batch 165, Loss: 0.934962, Accuracy: 75.99%\n",
      "Batch 166, Loss: 1.000272, Accuracy: 75.98%\n",
      "Batch 167, Loss: 0.954076, Accuracy: 75.99%\n",
      "Batch 168, Loss: 0.976661, Accuracy: 76.00%\n",
      "Batch 169, Loss: 0.932591, Accuracy: 76.03%\n",
      "Batch 170, Loss: 0.957714, Accuracy: 76.04%\n",
      "Batch 171, Loss: 0.893116, Accuracy: 76.09%\n",
      "Batch 172, Loss: 0.998858, Accuracy: 76.08%\n",
      "Batch 173, Loss: 1.018317, Accuracy: 76.07%\n",
      "Batch 174, Loss: 1.005828, Accuracy: 76.06%\n",
      "Batch 175, Loss: 0.985904, Accuracy: 76.05%\n",
      "Batch 176, Loss: 0.945922, Accuracy: 76.08%\n",
      "Batch 177, Loss: 1.042151, Accuracy: 76.06%\n",
      "Batch 178, Loss: 0.963820, Accuracy: 76.07%\n",
      "Batch 179, Loss: 1.020067, Accuracy: 76.04%\n",
      "Batch 180, Loss: 0.973143, Accuracy: 76.05%\n",
      "Batch 181, Loss: 1.048518, Accuracy: 76.03%\n",
      "Batch 182, Loss: 1.049074, Accuracy: 76.00%\n",
      "Batch 183, Loss: 0.924994, Accuracy: 76.03%\n",
      "Batch 184, Loss: 1.002027, Accuracy: 76.02%\n",
      "Batch 185, Loss: 0.992166, Accuracy: 76.01%\n",
      "Batch 186, Loss: 1.021539, Accuracy: 75.99%\n",
      "Batch 187, Loss: 1.024370, Accuracy: 75.95%\n",
      "Batch 188, Loss: 0.887004, Accuracy: 76.01%\n",
      "Batch 189, Loss: 1.089484, Accuracy: 75.96%\n",
      "Batch 190, Loss: 1.114959, Accuracy: 75.88%\n",
      "Batch 191, Loss: 0.978189, Accuracy: 75.89%\n",
      "Batch 192, Loss: 1.038738, Accuracy: 75.86%\n",
      "Batch 193, Loss: 1.004118, Accuracy: 75.84%\n",
      "Batch 194, Loss: 0.980983, Accuracy: 75.85%\n",
      "Batch 195, Loss: 0.982641, Accuracy: 75.84%\n",
      "Batch 196, Loss: 1.038043, Accuracy: 75.81%\n",
      "Batch 197, Loss: 1.041275, Accuracy: 75.78%\n",
      "Batch 198, Loss: 0.959497, Accuracy: 75.80%\n",
      "Batch 199, Loss: 1.049585, Accuracy: 75.78%\n",
      "Batch 200, Loss: 1.031889, Accuracy: 75.76%\n",
      "Batch 201, Loss: 0.879959, Accuracy: 75.82%\n",
      "Batch 202, Loss: 1.013595, Accuracy: 75.81%\n",
      "Batch 203, Loss: 0.958081, Accuracy: 75.83%\n",
      "Batch 204, Loss: 0.978921, Accuracy: 75.83%\n",
      "Batch 205, Loss: 1.077018, Accuracy: 75.77%\n",
      "Batch 206, Loss: 0.923697, Accuracy: 75.80%\n",
      "Batch 207, Loss: 0.963086, Accuracy: 75.82%\n",
      "Batch 208, Loss: 0.977929, Accuracy: 75.83%\n",
      "Batch 209, Loss: 0.985112, Accuracy: 75.84%\n",
      "Batch 210, Loss: 0.992615, Accuracy: 75.83%\n",
      "Batch 211, Loss: 1.052981, Accuracy: 75.79%\n",
      "Batch 212, Loss: 0.965576, Accuracy: 75.80%\n",
      "Batch 213, Loss: 0.986510, Accuracy: 75.80%\n",
      "Training - Epoch 66, Loss: 0.984906, Accuracy: 75.80%\n",
      "Validation Batch 1, Loss: 0.990445, Accuracy: 75.00%\n",
      "Validation Batch 2, Loss: 1.074680, Accuracy: 71.88%\n",
      "Validation Batch 3, Loss: 1.089963, Accuracy: 69.79%\n",
      "Validation Batch 4, Loss: 1.030736, Accuracy: 69.53%\n",
      "Validation Batch 5, Loss: 1.016338, Accuracy: 70.00%\n",
      "Validation Batch 6, Loss: 0.969554, Accuracy: 70.83%\n",
      "Validation Batch 7, Loss: 1.042588, Accuracy: 70.76%\n",
      "Validation Batch 8, Loss: 1.067370, Accuracy: 70.31%\n",
      "Validation Batch 9, Loss: 1.088222, Accuracy: 69.44%\n",
      "Validation Batch 10, Loss: 1.051938, Accuracy: 69.53%\n",
      "Validation Batch 11, Loss: 1.022413, Accuracy: 69.60%\n",
      "Validation Batch 12, Loss: 0.980068, Accuracy: 70.18%\n",
      "Validation Batch 13, Loss: 1.116018, Accuracy: 69.59%\n",
      "Validation Batch 14, Loss: 1.058663, Accuracy: 69.42%\n",
      "Validation Batch 15, Loss: 1.014327, Accuracy: 69.79%\n",
      "Validation Batch 16, Loss: 1.008948, Accuracy: 70.02%\n",
      "Validation Batch 17, Loss: 1.076427, Accuracy: 69.94%\n",
      "Validation Batch 18, Loss: 1.009617, Accuracy: 70.23%\n",
      "Validation Batch 19, Loss: 1.089489, Accuracy: 69.98%\n",
      "Validation Batch 20, Loss: 1.051122, Accuracy: 69.92%\n",
      "Validation Batch 21, Loss: 1.044801, Accuracy: 69.87%\n",
      "Validation Batch 22, Loss: 1.083704, Accuracy: 69.67%\n",
      "Validation Batch 23, Loss: 1.132794, Accuracy: 69.23%\n",
      "Validation Batch 24, Loss: 1.092244, Accuracy: 69.01%\n",
      "Validation Batch 25, Loss: 1.026700, Accuracy: 69.12%\n",
      "Validation Batch 26, Loss: 1.021975, Accuracy: 69.29%\n",
      "Validation Batch 27, Loss: 1.028824, Accuracy: 69.41%\n",
      "Validation - Epoch 66, Loss: 1.047406, Accuracy: 69.41%\n",
      "Patienceâ€”1\n",
      "Epoch 67\n",
      "Batch 1, Loss: 1.095930, Accuracy: 64.06%\n",
      "Batch 2, Loss: 1.034878, Accuracy: 67.19%\n",
      "Batch 3, Loss: 1.038811, Accuracy: 68.23%\n",
      "Batch 4, Loss: 0.954481, Accuracy: 70.31%\n",
      "Batch 5, Loss: 1.046062, Accuracy: 70.31%\n",
      "Batch 6, Loss: 0.880742, Accuracy: 73.44%\n",
      "Batch 7, Loss: 1.019367, Accuracy: 73.44%\n",
      "Batch 8, Loss: 1.005106, Accuracy: 73.44%\n",
      "Batch 9, Loss: 0.980960, Accuracy: 73.44%\n",
      "Batch 10, Loss: 0.958166, Accuracy: 74.06%\n",
      "Batch 11, Loss: 0.933346, Accuracy: 74.86%\n",
      "Batch 12, Loss: 0.913682, Accuracy: 75.65%\n",
      "Batch 13, Loss: 1.135368, Accuracy: 74.40%\n",
      "Batch 14, Loss: 1.094347, Accuracy: 73.55%\n",
      "Batch 15, Loss: 0.974197, Accuracy: 73.85%\n",
      "Batch 16, Loss: 0.935870, Accuracy: 74.32%\n",
      "Batch 17, Loss: 0.939474, Accuracy: 74.63%\n",
      "Batch 18, Loss: 0.979162, Accuracy: 74.65%\n",
      "Batch 19, Loss: 1.044347, Accuracy: 74.42%\n",
      "Batch 20, Loss: 0.937235, Accuracy: 74.77%\n",
      "Batch 21, Loss: 1.054126, Accuracy: 74.48%\n",
      "Batch 22, Loss: 0.924741, Accuracy: 74.79%\n",
      "Batch 23, Loss: 0.977228, Accuracy: 74.80%\n",
      "Batch 24, Loss: 0.952360, Accuracy: 74.87%\n",
      "Batch 25, Loss: 0.990680, Accuracy: 74.94%\n",
      "Batch 26, Loss: 0.974323, Accuracy: 75.06%\n",
      "Batch 27, Loss: 1.018914, Accuracy: 74.88%\n",
      "Batch 28, Loss: 1.002120, Accuracy: 74.78%\n",
      "Batch 29, Loss: 0.951467, Accuracy: 74.89%\n",
      "Batch 30, Loss: 1.013980, Accuracy: 74.79%\n",
      "Batch 31, Loss: 0.918985, Accuracy: 75.05%\n",
      "Batch 32, Loss: 0.993160, Accuracy: 75.00%\n",
      "Batch 33, Loss: 1.050592, Accuracy: 74.81%\n",
      "Batch 34, Loss: 0.953817, Accuracy: 74.86%\n",
      "Batch 35, Loss: 0.980906, Accuracy: 74.91%\n",
      "Batch 36, Loss: 0.936895, Accuracy: 75.13%\n",
      "Batch 37, Loss: 0.949546, Accuracy: 75.25%\n",
      "Batch 38, Loss: 1.030379, Accuracy: 75.12%\n",
      "Batch 39, Loss: 0.993405, Accuracy: 75.12%\n",
      "Batch 40, Loss: 0.994525, Accuracy: 75.16%\n",
      "Batch 41, Loss: 1.030640, Accuracy: 75.00%\n",
      "Batch 42, Loss: 0.924440, Accuracy: 75.19%\n",
      "Batch 43, Loss: 0.938646, Accuracy: 75.33%\n",
      "Batch 44, Loss: 1.018285, Accuracy: 75.32%\n",
      "Batch 45, Loss: 0.974416, Accuracy: 75.38%\n",
      "Batch 46, Loss: 1.044971, Accuracy: 75.20%\n",
      "Batch 47, Loss: 1.014704, Accuracy: 75.17%\n",
      "Batch 48, Loss: 0.997337, Accuracy: 75.13%\n",
      "Batch 49, Loss: 0.956654, Accuracy: 75.22%\n",
      "Batch 50, Loss: 0.903466, Accuracy: 75.41%\n",
      "Batch 51, Loss: 0.959743, Accuracy: 75.46%\n",
      "Batch 52, Loss: 1.026320, Accuracy: 75.36%\n",
      "Batch 53, Loss: 1.011263, Accuracy: 75.32%\n",
      "Batch 54, Loss: 0.942754, Accuracy: 75.43%\n",
      "Batch 55, Loss: 1.011281, Accuracy: 75.40%\n",
      "Batch 56, Loss: 1.023042, Accuracy: 75.33%\n",
      "Batch 57, Loss: 0.936205, Accuracy: 75.47%\n",
      "Batch 58, Loss: 1.015000, Accuracy: 75.40%\n",
      "Batch 59, Loss: 0.912889, Accuracy: 75.56%\n",
      "Batch 60, Loss: 0.917187, Accuracy: 75.70%\n",
      "Batch 61, Loss: 1.010161, Accuracy: 75.61%\n",
      "Batch 62, Loss: 0.913682, Accuracy: 75.76%\n",
      "Batch 63, Loss: 1.068437, Accuracy: 75.57%\n",
      "Batch 64, Loss: 0.968432, Accuracy: 75.61%\n",
      "Batch 65, Loss: 1.033352, Accuracy: 75.55%\n",
      "Batch 66, Loss: 1.053865, Accuracy: 75.45%\n",
      "Batch 67, Loss: 1.051741, Accuracy: 75.35%\n",
      "Batch 68, Loss: 1.018587, Accuracy: 75.30%\n",
      "Batch 69, Loss: 0.977578, Accuracy: 75.32%\n",
      "Batch 70, Loss: 1.042136, Accuracy: 75.25%\n",
      "Batch 71, Loss: 1.006711, Accuracy: 75.24%\n",
      "Batch 72, Loss: 0.970671, Accuracy: 75.28%\n",
      "Batch 73, Loss: 1.027608, Accuracy: 75.21%\n",
      "Batch 74, Loss: 0.953786, Accuracy: 75.25%\n",
      "Batch 75, Loss: 1.038972, Accuracy: 75.19%\n",
      "Batch 76, Loss: 0.982907, Accuracy: 75.19%\n",
      "Batch 77, Loss: 1.026242, Accuracy: 75.14%\n",
      "Batch 78, Loss: 0.967957, Accuracy: 75.16%\n",
      "Batch 79, Loss: 0.986939, Accuracy: 75.16%\n",
      "Batch 80, Loss: 0.876249, Accuracy: 75.31%\n",
      "Batch 81, Loss: 1.028566, Accuracy: 75.25%\n",
      "Batch 82, Loss: 1.029377, Accuracy: 75.17%\n",
      "Batch 83, Loss: 0.913906, Accuracy: 75.24%\n",
      "Batch 84, Loss: 0.983019, Accuracy: 75.26%\n",
      "Batch 85, Loss: 1.101058, Accuracy: 75.11%\n",
      "Batch 86, Loss: 0.902483, Accuracy: 75.22%\n",
      "Batch 87, Loss: 0.993943, Accuracy: 75.23%\n",
      "Batch 88, Loss: 1.010405, Accuracy: 75.20%\n",
      "Batch 89, Loss: 1.013202, Accuracy: 75.16%\n",
      "Batch 90, Loss: 0.929937, Accuracy: 75.21%\n",
      "Batch 91, Loss: 0.929776, Accuracy: 75.31%\n",
      "Batch 92, Loss: 0.917030, Accuracy: 75.41%\n",
      "Batch 93, Loss: 0.974249, Accuracy: 75.42%\n",
      "Batch 94, Loss: 0.927486, Accuracy: 75.48%\n",
      "Batch 95, Loss: 1.031449, Accuracy: 75.41%\n",
      "Batch 96, Loss: 1.018456, Accuracy: 75.37%\n",
      "Batch 97, Loss: 1.032705, Accuracy: 75.31%\n",
      "Batch 98, Loss: 0.931126, Accuracy: 75.40%\n",
      "Batch 99, Loss: 0.941852, Accuracy: 75.47%\n",
      "Batch 100, Loss: 0.987034, Accuracy: 75.48%\n",
      "Batch 101, Loss: 0.955920, Accuracy: 75.50%\n",
      "Batch 102, Loss: 1.075491, Accuracy: 75.40%\n",
      "Batch 103, Loss: 0.897936, Accuracy: 75.47%\n",
      "Batch 104, Loss: 0.993704, Accuracy: 75.48%\n",
      "Batch 105, Loss: 0.969173, Accuracy: 75.51%\n",
      "Batch 106, Loss: 1.081276, Accuracy: 75.41%\n",
      "Batch 107, Loss: 1.020101, Accuracy: 75.38%\n",
      "Batch 108, Loss: 0.957520, Accuracy: 75.41%\n",
      "Batch 109, Loss: 0.925719, Accuracy: 75.49%\n",
      "Batch 110, Loss: 0.946195, Accuracy: 75.53%\n",
      "Batch 111, Loss: 0.980998, Accuracy: 75.52%\n",
      "Batch 112, Loss: 0.940560, Accuracy: 75.56%\n",
      "Batch 113, Loss: 0.976042, Accuracy: 75.55%\n",
      "Batch 114, Loss: 1.014742, Accuracy: 75.52%\n",
      "Batch 115, Loss: 0.962139, Accuracy: 75.54%\n",
      "Batch 116, Loss: 0.930473, Accuracy: 75.59%\n",
      "Batch 117, Loss: 1.004608, Accuracy: 75.56%\n",
      "Batch 118, Loss: 0.937305, Accuracy: 75.60%\n",
      "Batch 119, Loss: 1.059107, Accuracy: 75.53%\n",
      "Batch 120, Loss: 1.062072, Accuracy: 75.46%\n",
      "Batch 121, Loss: 0.974913, Accuracy: 75.46%\n",
      "Batch 122, Loss: 0.958325, Accuracy: 75.50%\n",
      "Batch 123, Loss: 0.969073, Accuracy: 75.52%\n",
      "Batch 124, Loss: 0.974932, Accuracy: 75.52%\n",
      "Batch 125, Loss: 1.015373, Accuracy: 75.49%\n",
      "Batch 126, Loss: 0.979079, Accuracy: 75.50%\n",
      "Batch 127, Loss: 0.943294, Accuracy: 75.53%\n",
      "Batch 128, Loss: 1.005755, Accuracy: 75.52%\n",
      "Batch 129, Loss: 0.945088, Accuracy: 75.56%\n",
      "Batch 130, Loss: 0.952349, Accuracy: 75.59%\n",
      "Batch 131, Loss: 0.972763, Accuracy: 75.62%\n",
      "Batch 132, Loss: 0.927536, Accuracy: 75.65%\n",
      "Batch 133, Loss: 1.037073, Accuracy: 75.60%\n",
      "Batch 134, Loss: 0.932353, Accuracy: 75.63%\n",
      "Batch 135, Loss: 1.004877, Accuracy: 75.62%\n",
      "Batch 136, Loss: 0.908958, Accuracy: 75.69%\n",
      "Batch 137, Loss: 1.104179, Accuracy: 75.59%\n",
      "Batch 138, Loss: 1.042634, Accuracy: 75.54%\n",
      "Batch 139, Loss: 1.031453, Accuracy: 75.52%\n",
      "Batch 140, Loss: 0.927894, Accuracy: 75.57%\n",
      "Batch 141, Loss: 0.982405, Accuracy: 75.58%\n",
      "Batch 142, Loss: 1.014107, Accuracy: 75.55%\n",
      "Batch 143, Loss: 1.065162, Accuracy: 75.50%\n",
      "Batch 144, Loss: 0.965522, Accuracy: 75.53%\n",
      "Batch 145, Loss: 1.013645, Accuracy: 75.52%\n",
      "Batch 146, Loss: 0.942929, Accuracy: 75.54%\n",
      "Batch 147, Loss: 1.010675, Accuracy: 75.52%\n",
      "Batch 148, Loss: 0.964538, Accuracy: 75.55%\n",
      "Batch 149, Loss: 0.970217, Accuracy: 75.58%\n",
      "Batch 150, Loss: 0.941396, Accuracy: 75.61%\n",
      "Batch 151, Loss: 1.014327, Accuracy: 75.60%\n",
      "Batch 152, Loss: 0.976726, Accuracy: 75.61%\n",
      "Batch 153, Loss: 0.990369, Accuracy: 75.59%\n",
      "Batch 154, Loss: 0.887900, Accuracy: 75.66%\n",
      "Batch 155, Loss: 0.887467, Accuracy: 75.74%\n",
      "Batch 156, Loss: 0.939233, Accuracy: 75.76%\n",
      "Batch 157, Loss: 0.947913, Accuracy: 75.78%\n",
      "Batch 158, Loss: 0.922712, Accuracy: 75.82%\n",
      "Batch 159, Loss: 0.942676, Accuracy: 75.85%\n",
      "Batch 160, Loss: 1.001745, Accuracy: 75.83%\n",
      "Batch 161, Loss: 1.017534, Accuracy: 75.79%\n",
      "Batch 162, Loss: 0.901834, Accuracy: 75.83%\n",
      "Batch 163, Loss: 1.006929, Accuracy: 75.83%\n",
      "Batch 164, Loss: 1.049784, Accuracy: 75.78%\n",
      "Batch 165, Loss: 1.021259, Accuracy: 75.75%\n",
      "Batch 166, Loss: 1.019184, Accuracy: 75.72%\n",
      "Batch 167, Loss: 0.997365, Accuracy: 75.72%\n",
      "Batch 168, Loss: 0.968916, Accuracy: 75.73%\n",
      "Batch 169, Loss: 0.943743, Accuracy: 75.76%\n",
      "Batch 170, Loss: 0.997224, Accuracy: 75.74%\n",
      "Batch 171, Loss: 0.964554, Accuracy: 75.76%\n",
      "Batch 172, Loss: 0.987746, Accuracy: 75.74%\n",
      "Batch 173, Loss: 0.999991, Accuracy: 75.75%\n",
      "Batch 174, Loss: 1.022188, Accuracy: 75.72%\n",
      "Batch 175, Loss: 1.024922, Accuracy: 75.69%\n",
      "Batch 176, Loss: 1.040190, Accuracy: 75.66%\n",
      "Batch 177, Loss: 0.928353, Accuracy: 75.69%\n",
      "Batch 178, Loss: 0.937209, Accuracy: 75.72%\n",
      "Batch 179, Loss: 0.998953, Accuracy: 75.71%\n",
      "Batch 180, Loss: 1.007266, Accuracy: 75.69%\n",
      "Batch 181, Loss: 0.951918, Accuracy: 75.72%\n",
      "Batch 182, Loss: 0.979956, Accuracy: 75.72%\n",
      "Batch 183, Loss: 0.944061, Accuracy: 75.74%\n",
      "Batch 184, Loss: 1.153911, Accuracy: 75.65%\n",
      "Batch 185, Loss: 0.935873, Accuracy: 75.67%\n",
      "Batch 186, Loss: 0.877593, Accuracy: 75.74%\n",
      "Batch 187, Loss: 0.970638, Accuracy: 75.74%\n",
      "Batch 188, Loss: 0.953713, Accuracy: 75.76%\n",
      "Batch 189, Loss: 1.034554, Accuracy: 75.73%\n",
      "Batch 190, Loss: 1.031926, Accuracy: 75.70%\n",
      "Batch 191, Loss: 0.964213, Accuracy: 75.72%\n",
      "Batch 192, Loss: 1.004622, Accuracy: 75.72%\n",
      "Batch 193, Loss: 0.958465, Accuracy: 75.74%\n",
      "Batch 194, Loss: 0.967591, Accuracy: 75.76%\n",
      "Batch 195, Loss: 0.913351, Accuracy: 75.79%\n",
      "Batch 196, Loss: 0.916341, Accuracy: 75.83%\n",
      "Batch 197, Loss: 0.968637, Accuracy: 75.83%\n",
      "Batch 198, Loss: 0.974960, Accuracy: 75.84%\n",
      "Batch 199, Loss: 0.898086, Accuracy: 75.89%\n",
      "Batch 200, Loss: 0.961128, Accuracy: 75.89%\n",
      "Batch 201, Loss: 0.945000, Accuracy: 75.91%\n",
      "Batch 202, Loss: 0.985291, Accuracy: 75.91%\n",
      "Batch 203, Loss: 1.070285, Accuracy: 75.85%\n",
      "Batch 204, Loss: 1.060912, Accuracy: 75.82%\n",
      "Batch 205, Loss: 1.026493, Accuracy: 75.79%\n",
      "Batch 206, Loss: 1.084256, Accuracy: 75.72%\n",
      "Batch 207, Loss: 0.893747, Accuracy: 75.76%\n",
      "Batch 208, Loss: 1.028036, Accuracy: 75.74%\n",
      "Batch 209, Loss: 0.973639, Accuracy: 75.74%\n",
      "Batch 210, Loss: 0.946355, Accuracy: 75.77%\n",
      "Batch 211, Loss: 0.990939, Accuracy: 75.77%\n",
      "Batch 212, Loss: 0.946024, Accuracy: 75.79%\n",
      "Batch 213, Loss: 1.007209, Accuracy: 75.77%\n",
      "Training - Epoch 67, Loss: 0.983716, Accuracy: 75.77%\n",
      "Validation Batch 1, Loss: 0.999558, Accuracy: 75.00%\n",
      "Validation Batch 2, Loss: 1.094068, Accuracy: 71.09%\n",
      "Validation Batch 3, Loss: 1.099391, Accuracy: 68.75%\n",
      "Validation Batch 4, Loss: 1.041096, Accuracy: 68.36%\n",
      "Validation Batch 5, Loss: 1.033897, Accuracy: 69.38%\n",
      "Validation Batch 6, Loss: 0.981534, Accuracy: 70.57%\n",
      "Validation Batch 7, Loss: 1.052672, Accuracy: 70.54%\n",
      "Validation Batch 8, Loss: 1.071828, Accuracy: 70.12%\n",
      "Validation Batch 9, Loss: 1.095397, Accuracy: 69.10%\n",
      "Validation Batch 10, Loss: 1.063053, Accuracy: 69.06%\n",
      "Validation Batch 11, Loss: 1.030864, Accuracy: 69.03%\n",
      "Validation Batch 12, Loss: 0.984813, Accuracy: 69.53%\n",
      "Validation Batch 13, Loss: 1.120465, Accuracy: 68.99%\n",
      "Validation Batch 14, Loss: 1.063880, Accuracy: 68.97%\n",
      "Validation Batch 15, Loss: 1.021595, Accuracy: 69.38%\n",
      "Validation Batch 16, Loss: 1.024018, Accuracy: 69.43%\n",
      "Validation Batch 17, Loss: 1.084799, Accuracy: 69.21%\n",
      "Validation Batch 18, Loss: 1.016176, Accuracy: 69.53%\n",
      "Validation Batch 19, Loss: 1.111851, Accuracy: 69.08%\n",
      "Validation Batch 20, Loss: 1.062051, Accuracy: 68.91%\n",
      "Validation Batch 21, Loss: 1.055508, Accuracy: 68.90%\n",
      "Validation Batch 22, Loss: 1.099831, Accuracy: 68.68%\n",
      "Validation Batch 23, Loss: 1.141499, Accuracy: 68.34%\n",
      "Validation Batch 24, Loss: 1.095890, Accuracy: 68.16%\n",
      "Validation Batch 25, Loss: 1.040703, Accuracy: 68.12%\n",
      "Validation Batch 26, Loss: 1.021623, Accuracy: 68.33%\n",
      "Validation Batch 27, Loss: 1.039683, Accuracy: 68.47%\n",
      "Validation - Epoch 67, Loss: 1.057324, Accuracy: 68.47%\n",
      "Patienceâ€”2\n",
      "Epoch 68\n",
      "Batch 1, Loss: 0.995572, Accuracy: 75.00%\n",
      "Batch 2, Loss: 0.976494, Accuracy: 75.00%\n",
      "Batch 3, Loss: 1.071616, Accuracy: 72.40%\n",
      "Batch 4, Loss: 0.940003, Accuracy: 74.22%\n",
      "Batch 5, Loss: 0.994941, Accuracy: 74.69%\n",
      "Batch 6, Loss: 0.933630, Accuracy: 75.78%\n",
      "Batch 7, Loss: 1.010346, Accuracy: 75.45%\n",
      "Batch 8, Loss: 0.902084, Accuracy: 76.76%\n",
      "Batch 9, Loss: 0.943636, Accuracy: 77.26%\n",
      "Batch 10, Loss: 1.025685, Accuracy: 76.56%\n",
      "Batch 11, Loss: 0.994533, Accuracy: 76.42%\n",
      "Batch 12, Loss: 1.022145, Accuracy: 75.91%\n",
      "Batch 13, Loss: 0.983324, Accuracy: 76.08%\n",
      "Batch 14, Loss: 0.962782, Accuracy: 76.34%\n",
      "Batch 15, Loss: 0.986201, Accuracy: 76.35%\n",
      "Batch 16, Loss: 0.967990, Accuracy: 76.46%\n",
      "Batch 17, Loss: 1.011431, Accuracy: 76.29%\n",
      "Batch 18, Loss: 1.033701, Accuracy: 75.95%\n",
      "Batch 19, Loss: 1.046636, Accuracy: 75.66%\n",
      "Batch 20, Loss: 0.964112, Accuracy: 75.86%\n",
      "Batch 21, Loss: 0.960064, Accuracy: 75.97%\n",
      "Batch 22, Loss: 0.991632, Accuracy: 75.92%\n",
      "Batch 23, Loss: 1.030314, Accuracy: 75.68%\n",
      "Batch 24, Loss: 0.930760, Accuracy: 75.85%\n",
      "Batch 25, Loss: 0.921440, Accuracy: 76.12%\n",
      "Batch 26, Loss: 0.989692, Accuracy: 76.02%\n",
      "Batch 27, Loss: 0.963577, Accuracy: 76.10%\n",
      "Batch 28, Loss: 1.018816, Accuracy: 75.95%\n",
      "Batch 29, Loss: 1.007366, Accuracy: 75.86%\n",
      "Batch 30, Loss: 1.009068, Accuracy: 75.73%\n",
      "Batch 31, Loss: 1.035164, Accuracy: 75.66%\n",
      "Batch 32, Loss: 0.949559, Accuracy: 75.83%\n",
      "Batch 33, Loss: 1.009130, Accuracy: 75.71%\n",
      "Batch 34, Loss: 1.000709, Accuracy: 75.69%\n",
      "Batch 35, Loss: 1.106238, Accuracy: 75.36%\n",
      "Batch 36, Loss: 0.998655, Accuracy: 75.35%\n",
      "Batch 37, Loss: 0.923183, Accuracy: 75.51%\n",
      "Batch 38, Loss: 0.898029, Accuracy: 75.74%\n",
      "Batch 39, Loss: 0.952806, Accuracy: 75.80%\n",
      "Batch 40, Loss: 0.894839, Accuracy: 75.98%\n",
      "Batch 41, Loss: 1.020713, Accuracy: 75.84%\n",
      "Batch 42, Loss: 1.024959, Accuracy: 75.74%\n",
      "Batch 43, Loss: 0.969992, Accuracy: 75.80%\n",
      "Batch 44, Loss: 0.997596, Accuracy: 75.78%\n",
      "Batch 45, Loss: 0.974658, Accuracy: 75.76%\n",
      "Batch 46, Loss: 1.049593, Accuracy: 75.54%\n",
      "Batch 47, Loss: 0.915074, Accuracy: 75.70%\n",
      "Batch 48, Loss: 1.068418, Accuracy: 75.49%\n",
      "Batch 49, Loss: 1.038550, Accuracy: 75.29%\n",
      "Batch 50, Loss: 0.952154, Accuracy: 75.38%\n",
      "Batch 51, Loss: 1.053027, Accuracy: 75.18%\n",
      "Batch 52, Loss: 0.969134, Accuracy: 75.21%\n",
      "Batch 53, Loss: 1.009764, Accuracy: 75.18%\n",
      "Batch 54, Loss: 1.048135, Accuracy: 75.00%\n",
      "Batch 55, Loss: 0.972260, Accuracy: 75.03%\n",
      "Batch 56, Loss: 0.977595, Accuracy: 75.06%\n",
      "Batch 57, Loss: 1.015959, Accuracy: 75.05%\n",
      "Batch 58, Loss: 0.955657, Accuracy: 75.11%\n",
      "Batch 59, Loss: 0.983953, Accuracy: 75.08%\n",
      "Batch 60, Loss: 0.915266, Accuracy: 75.23%\n",
      "Batch 61, Loss: 0.976541, Accuracy: 75.28%\n",
      "Batch 62, Loss: 1.052841, Accuracy: 75.18%\n",
      "Batch 63, Loss: 0.950896, Accuracy: 75.25%\n",
      "Batch 64, Loss: 0.920474, Accuracy: 75.37%\n",
      "Batch 65, Loss: 0.902278, Accuracy: 75.50%\n",
      "Batch 66, Loss: 0.927483, Accuracy: 75.62%\n",
      "Batch 67, Loss: 1.024228, Accuracy: 75.56%\n",
      "Batch 68, Loss: 1.051531, Accuracy: 75.44%\n",
      "Batch 69, Loss: 0.954569, Accuracy: 75.50%\n",
      "Batch 70, Loss: 1.059658, Accuracy: 75.40%\n",
      "Batch 71, Loss: 0.919495, Accuracy: 75.53%\n",
      "Batch 72, Loss: 0.922037, Accuracy: 75.61%\n",
      "Batch 73, Loss: 0.984824, Accuracy: 75.58%\n",
      "Batch 74, Loss: 0.963563, Accuracy: 75.61%\n",
      "Batch 75, Loss: 0.969082, Accuracy: 75.65%\n",
      "Batch 76, Loss: 0.953895, Accuracy: 75.68%\n",
      "Batch 77, Loss: 1.002863, Accuracy: 75.65%\n",
      "Batch 78, Loss: 0.930702, Accuracy: 75.72%\n",
      "Batch 79, Loss: 1.040172, Accuracy: 75.59%\n",
      "Batch 80, Loss: 0.985560, Accuracy: 75.59%\n",
      "Batch 81, Loss: 0.999568, Accuracy: 75.56%\n",
      "Batch 82, Loss: 0.984523, Accuracy: 75.57%\n",
      "Batch 83, Loss: 1.108544, Accuracy: 75.40%\n",
      "Batch 84, Loss: 0.923165, Accuracy: 75.52%\n",
      "Batch 85, Loss: 0.994446, Accuracy: 75.50%\n",
      "Batch 86, Loss: 1.024867, Accuracy: 75.44%\n",
      "Batch 87, Loss: 0.997145, Accuracy: 75.41%\n",
      "Batch 88, Loss: 1.018177, Accuracy: 75.37%\n",
      "Batch 89, Loss: 0.979228, Accuracy: 75.39%\n",
      "Batch 90, Loss: 0.952244, Accuracy: 75.43%\n",
      "Batch 91, Loss: 0.993224, Accuracy: 75.43%\n",
      "Batch 92, Loss: 1.001061, Accuracy: 75.42%\n",
      "Batch 93, Loss: 0.975843, Accuracy: 75.47%\n",
      "Batch 94, Loss: 0.969532, Accuracy: 75.50%\n",
      "Batch 95, Loss: 1.012466, Accuracy: 75.49%\n",
      "Batch 96, Loss: 1.007161, Accuracy: 75.47%\n",
      "Batch 97, Loss: 1.067268, Accuracy: 75.39%\n",
      "Batch 98, Loss: 0.953823, Accuracy: 75.40%\n",
      "Batch 99, Loss: 1.040172, Accuracy: 75.36%\n",
      "Batch 100, Loss: 0.938110, Accuracy: 75.41%\n",
      "Batch 101, Loss: 1.032053, Accuracy: 75.36%\n",
      "Batch 102, Loss: 0.933439, Accuracy: 75.40%\n",
      "Batch 103, Loss: 0.862395, Accuracy: 75.53%\n",
      "Batch 104, Loss: 1.066189, Accuracy: 75.45%\n",
      "Batch 105, Loss: 1.087614, Accuracy: 75.36%\n",
      "Batch 106, Loss: 1.016417, Accuracy: 75.34%\n",
      "Batch 107, Loss: 0.999925, Accuracy: 75.35%\n",
      "Batch 108, Loss: 0.863829, Accuracy: 75.48%\n",
      "Batch 109, Loss: 1.001562, Accuracy: 75.46%\n",
      "Batch 110, Loss: 1.011513, Accuracy: 75.45%\n",
      "Batch 111, Loss: 1.058899, Accuracy: 75.38%\n",
      "Batch 112, Loss: 0.931764, Accuracy: 75.45%\n",
      "Batch 113, Loss: 1.006034, Accuracy: 75.44%\n",
      "Batch 114, Loss: 0.968488, Accuracy: 75.45%\n",
      "Batch 115, Loss: 0.919636, Accuracy: 75.53%\n",
      "Batch 116, Loss: 1.042284, Accuracy: 75.46%\n",
      "Batch 117, Loss: 1.017900, Accuracy: 75.44%\n",
      "Batch 118, Loss: 1.025017, Accuracy: 75.42%\n",
      "Batch 119, Loss: 1.029753, Accuracy: 75.35%\n",
      "Batch 120, Loss: 0.956939, Accuracy: 75.39%\n",
      "Batch 121, Loss: 0.896681, Accuracy: 75.46%\n",
      "Batch 122, Loss: 0.967032, Accuracy: 75.49%\n",
      "Batch 123, Loss: 0.960760, Accuracy: 75.50%\n",
      "Batch 124, Loss: 0.979310, Accuracy: 75.50%\n",
      "Batch 125, Loss: 0.924751, Accuracy: 75.56%\n",
      "Batch 126, Loss: 0.948844, Accuracy: 75.60%\n",
      "Batch 127, Loss: 1.006924, Accuracy: 75.57%\n",
      "Batch 128, Loss: 1.006944, Accuracy: 75.55%\n",
      "Batch 129, Loss: 1.000702, Accuracy: 75.53%\n",
      "Batch 130, Loss: 0.866793, Accuracy: 75.64%\n",
      "Batch 131, Loss: 0.983318, Accuracy: 75.64%\n",
      "Batch 132, Loss: 1.005331, Accuracy: 75.64%\n",
      "Batch 133, Loss: 0.957045, Accuracy: 75.67%\n",
      "Batch 134, Loss: 1.137917, Accuracy: 75.55%\n",
      "Batch 135, Loss: 0.956576, Accuracy: 75.57%\n",
      "Batch 136, Loss: 0.995768, Accuracy: 75.57%\n",
      "Batch 137, Loss: 1.030708, Accuracy: 75.54%\n",
      "Batch 138, Loss: 0.914294, Accuracy: 75.59%\n",
      "Batch 139, Loss: 0.995332, Accuracy: 75.56%\n",
      "Batch 140, Loss: 1.066347, Accuracy: 75.50%\n",
      "Batch 141, Loss: 1.055718, Accuracy: 75.44%\n",
      "Batch 142, Loss: 0.966219, Accuracy: 75.44%\n",
      "Batch 143, Loss: 1.050730, Accuracy: 75.39%\n",
      "Batch 144, Loss: 1.073110, Accuracy: 75.30%\n",
      "Batch 145, Loss: 0.990866, Accuracy: 75.30%\n",
      "Batch 146, Loss: 1.062215, Accuracy: 75.25%\n",
      "Batch 147, Loss: 0.920578, Accuracy: 75.30%\n",
      "Batch 148, Loss: 0.947562, Accuracy: 75.33%\n",
      "Batch 149, Loss: 0.924947, Accuracy: 75.39%\n",
      "Batch 150, Loss: 0.978920, Accuracy: 75.41%\n",
      "Batch 151, Loss: 0.978019, Accuracy: 75.41%\n",
      "Batch 152, Loss: 0.976768, Accuracy: 75.42%\n",
      "Batch 153, Loss: 0.989578, Accuracy: 75.42%\n",
      "Batch 154, Loss: 1.020801, Accuracy: 75.40%\n",
      "Batch 155, Loss: 0.940539, Accuracy: 75.43%\n",
      "Batch 156, Loss: 0.986198, Accuracy: 75.43%\n",
      "Batch 157, Loss: 1.067816, Accuracy: 75.38%\n",
      "Batch 158, Loss: 1.036166, Accuracy: 75.35%\n",
      "Batch 159, Loss: 0.931588, Accuracy: 75.37%\n",
      "Batch 160, Loss: 0.931990, Accuracy: 75.41%\n",
      "Batch 161, Loss: 1.017472, Accuracy: 75.40%\n",
      "Batch 162, Loss: 1.032358, Accuracy: 75.37%\n",
      "Batch 163, Loss: 0.982937, Accuracy: 75.37%\n",
      "Batch 164, Loss: 0.956855, Accuracy: 75.38%\n",
      "Batch 165, Loss: 0.967042, Accuracy: 75.38%\n",
      "Batch 166, Loss: 0.970589, Accuracy: 75.40%\n",
      "Batch 167, Loss: 0.956544, Accuracy: 75.41%\n",
      "Batch 168, Loss: 1.012679, Accuracy: 75.40%\n",
      "Batch 169, Loss: 0.977369, Accuracy: 75.39%\n",
      "Batch 170, Loss: 0.942834, Accuracy: 75.41%\n",
      "Batch 171, Loss: 0.986360, Accuracy: 75.42%\n",
      "Batch 172, Loss: 0.978885, Accuracy: 75.45%\n",
      "Batch 173, Loss: 0.886488, Accuracy: 75.51%\n",
      "Batch 174, Loss: 1.051151, Accuracy: 75.47%\n",
      "Batch 175, Loss: 1.021693, Accuracy: 75.44%\n",
      "Batch 176, Loss: 0.965856, Accuracy: 75.45%\n",
      "Batch 177, Loss: 1.012560, Accuracy: 75.44%\n",
      "Batch 178, Loss: 0.985948, Accuracy: 75.44%\n",
      "Batch 179, Loss: 0.922788, Accuracy: 75.48%\n",
      "Batch 180, Loss: 0.928864, Accuracy: 75.51%\n",
      "Batch 181, Loss: 1.004817, Accuracy: 75.51%\n",
      "Batch 182, Loss: 0.902143, Accuracy: 75.56%\n",
      "Batch 183, Loss: 0.950463, Accuracy: 75.57%\n",
      "Batch 184, Loss: 0.997621, Accuracy: 75.57%\n",
      "Batch 185, Loss: 0.951514, Accuracy: 75.59%\n",
      "Batch 186, Loss: 0.919670, Accuracy: 75.63%\n",
      "Batch 187, Loss: 0.908769, Accuracy: 75.68%\n",
      "Batch 188, Loss: 1.014513, Accuracy: 75.66%\n",
      "Batch 189, Loss: 0.921924, Accuracy: 75.69%\n",
      "Batch 190, Loss: 0.941916, Accuracy: 75.72%\n",
      "Batch 191, Loss: 0.913115, Accuracy: 75.75%\n",
      "Batch 192, Loss: 0.958963, Accuracy: 75.77%\n",
      "Batch 193, Loss: 1.007785, Accuracy: 75.76%\n",
      "Batch 194, Loss: 0.942095, Accuracy: 75.79%\n",
      "Batch 195, Loss: 0.979910, Accuracy: 75.79%\n",
      "Batch 196, Loss: 1.010956, Accuracy: 75.78%\n",
      "Batch 197, Loss: 0.935556, Accuracy: 75.81%\n",
      "Batch 198, Loss: 1.033709, Accuracy: 75.78%\n",
      "Batch 199, Loss: 0.993663, Accuracy: 75.79%\n",
      "Batch 200, Loss: 1.032908, Accuracy: 75.77%\n",
      "Batch 201, Loss: 0.995658, Accuracy: 75.75%\n",
      "Batch 202, Loss: 0.996461, Accuracy: 75.75%\n",
      "Batch 203, Loss: 0.937872, Accuracy: 75.78%\n",
      "Batch 204, Loss: 0.928552, Accuracy: 75.80%\n",
      "Batch 205, Loss: 0.973664, Accuracy: 75.79%\n",
      "Batch 206, Loss: 0.987492, Accuracy: 75.79%\n",
      "Batch 207, Loss: 0.923368, Accuracy: 75.82%\n",
      "Batch 208, Loss: 0.994510, Accuracy: 75.83%\n",
      "Batch 209, Loss: 0.969125, Accuracy: 75.83%\n",
      "Batch 210, Loss: 1.119678, Accuracy: 75.77%\n",
      "Batch 211, Loss: 0.994715, Accuracy: 75.76%\n",
      "Batch 212, Loss: 1.026110, Accuracy: 75.73%\n",
      "Batch 213, Loss: 0.965718, Accuracy: 75.73%\n",
      "Training - Epoch 68, Loss: 0.984315, Accuracy: 75.73%\n",
      "Validation Batch 1, Loss: 0.983320, Accuracy: 76.56%\n",
      "Validation Batch 2, Loss: 1.063320, Accuracy: 73.44%\n",
      "Validation Batch 3, Loss: 1.079164, Accuracy: 70.83%\n",
      "Validation Batch 4, Loss: 1.027248, Accuracy: 70.31%\n",
      "Validation Batch 5, Loss: 1.014035, Accuracy: 70.62%\n",
      "Validation Batch 6, Loss: 0.964984, Accuracy: 71.61%\n",
      "Validation Batch 7, Loss: 1.036505, Accuracy: 71.65%\n",
      "Validation Batch 8, Loss: 1.062577, Accuracy: 71.48%\n",
      "Validation Batch 9, Loss: 1.084493, Accuracy: 70.66%\n",
      "Validation Batch 10, Loss: 1.048111, Accuracy: 70.47%\n",
      "Validation Batch 11, Loss: 1.008047, Accuracy: 70.60%\n",
      "Validation Batch 12, Loss: 0.970405, Accuracy: 70.96%\n",
      "Validation Batch 13, Loss: 1.109243, Accuracy: 70.43%\n",
      "Validation Batch 14, Loss: 1.047464, Accuracy: 70.42%\n",
      "Validation Batch 15, Loss: 1.011796, Accuracy: 70.62%\n",
      "Validation Batch 16, Loss: 0.999666, Accuracy: 70.70%\n",
      "Validation Batch 17, Loss: 1.058400, Accuracy: 70.68%\n",
      "Validation Batch 18, Loss: 1.000125, Accuracy: 70.92%\n",
      "Validation Batch 19, Loss: 1.081282, Accuracy: 70.81%\n",
      "Validation Batch 20, Loss: 1.041765, Accuracy: 70.62%\n",
      "Validation Batch 21, Loss: 1.033736, Accuracy: 70.68%\n",
      "Validation Batch 22, Loss: 1.066266, Accuracy: 70.53%\n",
      "Validation Batch 23, Loss: 1.123272, Accuracy: 70.18%\n",
      "Validation Batch 24, Loss: 1.087089, Accuracy: 69.92%\n",
      "Validation Batch 25, Loss: 1.007412, Accuracy: 70.00%\n",
      "Validation Batch 26, Loss: 1.014402, Accuracy: 70.13%\n",
      "Validation Batch 27, Loss: 1.011797, Accuracy: 70.23%\n",
      "Validation - Epoch 68, Loss: 1.038367, Accuracy: 70.23%\n",
      "Patienceâ€”3\n",
      "Epoch 69\n",
      "Batch 1, Loss: 0.952357, Accuracy: 78.12%\n",
      "Batch 2, Loss: 0.997687, Accuracy: 76.56%\n",
      "Batch 3, Loss: 0.978930, Accuracy: 77.60%\n",
      "Batch 4, Loss: 1.012025, Accuracy: 76.56%\n",
      "Batch 5, Loss: 1.028588, Accuracy: 75.94%\n",
      "Batch 6, Loss: 0.903546, Accuracy: 77.34%\n",
      "Batch 7, Loss: 1.012615, Accuracy: 76.79%\n",
      "Batch 8, Loss: 1.062014, Accuracy: 75.78%\n",
      "Batch 9, Loss: 0.936884, Accuracy: 76.22%\n",
      "Batch 10, Loss: 0.958372, Accuracy: 76.41%\n",
      "Batch 11, Loss: 1.002607, Accuracy: 76.28%\n",
      "Batch 12, Loss: 0.925334, Accuracy: 76.95%\n",
      "Batch 13, Loss: 1.036359, Accuracy: 76.32%\n",
      "Batch 14, Loss: 1.000110, Accuracy: 76.23%\n",
      "Batch 15, Loss: 1.026785, Accuracy: 75.94%\n",
      "Batch 16, Loss: 0.933479, Accuracy: 76.27%\n",
      "Batch 17, Loss: 0.963462, Accuracy: 76.47%\n",
      "Batch 18, Loss: 0.932901, Accuracy: 76.74%\n",
      "Batch 19, Loss: 1.000841, Accuracy: 76.56%\n",
      "Batch 20, Loss: 0.955619, Accuracy: 76.64%\n",
      "Batch 21, Loss: 0.983518, Accuracy: 76.64%\n",
      "Batch 22, Loss: 0.896000, Accuracy: 77.06%\n",
      "Batch 23, Loss: 0.918738, Accuracy: 77.24%\n",
      "Batch 24, Loss: 0.941647, Accuracy: 77.41%\n",
      "Batch 25, Loss: 1.000430, Accuracy: 77.19%\n",
      "Batch 26, Loss: 0.930589, Accuracy: 77.34%\n",
      "Batch 27, Loss: 1.008732, Accuracy: 77.20%\n",
      "Batch 28, Loss: 1.003284, Accuracy: 77.06%\n",
      "Batch 29, Loss: 0.961025, Accuracy: 77.10%\n",
      "Batch 30, Loss: 1.060803, Accuracy: 76.82%\n",
      "Batch 31, Loss: 0.964108, Accuracy: 76.86%\n",
      "Batch 32, Loss: 1.003102, Accuracy: 76.81%\n",
      "Batch 33, Loss: 0.996460, Accuracy: 76.75%\n",
      "Batch 34, Loss: 1.022386, Accuracy: 76.52%\n",
      "Batch 35, Loss: 0.954525, Accuracy: 76.52%\n",
      "Batch 36, Loss: 0.956495, Accuracy: 76.65%\n",
      "Batch 37, Loss: 1.004776, Accuracy: 76.56%\n",
      "Batch 38, Loss: 1.028435, Accuracy: 76.40%\n",
      "Batch 39, Loss: 0.992441, Accuracy: 76.40%\n",
      "Batch 40, Loss: 0.975446, Accuracy: 76.45%\n",
      "Batch 41, Loss: 0.946805, Accuracy: 76.56%\n",
      "Batch 42, Loss: 1.010574, Accuracy: 76.53%\n",
      "Batch 43, Loss: 1.110563, Accuracy: 76.16%\n",
      "Batch 44, Loss: 0.887100, Accuracy: 76.42%\n",
      "Batch 45, Loss: 0.944532, Accuracy: 76.49%\n",
      "Batch 46, Loss: 0.974514, Accuracy: 76.49%\n",
      "Batch 47, Loss: 1.039294, Accuracy: 76.36%\n",
      "Batch 48, Loss: 1.036708, Accuracy: 76.20%\n",
      "Batch 49, Loss: 1.051612, Accuracy: 76.05%\n",
      "Batch 50, Loss: 1.024602, Accuracy: 75.97%\n",
      "Batch 51, Loss: 0.954870, Accuracy: 76.04%\n",
      "Batch 52, Loss: 1.017292, Accuracy: 75.96%\n",
      "Batch 53, Loss: 0.961375, Accuracy: 76.00%\n",
      "Batch 54, Loss: 0.983914, Accuracy: 75.93%\n",
      "Batch 55, Loss: 1.018533, Accuracy: 75.88%\n",
      "Batch 56, Loss: 0.961180, Accuracy: 75.89%\n",
      "Batch 57, Loss: 0.923283, Accuracy: 75.99%\n",
      "Batch 58, Loss: 0.928638, Accuracy: 76.08%\n",
      "Batch 59, Loss: 1.015833, Accuracy: 76.01%\n",
      "Batch 60, Loss: 0.946474, Accuracy: 76.09%\n",
      "Batch 61, Loss: 0.989100, Accuracy: 76.08%\n",
      "Batch 62, Loss: 0.963946, Accuracy: 76.08%\n",
      "Batch 63, Loss: 0.933524, Accuracy: 76.17%\n",
      "Batch 64, Loss: 0.976147, Accuracy: 76.15%\n",
      "Batch 65, Loss: 1.030923, Accuracy: 76.11%\n",
      "Batch 66, Loss: 1.063799, Accuracy: 75.95%\n",
      "Batch 67, Loss: 0.969677, Accuracy: 75.96%\n",
      "Batch 68, Loss: 0.988900, Accuracy: 75.92%\n",
      "Batch 69, Loss: 0.974031, Accuracy: 75.93%\n",
      "Batch 70, Loss: 0.980016, Accuracy: 75.92%\n",
      "Batch 71, Loss: 1.027550, Accuracy: 75.88%\n",
      "Batch 72, Loss: 0.970074, Accuracy: 75.91%\n",
      "Batch 73, Loss: 1.102075, Accuracy: 75.75%\n",
      "Batch 74, Loss: 0.971621, Accuracy: 75.78%\n",
      "Batch 75, Loss: 1.057913, Accuracy: 75.69%\n",
      "Batch 76, Loss: 0.967850, Accuracy: 75.70%\n",
      "Batch 77, Loss: 0.940174, Accuracy: 75.79%\n",
      "Batch 78, Loss: 0.962281, Accuracy: 75.82%\n",
      "Batch 79, Loss: 0.986301, Accuracy: 75.81%\n",
      "Batch 80, Loss: 0.917264, Accuracy: 75.90%\n",
      "Batch 81, Loss: 0.946485, Accuracy: 75.95%\n",
      "Batch 82, Loss: 0.986821, Accuracy: 75.97%\n",
      "Batch 83, Loss: 0.923764, Accuracy: 76.05%\n",
      "Batch 84, Loss: 0.907551, Accuracy: 76.15%\n",
      "Batch 85, Loss: 0.992401, Accuracy: 76.12%\n",
      "Batch 86, Loss: 0.938265, Accuracy: 76.18%\n",
      "Batch 87, Loss: 0.909499, Accuracy: 76.28%\n",
      "Batch 88, Loss: 1.021957, Accuracy: 76.23%\n",
      "Batch 89, Loss: 0.996400, Accuracy: 76.23%\n",
      "Batch 90, Loss: 0.975957, Accuracy: 76.23%\n",
      "Batch 91, Loss: 0.999755, Accuracy: 76.20%\n",
      "Batch 92, Loss: 1.045686, Accuracy: 76.10%\n",
      "Batch 93, Loss: 0.887464, Accuracy: 76.21%\n",
      "Batch 94, Loss: 0.919934, Accuracy: 76.28%\n",
      "Batch 95, Loss: 1.029039, Accuracy: 76.23%\n",
      "Batch 96, Loss: 1.004512, Accuracy: 76.20%\n",
      "Batch 97, Loss: 1.085747, Accuracy: 76.06%\n",
      "Batch 98, Loss: 1.027971, Accuracy: 76.00%\n",
      "Batch 99, Loss: 0.927343, Accuracy: 76.06%\n",
      "Batch 100, Loss: 1.087875, Accuracy: 75.95%\n",
      "Batch 101, Loss: 1.021803, Accuracy: 75.91%\n",
      "Batch 102, Loss: 0.959186, Accuracy: 75.92%\n",
      "Batch 103, Loss: 0.902082, Accuracy: 76.02%\n",
      "Batch 104, Loss: 1.000085, Accuracy: 75.98%\n",
      "Batch 105, Loss: 1.031547, Accuracy: 75.92%\n",
      "Batch 106, Loss: 0.934767, Accuracy: 75.96%\n",
      "Batch 107, Loss: 0.924969, Accuracy: 76.02%\n",
      "Batch 108, Loss: 1.050693, Accuracy: 75.95%\n",
      "Batch 109, Loss: 1.029577, Accuracy: 75.93%\n",
      "Batch 110, Loss: 0.946886, Accuracy: 75.98%\n",
      "Batch 111, Loss: 0.979919, Accuracy: 75.99%\n",
      "Batch 112, Loss: 0.999932, Accuracy: 75.98%\n",
      "Batch 113, Loss: 1.056198, Accuracy: 75.90%\n",
      "Batch 114, Loss: 1.000131, Accuracy: 75.89%\n",
      "Batch 115, Loss: 1.010334, Accuracy: 75.87%\n",
      "Batch 116, Loss: 1.007293, Accuracy: 75.86%\n",
      "Batch 117, Loss: 1.010293, Accuracy: 75.83%\n",
      "Batch 118, Loss: 0.929495, Accuracy: 75.89%\n",
      "Batch 119, Loss: 1.022071, Accuracy: 75.87%\n",
      "Batch 120, Loss: 0.988483, Accuracy: 75.86%\n",
      "Batch 121, Loss: 0.951578, Accuracy: 75.89%\n",
      "Batch 122, Loss: 0.940220, Accuracy: 75.93%\n",
      "Batch 123, Loss: 0.995633, Accuracy: 75.93%\n",
      "Batch 124, Loss: 0.932171, Accuracy: 75.97%\n",
      "Batch 125, Loss: 1.004668, Accuracy: 75.95%\n",
      "Batch 126, Loss: 1.055134, Accuracy: 75.91%\n",
      "Batch 127, Loss: 0.919198, Accuracy: 75.95%\n",
      "Batch 128, Loss: 0.951754, Accuracy: 75.96%\n",
      "Batch 129, Loss: 0.899025, Accuracy: 76.04%\n",
      "Batch 130, Loss: 1.063822, Accuracy: 75.96%\n",
      "Batch 131, Loss: 1.043190, Accuracy: 75.91%\n",
      "Batch 132, Loss: 1.053516, Accuracy: 75.85%\n",
      "Batch 133, Loss: 0.955716, Accuracy: 75.87%\n",
      "Batch 134, Loss: 0.865460, Accuracy: 75.96%\n",
      "Batch 135, Loss: 0.928222, Accuracy: 76.00%\n",
      "Batch 136, Loss: 1.014784, Accuracy: 75.98%\n",
      "Batch 137, Loss: 0.909818, Accuracy: 76.03%\n",
      "Batch 138, Loss: 1.059212, Accuracy: 75.97%\n",
      "Batch 139, Loss: 0.899011, Accuracy: 76.03%\n",
      "Batch 140, Loss: 0.988131, Accuracy: 76.04%\n",
      "Batch 141, Loss: 1.012598, Accuracy: 76.02%\n",
      "Batch 142, Loss: 0.988785, Accuracy: 76.00%\n",
      "Batch 143, Loss: 0.907182, Accuracy: 76.06%\n",
      "Batch 144, Loss: 0.972133, Accuracy: 76.07%\n",
      "Batch 145, Loss: 0.887980, Accuracy: 76.14%\n",
      "Batch 146, Loss: 0.991380, Accuracy: 76.12%\n",
      "Batch 147, Loss: 0.912426, Accuracy: 76.17%\n",
      "Batch 148, Loss: 0.994607, Accuracy: 76.16%\n",
      "Batch 149, Loss: 1.043918, Accuracy: 76.12%\n",
      "Batch 150, Loss: 0.916895, Accuracy: 76.17%\n",
      "Batch 151, Loss: 1.026376, Accuracy: 76.14%\n",
      "Batch 152, Loss: 0.907941, Accuracy: 76.18%\n",
      "Batch 153, Loss: 1.067346, Accuracy: 76.12%\n",
      "Batch 154, Loss: 0.988659, Accuracy: 76.12%\n",
      "Batch 155, Loss: 0.936234, Accuracy: 76.16%\n",
      "Batch 156, Loss: 0.889564, Accuracy: 76.22%\n",
      "Batch 157, Loss: 1.011330, Accuracy: 76.18%\n",
      "Batch 158, Loss: 0.954246, Accuracy: 76.21%\n",
      "Batch 159, Loss: 1.050124, Accuracy: 76.16%\n",
      "Batch 160, Loss: 1.012774, Accuracy: 76.13%\n",
      "Batch 161, Loss: 1.040728, Accuracy: 76.12%\n",
      "Batch 162, Loss: 1.010077, Accuracy: 76.08%\n",
      "Batch 163, Loss: 0.991853, Accuracy: 76.07%\n",
      "Batch 164, Loss: 1.009556, Accuracy: 76.04%\n",
      "Batch 165, Loss: 0.947895, Accuracy: 76.07%\n",
      "Batch 166, Loss: 0.938364, Accuracy: 76.10%\n",
      "Batch 167, Loss: 1.028154, Accuracy: 76.07%\n",
      "Batch 168, Loss: 0.971961, Accuracy: 76.07%\n",
      "Batch 169, Loss: 0.925204, Accuracy: 76.09%\n",
      "Batch 170, Loss: 1.063372, Accuracy: 76.03%\n",
      "Batch 171, Loss: 1.048969, Accuracy: 75.99%\n",
      "Batch 172, Loss: 1.070543, Accuracy: 75.93%\n",
      "Batch 173, Loss: 0.935200, Accuracy: 75.96%\n",
      "Batch 174, Loss: 0.959247, Accuracy: 75.97%\n",
      "Batch 175, Loss: 1.079291, Accuracy: 75.92%\n",
      "Batch 176, Loss: 0.987109, Accuracy: 75.92%\n",
      "Batch 177, Loss: 1.021444, Accuracy: 75.89%\n",
      "Batch 178, Loss: 0.977542, Accuracy: 75.90%\n",
      "Batch 179, Loss: 1.023777, Accuracy: 75.86%\n",
      "Batch 180, Loss: 1.002249, Accuracy: 75.85%\n",
      "Batch 181, Loss: 0.913044, Accuracy: 75.89%\n",
      "Batch 182, Loss: 0.967075, Accuracy: 75.91%\n",
      "Batch 183, Loss: 0.932729, Accuracy: 75.94%\n",
      "Batch 184, Loss: 1.056321, Accuracy: 75.89%\n",
      "Batch 185, Loss: 1.049504, Accuracy: 75.85%\n",
      "Batch 186, Loss: 0.902125, Accuracy: 75.91%\n",
      "Batch 187, Loss: 1.001761, Accuracy: 75.90%\n",
      "Batch 188, Loss: 0.930659, Accuracy: 75.93%\n",
      "Batch 189, Loss: 0.936998, Accuracy: 75.97%\n",
      "Batch 190, Loss: 0.989584, Accuracy: 75.96%\n",
      "Batch 191, Loss: 0.972294, Accuracy: 75.97%\n",
      "Batch 192, Loss: 0.990789, Accuracy: 75.97%\n",
      "Batch 193, Loss: 0.969691, Accuracy: 75.96%\n",
      "Batch 194, Loss: 0.921986, Accuracy: 76.01%\n",
      "Batch 195, Loss: 1.017231, Accuracy: 76.00%\n",
      "Batch 196, Loss: 1.045249, Accuracy: 75.96%\n",
      "Batch 197, Loss: 0.951226, Accuracy: 75.98%\n",
      "Batch 198, Loss: 0.972921, Accuracy: 75.97%\n",
      "Batch 199, Loss: 0.992344, Accuracy: 75.97%\n",
      "Batch 200, Loss: 1.023879, Accuracy: 75.95%\n",
      "Batch 201, Loss: 1.081523, Accuracy: 75.88%\n",
      "Batch 202, Loss: 0.940695, Accuracy: 75.90%\n",
      "Batch 203, Loss: 0.995174, Accuracy: 75.89%\n",
      "Batch 204, Loss: 0.979143, Accuracy: 75.90%\n",
      "Batch 205, Loss: 1.016690, Accuracy: 75.89%\n",
      "Batch 206, Loss: 0.893442, Accuracy: 75.95%\n",
      "Batch 207, Loss: 0.925814, Accuracy: 75.97%\n",
      "Batch 208, Loss: 1.055718, Accuracy: 75.95%\n",
      "Batch 209, Loss: 0.920754, Accuracy: 75.98%\n",
      "Batch 210, Loss: 0.947238, Accuracy: 76.00%\n",
      "Batch 211, Loss: 1.049182, Accuracy: 75.97%\n",
      "Batch 212, Loss: 0.986366, Accuracy: 75.97%\n",
      "Batch 213, Loss: 0.926807, Accuracy: 76.00%\n",
      "Training - Epoch 69, Loss: 0.982844, Accuracy: 76.00%\n",
      "Validation Batch 1, Loss: 0.990669, Accuracy: 75.00%\n",
      "Validation Batch 2, Loss: 1.065612, Accuracy: 72.66%\n",
      "Validation Batch 3, Loss: 1.084355, Accuracy: 69.79%\n",
      "Validation Batch 4, Loss: 1.032380, Accuracy: 69.14%\n",
      "Validation Batch 5, Loss: 1.017550, Accuracy: 69.38%\n",
      "Validation Batch 6, Loss: 0.969594, Accuracy: 70.57%\n",
      "Validation Batch 7, Loss: 1.038201, Accuracy: 70.54%\n",
      "Validation Batch 8, Loss: 1.059351, Accuracy: 70.31%\n",
      "Validation Batch 9, Loss: 1.084062, Accuracy: 69.44%\n",
      "Validation Batch 10, Loss: 1.056062, Accuracy: 69.38%\n",
      "Validation Batch 11, Loss: 1.015994, Accuracy: 69.46%\n",
      "Validation Batch 12, Loss: 0.974218, Accuracy: 69.92%\n",
      "Validation Batch 13, Loss: 1.107305, Accuracy: 69.47%\n",
      "Validation Batch 14, Loss: 1.050514, Accuracy: 69.42%\n",
      "Validation Batch 15, Loss: 1.012658, Accuracy: 69.79%\n",
      "Validation Batch 16, Loss: 1.010713, Accuracy: 69.73%\n",
      "Validation Batch 17, Loss: 1.063401, Accuracy: 69.67%\n",
      "Validation Batch 18, Loss: 1.003783, Accuracy: 69.97%\n",
      "Validation Batch 19, Loss: 1.074189, Accuracy: 69.90%\n",
      "Validation Batch 20, Loss: 1.045931, Accuracy: 69.77%\n",
      "Validation Batch 21, Loss: 1.043583, Accuracy: 69.72%\n",
      "Validation Batch 22, Loss: 1.066381, Accuracy: 69.60%\n",
      "Validation Batch 23, Loss: 1.125507, Accuracy: 69.29%\n",
      "Validation Batch 24, Loss: 1.087670, Accuracy: 69.08%\n",
      "Validation Batch 25, Loss: 1.019873, Accuracy: 69.19%\n",
      "Validation Batch 26, Loss: 1.015706, Accuracy: 69.35%\n",
      "Validation Batch 27, Loss: 1.026674, Accuracy: 69.47%\n",
      "Validation - Epoch 69, Loss: 1.042294, Accuracy: 69.47%\n",
      "Patienceâ€”4\n",
      "Epoch 70\n",
      "Batch 1, Loss: 0.962212, Accuracy: 75.00%\n",
      "Batch 2, Loss: 0.975419, Accuracy: 75.78%\n",
      "Batch 3, Loss: 0.922348, Accuracy: 77.60%\n",
      "Batch 4, Loss: 0.930703, Accuracy: 78.52%\n",
      "Batch 5, Loss: 1.069823, Accuracy: 75.94%\n",
      "Batch 6, Loss: 1.034046, Accuracy: 75.26%\n",
      "Batch 7, Loss: 1.008844, Accuracy: 74.55%\n",
      "Batch 8, Loss: 1.001567, Accuracy: 74.61%\n",
      "Batch 9, Loss: 0.932143, Accuracy: 75.35%\n",
      "Batch 10, Loss: 1.029963, Accuracy: 74.84%\n",
      "Batch 11, Loss: 0.931023, Accuracy: 75.14%\n",
      "Batch 12, Loss: 1.001344, Accuracy: 75.13%\n",
      "Batch 13, Loss: 0.994029, Accuracy: 75.24%\n",
      "Batch 14, Loss: 1.041890, Accuracy: 74.89%\n",
      "Batch 15, Loss: 0.913490, Accuracy: 75.52%\n",
      "Batch 16, Loss: 0.978616, Accuracy: 75.49%\n",
      "Batch 17, Loss: 0.979273, Accuracy: 75.55%\n",
      "Batch 18, Loss: 0.855181, Accuracy: 76.39%\n",
      "Batch 19, Loss: 0.967735, Accuracy: 76.48%\n",
      "Batch 20, Loss: 0.975585, Accuracy: 76.48%\n",
      "Batch 21, Loss: 1.011649, Accuracy: 76.34%\n",
      "Batch 22, Loss: 1.024271, Accuracy: 76.21%\n",
      "Batch 23, Loss: 0.964274, Accuracy: 76.29%\n",
      "Batch 24, Loss: 0.957615, Accuracy: 76.43%\n",
      "Batch 25, Loss: 1.013187, Accuracy: 76.31%\n",
      "Batch 26, Loss: 1.035251, Accuracy: 76.08%\n",
      "Batch 27, Loss: 1.021294, Accuracy: 75.93%\n",
      "Batch 28, Loss: 1.028251, Accuracy: 75.73%\n",
      "Batch 29, Loss: 0.951995, Accuracy: 75.81%\n",
      "Batch 30, Loss: 0.915793, Accuracy: 76.04%\n",
      "Batch 31, Loss: 0.987564, Accuracy: 76.01%\n",
      "Batch 32, Loss: 0.922054, Accuracy: 76.17%\n",
      "Batch 33, Loss: 0.993491, Accuracy: 76.14%\n",
      "Batch 34, Loss: 0.958483, Accuracy: 76.15%\n",
      "Batch 35, Loss: 0.993909, Accuracy: 76.07%\n",
      "Batch 36, Loss: 1.031695, Accuracy: 75.82%\n",
      "Batch 37, Loss: 1.028748, Accuracy: 75.68%\n",
      "Batch 38, Loss: 1.021136, Accuracy: 75.58%\n",
      "Batch 39, Loss: 0.924267, Accuracy: 75.72%\n",
      "Batch 40, Loss: 1.035990, Accuracy: 75.62%\n",
      "Batch 41, Loss: 1.014992, Accuracy: 75.53%\n",
      "Batch 42, Loss: 0.894365, Accuracy: 75.78%\n",
      "Batch 43, Loss: 0.952152, Accuracy: 75.84%\n",
      "Batch 44, Loss: 0.981220, Accuracy: 75.89%\n",
      "Batch 45, Loss: 1.011173, Accuracy: 75.83%\n",
      "Batch 46, Loss: 1.108730, Accuracy: 75.54%\n",
      "Batch 47, Loss: 1.067746, Accuracy: 75.33%\n",
      "Batch 48, Loss: 0.980505, Accuracy: 75.39%\n",
      "Batch 49, Loss: 0.941385, Accuracy: 75.48%\n",
      "Batch 50, Loss: 0.951501, Accuracy: 75.56%\n",
      "Batch 51, Loss: 0.945468, Accuracy: 75.64%\n",
      "Batch 52, Loss: 0.991019, Accuracy: 75.63%\n",
      "Batch 53, Loss: 0.956063, Accuracy: 75.74%\n",
      "Batch 54, Loss: 0.941378, Accuracy: 75.84%\n",
      "Batch 55, Loss: 1.017959, Accuracy: 75.77%\n",
      "Batch 56, Loss: 1.026361, Accuracy: 75.70%\n",
      "Batch 57, Loss: 0.967991, Accuracy: 75.71%\n",
      "Batch 58, Loss: 0.960672, Accuracy: 75.75%\n",
      "Batch 59, Loss: 1.020907, Accuracy: 75.64%\n",
      "Batch 60, Loss: 0.970507, Accuracy: 75.68%\n",
      "Batch 61, Loss: 1.043268, Accuracy: 75.56%\n",
      "Batch 62, Loss: 1.092870, Accuracy: 75.35%\n",
      "Batch 63, Loss: 0.957754, Accuracy: 75.42%\n",
      "Batch 64, Loss: 0.911657, Accuracy: 75.51%\n",
      "Batch 65, Loss: 1.070145, Accuracy: 75.36%\n",
      "Batch 66, Loss: 0.986698, Accuracy: 75.36%\n",
      "Batch 67, Loss: 0.965005, Accuracy: 75.42%\n",
      "Batch 68, Loss: 1.016641, Accuracy: 75.32%\n",
      "Batch 69, Loss: 0.942694, Accuracy: 75.38%\n",
      "Batch 70, Loss: 1.027594, Accuracy: 75.29%\n",
      "Batch 71, Loss: 0.999420, Accuracy: 75.26%\n",
      "Batch 72, Loss: 1.011303, Accuracy: 75.22%\n",
      "Batch 73, Loss: 0.918256, Accuracy: 75.32%\n",
      "Batch 74, Loss: 0.969096, Accuracy: 75.34%\n",
      "Batch 75, Loss: 1.017758, Accuracy: 75.27%\n",
      "Batch 76, Loss: 1.148509, Accuracy: 75.04%\n",
      "Batch 77, Loss: 1.016522, Accuracy: 75.00%\n",
      "Batch 78, Loss: 1.086716, Accuracy: 74.84%\n",
      "Batch 79, Loss: 0.907029, Accuracy: 74.96%\n",
      "Batch 80, Loss: 0.932874, Accuracy: 75.02%\n",
      "Batch 81, Loss: 1.038359, Accuracy: 74.96%\n",
      "Batch 82, Loss: 1.038390, Accuracy: 74.92%\n",
      "Batch 83, Loss: 0.941427, Accuracy: 75.00%\n",
      "Batch 84, Loss: 0.938370, Accuracy: 75.07%\n",
      "Batch 85, Loss: 0.964828, Accuracy: 75.09%\n",
      "Batch 86, Loss: 0.934342, Accuracy: 75.16%\n",
      "Batch 87, Loss: 0.948260, Accuracy: 75.22%\n",
      "Batch 88, Loss: 1.047040, Accuracy: 75.14%\n",
      "Batch 89, Loss: 0.881655, Accuracy: 75.26%\n",
      "Batch 90, Loss: 0.831431, Accuracy: 75.45%\n",
      "Batch 91, Loss: 1.047401, Accuracy: 75.38%\n",
      "Batch 92, Loss: 1.023876, Accuracy: 75.32%\n",
      "Batch 93, Loss: 0.981794, Accuracy: 75.34%\n",
      "Batch 94, Loss: 0.958170, Accuracy: 75.37%\n",
      "Batch 95, Loss: 1.003863, Accuracy: 75.35%\n",
      "Batch 96, Loss: 1.042214, Accuracy: 75.29%\n",
      "Batch 97, Loss: 0.891877, Accuracy: 75.40%\n",
      "Batch 98, Loss: 1.046660, Accuracy: 75.33%\n",
      "Batch 99, Loss: 0.993049, Accuracy: 75.32%\n",
      "Batch 100, Loss: 0.987707, Accuracy: 75.30%\n",
      "Batch 101, Loss: 0.954053, Accuracy: 75.32%\n",
      "Batch 102, Loss: 0.935288, Accuracy: 75.38%\n",
      "Batch 103, Loss: 0.936365, Accuracy: 75.42%\n",
      "Batch 104, Loss: 1.029719, Accuracy: 75.39%\n",
      "Batch 105, Loss: 0.988968, Accuracy: 75.39%\n",
      "Batch 106, Loss: 1.024150, Accuracy: 75.35%\n",
      "Batch 107, Loss: 0.937611, Accuracy: 75.41%\n",
      "Batch 108, Loss: 0.959456, Accuracy: 75.42%\n",
      "Batch 109, Loss: 0.952666, Accuracy: 75.46%\n",
      "Batch 110, Loss: 0.975029, Accuracy: 75.47%\n",
      "Batch 111, Loss: 0.941080, Accuracy: 75.52%\n",
      "Batch 112, Loss: 0.973699, Accuracy: 75.52%\n",
      "Batch 113, Loss: 1.020788, Accuracy: 75.47%\n",
      "Batch 114, Loss: 0.982198, Accuracy: 75.49%\n",
      "Batch 115, Loss: 1.023577, Accuracy: 75.46%\n",
      "Batch 116, Loss: 0.956467, Accuracy: 75.47%\n",
      "Batch 117, Loss: 0.948491, Accuracy: 75.52%\n",
      "Batch 118, Loss: 0.954628, Accuracy: 75.54%\n",
      "Batch 119, Loss: 0.963029, Accuracy: 75.56%\n",
      "Batch 120, Loss: 0.970757, Accuracy: 75.55%\n",
      "Batch 121, Loss: 0.945282, Accuracy: 75.57%\n",
      "Batch 122, Loss: 0.966732, Accuracy: 75.59%\n",
      "Batch 123, Loss: 0.946584, Accuracy: 75.62%\n",
      "Batch 124, Loss: 1.035051, Accuracy: 75.58%\n",
      "Batch 125, Loss: 0.995511, Accuracy: 75.58%\n",
      "Batch 126, Loss: 0.964824, Accuracy: 75.61%\n",
      "Batch 127, Loss: 0.995226, Accuracy: 75.62%\n",
      "Batch 128, Loss: 0.916076, Accuracy: 75.66%\n",
      "Batch 129, Loss: 0.966944, Accuracy: 75.68%\n",
      "Batch 130, Loss: 0.896046, Accuracy: 75.76%\n",
      "Batch 131, Loss: 1.025070, Accuracy: 75.73%\n",
      "Batch 132, Loss: 1.024563, Accuracy: 75.70%\n",
      "Batch 133, Loss: 0.970535, Accuracy: 75.72%\n",
      "Batch 134, Loss: 1.045559, Accuracy: 75.66%\n",
      "Batch 135, Loss: 0.936569, Accuracy: 75.69%\n",
      "Batch 136, Loss: 1.001806, Accuracy: 75.69%\n",
      "Batch 137, Loss: 1.030971, Accuracy: 75.66%\n",
      "Batch 138, Loss: 1.040778, Accuracy: 75.62%\n",
      "Batch 139, Loss: 0.936673, Accuracy: 75.65%\n",
      "Batch 140, Loss: 0.968475, Accuracy: 75.66%\n",
      "Batch 141, Loss: 0.980476, Accuracy: 75.66%\n",
      "Batch 142, Loss: 0.909017, Accuracy: 75.73%\n",
      "Batch 143, Loss: 0.995369, Accuracy: 75.72%\n",
      "Batch 144, Loss: 0.982452, Accuracy: 75.73%\n",
      "Batch 145, Loss: 0.947972, Accuracy: 75.74%\n",
      "Batch 146, Loss: 0.930950, Accuracy: 75.79%\n",
      "Batch 147, Loss: 0.980012, Accuracy: 75.80%\n",
      "Batch 148, Loss: 0.938221, Accuracy: 75.83%\n",
      "Batch 149, Loss: 0.948438, Accuracy: 75.87%\n",
      "Batch 150, Loss: 0.997873, Accuracy: 75.86%\n",
      "Batch 151, Loss: 1.057365, Accuracy: 75.82%\n",
      "Batch 152, Loss: 0.995038, Accuracy: 75.81%\n",
      "Batch 153, Loss: 0.971730, Accuracy: 75.83%\n",
      "Batch 154, Loss: 0.935463, Accuracy: 75.85%\n",
      "Batch 155, Loss: 0.915788, Accuracy: 75.91%\n",
      "Batch 156, Loss: 0.915724, Accuracy: 75.95%\n",
      "Batch 157, Loss: 0.993985, Accuracy: 75.95%\n",
      "Batch 158, Loss: 1.085076, Accuracy: 75.87%\n",
      "Batch 159, Loss: 0.953258, Accuracy: 75.89%\n",
      "Batch 160, Loss: 0.972907, Accuracy: 75.90%\n",
      "Batch 161, Loss: 0.928266, Accuracy: 75.94%\n",
      "Batch 162, Loss: 1.030555, Accuracy: 75.92%\n",
      "Batch 163, Loss: 1.020378, Accuracy: 75.89%\n",
      "Batch 164, Loss: 0.994340, Accuracy: 75.88%\n",
      "Batch 165, Loss: 0.940342, Accuracy: 75.90%\n",
      "Batch 166, Loss: 0.930970, Accuracy: 75.93%\n",
      "Batch 167, Loss: 0.972777, Accuracy: 75.94%\n",
      "Batch 168, Loss: 0.991111, Accuracy: 75.95%\n",
      "Batch 169, Loss: 0.930550, Accuracy: 75.97%\n",
      "Batch 170, Loss: 0.871762, Accuracy: 76.05%\n",
      "Batch 171, Loss: 1.080365, Accuracy: 76.00%\n",
      "Batch 172, Loss: 1.017573, Accuracy: 75.98%\n",
      "Batch 173, Loss: 1.011010, Accuracy: 75.97%\n",
      "Batch 174, Loss: 1.030208, Accuracy: 75.93%\n",
      "Batch 175, Loss: 0.998684, Accuracy: 75.92%\n",
      "Batch 176, Loss: 1.023426, Accuracy: 75.90%\n",
      "Batch 177, Loss: 0.988026, Accuracy: 75.91%\n",
      "Batch 178, Loss: 0.980027, Accuracy: 75.91%\n",
      "Batch 179, Loss: 1.016622, Accuracy: 75.89%\n",
      "Batch 180, Loss: 0.916908, Accuracy: 75.94%\n",
      "Batch 181, Loss: 0.962813, Accuracy: 75.95%\n",
      "Batch 182, Loss: 0.943384, Accuracy: 75.97%\n",
      "Batch 183, Loss: 0.970739, Accuracy: 75.97%\n",
      "Batch 184, Loss: 1.035068, Accuracy: 75.94%\n",
      "Batch 185, Loss: 0.962455, Accuracy: 75.95%\n",
      "Batch 186, Loss: 0.970812, Accuracy: 75.96%\n",
      "Batch 187, Loss: 0.954140, Accuracy: 75.98%\n",
      "Batch 188, Loss: 1.046345, Accuracy: 75.94%\n",
      "Batch 189, Loss: 0.974971, Accuracy: 75.95%\n",
      "Batch 190, Loss: 1.006202, Accuracy: 75.93%\n",
      "Batch 191, Loss: 0.945150, Accuracy: 75.95%\n",
      "Batch 192, Loss: 0.892026, Accuracy: 76.02%\n",
      "Batch 193, Loss: 1.016052, Accuracy: 76.00%\n",
      "Batch 194, Loss: 1.032788, Accuracy: 75.96%\n",
      "Batch 195, Loss: 1.064904, Accuracy: 75.91%\n",
      "Batch 196, Loss: 0.960120, Accuracy: 75.92%\n",
      "Batch 197, Loss: 1.021344, Accuracy: 75.91%\n",
      "Batch 198, Loss: 0.925071, Accuracy: 75.93%\n",
      "Batch 199, Loss: 1.023821, Accuracy: 75.91%\n",
      "Batch 200, Loss: 0.921673, Accuracy: 75.94%\n",
      "Batch 201, Loss: 1.031752, Accuracy: 75.91%\n",
      "Batch 202, Loss: 1.044650, Accuracy: 75.88%\n",
      "Batch 203, Loss: 0.960268, Accuracy: 75.89%\n",
      "Batch 204, Loss: 1.012612, Accuracy: 75.88%\n",
      "Batch 205, Loss: 0.998933, Accuracy: 75.87%\n",
      "Batch 206, Loss: 1.040830, Accuracy: 75.83%\n",
      "Batch 207, Loss: 0.994657, Accuracy: 75.82%\n",
      "Batch 208, Loss: 0.953229, Accuracy: 75.83%\n",
      "Batch 209, Loss: 1.006850, Accuracy: 75.82%\n",
      "Batch 210, Loss: 0.980052, Accuracy: 75.83%\n",
      "Batch 211, Loss: 0.988884, Accuracy: 75.83%\n",
      "Batch 212, Loss: 1.013283, Accuracy: 75.82%\n",
      "Batch 213, Loss: 0.928101, Accuracy: 75.84%\n",
      "Training - Epoch 70, Loss: 0.982596, Accuracy: 75.84%\n",
      "Validation Batch 1, Loss: 0.984428, Accuracy: 78.12%\n",
      "Validation Batch 2, Loss: 1.070433, Accuracy: 72.66%\n",
      "Validation Batch 3, Loss: 1.079285, Accuracy: 69.79%\n",
      "Validation Batch 4, Loss: 1.031883, Accuracy: 69.14%\n",
      "Validation Batch 5, Loss: 1.013303, Accuracy: 69.69%\n",
      "Validation Batch 6, Loss: 0.964494, Accuracy: 70.57%\n",
      "Validation Batch 7, Loss: 1.032250, Accuracy: 70.98%\n",
      "Validation Batch 8, Loss: 1.066378, Accuracy: 70.70%\n",
      "Validation Batch 9, Loss: 1.077897, Accuracy: 70.14%\n",
      "Validation Batch 10, Loss: 1.051035, Accuracy: 70.00%\n",
      "Validation Batch 11, Loss: 1.008036, Accuracy: 70.17%\n",
      "Validation Batch 12, Loss: 0.969601, Accuracy: 70.70%\n",
      "Validation Batch 13, Loss: 1.109756, Accuracy: 70.19%\n",
      "Validation Batch 14, Loss: 1.049286, Accuracy: 70.09%\n",
      "Validation Batch 15, Loss: 1.007837, Accuracy: 70.31%\n",
      "Validation Batch 16, Loss: 1.008809, Accuracy: 70.31%\n",
      "Validation Batch 17, Loss: 1.066787, Accuracy: 70.22%\n",
      "Validation Batch 18, Loss: 1.007976, Accuracy: 70.49%\n",
      "Validation Batch 19, Loss: 1.103656, Accuracy: 69.98%\n",
      "Validation Batch 20, Loss: 1.044779, Accuracy: 69.84%\n",
      "Validation Batch 21, Loss: 1.033004, Accuracy: 69.94%\n",
      "Validation Batch 22, Loss: 1.074364, Accuracy: 69.74%\n",
      "Validation Batch 23, Loss: 1.123258, Accuracy: 69.36%\n",
      "Validation Batch 24, Loss: 1.093193, Accuracy: 69.14%\n",
      "Validation Batch 25, Loss: 1.015517, Accuracy: 69.12%\n",
      "Validation Batch 26, Loss: 1.014165, Accuracy: 69.29%\n",
      "Validation Batch 27, Loss: 1.008591, Accuracy: 69.41%\n",
      "Validation - Epoch 70, Loss: 1.041111, Accuracy: 69.41%\n",
      "Patienceâ€”5\n",
      "Epoch 71\n",
      "Batch 1, Loss: 0.999198, Accuracy: 75.00%\n",
      "Batch 2, Loss: 0.996083, Accuracy: 75.78%\n",
      "Batch 3, Loss: 0.867661, Accuracy: 79.69%\n",
      "Batch 4, Loss: 0.966347, Accuracy: 79.30%\n",
      "Batch 5, Loss: 1.025799, Accuracy: 77.50%\n",
      "Batch 6, Loss: 1.019460, Accuracy: 76.56%\n",
      "Batch 7, Loss: 0.974105, Accuracy: 76.79%\n",
      "Batch 8, Loss: 0.956216, Accuracy: 76.95%\n",
      "Batch 9, Loss: 0.954195, Accuracy: 77.43%\n",
      "Batch 10, Loss: 1.088821, Accuracy: 76.09%\n",
      "Batch 11, Loss: 0.957198, Accuracy: 76.28%\n",
      "Batch 12, Loss: 0.908864, Accuracy: 76.82%\n",
      "Batch 13, Loss: 0.950734, Accuracy: 77.04%\n",
      "Batch 14, Loss: 0.958595, Accuracy: 77.23%\n",
      "Batch 15, Loss: 0.970044, Accuracy: 77.08%\n",
      "Batch 16, Loss: 1.008470, Accuracy: 76.86%\n",
      "Batch 17, Loss: 1.005585, Accuracy: 76.75%\n",
      "Batch 18, Loss: 0.966633, Accuracy: 76.82%\n",
      "Batch 19, Loss: 0.974789, Accuracy: 76.89%\n",
      "Batch 20, Loss: 0.938855, Accuracy: 77.11%\n",
      "Batch 21, Loss: 0.991555, Accuracy: 77.01%\n",
      "Batch 22, Loss: 0.960843, Accuracy: 77.06%\n",
      "Batch 23, Loss: 0.980458, Accuracy: 77.04%\n",
      "Batch 24, Loss: 0.963528, Accuracy: 77.08%\n",
      "Batch 25, Loss: 1.004237, Accuracy: 77.00%\n",
      "Batch 26, Loss: 0.982204, Accuracy: 76.92%\n",
      "Batch 27, Loss: 0.874910, Accuracy: 77.26%\n",
      "Batch 28, Loss: 0.987477, Accuracy: 77.18%\n",
      "Batch 29, Loss: 0.979693, Accuracy: 77.16%\n",
      "Batch 30, Loss: 0.983904, Accuracy: 77.19%\n",
      "Batch 31, Loss: 1.103837, Accuracy: 76.76%\n",
      "Batch 32, Loss: 1.009395, Accuracy: 76.61%\n",
      "Batch 33, Loss: 0.921936, Accuracy: 76.80%\n",
      "Batch 34, Loss: 0.943269, Accuracy: 76.93%\n",
      "Batch 35, Loss: 0.977268, Accuracy: 76.92%\n",
      "Batch 36, Loss: 0.980459, Accuracy: 76.87%\n",
      "Batch 37, Loss: 0.990149, Accuracy: 76.82%\n",
      "Batch 38, Loss: 1.052874, Accuracy: 76.60%\n",
      "Batch 39, Loss: 0.970893, Accuracy: 76.64%\n",
      "Batch 40, Loss: 0.948511, Accuracy: 76.68%\n",
      "Batch 41, Loss: 0.912604, Accuracy: 76.87%\n",
      "Batch 42, Loss: 0.982379, Accuracy: 76.79%\n",
      "Batch 43, Loss: 1.052254, Accuracy: 76.56%\n",
      "Batch 44, Loss: 0.999050, Accuracy: 76.46%\n",
      "Batch 45, Loss: 0.958267, Accuracy: 76.46%\n",
      "Batch 46, Loss: 1.056307, Accuracy: 76.26%\n",
      "Batch 47, Loss: 0.964634, Accuracy: 76.30%\n",
      "Batch 48, Loss: 0.979543, Accuracy: 76.33%\n",
      "Batch 49, Loss: 1.032810, Accuracy: 76.18%\n",
      "Batch 50, Loss: 1.025162, Accuracy: 76.03%\n",
      "Batch 51, Loss: 0.893020, Accuracy: 76.23%\n",
      "Batch 52, Loss: 0.973360, Accuracy: 76.23%\n",
      "Batch 53, Loss: 1.034911, Accuracy: 76.12%\n",
      "Batch 54, Loss: 1.000795, Accuracy: 76.07%\n",
      "Batch 55, Loss: 0.991245, Accuracy: 76.05%\n",
      "Batch 56, Loss: 0.920466, Accuracy: 76.17%\n",
      "Batch 57, Loss: 0.958807, Accuracy: 76.23%\n",
      "Batch 58, Loss: 1.026184, Accuracy: 76.19%\n",
      "Batch 59, Loss: 1.055711, Accuracy: 76.03%\n",
      "Batch 60, Loss: 0.960007, Accuracy: 76.04%\n",
      "Batch 61, Loss: 0.953385, Accuracy: 76.08%\n",
      "Batch 62, Loss: 0.915763, Accuracy: 76.18%\n",
      "Batch 63, Loss: 0.906929, Accuracy: 76.29%\n",
      "Batch 64, Loss: 1.002595, Accuracy: 76.25%\n",
      "Batch 65, Loss: 0.941175, Accuracy: 76.32%\n",
      "Batch 66, Loss: 0.925861, Accuracy: 76.42%\n",
      "Batch 67, Loss: 0.999175, Accuracy: 76.40%\n",
      "Batch 68, Loss: 1.041218, Accuracy: 76.31%\n",
      "Batch 69, Loss: 0.993910, Accuracy: 76.29%\n",
      "Batch 70, Loss: 0.941741, Accuracy: 76.34%\n",
      "Batch 71, Loss: 1.018187, Accuracy: 76.28%\n",
      "Batch 72, Loss: 0.965386, Accuracy: 76.30%\n",
      "Batch 73, Loss: 1.037951, Accuracy: 76.22%\n",
      "Batch 74, Loss: 0.988443, Accuracy: 76.22%\n",
      "Batch 75, Loss: 1.051740, Accuracy: 76.12%\n",
      "Batch 76, Loss: 0.885088, Accuracy: 76.21%\n",
      "Batch 77, Loss: 0.878137, Accuracy: 76.36%\n",
      "Batch 78, Loss: 0.998597, Accuracy: 76.32%\n",
      "Batch 79, Loss: 0.887712, Accuracy: 76.44%\n",
      "Batch 80, Loss: 1.030599, Accuracy: 76.33%\n",
      "Batch 81, Loss: 1.002873, Accuracy: 76.31%\n",
      "Batch 82, Loss: 0.983280, Accuracy: 76.30%\n",
      "Batch 83, Loss: 0.990161, Accuracy: 76.28%\n",
      "Batch 84, Loss: 0.988354, Accuracy: 76.26%\n",
      "Batch 85, Loss: 0.932074, Accuracy: 76.32%\n",
      "Batch 86, Loss: 1.003138, Accuracy: 76.29%\n",
      "Batch 87, Loss: 0.905838, Accuracy: 76.40%\n",
      "Batch 88, Loss: 0.923472, Accuracy: 76.47%\n",
      "Batch 89, Loss: 1.024666, Accuracy: 76.42%\n",
      "Batch 90, Loss: 0.979205, Accuracy: 76.42%\n",
      "Batch 91, Loss: 1.016755, Accuracy: 76.37%\n",
      "Batch 92, Loss: 1.064973, Accuracy: 76.29%\n",
      "Batch 93, Loss: 0.926152, Accuracy: 76.34%\n",
      "Batch 94, Loss: 0.999714, Accuracy: 76.33%\n",
      "Batch 95, Loss: 1.027254, Accuracy: 76.27%\n",
      "Batch 96, Loss: 0.986597, Accuracy: 76.25%\n",
      "Batch 97, Loss: 0.959400, Accuracy: 76.26%\n",
      "Batch 98, Loss: 1.003251, Accuracy: 76.21%\n",
      "Batch 99, Loss: 1.046424, Accuracy: 76.14%\n",
      "Batch 100, Loss: 0.951898, Accuracy: 76.17%\n",
      "Batch 101, Loss: 0.967156, Accuracy: 76.18%\n",
      "Batch 102, Loss: 1.014094, Accuracy: 76.13%\n",
      "Batch 103, Loss: 1.019433, Accuracy: 76.06%\n",
      "Batch 104, Loss: 1.033750, Accuracy: 76.02%\n",
      "Batch 105, Loss: 0.956485, Accuracy: 76.04%\n",
      "Batch 106, Loss: 1.014832, Accuracy: 76.00%\n",
      "Batch 107, Loss: 0.957151, Accuracy: 76.04%\n",
      "Batch 108, Loss: 0.889095, Accuracy: 76.13%\n",
      "Batch 109, Loss: 1.026763, Accuracy: 76.08%\n",
      "Batch 110, Loss: 1.056821, Accuracy: 75.99%\n",
      "Batch 111, Loss: 1.065895, Accuracy: 75.91%\n",
      "Batch 112, Loss: 0.960261, Accuracy: 75.93%\n",
      "Batch 113, Loss: 1.121326, Accuracy: 75.82%\n",
      "Batch 114, Loss: 0.927870, Accuracy: 75.88%\n",
      "Batch 115, Loss: 0.928166, Accuracy: 75.94%\n",
      "Batch 116, Loss: 0.983794, Accuracy: 75.92%\n",
      "Batch 117, Loss: 0.948858, Accuracy: 75.96%\n",
      "Batch 118, Loss: 1.030703, Accuracy: 75.93%\n",
      "Batch 119, Loss: 0.989883, Accuracy: 75.92%\n",
      "Batch 120, Loss: 0.931172, Accuracy: 75.96%\n",
      "Batch 121, Loss: 0.973104, Accuracy: 75.97%\n",
      "Batch 122, Loss: 0.984781, Accuracy: 75.99%\n",
      "Batch 123, Loss: 0.988880, Accuracy: 75.98%\n",
      "Batch 124, Loss: 1.066584, Accuracy: 75.91%\n",
      "Batch 125, Loss: 1.011721, Accuracy: 75.89%\n",
      "Batch 126, Loss: 0.954132, Accuracy: 75.91%\n",
      "Batch 127, Loss: 0.999462, Accuracy: 75.89%\n",
      "Batch 128, Loss: 0.980437, Accuracy: 75.89%\n",
      "Batch 129, Loss: 1.010869, Accuracy: 75.87%\n",
      "Batch 130, Loss: 1.057365, Accuracy: 75.82%\n",
      "Batch 131, Loss: 1.019952, Accuracy: 75.79%\n",
      "Batch 132, Loss: 0.992068, Accuracy: 75.78%\n",
      "Batch 133, Loss: 0.973738, Accuracy: 75.80%\n",
      "Batch 134, Loss: 0.943363, Accuracy: 75.83%\n",
      "Batch 135, Loss: 0.935130, Accuracy: 75.86%\n",
      "Batch 136, Loss: 1.023436, Accuracy: 75.84%\n",
      "Batch 137, Loss: 1.007901, Accuracy: 75.83%\n",
      "Batch 138, Loss: 1.001811, Accuracy: 75.84%\n",
      "Batch 139, Loss: 0.971752, Accuracy: 75.85%\n",
      "Batch 140, Loss: 0.914811, Accuracy: 75.92%\n",
      "Batch 141, Loss: 0.976780, Accuracy: 75.92%\n",
      "Batch 142, Loss: 0.962535, Accuracy: 75.94%\n",
      "Batch 143, Loss: 0.939522, Accuracy: 75.95%\n",
      "Batch 144, Loss: 0.988841, Accuracy: 75.93%\n",
      "Batch 145, Loss: 0.915125, Accuracy: 75.99%\n",
      "Batch 146, Loss: 0.907630, Accuracy: 76.04%\n",
      "Batch 147, Loss: 1.036041, Accuracy: 76.01%\n",
      "Batch 148, Loss: 0.951960, Accuracy: 76.02%\n",
      "Batch 149, Loss: 0.964447, Accuracy: 76.05%\n",
      "Batch 150, Loss: 0.958544, Accuracy: 76.06%\n",
      "Batch 151, Loss: 0.966442, Accuracy: 76.07%\n",
      "Batch 152, Loss: 0.878908, Accuracy: 76.14%\n",
      "Batch 153, Loss: 0.974731, Accuracy: 76.14%\n",
      "Batch 154, Loss: 0.965039, Accuracy: 76.17%\n",
      "Batch 155, Loss: 1.004587, Accuracy: 76.15%\n",
      "Batch 156, Loss: 0.974501, Accuracy: 76.15%\n",
      "Batch 157, Loss: 1.147486, Accuracy: 76.04%\n",
      "Batch 158, Loss: 0.933822, Accuracy: 76.08%\n",
      "Batch 159, Loss: 0.980922, Accuracy: 76.08%\n",
      "Batch 160, Loss: 1.066923, Accuracy: 76.04%\n",
      "Batch 161, Loss: 0.990708, Accuracy: 76.03%\n",
      "Batch 162, Loss: 1.003507, Accuracy: 76.01%\n",
      "Batch 163, Loss: 0.939198, Accuracy: 76.04%\n",
      "Batch 164, Loss: 1.001669, Accuracy: 76.00%\n",
      "Batch 165, Loss: 0.929128, Accuracy: 76.04%\n",
      "Batch 166, Loss: 0.952157, Accuracy: 76.06%\n",
      "Batch 167, Loss: 0.984546, Accuracy: 76.07%\n",
      "Batch 168, Loss: 0.948445, Accuracy: 76.09%\n",
      "Batch 169, Loss: 0.934871, Accuracy: 76.13%\n",
      "Batch 170, Loss: 0.945912, Accuracy: 76.15%\n",
      "Batch 171, Loss: 1.018450, Accuracy: 76.12%\n",
      "Batch 172, Loss: 1.057122, Accuracy: 76.08%\n",
      "Batch 173, Loss: 0.903377, Accuracy: 76.13%\n",
      "Batch 174, Loss: 1.010098, Accuracy: 76.11%\n",
      "Batch 175, Loss: 0.968143, Accuracy: 76.12%\n",
      "Batch 176, Loss: 0.963869, Accuracy: 76.13%\n",
      "Batch 177, Loss: 0.949951, Accuracy: 76.15%\n",
      "Batch 178, Loss: 0.986032, Accuracy: 76.15%\n",
      "Batch 179, Loss: 0.894326, Accuracy: 76.20%\n",
      "Batch 180, Loss: 1.073864, Accuracy: 76.15%\n",
      "Batch 181, Loss: 0.891411, Accuracy: 76.22%\n",
      "Batch 182, Loss: 1.020845, Accuracy: 76.19%\n",
      "Batch 183, Loss: 0.979928, Accuracy: 76.21%\n",
      "Batch 184, Loss: 0.954102, Accuracy: 76.22%\n",
      "Batch 185, Loss: 0.930551, Accuracy: 76.25%\n",
      "Batch 186, Loss: 1.031353, Accuracy: 76.22%\n",
      "Batch 187, Loss: 0.940746, Accuracy: 76.25%\n",
      "Batch 188, Loss: 1.025380, Accuracy: 76.23%\n",
      "Batch 189, Loss: 0.944323, Accuracy: 76.25%\n",
      "Batch 190, Loss: 0.998281, Accuracy: 76.24%\n",
      "Batch 191, Loss: 0.983350, Accuracy: 76.24%\n",
      "Batch 192, Loss: 0.971054, Accuracy: 76.25%\n",
      "Batch 193, Loss: 0.977291, Accuracy: 76.25%\n",
      "Batch 194, Loss: 0.952591, Accuracy: 76.26%\n",
      "Batch 195, Loss: 0.933393, Accuracy: 76.29%\n",
      "Batch 196, Loss: 0.979329, Accuracy: 76.29%\n",
      "Batch 197, Loss: 0.939332, Accuracy: 76.32%\n",
      "Batch 198, Loss: 0.946584, Accuracy: 76.33%\n",
      "Batch 199, Loss: 1.008166, Accuracy: 76.33%\n",
      "Batch 200, Loss: 0.985301, Accuracy: 76.33%\n",
      "Batch 201, Loss: 1.037171, Accuracy: 76.31%\n",
      "Batch 202, Loss: 1.031085, Accuracy: 76.28%\n",
      "Batch 203, Loss: 1.085632, Accuracy: 76.22%\n",
      "Batch 204, Loss: 0.957781, Accuracy: 76.24%\n",
      "Batch 205, Loss: 0.945707, Accuracy: 76.27%\n",
      "Batch 206, Loss: 0.996385, Accuracy: 76.26%\n",
      "Batch 207, Loss: 0.952032, Accuracy: 76.27%\n",
      "Batch 208, Loss: 0.903905, Accuracy: 76.30%\n",
      "Batch 209, Loss: 1.016165, Accuracy: 76.29%\n",
      "Batch 210, Loss: 0.935426, Accuracy: 76.32%\n",
      "Batch 211, Loss: 0.990631, Accuracy: 76.32%\n",
      "Batch 212, Loss: 1.035621, Accuracy: 76.29%\n",
      "Batch 213, Loss: 1.079332, Accuracy: 76.24%\n",
      "Training - Epoch 71, Loss: 0.980736, Accuracy: 76.24%\n",
      "Validation Batch 1, Loss: 0.988268, Accuracy: 78.12%\n",
      "Validation Batch 2, Loss: 1.056621, Accuracy: 73.44%\n",
      "Validation Batch 3, Loss: 1.076750, Accuracy: 70.83%\n",
      "Validation Batch 4, Loss: 1.024896, Accuracy: 70.70%\n",
      "Validation Batch 5, Loss: 1.001286, Accuracy: 71.56%\n",
      "Validation Batch 6, Loss: 0.945233, Accuracy: 73.18%\n",
      "Validation Batch 7, Loss: 1.026696, Accuracy: 73.21%\n",
      "Validation Batch 8, Loss: 1.058837, Accuracy: 72.66%\n",
      "Validation Batch 9, Loss: 1.076882, Accuracy: 72.05%\n",
      "Validation Batch 10, Loss: 1.040320, Accuracy: 71.88%\n",
      "Validation Batch 11, Loss: 1.004661, Accuracy: 72.16%\n",
      "Validation Batch 12, Loss: 0.968441, Accuracy: 72.53%\n",
      "Validation Batch 13, Loss: 1.106655, Accuracy: 71.88%\n",
      "Validation Batch 14, Loss: 1.051267, Accuracy: 71.65%\n",
      "Validation Batch 15, Loss: 1.004269, Accuracy: 71.88%\n",
      "Validation Batch 16, Loss: 0.996479, Accuracy: 72.27%\n",
      "Validation Batch 17, Loss: 1.055196, Accuracy: 72.06%\n",
      "Validation Batch 18, Loss: 1.009618, Accuracy: 72.22%\n",
      "Validation Batch 19, Loss: 1.084916, Accuracy: 71.88%\n",
      "Validation Batch 20, Loss: 1.050726, Accuracy: 71.72%\n",
      "Validation Batch 21, Loss: 1.034022, Accuracy: 71.65%\n",
      "Validation Batch 22, Loss: 1.062237, Accuracy: 71.59%\n",
      "Validation Batch 23, Loss: 1.120064, Accuracy: 71.20%\n",
      "Validation Batch 24, Loss: 1.090604, Accuracy: 70.90%\n",
      "Validation Batch 25, Loss: 1.002707, Accuracy: 70.88%\n",
      "Validation Batch 26, Loss: 1.019797, Accuracy: 70.97%\n",
      "Validation Batch 27, Loss: 1.009817, Accuracy: 70.99%\n",
      "Validation - Epoch 71, Loss: 1.035825, Accuracy: 70.99%\n",
      "Patienceâ€”0\n",
      "Epoch 72\n",
      "Batch 1, Loss: 0.993460, Accuracy: 75.00%\n",
      "Batch 2, Loss: 1.071332, Accuracy: 70.31%\n",
      "Batch 3, Loss: 1.004114, Accuracy: 71.35%\n",
      "Batch 4, Loss: 0.903791, Accuracy: 74.22%\n",
      "Batch 5, Loss: 0.883563, Accuracy: 76.56%\n",
      "Batch 6, Loss: 0.959701, Accuracy: 76.82%\n",
      "Batch 7, Loss: 0.969859, Accuracy: 76.79%\n",
      "Batch 8, Loss: 0.888888, Accuracy: 77.93%\n",
      "Batch 9, Loss: 0.872055, Accuracy: 79.17%\n",
      "Batch 10, Loss: 1.104251, Accuracy: 77.34%\n",
      "Batch 11, Loss: 0.978853, Accuracy: 77.27%\n",
      "Batch 12, Loss: 0.971377, Accuracy: 77.34%\n",
      "Batch 13, Loss: 0.991451, Accuracy: 77.16%\n",
      "Batch 14, Loss: 0.936543, Accuracy: 77.46%\n",
      "Batch 15, Loss: 0.985461, Accuracy: 77.29%\n",
      "Batch 16, Loss: 1.070787, Accuracy: 76.66%\n",
      "Batch 17, Loss: 0.994591, Accuracy: 76.65%\n",
      "Batch 18, Loss: 0.989456, Accuracy: 76.56%\n",
      "Batch 19, Loss: 1.069734, Accuracy: 76.07%\n",
      "Batch 20, Loss: 1.012129, Accuracy: 75.94%\n",
      "Batch 21, Loss: 1.012278, Accuracy: 75.74%\n",
      "Batch 22, Loss: 0.856756, Accuracy: 76.28%\n",
      "Batch 23, Loss: 0.949174, Accuracy: 76.43%\n",
      "Batch 24, Loss: 0.965055, Accuracy: 76.43%\n",
      "Batch 25, Loss: 0.939923, Accuracy: 76.62%\n",
      "Batch 26, Loss: 0.884653, Accuracy: 76.98%\n",
      "Batch 27, Loss: 1.011813, Accuracy: 76.85%\n",
      "Batch 28, Loss: 0.952490, Accuracy: 76.95%\n",
      "Batch 29, Loss: 0.953819, Accuracy: 77.05%\n",
      "Batch 30, Loss: 0.963115, Accuracy: 77.03%\n",
      "Batch 31, Loss: 1.002783, Accuracy: 76.97%\n",
      "Batch 32, Loss: 0.913398, Accuracy: 77.20%\n",
      "Batch 33, Loss: 0.982518, Accuracy: 77.13%\n",
      "Batch 34, Loss: 1.018300, Accuracy: 77.02%\n",
      "Batch 35, Loss: 1.012906, Accuracy: 76.92%\n",
      "Batch 36, Loss: 1.023996, Accuracy: 76.74%\n",
      "Batch 37, Loss: 0.998573, Accuracy: 76.65%\n",
      "Batch 38, Loss: 1.023011, Accuracy: 76.48%\n",
      "Batch 39, Loss: 0.903488, Accuracy: 76.72%\n",
      "Batch 40, Loss: 0.950988, Accuracy: 76.76%\n",
      "Batch 41, Loss: 0.893802, Accuracy: 76.94%\n",
      "Batch 42, Loss: 0.945132, Accuracy: 77.01%\n",
      "Batch 43, Loss: 1.013625, Accuracy: 76.93%\n",
      "Batch 44, Loss: 0.940848, Accuracy: 76.99%\n",
      "Batch 45, Loss: 1.025646, Accuracy: 76.88%\n",
      "Batch 46, Loss: 0.953555, Accuracy: 76.94%\n",
      "Batch 47, Loss: 0.955634, Accuracy: 76.96%\n",
      "Batch 48, Loss: 0.972881, Accuracy: 76.95%\n",
      "Batch 49, Loss: 0.936957, Accuracy: 77.07%\n",
      "Batch 50, Loss: 0.942015, Accuracy: 77.16%\n",
      "Batch 51, Loss: 1.008018, Accuracy: 77.08%\n",
      "Batch 52, Loss: 1.033930, Accuracy: 76.92%\n",
      "Batch 53, Loss: 1.040411, Accuracy: 76.77%\n",
      "Batch 54, Loss: 0.993861, Accuracy: 76.74%\n",
      "Batch 55, Loss: 0.969160, Accuracy: 76.73%\n",
      "Batch 56, Loss: 0.949176, Accuracy: 76.76%\n",
      "Batch 57, Loss: 1.039586, Accuracy: 76.64%\n",
      "Batch 58, Loss: 0.959935, Accuracy: 76.67%\n",
      "Batch 59, Loss: 1.006299, Accuracy: 76.62%\n",
      "Batch 60, Loss: 0.940988, Accuracy: 76.69%\n",
      "Batch 61, Loss: 0.899478, Accuracy: 76.79%\n",
      "Batch 62, Loss: 1.006656, Accuracy: 76.76%\n",
      "Batch 63, Loss: 0.992222, Accuracy: 76.74%\n",
      "Batch 64, Loss: 1.019519, Accuracy: 76.66%\n",
      "Batch 65, Loss: 0.931991, Accuracy: 76.75%\n",
      "Batch 66, Loss: 0.961515, Accuracy: 76.80%\n",
      "Batch 67, Loss: 0.954054, Accuracy: 76.82%\n",
      "Batch 68, Loss: 1.006304, Accuracy: 76.72%\n",
      "Batch 69, Loss: 1.017911, Accuracy: 76.63%\n",
      "Batch 70, Loss: 0.997791, Accuracy: 76.61%\n",
      "Batch 71, Loss: 1.048369, Accuracy: 76.47%\n",
      "Batch 72, Loss: 1.004363, Accuracy: 76.41%\n",
      "Batch 73, Loss: 1.017910, Accuracy: 76.37%\n",
      "Batch 74, Loss: 0.921880, Accuracy: 76.46%\n",
      "Batch 75, Loss: 0.965628, Accuracy: 76.48%\n",
      "Batch 76, Loss: 0.984542, Accuracy: 76.46%\n",
      "Batch 77, Loss: 0.881343, Accuracy: 76.58%\n",
      "Batch 78, Loss: 1.051476, Accuracy: 76.48%\n",
      "Batch 79, Loss: 0.991195, Accuracy: 76.44%\n",
      "Batch 80, Loss: 1.031106, Accuracy: 76.35%\n",
      "Batch 81, Loss: 0.969263, Accuracy: 76.37%\n",
      "Batch 82, Loss: 1.003695, Accuracy: 76.35%\n",
      "Batch 83, Loss: 0.965982, Accuracy: 76.36%\n",
      "Batch 84, Loss: 1.024940, Accuracy: 76.30%\n",
      "Batch 85, Loss: 1.031618, Accuracy: 76.23%\n",
      "Batch 86, Loss: 0.952681, Accuracy: 76.25%\n",
      "Batch 87, Loss: 1.062874, Accuracy: 76.17%\n",
      "Batch 88, Loss: 0.990117, Accuracy: 76.15%\n",
      "Batch 89, Loss: 0.996450, Accuracy: 76.12%\n",
      "Batch 90, Loss: 0.998054, Accuracy: 76.13%\n",
      "Batch 91, Loss: 0.930126, Accuracy: 76.20%\n",
      "Batch 92, Loss: 0.895278, Accuracy: 76.31%\n",
      "Batch 93, Loss: 0.978294, Accuracy: 76.29%\n",
      "Batch 94, Loss: 0.901196, Accuracy: 76.40%\n",
      "Batch 95, Loss: 1.022506, Accuracy: 76.35%\n",
      "Batch 96, Loss: 1.016634, Accuracy: 76.32%\n",
      "Batch 97, Loss: 1.063843, Accuracy: 76.21%\n",
      "Batch 98, Loss: 0.989353, Accuracy: 76.18%\n",
      "Batch 99, Loss: 0.958125, Accuracy: 76.22%\n",
      "Batch 100, Loss: 1.015037, Accuracy: 76.19%\n",
      "Batch 101, Loss: 0.974578, Accuracy: 76.21%\n",
      "Batch 102, Loss: 1.012995, Accuracy: 76.16%\n",
      "Batch 103, Loss: 1.057238, Accuracy: 76.08%\n",
      "Batch 104, Loss: 1.047511, Accuracy: 76.01%\n",
      "Batch 105, Loss: 1.058215, Accuracy: 75.91%\n",
      "Batch 106, Loss: 1.006637, Accuracy: 75.90%\n",
      "Batch 107, Loss: 0.930301, Accuracy: 75.96%\n",
      "Batch 108, Loss: 1.088706, Accuracy: 75.87%\n",
      "Batch 109, Loss: 0.931265, Accuracy: 75.90%\n",
      "Batch 110, Loss: 0.895793, Accuracy: 75.98%\n",
      "Batch 111, Loss: 0.918250, Accuracy: 76.04%\n",
      "Batch 112, Loss: 1.014541, Accuracy: 75.99%\n",
      "Batch 113, Loss: 1.017796, Accuracy: 75.95%\n",
      "Batch 114, Loss: 1.007785, Accuracy: 75.92%\n",
      "Batch 115, Loss: 0.989674, Accuracy: 75.92%\n",
      "Batch 116, Loss: 0.971636, Accuracy: 75.93%\n",
      "Batch 117, Loss: 0.932991, Accuracy: 75.97%\n",
      "Batch 118, Loss: 0.943446, Accuracy: 76.01%\n",
      "Batch 119, Loss: 0.995642, Accuracy: 76.00%\n",
      "Batch 120, Loss: 0.968182, Accuracy: 76.02%\n",
      "Batch 121, Loss: 1.032526, Accuracy: 75.97%\n",
      "Batch 122, Loss: 1.028767, Accuracy: 75.93%\n",
      "Batch 123, Loss: 0.955918, Accuracy: 75.95%\n",
      "Batch 124, Loss: 0.956909, Accuracy: 75.97%\n",
      "Batch 125, Loss: 1.030218, Accuracy: 75.92%\n",
      "Batch 126, Loss: 0.998646, Accuracy: 75.91%\n",
      "Batch 127, Loss: 1.016213, Accuracy: 75.89%\n",
      "Batch 128, Loss: 1.000301, Accuracy: 75.87%\n",
      "Batch 129, Loss: 0.954828, Accuracy: 75.88%\n",
      "Batch 130, Loss: 1.048965, Accuracy: 75.84%\n",
      "Batch 131, Loss: 1.041143, Accuracy: 75.80%\n",
      "Batch 132, Loss: 0.968916, Accuracy: 75.82%\n",
      "Batch 133, Loss: 0.911702, Accuracy: 75.87%\n",
      "Batch 134, Loss: 0.969144, Accuracy: 75.89%\n",
      "Batch 135, Loss: 1.028214, Accuracy: 75.86%\n",
      "Batch 136, Loss: 1.007302, Accuracy: 75.83%\n",
      "Batch 137, Loss: 0.987595, Accuracy: 75.82%\n",
      "Batch 138, Loss: 1.036079, Accuracy: 75.78%\n",
      "Batch 139, Loss: 1.007619, Accuracy: 75.75%\n",
      "Batch 140, Loss: 0.972481, Accuracy: 75.77%\n",
      "Batch 141, Loss: 0.994837, Accuracy: 75.76%\n",
      "Batch 142, Loss: 0.904450, Accuracy: 75.83%\n",
      "Batch 143, Loss: 1.034469, Accuracy: 75.79%\n",
      "Batch 144, Loss: 0.969710, Accuracy: 75.80%\n",
      "Batch 145, Loss: 1.012953, Accuracy: 75.79%\n",
      "Batch 146, Loss: 1.036038, Accuracy: 75.76%\n",
      "Batch 147, Loss: 1.011292, Accuracy: 75.75%\n",
      "Batch 148, Loss: 0.995158, Accuracy: 75.74%\n",
      "Batch 149, Loss: 0.956459, Accuracy: 75.78%\n",
      "Batch 150, Loss: 0.969000, Accuracy: 75.78%\n",
      "Batch 151, Loss: 0.927426, Accuracy: 75.82%\n",
      "Batch 152, Loss: 0.983478, Accuracy: 75.82%\n",
      "Batch 153, Loss: 1.006987, Accuracy: 75.81%\n",
      "Batch 154, Loss: 0.953538, Accuracy: 75.83%\n",
      "Batch 155, Loss: 1.001599, Accuracy: 75.82%\n",
      "Batch 156, Loss: 0.962803, Accuracy: 75.82%\n",
      "Batch 157, Loss: 1.017178, Accuracy: 75.79%\n",
      "Batch 158, Loss: 1.008416, Accuracy: 75.76%\n",
      "Batch 159, Loss: 0.946159, Accuracy: 75.80%\n",
      "Batch 160, Loss: 0.962936, Accuracy: 75.81%\n",
      "Batch 161, Loss: 0.965858, Accuracy: 75.83%\n",
      "Batch 162, Loss: 1.093015, Accuracy: 75.76%\n",
      "Batch 163, Loss: 0.976967, Accuracy: 75.77%\n",
      "Batch 164, Loss: 0.991701, Accuracy: 75.75%\n",
      "Batch 165, Loss: 0.889762, Accuracy: 75.83%\n",
      "Batch 166, Loss: 1.062201, Accuracy: 75.79%\n",
      "Batch 167, Loss: 1.004300, Accuracy: 75.78%\n",
      "Batch 168, Loss: 0.914984, Accuracy: 75.83%\n",
      "Batch 169, Loss: 0.933051, Accuracy: 75.86%\n",
      "Batch 170, Loss: 0.987208, Accuracy: 75.85%\n",
      "Batch 171, Loss: 0.972319, Accuracy: 75.87%\n",
      "Batch 172, Loss: 0.914416, Accuracy: 75.90%\n",
      "Batch 173, Loss: 0.932973, Accuracy: 75.93%\n",
      "Batch 174, Loss: 0.923096, Accuracy: 75.96%\n",
      "Batch 175, Loss: 0.950127, Accuracy: 75.98%\n",
      "Batch 176, Loss: 0.883880, Accuracy: 76.04%\n",
      "Batch 177, Loss: 0.923194, Accuracy: 76.08%\n",
      "Batch 178, Loss: 0.991815, Accuracy: 76.06%\n",
      "Batch 179, Loss: 1.030561, Accuracy: 76.05%\n",
      "Batch 180, Loss: 0.941668, Accuracy: 76.07%\n",
      "Batch 181, Loss: 1.011085, Accuracy: 76.06%\n",
      "Batch 182, Loss: 0.920722, Accuracy: 76.10%\n",
      "Batch 183, Loss: 0.961308, Accuracy: 76.10%\n",
      "Batch 184, Loss: 0.957354, Accuracy: 76.12%\n",
      "Batch 185, Loss: 0.964242, Accuracy: 76.15%\n",
      "Batch 186, Loss: 0.953621, Accuracy: 76.16%\n",
      "Batch 187, Loss: 0.886182, Accuracy: 76.21%\n",
      "Batch 188, Loss: 1.018883, Accuracy: 76.17%\n",
      "Batch 189, Loss: 0.972448, Accuracy: 76.17%\n",
      "Batch 190, Loss: 0.994838, Accuracy: 76.15%\n",
      "Batch 191, Loss: 1.038011, Accuracy: 76.12%\n",
      "Batch 192, Loss: 0.934071, Accuracy: 76.15%\n",
      "Batch 193, Loss: 1.053257, Accuracy: 76.11%\n",
      "Batch 194, Loss: 0.959835, Accuracy: 76.13%\n",
      "Batch 195, Loss: 0.990732, Accuracy: 76.12%\n",
      "Batch 196, Loss: 0.967567, Accuracy: 76.14%\n",
      "Batch 197, Loss: 1.065772, Accuracy: 76.09%\n",
      "Batch 198, Loss: 0.981228, Accuracy: 76.10%\n",
      "Batch 199, Loss: 0.998004, Accuracy: 76.08%\n",
      "Batch 200, Loss: 0.999688, Accuracy: 76.08%\n",
      "Batch 201, Loss: 0.960221, Accuracy: 76.09%\n",
      "Batch 202, Loss: 0.979404, Accuracy: 76.09%\n",
      "Batch 203, Loss: 0.948404, Accuracy: 76.11%\n",
      "Batch 204, Loss: 1.041315, Accuracy: 76.06%\n",
      "Batch 205, Loss: 0.978050, Accuracy: 76.07%\n",
      "Batch 206, Loss: 1.022588, Accuracy: 76.06%\n",
      "Batch 207, Loss: 1.028253, Accuracy: 76.04%\n",
      "Batch 208, Loss: 0.998026, Accuracy: 76.04%\n",
      "Batch 209, Loss: 0.992509, Accuracy: 76.04%\n",
      "Batch 210, Loss: 1.010964, Accuracy: 76.03%\n",
      "Batch 211, Loss: 0.992279, Accuracy: 76.02%\n",
      "Batch 212, Loss: 0.951074, Accuracy: 76.02%\n",
      "Batch 213, Loss: 1.102704, Accuracy: 75.96%\n",
      "Training - Epoch 72, Loss: 0.982312, Accuracy: 75.96%\n",
      "Validation Batch 1, Loss: 0.968007, Accuracy: 78.12%\n",
      "Validation Batch 2, Loss: 1.014166, Accuracy: 75.78%\n",
      "Validation Batch 3, Loss: 1.041043, Accuracy: 73.96%\n",
      "Validation Batch 4, Loss: 0.993895, Accuracy: 72.66%\n",
      "Validation Batch 5, Loss: 0.967141, Accuracy: 73.44%\n",
      "Validation Batch 6, Loss: 0.922605, Accuracy: 75.00%\n",
      "Validation Batch 7, Loss: 1.001935, Accuracy: 75.00%\n",
      "Validation Batch 8, Loss: 1.033181, Accuracy: 74.22%\n",
      "Validation Batch 9, Loss: 1.042731, Accuracy: 73.44%\n",
      "Validation Batch 10, Loss: 1.007324, Accuracy: 73.44%\n",
      "Validation Batch 11, Loss: 0.958624, Accuracy: 74.15%\n",
      "Validation Batch 12, Loss: 0.930376, Accuracy: 74.74%\n",
      "Validation Batch 13, Loss: 1.079641, Accuracy: 74.04%\n",
      "Validation Batch 14, Loss: 1.014115, Accuracy: 74.11%\n",
      "Validation Batch 15, Loss: 0.985454, Accuracy: 74.27%\n",
      "Validation Batch 16, Loss: 0.951766, Accuracy: 74.61%\n",
      "Validation Batch 17, Loss: 1.004832, Accuracy: 74.63%\n",
      "Validation Batch 18, Loss: 0.984318, Accuracy: 74.65%\n",
      "Validation Batch 19, Loss: 1.026240, Accuracy: 74.51%\n",
      "Validation Batch 20, Loss: 1.000633, Accuracy: 74.38%\n",
      "Validation Batch 21, Loss: 0.988334, Accuracy: 74.40%\n",
      "Validation Batch 22, Loss: 1.024326, Accuracy: 74.29%\n",
      "Validation Batch 23, Loss: 1.085605, Accuracy: 73.91%\n",
      "Validation Batch 24, Loss: 1.066011, Accuracy: 73.57%\n",
      "Validation Batch 25, Loss: 0.959736, Accuracy: 73.81%\n",
      "Validation Batch 26, Loss: 0.998857, Accuracy: 73.86%\n",
      "Validation Batch 27, Loss: 0.973707, Accuracy: 73.93%\n",
      "Validation - Epoch 72, Loss: 1.000911, Accuracy: 73.93%\n",
      "Patienceâ€”0\n",
      "Epoch 73\n",
      "Batch 1, Loss: 0.973615, Accuracy: 78.12%\n",
      "Batch 2, Loss: 0.957690, Accuracy: 78.91%\n",
      "Batch 3, Loss: 0.916135, Accuracy: 80.73%\n",
      "Batch 4, Loss: 0.936139, Accuracy: 80.86%\n",
      "Batch 5, Loss: 0.986823, Accuracy: 79.69%\n",
      "Batch 6, Loss: 0.969868, Accuracy: 79.69%\n",
      "Batch 7, Loss: 1.075826, Accuracy: 77.90%\n",
      "Batch 8, Loss: 1.062900, Accuracy: 76.17%\n",
      "Batch 9, Loss: 1.061396, Accuracy: 75.00%\n",
      "Batch 10, Loss: 0.987701, Accuracy: 75.00%\n",
      "Batch 11, Loss: 1.011862, Accuracy: 74.86%\n",
      "Batch 12, Loss: 0.994823, Accuracy: 74.87%\n",
      "Batch 13, Loss: 1.001534, Accuracy: 74.76%\n",
      "Batch 14, Loss: 1.008663, Accuracy: 74.67%\n",
      "Batch 15, Loss: 1.015253, Accuracy: 74.48%\n",
      "Batch 16, Loss: 0.960023, Accuracy: 74.90%\n",
      "Batch 17, Loss: 1.042260, Accuracy: 74.54%\n",
      "Batch 18, Loss: 0.950254, Accuracy: 74.83%\n",
      "Batch 19, Loss: 0.983173, Accuracy: 74.75%\n",
      "Batch 20, Loss: 0.890486, Accuracy: 75.16%\n",
      "Batch 21, Loss: 0.979590, Accuracy: 75.15%\n",
      "Batch 22, Loss: 0.950852, Accuracy: 75.43%\n",
      "Batch 23, Loss: 0.966850, Accuracy: 75.61%\n",
      "Batch 24, Loss: 0.959816, Accuracy: 75.78%\n",
      "Batch 25, Loss: 1.042690, Accuracy: 75.50%\n",
      "Batch 26, Loss: 0.988044, Accuracy: 75.42%\n",
      "Batch 27, Loss: 1.053463, Accuracy: 75.17%\n",
      "Batch 28, Loss: 1.011594, Accuracy: 75.06%\n",
      "Batch 29, Loss: 1.058488, Accuracy: 74.73%\n",
      "Batch 30, Loss: 1.028828, Accuracy: 74.53%\n",
      "Batch 31, Loss: 0.953267, Accuracy: 74.70%\n",
      "Batch 32, Loss: 0.959988, Accuracy: 74.85%\n",
      "Batch 33, Loss: 0.942487, Accuracy: 75.05%\n",
      "Batch 34, Loss: 0.996262, Accuracy: 75.05%\n",
      "Batch 35, Loss: 1.026206, Accuracy: 74.91%\n",
      "Batch 36, Loss: 0.980785, Accuracy: 74.91%\n",
      "Batch 37, Loss: 1.058630, Accuracy: 74.79%\n",
      "Batch 38, Loss: 0.983848, Accuracy: 74.84%\n",
      "Batch 39, Loss: 1.051477, Accuracy: 74.64%\n",
      "Batch 40, Loss: 0.992065, Accuracy: 74.65%\n",
      "Batch 41, Loss: 0.933974, Accuracy: 74.77%\n",
      "Batch 42, Loss: 0.913735, Accuracy: 74.96%\n",
      "Batch 43, Loss: 1.035357, Accuracy: 74.89%\n",
      "Batch 44, Loss: 0.998912, Accuracy: 74.89%\n",
      "Batch 45, Loss: 0.926951, Accuracy: 75.03%\n",
      "Batch 46, Loss: 1.024123, Accuracy: 75.00%\n",
      "Batch 47, Loss: 0.949137, Accuracy: 75.10%\n",
      "Batch 48, Loss: 1.000842, Accuracy: 75.07%\n",
      "Batch 49, Loss: 0.889033, Accuracy: 75.32%\n",
      "Batch 50, Loss: 0.985707, Accuracy: 75.34%\n",
      "Batch 51, Loss: 0.992571, Accuracy: 75.28%\n",
      "Batch 52, Loss: 1.015513, Accuracy: 75.18%\n",
      "Batch 53, Loss: 0.920635, Accuracy: 75.29%\n",
      "Batch 54, Loss: 0.934637, Accuracy: 75.41%\n",
      "Batch 55, Loss: 0.994744, Accuracy: 75.37%\n",
      "Batch 56, Loss: 1.007856, Accuracy: 75.31%\n",
      "Batch 57, Loss: 0.909916, Accuracy: 75.44%\n",
      "Batch 58, Loss: 0.959054, Accuracy: 75.48%\n",
      "Batch 59, Loss: 0.944949, Accuracy: 75.56%\n",
      "Batch 60, Loss: 0.970606, Accuracy: 75.60%\n",
      "Batch 61, Loss: 0.982774, Accuracy: 75.59%\n",
      "Batch 62, Loss: 1.051799, Accuracy: 75.50%\n",
      "Batch 63, Loss: 0.997506, Accuracy: 75.52%\n",
      "Batch 64, Loss: 0.984537, Accuracy: 75.49%\n",
      "Batch 65, Loss: 0.943457, Accuracy: 75.60%\n",
      "Batch 66, Loss: 0.986463, Accuracy: 75.62%\n",
      "Batch 67, Loss: 0.974545, Accuracy: 75.63%\n",
      "Batch 68, Loss: 0.937361, Accuracy: 75.69%\n",
      "Batch 69, Loss: 1.152703, Accuracy: 75.41%\n",
      "Batch 70, Loss: 0.976597, Accuracy: 75.42%\n",
      "Batch 71, Loss: 1.007996, Accuracy: 75.40%\n",
      "Batch 72, Loss: 1.021615, Accuracy: 75.35%\n",
      "Batch 73, Loss: 1.001301, Accuracy: 75.32%\n",
      "Batch 74, Loss: 1.054450, Accuracy: 75.21%\n",
      "Batch 75, Loss: 0.951267, Accuracy: 75.27%\n",
      "Batch 76, Loss: 0.959245, Accuracy: 75.31%\n",
      "Batch 77, Loss: 1.045330, Accuracy: 75.26%\n",
      "Batch 78, Loss: 0.976824, Accuracy: 75.28%\n",
      "Batch 79, Loss: 1.014719, Accuracy: 75.26%\n",
      "Batch 80, Loss: 0.956135, Accuracy: 75.31%\n",
      "Batch 81, Loss: 0.929499, Accuracy: 75.39%\n",
      "Batch 82, Loss: 0.961571, Accuracy: 75.44%\n",
      "Batch 83, Loss: 0.910858, Accuracy: 75.55%\n",
      "Batch 84, Loss: 0.965899, Accuracy: 75.56%\n",
      "Batch 85, Loss: 0.958514, Accuracy: 75.61%\n",
      "Batch 86, Loss: 0.950850, Accuracy: 75.64%\n",
      "Batch 87, Loss: 0.915991, Accuracy: 75.70%\n",
      "Batch 88, Loss: 0.990859, Accuracy: 75.71%\n",
      "Batch 89, Loss: 0.904880, Accuracy: 75.81%\n",
      "Batch 90, Loss: 1.012400, Accuracy: 75.78%\n",
      "Batch 91, Loss: 1.007158, Accuracy: 75.77%\n",
      "Batch 92, Loss: 0.997778, Accuracy: 75.75%\n",
      "Batch 93, Loss: 0.948082, Accuracy: 75.77%\n",
      "Batch 94, Loss: 1.065111, Accuracy: 75.66%\n",
      "Batch 95, Loss: 0.994362, Accuracy: 75.64%\n",
      "Batch 96, Loss: 1.020010, Accuracy: 75.60%\n",
      "Batch 97, Loss: 1.015261, Accuracy: 75.58%\n",
      "Batch 98, Loss: 0.921372, Accuracy: 75.65%\n",
      "Batch 99, Loss: 1.024571, Accuracy: 75.62%\n",
      "Batch 100, Loss: 0.965448, Accuracy: 75.61%\n",
      "Batch 101, Loss: 0.920280, Accuracy: 75.67%\n",
      "Batch 102, Loss: 1.016486, Accuracy: 75.64%\n",
      "Batch 103, Loss: 0.972898, Accuracy: 75.67%\n",
      "Batch 104, Loss: 1.030953, Accuracy: 75.60%\n",
      "Batch 105, Loss: 0.908726, Accuracy: 75.67%\n",
      "Batch 106, Loss: 1.079426, Accuracy: 75.57%\n",
      "Batch 107, Loss: 0.950579, Accuracy: 75.61%\n",
      "Batch 108, Loss: 0.992454, Accuracy: 75.62%\n",
      "Batch 109, Loss: 0.971634, Accuracy: 75.62%\n",
      "Batch 110, Loss: 0.974673, Accuracy: 75.60%\n",
      "Batch 111, Loss: 0.946971, Accuracy: 75.65%\n",
      "Batch 112, Loss: 0.988145, Accuracy: 75.64%\n",
      "Batch 113, Loss: 1.036642, Accuracy: 75.61%\n",
      "Batch 114, Loss: 1.050330, Accuracy: 75.55%\n",
      "Batch 115, Loss: 0.946885, Accuracy: 75.57%\n",
      "Batch 116, Loss: 0.977398, Accuracy: 75.58%\n",
      "Batch 117, Loss: 0.959095, Accuracy: 75.61%\n",
      "Batch 118, Loss: 0.924804, Accuracy: 75.68%\n",
      "Batch 119, Loss: 0.960011, Accuracy: 75.71%\n",
      "Batch 120, Loss: 0.968532, Accuracy: 75.73%\n",
      "Batch 121, Loss: 1.011426, Accuracy: 75.71%\n",
      "Batch 122, Loss: 1.031977, Accuracy: 75.67%\n",
      "Batch 123, Loss: 0.961160, Accuracy: 75.69%\n",
      "Batch 124, Loss: 0.945595, Accuracy: 75.74%\n",
      "Batch 125, Loss: 0.958420, Accuracy: 75.76%\n",
      "Batch 126, Loss: 0.968021, Accuracy: 75.79%\n",
      "Batch 127, Loss: 0.982590, Accuracy: 75.79%\n",
      "Batch 128, Loss: 1.033631, Accuracy: 75.74%\n",
      "Batch 129, Loss: 0.949008, Accuracy: 75.79%\n",
      "Batch 130, Loss: 1.031508, Accuracy: 75.73%\n",
      "Batch 131, Loss: 0.866883, Accuracy: 75.82%\n",
      "Batch 132, Loss: 0.996929, Accuracy: 75.82%\n",
      "Batch 133, Loss: 1.002096, Accuracy: 75.80%\n",
      "Batch 134, Loss: 1.052250, Accuracy: 75.73%\n",
      "Batch 135, Loss: 0.952341, Accuracy: 75.78%\n",
      "Batch 136, Loss: 0.910451, Accuracy: 75.83%\n",
      "Batch 137, Loss: 0.999129, Accuracy: 75.81%\n",
      "Batch 138, Loss: 0.983611, Accuracy: 75.80%\n",
      "Batch 139, Loss: 0.988970, Accuracy: 75.80%\n",
      "Batch 140, Loss: 1.040739, Accuracy: 75.76%\n",
      "Batch 141, Loss: 0.939773, Accuracy: 75.80%\n",
      "Batch 142, Loss: 1.014297, Accuracy: 75.77%\n",
      "Batch 143, Loss: 0.964921, Accuracy: 75.78%\n",
      "Batch 144, Loss: 1.045463, Accuracy: 75.74%\n",
      "Batch 145, Loss: 0.931467, Accuracy: 75.78%\n",
      "Batch 146, Loss: 1.032180, Accuracy: 75.75%\n",
      "Batch 147, Loss: 0.961059, Accuracy: 75.77%\n",
      "Batch 148, Loss: 0.959194, Accuracy: 75.79%\n",
      "Batch 149, Loss: 1.031444, Accuracy: 75.76%\n",
      "Batch 150, Loss: 0.946198, Accuracy: 75.78%\n",
      "Batch 151, Loss: 0.957047, Accuracy: 75.81%\n",
      "Batch 152, Loss: 0.959418, Accuracy: 75.82%\n",
      "Batch 153, Loss: 0.954725, Accuracy: 75.85%\n",
      "Batch 154, Loss: 0.952256, Accuracy: 75.87%\n",
      "Batch 155, Loss: 0.961109, Accuracy: 75.89%\n",
      "Batch 156, Loss: 1.017668, Accuracy: 75.86%\n",
      "Batch 157, Loss: 1.032041, Accuracy: 75.83%\n",
      "Batch 158, Loss: 0.870831, Accuracy: 75.90%\n",
      "Batch 159, Loss: 0.936757, Accuracy: 75.93%\n",
      "Batch 160, Loss: 0.975419, Accuracy: 75.93%\n",
      "Batch 161, Loss: 1.022509, Accuracy: 75.88%\n",
      "Batch 162, Loss: 0.954020, Accuracy: 75.91%\n",
      "Batch 163, Loss: 0.900697, Accuracy: 75.96%\n",
      "Batch 164, Loss: 0.902349, Accuracy: 76.00%\n",
      "Batch 165, Loss: 1.047257, Accuracy: 75.96%\n",
      "Batch 166, Loss: 0.939245, Accuracy: 76.00%\n",
      "Batch 167, Loss: 0.998745, Accuracy: 75.98%\n",
      "Batch 168, Loss: 0.943402, Accuracy: 76.00%\n",
      "Batch 169, Loss: 0.959798, Accuracy: 76.02%\n",
      "Batch 170, Loss: 1.033159, Accuracy: 75.97%\n",
      "Batch 171, Loss: 0.955517, Accuracy: 75.99%\n",
      "Batch 172, Loss: 1.030628, Accuracy: 75.96%\n",
      "Batch 173, Loss: 0.908443, Accuracy: 76.00%\n",
      "Batch 174, Loss: 1.012737, Accuracy: 75.99%\n",
      "Batch 175, Loss: 0.970070, Accuracy: 76.00%\n",
      "Batch 176, Loss: 1.031939, Accuracy: 75.97%\n",
      "Batch 177, Loss: 0.942677, Accuracy: 76.00%\n",
      "Batch 178, Loss: 0.927067, Accuracy: 76.04%\n",
      "Batch 179, Loss: 1.010904, Accuracy: 76.02%\n",
      "Batch 180, Loss: 0.945802, Accuracy: 76.04%\n",
      "Batch 181, Loss: 0.911286, Accuracy: 76.09%\n",
      "Batch 182, Loss: 0.989331, Accuracy: 76.08%\n",
      "Batch 183, Loss: 0.913952, Accuracy: 76.11%\n",
      "Batch 184, Loss: 1.077920, Accuracy: 76.06%\n",
      "Batch 185, Loss: 1.003668, Accuracy: 76.04%\n",
      "Batch 186, Loss: 0.979334, Accuracy: 76.03%\n",
      "Batch 187, Loss: 0.979277, Accuracy: 76.04%\n",
      "Batch 188, Loss: 1.008508, Accuracy: 76.03%\n",
      "Batch 189, Loss: 0.874620, Accuracy: 76.08%\n",
      "Batch 190, Loss: 1.002453, Accuracy: 76.07%\n",
      "Batch 191, Loss: 0.995016, Accuracy: 76.06%\n",
      "Batch 192, Loss: 0.934512, Accuracy: 76.08%\n",
      "Batch 193, Loss: 1.041311, Accuracy: 76.05%\n",
      "Batch 194, Loss: 0.975553, Accuracy: 76.06%\n",
      "Batch 195, Loss: 1.007009, Accuracy: 76.04%\n",
      "Batch 196, Loss: 1.000891, Accuracy: 76.03%\n",
      "Batch 197, Loss: 1.014627, Accuracy: 76.01%\n",
      "Batch 198, Loss: 0.991594, Accuracy: 76.01%\n",
      "Batch 199, Loss: 1.022002, Accuracy: 76.00%\n",
      "Batch 200, Loss: 0.966669, Accuracy: 75.99%\n",
      "Batch 201, Loss: 0.973165, Accuracy: 76.00%\n",
      "Batch 202, Loss: 0.971997, Accuracy: 76.00%\n",
      "Batch 203, Loss: 1.050047, Accuracy: 75.97%\n",
      "Batch 204, Loss: 1.004747, Accuracy: 75.97%\n",
      "Batch 205, Loss: 0.968763, Accuracy: 75.97%\n",
      "Batch 206, Loss: 0.933122, Accuracy: 75.99%\n",
      "Batch 207, Loss: 0.979597, Accuracy: 75.98%\n",
      "Batch 208, Loss: 1.005565, Accuracy: 75.98%\n",
      "Batch 209, Loss: 0.948200, Accuracy: 76.00%\n",
      "Batch 210, Loss: 0.969510, Accuracy: 76.00%\n",
      "Batch 211, Loss: 0.959795, Accuracy: 76.01%\n",
      "Batch 212, Loss: 1.037406, Accuracy: 75.99%\n",
      "Batch 213, Loss: 1.015805, Accuracy: 75.98%\n",
      "Training - Epoch 73, Loss: 0.981916, Accuracy: 75.98%\n",
      "Validation Batch 1, Loss: 0.969478, Accuracy: 78.12%\n",
      "Validation Batch 2, Loss: 1.016359, Accuracy: 75.00%\n",
      "Validation Batch 3, Loss: 1.044843, Accuracy: 73.44%\n",
      "Validation Batch 4, Loss: 1.006915, Accuracy: 72.66%\n",
      "Validation Batch 5, Loss: 0.974091, Accuracy: 73.44%\n",
      "Validation Batch 6, Loss: 0.935428, Accuracy: 74.48%\n",
      "Validation Batch 7, Loss: 1.007560, Accuracy: 74.55%\n",
      "Validation Batch 8, Loss: 1.042217, Accuracy: 73.83%\n",
      "Validation Batch 9, Loss: 1.052557, Accuracy: 73.09%\n",
      "Validation Batch 10, Loss: 1.014133, Accuracy: 73.12%\n",
      "Validation Batch 11, Loss: 0.971699, Accuracy: 73.72%\n",
      "Validation Batch 12, Loss: 0.943392, Accuracy: 74.22%\n",
      "Validation Batch 13, Loss: 1.084117, Accuracy: 73.56%\n",
      "Validation Batch 14, Loss: 1.027291, Accuracy: 73.44%\n",
      "Validation Batch 15, Loss: 0.987457, Accuracy: 73.54%\n",
      "Validation Batch 16, Loss: 0.971981, Accuracy: 73.93%\n",
      "Validation Batch 17, Loss: 1.015973, Accuracy: 73.99%\n",
      "Validation Batch 18, Loss: 0.986195, Accuracy: 74.05%\n",
      "Validation Batch 19, Loss: 1.043547, Accuracy: 73.93%\n",
      "Validation Batch 20, Loss: 1.013233, Accuracy: 73.75%\n",
      "Validation Batch 21, Loss: 0.998540, Accuracy: 73.81%\n",
      "Validation Batch 22, Loss: 1.039669, Accuracy: 73.72%\n",
      "Validation Batch 23, Loss: 1.096617, Accuracy: 73.30%\n",
      "Validation Batch 24, Loss: 1.074055, Accuracy: 72.98%\n",
      "Validation Batch 25, Loss: 0.968513, Accuracy: 73.19%\n",
      "Validation Batch 26, Loss: 1.006572, Accuracy: 73.20%\n",
      "Validation Batch 27, Loss: 0.972684, Accuracy: 73.34%\n",
      "Validation - Epoch 73, Loss: 1.009819, Accuracy: 73.34%\n",
      "Patienceâ€”1\n",
      "Epoch 74\n",
      "Batch 1, Loss: 0.981445, Accuracy: 76.56%\n",
      "Batch 2, Loss: 1.029605, Accuracy: 72.66%\n",
      "Batch 3, Loss: 0.864042, Accuracy: 78.12%\n",
      "Batch 4, Loss: 0.989186, Accuracy: 76.95%\n",
      "Batch 5, Loss: 0.982622, Accuracy: 77.19%\n",
      "Batch 6, Loss: 1.018885, Accuracy: 76.04%\n",
      "Batch 7, Loss: 0.950657, Accuracy: 76.56%\n",
      "Batch 8, Loss: 0.957351, Accuracy: 76.37%\n",
      "Batch 9, Loss: 1.023551, Accuracy: 75.69%\n",
      "Batch 10, Loss: 0.953764, Accuracy: 75.94%\n",
      "Batch 11, Loss: 1.005921, Accuracy: 75.85%\n",
      "Batch 12, Loss: 0.947190, Accuracy: 76.30%\n",
      "Batch 13, Loss: 0.953027, Accuracy: 76.56%\n",
      "Batch 14, Loss: 1.015578, Accuracy: 76.45%\n",
      "Batch 15, Loss: 0.938898, Accuracy: 76.67%\n",
      "Batch 16, Loss: 0.998232, Accuracy: 76.56%\n",
      "Batch 17, Loss: 0.954065, Accuracy: 76.56%\n",
      "Batch 18, Loss: 0.971486, Accuracy: 76.56%\n",
      "Batch 19, Loss: 1.013162, Accuracy: 76.32%\n",
      "Batch 20, Loss: 0.882692, Accuracy: 76.80%\n",
      "Batch 21, Loss: 0.905673, Accuracy: 77.16%\n",
      "Batch 22, Loss: 0.900277, Accuracy: 77.49%\n",
      "Batch 23, Loss: 0.879713, Accuracy: 77.92%\n",
      "Batch 24, Loss: 0.929951, Accuracy: 78.12%\n",
      "Batch 25, Loss: 1.017948, Accuracy: 77.81%\n",
      "Batch 26, Loss: 0.906135, Accuracy: 78.00%\n",
      "Batch 27, Loss: 1.057310, Accuracy: 77.60%\n",
      "Batch 28, Loss: 1.014753, Accuracy: 77.40%\n",
      "Batch 29, Loss: 1.037952, Accuracy: 77.10%\n",
      "Batch 30, Loss: 1.010692, Accuracy: 77.03%\n",
      "Batch 31, Loss: 0.968316, Accuracy: 77.02%\n",
      "Batch 32, Loss: 0.935751, Accuracy: 77.10%\n",
      "Batch 33, Loss: 0.897545, Accuracy: 77.37%\n",
      "Batch 34, Loss: 1.007349, Accuracy: 77.25%\n",
      "Batch 35, Loss: 1.034357, Accuracy: 77.10%\n",
      "Batch 36, Loss: 0.937924, Accuracy: 77.26%\n",
      "Batch 37, Loss: 0.928724, Accuracy: 77.45%\n",
      "Batch 38, Loss: 0.993467, Accuracy: 77.38%\n",
      "Batch 39, Loss: 0.937366, Accuracy: 77.48%\n",
      "Batch 40, Loss: 0.960514, Accuracy: 77.54%\n",
      "Batch 41, Loss: 1.094959, Accuracy: 77.13%\n",
      "Batch 42, Loss: 1.003129, Accuracy: 77.01%\n",
      "Batch 43, Loss: 0.991813, Accuracy: 76.93%\n",
      "Batch 44, Loss: 0.889898, Accuracy: 77.13%\n",
      "Batch 45, Loss: 0.968572, Accuracy: 77.12%\n",
      "Batch 46, Loss: 0.937508, Accuracy: 77.21%\n",
      "Batch 47, Loss: 0.939646, Accuracy: 77.29%\n",
      "Batch 48, Loss: 0.996433, Accuracy: 77.28%\n",
      "Batch 49, Loss: 1.005158, Accuracy: 77.20%\n",
      "Batch 50, Loss: 0.959929, Accuracy: 77.22%\n",
      "Batch 51, Loss: 0.989324, Accuracy: 77.18%\n",
      "Batch 52, Loss: 1.023539, Accuracy: 77.04%\n",
      "Batch 53, Loss: 0.970249, Accuracy: 77.03%\n",
      "Batch 54, Loss: 0.994115, Accuracy: 77.00%\n",
      "Batch 55, Loss: 1.060671, Accuracy: 76.85%\n",
      "Batch 56, Loss: 0.903128, Accuracy: 76.98%\n",
      "Batch 57, Loss: 1.040330, Accuracy: 76.84%\n",
      "Batch 58, Loss: 0.975364, Accuracy: 76.80%\n",
      "Batch 59, Loss: 1.030080, Accuracy: 76.67%\n",
      "Batch 60, Loss: 0.994987, Accuracy: 76.61%\n",
      "Batch 61, Loss: 1.025119, Accuracy: 76.54%\n",
      "Batch 62, Loss: 0.938293, Accuracy: 76.59%\n",
      "Batch 63, Loss: 1.014824, Accuracy: 76.51%\n",
      "Batch 64, Loss: 0.941955, Accuracy: 76.59%\n",
      "Batch 65, Loss: 1.038546, Accuracy: 76.44%\n",
      "Batch 66, Loss: 0.923007, Accuracy: 76.49%\n",
      "Batch 67, Loss: 0.985912, Accuracy: 76.52%\n",
      "Batch 68, Loss: 1.041395, Accuracy: 76.40%\n",
      "Batch 69, Loss: 0.990848, Accuracy: 76.36%\n",
      "Batch 70, Loss: 0.947907, Accuracy: 76.41%\n",
      "Batch 71, Loss: 0.994383, Accuracy: 76.39%\n",
      "Batch 72, Loss: 1.012643, Accuracy: 76.35%\n",
      "Batch 73, Loss: 1.085446, Accuracy: 76.20%\n",
      "Batch 74, Loss: 1.041029, Accuracy: 76.12%\n",
      "Batch 75, Loss: 1.035344, Accuracy: 76.04%\n",
      "Batch 76, Loss: 1.001767, Accuracy: 75.99%\n",
      "Batch 77, Loss: 0.949857, Accuracy: 76.03%\n",
      "Batch 78, Loss: 0.934020, Accuracy: 76.10%\n",
      "Batch 79, Loss: 0.999036, Accuracy: 76.07%\n",
      "Batch 80, Loss: 0.943591, Accuracy: 76.11%\n",
      "Batch 81, Loss: 0.983394, Accuracy: 76.10%\n",
      "Batch 82, Loss: 1.015595, Accuracy: 76.07%\n",
      "Batch 83, Loss: 0.963041, Accuracy: 76.09%\n",
      "Batch 84, Loss: 0.973542, Accuracy: 76.10%\n",
      "Batch 85, Loss: 0.960507, Accuracy: 76.12%\n",
      "Batch 86, Loss: 0.959678, Accuracy: 76.14%\n",
      "Batch 87, Loss: 1.020004, Accuracy: 76.10%\n",
      "Batch 88, Loss: 0.939538, Accuracy: 76.15%\n",
      "Batch 89, Loss: 0.928692, Accuracy: 76.21%\n",
      "Batch 90, Loss: 0.935957, Accuracy: 76.27%\n",
      "Batch 91, Loss: 0.919827, Accuracy: 76.36%\n",
      "Batch 92, Loss: 0.985496, Accuracy: 76.34%\n",
      "Batch 93, Loss: 1.029582, Accuracy: 76.29%\n",
      "Batch 94, Loss: 1.043180, Accuracy: 76.25%\n",
      "Batch 95, Loss: 1.007987, Accuracy: 76.23%\n",
      "Batch 96, Loss: 1.016058, Accuracy: 76.17%\n",
      "Batch 97, Loss: 1.042941, Accuracy: 76.11%\n",
      "Batch 98, Loss: 0.962106, Accuracy: 76.13%\n",
      "Batch 99, Loss: 0.920026, Accuracy: 76.20%\n",
      "Batch 100, Loss: 0.919778, Accuracy: 76.27%\n",
      "Batch 101, Loss: 0.969205, Accuracy: 76.30%\n",
      "Batch 102, Loss: 0.985579, Accuracy: 76.29%\n",
      "Batch 103, Loss: 1.039360, Accuracy: 76.21%\n",
      "Batch 104, Loss: 0.967010, Accuracy: 76.23%\n",
      "Batch 105, Loss: 1.052214, Accuracy: 76.18%\n",
      "Batch 106, Loss: 0.935535, Accuracy: 76.21%\n",
      "Batch 107, Loss: 1.017036, Accuracy: 76.18%\n",
      "Batch 108, Loss: 0.975634, Accuracy: 76.19%\n",
      "Batch 109, Loss: 0.973211, Accuracy: 76.19%\n",
      "Batch 110, Loss: 0.995268, Accuracy: 76.16%\n",
      "Batch 111, Loss: 0.984451, Accuracy: 76.14%\n",
      "Batch 112, Loss: 0.939427, Accuracy: 76.19%\n",
      "Batch 113, Loss: 0.992270, Accuracy: 76.19%\n",
      "Batch 114, Loss: 1.033161, Accuracy: 76.12%\n",
      "Batch 115, Loss: 0.989345, Accuracy: 76.13%\n",
      "Batch 116, Loss: 0.989696, Accuracy: 76.12%\n",
      "Batch 117, Loss: 0.976460, Accuracy: 76.11%\n",
      "Batch 118, Loss: 0.964442, Accuracy: 76.13%\n",
      "Batch 119, Loss: 0.946606, Accuracy: 76.17%\n",
      "Batch 120, Loss: 0.948082, Accuracy: 76.21%\n",
      "Batch 121, Loss: 1.013801, Accuracy: 76.19%\n",
      "Batch 122, Loss: 1.026141, Accuracy: 76.15%\n",
      "Batch 123, Loss: 1.039762, Accuracy: 76.11%\n",
      "Batch 124, Loss: 0.968160, Accuracy: 76.12%\n",
      "Batch 125, Loss: 0.976977, Accuracy: 76.11%\n",
      "Batch 126, Loss: 1.076360, Accuracy: 76.02%\n",
      "Batch 127, Loss: 1.021500, Accuracy: 75.98%\n",
      "Batch 128, Loss: 1.027653, Accuracy: 75.95%\n",
      "Batch 129, Loss: 0.926294, Accuracy: 76.01%\n",
      "Batch 130, Loss: 1.000630, Accuracy: 76.01%\n",
      "Batch 131, Loss: 0.966226, Accuracy: 76.04%\n",
      "Batch 132, Loss: 1.001055, Accuracy: 76.03%\n",
      "Batch 133, Loss: 0.932009, Accuracy: 76.06%\n",
      "Batch 134, Loss: 1.066535, Accuracy: 75.99%\n",
      "Batch 135, Loss: 0.959957, Accuracy: 76.00%\n",
      "Batch 136, Loss: 0.991997, Accuracy: 75.99%\n",
      "Batch 137, Loss: 0.960209, Accuracy: 76.00%\n",
      "Batch 138, Loss: 1.012878, Accuracy: 75.99%\n",
      "Batch 139, Loss: 0.912093, Accuracy: 76.05%\n",
      "Batch 140, Loss: 1.014262, Accuracy: 76.03%\n",
      "Batch 141, Loss: 0.964726, Accuracy: 76.04%\n",
      "Batch 142, Loss: 1.020620, Accuracy: 76.01%\n",
      "Batch 143, Loss: 0.928059, Accuracy: 76.05%\n",
      "Batch 144, Loss: 0.962059, Accuracy: 76.05%\n",
      "Batch 145, Loss: 1.057350, Accuracy: 76.01%\n",
      "Batch 146, Loss: 0.905699, Accuracy: 76.07%\n",
      "Batch 147, Loss: 0.930267, Accuracy: 76.12%\n",
      "Batch 148, Loss: 0.919848, Accuracy: 76.16%\n",
      "Batch 149, Loss: 0.933974, Accuracy: 76.20%\n",
      "Batch 150, Loss: 0.954377, Accuracy: 76.23%\n",
      "Batch 151, Loss: 0.899918, Accuracy: 76.28%\n",
      "Batch 152, Loss: 0.954412, Accuracy: 76.31%\n",
      "Batch 153, Loss: 1.027425, Accuracy: 76.27%\n",
      "Batch 154, Loss: 1.147786, Accuracy: 76.15%\n",
      "Batch 155, Loss: 0.901037, Accuracy: 76.21%\n",
      "Batch 156, Loss: 1.066102, Accuracy: 76.15%\n",
      "Batch 157, Loss: 0.991479, Accuracy: 76.13%\n",
      "Batch 158, Loss: 0.918126, Accuracy: 76.19%\n",
      "Batch 159, Loss: 1.066600, Accuracy: 76.12%\n",
      "Batch 160, Loss: 1.026449, Accuracy: 76.09%\n",
      "Batch 161, Loss: 0.940549, Accuracy: 76.11%\n",
      "Batch 162, Loss: 1.006519, Accuracy: 76.09%\n",
      "Batch 163, Loss: 1.016382, Accuracy: 76.07%\n",
      "Batch 164, Loss: 0.995012, Accuracy: 76.06%\n",
      "Batch 165, Loss: 1.059936, Accuracy: 76.01%\n",
      "Batch 166, Loss: 0.943933, Accuracy: 76.04%\n",
      "Batch 167, Loss: 0.996747, Accuracy: 76.02%\n",
      "Batch 168, Loss: 0.919984, Accuracy: 76.07%\n",
      "Batch 169, Loss: 0.926832, Accuracy: 76.11%\n",
      "Batch 170, Loss: 0.983260, Accuracy: 76.11%\n",
      "Batch 171, Loss: 0.937812, Accuracy: 76.13%\n",
      "Batch 172, Loss: 1.084143, Accuracy: 76.06%\n",
      "Batch 173, Loss: 1.035176, Accuracy: 76.03%\n",
      "Batch 174, Loss: 0.891192, Accuracy: 76.08%\n",
      "Batch 175, Loss: 0.982227, Accuracy: 76.07%\n",
      "Batch 176, Loss: 0.962482, Accuracy: 76.07%\n",
      "Batch 177, Loss: 1.004186, Accuracy: 76.08%\n",
      "Batch 178, Loss: 0.938363, Accuracy: 76.11%\n",
      "Batch 179, Loss: 0.898852, Accuracy: 76.15%\n",
      "Batch 180, Loss: 1.000681, Accuracy: 76.13%\n",
      "Batch 181, Loss: 1.028099, Accuracy: 76.10%\n",
      "Batch 182, Loss: 0.959147, Accuracy: 76.12%\n",
      "Batch 183, Loss: 0.901090, Accuracy: 76.18%\n",
      "Batch 184, Loss: 0.899075, Accuracy: 76.23%\n",
      "Batch 185, Loss: 0.908587, Accuracy: 76.28%\n",
      "Batch 186, Loss: 1.087566, Accuracy: 76.22%\n",
      "Batch 187, Loss: 1.006311, Accuracy: 76.20%\n",
      "Batch 188, Loss: 1.077415, Accuracy: 76.16%\n",
      "Batch 189, Loss: 0.981263, Accuracy: 76.16%\n",
      "Batch 190, Loss: 0.986434, Accuracy: 76.16%\n",
      "Batch 191, Loss: 1.103602, Accuracy: 76.09%\n",
      "Batch 192, Loss: 0.919826, Accuracy: 76.11%\n",
      "Batch 193, Loss: 1.001057, Accuracy: 76.10%\n",
      "Batch 194, Loss: 0.958344, Accuracy: 76.10%\n",
      "Batch 195, Loss: 0.929785, Accuracy: 76.14%\n",
      "Batch 196, Loss: 0.941879, Accuracy: 76.16%\n",
      "Batch 197, Loss: 0.979724, Accuracy: 76.17%\n",
      "Batch 198, Loss: 1.008852, Accuracy: 76.16%\n",
      "Batch 199, Loss: 0.971620, Accuracy: 76.16%\n",
      "Batch 200, Loss: 0.976206, Accuracy: 76.16%\n",
      "Batch 201, Loss: 0.902330, Accuracy: 76.20%\n",
      "Batch 202, Loss: 0.958260, Accuracy: 76.21%\n",
      "Batch 203, Loss: 0.954759, Accuracy: 76.23%\n",
      "Batch 204, Loss: 0.964483, Accuracy: 76.25%\n",
      "Batch 205, Loss: 0.991563, Accuracy: 76.24%\n",
      "Batch 206, Loss: 1.096449, Accuracy: 76.19%\n",
      "Batch 207, Loss: 1.019054, Accuracy: 76.16%\n",
      "Batch 208, Loss: 0.969744, Accuracy: 76.16%\n",
      "Batch 209, Loss: 0.878928, Accuracy: 76.21%\n",
      "Batch 210, Loss: 0.962627, Accuracy: 76.20%\n",
      "Batch 211, Loss: 0.961338, Accuracy: 76.20%\n",
      "Batch 212, Loss: 0.969345, Accuracy: 76.21%\n",
      "Batch 213, Loss: 0.980355, Accuracy: 76.21%\n",
      "Training - Epoch 74, Loss: 0.979990, Accuracy: 76.21%\n",
      "Validation Batch 1, Loss: 0.968209, Accuracy: 78.12%\n",
      "Validation Batch 2, Loss: 1.006385, Accuracy: 75.00%\n",
      "Validation Batch 3, Loss: 1.035076, Accuracy: 73.96%\n",
      "Validation Batch 4, Loss: 0.998145, Accuracy: 72.66%\n",
      "Validation Batch 5, Loss: 0.964451, Accuracy: 73.12%\n",
      "Validation Batch 6, Loss: 0.920262, Accuracy: 74.74%\n",
      "Validation Batch 7, Loss: 1.001504, Accuracy: 74.78%\n",
      "Validation Batch 8, Loss: 1.032445, Accuracy: 74.02%\n",
      "Validation Batch 9, Loss: 1.037699, Accuracy: 73.44%\n",
      "Validation Batch 10, Loss: 1.004507, Accuracy: 73.59%\n",
      "Validation Batch 11, Loss: 0.959882, Accuracy: 74.15%\n",
      "Validation Batch 12, Loss: 0.931847, Accuracy: 74.87%\n",
      "Validation Batch 13, Loss: 1.071483, Accuracy: 74.28%\n",
      "Validation Batch 14, Loss: 1.023701, Accuracy: 74.11%\n",
      "Validation Batch 15, Loss: 0.984832, Accuracy: 74.17%\n",
      "Validation Batch 16, Loss: 0.963271, Accuracy: 74.51%\n",
      "Validation Batch 17, Loss: 0.993977, Accuracy: 74.63%\n",
      "Validation Batch 18, Loss: 0.987039, Accuracy: 74.65%\n",
      "Validation Batch 19, Loss: 1.035800, Accuracy: 74.42%\n",
      "Validation Batch 20, Loss: 1.007544, Accuracy: 74.30%\n",
      "Validation Batch 21, Loss: 0.991649, Accuracy: 74.33%\n",
      "Validation Batch 22, Loss: 1.028283, Accuracy: 74.22%\n",
      "Validation Batch 23, Loss: 1.082419, Accuracy: 73.85%\n",
      "Validation Batch 24, Loss: 1.070252, Accuracy: 73.57%\n",
      "Validation Batch 25, Loss: 0.960293, Accuracy: 73.81%\n",
      "Validation Batch 26, Loss: 1.004400, Accuracy: 73.80%\n",
      "Validation Batch 27, Loss: 0.967035, Accuracy: 73.93%\n",
      "Validation - Epoch 74, Loss: 1.001200, Accuracy: 73.93%\n",
      "Patienceâ€”2\n",
      "Epoch 75\n",
      "Batch 1, Loss: 0.927597, Accuracy: 81.25%\n",
      "Batch 2, Loss: 0.994186, Accuracy: 78.91%\n",
      "Batch 3, Loss: 0.963334, Accuracy: 78.65%\n",
      "Batch 4, Loss: 1.057767, Accuracy: 76.17%\n",
      "Batch 5, Loss: 0.991714, Accuracy: 75.94%\n",
      "Batch 6, Loss: 0.888089, Accuracy: 77.86%\n",
      "Batch 7, Loss: 0.947483, Accuracy: 77.68%\n",
      "Batch 8, Loss: 1.071016, Accuracy: 76.37%\n",
      "Batch 9, Loss: 0.995651, Accuracy: 76.22%\n",
      "Batch 10, Loss: 0.937780, Accuracy: 76.72%\n",
      "Batch 11, Loss: 0.975634, Accuracy: 76.56%\n",
      "Batch 12, Loss: 0.950105, Accuracy: 76.82%\n",
      "Batch 13, Loss: 0.963263, Accuracy: 76.68%\n",
      "Batch 14, Loss: 0.996890, Accuracy: 76.56%\n",
      "Batch 15, Loss: 0.987797, Accuracy: 76.46%\n",
      "Batch 16, Loss: 0.939144, Accuracy: 76.66%\n",
      "Batch 17, Loss: 1.062391, Accuracy: 76.19%\n",
      "Batch 18, Loss: 1.003722, Accuracy: 76.04%\n",
      "Batch 19, Loss: 0.968343, Accuracy: 76.15%\n",
      "Batch 20, Loss: 0.954758, Accuracy: 76.17%\n",
      "Batch 21, Loss: 0.940970, Accuracy: 76.41%\n",
      "Batch 22, Loss: 1.005581, Accuracy: 76.35%\n",
      "Batch 23, Loss: 0.961379, Accuracy: 76.36%\n",
      "Batch 24, Loss: 0.948914, Accuracy: 76.56%\n",
      "Batch 25, Loss: 0.914249, Accuracy: 76.81%\n",
      "Batch 26, Loss: 0.944291, Accuracy: 76.98%\n",
      "Batch 27, Loss: 1.013906, Accuracy: 76.79%\n",
      "Batch 28, Loss: 0.929002, Accuracy: 76.95%\n",
      "Batch 29, Loss: 0.995891, Accuracy: 76.83%\n",
      "Batch 30, Loss: 0.970081, Accuracy: 76.88%\n",
      "Batch 31, Loss: 0.919650, Accuracy: 77.07%\n",
      "Batch 32, Loss: 0.909592, Accuracy: 77.29%\n",
      "Batch 33, Loss: 1.037186, Accuracy: 77.08%\n",
      "Batch 34, Loss: 0.932212, Accuracy: 77.16%\n",
      "Batch 35, Loss: 0.965410, Accuracy: 77.19%\n",
      "Batch 36, Loss: 0.978542, Accuracy: 77.17%\n",
      "Batch 37, Loss: 1.045919, Accuracy: 77.03%\n",
      "Batch 38, Loss: 0.999265, Accuracy: 76.93%\n",
      "Batch 39, Loss: 0.964024, Accuracy: 76.92%\n",
      "Batch 40, Loss: 0.942842, Accuracy: 76.99%\n",
      "Batch 41, Loss: 0.977502, Accuracy: 76.98%\n",
      "Batch 42, Loss: 1.022063, Accuracy: 76.90%\n",
      "Batch 43, Loss: 1.035279, Accuracy: 76.74%\n",
      "Batch 44, Loss: 0.990212, Accuracy: 76.67%\n",
      "Batch 45, Loss: 1.000765, Accuracy: 76.56%\n",
      "Batch 46, Loss: 0.944871, Accuracy: 76.66%\n",
      "Batch 47, Loss: 0.971102, Accuracy: 76.66%\n",
      "Batch 48, Loss: 0.980665, Accuracy: 76.63%\n",
      "Batch 49, Loss: 0.937366, Accuracy: 76.69%\n",
      "Batch 50, Loss: 1.012962, Accuracy: 76.59%\n",
      "Batch 51, Loss: 0.888685, Accuracy: 76.78%\n",
      "Batch 52, Loss: 0.862758, Accuracy: 76.98%\n",
      "Batch 53, Loss: 0.954899, Accuracy: 77.00%\n",
      "Batch 54, Loss: 0.934426, Accuracy: 77.08%\n",
      "Batch 55, Loss: 0.943136, Accuracy: 77.16%\n",
      "Batch 56, Loss: 0.941658, Accuracy: 77.18%\n",
      "Batch 57, Loss: 0.977329, Accuracy: 77.19%\n",
      "Batch 58, Loss: 1.044031, Accuracy: 77.05%\n",
      "Batch 59, Loss: 0.891649, Accuracy: 77.20%\n",
      "Batch 60, Loss: 1.005381, Accuracy: 77.14%\n",
      "Batch 61, Loss: 0.992859, Accuracy: 77.13%\n",
      "Batch 62, Loss: 0.974774, Accuracy: 77.09%\n",
      "Batch 63, Loss: 0.921082, Accuracy: 77.23%\n",
      "Batch 64, Loss: 1.025743, Accuracy: 77.15%\n",
      "Batch 65, Loss: 0.949146, Accuracy: 77.19%\n",
      "Batch 66, Loss: 0.907998, Accuracy: 77.27%\n",
      "Batch 67, Loss: 0.999450, Accuracy: 77.24%\n",
      "Batch 68, Loss: 0.920230, Accuracy: 77.32%\n",
      "Batch 69, Loss: 0.950099, Accuracy: 77.36%\n",
      "Batch 70, Loss: 0.942909, Accuracy: 77.37%\n",
      "Batch 71, Loss: 1.068579, Accuracy: 77.22%\n",
      "Batch 72, Loss: 1.041214, Accuracy: 77.13%\n",
      "Batch 73, Loss: 0.945602, Accuracy: 77.16%\n",
      "Batch 74, Loss: 0.948366, Accuracy: 77.22%\n",
      "Batch 75, Loss: 0.930275, Accuracy: 77.25%\n",
      "Batch 76, Loss: 0.949628, Accuracy: 77.28%\n",
      "Batch 77, Loss: 0.985234, Accuracy: 77.25%\n",
      "Batch 78, Loss: 0.980477, Accuracy: 77.20%\n",
      "Batch 79, Loss: 1.020613, Accuracy: 77.12%\n",
      "Batch 80, Loss: 0.986341, Accuracy: 77.09%\n",
      "Batch 81, Loss: 0.927140, Accuracy: 77.16%\n",
      "Batch 82, Loss: 1.098669, Accuracy: 77.00%\n",
      "Batch 83, Loss: 0.983697, Accuracy: 77.00%\n",
      "Batch 84, Loss: 0.998705, Accuracy: 76.93%\n",
      "Batch 85, Loss: 0.996097, Accuracy: 76.93%\n",
      "Batch 86, Loss: 1.015640, Accuracy: 76.89%\n",
      "Batch 87, Loss: 0.927645, Accuracy: 76.94%\n",
      "Batch 88, Loss: 1.004612, Accuracy: 76.90%\n",
      "Batch 89, Loss: 0.896200, Accuracy: 77.00%\n",
      "Batch 90, Loss: 0.939742, Accuracy: 77.03%\n",
      "Batch 91, Loss: 1.028192, Accuracy: 76.96%\n",
      "Batch 92, Loss: 0.990690, Accuracy: 76.95%\n",
      "Batch 93, Loss: 0.905451, Accuracy: 77.05%\n",
      "Batch 94, Loss: 0.919554, Accuracy: 77.11%\n",
      "Batch 95, Loss: 1.074834, Accuracy: 76.99%\n",
      "Batch 96, Loss: 1.021588, Accuracy: 76.94%\n",
      "Batch 97, Loss: 0.998554, Accuracy: 76.90%\n",
      "Batch 98, Loss: 1.059138, Accuracy: 76.80%\n",
      "Batch 99, Loss: 0.930644, Accuracy: 76.85%\n",
      "Batch 100, Loss: 1.043104, Accuracy: 76.77%\n",
      "Batch 101, Loss: 0.971971, Accuracy: 76.76%\n",
      "Batch 102, Loss: 0.952454, Accuracy: 76.78%\n",
      "Batch 103, Loss: 0.980462, Accuracy: 76.77%\n",
      "Batch 104, Loss: 0.860302, Accuracy: 76.89%\n",
      "Batch 105, Loss: 1.063321, Accuracy: 76.80%\n",
      "Batch 106, Loss: 0.922590, Accuracy: 76.84%\n",
      "Batch 107, Loss: 0.933395, Accuracy: 76.90%\n",
      "Batch 108, Loss: 1.073002, Accuracy: 76.81%\n",
      "Batch 109, Loss: 1.023266, Accuracy: 76.76%\n",
      "Batch 110, Loss: 0.910585, Accuracy: 76.83%\n",
      "Batch 111, Loss: 1.004345, Accuracy: 76.83%\n",
      "Batch 112, Loss: 1.055475, Accuracy: 76.77%\n",
      "Batch 113, Loss: 0.952524, Accuracy: 76.80%\n",
      "Batch 114, Loss: 0.989164, Accuracy: 76.78%\n",
      "Batch 115, Loss: 1.026441, Accuracy: 76.73%\n",
      "Batch 116, Loss: 0.977951, Accuracy: 76.71%\n",
      "Batch 117, Loss: 1.069693, Accuracy: 76.60%\n",
      "Batch 118, Loss: 0.979069, Accuracy: 76.60%\n",
      "Batch 119, Loss: 0.995204, Accuracy: 76.56%\n",
      "Batch 120, Loss: 1.003535, Accuracy: 76.54%\n",
      "Batch 121, Loss: 1.104873, Accuracy: 76.42%\n",
      "Batch 122, Loss: 0.976438, Accuracy: 76.41%\n",
      "Batch 123, Loss: 1.006962, Accuracy: 76.38%\n",
      "Batch 124, Loss: 0.974417, Accuracy: 76.40%\n",
      "Batch 125, Loss: 0.984439, Accuracy: 76.40%\n",
      "Batch 126, Loss: 1.038864, Accuracy: 76.35%\n",
      "Batch 127, Loss: 0.997436, Accuracy: 76.34%\n",
      "Batch 128, Loss: 0.968989, Accuracy: 76.34%\n",
      "Batch 129, Loss: 0.960274, Accuracy: 76.36%\n",
      "Batch 130, Loss: 0.970222, Accuracy: 76.36%\n",
      "Batch 131, Loss: 0.955325, Accuracy: 76.38%\n",
      "Batch 132, Loss: 0.932530, Accuracy: 76.43%\n",
      "Batch 133, Loss: 1.027439, Accuracy: 76.39%\n",
      "Batch 134, Loss: 0.941066, Accuracy: 76.42%\n",
      "Batch 135, Loss: 0.963052, Accuracy: 76.44%\n",
      "Batch 136, Loss: 1.009846, Accuracy: 76.44%\n",
      "Batch 137, Loss: 1.007023, Accuracy: 76.41%\n",
      "Batch 138, Loss: 1.111952, Accuracy: 76.31%\n",
      "Batch 139, Loss: 0.922437, Accuracy: 76.37%\n",
      "Batch 140, Loss: 0.939355, Accuracy: 76.41%\n",
      "Batch 141, Loss: 0.948510, Accuracy: 76.42%\n",
      "Batch 142, Loss: 0.911400, Accuracy: 76.47%\n",
      "Batch 143, Loss: 1.025634, Accuracy: 76.43%\n",
      "Batch 144, Loss: 0.975158, Accuracy: 76.44%\n",
      "Batch 145, Loss: 0.981993, Accuracy: 76.44%\n",
      "Batch 146, Loss: 0.958557, Accuracy: 76.47%\n",
      "Batch 147, Loss: 0.893152, Accuracy: 76.52%\n",
      "Batch 148, Loss: 0.943654, Accuracy: 76.55%\n",
      "Batch 149, Loss: 0.958232, Accuracy: 76.56%\n",
      "Batch 150, Loss: 1.052822, Accuracy: 76.52%\n",
      "Batch 151, Loss: 0.896339, Accuracy: 76.57%\n",
      "Batch 152, Loss: 0.951341, Accuracy: 76.59%\n",
      "Batch 153, Loss: 0.950361, Accuracy: 76.60%\n",
      "Batch 154, Loss: 0.959124, Accuracy: 76.61%\n",
      "Batch 155, Loss: 0.990516, Accuracy: 76.61%\n",
      "Batch 156, Loss: 0.913817, Accuracy: 76.66%\n",
      "Batch 157, Loss: 0.969667, Accuracy: 76.65%\n",
      "Batch 158, Loss: 1.092578, Accuracy: 76.58%\n",
      "Batch 159, Loss: 0.975734, Accuracy: 76.58%\n",
      "Batch 160, Loss: 1.006129, Accuracy: 76.53%\n",
      "Batch 161, Loss: 0.991410, Accuracy: 76.53%\n",
      "Batch 162, Loss: 0.920006, Accuracy: 76.56%\n",
      "Batch 163, Loss: 1.029694, Accuracy: 76.53%\n",
      "Batch 164, Loss: 0.968056, Accuracy: 76.53%\n",
      "Batch 165, Loss: 0.998916, Accuracy: 76.52%\n",
      "Batch 166, Loss: 0.948509, Accuracy: 76.53%\n",
      "Batch 167, Loss: 1.016229, Accuracy: 76.51%\n",
      "Batch 168, Loss: 1.094501, Accuracy: 76.43%\n",
      "Batch 169, Loss: 1.021268, Accuracy: 76.40%\n",
      "Batch 170, Loss: 0.985979, Accuracy: 76.40%\n",
      "Batch 171, Loss: 0.950126, Accuracy: 76.40%\n",
      "Batch 172, Loss: 0.896709, Accuracy: 76.44%\n",
      "Batch 173, Loss: 0.999193, Accuracy: 76.44%\n",
      "Batch 174, Loss: 1.032549, Accuracy: 76.41%\n",
      "Batch 175, Loss: 0.929346, Accuracy: 76.44%\n",
      "Batch 176, Loss: 0.955709, Accuracy: 76.45%\n",
      "Batch 177, Loss: 0.971789, Accuracy: 76.45%\n",
      "Batch 178, Loss: 0.869472, Accuracy: 76.52%\n",
      "Batch 179, Loss: 0.886894, Accuracy: 76.57%\n",
      "Batch 180, Loss: 0.988954, Accuracy: 76.56%\n",
      "Batch 181, Loss: 1.034538, Accuracy: 76.53%\n",
      "Batch 182, Loss: 0.937919, Accuracy: 76.55%\n",
      "Batch 183, Loss: 0.975774, Accuracy: 76.55%\n",
      "Batch 184, Loss: 0.943452, Accuracy: 76.56%\n",
      "Batch 185, Loss: 0.982557, Accuracy: 76.56%\n",
      "Batch 186, Loss: 1.071475, Accuracy: 76.52%\n",
      "Batch 187, Loss: 0.967210, Accuracy: 76.54%\n",
      "Batch 188, Loss: 0.917203, Accuracy: 76.58%\n",
      "Batch 189, Loss: 0.869737, Accuracy: 76.65%\n",
      "Batch 190, Loss: 0.911619, Accuracy: 76.69%\n",
      "Batch 191, Loss: 0.985746, Accuracy: 76.68%\n",
      "Batch 192, Loss: 1.014414, Accuracy: 76.66%\n",
      "Batch 193, Loss: 0.961969, Accuracy: 76.67%\n",
      "Batch 194, Loss: 0.839665, Accuracy: 76.74%\n",
      "Batch 195, Loss: 0.995917, Accuracy: 76.74%\n",
      "Batch 196, Loss: 0.975746, Accuracy: 76.75%\n",
      "Batch 197, Loss: 1.004215, Accuracy: 76.74%\n",
      "Batch 198, Loss: 0.969348, Accuracy: 76.74%\n",
      "Batch 199, Loss: 1.038322, Accuracy: 76.70%\n",
      "Batch 200, Loss: 1.006437, Accuracy: 76.69%\n",
      "Batch 201, Loss: 0.963218, Accuracy: 76.69%\n",
      "Batch 202, Loss: 0.945426, Accuracy: 76.71%\n",
      "Batch 203, Loss: 0.959112, Accuracy: 76.72%\n",
      "Batch 204, Loss: 1.011351, Accuracy: 76.71%\n",
      "Batch 205, Loss: 0.905575, Accuracy: 76.75%\n",
      "Batch 206, Loss: 0.969334, Accuracy: 76.74%\n",
      "Batch 207, Loss: 0.928528, Accuracy: 76.77%\n",
      "Batch 208, Loss: 0.943509, Accuracy: 76.79%\n",
      "Batch 209, Loss: 1.087691, Accuracy: 76.73%\n",
      "Batch 210, Loss: 0.985733, Accuracy: 76.71%\n",
      "Batch 211, Loss: 0.996189, Accuracy: 76.70%\n",
      "Batch 212, Loss: 0.997770, Accuracy: 76.69%\n",
      "Batch 213, Loss: 0.948131, Accuracy: 76.71%\n",
      "Training - Epoch 75, Loss: 0.975612, Accuracy: 76.71%\n",
      "Validation Batch 1, Loss: 1.005010, Accuracy: 73.44%\n",
      "Validation Batch 2, Loss: 1.062772, Accuracy: 71.09%\n",
      "Validation Batch 3, Loss: 1.074142, Accuracy: 69.27%\n",
      "Validation Batch 4, Loss: 1.041368, Accuracy: 68.75%\n",
      "Validation Batch 5, Loss: 1.013744, Accuracy: 69.69%\n",
      "Validation Batch 6, Loss: 0.969812, Accuracy: 71.09%\n",
      "Validation Batch 7, Loss: 1.031218, Accuracy: 71.43%\n",
      "Validation Batch 8, Loss: 1.061480, Accuracy: 70.90%\n",
      "Validation Batch 9, Loss: 1.080536, Accuracy: 70.31%\n",
      "Validation Batch 10, Loss: 1.053179, Accuracy: 70.16%\n",
      "Validation Batch 11, Loss: 1.021994, Accuracy: 70.31%\n",
      "Validation Batch 12, Loss: 0.981735, Accuracy: 70.70%\n",
      "Validation Batch 13, Loss: 1.103591, Accuracy: 70.19%\n",
      "Validation Batch 14, Loss: 1.066235, Accuracy: 70.09%\n",
      "Validation Batch 15, Loss: 1.012247, Accuracy: 70.31%\n",
      "Validation Batch 16, Loss: 1.022497, Accuracy: 70.31%\n",
      "Validation Batch 17, Loss: 1.062563, Accuracy: 70.22%\n",
      "Validation Batch 18, Loss: 1.011374, Accuracy: 70.40%\n",
      "Validation Batch 19, Loss: 1.085766, Accuracy: 70.23%\n",
      "Validation Batch 20, Loss: 1.053977, Accuracy: 69.92%\n",
      "Validation Batch 21, Loss: 1.048009, Accuracy: 69.87%\n",
      "Validation Batch 22, Loss: 1.089424, Accuracy: 69.67%\n",
      "Validation Batch 23, Loss: 1.122620, Accuracy: 69.36%\n",
      "Validation Batch 24, Loss: 1.095265, Accuracy: 69.14%\n",
      "Validation Batch 25, Loss: 1.030351, Accuracy: 69.00%\n",
      "Validation Batch 26, Loss: 1.015535, Accuracy: 69.11%\n",
      "Validation Batch 27, Loss: 1.023481, Accuracy: 69.17%\n",
      "Validation - Epoch 75, Loss: 1.045923, Accuracy: 69.17%\n",
      "Patienceâ€”3\n",
      "Epoch 76\n",
      "Batch 1, Loss: 0.953473, Accuracy: 79.69%\n",
      "Batch 2, Loss: 1.010653, Accuracy: 75.78%\n",
      "Batch 3, Loss: 0.969536, Accuracy: 76.56%\n",
      "Batch 4, Loss: 1.012859, Accuracy: 75.39%\n",
      "Batch 5, Loss: 1.032561, Accuracy: 74.38%\n",
      "Batch 6, Loss: 0.942304, Accuracy: 75.26%\n",
      "Batch 7, Loss: 1.037672, Accuracy: 74.33%\n",
      "Batch 8, Loss: 0.986485, Accuracy: 74.80%\n",
      "Batch 9, Loss: 0.931091, Accuracy: 75.17%\n",
      "Batch 10, Loss: 1.024746, Accuracy: 74.84%\n",
      "Batch 11, Loss: 1.014382, Accuracy: 74.72%\n",
      "Batch 12, Loss: 0.951174, Accuracy: 75.13%\n",
      "Batch 13, Loss: 0.951049, Accuracy: 75.60%\n",
      "Batch 14, Loss: 0.999358, Accuracy: 75.67%\n",
      "Batch 15, Loss: 0.902601, Accuracy: 76.25%\n",
      "Batch 16, Loss: 0.974969, Accuracy: 76.17%\n",
      "Batch 17, Loss: 0.935744, Accuracy: 76.38%\n",
      "Batch 18, Loss: 0.974681, Accuracy: 76.30%\n",
      "Batch 19, Loss: 0.945598, Accuracy: 76.40%\n",
      "Batch 20, Loss: 1.057547, Accuracy: 75.94%\n",
      "Batch 21, Loss: 0.971486, Accuracy: 76.04%\n",
      "Batch 22, Loss: 0.960673, Accuracy: 76.14%\n",
      "Batch 23, Loss: 0.925604, Accuracy: 76.36%\n",
      "Batch 24, Loss: 0.979893, Accuracy: 76.30%\n",
      "Batch 25, Loss: 1.015520, Accuracy: 76.12%\n",
      "Batch 26, Loss: 1.060467, Accuracy: 75.84%\n",
      "Batch 27, Loss: 0.934985, Accuracy: 76.04%\n",
      "Batch 28, Loss: 1.008606, Accuracy: 75.89%\n",
      "Batch 29, Loss: 0.912449, Accuracy: 76.13%\n",
      "Batch 30, Loss: 0.841091, Accuracy: 76.61%\n",
      "Batch 31, Loss: 0.896076, Accuracy: 76.86%\n",
      "Batch 32, Loss: 0.959639, Accuracy: 76.86%\n",
      "Batch 33, Loss: 0.966232, Accuracy: 76.89%\n",
      "Batch 34, Loss: 1.026877, Accuracy: 76.70%\n",
      "Batch 35, Loss: 0.973387, Accuracy: 76.74%\n",
      "Batch 36, Loss: 0.941546, Accuracy: 76.91%\n",
      "Batch 37, Loss: 0.973222, Accuracy: 76.86%\n",
      "Batch 38, Loss: 0.970201, Accuracy: 76.85%\n",
      "Batch 39, Loss: 1.066504, Accuracy: 76.56%\n",
      "Batch 40, Loss: 1.030453, Accuracy: 76.48%\n",
      "Batch 41, Loss: 0.977979, Accuracy: 76.49%\n",
      "Batch 42, Loss: 1.017826, Accuracy: 76.34%\n",
      "Batch 43, Loss: 0.957538, Accuracy: 76.45%\n",
      "Batch 44, Loss: 0.958803, Accuracy: 76.49%\n",
      "Batch 45, Loss: 0.975298, Accuracy: 76.49%\n",
      "Batch 46, Loss: 0.981511, Accuracy: 76.53%\n",
      "Batch 47, Loss: 0.906732, Accuracy: 76.70%\n",
      "Batch 48, Loss: 0.826905, Accuracy: 77.02%\n",
      "Batch 49, Loss: 0.912135, Accuracy: 77.10%\n",
      "Batch 50, Loss: 1.069341, Accuracy: 76.84%\n",
      "Batch 51, Loss: 0.953780, Accuracy: 76.87%\n",
      "Batch 52, Loss: 0.958961, Accuracy: 76.89%\n",
      "Batch 53, Loss: 0.992964, Accuracy: 76.86%\n",
      "Batch 54, Loss: 0.968288, Accuracy: 76.88%\n",
      "Batch 55, Loss: 1.030105, Accuracy: 76.79%\n",
      "Batch 56, Loss: 0.927024, Accuracy: 76.90%\n",
      "Batch 57, Loss: 0.997756, Accuracy: 76.86%\n",
      "Batch 58, Loss: 0.929838, Accuracy: 76.97%\n",
      "Batch 59, Loss: 0.945310, Accuracy: 76.99%\n",
      "Batch 60, Loss: 0.865672, Accuracy: 77.19%\n",
      "Batch 61, Loss: 0.967109, Accuracy: 77.20%\n",
      "Batch 62, Loss: 1.045328, Accuracy: 77.09%\n",
      "Batch 63, Loss: 0.984291, Accuracy: 77.08%\n",
      "Batch 64, Loss: 0.914577, Accuracy: 77.22%\n",
      "Batch 65, Loss: 1.009034, Accuracy: 77.14%\n",
      "Batch 66, Loss: 0.982500, Accuracy: 77.08%\n",
      "Batch 67, Loss: 1.037014, Accuracy: 76.96%\n",
      "Batch 68, Loss: 0.976881, Accuracy: 76.98%\n",
      "Batch 69, Loss: 0.932116, Accuracy: 77.02%\n",
      "Batch 70, Loss: 0.903881, Accuracy: 77.12%\n",
      "Batch 71, Loss: 1.022024, Accuracy: 77.07%\n",
      "Batch 72, Loss: 0.908298, Accuracy: 77.13%\n",
      "Batch 73, Loss: 1.086450, Accuracy: 76.97%\n",
      "Batch 74, Loss: 1.009643, Accuracy: 76.90%\n",
      "Batch 75, Loss: 0.995673, Accuracy: 76.88%\n",
      "Batch 76, Loss: 0.983824, Accuracy: 76.85%\n",
      "Batch 77, Loss: 0.995591, Accuracy: 76.83%\n",
      "Batch 78, Loss: 0.970349, Accuracy: 76.82%\n",
      "Batch 79, Loss: 1.017452, Accuracy: 76.76%\n",
      "Batch 80, Loss: 1.049203, Accuracy: 76.66%\n",
      "Batch 81, Loss: 0.890129, Accuracy: 76.76%\n",
      "Batch 82, Loss: 0.983015, Accuracy: 76.73%\n",
      "Batch 83, Loss: 1.030454, Accuracy: 76.66%\n",
      "Batch 84, Loss: 0.963928, Accuracy: 76.69%\n",
      "Batch 85, Loss: 0.999479, Accuracy: 76.65%\n",
      "Batch 86, Loss: 0.981075, Accuracy: 76.64%\n",
      "Batch 87, Loss: 0.883197, Accuracy: 76.76%\n",
      "Batch 88, Loss: 1.035118, Accuracy: 76.69%\n",
      "Batch 89, Loss: 1.107005, Accuracy: 76.54%\n",
      "Batch 90, Loss: 0.910064, Accuracy: 76.63%\n",
      "Batch 91, Loss: 0.932448, Accuracy: 76.68%\n",
      "Batch 92, Loss: 0.921193, Accuracy: 76.75%\n",
      "Batch 93, Loss: 1.015815, Accuracy: 76.70%\n",
      "Batch 94, Loss: 0.930633, Accuracy: 76.76%\n",
      "Batch 95, Loss: 1.036917, Accuracy: 76.68%\n",
      "Batch 96, Loss: 0.969327, Accuracy: 76.68%\n",
      "Batch 97, Loss: 0.922037, Accuracy: 76.76%\n",
      "Batch 98, Loss: 0.932496, Accuracy: 76.82%\n",
      "Batch 99, Loss: 1.019864, Accuracy: 76.77%\n",
      "Batch 100, Loss: 0.960317, Accuracy: 76.78%\n",
      "Batch 101, Loss: 0.931814, Accuracy: 76.83%\n",
      "Batch 102, Loss: 0.947525, Accuracy: 76.84%\n",
      "Batch 103, Loss: 0.986845, Accuracy: 76.85%\n",
      "Batch 104, Loss: 0.989616, Accuracy: 76.82%\n",
      "Batch 105, Loss: 0.991602, Accuracy: 76.79%\n",
      "Batch 106, Loss: 1.007525, Accuracy: 76.75%\n",
      "Batch 107, Loss: 0.998919, Accuracy: 76.71%\n",
      "Batch 108, Loss: 0.991462, Accuracy: 76.69%\n",
      "Batch 109, Loss: 0.960362, Accuracy: 76.69%\n",
      "Batch 110, Loss: 0.888802, Accuracy: 76.76%\n",
      "Batch 111, Loss: 0.975560, Accuracy: 76.76%\n",
      "Batch 112, Loss: 0.958318, Accuracy: 76.79%\n",
      "Batch 113, Loss: 0.899904, Accuracy: 76.87%\n",
      "Batch 114, Loss: 0.957356, Accuracy: 76.88%\n",
      "Batch 115, Loss: 0.963110, Accuracy: 76.88%\n",
      "Batch 116, Loss: 0.929924, Accuracy: 76.93%\n",
      "Batch 117, Loss: 0.957911, Accuracy: 76.95%\n",
      "Batch 118, Loss: 1.061207, Accuracy: 76.89%\n",
      "Batch 119, Loss: 0.912986, Accuracy: 76.94%\n",
      "Batch 120, Loss: 0.964260, Accuracy: 76.95%\n",
      "Batch 121, Loss: 0.980259, Accuracy: 76.95%\n",
      "Batch 122, Loss: 0.927912, Accuracy: 76.99%\n",
      "Batch 123, Loss: 0.978385, Accuracy: 76.98%\n",
      "Batch 124, Loss: 0.956039, Accuracy: 77.00%\n",
      "Batch 125, Loss: 0.932930, Accuracy: 77.04%\n",
      "Batch 126, Loss: 0.895038, Accuracy: 77.10%\n",
      "Batch 127, Loss: 1.041973, Accuracy: 77.03%\n",
      "Batch 128, Loss: 0.997313, Accuracy: 77.01%\n",
      "Batch 129, Loss: 0.918454, Accuracy: 77.06%\n",
      "Batch 130, Loss: 0.955981, Accuracy: 77.08%\n",
      "Batch 131, Loss: 1.020949, Accuracy: 77.04%\n",
      "Batch 132, Loss: 0.921384, Accuracy: 77.07%\n",
      "Batch 133, Loss: 0.927102, Accuracy: 77.11%\n",
      "Batch 134, Loss: 0.986165, Accuracy: 77.10%\n",
      "Batch 135, Loss: 1.012839, Accuracy: 77.06%\n",
      "Batch 136, Loss: 1.021607, Accuracy: 77.01%\n",
      "Batch 137, Loss: 1.025278, Accuracy: 76.96%\n",
      "Batch 138, Loss: 0.926636, Accuracy: 76.98%\n",
      "Batch 139, Loss: 0.937341, Accuracy: 77.00%\n",
      "Batch 140, Loss: 1.005560, Accuracy: 76.99%\n",
      "Batch 141, Loss: 0.967492, Accuracy: 76.99%\n",
      "Batch 142, Loss: 0.954331, Accuracy: 77.01%\n",
      "Batch 143, Loss: 1.036531, Accuracy: 76.97%\n",
      "Batch 144, Loss: 0.973350, Accuracy: 76.97%\n",
      "Batch 145, Loss: 1.018731, Accuracy: 76.93%\n",
      "Batch 146, Loss: 0.947644, Accuracy: 76.95%\n",
      "Batch 147, Loss: 1.003146, Accuracy: 76.92%\n",
      "Batch 148, Loss: 1.014650, Accuracy: 76.88%\n",
      "Batch 149, Loss: 0.998737, Accuracy: 76.87%\n",
      "Batch 150, Loss: 1.078325, Accuracy: 76.79%\n",
      "Batch 151, Loss: 1.035222, Accuracy: 76.75%\n",
      "Batch 152, Loss: 0.994659, Accuracy: 76.74%\n",
      "Batch 153, Loss: 0.923441, Accuracy: 76.77%\n",
      "Batch 154, Loss: 0.866954, Accuracy: 76.85%\n",
      "Batch 155, Loss: 1.000819, Accuracy: 76.83%\n",
      "Batch 156, Loss: 0.978777, Accuracy: 76.84%\n",
      "Batch 157, Loss: 1.072170, Accuracy: 76.78%\n",
      "Batch 158, Loss: 1.003234, Accuracy: 76.76%\n",
      "Batch 159, Loss: 0.986184, Accuracy: 76.73%\n",
      "Batch 160, Loss: 0.970114, Accuracy: 76.75%\n",
      "Batch 161, Loss: 1.081607, Accuracy: 76.67%\n",
      "Batch 162, Loss: 0.949622, Accuracy: 76.69%\n",
      "Batch 163, Loss: 0.972064, Accuracy: 76.68%\n",
      "Batch 164, Loss: 0.960741, Accuracy: 76.70%\n",
      "Batch 165, Loss: 0.952105, Accuracy: 76.70%\n",
      "Batch 166, Loss: 0.934284, Accuracy: 76.71%\n",
      "Batch 167, Loss: 1.052230, Accuracy: 76.67%\n",
      "Batch 168, Loss: 0.958728, Accuracy: 76.68%\n",
      "Batch 169, Loss: 0.971909, Accuracy: 76.70%\n",
      "Batch 170, Loss: 0.971909, Accuracy: 76.70%\n",
      "Batch 171, Loss: 1.024542, Accuracy: 76.67%\n",
      "Batch 172, Loss: 1.070395, Accuracy: 76.62%\n",
      "Batch 173, Loss: 0.993154, Accuracy: 76.62%\n",
      "Batch 174, Loss: 0.970759, Accuracy: 76.62%\n",
      "Batch 175, Loss: 1.010394, Accuracy: 76.57%\n",
      "Batch 176, Loss: 0.998081, Accuracy: 76.56%\n",
      "Batch 177, Loss: 0.958225, Accuracy: 76.57%\n",
      "Batch 178, Loss: 0.968772, Accuracy: 76.57%\n",
      "Batch 179, Loss: 1.101277, Accuracy: 76.51%\n",
      "Batch 180, Loss: 0.912953, Accuracy: 76.55%\n",
      "Batch 181, Loss: 1.006897, Accuracy: 76.54%\n",
      "Batch 182, Loss: 0.947936, Accuracy: 76.55%\n",
      "Batch 183, Loss: 0.992733, Accuracy: 76.55%\n",
      "Batch 184, Loss: 0.937713, Accuracy: 76.57%\n",
      "Batch 185, Loss: 0.922711, Accuracy: 76.60%\n",
      "Batch 186, Loss: 0.935329, Accuracy: 76.63%\n",
      "Batch 187, Loss: 0.979226, Accuracy: 76.64%\n",
      "Batch 188, Loss: 0.964575, Accuracy: 76.65%\n",
      "Batch 189, Loss: 0.959887, Accuracy: 76.66%\n",
      "Batch 190, Loss: 0.920028, Accuracy: 76.69%\n",
      "Batch 191, Loss: 1.009884, Accuracy: 76.67%\n",
      "Batch 192, Loss: 1.087665, Accuracy: 76.61%\n",
      "Batch 193, Loss: 0.954090, Accuracy: 76.62%\n",
      "Batch 194, Loss: 0.904927, Accuracy: 76.66%\n",
      "Batch 195, Loss: 1.026240, Accuracy: 76.63%\n",
      "Batch 196, Loss: 0.980059, Accuracy: 76.63%\n",
      "Batch 197, Loss: 0.952500, Accuracy: 76.64%\n",
      "Batch 198, Loss: 0.977351, Accuracy: 76.64%\n",
      "Batch 199, Loss: 1.027536, Accuracy: 76.61%\n",
      "Batch 200, Loss: 0.892464, Accuracy: 76.65%\n",
      "Batch 201, Loss: 0.968529, Accuracy: 76.66%\n",
      "Batch 202, Loss: 1.061196, Accuracy: 76.61%\n",
      "Batch 203, Loss: 1.053705, Accuracy: 76.57%\n",
      "Batch 204, Loss: 0.941279, Accuracy: 76.59%\n",
      "Batch 205, Loss: 0.953935, Accuracy: 76.60%\n",
      "Batch 206, Loss: 1.078603, Accuracy: 76.55%\n",
      "Batch 207, Loss: 0.908947, Accuracy: 76.59%\n",
      "Batch 208, Loss: 1.061903, Accuracy: 76.54%\n",
      "Batch 209, Loss: 1.033784, Accuracy: 76.50%\n",
      "Batch 210, Loss: 0.965657, Accuracy: 76.52%\n",
      "Batch 211, Loss: 0.921729, Accuracy: 76.55%\n",
      "Batch 212, Loss: 1.033766, Accuracy: 76.51%\n",
      "Batch 213, Loss: 1.022977, Accuracy: 76.49%\n",
      "Training - Epoch 76, Loss: 0.976813, Accuracy: 76.49%\n",
      "Validation Batch 1, Loss: 0.991601, Accuracy: 73.44%\n",
      "Validation Batch 2, Loss: 1.044603, Accuracy: 71.88%\n",
      "Validation Batch 3, Loss: 1.063273, Accuracy: 70.83%\n",
      "Validation Batch 4, Loss: 1.029050, Accuracy: 70.31%\n",
      "Validation Batch 5, Loss: 0.998780, Accuracy: 70.94%\n",
      "Validation Batch 6, Loss: 0.955818, Accuracy: 72.14%\n",
      "Validation Batch 7, Loss: 1.020193, Accuracy: 72.54%\n",
      "Validation Batch 8, Loss: 1.058860, Accuracy: 71.88%\n",
      "Validation Batch 9, Loss: 1.072763, Accuracy: 71.18%\n",
      "Validation Batch 10, Loss: 1.041139, Accuracy: 71.09%\n",
      "Validation Batch 11, Loss: 0.998022, Accuracy: 71.31%\n",
      "Validation Batch 12, Loss: 0.972410, Accuracy: 71.61%\n",
      "Validation Batch 13, Loss: 1.097481, Accuracy: 71.15%\n",
      "Validation Batch 14, Loss: 1.054148, Accuracy: 70.98%\n",
      "Validation Batch 15, Loss: 0.997887, Accuracy: 71.25%\n",
      "Validation Batch 16, Loss: 1.007897, Accuracy: 71.39%\n",
      "Validation Batch 17, Loss: 1.048456, Accuracy: 71.32%\n",
      "Validation Batch 18, Loss: 0.996066, Accuracy: 71.44%\n",
      "Validation Batch 19, Loss: 1.069593, Accuracy: 71.22%\n",
      "Validation Batch 20, Loss: 1.035355, Accuracy: 71.17%\n",
      "Validation Batch 21, Loss: 1.026741, Accuracy: 71.21%\n",
      "Validation Batch 22, Loss: 1.061839, Accuracy: 71.02%\n",
      "Validation Batch 23, Loss: 1.113688, Accuracy: 70.72%\n",
      "Validation Batch 24, Loss: 1.085150, Accuracy: 70.44%\n",
      "Validation Batch 25, Loss: 1.008463, Accuracy: 70.56%\n",
      "Validation Batch 26, Loss: 1.014112, Accuracy: 70.67%\n",
      "Validation Batch 27, Loss: 1.012773, Accuracy: 70.76%\n",
      "Validation - Epoch 76, Loss: 1.032450, Accuracy: 70.76%\n",
      "Patienceâ€”4\n",
      "Epoch 77\n",
      "Batch 1, Loss: 1.035771, Accuracy: 70.31%\n",
      "Batch 2, Loss: 1.014082, Accuracy: 71.09%\n",
      "Batch 3, Loss: 0.973136, Accuracy: 72.92%\n",
      "Batch 4, Loss: 1.036559, Accuracy: 71.88%\n",
      "Batch 5, Loss: 1.073986, Accuracy: 70.31%\n",
      "Batch 6, Loss: 0.959946, Accuracy: 71.61%\n",
      "Batch 7, Loss: 0.945331, Accuracy: 72.54%\n",
      "Batch 8, Loss: 1.010570, Accuracy: 72.85%\n",
      "Batch 9, Loss: 0.885949, Accuracy: 74.65%\n",
      "Batch 10, Loss: 1.048627, Accuracy: 74.22%\n",
      "Batch 11, Loss: 1.008727, Accuracy: 74.01%\n",
      "Batch 12, Loss: 0.948742, Accuracy: 74.22%\n",
      "Batch 13, Loss: 1.009076, Accuracy: 74.16%\n",
      "Batch 14, Loss: 0.984713, Accuracy: 74.22%\n",
      "Batch 15, Loss: 0.950120, Accuracy: 74.48%\n",
      "Batch 16, Loss: 0.919004, Accuracy: 75.00%\n",
      "Batch 17, Loss: 1.004810, Accuracy: 74.91%\n",
      "Batch 18, Loss: 0.957460, Accuracy: 75.09%\n",
      "Batch 19, Loss: 1.002621, Accuracy: 75.08%\n",
      "Batch 20, Loss: 1.006881, Accuracy: 74.92%\n",
      "Batch 21, Loss: 0.981618, Accuracy: 74.93%\n",
      "Batch 22, Loss: 0.932451, Accuracy: 75.21%\n",
      "Batch 23, Loss: 1.056219, Accuracy: 74.93%\n",
      "Batch 24, Loss: 0.980553, Accuracy: 74.93%\n",
      "Batch 25, Loss: 0.971562, Accuracy: 75.06%\n",
      "Batch 26, Loss: 0.928817, Accuracy: 75.36%\n",
      "Batch 27, Loss: 0.966785, Accuracy: 75.46%\n",
      "Batch 28, Loss: 0.984561, Accuracy: 75.50%\n",
      "Batch 29, Loss: 1.004578, Accuracy: 75.38%\n",
      "Batch 30, Loss: 1.067048, Accuracy: 75.05%\n",
      "Batch 31, Loss: 1.010113, Accuracy: 75.00%\n",
      "Batch 32, Loss: 0.906492, Accuracy: 75.29%\n",
      "Batch 33, Loss: 0.918111, Accuracy: 75.47%\n",
      "Batch 34, Loss: 0.910993, Accuracy: 75.74%\n",
      "Batch 35, Loss: 0.931298, Accuracy: 75.94%\n",
      "Batch 36, Loss: 0.951844, Accuracy: 76.09%\n",
      "Batch 37, Loss: 0.993871, Accuracy: 76.10%\n",
      "Batch 38, Loss: 0.992627, Accuracy: 76.03%\n",
      "Batch 39, Loss: 0.989304, Accuracy: 76.00%\n",
      "Batch 40, Loss: 0.938608, Accuracy: 76.17%\n",
      "Batch 41, Loss: 0.933009, Accuracy: 76.30%\n",
      "Batch 42, Loss: 0.931268, Accuracy: 76.41%\n",
      "Batch 43, Loss: 1.001376, Accuracy: 76.31%\n",
      "Batch 44, Loss: 0.935451, Accuracy: 76.38%\n",
      "Batch 45, Loss: 0.931624, Accuracy: 76.49%\n",
      "Batch 46, Loss: 0.944458, Accuracy: 76.60%\n",
      "Batch 47, Loss: 0.951380, Accuracy: 76.63%\n",
      "Batch 48, Loss: 0.937901, Accuracy: 76.69%\n",
      "Batch 49, Loss: 0.951549, Accuracy: 76.75%\n",
      "Batch 50, Loss: 0.977870, Accuracy: 76.75%\n",
      "Batch 51, Loss: 0.966394, Accuracy: 76.75%\n",
      "Batch 52, Loss: 0.984525, Accuracy: 76.74%\n",
      "Batch 53, Loss: 0.887380, Accuracy: 76.95%\n",
      "Batch 54, Loss: 1.005699, Accuracy: 76.88%\n",
      "Batch 55, Loss: 0.936445, Accuracy: 76.96%\n",
      "Batch 56, Loss: 0.959579, Accuracy: 77.01%\n",
      "Batch 57, Loss: 0.942308, Accuracy: 77.08%\n",
      "Batch 58, Loss: 0.930629, Accuracy: 77.13%\n",
      "Batch 59, Loss: 0.991949, Accuracy: 77.09%\n",
      "Batch 60, Loss: 0.993997, Accuracy: 77.03%\n",
      "Batch 61, Loss: 1.053761, Accuracy: 76.90%\n",
      "Batch 62, Loss: 0.921067, Accuracy: 76.99%\n",
      "Batch 63, Loss: 1.004469, Accuracy: 76.93%\n",
      "Batch 64, Loss: 0.997707, Accuracy: 76.90%\n",
      "Batch 65, Loss: 0.929857, Accuracy: 77.00%\n",
      "Batch 66, Loss: 0.917241, Accuracy: 77.08%\n",
      "Batch 67, Loss: 0.930427, Accuracy: 77.15%\n",
      "Batch 68, Loss: 0.963756, Accuracy: 77.18%\n",
      "Batch 69, Loss: 0.924812, Accuracy: 77.26%\n",
      "Batch 70, Loss: 0.910698, Accuracy: 77.34%\n",
      "Batch 71, Loss: 0.933164, Accuracy: 77.38%\n",
      "Batch 72, Loss: 1.030733, Accuracy: 77.30%\n",
      "Batch 73, Loss: 0.962673, Accuracy: 77.29%\n",
      "Batch 74, Loss: 0.989496, Accuracy: 77.28%\n",
      "Batch 75, Loss: 0.951660, Accuracy: 77.31%\n",
      "Batch 76, Loss: 0.973996, Accuracy: 77.30%\n",
      "Batch 77, Loss: 0.982286, Accuracy: 77.27%\n",
      "Batch 78, Loss: 0.943691, Accuracy: 77.30%\n",
      "Batch 79, Loss: 0.985240, Accuracy: 77.27%\n",
      "Batch 80, Loss: 0.985959, Accuracy: 77.27%\n",
      "Batch 81, Loss: 0.944608, Accuracy: 77.31%\n",
      "Batch 82, Loss: 0.935305, Accuracy: 77.36%\n",
      "Batch 83, Loss: 0.982744, Accuracy: 77.35%\n",
      "Batch 84, Loss: 0.978861, Accuracy: 77.34%\n",
      "Batch 85, Loss: 0.996778, Accuracy: 77.30%\n",
      "Batch 86, Loss: 1.076553, Accuracy: 77.16%\n",
      "Batch 87, Loss: 0.995851, Accuracy: 77.14%\n",
      "Batch 88, Loss: 0.968501, Accuracy: 77.11%\n",
      "Batch 89, Loss: 1.000329, Accuracy: 77.09%\n",
      "Batch 90, Loss: 1.027495, Accuracy: 77.01%\n",
      "Batch 91, Loss: 1.052944, Accuracy: 76.94%\n",
      "Batch 92, Loss: 1.070269, Accuracy: 76.85%\n",
      "Batch 93, Loss: 0.984606, Accuracy: 76.86%\n",
      "Batch 94, Loss: 0.886477, Accuracy: 76.96%\n",
      "Batch 95, Loss: 0.974442, Accuracy: 76.97%\n",
      "Batch 96, Loss: 0.892330, Accuracy: 77.07%\n",
      "Batch 97, Loss: 0.966980, Accuracy: 77.08%\n",
      "Batch 98, Loss: 0.918583, Accuracy: 77.14%\n",
      "Batch 99, Loss: 0.930040, Accuracy: 77.18%\n",
      "Batch 100, Loss: 0.931772, Accuracy: 77.23%\n",
      "Batch 101, Loss: 0.951565, Accuracy: 77.26%\n",
      "Batch 102, Loss: 0.960588, Accuracy: 77.27%\n",
      "Batch 103, Loss: 0.999179, Accuracy: 77.23%\n",
      "Batch 104, Loss: 0.983080, Accuracy: 77.21%\n",
      "Batch 105, Loss: 1.015295, Accuracy: 77.17%\n",
      "Batch 106, Loss: 0.985684, Accuracy: 77.15%\n",
      "Batch 107, Loss: 0.974598, Accuracy: 77.13%\n",
      "Batch 108, Loss: 1.012494, Accuracy: 77.10%\n",
      "Batch 109, Loss: 1.009062, Accuracy: 77.08%\n",
      "Batch 110, Loss: 0.912325, Accuracy: 77.14%\n",
      "Batch 111, Loss: 1.086121, Accuracy: 77.06%\n",
      "Batch 112, Loss: 0.860343, Accuracy: 77.15%\n",
      "Batch 113, Loss: 0.935321, Accuracy: 77.17%\n",
      "Batch 114, Loss: 0.996493, Accuracy: 77.15%\n",
      "Batch 115, Loss: 0.942560, Accuracy: 77.19%\n",
      "Batch 116, Loss: 0.995128, Accuracy: 77.18%\n",
      "Batch 117, Loss: 0.922584, Accuracy: 77.23%\n",
      "Batch 118, Loss: 0.951646, Accuracy: 77.24%\n",
      "Batch 119, Loss: 0.910104, Accuracy: 77.28%\n",
      "Batch 120, Loss: 0.874209, Accuracy: 77.37%\n",
      "Batch 121, Loss: 1.030853, Accuracy: 77.31%\n",
      "Batch 122, Loss: 0.919342, Accuracy: 77.36%\n",
      "Batch 123, Loss: 0.914326, Accuracy: 77.40%\n",
      "Batch 124, Loss: 0.971000, Accuracy: 77.39%\n",
      "Batch 125, Loss: 0.947616, Accuracy: 77.41%\n",
      "Batch 126, Loss: 0.992934, Accuracy: 77.39%\n",
      "Batch 127, Loss: 0.919336, Accuracy: 77.44%\n",
      "Batch 128, Loss: 1.030249, Accuracy: 77.39%\n",
      "Batch 129, Loss: 0.972248, Accuracy: 77.41%\n",
      "Batch 130, Loss: 0.970442, Accuracy: 77.40%\n",
      "Batch 131, Loss: 1.030497, Accuracy: 77.36%\n",
      "Batch 132, Loss: 0.959051, Accuracy: 77.38%\n",
      "Batch 133, Loss: 0.972983, Accuracy: 77.38%\n",
      "Batch 134, Loss: 1.126298, Accuracy: 77.27%\n",
      "Batch 135, Loss: 0.990182, Accuracy: 77.23%\n",
      "Batch 136, Loss: 0.880050, Accuracy: 77.31%\n",
      "Batch 137, Loss: 0.935374, Accuracy: 77.33%\n",
      "Batch 138, Loss: 0.872231, Accuracy: 77.40%\n",
      "Batch 139, Loss: 0.904891, Accuracy: 77.43%\n",
      "Batch 140, Loss: 0.927949, Accuracy: 77.46%\n",
      "Batch 141, Loss: 0.943906, Accuracy: 77.47%\n",
      "Batch 142, Loss: 0.980127, Accuracy: 77.46%\n",
      "Batch 143, Loss: 0.958223, Accuracy: 77.47%\n",
      "Batch 144, Loss: 0.989246, Accuracy: 77.45%\n",
      "Batch 145, Loss: 0.915458, Accuracy: 77.49%\n",
      "Batch 146, Loss: 1.013520, Accuracy: 77.46%\n",
      "Batch 147, Loss: 1.068271, Accuracy: 77.39%\n",
      "Batch 148, Loss: 0.974379, Accuracy: 77.40%\n",
      "Batch 149, Loss: 0.947007, Accuracy: 77.41%\n",
      "Batch 150, Loss: 1.011962, Accuracy: 77.38%\n",
      "Batch 151, Loss: 0.925832, Accuracy: 77.41%\n",
      "Batch 152, Loss: 0.907284, Accuracy: 77.48%\n",
      "Batch 153, Loss: 0.962289, Accuracy: 77.49%\n",
      "Batch 154, Loss: 0.912749, Accuracy: 77.55%\n",
      "Batch 155, Loss: 1.038717, Accuracy: 77.50%\n",
      "Batch 156, Loss: 0.920294, Accuracy: 77.53%\n",
      "Batch 157, Loss: 0.948197, Accuracy: 77.54%\n",
      "Batch 158, Loss: 1.056416, Accuracy: 77.48%\n",
      "Batch 159, Loss: 0.951394, Accuracy: 77.50%\n",
      "Batch 160, Loss: 1.010799, Accuracy: 77.47%\n",
      "Batch 161, Loss: 0.952809, Accuracy: 77.47%\n",
      "Batch 162, Loss: 0.926683, Accuracy: 77.49%\n",
      "Batch 163, Loss: 0.972236, Accuracy: 77.48%\n",
      "Batch 164, Loss: 0.901921, Accuracy: 77.53%\n",
      "Batch 165, Loss: 1.071843, Accuracy: 77.47%\n",
      "Batch 166, Loss: 0.942998, Accuracy: 77.48%\n",
      "Batch 167, Loss: 0.977627, Accuracy: 77.48%\n",
      "Batch 168, Loss: 0.957690, Accuracy: 77.49%\n",
      "Batch 169, Loss: 1.015341, Accuracy: 77.46%\n",
      "Batch 170, Loss: 0.989312, Accuracy: 77.44%\n",
      "Batch 171, Loss: 0.925116, Accuracy: 77.47%\n",
      "Batch 172, Loss: 1.111671, Accuracy: 77.38%\n",
      "Batch 173, Loss: 0.997305, Accuracy: 77.36%\n",
      "Batch 174, Loss: 0.972568, Accuracy: 77.33%\n",
      "Batch 175, Loss: 1.026965, Accuracy: 77.29%\n",
      "Batch 176, Loss: 0.996180, Accuracy: 77.28%\n",
      "Batch 177, Loss: 1.043644, Accuracy: 77.23%\n",
      "Batch 178, Loss: 0.956472, Accuracy: 77.24%\n",
      "Batch 179, Loss: 1.023671, Accuracy: 77.21%\n",
      "Batch 180, Loss: 0.948257, Accuracy: 77.22%\n",
      "Batch 181, Loss: 0.927972, Accuracy: 77.25%\n",
      "Batch 182, Loss: 1.073049, Accuracy: 77.19%\n",
      "Batch 183, Loss: 1.013617, Accuracy: 77.16%\n",
      "Batch 184, Loss: 0.940563, Accuracy: 77.18%\n",
      "Batch 185, Loss: 0.989690, Accuracy: 77.16%\n",
      "Batch 186, Loss: 0.989036, Accuracy: 77.15%\n",
      "Batch 187, Loss: 0.970658, Accuracy: 77.15%\n",
      "Batch 188, Loss: 0.940898, Accuracy: 77.17%\n",
      "Batch 189, Loss: 0.979369, Accuracy: 77.16%\n",
      "Batch 190, Loss: 1.000079, Accuracy: 77.14%\n",
      "Batch 191, Loss: 0.965278, Accuracy: 77.15%\n",
      "Batch 192, Loss: 0.961325, Accuracy: 77.15%\n",
      "Batch 193, Loss: 1.025344, Accuracy: 77.11%\n",
      "Batch 194, Loss: 1.036308, Accuracy: 77.08%\n",
      "Batch 195, Loss: 0.966201, Accuracy: 77.07%\n",
      "Batch 196, Loss: 1.040442, Accuracy: 77.02%\n",
      "Batch 197, Loss: 0.964857, Accuracy: 77.01%\n",
      "Batch 198, Loss: 1.041173, Accuracy: 76.97%\n",
      "Batch 199, Loss: 1.003034, Accuracy: 76.95%\n",
      "Batch 200, Loss: 0.984485, Accuracy: 76.95%\n",
      "Batch 201, Loss: 1.059536, Accuracy: 76.90%\n",
      "Batch 202, Loss: 0.930658, Accuracy: 76.91%\n",
      "Batch 203, Loss: 1.051207, Accuracy: 76.87%\n",
      "Batch 204, Loss: 0.922097, Accuracy: 76.90%\n",
      "Batch 205, Loss: 0.958722, Accuracy: 76.90%\n",
      "Batch 206, Loss: 0.850337, Accuracy: 76.96%\n",
      "Batch 207, Loss: 0.999276, Accuracy: 76.96%\n",
      "Batch 208, Loss: 1.077630, Accuracy: 76.91%\n",
      "Batch 209, Loss: 1.020077, Accuracy: 76.89%\n",
      "Batch 210, Loss: 0.943946, Accuracy: 76.89%\n",
      "Batch 211, Loss: 0.919496, Accuracy: 76.91%\n",
      "Batch 212, Loss: 1.040160, Accuracy: 76.89%\n",
      "Batch 213, Loss: 0.988878, Accuracy: 76.88%\n",
      "Training - Epoch 77, Loss: 0.973542, Accuracy: 76.88%\n",
      "Validation Batch 1, Loss: 0.966517, Accuracy: 78.12%\n",
      "Validation Batch 2, Loss: 1.018556, Accuracy: 74.22%\n",
      "Validation Batch 3, Loss: 1.038627, Accuracy: 73.44%\n",
      "Validation Batch 4, Loss: 0.997468, Accuracy: 72.66%\n",
      "Validation Batch 5, Loss: 0.971431, Accuracy: 73.75%\n",
      "Validation Batch 6, Loss: 0.930531, Accuracy: 75.00%\n",
      "Validation Batch 7, Loss: 1.008101, Accuracy: 75.00%\n",
      "Validation Batch 8, Loss: 1.038258, Accuracy: 74.41%\n",
      "Validation Batch 9, Loss: 1.045517, Accuracy: 73.78%\n",
      "Validation Batch 10, Loss: 1.011696, Accuracy: 73.75%\n",
      "Validation Batch 11, Loss: 0.966025, Accuracy: 74.43%\n",
      "Validation Batch 12, Loss: 0.948537, Accuracy: 74.74%\n",
      "Validation Batch 13, Loss: 1.083130, Accuracy: 74.04%\n",
      "Validation Batch 14, Loss: 1.026244, Accuracy: 73.77%\n",
      "Validation Batch 15, Loss: 0.987585, Accuracy: 73.96%\n",
      "Validation Batch 16, Loss: 0.971106, Accuracy: 74.22%\n",
      "Validation Batch 17, Loss: 1.012606, Accuracy: 74.26%\n",
      "Validation Batch 18, Loss: 0.985160, Accuracy: 74.22%\n",
      "Validation Batch 19, Loss: 1.033134, Accuracy: 74.10%\n",
      "Validation Batch 20, Loss: 1.009399, Accuracy: 73.98%\n",
      "Validation Batch 21, Loss: 0.997017, Accuracy: 74.03%\n",
      "Validation Batch 22, Loss: 1.033944, Accuracy: 74.01%\n",
      "Validation Batch 23, Loss: 1.094079, Accuracy: 73.57%\n",
      "Validation Batch 24, Loss: 1.068052, Accuracy: 73.31%\n",
      "Validation Batch 25, Loss: 0.975265, Accuracy: 73.50%\n",
      "Validation Batch 26, Loss: 1.004054, Accuracy: 73.50%\n",
      "Validation Batch 27, Loss: 0.974812, Accuracy: 73.63%\n",
      "Validation - Epoch 77, Loss: 1.007291, Accuracy: 73.63%\n",
      "Patienceâ€”5\n",
      "Epoch 78\n",
      "Batch 1, Loss: 0.993236, Accuracy: 71.88%\n",
      "Batch 2, Loss: 0.895594, Accuracy: 78.12%\n",
      "Batch 3, Loss: 1.005444, Accuracy: 75.52%\n",
      "Batch 4, Loss: 1.010930, Accuracy: 75.39%\n",
      "Batch 5, Loss: 1.010610, Accuracy: 75.00%\n",
      "Batch 6, Loss: 0.992300, Accuracy: 75.00%\n",
      "Batch 7, Loss: 0.964119, Accuracy: 75.22%\n",
      "Batch 8, Loss: 0.947873, Accuracy: 75.78%\n",
      "Batch 9, Loss: 0.975741, Accuracy: 75.87%\n",
      "Batch 10, Loss: 1.026139, Accuracy: 75.16%\n",
      "Batch 11, Loss: 0.912980, Accuracy: 75.99%\n",
      "Batch 12, Loss: 0.908722, Accuracy: 76.69%\n",
      "Batch 13, Loss: 0.998288, Accuracy: 76.56%\n",
      "Batch 14, Loss: 0.965799, Accuracy: 76.56%\n",
      "Batch 15, Loss: 1.037448, Accuracy: 76.15%\n",
      "Batch 16, Loss: 0.919394, Accuracy: 76.56%\n",
      "Batch 17, Loss: 0.896055, Accuracy: 77.02%\n",
      "Batch 18, Loss: 0.882496, Accuracy: 77.60%\n",
      "Batch 19, Loss: 1.045849, Accuracy: 77.14%\n",
      "Batch 20, Loss: 1.031552, Accuracy: 76.80%\n",
      "Batch 21, Loss: 1.006349, Accuracy: 76.64%\n",
      "Batch 22, Loss: 0.933382, Accuracy: 76.85%\n",
      "Batch 23, Loss: 0.955392, Accuracy: 76.97%\n",
      "Batch 24, Loss: 1.019448, Accuracy: 76.76%\n",
      "Batch 25, Loss: 1.007786, Accuracy: 76.62%\n",
      "Batch 26, Loss: 1.026520, Accuracy: 76.44%\n",
      "Batch 27, Loss: 0.969466, Accuracy: 76.56%\n",
      "Batch 28, Loss: 1.039867, Accuracy: 76.34%\n",
      "Batch 29, Loss: 0.997752, Accuracy: 76.29%\n",
      "Batch 30, Loss: 0.898578, Accuracy: 76.61%\n",
      "Batch 31, Loss: 0.941250, Accuracy: 76.76%\n",
      "Batch 32, Loss: 0.946682, Accuracy: 76.90%\n",
      "Batch 33, Loss: 1.010609, Accuracy: 76.75%\n",
      "Batch 34, Loss: 0.969807, Accuracy: 76.75%\n",
      "Batch 35, Loss: 0.940080, Accuracy: 76.83%\n",
      "Batch 36, Loss: 1.120883, Accuracy: 76.35%\n",
      "Batch 37, Loss: 1.009105, Accuracy: 76.31%\n",
      "Batch 38, Loss: 1.049962, Accuracy: 76.15%\n",
      "Batch 39, Loss: 0.995063, Accuracy: 76.08%\n",
      "Batch 40, Loss: 0.967675, Accuracy: 76.09%\n",
      "Batch 41, Loss: 0.993933, Accuracy: 76.07%\n",
      "Batch 42, Loss: 1.009565, Accuracy: 75.97%\n",
      "Batch 43, Loss: 0.909848, Accuracy: 76.16%\n",
      "Batch 44, Loss: 0.972563, Accuracy: 76.17%\n",
      "Batch 45, Loss: 0.951124, Accuracy: 76.25%\n",
      "Batch 46, Loss: 1.059438, Accuracy: 76.05%\n",
      "Batch 47, Loss: 0.914392, Accuracy: 76.20%\n",
      "Batch 48, Loss: 0.935060, Accuracy: 76.24%\n",
      "Batch 49, Loss: 0.959871, Accuracy: 76.34%\n",
      "Batch 50, Loss: 0.926455, Accuracy: 76.44%\n",
      "Batch 51, Loss: 0.924906, Accuracy: 76.50%\n",
      "Batch 52, Loss: 0.970253, Accuracy: 76.53%\n",
      "Batch 53, Loss: 0.951689, Accuracy: 76.62%\n",
      "Batch 54, Loss: 0.909506, Accuracy: 76.77%\n",
      "Batch 55, Loss: 1.007378, Accuracy: 76.76%\n",
      "Batch 56, Loss: 0.968473, Accuracy: 76.79%\n",
      "Batch 57, Loss: 0.896315, Accuracy: 76.95%\n",
      "Batch 58, Loss: 1.044761, Accuracy: 76.83%\n",
      "Batch 59, Loss: 1.060306, Accuracy: 76.67%\n",
      "Batch 60, Loss: 1.007175, Accuracy: 76.61%\n",
      "Batch 61, Loss: 1.001381, Accuracy: 76.56%\n",
      "Batch 62, Loss: 1.002307, Accuracy: 76.56%\n",
      "Batch 63, Loss: 1.059802, Accuracy: 76.41%\n",
      "Batch 64, Loss: 0.963636, Accuracy: 76.46%\n",
      "Batch 65, Loss: 1.030873, Accuracy: 76.35%\n",
      "Batch 66, Loss: 0.947480, Accuracy: 76.40%\n",
      "Batch 67, Loss: 0.912276, Accuracy: 76.49%\n",
      "Batch 68, Loss: 0.932057, Accuracy: 76.56%\n",
      "Batch 69, Loss: 0.971743, Accuracy: 76.59%\n",
      "Batch 70, Loss: 0.962565, Accuracy: 76.63%\n",
      "Batch 71, Loss: 0.922954, Accuracy: 76.72%\n",
      "Batch 72, Loss: 0.835365, Accuracy: 76.91%\n",
      "Batch 73, Loss: 0.914606, Accuracy: 76.99%\n",
      "Batch 74, Loss: 0.968637, Accuracy: 76.96%\n",
      "Batch 75, Loss: 0.990821, Accuracy: 76.94%\n",
      "Batch 76, Loss: 0.948668, Accuracy: 76.97%\n",
      "Batch 77, Loss: 1.146885, Accuracy: 76.72%\n",
      "Batch 78, Loss: 0.866465, Accuracy: 76.88%\n",
      "Batch 79, Loss: 0.935839, Accuracy: 76.92%\n",
      "Batch 80, Loss: 0.953611, Accuracy: 76.95%\n",
      "Batch 81, Loss: 0.966314, Accuracy: 76.97%\n",
      "Batch 82, Loss: 0.972353, Accuracy: 76.98%\n",
      "Batch 83, Loss: 0.862844, Accuracy: 77.11%\n",
      "Batch 84, Loss: 0.940228, Accuracy: 77.16%\n",
      "Batch 85, Loss: 0.991150, Accuracy: 77.13%\n",
      "Batch 86, Loss: 0.921310, Accuracy: 77.20%\n",
      "Batch 87, Loss: 0.939585, Accuracy: 77.23%\n",
      "Batch 88, Loss: 1.036970, Accuracy: 77.17%\n",
      "Batch 89, Loss: 0.932664, Accuracy: 77.19%\n",
      "Batch 90, Loss: 0.977466, Accuracy: 77.19%\n",
      "Batch 91, Loss: 1.021174, Accuracy: 77.13%\n",
      "Batch 92, Loss: 1.016207, Accuracy: 77.07%\n",
      "Batch 93, Loss: 0.897176, Accuracy: 77.17%\n",
      "Batch 94, Loss: 0.931026, Accuracy: 77.21%\n",
      "Batch 95, Loss: 0.905763, Accuracy: 77.27%\n",
      "Batch 96, Loss: 1.086699, Accuracy: 77.13%\n",
      "Batch 97, Loss: 0.991453, Accuracy: 77.11%\n",
      "Batch 98, Loss: 0.958696, Accuracy: 77.14%\n",
      "Batch 99, Loss: 1.022022, Accuracy: 77.08%\n",
      "Batch 100, Loss: 0.929523, Accuracy: 77.16%\n",
      "Batch 101, Loss: 0.908899, Accuracy: 77.24%\n",
      "Batch 102, Loss: 0.990515, Accuracy: 77.22%\n",
      "Batch 103, Loss: 1.022557, Accuracy: 77.18%\n",
      "Batch 104, Loss: 0.970825, Accuracy: 77.18%\n",
      "Batch 105, Loss: 0.900114, Accuracy: 77.26%\n",
      "Batch 106, Loss: 0.972526, Accuracy: 77.21%\n",
      "Batch 107, Loss: 1.016693, Accuracy: 77.16%\n",
      "Batch 108, Loss: 0.947191, Accuracy: 77.18%\n",
      "Batch 109, Loss: 1.040488, Accuracy: 77.11%\n",
      "Batch 110, Loss: 1.023342, Accuracy: 77.06%\n",
      "Batch 111, Loss: 0.920975, Accuracy: 77.11%\n",
      "Batch 112, Loss: 1.038801, Accuracy: 77.06%\n",
      "Batch 113, Loss: 0.997912, Accuracy: 77.05%\n",
      "Batch 114, Loss: 0.879442, Accuracy: 77.12%\n",
      "Batch 115, Loss: 0.942404, Accuracy: 77.16%\n",
      "Batch 116, Loss: 0.985942, Accuracy: 77.14%\n",
      "Batch 117, Loss: 0.950743, Accuracy: 77.15%\n",
      "Batch 118, Loss: 0.961611, Accuracy: 77.17%\n",
      "Batch 119, Loss: 1.081468, Accuracy: 77.07%\n",
      "Batch 120, Loss: 1.028140, Accuracy: 77.02%\n",
      "Batch 121, Loss: 0.951955, Accuracy: 77.03%\n",
      "Batch 122, Loss: 1.044290, Accuracy: 76.96%\n",
      "Batch 123, Loss: 1.056135, Accuracy: 76.85%\n",
      "Batch 124, Loss: 0.909686, Accuracy: 76.90%\n",
      "Batch 125, Loss: 0.957722, Accuracy: 76.91%\n",
      "Batch 126, Loss: 1.017159, Accuracy: 76.85%\n",
      "Batch 127, Loss: 1.039250, Accuracy: 76.81%\n",
      "Batch 128, Loss: 1.056486, Accuracy: 76.73%\n",
      "Batch 129, Loss: 0.982191, Accuracy: 76.72%\n",
      "Batch 130, Loss: 1.055254, Accuracy: 76.66%\n",
      "Batch 131, Loss: 1.072956, Accuracy: 76.59%\n",
      "Batch 132, Loss: 0.958829, Accuracy: 76.61%\n",
      "Batch 133, Loss: 0.939412, Accuracy: 76.63%\n",
      "Batch 134, Loss: 0.995397, Accuracy: 76.61%\n",
      "Batch 135, Loss: 0.922082, Accuracy: 76.64%\n",
      "Batch 136, Loss: 0.867702, Accuracy: 76.72%\n",
      "Batch 137, Loss: 0.910425, Accuracy: 76.77%\n",
      "Batch 138, Loss: 1.019002, Accuracy: 76.74%\n",
      "Batch 139, Loss: 0.937115, Accuracy: 76.78%\n",
      "Batch 140, Loss: 0.984247, Accuracy: 76.76%\n",
      "Batch 141, Loss: 0.982296, Accuracy: 76.77%\n",
      "Batch 142, Loss: 0.945096, Accuracy: 76.78%\n",
      "Batch 143, Loss: 0.997737, Accuracy: 76.75%\n",
      "Batch 144, Loss: 0.979979, Accuracy: 76.74%\n",
      "Batch 145, Loss: 0.956290, Accuracy: 76.75%\n",
      "Batch 146, Loss: 0.920294, Accuracy: 76.79%\n",
      "Batch 147, Loss: 0.966659, Accuracy: 76.80%\n",
      "Batch 148, Loss: 0.921461, Accuracy: 76.83%\n",
      "Batch 149, Loss: 0.992357, Accuracy: 76.80%\n",
      "Batch 150, Loss: 1.041499, Accuracy: 76.76%\n",
      "Batch 151, Loss: 1.053255, Accuracy: 76.70%\n",
      "Batch 152, Loss: 0.896062, Accuracy: 76.77%\n",
      "Batch 153, Loss: 0.946184, Accuracy: 76.80%\n",
      "Batch 154, Loss: 1.001907, Accuracy: 76.79%\n",
      "Batch 155, Loss: 0.950806, Accuracy: 76.79%\n",
      "Batch 156, Loss: 0.928159, Accuracy: 76.82%\n",
      "Batch 157, Loss: 0.934429, Accuracy: 76.84%\n",
      "Batch 158, Loss: 0.947361, Accuracy: 76.86%\n",
      "Batch 159, Loss: 1.153791, Accuracy: 76.73%\n",
      "Batch 160, Loss: 1.005405, Accuracy: 76.71%\n",
      "Batch 161, Loss: 0.971147, Accuracy: 76.71%\n",
      "Batch 162, Loss: 0.971733, Accuracy: 76.71%\n",
      "Batch 163, Loss: 0.926832, Accuracy: 76.75%\n",
      "Batch 164, Loss: 0.976358, Accuracy: 76.74%\n",
      "Batch 165, Loss: 0.912913, Accuracy: 76.79%\n",
      "Batch 166, Loss: 0.980828, Accuracy: 76.77%\n",
      "Batch 167, Loss: 1.016838, Accuracy: 76.74%\n",
      "Batch 168, Loss: 1.070074, Accuracy: 76.67%\n",
      "Batch 169, Loss: 0.931417, Accuracy: 76.68%\n",
      "Batch 170, Loss: 0.963004, Accuracy: 76.70%\n",
      "Batch 171, Loss: 0.969024, Accuracy: 76.71%\n",
      "Batch 172, Loss: 0.958186, Accuracy: 76.72%\n",
      "Batch 173, Loss: 0.994267, Accuracy: 76.71%\n",
      "Batch 174, Loss: 0.959693, Accuracy: 76.71%\n",
      "Batch 175, Loss: 1.005606, Accuracy: 76.68%\n",
      "Batch 176, Loss: 0.986125, Accuracy: 76.68%\n",
      "Batch 177, Loss: 0.952283, Accuracy: 76.67%\n",
      "Batch 178, Loss: 0.940284, Accuracy: 76.69%\n",
      "Batch 179, Loss: 0.957187, Accuracy: 76.69%\n",
      "Batch 180, Loss: 1.018244, Accuracy: 76.68%\n",
      "Batch 181, Loss: 1.027014, Accuracy: 76.65%\n",
      "Batch 182, Loss: 0.890827, Accuracy: 76.70%\n",
      "Batch 183, Loss: 0.943004, Accuracy: 76.71%\n",
      "Batch 184, Loss: 1.099278, Accuracy: 76.64%\n",
      "Batch 185, Loss: 0.979137, Accuracy: 76.63%\n",
      "Batch 186, Loss: 0.979527, Accuracy: 76.63%\n",
      "Batch 187, Loss: 0.943663, Accuracy: 76.64%\n",
      "Batch 188, Loss: 1.025761, Accuracy: 76.61%\n",
      "Batch 189, Loss: 0.990420, Accuracy: 76.61%\n",
      "Batch 190, Loss: 0.991035, Accuracy: 76.60%\n",
      "Batch 191, Loss: 0.959047, Accuracy: 76.60%\n",
      "Batch 192, Loss: 0.914533, Accuracy: 76.63%\n",
      "Batch 193, Loss: 0.991130, Accuracy: 76.63%\n",
      "Batch 194, Loss: 0.977443, Accuracy: 76.62%\n",
      "Batch 195, Loss: 0.939656, Accuracy: 76.65%\n",
      "Batch 196, Loss: 0.978661, Accuracy: 76.65%\n",
      "Batch 197, Loss: 0.930287, Accuracy: 76.68%\n",
      "Batch 198, Loss: 1.043839, Accuracy: 76.64%\n",
      "Batch 199, Loss: 0.983036, Accuracy: 76.63%\n",
      "Batch 200, Loss: 0.904538, Accuracy: 76.67%\n",
      "Batch 201, Loss: 0.972168, Accuracy: 76.68%\n",
      "Batch 202, Loss: 0.974220, Accuracy: 76.69%\n",
      "Batch 203, Loss: 0.949145, Accuracy: 76.71%\n",
      "Batch 204, Loss: 1.009400, Accuracy: 76.69%\n",
      "Batch 205, Loss: 0.980303, Accuracy: 76.68%\n",
      "Batch 206, Loss: 0.967712, Accuracy: 76.69%\n",
      "Batch 207, Loss: 1.027226, Accuracy: 76.65%\n",
      "Batch 208, Loss: 0.949499, Accuracy: 76.67%\n",
      "Batch 209, Loss: 1.003233, Accuracy: 76.64%\n",
      "Batch 210, Loss: 1.059021, Accuracy: 76.60%\n",
      "Batch 211, Loss: 0.972885, Accuracy: 76.61%\n",
      "Batch 212, Loss: 0.938384, Accuracy: 76.63%\n",
      "Batch 213, Loss: 0.969346, Accuracy: 76.63%\n",
      "Training - Epoch 78, Loss: 0.974991, Accuracy: 76.63%\n",
      "Validation Batch 1, Loss: 0.937843, Accuracy: 81.25%\n",
      "Validation Batch 2, Loss: 1.001394, Accuracy: 78.12%\n",
      "Validation Batch 3, Loss: 1.028626, Accuracy: 76.04%\n",
      "Validation Batch 4, Loss: 0.983590, Accuracy: 75.78%\n",
      "Validation Batch 5, Loss: 0.965158, Accuracy: 76.25%\n",
      "Validation Batch 6, Loss: 0.937990, Accuracy: 76.82%\n",
      "Validation Batch 7, Loss: 1.001652, Accuracy: 76.12%\n",
      "Validation Batch 8, Loss: 1.037974, Accuracy: 75.39%\n",
      "Validation Batch 9, Loss: 1.033845, Accuracy: 75.00%\n",
      "Validation Batch 10, Loss: 1.016459, Accuracy: 74.84%\n",
      "Validation Batch 11, Loss: 0.955627, Accuracy: 75.28%\n",
      "Validation Batch 12, Loss: 0.932129, Accuracy: 75.52%\n",
      "Validation Batch 13, Loss: 1.077818, Accuracy: 74.64%\n",
      "Validation Batch 14, Loss: 1.010443, Accuracy: 74.55%\n",
      "Validation Batch 15, Loss: 0.971673, Accuracy: 74.79%\n",
      "Validation Batch 16, Loss: 0.950744, Accuracy: 75.10%\n",
      "Validation Batch 17, Loss: 1.006813, Accuracy: 75.09%\n",
      "Validation Batch 18, Loss: 0.958863, Accuracy: 75.35%\n",
      "Validation Batch 19, Loss: 1.019581, Accuracy: 75.33%\n",
      "Validation Batch 20, Loss: 0.983046, Accuracy: 75.39%\n",
      "Validation Batch 21, Loss: 0.983848, Accuracy: 75.45%\n",
      "Validation Batch 22, Loss: 1.023132, Accuracy: 75.21%\n",
      "Validation Batch 23, Loss: 1.080713, Accuracy: 74.80%\n",
      "Validation Batch 24, Loss: 1.055622, Accuracy: 74.54%\n",
      "Validation Batch 25, Loss: 0.959970, Accuracy: 74.69%\n",
      "Validation Batch 26, Loss: 0.992508, Accuracy: 74.70%\n",
      "Validation Batch 27, Loss: 0.953238, Accuracy: 74.87%\n",
      "Validation - Epoch 78, Loss: 0.994826, Accuracy: 74.87%\n",
      "Patienceâ€”0\n",
      "Epoch 79\n",
      "Batch 1, Loss: 0.925660, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.954472, Accuracy: 82.03%\n",
      "Batch 3, Loss: 0.995766, Accuracy: 79.69%\n",
      "Batch 4, Loss: 0.962308, Accuracy: 78.52%\n",
      "Batch 5, Loss: 1.033066, Accuracy: 76.56%\n",
      "Batch 6, Loss: 0.930054, Accuracy: 77.60%\n",
      "Batch 7, Loss: 0.940507, Accuracy: 78.12%\n",
      "Batch 8, Loss: 0.997557, Accuracy: 77.34%\n",
      "Batch 9, Loss: 1.000262, Accuracy: 77.08%\n",
      "Batch 10, Loss: 0.996759, Accuracy: 76.88%\n",
      "Batch 11, Loss: 0.912965, Accuracy: 77.56%\n",
      "Batch 12, Loss: 1.045016, Accuracy: 76.69%\n",
      "Batch 13, Loss: 0.995960, Accuracy: 76.56%\n",
      "Batch 14, Loss: 1.027043, Accuracy: 76.23%\n",
      "Batch 15, Loss: 0.970060, Accuracy: 76.04%\n",
      "Batch 16, Loss: 0.979100, Accuracy: 75.98%\n",
      "Batch 17, Loss: 0.909396, Accuracy: 76.47%\n",
      "Batch 18, Loss: 0.984719, Accuracy: 76.48%\n",
      "Batch 19, Loss: 0.972167, Accuracy: 76.40%\n",
      "Batch 20, Loss: 1.046253, Accuracy: 76.02%\n",
      "Batch 21, Loss: 1.016786, Accuracy: 75.89%\n",
      "Batch 22, Loss: 1.076142, Accuracy: 75.50%\n",
      "Batch 23, Loss: 0.908448, Accuracy: 75.88%\n",
      "Batch 24, Loss: 1.045271, Accuracy: 75.59%\n",
      "Batch 25, Loss: 1.011128, Accuracy: 75.56%\n",
      "Batch 26, Loss: 0.984859, Accuracy: 75.66%\n",
      "Batch 27, Loss: 0.982747, Accuracy: 75.64%\n",
      "Batch 28, Loss: 0.997918, Accuracy: 75.56%\n",
      "Batch 29, Loss: 1.029356, Accuracy: 75.32%\n",
      "Batch 30, Loss: 1.025938, Accuracy: 75.21%\n",
      "Batch 31, Loss: 0.983925, Accuracy: 75.20%\n",
      "Batch 32, Loss: 0.982537, Accuracy: 75.24%\n",
      "Batch 33, Loss: 0.961559, Accuracy: 75.38%\n",
      "Batch 34, Loss: 0.981976, Accuracy: 75.41%\n",
      "Batch 35, Loss: 0.988138, Accuracy: 75.45%\n",
      "Batch 36, Loss: 0.954968, Accuracy: 75.52%\n",
      "Batch 37, Loss: 0.938629, Accuracy: 75.68%\n",
      "Batch 38, Loss: 1.068280, Accuracy: 75.45%\n",
      "Batch 39, Loss: 1.017673, Accuracy: 75.40%\n",
      "Batch 40, Loss: 0.935107, Accuracy: 75.55%\n",
      "Batch 41, Loss: 0.963661, Accuracy: 75.57%\n",
      "Batch 42, Loss: 1.008245, Accuracy: 75.48%\n",
      "Batch 43, Loss: 0.931461, Accuracy: 75.62%\n",
      "Batch 44, Loss: 1.011717, Accuracy: 75.53%\n",
      "Batch 45, Loss: 0.928901, Accuracy: 75.66%\n",
      "Batch 46, Loss: 1.009680, Accuracy: 75.58%\n",
      "Batch 47, Loss: 0.961905, Accuracy: 75.66%\n",
      "Batch 48, Loss: 0.928608, Accuracy: 75.85%\n",
      "Batch 49, Loss: 0.950192, Accuracy: 75.89%\n",
      "Batch 50, Loss: 0.977356, Accuracy: 75.91%\n",
      "Batch 51, Loss: 0.953036, Accuracy: 75.98%\n",
      "Batch 52, Loss: 0.972685, Accuracy: 75.96%\n",
      "Batch 53, Loss: 1.008862, Accuracy: 75.94%\n",
      "Batch 54, Loss: 0.943935, Accuracy: 76.01%\n",
      "Batch 55, Loss: 0.964389, Accuracy: 76.02%\n",
      "Batch 56, Loss: 0.971489, Accuracy: 76.00%\n",
      "Batch 57, Loss: 0.947461, Accuracy: 76.04%\n",
      "Batch 58, Loss: 1.002273, Accuracy: 76.00%\n",
      "Batch 59, Loss: 0.969516, Accuracy: 76.03%\n",
      "Batch 60, Loss: 0.993212, Accuracy: 76.02%\n",
      "Batch 61, Loss: 0.999677, Accuracy: 75.97%\n",
      "Batch 62, Loss: 0.987408, Accuracy: 75.96%\n",
      "Batch 63, Loss: 0.984672, Accuracy: 75.99%\n",
      "Batch 64, Loss: 0.941063, Accuracy: 76.07%\n",
      "Batch 65, Loss: 0.932175, Accuracy: 76.15%\n",
      "Batch 66, Loss: 0.908387, Accuracy: 76.25%\n",
      "Batch 67, Loss: 0.980510, Accuracy: 76.26%\n",
      "Batch 68, Loss: 0.919975, Accuracy: 76.36%\n",
      "Batch 69, Loss: 1.000121, Accuracy: 76.31%\n",
      "Batch 70, Loss: 1.033716, Accuracy: 76.21%\n",
      "Batch 71, Loss: 0.989735, Accuracy: 76.21%\n",
      "Batch 72, Loss: 1.028061, Accuracy: 76.13%\n",
      "Batch 73, Loss: 0.984575, Accuracy: 76.13%\n",
      "Batch 74, Loss: 0.980393, Accuracy: 76.14%\n",
      "Batch 75, Loss: 1.050771, Accuracy: 76.06%\n",
      "Batch 76, Loss: 0.978392, Accuracy: 76.05%\n",
      "Batch 77, Loss: 0.939189, Accuracy: 76.08%\n",
      "Batch 78, Loss: 0.943034, Accuracy: 76.10%\n",
      "Batch 79, Loss: 0.998982, Accuracy: 76.11%\n",
      "Batch 80, Loss: 0.961859, Accuracy: 76.15%\n",
      "Batch 81, Loss: 0.919930, Accuracy: 76.25%\n",
      "Batch 82, Loss: 1.079369, Accuracy: 76.12%\n",
      "Batch 83, Loss: 0.999511, Accuracy: 76.09%\n",
      "Batch 84, Loss: 0.871214, Accuracy: 76.23%\n",
      "Batch 85, Loss: 0.948197, Accuracy: 76.25%\n",
      "Batch 86, Loss: 0.918537, Accuracy: 76.31%\n",
      "Batch 87, Loss: 0.977959, Accuracy: 76.31%\n",
      "Batch 88, Loss: 0.954901, Accuracy: 76.35%\n",
      "Batch 89, Loss: 1.022899, Accuracy: 76.30%\n",
      "Batch 90, Loss: 0.993898, Accuracy: 76.30%\n",
      "Batch 91, Loss: 1.007528, Accuracy: 76.25%\n",
      "Batch 92, Loss: 0.959035, Accuracy: 76.29%\n",
      "Batch 93, Loss: 1.013708, Accuracy: 76.23%\n",
      "Batch 94, Loss: 0.925050, Accuracy: 76.30%\n",
      "Batch 95, Loss: 1.052079, Accuracy: 76.20%\n",
      "Batch 96, Loss: 0.971661, Accuracy: 76.20%\n",
      "Batch 97, Loss: 0.943091, Accuracy: 76.24%\n",
      "Batch 98, Loss: 1.005471, Accuracy: 76.21%\n",
      "Batch 99, Loss: 0.993040, Accuracy: 76.22%\n",
      "Batch 100, Loss: 0.988361, Accuracy: 76.20%\n",
      "Batch 101, Loss: 1.000992, Accuracy: 76.21%\n",
      "Batch 102, Loss: 0.939494, Accuracy: 76.24%\n",
      "Batch 103, Loss: 1.011414, Accuracy: 76.23%\n",
      "Batch 104, Loss: 1.062852, Accuracy: 76.14%\n",
      "Batch 105, Loss: 1.013837, Accuracy: 76.13%\n",
      "Batch 106, Loss: 1.018938, Accuracy: 76.09%\n",
      "Batch 107, Loss: 0.943679, Accuracy: 76.12%\n",
      "Batch 108, Loss: 1.038701, Accuracy: 76.07%\n",
      "Batch 109, Loss: 0.965639, Accuracy: 76.06%\n",
      "Batch 110, Loss: 0.881193, Accuracy: 76.14%\n",
      "Batch 111, Loss: 1.018416, Accuracy: 76.11%\n",
      "Batch 112, Loss: 1.038974, Accuracy: 76.03%\n",
      "Batch 113, Loss: 0.913585, Accuracy: 76.08%\n",
      "Batch 114, Loss: 0.985683, Accuracy: 76.07%\n",
      "Batch 115, Loss: 0.943130, Accuracy: 76.10%\n",
      "Batch 116, Loss: 0.944978, Accuracy: 76.12%\n",
      "Batch 117, Loss: 0.983578, Accuracy: 76.12%\n",
      "Batch 118, Loss: 0.933668, Accuracy: 76.18%\n",
      "Batch 119, Loss: 0.962550, Accuracy: 76.19%\n",
      "Batch 120, Loss: 0.972813, Accuracy: 76.18%\n",
      "Batch 121, Loss: 0.897875, Accuracy: 76.27%\n",
      "Batch 122, Loss: 1.088094, Accuracy: 76.17%\n",
      "Batch 123, Loss: 0.954422, Accuracy: 76.18%\n",
      "Batch 124, Loss: 0.927574, Accuracy: 76.23%\n",
      "Batch 125, Loss: 0.948348, Accuracy: 76.25%\n",
      "Batch 126, Loss: 0.943407, Accuracy: 76.29%\n",
      "Batch 127, Loss: 0.884234, Accuracy: 76.38%\n",
      "Batch 128, Loss: 0.903647, Accuracy: 76.46%\n",
      "Batch 129, Loss: 0.899222, Accuracy: 76.51%\n",
      "Batch 130, Loss: 0.981215, Accuracy: 76.51%\n",
      "Batch 131, Loss: 1.051787, Accuracy: 76.44%\n",
      "Batch 132, Loss: 0.992796, Accuracy: 76.43%\n",
      "Batch 133, Loss: 0.881883, Accuracy: 76.50%\n",
      "Batch 134, Loss: 0.999216, Accuracy: 76.48%\n",
      "Batch 135, Loss: 0.971108, Accuracy: 76.49%\n",
      "Batch 136, Loss: 0.976780, Accuracy: 76.49%\n",
      "Batch 137, Loss: 0.967262, Accuracy: 76.52%\n",
      "Batch 138, Loss: 1.056846, Accuracy: 76.46%\n",
      "Batch 139, Loss: 0.985724, Accuracy: 76.44%\n",
      "Batch 140, Loss: 0.990597, Accuracy: 76.42%\n",
      "Batch 141, Loss: 1.020714, Accuracy: 76.39%\n",
      "Batch 142, Loss: 0.984253, Accuracy: 76.39%\n",
      "Batch 143, Loss: 0.937382, Accuracy: 76.41%\n",
      "Batch 144, Loss: 0.871008, Accuracy: 76.50%\n",
      "Batch 145, Loss: 1.014700, Accuracy: 76.48%\n",
      "Batch 146, Loss: 1.007271, Accuracy: 76.44%\n",
      "Batch 147, Loss: 0.891806, Accuracy: 76.51%\n",
      "Batch 148, Loss: 0.853520, Accuracy: 76.58%\n",
      "Batch 149, Loss: 1.020260, Accuracy: 76.54%\n",
      "Batch 150, Loss: 0.931016, Accuracy: 76.57%\n",
      "Batch 151, Loss: 0.983402, Accuracy: 76.55%\n",
      "Batch 152, Loss: 0.957224, Accuracy: 76.56%\n",
      "Batch 153, Loss: 0.999618, Accuracy: 76.55%\n",
      "Batch 154, Loss: 1.042644, Accuracy: 76.51%\n",
      "Batch 155, Loss: 0.920467, Accuracy: 76.55%\n",
      "Batch 156, Loss: 0.997162, Accuracy: 76.52%\n",
      "Batch 157, Loss: 0.910411, Accuracy: 76.57%\n",
      "Batch 158, Loss: 0.950573, Accuracy: 76.59%\n",
      "Batch 159, Loss: 0.985909, Accuracy: 76.58%\n",
      "Batch 160, Loss: 0.950463, Accuracy: 76.59%\n",
      "Batch 161, Loss: 0.928722, Accuracy: 76.62%\n",
      "Batch 162, Loss: 0.970662, Accuracy: 76.63%\n",
      "Batch 163, Loss: 0.966806, Accuracy: 76.63%\n",
      "Batch 164, Loss: 0.947996, Accuracy: 76.66%\n",
      "Batch 165, Loss: 0.993437, Accuracy: 76.64%\n",
      "Batch 166, Loss: 0.988246, Accuracy: 76.63%\n",
      "Batch 167, Loss: 1.004408, Accuracy: 76.62%\n",
      "Batch 168, Loss: 1.071992, Accuracy: 76.56%\n",
      "Batch 169, Loss: 0.959443, Accuracy: 76.56%\n",
      "Batch 170, Loss: 0.882266, Accuracy: 76.63%\n",
      "Batch 171, Loss: 0.958962, Accuracy: 76.64%\n",
      "Batch 172, Loss: 1.020782, Accuracy: 76.61%\n",
      "Batch 173, Loss: 0.989026, Accuracy: 76.61%\n",
      "Batch 174, Loss: 0.956156, Accuracy: 76.62%\n",
      "Batch 175, Loss: 1.013709, Accuracy: 76.60%\n",
      "Batch 176, Loss: 0.968672, Accuracy: 76.61%\n",
      "Batch 177, Loss: 0.976153, Accuracy: 76.59%\n",
      "Batch 178, Loss: 0.935883, Accuracy: 76.62%\n",
      "Batch 179, Loss: 0.960529, Accuracy: 76.63%\n",
      "Batch 180, Loss: 0.966627, Accuracy: 76.63%\n",
      "Batch 181, Loss: 0.946304, Accuracy: 76.65%\n",
      "Batch 182, Loss: 0.934746, Accuracy: 76.68%\n",
      "Batch 183, Loss: 1.035769, Accuracy: 76.64%\n",
      "Batch 184, Loss: 0.932361, Accuracy: 76.66%\n",
      "Batch 185, Loss: 0.930632, Accuracy: 76.70%\n",
      "Batch 186, Loss: 0.950864, Accuracy: 76.71%\n",
      "Batch 187, Loss: 0.905669, Accuracy: 76.75%\n",
      "Batch 188, Loss: 0.938170, Accuracy: 76.78%\n",
      "Batch 189, Loss: 1.057942, Accuracy: 76.74%\n",
      "Batch 190, Loss: 0.884400, Accuracy: 76.78%\n",
      "Batch 191, Loss: 0.871524, Accuracy: 76.84%\n",
      "Batch 192, Loss: 0.945769, Accuracy: 76.85%\n",
      "Batch 193, Loss: 0.889585, Accuracy: 76.90%\n",
      "Batch 194, Loss: 1.030017, Accuracy: 76.87%\n",
      "Batch 195, Loss: 0.949151, Accuracy: 76.88%\n",
      "Batch 196, Loss: 1.006756, Accuracy: 76.87%\n",
      "Batch 197, Loss: 0.945663, Accuracy: 76.88%\n",
      "Batch 198, Loss: 0.943556, Accuracy: 76.91%\n",
      "Batch 199, Loss: 0.930227, Accuracy: 76.93%\n",
      "Batch 200, Loss: 0.943582, Accuracy: 76.95%\n",
      "Batch 201, Loss: 0.908613, Accuracy: 76.99%\n",
      "Batch 202, Loss: 0.931021, Accuracy: 77.00%\n",
      "Batch 203, Loss: 1.098434, Accuracy: 76.94%\n",
      "Batch 204, Loss: 1.034279, Accuracy: 76.91%\n",
      "Batch 205, Loss: 0.989485, Accuracy: 76.90%\n",
      "Batch 206, Loss: 1.085732, Accuracy: 76.84%\n",
      "Batch 207, Loss: 0.993801, Accuracy: 76.83%\n",
      "Batch 208, Loss: 0.970508, Accuracy: 76.85%\n",
      "Batch 209, Loss: 1.045711, Accuracy: 76.80%\n",
      "Batch 210, Loss: 0.970824, Accuracy: 76.80%\n",
      "Batch 211, Loss: 0.925716, Accuracy: 76.83%\n",
      "Batch 212, Loss: 1.033957, Accuracy: 76.80%\n",
      "Batch 213, Loss: 1.015363, Accuracy: 76.77%\n",
      "Training - Epoch 79, Loss: 0.974177, Accuracy: 76.77%\n",
      "Validation Batch 1, Loss: 0.944779, Accuracy: 79.69%\n",
      "Validation Batch 2, Loss: 0.968449, Accuracy: 78.91%\n",
      "Validation Batch 3, Loss: 1.013839, Accuracy: 77.08%\n",
      "Validation Batch 4, Loss: 0.967676, Accuracy: 77.34%\n",
      "Validation Batch 5, Loss: 0.935251, Accuracy: 78.12%\n",
      "Validation Batch 6, Loss: 0.889442, Accuracy: 79.43%\n",
      "Validation Batch 7, Loss: 0.977460, Accuracy: 79.02%\n",
      "Validation Batch 8, Loss: 0.993627, Accuracy: 78.52%\n",
      "Validation Batch 9, Loss: 1.013240, Accuracy: 77.95%\n",
      "Validation Batch 10, Loss: 0.983135, Accuracy: 77.81%\n",
      "Validation Batch 11, Loss: 0.931719, Accuracy: 78.12%\n",
      "Validation Batch 12, Loss: 0.917061, Accuracy: 78.39%\n",
      "Validation Batch 13, Loss: 1.044126, Accuracy: 77.64%\n",
      "Validation Batch 14, Loss: 0.996957, Accuracy: 77.46%\n",
      "Validation Batch 15, Loss: 0.974599, Accuracy: 77.40%\n",
      "Validation Batch 16, Loss: 0.931470, Accuracy: 77.73%\n",
      "Validation Batch 17, Loss: 0.969196, Accuracy: 77.85%\n",
      "Validation Batch 18, Loss: 0.972447, Accuracy: 77.95%\n",
      "Validation Batch 19, Loss: 0.993510, Accuracy: 77.80%\n",
      "Validation Batch 20, Loss: 0.972943, Accuracy: 77.66%\n",
      "Validation Batch 21, Loss: 0.959999, Accuracy: 77.75%\n",
      "Validation Batch 22, Loss: 0.992047, Accuracy: 77.63%\n",
      "Validation Batch 23, Loss: 1.061256, Accuracy: 77.17%\n",
      "Validation Batch 24, Loss: 1.048205, Accuracy: 76.82%\n",
      "Validation Batch 25, Loss: 0.940004, Accuracy: 76.94%\n",
      "Validation Batch 26, Loss: 0.992056, Accuracy: 76.92%\n",
      "Validation Batch 27, Loss: 0.945052, Accuracy: 76.98%\n",
      "Validation - Epoch 79, Loss: 0.975168, Accuracy: 76.98%\n",
      "Patienceâ€”0\n",
      "Epoch 80\n",
      "Batch 1, Loss: 0.908923, Accuracy: 84.38%\n",
      "Batch 2, Loss: 0.956189, Accuracy: 82.03%\n",
      "Batch 3, Loss: 0.981288, Accuracy: 79.69%\n",
      "Batch 4, Loss: 0.998861, Accuracy: 77.73%\n",
      "Batch 5, Loss: 0.968302, Accuracy: 77.50%\n",
      "Batch 6, Loss: 0.939578, Accuracy: 77.60%\n",
      "Batch 7, Loss: 1.060972, Accuracy: 76.34%\n",
      "Batch 8, Loss: 0.923345, Accuracy: 76.95%\n",
      "Batch 9, Loss: 0.970519, Accuracy: 76.74%\n",
      "Batch 10, Loss: 1.045642, Accuracy: 76.09%\n",
      "Batch 11, Loss: 0.951015, Accuracy: 76.42%\n",
      "Batch 12, Loss: 0.975199, Accuracy: 76.43%\n",
      "Batch 13, Loss: 0.958610, Accuracy: 76.56%\n",
      "Batch 14, Loss: 0.973387, Accuracy: 76.67%\n",
      "Batch 15, Loss: 0.959643, Accuracy: 76.67%\n",
      "Batch 16, Loss: 0.987832, Accuracy: 76.46%\n",
      "Batch 17, Loss: 0.893858, Accuracy: 76.93%\n",
      "Batch 18, Loss: 1.048271, Accuracy: 76.48%\n",
      "Batch 19, Loss: 0.967762, Accuracy: 76.56%\n",
      "Batch 20, Loss: 0.962130, Accuracy: 76.64%\n",
      "Batch 21, Loss: 0.937329, Accuracy: 76.86%\n",
      "Batch 22, Loss: 1.038985, Accuracy: 76.56%\n",
      "Batch 23, Loss: 1.027990, Accuracy: 76.29%\n",
      "Batch 24, Loss: 0.956688, Accuracy: 76.37%\n",
      "Batch 25, Loss: 0.973021, Accuracy: 76.50%\n",
      "Batch 26, Loss: 0.971928, Accuracy: 76.56%\n",
      "Batch 27, Loss: 1.054811, Accuracy: 76.27%\n",
      "Batch 28, Loss: 1.008434, Accuracy: 76.17%\n",
      "Batch 29, Loss: 0.993095, Accuracy: 76.13%\n",
      "Batch 30, Loss: 0.968345, Accuracy: 76.15%\n",
      "Batch 31, Loss: 0.977869, Accuracy: 76.16%\n",
      "Batch 32, Loss: 1.008169, Accuracy: 76.12%\n",
      "Batch 33, Loss: 0.976821, Accuracy: 76.14%\n",
      "Batch 34, Loss: 0.938237, Accuracy: 76.24%\n",
      "Batch 35, Loss: 1.001287, Accuracy: 76.21%\n",
      "Batch 36, Loss: 0.982282, Accuracy: 76.26%\n",
      "Batch 37, Loss: 0.954681, Accuracy: 76.35%\n",
      "Batch 38, Loss: 1.000377, Accuracy: 76.23%\n",
      "Batch 39, Loss: 0.919708, Accuracy: 76.36%\n",
      "Batch 40, Loss: 0.921362, Accuracy: 76.48%\n",
      "Batch 41, Loss: 0.860198, Accuracy: 76.79%\n",
      "Batch 42, Loss: 0.977123, Accuracy: 76.75%\n",
      "Batch 43, Loss: 0.994258, Accuracy: 76.71%\n",
      "Batch 44, Loss: 0.994118, Accuracy: 76.63%\n",
      "Batch 45, Loss: 0.945917, Accuracy: 76.70%\n",
      "Batch 46, Loss: 0.946522, Accuracy: 76.73%\n",
      "Batch 47, Loss: 1.026040, Accuracy: 76.63%\n",
      "Batch 48, Loss: 0.959531, Accuracy: 76.63%\n",
      "Batch 49, Loss: 1.011902, Accuracy: 76.53%\n",
      "Batch 50, Loss: 0.895929, Accuracy: 76.69%\n",
      "Batch 51, Loss: 0.915101, Accuracy: 76.84%\n",
      "Batch 52, Loss: 1.057007, Accuracy: 76.68%\n",
      "Batch 53, Loss: 0.956734, Accuracy: 76.71%\n",
      "Batch 54, Loss: 1.052155, Accuracy: 76.56%\n",
      "Batch 55, Loss: 1.001415, Accuracy: 76.51%\n",
      "Batch 56, Loss: 0.975927, Accuracy: 76.51%\n",
      "Batch 57, Loss: 0.976518, Accuracy: 76.48%\n",
      "Batch 58, Loss: 0.886971, Accuracy: 76.64%\n",
      "Batch 59, Loss: 0.954974, Accuracy: 76.64%\n",
      "Batch 60, Loss: 0.909681, Accuracy: 76.77%\n",
      "Batch 61, Loss: 0.991506, Accuracy: 76.72%\n",
      "Batch 62, Loss: 1.060280, Accuracy: 76.56%\n",
      "Batch 63, Loss: 0.950542, Accuracy: 76.64%\n",
      "Batch 64, Loss: 0.919260, Accuracy: 76.76%\n",
      "Batch 65, Loss: 1.001276, Accuracy: 76.73%\n",
      "Batch 66, Loss: 0.975235, Accuracy: 76.70%\n",
      "Batch 67, Loss: 0.905419, Accuracy: 76.84%\n",
      "Batch 68, Loss: 0.987284, Accuracy: 76.82%\n",
      "Batch 69, Loss: 0.930815, Accuracy: 76.90%\n",
      "Batch 70, Loss: 1.013121, Accuracy: 76.83%\n",
      "Batch 71, Loss: 0.973507, Accuracy: 76.83%\n",
      "Batch 72, Loss: 1.092268, Accuracy: 76.67%\n",
      "Batch 73, Loss: 0.950387, Accuracy: 76.71%\n",
      "Batch 74, Loss: 0.926672, Accuracy: 76.79%\n",
      "Batch 75, Loss: 0.991345, Accuracy: 76.75%\n",
      "Batch 76, Loss: 1.019518, Accuracy: 76.67%\n",
      "Batch 77, Loss: 1.009813, Accuracy: 76.60%\n",
      "Batch 78, Loss: 1.027563, Accuracy: 76.52%\n",
      "Batch 79, Loss: 0.926337, Accuracy: 76.58%\n",
      "Batch 80, Loss: 0.927722, Accuracy: 76.64%\n",
      "Batch 81, Loss: 1.002000, Accuracy: 76.60%\n",
      "Batch 82, Loss: 0.916012, Accuracy: 76.70%\n",
      "Batch 83, Loss: 0.955086, Accuracy: 76.71%\n",
      "Batch 84, Loss: 0.967747, Accuracy: 76.73%\n",
      "Batch 85, Loss: 1.010387, Accuracy: 76.69%\n",
      "Batch 86, Loss: 0.957991, Accuracy: 76.69%\n",
      "Batch 87, Loss: 0.994013, Accuracy: 76.67%\n",
      "Batch 88, Loss: 0.965726, Accuracy: 76.69%\n",
      "Batch 89, Loss: 0.940117, Accuracy: 76.72%\n",
      "Batch 90, Loss: 0.897939, Accuracy: 76.82%\n",
      "Batch 91, Loss: 0.983356, Accuracy: 76.82%\n",
      "Batch 92, Loss: 0.926766, Accuracy: 76.89%\n",
      "Batch 93, Loss: 0.969114, Accuracy: 76.88%\n",
      "Batch 94, Loss: 0.896500, Accuracy: 76.98%\n",
      "Batch 95, Loss: 0.994307, Accuracy: 76.96%\n",
      "Batch 96, Loss: 0.980268, Accuracy: 76.97%\n",
      "Batch 97, Loss: 0.974092, Accuracy: 76.97%\n",
      "Batch 98, Loss: 0.915732, Accuracy: 77.01%\n",
      "Batch 99, Loss: 0.914891, Accuracy: 77.05%\n",
      "Batch 100, Loss: 1.017470, Accuracy: 77.02%\n",
      "Batch 101, Loss: 1.007322, Accuracy: 77.00%\n",
      "Batch 102, Loss: 0.966795, Accuracy: 77.01%\n",
      "Batch 103, Loss: 0.899413, Accuracy: 77.09%\n",
      "Batch 104, Loss: 0.990398, Accuracy: 77.06%\n",
      "Batch 105, Loss: 1.060806, Accuracy: 76.96%\n",
      "Batch 106, Loss: 1.015450, Accuracy: 76.92%\n",
      "Batch 107, Loss: 1.005417, Accuracy: 76.88%\n",
      "Batch 108, Loss: 1.001002, Accuracy: 76.84%\n",
      "Batch 109, Loss: 0.887113, Accuracy: 76.92%\n",
      "Batch 110, Loss: 0.930111, Accuracy: 76.97%\n",
      "Batch 111, Loss: 0.928457, Accuracy: 77.03%\n",
      "Batch 112, Loss: 0.926299, Accuracy: 77.06%\n",
      "Batch 113, Loss: 0.982961, Accuracy: 77.03%\n",
      "Batch 114, Loss: 0.981059, Accuracy: 77.01%\n",
      "Batch 115, Loss: 0.919479, Accuracy: 77.08%\n",
      "Batch 116, Loss: 1.029868, Accuracy: 77.03%\n",
      "Batch 117, Loss: 1.054605, Accuracy: 76.95%\n",
      "Batch 118, Loss: 1.003855, Accuracy: 76.91%\n",
      "Batch 119, Loss: 0.964289, Accuracy: 76.90%\n",
      "Batch 120, Loss: 1.049866, Accuracy: 76.82%\n",
      "Batch 121, Loss: 0.962488, Accuracy: 76.85%\n",
      "Batch 122, Loss: 1.033173, Accuracy: 76.81%\n",
      "Batch 123, Loss: 0.889375, Accuracy: 76.88%\n",
      "Batch 124, Loss: 0.960581, Accuracy: 76.88%\n",
      "Batch 125, Loss: 0.954822, Accuracy: 76.89%\n",
      "Batch 126, Loss: 0.888238, Accuracy: 76.98%\n",
      "Batch 127, Loss: 0.887955, Accuracy: 77.05%\n",
      "Batch 128, Loss: 1.015832, Accuracy: 77.00%\n",
      "Batch 129, Loss: 0.987017, Accuracy: 77.01%\n",
      "Batch 130, Loss: 0.960929, Accuracy: 77.03%\n",
      "Batch 131, Loss: 0.971769, Accuracy: 77.03%\n",
      "Batch 132, Loss: 1.000318, Accuracy: 77.00%\n",
      "Batch 133, Loss: 0.980635, Accuracy: 76.99%\n",
      "Batch 134, Loss: 0.987719, Accuracy: 76.97%\n",
      "Batch 135, Loss: 0.969140, Accuracy: 76.97%\n",
      "Batch 136, Loss: 0.808936, Accuracy: 77.09%\n",
      "Batch 137, Loss: 0.987798, Accuracy: 77.08%\n",
      "Batch 138, Loss: 0.944913, Accuracy: 77.08%\n",
      "Batch 139, Loss: 0.970491, Accuracy: 77.10%\n",
      "Batch 140, Loss: 0.962930, Accuracy: 77.12%\n",
      "Batch 141, Loss: 0.978970, Accuracy: 77.13%\n",
      "Batch 142, Loss: 0.937830, Accuracy: 77.16%\n",
      "Batch 143, Loss: 0.988326, Accuracy: 77.13%\n",
      "Batch 144, Loss: 0.927228, Accuracy: 77.16%\n",
      "Batch 145, Loss: 0.947456, Accuracy: 77.19%\n",
      "Batch 146, Loss: 0.899716, Accuracy: 77.24%\n",
      "Batch 147, Loss: 1.084211, Accuracy: 77.14%\n",
      "Batch 148, Loss: 0.885402, Accuracy: 77.20%\n",
      "Batch 149, Loss: 0.972538, Accuracy: 77.19%\n",
      "Batch 150, Loss: 1.028846, Accuracy: 77.15%\n",
      "Batch 151, Loss: 0.981607, Accuracy: 77.14%\n",
      "Batch 152, Loss: 0.968727, Accuracy: 77.13%\n",
      "Batch 153, Loss: 0.970557, Accuracy: 77.13%\n",
      "Batch 154, Loss: 0.951409, Accuracy: 77.15%\n",
      "Batch 155, Loss: 0.993012, Accuracy: 77.12%\n",
      "Batch 156, Loss: 1.014596, Accuracy: 77.07%\n",
      "Batch 157, Loss: 0.996855, Accuracy: 77.06%\n",
      "Batch 158, Loss: 0.958762, Accuracy: 77.07%\n",
      "Batch 159, Loss: 0.934007, Accuracy: 77.08%\n",
      "Batch 160, Loss: 0.959816, Accuracy: 77.10%\n",
      "Batch 161, Loss: 1.037786, Accuracy: 77.05%\n",
      "Batch 162, Loss: 1.006805, Accuracy: 77.03%\n",
      "Batch 163, Loss: 0.966631, Accuracy: 77.04%\n",
      "Batch 164, Loss: 0.995682, Accuracy: 77.03%\n",
      "Batch 165, Loss: 0.989334, Accuracy: 77.00%\n",
      "Batch 166, Loss: 1.103183, Accuracy: 76.92%\n",
      "Batch 167, Loss: 1.006194, Accuracy: 76.91%\n",
      "Batch 168, Loss: 0.997736, Accuracy: 76.89%\n",
      "Batch 169, Loss: 0.994715, Accuracy: 76.89%\n",
      "Batch 170, Loss: 0.931421, Accuracy: 76.91%\n",
      "Batch 171, Loss: 0.967880, Accuracy: 76.91%\n",
      "Batch 172, Loss: 0.968342, Accuracy: 76.91%\n",
      "Batch 173, Loss: 1.014611, Accuracy: 76.88%\n",
      "Batch 174, Loss: 0.978243, Accuracy: 76.87%\n",
      "Batch 175, Loss: 0.913537, Accuracy: 76.89%\n",
      "Batch 176, Loss: 0.884207, Accuracy: 76.95%\n",
      "Batch 177, Loss: 0.984355, Accuracy: 76.94%\n",
      "Batch 178, Loss: 0.922797, Accuracy: 76.97%\n",
      "Batch 179, Loss: 0.940730, Accuracy: 76.99%\n",
      "Batch 180, Loss: 0.995474, Accuracy: 76.98%\n",
      "Batch 181, Loss: 1.026232, Accuracy: 76.95%\n",
      "Batch 182, Loss: 0.925401, Accuracy: 76.97%\n",
      "Batch 183, Loss: 0.890778, Accuracy: 77.02%\n",
      "Batch 184, Loss: 0.999707, Accuracy: 77.00%\n",
      "Batch 185, Loss: 0.925951, Accuracy: 77.02%\n",
      "Batch 186, Loss: 1.009267, Accuracy: 76.99%\n",
      "Batch 187, Loss: 1.067535, Accuracy: 76.95%\n",
      "Batch 188, Loss: 1.005545, Accuracy: 76.93%\n",
      "Batch 189, Loss: 1.130983, Accuracy: 76.84%\n",
      "Batch 190, Loss: 0.944197, Accuracy: 76.84%\n",
      "Batch 191, Loss: 0.876094, Accuracy: 76.91%\n",
      "Batch 192, Loss: 0.941800, Accuracy: 76.91%\n",
      "Batch 193, Loss: 1.018677, Accuracy: 76.88%\n",
      "Batch 194, Loss: 0.992573, Accuracy: 76.87%\n",
      "Batch 195, Loss: 0.914597, Accuracy: 76.90%\n",
      "Batch 196, Loss: 0.982349, Accuracy: 76.89%\n",
      "Batch 197, Loss: 0.998451, Accuracy: 76.89%\n",
      "Batch 198, Loss: 0.988806, Accuracy: 76.88%\n",
      "Batch 199, Loss: 0.974937, Accuracy: 76.89%\n",
      "Batch 200, Loss: 0.902586, Accuracy: 76.94%\n",
      "Batch 201, Loss: 0.918176, Accuracy: 76.97%\n",
      "Batch 202, Loss: 0.968980, Accuracy: 76.99%\n",
      "Batch 203, Loss: 0.917618, Accuracy: 77.02%\n",
      "Batch 204, Loss: 0.976662, Accuracy: 77.03%\n",
      "Batch 205, Loss: 0.965096, Accuracy: 77.04%\n",
      "Batch 206, Loss: 0.943001, Accuracy: 77.05%\n",
      "Batch 207, Loss: 0.993736, Accuracy: 77.04%\n",
      "Batch 208, Loss: 0.880341, Accuracy: 77.08%\n",
      "Batch 209, Loss: 0.974855, Accuracy: 77.09%\n",
      "Batch 210, Loss: 1.066830, Accuracy: 77.05%\n",
      "Batch 211, Loss: 0.923063, Accuracy: 77.07%\n",
      "Batch 212, Loss: 0.919794, Accuracy: 77.09%\n",
      "Batch 213, Loss: 0.957767, Accuracy: 77.09%\n",
      "Training - Epoch 80, Loss: 0.970571, Accuracy: 77.09%\n",
      "Validation Batch 1, Loss: 0.934868, Accuracy: 82.81%\n",
      "Validation Batch 2, Loss: 1.000584, Accuracy: 78.91%\n",
      "Validation Batch 3, Loss: 1.024937, Accuracy: 76.56%\n",
      "Validation Batch 4, Loss: 0.971334, Accuracy: 76.56%\n",
      "Validation Batch 5, Loss: 0.955125, Accuracy: 77.50%\n",
      "Validation Batch 6, Loss: 0.938409, Accuracy: 77.86%\n",
      "Validation Batch 7, Loss: 1.006111, Accuracy: 77.01%\n",
      "Validation Batch 8, Loss: 1.040182, Accuracy: 76.17%\n",
      "Validation Batch 9, Loss: 1.035162, Accuracy: 75.52%\n",
      "Validation Batch 10, Loss: 1.011785, Accuracy: 75.31%\n",
      "Validation Batch 11, Loss: 0.958605, Accuracy: 75.71%\n",
      "Validation Batch 12, Loss: 0.928555, Accuracy: 75.91%\n",
      "Validation Batch 13, Loss: 1.079034, Accuracy: 75.00%\n",
      "Validation Batch 14, Loss: 1.008005, Accuracy: 74.89%\n",
      "Validation Batch 15, Loss: 0.973894, Accuracy: 75.00%\n",
      "Validation Batch 16, Loss: 0.949763, Accuracy: 75.29%\n",
      "Validation Batch 17, Loss: 1.010154, Accuracy: 75.28%\n",
      "Validation Batch 18, Loss: 0.965015, Accuracy: 75.52%\n",
      "Validation Batch 19, Loss: 1.019360, Accuracy: 75.41%\n",
      "Validation Batch 20, Loss: 0.982276, Accuracy: 75.47%\n",
      "Validation Batch 21, Loss: 0.985148, Accuracy: 75.52%\n",
      "Validation Batch 22, Loss: 1.028015, Accuracy: 75.21%\n",
      "Validation Batch 23, Loss: 1.071404, Accuracy: 74.86%\n",
      "Validation Batch 24, Loss: 1.045364, Accuracy: 74.67%\n",
      "Validation Batch 25, Loss: 0.957949, Accuracy: 74.75%\n",
      "Validation Batch 26, Loss: 0.988776, Accuracy: 74.76%\n",
      "Validation Batch 27, Loss: 0.950262, Accuracy: 74.93%\n",
      "Validation - Epoch 80, Loss: 0.993336, Accuracy: 74.93%\n",
      "Patienceâ€”1\n",
      "Epoch 81\n",
      "Batch 1, Loss: 0.987768, Accuracy: 75.00%\n",
      "Batch 2, Loss: 0.920554, Accuracy: 78.91%\n",
      "Batch 3, Loss: 0.941424, Accuracy: 79.17%\n",
      "Batch 4, Loss: 0.900025, Accuracy: 80.47%\n",
      "Batch 5, Loss: 0.920605, Accuracy: 80.94%\n",
      "Batch 6, Loss: 0.965730, Accuracy: 80.73%\n",
      "Batch 7, Loss: 0.957608, Accuracy: 80.58%\n",
      "Batch 8, Loss: 0.977211, Accuracy: 79.88%\n",
      "Batch 9, Loss: 0.967084, Accuracy: 79.51%\n",
      "Batch 10, Loss: 0.947307, Accuracy: 79.53%\n",
      "Batch 11, Loss: 0.974436, Accuracy: 79.26%\n",
      "Batch 12, Loss: 0.891507, Accuracy: 79.82%\n",
      "Batch 13, Loss: 0.944859, Accuracy: 79.81%\n",
      "Batch 14, Loss: 1.024503, Accuracy: 79.13%\n",
      "Batch 15, Loss: 0.989705, Accuracy: 78.85%\n",
      "Batch 16, Loss: 0.949615, Accuracy: 78.81%\n",
      "Batch 17, Loss: 0.974726, Accuracy: 78.68%\n",
      "Batch 18, Loss: 0.935898, Accuracy: 78.82%\n",
      "Batch 19, Loss: 0.920338, Accuracy: 78.95%\n",
      "Batch 20, Loss: 0.924105, Accuracy: 79.06%\n",
      "Batch 21, Loss: 1.013299, Accuracy: 78.72%\n",
      "Batch 22, Loss: 0.964392, Accuracy: 78.62%\n",
      "Batch 23, Loss: 0.941253, Accuracy: 78.67%\n",
      "Batch 24, Loss: 0.938936, Accuracy: 78.78%\n",
      "Batch 25, Loss: 0.882093, Accuracy: 79.12%\n",
      "Batch 26, Loss: 0.985183, Accuracy: 79.03%\n",
      "Batch 27, Loss: 1.012631, Accuracy: 78.76%\n",
      "Batch 28, Loss: 0.888094, Accuracy: 79.02%\n",
      "Batch 29, Loss: 0.941950, Accuracy: 79.04%\n",
      "Batch 30, Loss: 0.924946, Accuracy: 79.11%\n",
      "Batch 31, Loss: 0.926053, Accuracy: 79.23%\n",
      "Batch 32, Loss: 1.090665, Accuracy: 78.76%\n",
      "Batch 33, Loss: 1.057319, Accuracy: 78.41%\n",
      "Batch 34, Loss: 0.885284, Accuracy: 78.68%\n",
      "Batch 35, Loss: 1.024497, Accuracy: 78.44%\n",
      "Batch 36, Loss: 0.995353, Accuracy: 78.34%\n",
      "Batch 37, Loss: 1.047449, Accuracy: 78.08%\n",
      "Batch 38, Loss: 1.108108, Accuracy: 77.67%\n",
      "Batch 39, Loss: 0.953198, Accuracy: 77.72%\n",
      "Batch 40, Loss: 0.982070, Accuracy: 77.66%\n",
      "Batch 41, Loss: 0.935446, Accuracy: 77.74%\n",
      "Batch 42, Loss: 0.908479, Accuracy: 77.90%\n",
      "Batch 43, Loss: 0.967715, Accuracy: 77.83%\n",
      "Batch 44, Loss: 1.009920, Accuracy: 77.73%\n",
      "Batch 45, Loss: 0.963098, Accuracy: 77.74%\n",
      "Batch 46, Loss: 0.931085, Accuracy: 77.82%\n",
      "Batch 47, Loss: 1.077808, Accuracy: 77.56%\n",
      "Batch 48, Loss: 0.940187, Accuracy: 77.57%\n",
      "Batch 49, Loss: 0.952251, Accuracy: 77.58%\n",
      "Batch 50, Loss: 0.995498, Accuracy: 77.53%\n",
      "Batch 51, Loss: 0.965795, Accuracy: 77.48%\n",
      "Batch 52, Loss: 0.950864, Accuracy: 77.52%\n",
      "Batch 53, Loss: 0.928106, Accuracy: 77.59%\n",
      "Batch 54, Loss: 0.951658, Accuracy: 77.63%\n",
      "Batch 55, Loss: 1.030437, Accuracy: 77.53%\n",
      "Batch 56, Loss: 0.918007, Accuracy: 77.59%\n",
      "Batch 57, Loss: 1.005125, Accuracy: 77.52%\n",
      "Batch 58, Loss: 0.918928, Accuracy: 77.61%\n",
      "Batch 59, Loss: 0.965088, Accuracy: 77.62%\n",
      "Batch 60, Loss: 1.025648, Accuracy: 77.53%\n",
      "Batch 61, Loss: 0.901815, Accuracy: 77.66%\n",
      "Batch 62, Loss: 0.929173, Accuracy: 77.72%\n",
      "Batch 63, Loss: 0.955981, Accuracy: 77.73%\n",
      "Batch 64, Loss: 0.908514, Accuracy: 77.83%\n",
      "Batch 65, Loss: 0.905115, Accuracy: 77.91%\n",
      "Batch 66, Loss: 1.120668, Accuracy: 77.63%\n",
      "Batch 67, Loss: 0.979571, Accuracy: 77.64%\n",
      "Batch 68, Loss: 0.966680, Accuracy: 77.62%\n",
      "Batch 69, Loss: 0.968983, Accuracy: 77.63%\n",
      "Batch 70, Loss: 0.987386, Accuracy: 77.59%\n",
      "Batch 71, Loss: 0.978478, Accuracy: 77.55%\n",
      "Batch 72, Loss: 0.896336, Accuracy: 77.67%\n",
      "Batch 73, Loss: 1.029241, Accuracy: 77.57%\n",
      "Batch 74, Loss: 0.877951, Accuracy: 77.72%\n",
      "Batch 75, Loss: 0.955064, Accuracy: 77.75%\n",
      "Batch 76, Loss: 0.930520, Accuracy: 77.80%\n",
      "Batch 77, Loss: 0.943691, Accuracy: 77.84%\n",
      "Batch 78, Loss: 0.989010, Accuracy: 77.78%\n",
      "Batch 79, Loss: 0.923893, Accuracy: 77.85%\n",
      "Batch 80, Loss: 0.940858, Accuracy: 77.89%\n",
      "Batch 81, Loss: 0.983630, Accuracy: 77.87%\n",
      "Batch 82, Loss: 0.932598, Accuracy: 77.90%\n",
      "Batch 83, Loss: 0.975477, Accuracy: 77.90%\n",
      "Batch 84, Loss: 0.963642, Accuracy: 77.90%\n",
      "Batch 85, Loss: 0.984516, Accuracy: 77.87%\n",
      "Batch 86, Loss: 0.955609, Accuracy: 77.87%\n",
      "Batch 87, Loss: 0.982964, Accuracy: 77.86%\n",
      "Batch 88, Loss: 0.927363, Accuracy: 77.89%\n",
      "Batch 89, Loss: 0.980073, Accuracy: 77.86%\n",
      "Batch 90, Loss: 0.895302, Accuracy: 77.93%\n",
      "Batch 91, Loss: 0.975680, Accuracy: 77.94%\n",
      "Batch 92, Loss: 0.936295, Accuracy: 77.97%\n",
      "Batch 93, Loss: 0.963241, Accuracy: 77.96%\n",
      "Batch 94, Loss: 0.980519, Accuracy: 77.93%\n",
      "Batch 95, Loss: 0.872977, Accuracy: 78.01%\n",
      "Batch 96, Loss: 1.014427, Accuracy: 77.96%\n",
      "Batch 97, Loss: 1.016476, Accuracy: 77.88%\n",
      "Batch 98, Loss: 1.006229, Accuracy: 77.84%\n",
      "Batch 99, Loss: 0.924401, Accuracy: 77.89%\n",
      "Batch 100, Loss: 1.040366, Accuracy: 77.81%\n",
      "Batch 101, Loss: 0.966756, Accuracy: 77.82%\n",
      "Batch 102, Loss: 0.984951, Accuracy: 77.82%\n",
      "Batch 103, Loss: 0.981575, Accuracy: 77.81%\n",
      "Batch 104, Loss: 1.066748, Accuracy: 77.69%\n",
      "Batch 105, Loss: 1.014128, Accuracy: 77.65%\n",
      "Batch 106, Loss: 1.001782, Accuracy: 77.64%\n",
      "Batch 107, Loss: 1.010219, Accuracy: 77.60%\n",
      "Batch 108, Loss: 0.907254, Accuracy: 77.66%\n",
      "Batch 109, Loss: 0.976157, Accuracy: 77.65%\n",
      "Batch 110, Loss: 0.946194, Accuracy: 77.67%\n",
      "Batch 111, Loss: 0.955722, Accuracy: 77.67%\n",
      "Batch 112, Loss: 0.999739, Accuracy: 77.66%\n",
      "Batch 113, Loss: 1.036841, Accuracy: 77.61%\n",
      "Batch 114, Loss: 0.899369, Accuracy: 77.67%\n",
      "Batch 115, Loss: 0.963475, Accuracy: 77.66%\n",
      "Batch 116, Loss: 0.932495, Accuracy: 77.69%\n",
      "Batch 117, Loss: 0.919864, Accuracy: 77.74%\n",
      "Batch 118, Loss: 0.937498, Accuracy: 77.77%\n",
      "Batch 119, Loss: 0.925623, Accuracy: 77.78%\n",
      "Batch 120, Loss: 0.893641, Accuracy: 77.85%\n",
      "Batch 121, Loss: 0.946246, Accuracy: 77.87%\n",
      "Batch 122, Loss: 0.947714, Accuracy: 77.88%\n",
      "Batch 123, Loss: 0.967978, Accuracy: 77.90%\n",
      "Batch 124, Loss: 0.984283, Accuracy: 77.86%\n",
      "Batch 125, Loss: 1.005470, Accuracy: 77.83%\n",
      "Batch 126, Loss: 1.035912, Accuracy: 77.78%\n",
      "Batch 127, Loss: 0.966956, Accuracy: 77.79%\n",
      "Batch 128, Loss: 0.963238, Accuracy: 77.80%\n",
      "Batch 129, Loss: 0.988663, Accuracy: 77.76%\n",
      "Batch 130, Loss: 0.874359, Accuracy: 77.84%\n",
      "Batch 131, Loss: 1.071169, Accuracy: 77.74%\n",
      "Batch 132, Loss: 0.895437, Accuracy: 77.79%\n",
      "Batch 133, Loss: 0.962794, Accuracy: 77.78%\n",
      "Batch 134, Loss: 0.899232, Accuracy: 77.82%\n",
      "Batch 135, Loss: 1.031146, Accuracy: 77.78%\n",
      "Batch 136, Loss: 0.922553, Accuracy: 77.81%\n",
      "Batch 137, Loss: 0.987735, Accuracy: 77.78%\n",
      "Batch 138, Loss: 0.994902, Accuracy: 77.75%\n",
      "Batch 139, Loss: 0.968105, Accuracy: 77.77%\n",
      "Batch 140, Loss: 1.020375, Accuracy: 77.72%\n",
      "Batch 141, Loss: 0.984982, Accuracy: 77.69%\n",
      "Batch 142, Loss: 1.024260, Accuracy: 77.64%\n",
      "Batch 143, Loss: 0.980950, Accuracy: 77.61%\n",
      "Batch 144, Loss: 0.920303, Accuracy: 77.65%\n",
      "Batch 145, Loss: 0.991047, Accuracy: 77.61%\n",
      "Batch 146, Loss: 0.979476, Accuracy: 77.59%\n",
      "Batch 147, Loss: 0.932200, Accuracy: 77.61%\n",
      "Batch 148, Loss: 1.055443, Accuracy: 77.55%\n",
      "Batch 149, Loss: 0.937954, Accuracy: 77.57%\n",
      "Batch 150, Loss: 0.912827, Accuracy: 77.60%\n",
      "Batch 151, Loss: 0.883213, Accuracy: 77.65%\n",
      "Batch 152, Loss: 1.073251, Accuracy: 77.57%\n",
      "Batch 153, Loss: 0.972383, Accuracy: 77.56%\n",
      "Batch 154, Loss: 0.984155, Accuracy: 77.57%\n",
      "Batch 155, Loss: 0.979491, Accuracy: 77.56%\n",
      "Batch 156, Loss: 0.916690, Accuracy: 77.60%\n",
      "Batch 157, Loss: 0.981568, Accuracy: 77.59%\n",
      "Batch 158, Loss: 0.984235, Accuracy: 77.58%\n",
      "Batch 159, Loss: 1.039826, Accuracy: 77.54%\n",
      "Batch 160, Loss: 0.931281, Accuracy: 77.56%\n",
      "Batch 161, Loss: 1.010319, Accuracy: 77.52%\n",
      "Batch 162, Loss: 0.899186, Accuracy: 77.58%\n",
      "Batch 163, Loss: 0.948048, Accuracy: 77.59%\n",
      "Batch 164, Loss: 1.031911, Accuracy: 77.54%\n",
      "Batch 165, Loss: 1.022713, Accuracy: 77.51%\n",
      "Batch 166, Loss: 0.986072, Accuracy: 77.51%\n",
      "Batch 167, Loss: 0.957054, Accuracy: 77.52%\n",
      "Batch 168, Loss: 1.007875, Accuracy: 77.49%\n",
      "Batch 169, Loss: 0.974339, Accuracy: 77.50%\n",
      "Batch 170, Loss: 0.994759, Accuracy: 77.49%\n",
      "Batch 171, Loss: 0.926372, Accuracy: 77.52%\n",
      "Batch 172, Loss: 0.984721, Accuracy: 77.52%\n",
      "Batch 173, Loss: 0.958473, Accuracy: 77.52%\n",
      "Batch 174, Loss: 1.096947, Accuracy: 77.45%\n",
      "Batch 175, Loss: 0.990474, Accuracy: 77.42%\n",
      "Batch 176, Loss: 1.028308, Accuracy: 77.39%\n",
      "Batch 177, Loss: 0.980944, Accuracy: 77.38%\n",
      "Batch 178, Loss: 0.931098, Accuracy: 77.41%\n",
      "Batch 179, Loss: 0.951426, Accuracy: 77.42%\n",
      "Batch 180, Loss: 0.983458, Accuracy: 77.40%\n",
      "Batch 181, Loss: 1.018661, Accuracy: 77.37%\n",
      "Batch 182, Loss: 0.943552, Accuracy: 77.39%\n",
      "Batch 183, Loss: 0.847744, Accuracy: 77.46%\n",
      "Batch 184, Loss: 1.035056, Accuracy: 77.42%\n",
      "Batch 185, Loss: 0.996041, Accuracy: 77.41%\n",
      "Batch 186, Loss: 0.970131, Accuracy: 77.41%\n",
      "Batch 187, Loss: 0.968135, Accuracy: 77.40%\n",
      "Batch 188, Loss: 1.006628, Accuracy: 77.39%\n",
      "Batch 189, Loss: 1.006616, Accuracy: 77.36%\n",
      "Batch 190, Loss: 0.996309, Accuracy: 77.35%\n",
      "Batch 191, Loss: 0.981810, Accuracy: 77.35%\n",
      "Batch 192, Loss: 0.977087, Accuracy: 77.34%\n",
      "Batch 193, Loss: 0.996073, Accuracy: 77.33%\n",
      "Batch 194, Loss: 0.946194, Accuracy: 77.35%\n",
      "Batch 195, Loss: 0.989180, Accuracy: 77.34%\n",
      "Batch 196, Loss: 1.008342, Accuracy: 77.31%\n",
      "Batch 197, Loss: 0.971098, Accuracy: 77.31%\n",
      "Batch 198, Loss: 0.975978, Accuracy: 77.30%\n",
      "Batch 199, Loss: 0.965904, Accuracy: 77.31%\n",
      "Batch 200, Loss: 0.979957, Accuracy: 77.30%\n",
      "Batch 201, Loss: 0.964287, Accuracy: 77.31%\n",
      "Batch 202, Loss: 1.007453, Accuracy: 77.28%\n",
      "Batch 203, Loss: 1.038373, Accuracy: 77.24%\n",
      "Batch 204, Loss: 0.920045, Accuracy: 77.27%\n",
      "Batch 205, Loss: 0.952663, Accuracy: 77.27%\n",
      "Batch 206, Loss: 1.044664, Accuracy: 77.23%\n",
      "Batch 207, Loss: 0.955397, Accuracy: 77.23%\n",
      "Batch 208, Loss: 1.036826, Accuracy: 77.20%\n",
      "Batch 209, Loss: 0.911491, Accuracy: 77.24%\n",
      "Batch 210, Loss: 0.906550, Accuracy: 77.26%\n",
      "Batch 211, Loss: 1.013556, Accuracy: 77.24%\n",
      "Batch 212, Loss: 0.937394, Accuracy: 77.26%\n",
      "Batch 213, Loss: 1.002891, Accuracy: 77.26%\n",
      "Training - Epoch 81, Loss: 0.969371, Accuracy: 77.26%\n",
      "Validation Batch 1, Loss: 0.965733, Accuracy: 76.56%\n",
      "Validation Batch 2, Loss: 1.010205, Accuracy: 75.00%\n",
      "Validation Batch 3, Loss: 1.033511, Accuracy: 73.96%\n",
      "Validation Batch 4, Loss: 0.995407, Accuracy: 73.05%\n",
      "Validation Batch 5, Loss: 0.959402, Accuracy: 74.38%\n",
      "Validation Batch 6, Loss: 0.920088, Accuracy: 75.78%\n",
      "Validation Batch 7, Loss: 1.002438, Accuracy: 75.67%\n",
      "Validation Batch 8, Loss: 1.032309, Accuracy: 74.80%\n",
      "Validation Batch 9, Loss: 1.039077, Accuracy: 74.13%\n",
      "Validation Batch 10, Loss: 1.009711, Accuracy: 74.06%\n",
      "Validation Batch 11, Loss: 0.958376, Accuracy: 74.72%\n",
      "Validation Batch 12, Loss: 0.942870, Accuracy: 75.13%\n",
      "Validation Batch 13, Loss: 1.075624, Accuracy: 74.52%\n",
      "Validation Batch 14, Loss: 1.027519, Accuracy: 74.22%\n",
      "Validation Batch 15, Loss: 0.982645, Accuracy: 74.38%\n",
      "Validation Batch 16, Loss: 0.959720, Accuracy: 74.80%\n",
      "Validation Batch 17, Loss: 1.008687, Accuracy: 74.82%\n",
      "Validation Batch 18, Loss: 0.986740, Accuracy: 74.83%\n",
      "Validation Batch 19, Loss: 1.031821, Accuracy: 74.59%\n",
      "Validation Batch 20, Loss: 1.000877, Accuracy: 74.53%\n",
      "Validation Batch 21, Loss: 0.997660, Accuracy: 74.48%\n",
      "Validation Batch 22, Loss: 1.038492, Accuracy: 74.36%\n",
      "Validation Batch 23, Loss: 1.085978, Accuracy: 73.98%\n",
      "Validation Batch 24, Loss: 1.063915, Accuracy: 73.70%\n",
      "Validation Batch 25, Loss: 0.971612, Accuracy: 73.94%\n",
      "Validation Batch 26, Loss: 1.003281, Accuracy: 73.92%\n",
      "Validation Batch 27, Loss: 0.960326, Accuracy: 74.10%\n",
      "Validation - Epoch 81, Loss: 1.002371, Accuracy: 74.10%\n",
      "Patienceâ€”2\n",
      "Epoch 82\n",
      "Batch 1, Loss: 1.064533, Accuracy: 68.75%\n",
      "Batch 2, Loss: 0.919362, Accuracy: 75.00%\n",
      "Batch 3, Loss: 0.986866, Accuracy: 75.52%\n",
      "Batch 4, Loss: 0.949734, Accuracy: 76.17%\n",
      "Batch 5, Loss: 0.917153, Accuracy: 77.50%\n",
      "Batch 6, Loss: 1.023037, Accuracy: 76.56%\n",
      "Batch 7, Loss: 0.939219, Accuracy: 77.46%\n",
      "Batch 8, Loss: 0.985079, Accuracy: 77.34%\n",
      "Batch 9, Loss: 0.989356, Accuracy: 77.08%\n",
      "Batch 10, Loss: 1.050327, Accuracy: 76.41%\n",
      "Batch 11, Loss: 0.925490, Accuracy: 76.85%\n",
      "Batch 12, Loss: 1.045805, Accuracy: 76.17%\n",
      "Batch 13, Loss: 1.003795, Accuracy: 75.96%\n",
      "Batch 14, Loss: 0.999969, Accuracy: 75.67%\n",
      "Batch 15, Loss: 1.072855, Accuracy: 75.10%\n",
      "Batch 16, Loss: 1.033409, Accuracy: 74.90%\n",
      "Batch 17, Loss: 0.992103, Accuracy: 74.82%\n",
      "Batch 18, Loss: 0.971794, Accuracy: 74.91%\n",
      "Batch 19, Loss: 0.892020, Accuracy: 75.41%\n",
      "Batch 20, Loss: 1.025192, Accuracy: 75.16%\n",
      "Batch 21, Loss: 0.939543, Accuracy: 75.45%\n",
      "Batch 22, Loss: 0.910716, Accuracy: 75.78%\n",
      "Batch 23, Loss: 0.964997, Accuracy: 75.88%\n",
      "Batch 24, Loss: 1.008811, Accuracy: 75.78%\n",
      "Batch 25, Loss: 0.930543, Accuracy: 76.00%\n",
      "Batch 26, Loss: 0.881620, Accuracy: 76.38%\n",
      "Batch 27, Loss: 0.963554, Accuracy: 76.45%\n",
      "Batch 28, Loss: 0.914106, Accuracy: 76.62%\n",
      "Batch 29, Loss: 1.041760, Accuracy: 76.40%\n",
      "Batch 30, Loss: 1.043549, Accuracy: 76.20%\n",
      "Batch 31, Loss: 0.965935, Accuracy: 76.16%\n",
      "Batch 32, Loss: 0.951410, Accuracy: 76.27%\n",
      "Batch 33, Loss: 0.994034, Accuracy: 76.23%\n",
      "Batch 34, Loss: 0.938459, Accuracy: 76.38%\n",
      "Batch 35, Loss: 0.949634, Accuracy: 76.43%\n",
      "Batch 36, Loss: 0.962193, Accuracy: 76.43%\n",
      "Batch 37, Loss: 1.072683, Accuracy: 76.18%\n",
      "Batch 38, Loss: 0.911067, Accuracy: 76.44%\n",
      "Batch 39, Loss: 0.942116, Accuracy: 76.52%\n",
      "Batch 40, Loss: 0.955865, Accuracy: 76.60%\n",
      "Batch 41, Loss: 0.919770, Accuracy: 76.79%\n",
      "Batch 42, Loss: 0.935468, Accuracy: 76.86%\n",
      "Batch 43, Loss: 0.931384, Accuracy: 76.96%\n",
      "Batch 44, Loss: 0.950386, Accuracy: 77.02%\n",
      "Batch 45, Loss: 1.004242, Accuracy: 76.91%\n",
      "Batch 46, Loss: 1.014601, Accuracy: 76.83%\n",
      "Batch 47, Loss: 0.998734, Accuracy: 76.76%\n",
      "Batch 48, Loss: 0.914867, Accuracy: 76.92%\n",
      "Batch 49, Loss: 1.021258, Accuracy: 76.85%\n",
      "Batch 50, Loss: 0.890852, Accuracy: 77.00%\n",
      "Batch 51, Loss: 1.014392, Accuracy: 76.90%\n",
      "Batch 52, Loss: 1.024397, Accuracy: 76.77%\n",
      "Batch 53, Loss: 0.919986, Accuracy: 76.89%\n",
      "Batch 54, Loss: 0.979079, Accuracy: 76.85%\n",
      "Batch 55, Loss: 0.965458, Accuracy: 76.85%\n",
      "Batch 56, Loss: 1.036684, Accuracy: 76.73%\n",
      "Batch 57, Loss: 1.004401, Accuracy: 76.67%\n",
      "Batch 58, Loss: 0.891342, Accuracy: 76.80%\n",
      "Batch 59, Loss: 1.092470, Accuracy: 76.59%\n",
      "Batch 60, Loss: 0.963905, Accuracy: 76.61%\n",
      "Batch 61, Loss: 0.910780, Accuracy: 76.74%\n",
      "Batch 62, Loss: 0.927539, Accuracy: 76.81%\n",
      "Batch 63, Loss: 0.915432, Accuracy: 76.91%\n",
      "Batch 64, Loss: 1.061827, Accuracy: 76.76%\n",
      "Batch 65, Loss: 1.013408, Accuracy: 76.68%\n",
      "Batch 66, Loss: 0.918931, Accuracy: 76.80%\n",
      "Batch 67, Loss: 1.020876, Accuracy: 76.73%\n",
      "Batch 68, Loss: 0.933073, Accuracy: 76.79%\n",
      "Batch 69, Loss: 0.943928, Accuracy: 76.86%\n",
      "Batch 70, Loss: 0.975078, Accuracy: 76.85%\n",
      "Batch 71, Loss: 0.984871, Accuracy: 76.85%\n",
      "Batch 72, Loss: 0.973130, Accuracy: 76.84%\n",
      "Batch 73, Loss: 1.033985, Accuracy: 76.73%\n",
      "Batch 74, Loss: 1.012982, Accuracy: 76.65%\n",
      "Batch 75, Loss: 0.929286, Accuracy: 76.71%\n",
      "Batch 76, Loss: 0.964893, Accuracy: 76.69%\n",
      "Batch 77, Loss: 0.955674, Accuracy: 76.70%\n",
      "Batch 78, Loss: 0.966051, Accuracy: 76.68%\n",
      "Batch 79, Loss: 0.966803, Accuracy: 76.68%\n",
      "Batch 80, Loss: 0.888450, Accuracy: 76.80%\n",
      "Batch 81, Loss: 0.958781, Accuracy: 76.81%\n",
      "Batch 82, Loss: 0.938127, Accuracy: 76.83%\n",
      "Batch 83, Loss: 0.950514, Accuracy: 76.84%\n",
      "Batch 84, Loss: 0.998573, Accuracy: 76.82%\n",
      "Batch 85, Loss: 0.962323, Accuracy: 76.84%\n",
      "Batch 86, Loss: 0.893005, Accuracy: 76.94%\n",
      "Batch 87, Loss: 0.909133, Accuracy: 77.01%\n",
      "Batch 88, Loss: 0.970979, Accuracy: 77.02%\n",
      "Batch 89, Loss: 0.913304, Accuracy: 77.11%\n",
      "Batch 90, Loss: 0.949601, Accuracy: 77.15%\n",
      "Batch 91, Loss: 0.910836, Accuracy: 77.23%\n",
      "Batch 92, Loss: 0.960175, Accuracy: 77.24%\n",
      "Batch 93, Loss: 1.024805, Accuracy: 77.17%\n",
      "Batch 94, Loss: 1.044232, Accuracy: 77.09%\n",
      "Batch 95, Loss: 0.993531, Accuracy: 77.06%\n",
      "Batch 96, Loss: 1.025767, Accuracy: 76.99%\n",
      "Batch 97, Loss: 0.958009, Accuracy: 77.01%\n",
      "Batch 98, Loss: 0.919499, Accuracy: 77.09%\n",
      "Batch 99, Loss: 1.039882, Accuracy: 77.00%\n",
      "Batch 100, Loss: 0.968302, Accuracy: 77.02%\n",
      "Batch 101, Loss: 0.948267, Accuracy: 77.04%\n",
      "Batch 102, Loss: 0.937235, Accuracy: 77.08%\n",
      "Batch 103, Loss: 1.019386, Accuracy: 77.02%\n",
      "Batch 104, Loss: 1.092742, Accuracy: 76.89%\n",
      "Batch 105, Loss: 0.943954, Accuracy: 76.93%\n",
      "Batch 106, Loss: 0.951115, Accuracy: 76.96%\n",
      "Batch 107, Loss: 0.952494, Accuracy: 76.99%\n",
      "Batch 108, Loss: 0.990939, Accuracy: 76.95%\n",
      "Batch 109, Loss: 0.889611, Accuracy: 77.02%\n",
      "Batch 110, Loss: 0.979188, Accuracy: 77.03%\n",
      "Batch 111, Loss: 0.975265, Accuracy: 77.03%\n",
      "Batch 112, Loss: 0.928728, Accuracy: 77.06%\n",
      "Batch 113, Loss: 0.932553, Accuracy: 77.10%\n",
      "Batch 114, Loss: 0.966570, Accuracy: 77.08%\n",
      "Batch 115, Loss: 0.938205, Accuracy: 77.11%\n",
      "Batch 116, Loss: 0.856273, Accuracy: 77.22%\n",
      "Batch 117, Loss: 0.972478, Accuracy: 77.23%\n",
      "Batch 118, Loss: 0.962241, Accuracy: 77.25%\n",
      "Batch 119, Loss: 0.992657, Accuracy: 77.23%\n",
      "Batch 120, Loss: 0.991021, Accuracy: 77.20%\n",
      "Batch 121, Loss: 0.940949, Accuracy: 77.23%\n",
      "Batch 122, Loss: 0.931834, Accuracy: 77.28%\n",
      "Batch 123, Loss: 0.980294, Accuracy: 77.29%\n",
      "Batch 124, Loss: 0.901472, Accuracy: 77.34%\n",
      "Batch 125, Loss: 1.013250, Accuracy: 77.29%\n",
      "Batch 126, Loss: 0.948930, Accuracy: 77.31%\n",
      "Batch 127, Loss: 0.991234, Accuracy: 77.29%\n",
      "Batch 128, Loss: 0.969676, Accuracy: 77.29%\n",
      "Batch 129, Loss: 0.988122, Accuracy: 77.28%\n",
      "Batch 130, Loss: 1.005129, Accuracy: 77.25%\n",
      "Batch 131, Loss: 0.904876, Accuracy: 77.29%\n",
      "Batch 132, Loss: 1.019717, Accuracy: 77.26%\n",
      "Batch 133, Loss: 0.988313, Accuracy: 77.22%\n",
      "Batch 134, Loss: 0.960236, Accuracy: 77.23%\n",
      "Batch 135, Loss: 0.920862, Accuracy: 77.26%\n",
      "Batch 136, Loss: 0.913818, Accuracy: 77.30%\n",
      "Batch 137, Loss: 0.919509, Accuracy: 77.33%\n",
      "Batch 138, Loss: 0.985102, Accuracy: 77.31%\n",
      "Batch 139, Loss: 1.021074, Accuracy: 77.26%\n",
      "Batch 140, Loss: 1.050541, Accuracy: 77.19%\n",
      "Batch 141, Loss: 0.949128, Accuracy: 77.21%\n",
      "Batch 142, Loss: 0.946768, Accuracy: 77.24%\n",
      "Batch 143, Loss: 0.954573, Accuracy: 77.25%\n",
      "Batch 144, Loss: 0.954985, Accuracy: 77.26%\n",
      "Batch 145, Loss: 1.085323, Accuracy: 77.17%\n",
      "Batch 146, Loss: 0.922197, Accuracy: 77.19%\n",
      "Batch 147, Loss: 0.918080, Accuracy: 77.23%\n",
      "Batch 148, Loss: 0.833531, Accuracy: 77.33%\n",
      "Batch 149, Loss: 1.116296, Accuracy: 77.22%\n",
      "Batch 150, Loss: 0.941394, Accuracy: 77.24%\n",
      "Batch 151, Loss: 0.962074, Accuracy: 77.25%\n",
      "Batch 152, Loss: 0.925013, Accuracy: 77.28%\n",
      "Batch 153, Loss: 0.920467, Accuracy: 77.32%\n",
      "Batch 154, Loss: 0.983589, Accuracy: 77.30%\n",
      "Batch 155, Loss: 0.904375, Accuracy: 77.34%\n",
      "Batch 156, Loss: 0.910717, Accuracy: 77.37%\n",
      "Batch 157, Loss: 0.909901, Accuracy: 77.41%\n",
      "Batch 158, Loss: 0.949527, Accuracy: 77.41%\n",
      "Batch 159, Loss: 0.931999, Accuracy: 77.43%\n",
      "Batch 160, Loss: 0.979993, Accuracy: 77.42%\n",
      "Batch 161, Loss: 0.933845, Accuracy: 77.44%\n",
      "Batch 162, Loss: 0.894312, Accuracy: 77.48%\n",
      "Batch 163, Loss: 0.998572, Accuracy: 77.46%\n",
      "Batch 164, Loss: 0.949318, Accuracy: 77.47%\n",
      "Batch 165, Loss: 0.986606, Accuracy: 77.43%\n",
      "Batch 166, Loss: 0.958692, Accuracy: 77.46%\n",
      "Batch 167, Loss: 0.962828, Accuracy: 77.47%\n",
      "Batch 168, Loss: 0.988675, Accuracy: 77.46%\n",
      "Batch 169, Loss: 0.910906, Accuracy: 77.50%\n",
      "Batch 170, Loss: 1.040256, Accuracy: 77.44%\n",
      "Batch 171, Loss: 1.039226, Accuracy: 77.41%\n",
      "Batch 172, Loss: 1.031904, Accuracy: 77.36%\n",
      "Batch 173, Loss: 1.000717, Accuracy: 77.35%\n",
      "Batch 174, Loss: 0.950207, Accuracy: 77.35%\n",
      "Batch 175, Loss: 1.001646, Accuracy: 77.34%\n",
      "Batch 176, Loss: 1.059029, Accuracy: 77.29%\n",
      "Batch 177, Loss: 1.013150, Accuracy: 77.27%\n",
      "Batch 178, Loss: 0.929107, Accuracy: 77.30%\n",
      "Batch 179, Loss: 0.970572, Accuracy: 77.30%\n",
      "Batch 180, Loss: 0.978184, Accuracy: 77.30%\n",
      "Batch 181, Loss: 0.941141, Accuracy: 77.31%\n",
      "Batch 182, Loss: 0.989290, Accuracy: 77.31%\n",
      "Batch 183, Loss: 1.051283, Accuracy: 77.25%\n",
      "Batch 184, Loss: 0.933675, Accuracy: 77.26%\n",
      "Batch 185, Loss: 1.042130, Accuracy: 77.21%\n",
      "Batch 186, Loss: 1.015265, Accuracy: 77.18%\n",
      "Batch 187, Loss: 0.912152, Accuracy: 77.21%\n",
      "Batch 188, Loss: 0.943329, Accuracy: 77.22%\n",
      "Batch 189, Loss: 0.946989, Accuracy: 77.24%\n",
      "Batch 190, Loss: 1.077098, Accuracy: 77.18%\n",
      "Batch 191, Loss: 1.024428, Accuracy: 77.15%\n",
      "Batch 192, Loss: 0.978609, Accuracy: 77.16%\n",
      "Batch 193, Loss: 0.964765, Accuracy: 77.15%\n",
      "Batch 194, Loss: 0.938333, Accuracy: 77.17%\n",
      "Batch 195, Loss: 0.931582, Accuracy: 77.21%\n",
      "Batch 196, Loss: 0.979408, Accuracy: 77.20%\n",
      "Batch 197, Loss: 0.903915, Accuracy: 77.24%\n",
      "Batch 198, Loss: 0.854473, Accuracy: 77.30%\n",
      "Batch 199, Loss: 0.923828, Accuracy: 77.32%\n",
      "Batch 200, Loss: 1.002103, Accuracy: 77.30%\n",
      "Batch 201, Loss: 1.013863, Accuracy: 77.28%\n",
      "Batch 202, Loss: 0.919268, Accuracy: 77.31%\n",
      "Batch 203, Loss: 0.918985, Accuracy: 77.33%\n",
      "Batch 204, Loss: 0.950204, Accuracy: 77.34%\n",
      "Batch 205, Loss: 1.051021, Accuracy: 77.29%\n",
      "Batch 206, Loss: 1.027234, Accuracy: 77.26%\n",
      "Batch 207, Loss: 0.996809, Accuracy: 77.23%\n",
      "Batch 208, Loss: 0.924138, Accuracy: 77.26%\n",
      "Batch 209, Loss: 0.903702, Accuracy: 77.30%\n",
      "Batch 210, Loss: 0.918124, Accuracy: 77.33%\n",
      "Batch 211, Loss: 0.947952, Accuracy: 77.33%\n",
      "Batch 212, Loss: 1.064414, Accuracy: 77.29%\n",
      "Batch 213, Loss: 0.942147, Accuracy: 77.31%\n",
      "Training - Epoch 82, Loss: 0.968051, Accuracy: 77.31%\n",
      "Validation Batch 1, Loss: 0.943825, Accuracy: 81.25%\n",
      "Validation Batch 2, Loss: 0.987927, Accuracy: 78.91%\n",
      "Validation Batch 3, Loss: 1.020564, Accuracy: 76.56%\n",
      "Validation Batch 4, Loss: 0.974395, Accuracy: 76.17%\n",
      "Validation Batch 5, Loss: 0.943785, Accuracy: 77.19%\n",
      "Validation Batch 6, Loss: 0.913132, Accuracy: 77.86%\n",
      "Validation Batch 7, Loss: 0.992791, Accuracy: 77.23%\n",
      "Validation Batch 8, Loss: 1.018142, Accuracy: 76.37%\n",
      "Validation Batch 9, Loss: 1.024003, Accuracy: 75.69%\n",
      "Validation Batch 10, Loss: 1.001737, Accuracy: 75.47%\n",
      "Validation Batch 11, Loss: 0.943668, Accuracy: 75.99%\n",
      "Validation Batch 12, Loss: 0.920376, Accuracy: 76.56%\n",
      "Validation Batch 13, Loss: 1.064460, Accuracy: 75.72%\n",
      "Validation Batch 14, Loss: 1.004627, Accuracy: 75.56%\n",
      "Validation Batch 15, Loss: 0.968242, Accuracy: 75.73%\n",
      "Validation Batch 16, Loss: 0.934108, Accuracy: 76.17%\n",
      "Validation Batch 17, Loss: 0.990129, Accuracy: 76.19%\n",
      "Validation Batch 18, Loss: 0.975224, Accuracy: 76.22%\n",
      "Validation Batch 19, Loss: 1.012432, Accuracy: 76.07%\n",
      "Validation Batch 20, Loss: 0.980590, Accuracy: 76.02%\n",
      "Validation Batch 21, Loss: 0.976187, Accuracy: 76.04%\n",
      "Validation Batch 22, Loss: 1.020899, Accuracy: 75.78%\n",
      "Validation Batch 23, Loss: 1.070731, Accuracy: 75.34%\n",
      "Validation Batch 24, Loss: 1.054720, Accuracy: 75.07%\n",
      "Validation Batch 25, Loss: 0.946399, Accuracy: 75.25%\n",
      "Validation Batch 26, Loss: 0.991364, Accuracy: 75.24%\n",
      "Validation Batch 27, Loss: 0.940767, Accuracy: 75.40%\n",
      "Validation - Epoch 82, Loss: 0.985749, Accuracy: 75.40%\n",
      "Patienceâ€”3\n",
      "Epoch 83\n",
      "Batch 1, Loss: 0.950844, Accuracy: 76.56%\n",
      "Batch 2, Loss: 0.882290, Accuracy: 82.03%\n",
      "Batch 3, Loss: 0.966256, Accuracy: 81.25%\n",
      "Batch 4, Loss: 0.984666, Accuracy: 79.30%\n",
      "Batch 5, Loss: 0.965083, Accuracy: 79.38%\n",
      "Batch 6, Loss: 0.939551, Accuracy: 79.69%\n",
      "Batch 7, Loss: 1.002366, Accuracy: 78.79%\n",
      "Batch 8, Loss: 0.912999, Accuracy: 79.49%\n",
      "Batch 9, Loss: 0.981014, Accuracy: 78.99%\n",
      "Batch 10, Loss: 1.085157, Accuracy: 77.50%\n",
      "Batch 11, Loss: 0.957146, Accuracy: 77.84%\n",
      "Batch 12, Loss: 0.977320, Accuracy: 77.73%\n",
      "Batch 13, Loss: 0.947580, Accuracy: 77.76%\n",
      "Batch 14, Loss: 0.922469, Accuracy: 78.12%\n",
      "Batch 15, Loss: 1.099067, Accuracy: 77.19%\n",
      "Batch 16, Loss: 0.923085, Accuracy: 77.54%\n",
      "Batch 17, Loss: 0.931526, Accuracy: 77.67%\n",
      "Batch 18, Loss: 0.924147, Accuracy: 77.95%\n",
      "Batch 19, Loss: 0.975369, Accuracy: 77.96%\n",
      "Batch 20, Loss: 1.027304, Accuracy: 77.66%\n",
      "Batch 21, Loss: 0.986601, Accuracy: 77.60%\n",
      "Batch 22, Loss: 0.970087, Accuracy: 77.56%\n",
      "Batch 23, Loss: 0.869018, Accuracy: 78.12%\n",
      "Batch 24, Loss: 0.977693, Accuracy: 78.12%\n",
      "Batch 25, Loss: 1.042544, Accuracy: 77.75%\n",
      "Batch 26, Loss: 0.958533, Accuracy: 77.64%\n",
      "Batch 27, Loss: 0.913359, Accuracy: 77.89%\n",
      "Batch 28, Loss: 0.960914, Accuracy: 77.90%\n",
      "Batch 29, Loss: 0.998145, Accuracy: 77.75%\n",
      "Batch 30, Loss: 0.934445, Accuracy: 77.86%\n",
      "Batch 31, Loss: 0.969932, Accuracy: 77.87%\n",
      "Batch 32, Loss: 0.884886, Accuracy: 78.17%\n",
      "Batch 33, Loss: 0.929087, Accuracy: 78.27%\n",
      "Batch 34, Loss: 0.981401, Accuracy: 78.17%\n",
      "Batch 35, Loss: 1.047894, Accuracy: 77.86%\n",
      "Batch 36, Loss: 1.069391, Accuracy: 77.56%\n",
      "Batch 37, Loss: 0.987701, Accuracy: 77.49%\n",
      "Batch 38, Loss: 0.928153, Accuracy: 77.59%\n",
      "Batch 39, Loss: 1.023657, Accuracy: 77.48%\n",
      "Batch 40, Loss: 0.918006, Accuracy: 77.62%\n",
      "Batch 41, Loss: 0.967773, Accuracy: 77.63%\n",
      "Batch 42, Loss: 0.982184, Accuracy: 77.60%\n",
      "Batch 43, Loss: 0.997256, Accuracy: 77.51%\n",
      "Batch 44, Loss: 1.015157, Accuracy: 77.38%\n",
      "Batch 45, Loss: 0.981773, Accuracy: 77.33%\n",
      "Batch 46, Loss: 0.865832, Accuracy: 77.55%\n",
      "Batch 47, Loss: 0.931913, Accuracy: 77.63%\n",
      "Batch 48, Loss: 0.908634, Accuracy: 77.77%\n",
      "Batch 49, Loss: 0.965527, Accuracy: 77.77%\n",
      "Batch 50, Loss: 1.050094, Accuracy: 77.59%\n",
      "Batch 51, Loss: 1.079231, Accuracy: 77.33%\n",
      "Batch 52, Loss: 0.997303, Accuracy: 77.28%\n",
      "Batch 53, Loss: 1.030669, Accuracy: 77.15%\n",
      "Batch 54, Loss: 0.946323, Accuracy: 77.17%\n",
      "Batch 55, Loss: 0.952462, Accuracy: 77.22%\n",
      "Batch 56, Loss: 0.883350, Accuracy: 77.37%\n",
      "Batch 57, Loss: 1.016428, Accuracy: 77.28%\n",
      "Batch 58, Loss: 0.865748, Accuracy: 77.42%\n",
      "Batch 59, Loss: 0.967284, Accuracy: 77.41%\n",
      "Batch 60, Loss: 1.009740, Accuracy: 77.34%\n",
      "Batch 61, Loss: 1.011078, Accuracy: 77.28%\n",
      "Batch 62, Loss: 1.007717, Accuracy: 77.22%\n",
      "Batch 63, Loss: 0.979858, Accuracy: 77.18%\n",
      "Batch 64, Loss: 0.906470, Accuracy: 77.27%\n",
      "Batch 65, Loss: 0.964437, Accuracy: 77.28%\n",
      "Batch 66, Loss: 0.994172, Accuracy: 77.20%\n",
      "Batch 67, Loss: 0.925627, Accuracy: 77.29%\n",
      "Batch 68, Loss: 0.901358, Accuracy: 77.41%\n",
      "Batch 69, Loss: 0.987209, Accuracy: 77.36%\n",
      "Batch 70, Loss: 0.979436, Accuracy: 77.37%\n",
      "Batch 71, Loss: 1.098128, Accuracy: 77.20%\n",
      "Batch 72, Loss: 1.006906, Accuracy: 77.15%\n",
      "Batch 73, Loss: 1.024933, Accuracy: 77.08%\n",
      "Batch 74, Loss: 0.951134, Accuracy: 77.11%\n",
      "Batch 75, Loss: 0.971605, Accuracy: 77.15%\n",
      "Batch 76, Loss: 0.980989, Accuracy: 77.14%\n",
      "Batch 77, Loss: 0.985891, Accuracy: 77.11%\n",
      "Batch 78, Loss: 1.031289, Accuracy: 77.02%\n",
      "Batch 79, Loss: 0.901861, Accuracy: 77.12%\n",
      "Batch 80, Loss: 0.986921, Accuracy: 77.09%\n",
      "Batch 81, Loss: 1.006018, Accuracy: 77.03%\n",
      "Batch 82, Loss: 0.988243, Accuracy: 76.98%\n",
      "Batch 83, Loss: 0.980214, Accuracy: 76.96%\n",
      "Batch 84, Loss: 0.950995, Accuracy: 76.99%\n",
      "Batch 85, Loss: 0.887144, Accuracy: 77.10%\n",
      "Batch 86, Loss: 0.988661, Accuracy: 77.05%\n",
      "Batch 87, Loss: 0.985791, Accuracy: 77.03%\n",
      "Batch 88, Loss: 0.964984, Accuracy: 77.04%\n",
      "Batch 89, Loss: 1.012251, Accuracy: 77.00%\n",
      "Batch 90, Loss: 0.923097, Accuracy: 77.03%\n",
      "Batch 91, Loss: 0.908177, Accuracy: 77.09%\n",
      "Batch 92, Loss: 0.880016, Accuracy: 77.21%\n",
      "Batch 93, Loss: 1.087758, Accuracy: 77.07%\n",
      "Batch 94, Loss: 0.954513, Accuracy: 77.13%\n",
      "Batch 95, Loss: 0.964998, Accuracy: 77.14%\n",
      "Batch 96, Loss: 1.054619, Accuracy: 77.05%\n",
      "Batch 97, Loss: 0.981011, Accuracy: 77.05%\n",
      "Batch 98, Loss: 1.016095, Accuracy: 77.01%\n",
      "Batch 99, Loss: 1.007369, Accuracy: 76.96%\n",
      "Batch 100, Loss: 0.953142, Accuracy: 76.97%\n",
      "Batch 101, Loss: 1.007351, Accuracy: 76.92%\n",
      "Batch 102, Loss: 1.023358, Accuracy: 76.88%\n",
      "Batch 103, Loss: 0.960002, Accuracy: 76.90%\n",
      "Batch 104, Loss: 1.016950, Accuracy: 76.86%\n",
      "Batch 105, Loss: 0.996292, Accuracy: 76.86%\n",
      "Batch 106, Loss: 1.029512, Accuracy: 76.80%\n",
      "Batch 107, Loss: 0.986435, Accuracy: 76.78%\n",
      "Batch 108, Loss: 1.021848, Accuracy: 76.74%\n",
      "Batch 109, Loss: 0.948434, Accuracy: 76.73%\n",
      "Batch 110, Loss: 0.906565, Accuracy: 76.80%\n",
      "Batch 111, Loss: 0.989460, Accuracy: 76.80%\n",
      "Batch 112, Loss: 0.994337, Accuracy: 76.79%\n",
      "Batch 113, Loss: 1.003561, Accuracy: 76.74%\n",
      "Batch 114, Loss: 0.993242, Accuracy: 76.73%\n",
      "Batch 115, Loss: 0.978502, Accuracy: 76.74%\n",
      "Batch 116, Loss: 1.008147, Accuracy: 76.71%\n",
      "Batch 117, Loss: 0.945800, Accuracy: 76.72%\n",
      "Batch 118, Loss: 0.966492, Accuracy: 76.76%\n",
      "Batch 119, Loss: 0.948690, Accuracy: 76.77%\n",
      "Batch 120, Loss: 0.934295, Accuracy: 76.82%\n",
      "Batch 121, Loss: 1.050945, Accuracy: 76.76%\n",
      "Batch 122, Loss: 0.992671, Accuracy: 76.75%\n",
      "Batch 123, Loss: 0.978110, Accuracy: 76.75%\n",
      "Batch 124, Loss: 1.030351, Accuracy: 76.73%\n",
      "Batch 125, Loss: 0.971174, Accuracy: 76.74%\n",
      "Batch 126, Loss: 0.928222, Accuracy: 76.79%\n",
      "Batch 127, Loss: 1.041295, Accuracy: 76.73%\n",
      "Batch 128, Loss: 1.007007, Accuracy: 76.72%\n",
      "Batch 129, Loss: 0.950695, Accuracy: 76.73%\n",
      "Batch 130, Loss: 0.964376, Accuracy: 76.74%\n",
      "Batch 131, Loss: 0.886842, Accuracy: 76.81%\n",
      "Batch 132, Loss: 0.990715, Accuracy: 76.81%\n",
      "Batch 133, Loss: 0.963080, Accuracy: 76.81%\n",
      "Batch 134, Loss: 0.966595, Accuracy: 76.82%\n",
      "Batch 135, Loss: 0.929985, Accuracy: 76.85%\n",
      "Batch 136, Loss: 0.921457, Accuracy: 76.91%\n",
      "Batch 137, Loss: 1.000758, Accuracy: 76.88%\n",
      "Batch 138, Loss: 0.968125, Accuracy: 76.89%\n",
      "Batch 139, Loss: 0.932791, Accuracy: 76.91%\n",
      "Batch 140, Loss: 1.021163, Accuracy: 76.86%\n",
      "Batch 141, Loss: 0.902058, Accuracy: 76.93%\n",
      "Batch 142, Loss: 0.927136, Accuracy: 76.95%\n",
      "Batch 143, Loss: 0.982263, Accuracy: 76.94%\n",
      "Batch 144, Loss: 1.028918, Accuracy: 76.91%\n",
      "Batch 145, Loss: 0.899619, Accuracy: 76.96%\n",
      "Batch 146, Loss: 0.979756, Accuracy: 76.95%\n",
      "Batch 147, Loss: 0.867242, Accuracy: 77.02%\n",
      "Batch 148, Loss: 0.903268, Accuracy: 77.07%\n",
      "Batch 149, Loss: 0.998079, Accuracy: 77.06%\n",
      "Batch 150, Loss: 1.057673, Accuracy: 76.99%\n",
      "Batch 151, Loss: 0.988105, Accuracy: 76.98%\n",
      "Batch 152, Loss: 0.920080, Accuracy: 77.01%\n",
      "Batch 153, Loss: 1.053705, Accuracy: 76.95%\n",
      "Batch 154, Loss: 0.997685, Accuracy: 76.93%\n",
      "Batch 155, Loss: 0.889925, Accuracy: 77.01%\n",
      "Batch 156, Loss: 1.042413, Accuracy: 76.95%\n",
      "Batch 157, Loss: 1.002896, Accuracy: 76.93%\n",
      "Batch 158, Loss: 0.896642, Accuracy: 76.99%\n",
      "Batch 159, Loss: 0.919056, Accuracy: 77.02%\n",
      "Batch 160, Loss: 0.941329, Accuracy: 77.04%\n",
      "Batch 161, Loss: 0.928013, Accuracy: 77.08%\n",
      "Batch 162, Loss: 0.922527, Accuracy: 77.11%\n",
      "Batch 163, Loss: 0.922870, Accuracy: 77.14%\n",
      "Batch 164, Loss: 0.941178, Accuracy: 77.15%\n",
      "Batch 165, Loss: 0.995331, Accuracy: 77.12%\n",
      "Batch 166, Loss: 0.928347, Accuracy: 77.16%\n",
      "Batch 167, Loss: 0.939454, Accuracy: 77.17%\n",
      "Batch 168, Loss: 0.959108, Accuracy: 77.18%\n",
      "Batch 169, Loss: 0.948973, Accuracy: 77.19%\n",
      "Batch 170, Loss: 1.016372, Accuracy: 77.16%\n",
      "Batch 171, Loss: 0.883874, Accuracy: 77.22%\n",
      "Batch 172, Loss: 0.972765, Accuracy: 77.22%\n",
      "Batch 173, Loss: 0.958756, Accuracy: 77.22%\n",
      "Batch 174, Loss: 1.047027, Accuracy: 77.18%\n",
      "Batch 175, Loss: 0.928842, Accuracy: 77.21%\n",
      "Batch 176, Loss: 0.900171, Accuracy: 77.26%\n",
      "Batch 177, Loss: 0.954917, Accuracy: 77.28%\n",
      "Batch 178, Loss: 0.974935, Accuracy: 77.27%\n",
      "Batch 179, Loss: 0.942694, Accuracy: 77.28%\n",
      "Batch 180, Loss: 0.875845, Accuracy: 77.33%\n",
      "Batch 181, Loss: 0.952048, Accuracy: 77.33%\n",
      "Batch 182, Loss: 0.884964, Accuracy: 77.37%\n",
      "Batch 183, Loss: 1.032514, Accuracy: 77.33%\n",
      "Batch 184, Loss: 1.073782, Accuracy: 77.28%\n",
      "Batch 185, Loss: 0.960543, Accuracy: 77.28%\n",
      "Batch 186, Loss: 0.939230, Accuracy: 77.31%\n",
      "Batch 187, Loss: 1.011909, Accuracy: 77.29%\n",
      "Batch 188, Loss: 1.005516, Accuracy: 77.27%\n",
      "Batch 189, Loss: 1.007559, Accuracy: 77.25%\n",
      "Batch 190, Loss: 0.986189, Accuracy: 77.24%\n",
      "Batch 191, Loss: 0.959630, Accuracy: 77.24%\n",
      "Batch 192, Loss: 0.942854, Accuracy: 77.25%\n",
      "Batch 193, Loss: 0.965277, Accuracy: 77.26%\n",
      "Batch 194, Loss: 0.994777, Accuracy: 77.25%\n",
      "Batch 195, Loss: 0.934818, Accuracy: 77.27%\n",
      "Batch 196, Loss: 0.954839, Accuracy: 77.28%\n",
      "Batch 197, Loss: 1.029002, Accuracy: 77.24%\n",
      "Batch 198, Loss: 1.014151, Accuracy: 77.21%\n",
      "Batch 199, Loss: 0.935455, Accuracy: 77.23%\n",
      "Batch 200, Loss: 0.973938, Accuracy: 77.24%\n",
      "Batch 201, Loss: 0.889782, Accuracy: 77.29%\n",
      "Batch 202, Loss: 0.962982, Accuracy: 77.29%\n",
      "Batch 203, Loss: 0.991763, Accuracy: 77.29%\n",
      "Batch 204, Loss: 0.936204, Accuracy: 77.31%\n",
      "Batch 205, Loss: 0.908949, Accuracy: 77.33%\n",
      "Batch 206, Loss: 0.903265, Accuracy: 77.37%\n",
      "Batch 207, Loss: 0.979471, Accuracy: 77.37%\n",
      "Batch 208, Loss: 0.966777, Accuracy: 77.37%\n",
      "Batch 209, Loss: 0.927817, Accuracy: 77.39%\n",
      "Batch 210, Loss: 1.063259, Accuracy: 77.35%\n",
      "Batch 211, Loss: 0.962822, Accuracy: 77.36%\n",
      "Batch 212, Loss: 0.950662, Accuracy: 77.37%\n",
      "Batch 213, Loss: 0.915711, Accuracy: 77.40%\n",
      "Training - Epoch 83, Loss: 0.969143, Accuracy: 77.40%\n",
      "Validation Batch 1, Loss: 0.931367, Accuracy: 79.69%\n",
      "Validation Batch 2, Loss: 0.983111, Accuracy: 77.34%\n",
      "Validation Batch 3, Loss: 1.019294, Accuracy: 75.52%\n",
      "Validation Batch 4, Loss: 0.962074, Accuracy: 76.17%\n",
      "Validation Batch 5, Loss: 0.936565, Accuracy: 77.50%\n",
      "Validation Batch 6, Loss: 0.915508, Accuracy: 78.12%\n",
      "Validation Batch 7, Loss: 0.988263, Accuracy: 77.68%\n",
      "Validation Batch 8, Loss: 1.021604, Accuracy: 76.76%\n",
      "Validation Batch 9, Loss: 1.019486, Accuracy: 76.22%\n",
      "Validation Batch 10, Loss: 1.000243, Accuracy: 75.94%\n",
      "Validation Batch 11, Loss: 0.937906, Accuracy: 76.42%\n",
      "Validation Batch 12, Loss: 0.917921, Accuracy: 77.21%\n",
      "Validation Batch 13, Loss: 1.061298, Accuracy: 76.20%\n",
      "Validation Batch 14, Loss: 0.996879, Accuracy: 76.12%\n",
      "Validation Batch 15, Loss: 0.967554, Accuracy: 76.25%\n",
      "Validation Batch 16, Loss: 0.928347, Accuracy: 76.76%\n",
      "Validation Batch 17, Loss: 0.990818, Accuracy: 76.65%\n",
      "Validation Batch 18, Loss: 0.961075, Accuracy: 76.82%\n",
      "Validation Batch 19, Loss: 0.999226, Accuracy: 76.64%\n",
      "Validation Batch 20, Loss: 0.963640, Accuracy: 76.80%\n",
      "Validation Batch 21, Loss: 0.970006, Accuracy: 76.79%\n",
      "Validation Batch 22, Loss: 1.003875, Accuracy: 76.63%\n",
      "Validation Batch 23, Loss: 1.058230, Accuracy: 76.22%\n",
      "Validation Batch 24, Loss: 1.041654, Accuracy: 75.98%\n",
      "Validation Batch 25, Loss: 0.939790, Accuracy: 76.12%\n",
      "Validation Batch 26, Loss: 0.985578, Accuracy: 76.08%\n",
      "Validation Batch 27, Loss: 0.936030, Accuracy: 76.22%\n",
      "Validation - Epoch 83, Loss: 0.979161, Accuracy: 76.22%\n",
      "Patienceâ€”4\n",
      "Epoch 84\n",
      "Batch 1, Loss: 0.932049, Accuracy: 79.69%\n",
      "Batch 2, Loss: 0.945784, Accuracy: 79.69%\n",
      "Batch 3, Loss: 0.988784, Accuracy: 78.12%\n",
      "Batch 4, Loss: 0.941207, Accuracy: 78.52%\n",
      "Batch 5, Loss: 0.961708, Accuracy: 78.44%\n",
      "Batch 6, Loss: 0.992571, Accuracy: 77.86%\n",
      "Batch 7, Loss: 0.925362, Accuracy: 78.57%\n",
      "Batch 8, Loss: 0.932874, Accuracy: 79.10%\n",
      "Batch 9, Loss: 0.971291, Accuracy: 78.99%\n",
      "Batch 10, Loss: 0.894134, Accuracy: 79.69%\n",
      "Batch 11, Loss: 0.947378, Accuracy: 79.83%\n",
      "Batch 12, Loss: 0.955095, Accuracy: 79.69%\n",
      "Batch 13, Loss: 0.912211, Accuracy: 79.81%\n",
      "Batch 14, Loss: 1.011235, Accuracy: 79.24%\n",
      "Batch 15, Loss: 1.020403, Accuracy: 78.75%\n",
      "Batch 16, Loss: 1.027404, Accuracy: 78.22%\n",
      "Batch 17, Loss: 0.974212, Accuracy: 78.22%\n",
      "Batch 18, Loss: 0.989830, Accuracy: 77.95%\n",
      "Batch 19, Loss: 0.958906, Accuracy: 77.96%\n",
      "Batch 20, Loss: 0.983556, Accuracy: 77.73%\n",
      "Batch 21, Loss: 0.952884, Accuracy: 77.83%\n",
      "Batch 22, Loss: 0.854875, Accuracy: 78.41%\n",
      "Batch 23, Loss: 0.945360, Accuracy: 78.40%\n",
      "Batch 24, Loss: 0.945630, Accuracy: 78.45%\n",
      "Batch 25, Loss: 0.998841, Accuracy: 78.25%\n",
      "Batch 26, Loss: 0.949135, Accuracy: 78.31%\n",
      "Batch 27, Loss: 0.913424, Accuracy: 78.47%\n",
      "Batch 28, Loss: 0.877905, Accuracy: 78.74%\n",
      "Batch 29, Loss: 0.934304, Accuracy: 78.83%\n",
      "Batch 30, Loss: 0.949162, Accuracy: 78.85%\n",
      "Batch 31, Loss: 1.045061, Accuracy: 78.58%\n",
      "Batch 32, Loss: 1.058802, Accuracy: 78.22%\n",
      "Batch 33, Loss: 0.929254, Accuracy: 78.31%\n",
      "Batch 34, Loss: 0.954314, Accuracy: 78.40%\n",
      "Batch 35, Loss: 0.950766, Accuracy: 78.48%\n",
      "Batch 36, Loss: 0.976486, Accuracy: 78.47%\n",
      "Batch 37, Loss: 0.905047, Accuracy: 78.63%\n",
      "Batch 38, Loss: 0.958918, Accuracy: 78.66%\n",
      "Batch 39, Loss: 0.907224, Accuracy: 78.81%\n",
      "Batch 40, Loss: 0.956304, Accuracy: 78.75%\n",
      "Batch 41, Loss: 1.022597, Accuracy: 78.58%\n",
      "Batch 42, Loss: 1.032309, Accuracy: 78.42%\n",
      "Batch 43, Loss: 1.030802, Accuracy: 78.23%\n",
      "Batch 44, Loss: 1.006150, Accuracy: 78.05%\n",
      "Batch 45, Loss: 0.986822, Accuracy: 78.02%\n",
      "Batch 46, Loss: 0.964518, Accuracy: 78.02%\n",
      "Batch 47, Loss: 0.939114, Accuracy: 78.09%\n",
      "Batch 48, Loss: 0.970612, Accuracy: 78.09%\n",
      "Batch 49, Loss: 0.988131, Accuracy: 78.03%\n",
      "Batch 50, Loss: 0.978226, Accuracy: 78.03%\n",
      "Batch 51, Loss: 0.993199, Accuracy: 77.94%\n",
      "Batch 52, Loss: 1.090865, Accuracy: 77.70%\n",
      "Batch 53, Loss: 0.990822, Accuracy: 77.65%\n",
      "Batch 54, Loss: 0.941145, Accuracy: 77.72%\n",
      "Batch 55, Loss: 0.966238, Accuracy: 77.73%\n",
      "Batch 56, Loss: 0.910610, Accuracy: 77.82%\n",
      "Batch 57, Loss: 1.003265, Accuracy: 77.77%\n",
      "Batch 58, Loss: 0.910699, Accuracy: 77.86%\n",
      "Batch 59, Loss: 0.969385, Accuracy: 77.83%\n",
      "Batch 60, Loss: 0.948736, Accuracy: 77.86%\n",
      "Batch 61, Loss: 0.938720, Accuracy: 77.92%\n",
      "Batch 62, Loss: 0.961957, Accuracy: 77.90%\n",
      "Batch 63, Loss: 0.938047, Accuracy: 77.90%\n",
      "Batch 64, Loss: 1.032427, Accuracy: 77.81%\n",
      "Batch 65, Loss: 1.011393, Accuracy: 77.72%\n",
      "Batch 66, Loss: 0.954261, Accuracy: 77.72%\n",
      "Batch 67, Loss: 0.912975, Accuracy: 77.82%\n",
      "Batch 68, Loss: 1.027984, Accuracy: 77.73%\n",
      "Batch 69, Loss: 0.952320, Accuracy: 77.74%\n",
      "Batch 70, Loss: 0.937563, Accuracy: 77.79%\n",
      "Batch 71, Loss: 0.946309, Accuracy: 77.84%\n",
      "Batch 72, Loss: 0.983410, Accuracy: 77.82%\n",
      "Batch 73, Loss: 0.973046, Accuracy: 77.83%\n",
      "Batch 74, Loss: 0.993921, Accuracy: 77.77%\n",
      "Batch 75, Loss: 0.898828, Accuracy: 77.83%\n",
      "Batch 76, Loss: 0.930255, Accuracy: 77.88%\n",
      "Batch 77, Loss: 0.961291, Accuracy: 77.88%\n",
      "Batch 78, Loss: 0.997371, Accuracy: 77.84%\n",
      "Batch 79, Loss: 1.019161, Accuracy: 77.77%\n",
      "Batch 80, Loss: 1.101356, Accuracy: 77.58%\n",
      "Batch 81, Loss: 0.919053, Accuracy: 77.64%\n",
      "Batch 82, Loss: 0.960982, Accuracy: 77.63%\n",
      "Batch 83, Loss: 1.054706, Accuracy: 77.48%\n",
      "Batch 84, Loss: 0.961560, Accuracy: 77.49%\n",
      "Batch 85, Loss: 0.991603, Accuracy: 77.46%\n",
      "Batch 86, Loss: 0.981665, Accuracy: 77.45%\n",
      "Batch 87, Loss: 0.950004, Accuracy: 77.48%\n",
      "Batch 88, Loss: 1.000666, Accuracy: 77.41%\n",
      "Batch 89, Loss: 0.966565, Accuracy: 77.42%\n",
      "Batch 90, Loss: 1.025109, Accuracy: 77.34%\n",
      "Batch 91, Loss: 0.932497, Accuracy: 77.37%\n",
      "Batch 92, Loss: 0.969480, Accuracy: 77.36%\n",
      "Batch 93, Loss: 0.953552, Accuracy: 77.35%\n",
      "Batch 94, Loss: 0.980842, Accuracy: 77.33%\n",
      "Batch 95, Loss: 0.928443, Accuracy: 77.37%\n",
      "Batch 96, Loss: 0.928540, Accuracy: 77.43%\n",
      "Batch 97, Loss: 0.977552, Accuracy: 77.42%\n",
      "Batch 98, Loss: 0.982552, Accuracy: 77.39%\n",
      "Batch 99, Loss: 0.923778, Accuracy: 77.45%\n",
      "Batch 100, Loss: 0.922686, Accuracy: 77.52%\n",
      "Batch 101, Loss: 0.953054, Accuracy: 77.54%\n",
      "Batch 102, Loss: 0.905091, Accuracy: 77.60%\n",
      "Batch 103, Loss: 0.943162, Accuracy: 77.61%\n",
      "Batch 104, Loss: 0.943978, Accuracy: 77.63%\n",
      "Batch 105, Loss: 0.980840, Accuracy: 77.60%\n",
      "Batch 106, Loss: 0.894399, Accuracy: 77.70%\n",
      "Batch 107, Loss: 0.938382, Accuracy: 77.72%\n",
      "Batch 108, Loss: 0.992295, Accuracy: 77.71%\n",
      "Batch 109, Loss: 0.964978, Accuracy: 77.71%\n",
      "Batch 110, Loss: 0.944597, Accuracy: 77.73%\n",
      "Batch 111, Loss: 1.000377, Accuracy: 77.67%\n",
      "Batch 112, Loss: 1.046465, Accuracy: 77.59%\n",
      "Batch 113, Loss: 0.901711, Accuracy: 77.67%\n",
      "Batch 114, Loss: 1.028152, Accuracy: 77.62%\n",
      "Batch 115, Loss: 0.939806, Accuracy: 77.65%\n",
      "Batch 116, Loss: 0.981534, Accuracy: 77.64%\n",
      "Batch 117, Loss: 0.951860, Accuracy: 77.66%\n",
      "Batch 118, Loss: 1.044022, Accuracy: 77.60%\n",
      "Batch 119, Loss: 1.011662, Accuracy: 77.55%\n",
      "Batch 120, Loss: 0.966900, Accuracy: 77.54%\n",
      "Batch 121, Loss: 0.921360, Accuracy: 77.58%\n",
      "Batch 122, Loss: 0.876615, Accuracy: 77.65%\n",
      "Batch 123, Loss: 0.986679, Accuracy: 77.62%\n",
      "Batch 124, Loss: 0.960782, Accuracy: 77.61%\n",
      "Batch 125, Loss: 0.958320, Accuracy: 77.62%\n",
      "Batch 126, Loss: 1.014977, Accuracy: 77.57%\n",
      "Batch 127, Loss: 0.963406, Accuracy: 77.57%\n",
      "Batch 128, Loss: 0.930146, Accuracy: 77.60%\n",
      "Batch 129, Loss: 0.919279, Accuracy: 77.64%\n",
      "Batch 130, Loss: 1.014230, Accuracy: 77.61%\n",
      "Batch 131, Loss: 0.956985, Accuracy: 77.61%\n",
      "Batch 132, Loss: 0.920315, Accuracy: 77.64%\n",
      "Batch 133, Loss: 0.896848, Accuracy: 77.68%\n",
      "Batch 134, Loss: 0.958227, Accuracy: 77.68%\n",
      "Batch 135, Loss: 0.979146, Accuracy: 77.67%\n",
      "Batch 136, Loss: 1.028805, Accuracy: 77.62%\n",
      "Batch 137, Loss: 0.959975, Accuracy: 77.63%\n",
      "Batch 138, Loss: 0.915113, Accuracy: 77.68%\n",
      "Batch 139, Loss: 1.004151, Accuracy: 77.66%\n",
      "Batch 140, Loss: 0.967357, Accuracy: 77.67%\n",
      "Batch 141, Loss: 1.016109, Accuracy: 77.63%\n",
      "Batch 142, Loss: 0.943390, Accuracy: 77.64%\n",
      "Batch 143, Loss: 0.997485, Accuracy: 77.62%\n",
      "Batch 144, Loss: 1.024806, Accuracy: 77.58%\n",
      "Batch 145, Loss: 1.064699, Accuracy: 77.52%\n",
      "Batch 146, Loss: 1.034614, Accuracy: 77.48%\n",
      "Batch 147, Loss: 0.948095, Accuracy: 77.50%\n",
      "Batch 148, Loss: 0.980787, Accuracy: 77.49%\n",
      "Batch 149, Loss: 0.966108, Accuracy: 77.51%\n",
      "Batch 150, Loss: 0.893804, Accuracy: 77.55%\n",
      "Batch 151, Loss: 0.915778, Accuracy: 77.59%\n",
      "Batch 152, Loss: 1.010685, Accuracy: 77.56%\n",
      "Batch 153, Loss: 0.842983, Accuracy: 77.65%\n",
      "Batch 154, Loss: 0.958755, Accuracy: 77.64%\n",
      "Batch 155, Loss: 1.003405, Accuracy: 77.62%\n",
      "Batch 156, Loss: 0.908739, Accuracy: 77.63%\n",
      "Batch 157, Loss: 0.910330, Accuracy: 77.68%\n",
      "Batch 158, Loss: 0.996026, Accuracy: 77.65%\n",
      "Batch 159, Loss: 0.920772, Accuracy: 77.68%\n",
      "Batch 160, Loss: 0.888919, Accuracy: 77.72%\n",
      "Batch 161, Loss: 0.994129, Accuracy: 77.71%\n",
      "Batch 162, Loss: 0.946708, Accuracy: 77.73%\n",
      "Batch 163, Loss: 0.964559, Accuracy: 77.72%\n",
      "Batch 164, Loss: 1.074629, Accuracy: 77.66%\n",
      "Batch 165, Loss: 0.978743, Accuracy: 77.66%\n",
      "Batch 166, Loss: 1.040345, Accuracy: 77.62%\n",
      "Batch 167, Loss: 0.978660, Accuracy: 77.62%\n",
      "Batch 168, Loss: 0.950742, Accuracy: 77.62%\n",
      "Batch 169, Loss: 1.074078, Accuracy: 77.55%\n",
      "Batch 170, Loss: 0.931798, Accuracy: 77.58%\n",
      "Batch 171, Loss: 1.032084, Accuracy: 77.55%\n",
      "Batch 172, Loss: 0.946817, Accuracy: 77.56%\n",
      "Batch 173, Loss: 0.983784, Accuracy: 77.55%\n",
      "Batch 174, Loss: 0.950791, Accuracy: 77.55%\n",
      "Batch 175, Loss: 0.979800, Accuracy: 77.54%\n",
      "Batch 176, Loss: 0.981028, Accuracy: 77.51%\n",
      "Batch 177, Loss: 1.049478, Accuracy: 77.46%\n",
      "Batch 178, Loss: 0.950979, Accuracy: 77.48%\n",
      "Batch 179, Loss: 1.030124, Accuracy: 77.44%\n",
      "Batch 180, Loss: 0.922409, Accuracy: 77.46%\n",
      "Batch 181, Loss: 1.015639, Accuracy: 77.43%\n",
      "Batch 182, Loss: 0.977093, Accuracy: 77.42%\n",
      "Batch 183, Loss: 0.935464, Accuracy: 77.44%\n",
      "Batch 184, Loss: 0.925979, Accuracy: 77.47%\n",
      "Batch 185, Loss: 1.007353, Accuracy: 77.44%\n",
      "Batch 186, Loss: 0.909349, Accuracy: 77.46%\n",
      "Batch 187, Loss: 0.969836, Accuracy: 77.46%\n",
      "Batch 188, Loss: 0.959517, Accuracy: 77.48%\n",
      "Batch 189, Loss: 0.867274, Accuracy: 77.54%\n",
      "Batch 190, Loss: 1.018967, Accuracy: 77.51%\n",
      "Batch 191, Loss: 0.975641, Accuracy: 77.50%\n",
      "Batch 192, Loss: 0.953793, Accuracy: 77.51%\n",
      "Batch 193, Loss: 1.005264, Accuracy: 77.50%\n",
      "Batch 194, Loss: 0.954707, Accuracy: 77.50%\n",
      "Batch 195, Loss: 1.053750, Accuracy: 77.46%\n",
      "Batch 196, Loss: 0.975255, Accuracy: 77.46%\n",
      "Batch 197, Loss: 1.024808, Accuracy: 77.43%\n",
      "Batch 198, Loss: 0.915366, Accuracy: 77.45%\n",
      "Batch 199, Loss: 0.983294, Accuracy: 77.45%\n",
      "Batch 200, Loss: 0.962608, Accuracy: 77.45%\n",
      "Batch 201, Loss: 0.956142, Accuracy: 77.46%\n",
      "Batch 202, Loss: 0.964367, Accuracy: 77.48%\n",
      "Batch 203, Loss: 0.895835, Accuracy: 77.52%\n",
      "Batch 204, Loss: 0.972833, Accuracy: 77.51%\n",
      "Batch 205, Loss: 0.895773, Accuracy: 77.56%\n",
      "Batch 206, Loss: 0.946483, Accuracy: 77.58%\n",
      "Batch 207, Loss: 0.999261, Accuracy: 77.57%\n",
      "Batch 208, Loss: 0.930088, Accuracy: 77.58%\n",
      "Batch 209, Loss: 0.873661, Accuracy: 77.64%\n",
      "Batch 210, Loss: 0.924984, Accuracy: 77.65%\n",
      "Batch 211, Loss: 0.992308, Accuracy: 77.64%\n",
      "Batch 212, Loss: 0.950076, Accuracy: 77.65%\n",
      "Batch 213, Loss: 0.948754, Accuracy: 77.65%\n",
      "Training - Epoch 84, Loss: 0.965980, Accuracy: 77.65%\n",
      "Validation Batch 1, Loss: 0.971921, Accuracy: 78.12%\n",
      "Validation Batch 2, Loss: 1.033360, Accuracy: 74.22%\n",
      "Validation Batch 3, Loss: 1.046894, Accuracy: 72.92%\n",
      "Validation Batch 4, Loss: 1.014970, Accuracy: 72.27%\n",
      "Validation Batch 5, Loss: 0.989137, Accuracy: 72.81%\n",
      "Validation Batch 6, Loss: 0.957528, Accuracy: 73.44%\n",
      "Validation Batch 7, Loss: 1.021382, Accuracy: 73.21%\n",
      "Validation Batch 8, Loss: 1.056629, Accuracy: 72.66%\n",
      "Validation Batch 9, Loss: 1.059612, Accuracy: 72.22%\n",
      "Validation Batch 10, Loss: 1.034951, Accuracy: 72.19%\n",
      "Validation Batch 11, Loss: 0.981581, Accuracy: 72.59%\n",
      "Validation Batch 12, Loss: 0.961550, Accuracy: 73.05%\n",
      "Validation Batch 13, Loss: 1.098474, Accuracy: 72.36%\n",
      "Validation Batch 14, Loss: 1.040636, Accuracy: 72.21%\n",
      "Validation Batch 15, Loss: 0.995105, Accuracy: 72.40%\n",
      "Validation Batch 16, Loss: 0.983311, Accuracy: 72.56%\n",
      "Validation Batch 17, Loss: 1.043095, Accuracy: 72.33%\n",
      "Validation Batch 18, Loss: 0.991646, Accuracy: 72.57%\n",
      "Validation Batch 19, Loss: 1.061767, Accuracy: 72.37%\n",
      "Validation Batch 20, Loss: 1.021150, Accuracy: 72.27%\n",
      "Validation Batch 21, Loss: 1.019203, Accuracy: 72.17%\n",
      "Validation Batch 22, Loss: 1.061930, Accuracy: 71.95%\n",
      "Validation Batch 23, Loss: 1.103099, Accuracy: 71.60%\n",
      "Validation Batch 24, Loss: 1.069749, Accuracy: 71.35%\n",
      "Validation Batch 25, Loss: 1.010620, Accuracy: 71.44%\n",
      "Validation Batch 26, Loss: 1.004942, Accuracy: 71.51%\n",
      "Validation Batch 27, Loss: 0.977087, Accuracy: 71.64%\n",
      "Validation - Epoch 84, Loss: 1.022642, Accuracy: 71.64%\n",
      "Patienceâ€”5\n",
      "Epoch 85\n",
      "Batch 1, Loss: 0.953648, Accuracy: 76.56%\n",
      "Batch 2, Loss: 0.960234, Accuracy: 77.34%\n",
      "Batch 3, Loss: 0.977640, Accuracy: 77.08%\n",
      "Batch 4, Loss: 0.959351, Accuracy: 77.34%\n",
      "Batch 5, Loss: 0.960281, Accuracy: 77.50%\n",
      "Batch 6, Loss: 0.942515, Accuracy: 77.86%\n",
      "Batch 7, Loss: 0.931065, Accuracy: 78.35%\n",
      "Batch 8, Loss: 1.008928, Accuracy: 77.73%\n",
      "Batch 9, Loss: 0.916079, Accuracy: 78.30%\n",
      "Batch 10, Loss: 1.051300, Accuracy: 77.34%\n",
      "Batch 11, Loss: 0.992716, Accuracy: 77.13%\n",
      "Batch 12, Loss: 1.000390, Accuracy: 76.69%\n",
      "Batch 13, Loss: 0.902982, Accuracy: 77.40%\n",
      "Batch 14, Loss: 1.040183, Accuracy: 77.01%\n",
      "Batch 15, Loss: 0.974223, Accuracy: 77.08%\n",
      "Batch 16, Loss: 0.963191, Accuracy: 77.05%\n",
      "Batch 17, Loss: 0.959133, Accuracy: 77.21%\n",
      "Batch 18, Loss: 0.966955, Accuracy: 77.17%\n",
      "Batch 19, Loss: 0.996205, Accuracy: 77.06%\n",
      "Batch 20, Loss: 0.998601, Accuracy: 77.03%\n",
      "Batch 21, Loss: 1.072730, Accuracy: 76.49%\n",
      "Batch 22, Loss: 0.943903, Accuracy: 76.56%\n",
      "Batch 23, Loss: 0.937068, Accuracy: 76.70%\n",
      "Batch 24, Loss: 0.950884, Accuracy: 76.76%\n",
      "Batch 25, Loss: 0.940145, Accuracy: 76.94%\n",
      "Batch 26, Loss: 1.015379, Accuracy: 76.74%\n",
      "Batch 27, Loss: 0.952980, Accuracy: 76.91%\n",
      "Batch 28, Loss: 0.972456, Accuracy: 77.01%\n",
      "Batch 29, Loss: 1.007937, Accuracy: 76.89%\n",
      "Batch 30, Loss: 0.971394, Accuracy: 76.88%\n",
      "Batch 31, Loss: 0.996172, Accuracy: 76.81%\n",
      "Batch 32, Loss: 0.916853, Accuracy: 76.95%\n",
      "Batch 33, Loss: 0.971706, Accuracy: 76.89%\n",
      "Batch 34, Loss: 1.011781, Accuracy: 76.70%\n",
      "Batch 35, Loss: 1.007687, Accuracy: 76.61%\n",
      "Batch 36, Loss: 0.957302, Accuracy: 76.61%\n",
      "Batch 37, Loss: 0.916799, Accuracy: 76.82%\n",
      "Batch 38, Loss: 0.973773, Accuracy: 76.81%\n",
      "Batch 39, Loss: 1.021994, Accuracy: 76.64%\n",
      "Batch 40, Loss: 0.958599, Accuracy: 76.64%\n",
      "Batch 41, Loss: 1.060527, Accuracy: 76.41%\n",
      "Batch 42, Loss: 0.945118, Accuracy: 76.53%\n",
      "Batch 43, Loss: 0.939107, Accuracy: 76.64%\n",
      "Batch 44, Loss: 0.970344, Accuracy: 76.63%\n",
      "Batch 45, Loss: 0.978964, Accuracy: 76.60%\n",
      "Batch 46, Loss: 1.057820, Accuracy: 76.36%\n",
      "Batch 47, Loss: 0.955600, Accuracy: 76.36%\n",
      "Batch 48, Loss: 1.063586, Accuracy: 76.17%\n",
      "Batch 49, Loss: 0.948288, Accuracy: 76.18%\n",
      "Batch 50, Loss: 0.987034, Accuracy: 76.25%\n",
      "Batch 51, Loss: 0.909825, Accuracy: 76.38%\n",
      "Batch 52, Loss: 0.869834, Accuracy: 76.59%\n",
      "Batch 53, Loss: 0.970391, Accuracy: 76.62%\n",
      "Batch 54, Loss: 1.010657, Accuracy: 76.59%\n",
      "Batch 55, Loss: 1.050948, Accuracy: 76.42%\n",
      "Batch 56, Loss: 0.928554, Accuracy: 76.53%\n",
      "Batch 57, Loss: 0.995137, Accuracy: 76.54%\n",
      "Batch 58, Loss: 0.930699, Accuracy: 76.64%\n",
      "Batch 59, Loss: 0.942826, Accuracy: 76.75%\n",
      "Batch 60, Loss: 0.941383, Accuracy: 76.85%\n",
      "Batch 61, Loss: 1.026006, Accuracy: 76.79%\n",
      "Batch 62, Loss: 0.900244, Accuracy: 76.89%\n",
      "Batch 63, Loss: 1.053037, Accuracy: 76.76%\n",
      "Batch 64, Loss: 0.866547, Accuracy: 76.98%\n",
      "Batch 65, Loss: 0.969072, Accuracy: 77.00%\n",
      "Batch 66, Loss: 0.975688, Accuracy: 76.99%\n",
      "Batch 67, Loss: 0.955837, Accuracy: 77.03%\n",
      "Batch 68, Loss: 1.029616, Accuracy: 76.95%\n",
      "Batch 69, Loss: 0.938489, Accuracy: 76.99%\n",
      "Batch 70, Loss: 1.023584, Accuracy: 76.92%\n",
      "Batch 71, Loss: 0.942288, Accuracy: 76.98%\n",
      "Batch 72, Loss: 0.943059, Accuracy: 77.02%\n",
      "Batch 73, Loss: 0.942338, Accuracy: 77.05%\n",
      "Batch 74, Loss: 0.968694, Accuracy: 77.05%\n",
      "Batch 75, Loss: 0.993805, Accuracy: 77.02%\n",
      "Batch 76, Loss: 0.914553, Accuracy: 77.08%\n",
      "Batch 77, Loss: 0.965714, Accuracy: 77.09%\n",
      "Batch 78, Loss: 0.948399, Accuracy: 77.12%\n",
      "Batch 79, Loss: 0.986920, Accuracy: 77.12%\n",
      "Batch 80, Loss: 0.948189, Accuracy: 77.13%\n",
      "Batch 81, Loss: 0.972499, Accuracy: 77.14%\n",
      "Batch 82, Loss: 0.910859, Accuracy: 77.23%\n",
      "Batch 83, Loss: 1.009642, Accuracy: 77.16%\n",
      "Batch 84, Loss: 1.004686, Accuracy: 77.12%\n",
      "Batch 85, Loss: 0.970517, Accuracy: 77.13%\n",
      "Batch 86, Loss: 0.923118, Accuracy: 77.18%\n",
      "Batch 87, Loss: 0.923843, Accuracy: 77.23%\n",
      "Batch 88, Loss: 0.982908, Accuracy: 77.22%\n",
      "Batch 89, Loss: 0.939034, Accuracy: 77.25%\n",
      "Batch 90, Loss: 1.011251, Accuracy: 77.20%\n",
      "Batch 91, Loss: 1.079274, Accuracy: 77.08%\n",
      "Batch 92, Loss: 0.946744, Accuracy: 77.12%\n",
      "Batch 93, Loss: 0.960514, Accuracy: 77.13%\n",
      "Batch 94, Loss: 0.993984, Accuracy: 77.09%\n",
      "Batch 95, Loss: 0.925994, Accuracy: 77.14%\n",
      "Batch 96, Loss: 0.896008, Accuracy: 77.21%\n",
      "Batch 97, Loss: 1.011448, Accuracy: 77.17%\n",
      "Batch 98, Loss: 0.962135, Accuracy: 77.18%\n",
      "Batch 99, Loss: 1.035955, Accuracy: 77.15%\n",
      "Batch 100, Loss: 0.920163, Accuracy: 77.19%\n",
      "Batch 101, Loss: 0.964137, Accuracy: 77.18%\n",
      "Batch 102, Loss: 1.000997, Accuracy: 77.14%\n",
      "Batch 103, Loss: 0.980785, Accuracy: 77.14%\n",
      "Batch 104, Loss: 0.940252, Accuracy: 77.18%\n",
      "Batch 105, Loss: 0.938581, Accuracy: 77.20%\n",
      "Batch 106, Loss: 0.926162, Accuracy: 77.24%\n",
      "Batch 107, Loss: 0.851701, Accuracy: 77.35%\n",
      "Batch 108, Loss: 0.964261, Accuracy: 77.36%\n",
      "Batch 109, Loss: 0.954562, Accuracy: 77.37%\n",
      "Batch 110, Loss: 0.953018, Accuracy: 77.37%\n",
      "Batch 111, Loss: 1.000255, Accuracy: 77.34%\n",
      "Batch 112, Loss: 0.932471, Accuracy: 77.36%\n",
      "Batch 113, Loss: 0.947669, Accuracy: 77.38%\n",
      "Batch 114, Loss: 1.028278, Accuracy: 77.32%\n",
      "Batch 115, Loss: 0.929445, Accuracy: 77.35%\n",
      "Batch 116, Loss: 0.968562, Accuracy: 77.36%\n",
      "Batch 117, Loss: 0.986496, Accuracy: 77.35%\n",
      "Batch 118, Loss: 0.947341, Accuracy: 77.37%\n",
      "Batch 119, Loss: 0.940573, Accuracy: 77.40%\n",
      "Batch 120, Loss: 1.032341, Accuracy: 77.33%\n",
      "Batch 121, Loss: 0.947307, Accuracy: 77.35%\n",
      "Batch 122, Loss: 0.913790, Accuracy: 77.39%\n",
      "Batch 123, Loss: 0.875945, Accuracy: 77.48%\n",
      "Batch 124, Loss: 0.939255, Accuracy: 77.49%\n",
      "Batch 125, Loss: 1.095231, Accuracy: 77.39%\n",
      "Batch 126, Loss: 0.896372, Accuracy: 77.43%\n",
      "Batch 127, Loss: 0.998378, Accuracy: 77.42%\n",
      "Batch 128, Loss: 1.000985, Accuracy: 77.40%\n",
      "Batch 129, Loss: 1.048692, Accuracy: 77.34%\n",
      "Batch 130, Loss: 0.997749, Accuracy: 77.31%\n",
      "Batch 131, Loss: 0.983348, Accuracy: 77.29%\n",
      "Batch 132, Loss: 0.945851, Accuracy: 77.31%\n",
      "Batch 133, Loss: 0.968658, Accuracy: 77.30%\n",
      "Batch 134, Loss: 1.018035, Accuracy: 77.26%\n",
      "Batch 135, Loss: 0.973128, Accuracy: 77.27%\n",
      "Batch 136, Loss: 1.026508, Accuracy: 77.24%\n",
      "Batch 137, Loss: 0.961715, Accuracy: 77.25%\n",
      "Batch 138, Loss: 1.003204, Accuracy: 77.22%\n",
      "Batch 139, Loss: 0.930402, Accuracy: 77.26%\n",
      "Batch 140, Loss: 0.949560, Accuracy: 77.28%\n",
      "Batch 141, Loss: 0.940487, Accuracy: 77.30%\n",
      "Batch 142, Loss: 0.986373, Accuracy: 77.29%\n",
      "Batch 143, Loss: 0.868176, Accuracy: 77.37%\n",
      "Batch 144, Loss: 1.025576, Accuracy: 77.32%\n",
      "Batch 145, Loss: 0.964051, Accuracy: 77.33%\n",
      "Batch 146, Loss: 0.863133, Accuracy: 77.40%\n",
      "Batch 147, Loss: 0.927447, Accuracy: 77.42%\n",
      "Batch 148, Loss: 1.000168, Accuracy: 77.41%\n",
      "Batch 149, Loss: 0.948999, Accuracy: 77.42%\n",
      "Batch 150, Loss: 0.968379, Accuracy: 77.43%\n",
      "Batch 151, Loss: 1.003400, Accuracy: 77.40%\n",
      "Batch 152, Loss: 0.946806, Accuracy: 77.43%\n",
      "Batch 153, Loss: 1.068233, Accuracy: 77.36%\n",
      "Batch 154, Loss: 0.906547, Accuracy: 77.41%\n",
      "Batch 155, Loss: 0.952466, Accuracy: 77.42%\n",
      "Batch 156, Loss: 0.958533, Accuracy: 77.43%\n",
      "Batch 157, Loss: 0.954769, Accuracy: 77.45%\n",
      "Batch 158, Loss: 0.948854, Accuracy: 77.46%\n",
      "Batch 159, Loss: 1.000269, Accuracy: 77.44%\n",
      "Batch 160, Loss: 0.948056, Accuracy: 77.45%\n",
      "Batch 161, Loss: 0.915231, Accuracy: 77.48%\n",
      "Batch 162, Loss: 0.970479, Accuracy: 77.50%\n",
      "Batch 163, Loss: 0.920111, Accuracy: 77.53%\n",
      "Batch 164, Loss: 0.942622, Accuracy: 77.54%\n",
      "Batch 165, Loss: 0.986950, Accuracy: 77.53%\n",
      "Batch 166, Loss: 0.942731, Accuracy: 77.53%\n",
      "Batch 167, Loss: 0.958875, Accuracy: 77.54%\n",
      "Batch 168, Loss: 1.017247, Accuracy: 77.50%\n",
      "Batch 169, Loss: 1.044730, Accuracy: 77.46%\n",
      "Batch 170, Loss: 0.938820, Accuracy: 77.47%\n",
      "Batch 171, Loss: 0.922686, Accuracy: 77.50%\n",
      "Batch 172, Loss: 0.917695, Accuracy: 77.53%\n",
      "Batch 173, Loss: 1.007747, Accuracy: 77.50%\n",
      "Batch 174, Loss: 0.987978, Accuracy: 77.49%\n",
      "Batch 175, Loss: 0.909012, Accuracy: 77.52%\n",
      "Batch 176, Loss: 0.960698, Accuracy: 77.53%\n",
      "Batch 177, Loss: 0.926265, Accuracy: 77.56%\n",
      "Batch 178, Loss: 0.902680, Accuracy: 77.60%\n",
      "Batch 179, Loss: 0.981713, Accuracy: 77.59%\n",
      "Batch 180, Loss: 1.012601, Accuracy: 77.56%\n",
      "Batch 181, Loss: 0.923931, Accuracy: 77.59%\n",
      "Batch 182, Loss: 0.965894, Accuracy: 77.59%\n",
      "Batch 183, Loss: 0.960670, Accuracy: 77.60%\n",
      "Batch 184, Loss: 0.957342, Accuracy: 77.60%\n",
      "Batch 185, Loss: 0.969029, Accuracy: 77.60%\n",
      "Batch 186, Loss: 1.022147, Accuracy: 77.58%\n",
      "Batch 187, Loss: 0.940920, Accuracy: 77.59%\n",
      "Batch 188, Loss: 1.066906, Accuracy: 77.53%\n",
      "Batch 189, Loss: 1.054816, Accuracy: 77.50%\n",
      "Batch 190, Loss: 0.934905, Accuracy: 77.52%\n",
      "Batch 191, Loss: 1.046121, Accuracy: 77.49%\n",
      "Batch 192, Loss: 0.903114, Accuracy: 77.52%\n",
      "Batch 193, Loss: 1.012215, Accuracy: 77.50%\n",
      "Batch 194, Loss: 0.936671, Accuracy: 77.52%\n",
      "Batch 195, Loss: 0.897760, Accuracy: 77.56%\n",
      "Batch 196, Loss: 1.025360, Accuracy: 77.53%\n",
      "Batch 197, Loss: 0.956160, Accuracy: 77.54%\n",
      "Batch 198, Loss: 0.945473, Accuracy: 77.55%\n",
      "Batch 199, Loss: 0.958419, Accuracy: 77.56%\n",
      "Batch 200, Loss: 0.974460, Accuracy: 77.55%\n",
      "Batch 201, Loss: 1.044734, Accuracy: 77.53%\n",
      "Batch 202, Loss: 0.941352, Accuracy: 77.54%\n",
      "Batch 203, Loss: 0.919776, Accuracy: 77.58%\n",
      "Batch 204, Loss: 0.999491, Accuracy: 77.55%\n",
      "Batch 205, Loss: 0.891240, Accuracy: 77.59%\n",
      "Batch 206, Loss: 0.931008, Accuracy: 77.61%\n",
      "Batch 207, Loss: 0.986608, Accuracy: 77.60%\n",
      "Batch 208, Loss: 0.937719, Accuracy: 77.61%\n",
      "Batch 209, Loss: 0.893569, Accuracy: 77.65%\n",
      "Batch 210, Loss: 0.913888, Accuracy: 77.68%\n",
      "Batch 211, Loss: 1.007610, Accuracy: 77.65%\n",
      "Batch 212, Loss: 0.967837, Accuracy: 77.65%\n",
      "Batch 213, Loss: 1.002446, Accuracy: 77.62%\n",
      "Training - Epoch 85, Loss: 0.967050, Accuracy: 77.62%\n",
      "Validation Batch 1, Loss: 0.935937, Accuracy: 82.81%\n",
      "Validation Batch 2, Loss: 0.981429, Accuracy: 78.91%\n",
      "Validation Batch 3, Loss: 1.017405, Accuracy: 77.08%\n",
      "Validation Batch 4, Loss: 0.971601, Accuracy: 76.17%\n",
      "Validation Batch 5, Loss: 0.946693, Accuracy: 76.88%\n",
      "Validation Batch 6, Loss: 0.909222, Accuracy: 77.86%\n",
      "Validation Batch 7, Loss: 0.987032, Accuracy: 77.46%\n",
      "Validation Batch 8, Loss: 1.015957, Accuracy: 76.56%\n",
      "Validation Batch 9, Loss: 1.023306, Accuracy: 75.87%\n",
      "Validation Batch 10, Loss: 0.998763, Accuracy: 75.78%\n",
      "Validation Batch 11, Loss: 0.933960, Accuracy: 76.42%\n",
      "Validation Batch 12, Loss: 0.914192, Accuracy: 77.08%\n",
      "Validation Batch 13, Loss: 1.054243, Accuracy: 76.44%\n",
      "Validation Batch 14, Loss: 1.003510, Accuracy: 76.34%\n",
      "Validation Batch 15, Loss: 0.969445, Accuracy: 76.46%\n",
      "Validation Batch 16, Loss: 0.932039, Accuracy: 76.86%\n",
      "Validation Batch 17, Loss: 0.985069, Accuracy: 76.84%\n",
      "Validation Batch 18, Loss: 0.964502, Accuracy: 76.91%\n",
      "Validation Batch 19, Loss: 1.004471, Accuracy: 76.81%\n",
      "Validation Batch 20, Loss: 0.972015, Accuracy: 76.80%\n",
      "Validation Batch 21, Loss: 0.974450, Accuracy: 76.79%\n",
      "Validation Batch 22, Loss: 1.007967, Accuracy: 76.63%\n",
      "Validation Batch 23, Loss: 1.063824, Accuracy: 76.22%\n",
      "Validation Batch 24, Loss: 1.051267, Accuracy: 75.91%\n",
      "Validation Batch 25, Loss: 0.948345, Accuracy: 76.06%\n",
      "Validation Batch 26, Loss: 0.987881, Accuracy: 76.02%\n",
      "Validation Batch 27, Loss: 0.937060, Accuracy: 76.16%\n",
      "Validation - Epoch 85, Loss: 0.981170, Accuracy: 76.16%\n",
      "Patienceâ€”6\n",
      "Epoch 86\n",
      "Batch 1, Loss: 0.957452, Accuracy: 78.12%\n",
      "Batch 2, Loss: 1.046866, Accuracy: 73.44%\n",
      "Batch 3, Loss: 1.023112, Accuracy: 72.92%\n",
      "Batch 4, Loss: 0.931809, Accuracy: 75.00%\n",
      "Batch 5, Loss: 0.959510, Accuracy: 75.62%\n",
      "Batch 6, Loss: 0.896409, Accuracy: 77.34%\n",
      "Batch 7, Loss: 0.978825, Accuracy: 77.23%\n",
      "Batch 8, Loss: 0.919875, Accuracy: 77.93%\n",
      "Batch 9, Loss: 1.015615, Accuracy: 77.08%\n",
      "Batch 10, Loss: 0.916226, Accuracy: 77.66%\n",
      "Batch 11, Loss: 0.895313, Accuracy: 78.41%\n",
      "Batch 12, Loss: 0.984456, Accuracy: 78.12%\n",
      "Batch 13, Loss: 0.999435, Accuracy: 77.88%\n",
      "Batch 14, Loss: 0.899056, Accuracy: 78.24%\n",
      "Batch 15, Loss: 0.931715, Accuracy: 78.44%\n",
      "Batch 16, Loss: 1.007741, Accuracy: 78.12%\n",
      "Batch 17, Loss: 0.929742, Accuracy: 78.31%\n",
      "Batch 18, Loss: 0.901782, Accuracy: 78.73%\n",
      "Batch 19, Loss: 0.984078, Accuracy: 78.62%\n",
      "Batch 20, Loss: 1.102477, Accuracy: 77.81%\n",
      "Batch 21, Loss: 0.986044, Accuracy: 77.83%\n",
      "Batch 22, Loss: 0.961656, Accuracy: 77.77%\n",
      "Batch 23, Loss: 0.979138, Accuracy: 77.65%\n",
      "Batch 24, Loss: 0.953073, Accuracy: 77.67%\n",
      "Batch 25, Loss: 0.994476, Accuracy: 77.56%\n",
      "Batch 26, Loss: 0.971562, Accuracy: 77.52%\n",
      "Batch 27, Loss: 0.973559, Accuracy: 77.43%\n",
      "Batch 28, Loss: 0.910529, Accuracy: 77.62%\n",
      "Batch 29, Loss: 0.995864, Accuracy: 77.48%\n",
      "Batch 30, Loss: 0.971478, Accuracy: 77.45%\n",
      "Batch 31, Loss: 0.962473, Accuracy: 77.52%\n",
      "Batch 32, Loss: 1.047699, Accuracy: 77.29%\n",
      "Batch 33, Loss: 0.907603, Accuracy: 77.51%\n",
      "Batch 34, Loss: 0.960728, Accuracy: 77.44%\n",
      "Batch 35, Loss: 0.961018, Accuracy: 77.46%\n",
      "Batch 36, Loss: 0.976150, Accuracy: 77.43%\n",
      "Batch 37, Loss: 1.036797, Accuracy: 77.24%\n",
      "Batch 38, Loss: 1.010773, Accuracy: 77.14%\n",
      "Batch 39, Loss: 1.044124, Accuracy: 76.92%\n",
      "Batch 40, Loss: 1.057781, Accuracy: 76.72%\n",
      "Batch 41, Loss: 0.946629, Accuracy: 76.79%\n",
      "Batch 42, Loss: 0.931386, Accuracy: 76.90%\n",
      "Batch 43, Loss: 0.926886, Accuracy: 77.00%\n",
      "Batch 44, Loss: 0.991490, Accuracy: 76.95%\n",
      "Batch 45, Loss: 1.011830, Accuracy: 76.88%\n",
      "Batch 46, Loss: 0.910686, Accuracy: 77.07%\n",
      "Batch 47, Loss: 0.994794, Accuracy: 77.03%\n",
      "Batch 48, Loss: 0.984578, Accuracy: 76.95%\n",
      "Batch 49, Loss: 0.906301, Accuracy: 77.07%\n",
      "Batch 50, Loss: 0.939159, Accuracy: 77.16%\n",
      "Batch 51, Loss: 0.953821, Accuracy: 77.18%\n",
      "Batch 52, Loss: 0.950219, Accuracy: 77.22%\n",
      "Batch 53, Loss: 0.933760, Accuracy: 77.30%\n",
      "Batch 54, Loss: 1.045087, Accuracy: 77.11%\n",
      "Batch 55, Loss: 0.897816, Accuracy: 77.24%\n",
      "Batch 56, Loss: 0.906835, Accuracy: 77.34%\n",
      "Batch 57, Loss: 0.977921, Accuracy: 77.33%\n",
      "Batch 58, Loss: 0.971330, Accuracy: 77.34%\n",
      "Batch 59, Loss: 0.955599, Accuracy: 77.33%\n",
      "Batch 60, Loss: 1.002572, Accuracy: 77.29%\n",
      "Batch 61, Loss: 0.956621, Accuracy: 77.31%\n",
      "Batch 62, Loss: 0.941074, Accuracy: 77.37%\n",
      "Batch 63, Loss: 0.961078, Accuracy: 77.38%\n",
      "Batch 64, Loss: 0.977772, Accuracy: 77.37%\n",
      "Batch 65, Loss: 0.987861, Accuracy: 77.31%\n",
      "Batch 66, Loss: 0.916817, Accuracy: 77.37%\n",
      "Batch 67, Loss: 0.933307, Accuracy: 77.45%\n",
      "Batch 68, Loss: 0.914749, Accuracy: 77.55%\n",
      "Batch 69, Loss: 0.965762, Accuracy: 77.56%\n",
      "Batch 70, Loss: 0.997644, Accuracy: 77.50%\n",
      "Batch 71, Loss: 0.909451, Accuracy: 77.60%\n",
      "Batch 72, Loss: 1.049648, Accuracy: 77.47%\n",
      "Batch 73, Loss: 0.908550, Accuracy: 77.57%\n",
      "Batch 74, Loss: 0.966642, Accuracy: 77.58%\n",
      "Batch 75, Loss: 1.015030, Accuracy: 77.50%\n",
      "Batch 76, Loss: 1.029251, Accuracy: 77.43%\n",
      "Batch 77, Loss: 1.005942, Accuracy: 77.37%\n",
      "Batch 78, Loss: 0.962186, Accuracy: 77.38%\n",
      "Batch 79, Loss: 0.874905, Accuracy: 77.53%\n",
      "Batch 80, Loss: 0.934458, Accuracy: 77.60%\n",
      "Batch 81, Loss: 1.000265, Accuracy: 77.53%\n",
      "Batch 82, Loss: 0.944004, Accuracy: 77.57%\n",
      "Batch 83, Loss: 1.067464, Accuracy: 77.47%\n",
      "Batch 84, Loss: 0.957268, Accuracy: 77.47%\n",
      "Batch 85, Loss: 1.009324, Accuracy: 77.41%\n",
      "Batch 86, Loss: 0.955742, Accuracy: 77.42%\n",
      "Batch 87, Loss: 0.994811, Accuracy: 77.37%\n",
      "Batch 88, Loss: 0.987034, Accuracy: 77.36%\n",
      "Batch 89, Loss: 0.949375, Accuracy: 77.37%\n",
      "Batch 90, Loss: 0.822917, Accuracy: 77.53%\n",
      "Batch 91, Loss: 0.994654, Accuracy: 77.52%\n",
      "Batch 92, Loss: 0.936633, Accuracy: 77.55%\n",
      "Batch 93, Loss: 0.998595, Accuracy: 77.52%\n",
      "Batch 94, Loss: 1.029103, Accuracy: 77.46%\n",
      "Batch 95, Loss: 0.933629, Accuracy: 77.48%\n",
      "Batch 96, Loss: 0.944290, Accuracy: 77.52%\n",
      "Batch 97, Loss: 1.033913, Accuracy: 77.46%\n",
      "Batch 98, Loss: 0.928083, Accuracy: 77.50%\n",
      "Batch 99, Loss: 0.952212, Accuracy: 77.54%\n",
      "Batch 100, Loss: 0.962634, Accuracy: 77.55%\n",
      "Batch 101, Loss: 0.953092, Accuracy: 77.57%\n",
      "Batch 102, Loss: 0.971762, Accuracy: 77.56%\n",
      "Batch 103, Loss: 0.967832, Accuracy: 77.56%\n",
      "Batch 104, Loss: 0.977180, Accuracy: 77.55%\n",
      "Batch 105, Loss: 0.993478, Accuracy: 77.53%\n",
      "Batch 106, Loss: 1.015046, Accuracy: 77.46%\n",
      "Batch 107, Loss: 0.949231, Accuracy: 77.51%\n",
      "Batch 108, Loss: 0.966997, Accuracy: 77.49%\n",
      "Batch 109, Loss: 1.009196, Accuracy: 77.45%\n",
      "Batch 110, Loss: 0.969294, Accuracy: 77.46%\n",
      "Batch 111, Loss: 0.990439, Accuracy: 77.45%\n",
      "Batch 112, Loss: 0.978370, Accuracy: 77.43%\n",
      "Batch 113, Loss: 0.936540, Accuracy: 77.46%\n",
      "Batch 114, Loss: 1.036718, Accuracy: 77.38%\n",
      "Batch 115, Loss: 0.915727, Accuracy: 77.43%\n",
      "Batch 116, Loss: 0.911062, Accuracy: 77.51%\n",
      "Batch 117, Loss: 1.020642, Accuracy: 77.48%\n",
      "Batch 118, Loss: 0.963987, Accuracy: 77.50%\n",
      "Batch 119, Loss: 0.976953, Accuracy: 77.48%\n",
      "Batch 120, Loss: 0.910090, Accuracy: 77.53%\n",
      "Batch 121, Loss: 0.915285, Accuracy: 77.58%\n",
      "Batch 122, Loss: 1.071480, Accuracy: 77.48%\n",
      "Batch 123, Loss: 0.989751, Accuracy: 77.48%\n",
      "Batch 124, Loss: 0.935530, Accuracy: 77.49%\n",
      "Batch 125, Loss: 0.901512, Accuracy: 77.54%\n",
      "Batch 126, Loss: 0.901203, Accuracy: 77.60%\n",
      "Batch 127, Loss: 0.995690, Accuracy: 77.60%\n",
      "Batch 128, Loss: 0.950157, Accuracy: 77.61%\n",
      "Batch 129, Loss: 0.969879, Accuracy: 77.62%\n",
      "Batch 130, Loss: 1.019783, Accuracy: 77.58%\n",
      "Batch 131, Loss: 0.968062, Accuracy: 77.58%\n",
      "Batch 132, Loss: 0.952565, Accuracy: 77.59%\n",
      "Batch 133, Loss: 0.982840, Accuracy: 77.57%\n",
      "Batch 134, Loss: 1.021874, Accuracy: 77.52%\n",
      "Batch 135, Loss: 0.898651, Accuracy: 77.58%\n",
      "Batch 136, Loss: 1.036623, Accuracy: 77.55%\n",
      "Batch 137, Loss: 0.974551, Accuracy: 77.55%\n",
      "Batch 138, Loss: 1.039952, Accuracy: 77.50%\n",
      "Batch 139, Loss: 0.972185, Accuracy: 77.48%\n",
      "Batch 140, Loss: 1.013546, Accuracy: 77.44%\n",
      "Batch 141, Loss: 0.902925, Accuracy: 77.49%\n",
      "Batch 142, Loss: 0.995279, Accuracy: 77.48%\n",
      "Batch 143, Loss: 0.941934, Accuracy: 77.50%\n",
      "Batch 144, Loss: 0.919963, Accuracy: 77.54%\n",
      "Batch 145, Loss: 0.979270, Accuracy: 77.52%\n",
      "Batch 146, Loss: 0.998492, Accuracy: 77.50%\n",
      "Batch 147, Loss: 0.966197, Accuracy: 77.50%\n",
      "Batch 148, Loss: 1.011141, Accuracy: 77.45%\n",
      "Batch 149, Loss: 0.935329, Accuracy: 77.47%\n",
      "Batch 150, Loss: 0.960914, Accuracy: 77.49%\n",
      "Batch 151, Loss: 1.013986, Accuracy: 77.45%\n",
      "Batch 152, Loss: 0.959205, Accuracy: 77.47%\n",
      "Batch 153, Loss: 0.972978, Accuracy: 77.47%\n",
      "Batch 154, Loss: 1.050181, Accuracy: 77.39%\n",
      "Batch 155, Loss: 1.038520, Accuracy: 77.35%\n",
      "Batch 156, Loss: 0.947911, Accuracy: 77.36%\n",
      "Batch 157, Loss: 1.017097, Accuracy: 77.32%\n",
      "Batch 158, Loss: 0.920537, Accuracy: 77.35%\n",
      "Batch 159, Loss: 0.949307, Accuracy: 77.38%\n",
      "Batch 160, Loss: 0.947626, Accuracy: 77.39%\n",
      "Batch 161, Loss: 0.930075, Accuracy: 77.41%\n",
      "Batch 162, Loss: 0.943835, Accuracy: 77.43%\n",
      "Batch 163, Loss: 0.934808, Accuracy: 77.45%\n",
      "Batch 164, Loss: 1.040096, Accuracy: 77.40%\n",
      "Batch 165, Loss: 0.997733, Accuracy: 77.37%\n",
      "Batch 166, Loss: 0.973146, Accuracy: 77.36%\n",
      "Batch 167, Loss: 0.943620, Accuracy: 77.38%\n",
      "Batch 168, Loss: 0.949200, Accuracy: 77.38%\n",
      "Batch 169, Loss: 1.013542, Accuracy: 77.33%\n",
      "Batch 170, Loss: 0.957530, Accuracy: 77.33%\n",
      "Batch 171, Loss: 1.002134, Accuracy: 77.31%\n",
      "Batch 172, Loss: 0.893405, Accuracy: 77.36%\n",
      "Batch 173, Loss: 0.995647, Accuracy: 77.34%\n",
      "Batch 174, Loss: 0.958440, Accuracy: 77.35%\n",
      "Batch 175, Loss: 0.951834, Accuracy: 77.37%\n",
      "Batch 176, Loss: 0.871665, Accuracy: 77.42%\n",
      "Batch 177, Loss: 0.973190, Accuracy: 77.43%\n",
      "Batch 178, Loss: 0.928537, Accuracy: 77.46%\n",
      "Batch 179, Loss: 1.029660, Accuracy: 77.43%\n",
      "Batch 180, Loss: 0.925140, Accuracy: 77.45%\n",
      "Batch 181, Loss: 0.906186, Accuracy: 77.49%\n",
      "Batch 182, Loss: 0.916941, Accuracy: 77.52%\n",
      "Batch 183, Loss: 0.936171, Accuracy: 77.54%\n",
      "Batch 184, Loss: 0.911244, Accuracy: 77.56%\n",
      "Batch 185, Loss: 0.954383, Accuracy: 77.57%\n",
      "Batch 186, Loss: 1.008001, Accuracy: 77.54%\n",
      "Batch 187, Loss: 0.957976, Accuracy: 77.54%\n",
      "Batch 188, Loss: 0.940623, Accuracy: 77.54%\n",
      "Batch 189, Loss: 1.004975, Accuracy: 77.53%\n",
      "Batch 190, Loss: 0.970484, Accuracy: 77.52%\n",
      "Batch 191, Loss: 0.978652, Accuracy: 77.52%\n",
      "Batch 192, Loss: 0.946620, Accuracy: 77.54%\n",
      "Batch 193, Loss: 0.986743, Accuracy: 77.53%\n",
      "Batch 194, Loss: 1.029448, Accuracy: 77.49%\n",
      "Batch 195, Loss: 0.990513, Accuracy: 77.49%\n",
      "Batch 196, Loss: 1.019732, Accuracy: 77.46%\n",
      "Batch 197, Loss: 1.004866, Accuracy: 77.44%\n",
      "Batch 198, Loss: 0.967071, Accuracy: 77.44%\n",
      "Batch 199, Loss: 0.982985, Accuracy: 77.43%\n",
      "Batch 200, Loss: 0.831340, Accuracy: 77.50%\n",
      "Batch 201, Loss: 0.978032, Accuracy: 77.50%\n",
      "Batch 202, Loss: 0.900851, Accuracy: 77.52%\n",
      "Batch 203, Loss: 0.987070, Accuracy: 77.50%\n",
      "Batch 204, Loss: 0.998450, Accuracy: 77.50%\n",
      "Batch 205, Loss: 0.939713, Accuracy: 77.50%\n",
      "Batch 206, Loss: 1.012700, Accuracy: 77.47%\n",
      "Batch 207, Loss: 0.943440, Accuracy: 77.48%\n",
      "Batch 208, Loss: 0.919011, Accuracy: 77.50%\n",
      "Batch 209, Loss: 1.011791, Accuracy: 77.48%\n",
      "Batch 210, Loss: 0.931072, Accuracy: 77.50%\n",
      "Batch 211, Loss: 1.052740, Accuracy: 77.46%\n",
      "Batch 212, Loss: 0.981396, Accuracy: 77.45%\n",
      "Batch 213, Loss: 0.920292, Accuracy: 77.48%\n",
      "Training - Epoch 86, Loss: 0.967520, Accuracy: 77.48%\n",
      "Validation Batch 1, Loss: 0.932190, Accuracy: 82.81%\n",
      "Validation Batch 2, Loss: 0.981242, Accuracy: 78.91%\n",
      "Validation Batch 3, Loss: 1.017893, Accuracy: 77.08%\n",
      "Validation Batch 4, Loss: 0.970843, Accuracy: 76.95%\n",
      "Validation Batch 5, Loss: 0.944472, Accuracy: 77.50%\n",
      "Validation Batch 6, Loss: 0.910359, Accuracy: 78.39%\n",
      "Validation Batch 7, Loss: 0.986170, Accuracy: 77.90%\n",
      "Validation Batch 8, Loss: 1.009193, Accuracy: 77.15%\n",
      "Validation Batch 9, Loss: 1.022153, Accuracy: 76.39%\n",
      "Validation Batch 10, Loss: 0.997471, Accuracy: 76.25%\n",
      "Validation Batch 11, Loss: 0.934106, Accuracy: 76.70%\n",
      "Validation Batch 12, Loss: 0.908158, Accuracy: 77.34%\n",
      "Validation Batch 13, Loss: 1.048312, Accuracy: 76.68%\n",
      "Validation Batch 14, Loss: 0.996301, Accuracy: 76.56%\n",
      "Validation Batch 15, Loss: 0.973173, Accuracy: 76.67%\n",
      "Validation Batch 16, Loss: 0.929942, Accuracy: 77.05%\n",
      "Validation Batch 17, Loss: 0.975750, Accuracy: 77.02%\n",
      "Validation Batch 18, Loss: 0.959750, Accuracy: 77.17%\n",
      "Validation Batch 19, Loss: 0.993754, Accuracy: 77.06%\n",
      "Validation Batch 20, Loss: 0.965699, Accuracy: 77.03%\n",
      "Validation Batch 21, Loss: 0.974392, Accuracy: 77.16%\n",
      "Validation Batch 22, Loss: 1.001460, Accuracy: 77.06%\n",
      "Validation Batch 23, Loss: 1.060608, Accuracy: 76.56%\n",
      "Validation Batch 24, Loss: 1.045843, Accuracy: 76.24%\n",
      "Validation Batch 25, Loss: 0.953346, Accuracy: 76.38%\n",
      "Validation Batch 26, Loss: 0.984796, Accuracy: 76.32%\n",
      "Validation Batch 27, Loss: 0.939566, Accuracy: 76.45%\n",
      "Validation - Epoch 86, Loss: 0.978405, Accuracy: 76.45%\n",
      "Patienceâ€”7\n",
      "Epoch 87\n",
      "Batch 1, Loss: 0.936759, Accuracy: 81.25%\n",
      "Batch 2, Loss: 0.961916, Accuracy: 79.69%\n",
      "Batch 3, Loss: 0.913392, Accuracy: 80.73%\n",
      "Batch 4, Loss: 1.029364, Accuracy: 78.52%\n",
      "Batch 5, Loss: 0.952971, Accuracy: 78.75%\n",
      "Batch 6, Loss: 0.975431, Accuracy: 78.65%\n",
      "Batch 7, Loss: 1.014575, Accuracy: 77.90%\n",
      "Batch 8, Loss: 0.946835, Accuracy: 78.12%\n",
      "Batch 9, Loss: 0.915754, Accuracy: 78.82%\n",
      "Batch 10, Loss: 1.039443, Accuracy: 77.81%\n",
      "Batch 11, Loss: 0.972980, Accuracy: 77.70%\n",
      "Batch 12, Loss: 1.000453, Accuracy: 77.34%\n",
      "Batch 13, Loss: 1.001098, Accuracy: 76.92%\n",
      "Batch 14, Loss: 0.943068, Accuracy: 77.23%\n",
      "Batch 15, Loss: 0.915970, Accuracy: 77.60%\n",
      "Batch 16, Loss: 0.934888, Accuracy: 77.83%\n",
      "Batch 17, Loss: 0.925027, Accuracy: 78.12%\n",
      "Batch 18, Loss: 0.985073, Accuracy: 78.04%\n",
      "Batch 19, Loss: 0.852997, Accuracy: 78.62%\n",
      "Batch 20, Loss: 0.934188, Accuracy: 78.75%\n",
      "Batch 21, Loss: 0.941816, Accuracy: 78.87%\n",
      "Batch 22, Loss: 0.970604, Accuracy: 78.76%\n",
      "Batch 23, Loss: 1.054948, Accuracy: 78.26%\n",
      "Batch 24, Loss: 0.957125, Accuracy: 78.32%\n",
      "Batch 25, Loss: 0.955371, Accuracy: 78.38%\n",
      "Batch 26, Loss: 0.955206, Accuracy: 78.43%\n",
      "Batch 27, Loss: 0.843740, Accuracy: 78.82%\n",
      "Batch 28, Loss: 1.053722, Accuracy: 78.46%\n",
      "Batch 29, Loss: 0.908037, Accuracy: 78.66%\n",
      "Batch 30, Loss: 0.993260, Accuracy: 78.59%\n",
      "Batch 31, Loss: 0.928904, Accuracy: 78.68%\n",
      "Batch 32, Loss: 0.975846, Accuracy: 78.61%\n",
      "Batch 33, Loss: 0.962923, Accuracy: 78.60%\n",
      "Batch 34, Loss: 0.991517, Accuracy: 78.45%\n",
      "Batch 35, Loss: 0.962366, Accuracy: 78.39%\n",
      "Batch 36, Loss: 0.932468, Accuracy: 78.52%\n",
      "Batch 37, Loss: 1.053673, Accuracy: 78.21%\n",
      "Batch 38, Loss: 0.981334, Accuracy: 78.12%\n",
      "Batch 39, Loss: 0.863960, Accuracy: 78.37%\n",
      "Batch 40, Loss: 0.926410, Accuracy: 78.48%\n",
      "Batch 41, Loss: 0.979783, Accuracy: 78.43%\n",
      "Batch 42, Loss: 1.090290, Accuracy: 78.09%\n",
      "Batch 43, Loss: 0.951141, Accuracy: 78.09%\n",
      "Batch 44, Loss: 0.914702, Accuracy: 78.23%\n",
      "Batch 45, Loss: 0.981953, Accuracy: 78.16%\n",
      "Batch 46, Loss: 0.963911, Accuracy: 78.16%\n",
      "Batch 47, Loss: 0.931351, Accuracy: 78.22%\n",
      "Batch 48, Loss: 0.956059, Accuracy: 78.22%\n",
      "Batch 49, Loss: 0.976305, Accuracy: 78.22%\n",
      "Batch 50, Loss: 0.949745, Accuracy: 78.22%\n",
      "Batch 51, Loss: 0.921199, Accuracy: 78.31%\n",
      "Batch 52, Loss: 0.974343, Accuracy: 78.25%\n",
      "Batch 53, Loss: 0.861589, Accuracy: 78.42%\n",
      "Batch 54, Loss: 0.931023, Accuracy: 78.44%\n",
      "Batch 55, Loss: 0.882707, Accuracy: 78.61%\n",
      "Batch 56, Loss: 0.900310, Accuracy: 78.74%\n",
      "Batch 57, Loss: 0.961170, Accuracy: 78.73%\n",
      "Batch 58, Loss: 1.033620, Accuracy: 78.58%\n",
      "Batch 59, Loss: 0.997239, Accuracy: 78.50%\n",
      "Batch 60, Loss: 0.890309, Accuracy: 78.62%\n",
      "Batch 61, Loss: 0.963297, Accuracy: 78.59%\n",
      "Batch 62, Loss: 1.036084, Accuracy: 78.43%\n",
      "Batch 63, Loss: 0.939009, Accuracy: 78.47%\n",
      "Batch 64, Loss: 0.949255, Accuracy: 78.49%\n",
      "Batch 65, Loss: 1.001015, Accuracy: 78.44%\n",
      "Batch 66, Loss: 0.965537, Accuracy: 78.41%\n",
      "Batch 67, Loss: 0.959736, Accuracy: 78.40%\n",
      "Batch 68, Loss: 0.903669, Accuracy: 78.49%\n",
      "Batch 69, Loss: 0.886984, Accuracy: 78.58%\n",
      "Batch 70, Loss: 1.079599, Accuracy: 78.37%\n",
      "Batch 71, Loss: 0.928327, Accuracy: 78.41%\n",
      "Batch 72, Loss: 0.977698, Accuracy: 78.39%\n",
      "Batch 73, Loss: 0.970215, Accuracy: 78.34%\n",
      "Batch 74, Loss: 0.954188, Accuracy: 78.34%\n",
      "Batch 75, Loss: 0.932232, Accuracy: 78.35%\n",
      "Batch 76, Loss: 0.976865, Accuracy: 78.33%\n",
      "Batch 77, Loss: 0.897685, Accuracy: 78.43%\n",
      "Batch 78, Loss: 1.032842, Accuracy: 78.31%\n",
      "Batch 79, Loss: 0.994110, Accuracy: 78.26%\n",
      "Batch 80, Loss: 0.917938, Accuracy: 78.32%\n",
      "Batch 81, Loss: 1.061208, Accuracy: 78.18%\n",
      "Batch 82, Loss: 0.920439, Accuracy: 78.22%\n",
      "Batch 83, Loss: 0.943644, Accuracy: 78.26%\n",
      "Batch 84, Loss: 0.931052, Accuracy: 78.29%\n",
      "Batch 85, Loss: 0.940367, Accuracy: 78.33%\n",
      "Batch 86, Loss: 1.008402, Accuracy: 78.27%\n",
      "Batch 87, Loss: 1.120308, Accuracy: 78.09%\n",
      "Batch 88, Loss: 0.959134, Accuracy: 78.11%\n",
      "Batch 89, Loss: 0.926473, Accuracy: 78.14%\n",
      "Batch 90, Loss: 1.025282, Accuracy: 78.06%\n",
      "Batch 91, Loss: 1.012183, Accuracy: 78.00%\n",
      "Batch 92, Loss: 0.875903, Accuracy: 78.12%\n",
      "Batch 93, Loss: 0.918470, Accuracy: 78.18%\n",
      "Batch 94, Loss: 0.959152, Accuracy: 78.19%\n",
      "Batch 95, Loss: 0.982992, Accuracy: 78.19%\n",
      "Batch 96, Loss: 0.970675, Accuracy: 78.17%\n",
      "Batch 97, Loss: 0.987448, Accuracy: 78.14%\n",
      "Batch 98, Loss: 1.016055, Accuracy: 78.09%\n",
      "Batch 99, Loss: 0.909970, Accuracy: 78.17%\n",
      "Batch 100, Loss: 0.917254, Accuracy: 78.22%\n",
      "Batch 101, Loss: 0.960711, Accuracy: 78.20%\n",
      "Batch 102, Loss: 0.948009, Accuracy: 78.22%\n",
      "Batch 103, Loss: 0.936132, Accuracy: 78.23%\n",
      "Batch 104, Loss: 0.996838, Accuracy: 78.20%\n",
      "Batch 105, Loss: 0.973627, Accuracy: 78.18%\n",
      "Batch 106, Loss: 0.950970, Accuracy: 78.18%\n",
      "Batch 107, Loss: 0.988047, Accuracy: 78.17%\n",
      "Batch 108, Loss: 0.917465, Accuracy: 78.21%\n",
      "Batch 109, Loss: 1.033847, Accuracy: 78.12%\n",
      "Batch 110, Loss: 0.922353, Accuracy: 78.15%\n",
      "Batch 111, Loss: 0.932662, Accuracy: 78.18%\n",
      "Batch 112, Loss: 0.936823, Accuracy: 78.21%\n",
      "Batch 113, Loss: 0.911589, Accuracy: 78.26%\n",
      "Batch 114, Loss: 0.990448, Accuracy: 78.23%\n",
      "Batch 115, Loss: 0.940196, Accuracy: 78.25%\n",
      "Batch 116, Loss: 1.061413, Accuracy: 78.14%\n",
      "Batch 117, Loss: 0.893225, Accuracy: 78.21%\n",
      "Batch 118, Loss: 1.029131, Accuracy: 78.14%\n",
      "Batch 119, Loss: 0.910049, Accuracy: 78.18%\n",
      "Batch 120, Loss: 1.000535, Accuracy: 78.15%\n",
      "Batch 121, Loss: 1.008413, Accuracy: 78.11%\n",
      "Batch 122, Loss: 1.033017, Accuracy: 78.05%\n",
      "Batch 123, Loss: 0.985844, Accuracy: 78.04%\n",
      "Batch 124, Loss: 0.986964, Accuracy: 78.02%\n",
      "Batch 125, Loss: 0.966548, Accuracy: 78.03%\n",
      "Batch 126, Loss: 0.924032, Accuracy: 78.06%\n",
      "Batch 127, Loss: 0.923386, Accuracy: 78.09%\n",
      "Batch 128, Loss: 0.891141, Accuracy: 78.14%\n",
      "Batch 129, Loss: 0.995887, Accuracy: 78.10%\n",
      "Batch 130, Loss: 0.938922, Accuracy: 78.11%\n",
      "Batch 131, Loss: 0.900159, Accuracy: 78.17%\n",
      "Batch 132, Loss: 0.932755, Accuracy: 78.20%\n",
      "Batch 133, Loss: 0.983943, Accuracy: 78.17%\n",
      "Batch 134, Loss: 0.943175, Accuracy: 78.19%\n",
      "Batch 135, Loss: 0.974994, Accuracy: 78.19%\n",
      "Batch 136, Loss: 0.931116, Accuracy: 78.22%\n",
      "Batch 137, Loss: 0.939652, Accuracy: 78.24%\n",
      "Batch 138, Loss: 1.033103, Accuracy: 78.19%\n",
      "Batch 139, Loss: 0.908389, Accuracy: 78.23%\n",
      "Batch 140, Loss: 0.959469, Accuracy: 78.24%\n",
      "Batch 141, Loss: 0.891591, Accuracy: 78.28%\n",
      "Batch 142, Loss: 0.882160, Accuracy: 78.35%\n",
      "Batch 143, Loss: 0.926163, Accuracy: 78.38%\n",
      "Batch 144, Loss: 1.037701, Accuracy: 78.32%\n",
      "Batch 145, Loss: 0.963589, Accuracy: 78.31%\n",
      "Batch 146, Loss: 0.960366, Accuracy: 78.32%\n",
      "Batch 147, Loss: 0.975286, Accuracy: 78.31%\n",
      "Batch 148, Loss: 0.932625, Accuracy: 78.32%\n",
      "Batch 149, Loss: 1.016126, Accuracy: 78.28%\n",
      "Batch 150, Loss: 1.030578, Accuracy: 78.24%\n",
      "Batch 151, Loss: 0.918127, Accuracy: 78.27%\n",
      "Batch 152, Loss: 0.974370, Accuracy: 78.27%\n",
      "Batch 153, Loss: 0.930202, Accuracy: 78.29%\n",
      "Batch 154, Loss: 0.921912, Accuracy: 78.32%\n",
      "Batch 155, Loss: 1.050042, Accuracy: 78.25%\n",
      "Batch 156, Loss: 0.953336, Accuracy: 78.26%\n",
      "Batch 157, Loss: 0.834019, Accuracy: 78.34%\n",
      "Batch 158, Loss: 1.018366, Accuracy: 78.31%\n",
      "Batch 159, Loss: 0.966591, Accuracy: 78.31%\n",
      "Batch 160, Loss: 0.951993, Accuracy: 78.31%\n",
      "Batch 161, Loss: 0.984105, Accuracy: 78.29%\n",
      "Batch 162, Loss: 0.998766, Accuracy: 78.26%\n",
      "Batch 163, Loss: 0.881797, Accuracy: 78.31%\n",
      "Batch 164, Loss: 0.892103, Accuracy: 78.35%\n",
      "Batch 165, Loss: 0.931203, Accuracy: 78.38%\n",
      "Batch 166, Loss: 0.972864, Accuracy: 78.39%\n",
      "Batch 167, Loss: 1.035532, Accuracy: 78.34%\n",
      "Batch 168, Loss: 0.971549, Accuracy: 78.33%\n",
      "Batch 169, Loss: 0.965126, Accuracy: 78.32%\n",
      "Batch 170, Loss: 0.919894, Accuracy: 78.35%\n",
      "Batch 171, Loss: 0.925601, Accuracy: 78.36%\n",
      "Batch 172, Loss: 0.947757, Accuracy: 78.37%\n",
      "Batch 173, Loss: 0.976639, Accuracy: 78.36%\n",
      "Batch 174, Loss: 0.936379, Accuracy: 78.38%\n",
      "Batch 175, Loss: 1.013019, Accuracy: 78.35%\n",
      "Batch 176, Loss: 0.987465, Accuracy: 78.34%\n",
      "Batch 177, Loss: 0.965212, Accuracy: 78.33%\n",
      "Batch 178, Loss: 0.941277, Accuracy: 78.34%\n",
      "Batch 179, Loss: 0.977700, Accuracy: 78.34%\n",
      "Batch 180, Loss: 1.025912, Accuracy: 78.29%\n",
      "Batch 181, Loss: 1.030426, Accuracy: 78.24%\n",
      "Batch 182, Loss: 1.079775, Accuracy: 78.17%\n",
      "Batch 183, Loss: 1.086647, Accuracy: 78.10%\n",
      "Batch 184, Loss: 0.943246, Accuracy: 78.11%\n",
      "Batch 185, Loss: 0.921800, Accuracy: 78.12%\n",
      "Batch 186, Loss: 0.905573, Accuracy: 78.18%\n",
      "Batch 187, Loss: 1.033475, Accuracy: 78.14%\n",
      "Batch 188, Loss: 0.908688, Accuracy: 78.17%\n",
      "Batch 189, Loss: 0.966945, Accuracy: 78.16%\n",
      "Batch 190, Loss: 0.991134, Accuracy: 78.15%\n",
      "Batch 191, Loss: 0.920127, Accuracy: 78.17%\n",
      "Batch 192, Loss: 0.872000, Accuracy: 78.21%\n",
      "Batch 193, Loss: 0.947818, Accuracy: 78.21%\n",
      "Batch 194, Loss: 0.975601, Accuracy: 78.21%\n",
      "Batch 195, Loss: 0.946392, Accuracy: 78.23%\n",
      "Batch 196, Loss: 0.908742, Accuracy: 78.26%\n",
      "Batch 197, Loss: 0.955551, Accuracy: 78.26%\n",
      "Batch 198, Loss: 0.946196, Accuracy: 78.27%\n",
      "Batch 199, Loss: 0.986597, Accuracy: 78.26%\n",
      "Batch 200, Loss: 0.930882, Accuracy: 78.27%\n",
      "Batch 201, Loss: 1.039611, Accuracy: 78.23%\n",
      "Batch 202, Loss: 0.860000, Accuracy: 78.28%\n",
      "Batch 203, Loss: 0.957261, Accuracy: 78.28%\n",
      "Batch 204, Loss: 1.106132, Accuracy: 78.20%\n",
      "Batch 205, Loss: 0.957541, Accuracy: 78.19%\n",
      "Batch 206, Loss: 1.027334, Accuracy: 78.15%\n",
      "Batch 207, Loss: 0.893476, Accuracy: 78.19%\n",
      "Batch 208, Loss: 0.889475, Accuracy: 78.23%\n",
      "Batch 209, Loss: 0.942929, Accuracy: 78.24%\n",
      "Batch 210, Loss: 0.872901, Accuracy: 78.29%\n",
      "Batch 211, Loss: 1.012326, Accuracy: 78.26%\n",
      "Batch 212, Loss: 0.974682, Accuracy: 78.25%\n",
      "Batch 213, Loss: 0.920051, Accuracy: 78.27%\n",
      "Training - Epoch 87, Loss: 0.960367, Accuracy: 78.27%\n",
      "Validation Batch 1, Loss: 0.927527, Accuracy: 81.25%\n",
      "Validation Batch 2, Loss: 0.983198, Accuracy: 78.12%\n",
      "Validation Batch 3, Loss: 1.023917, Accuracy: 76.04%\n",
      "Validation Batch 4, Loss: 0.962748, Accuracy: 76.17%\n",
      "Validation Batch 5, Loss: 0.950024, Accuracy: 76.88%\n",
      "Validation Batch 6, Loss: 0.929124, Accuracy: 77.60%\n",
      "Validation Batch 7, Loss: 0.997217, Accuracy: 77.01%\n",
      "Validation Batch 8, Loss: 1.027516, Accuracy: 76.17%\n",
      "Validation Batch 9, Loss: 1.027535, Accuracy: 75.69%\n",
      "Validation Batch 10, Loss: 1.003512, Accuracy: 75.47%\n",
      "Validation Batch 11, Loss: 0.949564, Accuracy: 75.71%\n",
      "Validation Batch 12, Loss: 0.912474, Accuracy: 76.43%\n",
      "Validation Batch 13, Loss: 1.064126, Accuracy: 75.48%\n",
      "Validation Batch 14, Loss: 1.002309, Accuracy: 75.33%\n",
      "Validation Batch 15, Loss: 0.961430, Accuracy: 75.52%\n",
      "Validation Batch 16, Loss: 0.933053, Accuracy: 75.98%\n",
      "Validation Batch 17, Loss: 0.996918, Accuracy: 75.92%\n",
      "Validation Batch 18, Loss: 0.952573, Accuracy: 76.13%\n",
      "Validation Batch 19, Loss: 1.002725, Accuracy: 76.07%\n",
      "Validation Batch 20, Loss: 0.968182, Accuracy: 76.17%\n",
      "Validation Batch 21, Loss: 0.983138, Accuracy: 76.19%\n",
      "Validation Batch 22, Loss: 1.014243, Accuracy: 75.99%\n",
      "Validation Batch 23, Loss: 1.068309, Accuracy: 75.54%\n",
      "Validation Batch 24, Loss: 1.052184, Accuracy: 75.26%\n",
      "Validation Batch 25, Loss: 0.965736, Accuracy: 75.38%\n",
      "Validation Batch 26, Loss: 0.982473, Accuracy: 75.42%\n",
      "Validation Batch 27, Loss: 0.943041, Accuracy: 75.57%\n",
      "Validation - Epoch 87, Loss: 0.984622, Accuracy: 75.57%\n",
      "Patienceâ€”8\n",
      "Epoch 88\n",
      "Batch 1, Loss: 1.018642, Accuracy: 71.88%\n",
      "Batch 2, Loss: 0.922513, Accuracy: 76.56%\n",
      "Batch 3, Loss: 0.899756, Accuracy: 78.12%\n",
      "Batch 4, Loss: 1.086040, Accuracy: 74.61%\n",
      "Batch 5, Loss: 0.973552, Accuracy: 74.69%\n",
      "Batch 6, Loss: 0.925899, Accuracy: 75.78%\n",
      "Batch 7, Loss: 0.854774, Accuracy: 77.90%\n",
      "Batch 8, Loss: 1.004505, Accuracy: 77.34%\n",
      "Batch 9, Loss: 0.996052, Accuracy: 77.26%\n",
      "Batch 10, Loss: 0.884649, Accuracy: 78.28%\n",
      "Batch 11, Loss: 0.939754, Accuracy: 78.55%\n",
      "Batch 12, Loss: 0.953493, Accuracy: 78.65%\n",
      "Batch 13, Loss: 0.920633, Accuracy: 78.97%\n",
      "Batch 14, Loss: 0.981604, Accuracy: 78.79%\n",
      "Batch 15, Loss: 0.962217, Accuracy: 78.75%\n",
      "Batch 16, Loss: 0.949142, Accuracy: 78.81%\n",
      "Batch 17, Loss: 1.047640, Accuracy: 78.12%\n",
      "Batch 18, Loss: 0.960405, Accuracy: 78.21%\n",
      "Batch 19, Loss: 0.950743, Accuracy: 78.29%\n",
      "Batch 20, Loss: 0.958358, Accuracy: 78.36%\n",
      "Batch 21, Loss: 0.915270, Accuracy: 78.57%\n",
      "Batch 22, Loss: 0.941790, Accuracy: 78.62%\n",
      "Batch 23, Loss: 0.989540, Accuracy: 78.46%\n",
      "Batch 24, Loss: 1.052080, Accuracy: 78.06%\n",
      "Batch 25, Loss: 1.061130, Accuracy: 77.69%\n",
      "Batch 26, Loss: 1.012478, Accuracy: 77.46%\n",
      "Batch 27, Loss: 0.956009, Accuracy: 77.55%\n",
      "Batch 28, Loss: 0.988984, Accuracy: 77.40%\n",
      "Batch 29, Loss: 1.021471, Accuracy: 77.16%\n",
      "Batch 30, Loss: 0.866378, Accuracy: 77.55%\n",
      "Batch 31, Loss: 0.996884, Accuracy: 77.47%\n",
      "Batch 32, Loss: 0.995185, Accuracy: 77.39%\n",
      "Batch 33, Loss: 1.012104, Accuracy: 77.27%\n",
      "Batch 34, Loss: 0.941789, Accuracy: 77.39%\n",
      "Batch 35, Loss: 0.971863, Accuracy: 77.37%\n",
      "Batch 36, Loss: 0.940824, Accuracy: 77.47%\n",
      "Batch 37, Loss: 0.944013, Accuracy: 77.53%\n",
      "Batch 38, Loss: 0.990222, Accuracy: 77.43%\n",
      "Batch 39, Loss: 0.997279, Accuracy: 77.32%\n",
      "Batch 40, Loss: 0.881616, Accuracy: 77.50%\n",
      "Batch 41, Loss: 0.991859, Accuracy: 77.48%\n",
      "Batch 42, Loss: 0.946182, Accuracy: 77.49%\n",
      "Batch 43, Loss: 0.912801, Accuracy: 77.65%\n",
      "Batch 44, Loss: 1.068546, Accuracy: 77.38%\n",
      "Batch 45, Loss: 0.977161, Accuracy: 77.40%\n",
      "Batch 46, Loss: 0.928792, Accuracy: 77.48%\n",
      "Batch 47, Loss: 0.941390, Accuracy: 77.56%\n",
      "Batch 48, Loss: 0.911304, Accuracy: 77.70%\n",
      "Batch 49, Loss: 0.950169, Accuracy: 77.74%\n",
      "Batch 50, Loss: 0.917690, Accuracy: 77.81%\n",
      "Batch 51, Loss: 0.964767, Accuracy: 77.82%\n",
      "Batch 52, Loss: 0.949165, Accuracy: 77.85%\n",
      "Batch 53, Loss: 1.027898, Accuracy: 77.74%\n",
      "Batch 54, Loss: 0.912817, Accuracy: 77.86%\n",
      "Batch 55, Loss: 0.953959, Accuracy: 77.87%\n",
      "Batch 56, Loss: 1.061701, Accuracy: 77.71%\n",
      "Batch 57, Loss: 1.012104, Accuracy: 77.60%\n",
      "Batch 58, Loss: 0.879061, Accuracy: 77.77%\n",
      "Batch 59, Loss: 1.004703, Accuracy: 77.70%\n",
      "Batch 60, Loss: 0.981312, Accuracy: 77.66%\n",
      "Batch 61, Loss: 0.984976, Accuracy: 77.61%\n",
      "Batch 62, Loss: 0.965389, Accuracy: 77.62%\n",
      "Batch 63, Loss: 0.992034, Accuracy: 77.58%\n",
      "Batch 64, Loss: 0.901969, Accuracy: 77.71%\n",
      "Batch 65, Loss: 0.988660, Accuracy: 77.67%\n",
      "Batch 66, Loss: 0.975087, Accuracy: 77.65%\n",
      "Batch 67, Loss: 0.979316, Accuracy: 77.64%\n",
      "Batch 68, Loss: 0.943763, Accuracy: 77.71%\n",
      "Batch 69, Loss: 0.984097, Accuracy: 77.69%\n",
      "Batch 70, Loss: 1.028743, Accuracy: 77.63%\n",
      "Batch 71, Loss: 0.949681, Accuracy: 77.66%\n",
      "Batch 72, Loss: 0.867270, Accuracy: 77.82%\n",
      "Batch 73, Loss: 1.109797, Accuracy: 77.59%\n",
      "Batch 74, Loss: 0.983962, Accuracy: 77.55%\n",
      "Batch 75, Loss: 1.017144, Accuracy: 77.48%\n",
      "Batch 76, Loss: 0.984911, Accuracy: 77.45%\n",
      "Batch 77, Loss: 0.999593, Accuracy: 77.37%\n",
      "Batch 78, Loss: 0.939287, Accuracy: 77.46%\n",
      "Batch 79, Loss: 0.991197, Accuracy: 77.43%\n",
      "Batch 80, Loss: 0.971247, Accuracy: 77.42%\n",
      "Batch 81, Loss: 0.880441, Accuracy: 77.55%\n",
      "Batch 82, Loss: 0.998905, Accuracy: 77.52%\n",
      "Batch 83, Loss: 0.995337, Accuracy: 77.48%\n",
      "Batch 84, Loss: 0.974563, Accuracy: 77.47%\n",
      "Batch 85, Loss: 0.980154, Accuracy: 77.44%\n",
      "Batch 86, Loss: 0.889194, Accuracy: 77.53%\n",
      "Batch 87, Loss: 0.855155, Accuracy: 77.68%\n",
      "Batch 88, Loss: 0.928057, Accuracy: 77.72%\n",
      "Batch 89, Loss: 1.052233, Accuracy: 77.62%\n",
      "Batch 90, Loss: 0.976917, Accuracy: 77.59%\n",
      "Batch 91, Loss: 1.009862, Accuracy: 77.54%\n",
      "Batch 92, Loss: 0.955677, Accuracy: 77.56%\n",
      "Batch 93, Loss: 0.906751, Accuracy: 77.62%\n",
      "Batch 94, Loss: 0.956086, Accuracy: 77.64%\n",
      "Batch 95, Loss: 0.962238, Accuracy: 77.63%\n",
      "Batch 96, Loss: 0.982258, Accuracy: 77.62%\n",
      "Batch 97, Loss: 0.983972, Accuracy: 77.61%\n",
      "Batch 98, Loss: 0.966675, Accuracy: 77.60%\n",
      "Batch 99, Loss: 0.945303, Accuracy: 77.64%\n",
      "Batch 100, Loss: 0.911015, Accuracy: 77.70%\n",
      "Batch 101, Loss: 0.940155, Accuracy: 77.72%\n",
      "Batch 102, Loss: 0.906785, Accuracy: 77.77%\n",
      "Batch 103, Loss: 0.957570, Accuracy: 77.78%\n",
      "Batch 104, Loss: 0.975790, Accuracy: 77.76%\n",
      "Batch 105, Loss: 0.920629, Accuracy: 77.80%\n",
      "Batch 106, Loss: 0.989024, Accuracy: 77.77%\n",
      "Batch 107, Loss: 0.976410, Accuracy: 77.77%\n",
      "Batch 108, Loss: 0.989284, Accuracy: 77.75%\n",
      "Batch 109, Loss: 0.931151, Accuracy: 77.77%\n",
      "Batch 110, Loss: 0.935213, Accuracy: 77.80%\n",
      "Batch 111, Loss: 1.028055, Accuracy: 77.73%\n",
      "Batch 112, Loss: 0.864988, Accuracy: 77.82%\n",
      "Batch 113, Loss: 1.001384, Accuracy: 77.79%\n",
      "Batch 114, Loss: 0.958366, Accuracy: 77.80%\n",
      "Batch 115, Loss: 0.981578, Accuracy: 77.80%\n",
      "Batch 116, Loss: 0.972090, Accuracy: 77.79%\n",
      "Batch 117, Loss: 0.901452, Accuracy: 77.84%\n",
      "Batch 118, Loss: 1.030580, Accuracy: 77.78%\n",
      "Batch 119, Loss: 0.992076, Accuracy: 77.74%\n",
      "Batch 120, Loss: 0.941326, Accuracy: 77.79%\n",
      "Batch 121, Loss: 0.942920, Accuracy: 77.80%\n",
      "Batch 122, Loss: 0.999653, Accuracy: 77.77%\n",
      "Batch 123, Loss: 0.970902, Accuracy: 77.74%\n",
      "Batch 124, Loss: 0.937595, Accuracy: 77.77%\n",
      "Batch 125, Loss: 1.050635, Accuracy: 77.69%\n",
      "Batch 126, Loss: 0.975265, Accuracy: 77.68%\n",
      "Batch 127, Loss: 0.926203, Accuracy: 77.71%\n",
      "Batch 128, Loss: 1.001765, Accuracy: 77.67%\n",
      "Batch 129, Loss: 0.949346, Accuracy: 77.69%\n",
      "Batch 130, Loss: 0.963315, Accuracy: 77.72%\n",
      "Batch 131, Loss: 0.967875, Accuracy: 77.72%\n",
      "Batch 132, Loss: 0.982581, Accuracy: 77.72%\n",
      "Batch 133, Loss: 1.011603, Accuracy: 77.69%\n",
      "Batch 134, Loss: 0.910012, Accuracy: 77.74%\n",
      "Batch 135, Loss: 1.015735, Accuracy: 77.70%\n",
      "Batch 136, Loss: 1.010213, Accuracy: 77.67%\n",
      "Batch 137, Loss: 0.966061, Accuracy: 77.66%\n",
      "Batch 138, Loss: 0.975627, Accuracy: 77.65%\n",
      "Batch 139, Loss: 0.943104, Accuracy: 77.68%\n",
      "Batch 140, Loss: 0.980102, Accuracy: 77.68%\n",
      "Batch 141, Loss: 1.002834, Accuracy: 77.65%\n",
      "Batch 142, Loss: 1.018034, Accuracy: 77.61%\n",
      "Batch 143, Loss: 0.865636, Accuracy: 77.69%\n",
      "Batch 144, Loss: 0.931157, Accuracy: 77.70%\n",
      "Batch 145, Loss: 0.970830, Accuracy: 77.70%\n",
      "Batch 146, Loss: 0.913105, Accuracy: 77.74%\n",
      "Batch 147, Loss: 0.921056, Accuracy: 77.76%\n",
      "Batch 148, Loss: 0.938993, Accuracy: 77.78%\n",
      "Batch 149, Loss: 0.942447, Accuracy: 77.79%\n",
      "Batch 150, Loss: 0.979496, Accuracy: 77.76%\n",
      "Batch 151, Loss: 0.943927, Accuracy: 77.78%\n",
      "Batch 152, Loss: 1.076552, Accuracy: 77.70%\n",
      "Batch 153, Loss: 0.963745, Accuracy: 77.70%\n",
      "Batch 154, Loss: 0.890006, Accuracy: 77.75%\n",
      "Batch 155, Loss: 0.937085, Accuracy: 77.75%\n",
      "Batch 156, Loss: 0.934945, Accuracy: 77.77%\n",
      "Batch 157, Loss: 0.880635, Accuracy: 77.83%\n",
      "Batch 158, Loss: 0.995092, Accuracy: 77.80%\n",
      "Batch 159, Loss: 0.915750, Accuracy: 77.83%\n",
      "Batch 160, Loss: 1.004731, Accuracy: 77.81%\n",
      "Batch 161, Loss: 1.028390, Accuracy: 77.77%\n",
      "Batch 162, Loss: 0.989891, Accuracy: 77.74%\n",
      "Batch 163, Loss: 0.970093, Accuracy: 77.72%\n",
      "Batch 164, Loss: 0.873867, Accuracy: 77.77%\n",
      "Batch 165, Loss: 0.902626, Accuracy: 77.81%\n",
      "Batch 166, Loss: 0.937901, Accuracy: 77.82%\n",
      "Batch 167, Loss: 0.993839, Accuracy: 77.82%\n",
      "Batch 168, Loss: 0.942224, Accuracy: 77.83%\n",
      "Batch 169, Loss: 0.887329, Accuracy: 77.88%\n",
      "Batch 170, Loss: 0.936132, Accuracy: 77.90%\n",
      "Batch 171, Loss: 1.024100, Accuracy: 77.85%\n",
      "Batch 172, Loss: 0.998056, Accuracy: 77.83%\n",
      "Batch 173, Loss: 0.934117, Accuracy: 77.84%\n",
      "Batch 174, Loss: 0.931845, Accuracy: 77.85%\n",
      "Batch 175, Loss: 1.019222, Accuracy: 77.83%\n",
      "Batch 176, Loss: 1.046763, Accuracy: 77.79%\n",
      "Batch 177, Loss: 0.918966, Accuracy: 77.81%\n",
      "Batch 178, Loss: 1.029775, Accuracy: 77.77%\n",
      "Batch 179, Loss: 0.983407, Accuracy: 77.78%\n",
      "Batch 180, Loss: 0.991945, Accuracy: 77.76%\n",
      "Batch 181, Loss: 1.008375, Accuracy: 77.73%\n",
      "Batch 182, Loss: 1.023935, Accuracy: 77.71%\n",
      "Batch 183, Loss: 0.932313, Accuracy: 77.73%\n",
      "Batch 184, Loss: 0.980666, Accuracy: 77.73%\n",
      "Batch 185, Loss: 0.930611, Accuracy: 77.77%\n",
      "Batch 186, Loss: 1.046200, Accuracy: 77.73%\n",
      "Batch 187, Loss: 0.955285, Accuracy: 77.73%\n",
      "Batch 188, Loss: 0.883794, Accuracy: 77.77%\n",
      "Batch 189, Loss: 0.928844, Accuracy: 77.79%\n",
      "Batch 190, Loss: 1.001696, Accuracy: 77.77%\n",
      "Batch 191, Loss: 0.884736, Accuracy: 77.81%\n",
      "Batch 192, Loss: 0.929935, Accuracy: 77.83%\n",
      "Batch 193, Loss: 0.988573, Accuracy: 77.82%\n",
      "Batch 194, Loss: 1.012103, Accuracy: 77.79%\n",
      "Batch 195, Loss: 0.935016, Accuracy: 77.80%\n",
      "Batch 196, Loss: 1.052395, Accuracy: 77.77%\n",
      "Batch 197, Loss: 0.962425, Accuracy: 77.76%\n",
      "Batch 198, Loss: 0.960836, Accuracy: 77.77%\n",
      "Batch 199, Loss: 0.893167, Accuracy: 77.80%\n",
      "Batch 200, Loss: 0.915100, Accuracy: 77.83%\n",
      "Batch 201, Loss: 0.957979, Accuracy: 77.84%\n",
      "Batch 202, Loss: 0.975325, Accuracy: 77.83%\n",
      "Batch 203, Loss: 0.909634, Accuracy: 77.86%\n",
      "Batch 204, Loss: 0.938129, Accuracy: 77.87%\n",
      "Batch 205, Loss: 0.914973, Accuracy: 77.90%\n",
      "Batch 206, Loss: 0.986961, Accuracy: 77.90%\n",
      "Batch 207, Loss: 0.832523, Accuracy: 77.97%\n",
      "Batch 208, Loss: 0.894579, Accuracy: 78.00%\n",
      "Batch 209, Loss: 0.992696, Accuracy: 77.98%\n",
      "Batch 210, Loss: 0.929637, Accuracy: 78.00%\n",
      "Batch 211, Loss: 1.064433, Accuracy: 77.95%\n",
      "Batch 212, Loss: 0.927626, Accuracy: 77.97%\n",
      "Batch 213, Loss: 1.039627, Accuracy: 77.93%\n",
      "Training - Epoch 88, Loss: 0.963665, Accuracy: 77.93%\n",
      "Validation Batch 1, Loss: 0.931962, Accuracy: 82.81%\n",
      "Validation Batch 2, Loss: 0.983132, Accuracy: 78.91%\n",
      "Validation Batch 3, Loss: 1.017246, Accuracy: 77.08%\n",
      "Validation Batch 4, Loss: 0.974307, Accuracy: 77.34%\n",
      "Validation Batch 5, Loss: 0.945638, Accuracy: 77.81%\n",
      "Validation Batch 6, Loss: 0.922429, Accuracy: 78.39%\n",
      "Validation Batch 7, Loss: 0.991853, Accuracy: 77.90%\n",
      "Validation Batch 8, Loss: 1.020012, Accuracy: 76.76%\n",
      "Validation Batch 9, Loss: 1.022072, Accuracy: 76.22%\n",
      "Validation Batch 10, Loss: 1.001541, Accuracy: 76.09%\n",
      "Validation Batch 11, Loss: 0.943829, Accuracy: 76.42%\n",
      "Validation Batch 12, Loss: 0.920062, Accuracy: 76.95%\n",
      "Validation Batch 13, Loss: 1.058562, Accuracy: 76.08%\n",
      "Validation Batch 14, Loss: 1.005545, Accuracy: 75.78%\n",
      "Validation Batch 15, Loss: 0.965229, Accuracy: 75.94%\n",
      "Validation Batch 16, Loss: 0.938157, Accuracy: 76.27%\n",
      "Validation Batch 17, Loss: 0.995086, Accuracy: 76.19%\n",
      "Validation Batch 18, Loss: 0.956027, Accuracy: 76.39%\n",
      "Validation Batch 19, Loss: 1.002454, Accuracy: 76.23%\n",
      "Validation Batch 20, Loss: 0.970477, Accuracy: 76.17%\n",
      "Validation Batch 21, Loss: 0.984509, Accuracy: 76.19%\n",
      "Validation Batch 22, Loss: 1.013549, Accuracy: 75.92%\n",
      "Validation Batch 23, Loss: 1.069185, Accuracy: 75.48%\n",
      "Validation Batch 24, Loss: 1.048320, Accuracy: 75.26%\n",
      "Validation Batch 25, Loss: 0.962467, Accuracy: 75.38%\n",
      "Validation Batch 26, Loss: 0.987172, Accuracy: 75.30%\n",
      "Validation Batch 27, Loss: 0.933477, Accuracy: 75.46%\n",
      "Validation - Epoch 88, Loss: 0.983863, Accuracy: 75.46%\n",
      "Patienceâ€”9\n",
      "Epoch 89\n",
      "Batch 1, Loss: 0.974919, Accuracy: 78.12%\n",
      "Batch 2, Loss: 0.990983, Accuracy: 77.34%\n",
      "Batch 3, Loss: 0.974247, Accuracy: 77.60%\n",
      "Batch 4, Loss: 0.896250, Accuracy: 79.69%\n",
      "Batch 5, Loss: 0.954562, Accuracy: 79.38%\n",
      "Batch 6, Loss: 0.960182, Accuracy: 79.17%\n",
      "Batch 7, Loss: 0.933748, Accuracy: 79.46%\n",
      "Batch 8, Loss: 0.953557, Accuracy: 79.49%\n",
      "Batch 9, Loss: 0.982047, Accuracy: 79.17%\n",
      "Batch 10, Loss: 0.930978, Accuracy: 79.38%\n",
      "Batch 11, Loss: 0.921355, Accuracy: 79.55%\n",
      "Batch 12, Loss: 1.011027, Accuracy: 79.04%\n",
      "Batch 13, Loss: 1.019965, Accuracy: 78.37%\n",
      "Batch 14, Loss: 0.915082, Accuracy: 78.68%\n",
      "Batch 15, Loss: 0.957035, Accuracy: 78.65%\n",
      "Batch 16, Loss: 0.971903, Accuracy: 78.52%\n",
      "Batch 17, Loss: 1.002508, Accuracy: 78.22%\n",
      "Batch 18, Loss: 0.903167, Accuracy: 78.56%\n",
      "Batch 19, Loss: 0.916405, Accuracy: 78.87%\n",
      "Batch 20, Loss: 0.957195, Accuracy: 78.83%\n",
      "Batch 21, Loss: 1.033828, Accuracy: 78.35%\n",
      "Batch 22, Loss: 1.000846, Accuracy: 78.12%\n",
      "Batch 23, Loss: 0.921812, Accuracy: 78.33%\n",
      "Batch 24, Loss: 0.994658, Accuracy: 78.12%\n",
      "Batch 25, Loss: 0.990641, Accuracy: 78.06%\n",
      "Batch 26, Loss: 0.990596, Accuracy: 77.88%\n",
      "Batch 27, Loss: 0.937419, Accuracy: 78.07%\n",
      "Batch 28, Loss: 0.938285, Accuracy: 78.07%\n",
      "Batch 29, Loss: 0.918247, Accuracy: 78.23%\n",
      "Batch 30, Loss: 1.045423, Accuracy: 77.92%\n",
      "Batch 31, Loss: 0.996503, Accuracy: 77.82%\n",
      "Batch 32, Loss: 1.017305, Accuracy: 77.59%\n",
      "Batch 33, Loss: 0.947408, Accuracy: 77.70%\n",
      "Batch 34, Loss: 0.973388, Accuracy: 77.62%\n",
      "Batch 35, Loss: 0.927847, Accuracy: 77.77%\n",
      "Batch 36, Loss: 0.998793, Accuracy: 77.65%\n",
      "Batch 37, Loss: 0.983646, Accuracy: 77.62%\n",
      "Batch 38, Loss: 0.990844, Accuracy: 77.51%\n",
      "Batch 39, Loss: 1.007417, Accuracy: 77.40%\n",
      "Batch 40, Loss: 1.053368, Accuracy: 77.19%\n",
      "Batch 41, Loss: 0.947558, Accuracy: 77.25%\n",
      "Batch 42, Loss: 0.970415, Accuracy: 77.31%\n",
      "Batch 43, Loss: 0.977165, Accuracy: 77.25%\n",
      "Batch 44, Loss: 0.905406, Accuracy: 77.41%\n",
      "Batch 45, Loss: 0.956775, Accuracy: 77.47%\n",
      "Batch 46, Loss: 0.971297, Accuracy: 77.45%\n",
      "Batch 47, Loss: 0.930807, Accuracy: 77.56%\n",
      "Batch 48, Loss: 0.954351, Accuracy: 77.60%\n",
      "Batch 49, Loss: 1.010202, Accuracy: 77.52%\n",
      "Batch 50, Loss: 0.955708, Accuracy: 77.56%\n",
      "Batch 51, Loss: 1.010776, Accuracy: 77.45%\n",
      "Batch 52, Loss: 1.041975, Accuracy: 77.28%\n",
      "Batch 53, Loss: 0.953975, Accuracy: 77.30%\n",
      "Batch 54, Loss: 0.956792, Accuracy: 77.31%\n",
      "Batch 55, Loss: 1.038053, Accuracy: 77.13%\n",
      "Batch 56, Loss: 0.949302, Accuracy: 77.18%\n",
      "Batch 57, Loss: 0.933635, Accuracy: 77.25%\n",
      "Batch 58, Loss: 0.986192, Accuracy: 77.26%\n",
      "Batch 59, Loss: 0.972542, Accuracy: 77.25%\n",
      "Batch 60, Loss: 0.969022, Accuracy: 77.27%\n",
      "Batch 61, Loss: 0.943374, Accuracy: 77.31%\n",
      "Batch 62, Loss: 0.993824, Accuracy: 77.27%\n",
      "Batch 63, Loss: 1.022973, Accuracy: 77.18%\n",
      "Batch 64, Loss: 0.980092, Accuracy: 77.15%\n",
      "Batch 65, Loss: 0.980933, Accuracy: 77.14%\n",
      "Batch 66, Loss: 0.946245, Accuracy: 77.13%\n",
      "Batch 67, Loss: 0.922731, Accuracy: 77.22%\n",
      "Batch 68, Loss: 0.925488, Accuracy: 77.30%\n",
      "Batch 69, Loss: 0.966833, Accuracy: 77.33%\n",
      "Batch 70, Loss: 0.910667, Accuracy: 77.41%\n",
      "Batch 71, Loss: 0.977702, Accuracy: 77.38%\n",
      "Batch 72, Loss: 1.016066, Accuracy: 77.30%\n",
      "Batch 73, Loss: 0.945683, Accuracy: 77.33%\n",
      "Batch 74, Loss: 0.977828, Accuracy: 77.32%\n",
      "Batch 75, Loss: 1.002540, Accuracy: 77.29%\n",
      "Batch 76, Loss: 0.909887, Accuracy: 77.36%\n",
      "Batch 77, Loss: 1.015774, Accuracy: 77.31%\n",
      "Batch 78, Loss: 0.943400, Accuracy: 77.32%\n",
      "Batch 79, Loss: 0.928832, Accuracy: 77.37%\n",
      "Batch 80, Loss: 0.900060, Accuracy: 77.50%\n",
      "Batch 81, Loss: 0.937002, Accuracy: 77.53%\n",
      "Batch 82, Loss: 0.928099, Accuracy: 77.57%\n",
      "Batch 83, Loss: 1.000069, Accuracy: 77.54%\n",
      "Batch 84, Loss: 1.036138, Accuracy: 77.44%\n",
      "Batch 85, Loss: 0.951345, Accuracy: 77.44%\n",
      "Batch 86, Loss: 0.924853, Accuracy: 77.47%\n",
      "Batch 87, Loss: 0.985455, Accuracy: 77.44%\n",
      "Batch 88, Loss: 0.994619, Accuracy: 77.38%\n",
      "Batch 89, Loss: 0.932349, Accuracy: 77.42%\n",
      "Batch 90, Loss: 0.993774, Accuracy: 77.40%\n",
      "Batch 91, Loss: 1.000526, Accuracy: 77.35%\n",
      "Batch 92, Loss: 0.907243, Accuracy: 77.41%\n",
      "Batch 93, Loss: 0.971855, Accuracy: 77.40%\n",
      "Batch 94, Loss: 0.943215, Accuracy: 77.43%\n",
      "Batch 95, Loss: 0.891726, Accuracy: 77.50%\n",
      "Batch 96, Loss: 0.876146, Accuracy: 77.60%\n",
      "Batch 97, Loss: 0.919634, Accuracy: 77.66%\n",
      "Batch 98, Loss: 0.898332, Accuracy: 77.74%\n",
      "Batch 99, Loss: 1.016878, Accuracy: 77.70%\n",
      "Batch 100, Loss: 0.972224, Accuracy: 77.70%\n",
      "Batch 101, Loss: 0.890085, Accuracy: 77.75%\n",
      "Batch 102, Loss: 1.012211, Accuracy: 77.70%\n",
      "Batch 103, Loss: 0.980558, Accuracy: 77.65%\n",
      "Batch 104, Loss: 0.964963, Accuracy: 77.66%\n",
      "Batch 105, Loss: 1.036772, Accuracy: 77.59%\n",
      "Batch 106, Loss: 0.963194, Accuracy: 77.58%\n",
      "Batch 107, Loss: 0.914087, Accuracy: 77.63%\n",
      "Batch 108, Loss: 1.012418, Accuracy: 77.58%\n",
      "Batch 109, Loss: 0.925503, Accuracy: 77.62%\n",
      "Batch 110, Loss: 0.926148, Accuracy: 77.67%\n",
      "Batch 111, Loss: 0.953896, Accuracy: 77.69%\n",
      "Batch 112, Loss: 1.000623, Accuracy: 77.65%\n",
      "Batch 113, Loss: 0.982341, Accuracy: 77.63%\n",
      "Batch 114, Loss: 0.955502, Accuracy: 77.63%\n",
      "Batch 115, Loss: 0.994662, Accuracy: 77.61%\n",
      "Batch 116, Loss: 1.007404, Accuracy: 77.55%\n",
      "Batch 117, Loss: 1.037646, Accuracy: 77.47%\n",
      "Batch 118, Loss: 0.962881, Accuracy: 77.49%\n",
      "Batch 119, Loss: 1.030542, Accuracy: 77.46%\n",
      "Batch 120, Loss: 0.937692, Accuracy: 77.49%\n",
      "Batch 121, Loss: 0.903953, Accuracy: 77.56%\n",
      "Batch 122, Loss: 0.921654, Accuracy: 77.60%\n",
      "Batch 123, Loss: 0.980102, Accuracy: 77.59%\n",
      "Batch 124, Loss: 1.091562, Accuracy: 77.49%\n",
      "Batch 125, Loss: 0.917556, Accuracy: 77.54%\n",
      "Batch 126, Loss: 0.963818, Accuracy: 77.55%\n",
      "Batch 127, Loss: 0.952421, Accuracy: 77.57%\n",
      "Batch 128, Loss: 0.901803, Accuracy: 77.62%\n",
      "Batch 129, Loss: 0.875531, Accuracy: 77.69%\n",
      "Batch 130, Loss: 0.894584, Accuracy: 77.74%\n",
      "Batch 131, Loss: 0.910855, Accuracy: 77.78%\n",
      "Batch 132, Loss: 0.924352, Accuracy: 77.81%\n",
      "Batch 133, Loss: 0.927472, Accuracy: 77.83%\n",
      "Batch 134, Loss: 0.921746, Accuracy: 77.87%\n",
      "Batch 135, Loss: 0.938928, Accuracy: 77.89%\n",
      "Batch 136, Loss: 0.940839, Accuracy: 77.91%\n",
      "Batch 137, Loss: 0.976698, Accuracy: 77.90%\n",
      "Batch 138, Loss: 0.901942, Accuracy: 77.94%\n",
      "Batch 139, Loss: 0.979174, Accuracy: 77.93%\n",
      "Batch 140, Loss: 0.886886, Accuracy: 78.00%\n",
      "Batch 141, Loss: 1.004877, Accuracy: 77.96%\n",
      "Batch 142, Loss: 0.983803, Accuracy: 77.94%\n",
      "Batch 143, Loss: 0.998966, Accuracy: 77.92%\n",
      "Batch 144, Loss: 1.054521, Accuracy: 77.85%\n",
      "Batch 145, Loss: 0.960357, Accuracy: 77.86%\n",
      "Batch 146, Loss: 0.911077, Accuracy: 77.89%\n",
      "Batch 147, Loss: 1.129141, Accuracy: 77.77%\n",
      "Batch 148, Loss: 0.965829, Accuracy: 77.78%\n",
      "Batch 149, Loss: 0.952960, Accuracy: 77.79%\n",
      "Batch 150, Loss: 0.957231, Accuracy: 77.80%\n",
      "Batch 151, Loss: 0.964968, Accuracy: 77.81%\n",
      "Batch 152, Loss: 0.861939, Accuracy: 77.89%\n",
      "Batch 153, Loss: 0.985680, Accuracy: 77.87%\n",
      "Batch 154, Loss: 1.004548, Accuracy: 77.84%\n",
      "Batch 155, Loss: 0.903528, Accuracy: 77.89%\n",
      "Batch 156, Loss: 0.981100, Accuracy: 77.87%\n",
      "Batch 157, Loss: 1.048965, Accuracy: 77.82%\n",
      "Batch 158, Loss: 0.971178, Accuracy: 77.81%\n",
      "Batch 159, Loss: 0.867052, Accuracy: 77.86%\n",
      "Batch 160, Loss: 1.031963, Accuracy: 77.82%\n",
      "Batch 161, Loss: 0.931734, Accuracy: 77.83%\n",
      "Batch 162, Loss: 0.983793, Accuracy: 77.83%\n",
      "Batch 163, Loss: 0.949685, Accuracy: 77.84%\n",
      "Batch 164, Loss: 0.906757, Accuracy: 77.87%\n",
      "Batch 165, Loss: 0.974788, Accuracy: 77.87%\n",
      "Batch 166, Loss: 0.863629, Accuracy: 77.93%\n",
      "Batch 167, Loss: 0.898543, Accuracy: 77.97%\n",
      "Batch 168, Loss: 0.954488, Accuracy: 77.98%\n",
      "Batch 169, Loss: 0.890352, Accuracy: 78.02%\n",
      "Batch 170, Loss: 0.986082, Accuracy: 78.01%\n",
      "Batch 171, Loss: 0.988943, Accuracy: 77.99%\n",
      "Batch 172, Loss: 0.910394, Accuracy: 78.02%\n",
      "Batch 173, Loss: 0.940012, Accuracy: 78.03%\n",
      "Batch 174, Loss: 0.881258, Accuracy: 78.09%\n",
      "Batch 175, Loss: 0.943437, Accuracy: 78.09%\n",
      "Batch 176, Loss: 1.010641, Accuracy: 78.06%\n",
      "Batch 177, Loss: 0.961808, Accuracy: 78.06%\n",
      "Batch 178, Loss: 0.963876, Accuracy: 78.05%\n",
      "Batch 179, Loss: 0.950494, Accuracy: 78.06%\n",
      "Batch 180, Loss: 0.969841, Accuracy: 78.06%\n",
      "Batch 181, Loss: 0.947979, Accuracy: 78.07%\n",
      "Batch 182, Loss: 1.005672, Accuracy: 78.05%\n",
      "Batch 183, Loss: 0.979689, Accuracy: 78.05%\n",
      "Batch 184, Loss: 0.978879, Accuracy: 78.04%\n",
      "Batch 185, Loss: 0.907154, Accuracy: 78.07%\n",
      "Batch 186, Loss: 0.995894, Accuracy: 78.05%\n",
      "Batch 187, Loss: 1.031013, Accuracy: 78.02%\n",
      "Batch 188, Loss: 0.987450, Accuracy: 78.02%\n",
      "Batch 189, Loss: 0.955782, Accuracy: 78.02%\n",
      "Batch 190, Loss: 0.927316, Accuracy: 78.03%\n",
      "Batch 191, Loss: 1.046656, Accuracy: 77.99%\n",
      "Batch 192, Loss: 0.970559, Accuracy: 77.98%\n",
      "Batch 193, Loss: 1.002464, Accuracy: 77.95%\n",
      "Batch 194, Loss: 1.004504, Accuracy: 77.94%\n",
      "Batch 195, Loss: 1.013327, Accuracy: 77.91%\n",
      "Batch 196, Loss: 0.897962, Accuracy: 77.94%\n",
      "Batch 197, Loss: 0.986915, Accuracy: 77.92%\n",
      "Batch 198, Loss: 0.956824, Accuracy: 77.93%\n",
      "Batch 199, Loss: 0.991447, Accuracy: 77.91%\n",
      "Batch 200, Loss: 0.950326, Accuracy: 77.91%\n",
      "Batch 201, Loss: 0.964171, Accuracy: 77.91%\n",
      "Batch 202, Loss: 0.915717, Accuracy: 77.92%\n",
      "Batch 203, Loss: 0.894821, Accuracy: 77.96%\n",
      "Batch 204, Loss: 0.962836, Accuracy: 77.96%\n",
      "Batch 205, Loss: 0.926140, Accuracy: 77.97%\n",
      "Batch 206, Loss: 0.922745, Accuracy: 77.99%\n",
      "Batch 207, Loss: 0.991799, Accuracy: 77.98%\n",
      "Batch 208, Loss: 0.911654, Accuracy: 78.00%\n",
      "Batch 209, Loss: 0.958122, Accuracy: 78.00%\n",
      "Batch 210, Loss: 0.961732, Accuracy: 78.00%\n",
      "Batch 211, Loss: 0.923992, Accuracy: 78.02%\n",
      "Batch 212, Loss: 0.883643, Accuracy: 78.05%\n",
      "Batch 213, Loss: 0.972433, Accuracy: 78.05%\n",
      "Training - Epoch 89, Loss: 0.961543, Accuracy: 78.05%\n",
      "Validation Batch 1, Loss: 0.938427, Accuracy: 81.25%\n",
      "Validation Batch 2, Loss: 0.983570, Accuracy: 78.91%\n",
      "Validation Batch 3, Loss: 1.017421, Accuracy: 77.08%\n",
      "Validation Batch 4, Loss: 0.984364, Accuracy: 76.17%\n",
      "Validation Batch 5, Loss: 0.949247, Accuracy: 76.88%\n",
      "Validation Batch 6, Loss: 0.914066, Accuracy: 77.86%\n",
      "Validation Batch 7, Loss: 0.990924, Accuracy: 77.46%\n",
      "Validation Batch 8, Loss: 1.019193, Accuracy: 76.56%\n",
      "Validation Batch 9, Loss: 1.019557, Accuracy: 76.04%\n",
      "Validation Batch 10, Loss: 1.001221, Accuracy: 75.78%\n",
      "Validation Batch 11, Loss: 0.937035, Accuracy: 76.28%\n",
      "Validation Batch 12, Loss: 0.913923, Accuracy: 76.82%\n",
      "Validation Batch 13, Loss: 1.051694, Accuracy: 76.20%\n",
      "Validation Batch 14, Loss: 1.004545, Accuracy: 76.12%\n",
      "Validation Batch 15, Loss: 0.965506, Accuracy: 76.25%\n",
      "Validation Batch 16, Loss: 0.938852, Accuracy: 76.46%\n",
      "Validation Batch 17, Loss: 0.992788, Accuracy: 76.38%\n",
      "Validation Batch 18, Loss: 0.970207, Accuracy: 76.48%\n",
      "Validation Batch 19, Loss: 1.008234, Accuracy: 76.40%\n",
      "Validation Batch 20, Loss: 0.971693, Accuracy: 76.25%\n",
      "Validation Batch 21, Loss: 0.989450, Accuracy: 76.26%\n",
      "Validation Batch 22, Loss: 1.020722, Accuracy: 76.07%\n",
      "Validation Batch 23, Loss: 1.077153, Accuracy: 75.61%\n",
      "Validation Batch 24, Loss: 1.049980, Accuracy: 75.33%\n",
      "Validation Batch 25, Loss: 0.966764, Accuracy: 75.44%\n",
      "Validation Batch 26, Loss: 0.986917, Accuracy: 75.42%\n",
      "Validation Batch 27, Loss: 0.939753, Accuracy: 75.57%\n",
      "Validation - Epoch 89, Loss: 0.985304, Accuracy: 75.57%\n",
      "Patienceâ€”10\n",
      "Epoch 90\n",
      "Batch 1, Loss: 0.857151, Accuracy: 89.06%\n",
      "Batch 2, Loss: 0.997987, Accuracy: 81.25%\n",
      "Batch 3, Loss: 1.069177, Accuracy: 77.08%\n",
      "Batch 4, Loss: 0.923715, Accuracy: 78.12%\n",
      "Batch 5, Loss: 0.994341, Accuracy: 77.50%\n",
      "Batch 6, Loss: 0.983181, Accuracy: 77.34%\n",
      "Batch 7, Loss: 1.031986, Accuracy: 76.56%\n",
      "Batch 8, Loss: 0.936127, Accuracy: 77.34%\n",
      "Batch 9, Loss: 0.922689, Accuracy: 78.12%\n",
      "Batch 10, Loss: 0.980656, Accuracy: 77.81%\n",
      "Batch 11, Loss: 0.928295, Accuracy: 78.12%\n",
      "Batch 12, Loss: 0.960083, Accuracy: 77.99%\n",
      "Batch 13, Loss: 1.003685, Accuracy: 77.76%\n",
      "Batch 14, Loss: 1.022499, Accuracy: 77.46%\n",
      "Batch 15, Loss: 0.912986, Accuracy: 77.81%\n",
      "Batch 16, Loss: 1.011995, Accuracy: 77.44%\n",
      "Batch 17, Loss: 0.923538, Accuracy: 77.67%\n",
      "Batch 18, Loss: 0.904355, Accuracy: 78.12%\n",
      "Batch 19, Loss: 0.991128, Accuracy: 78.04%\n",
      "Batch 20, Loss: 1.026670, Accuracy: 77.58%\n",
      "Batch 21, Loss: 0.916977, Accuracy: 77.90%\n",
      "Batch 22, Loss: 0.845008, Accuracy: 78.48%\n",
      "Batch 23, Loss: 0.982272, Accuracy: 78.33%\n",
      "Batch 24, Loss: 0.955749, Accuracy: 78.39%\n",
      "Batch 25, Loss: 0.937936, Accuracy: 78.38%\n",
      "Batch 26, Loss: 0.934381, Accuracy: 78.49%\n",
      "Batch 27, Loss: 0.939350, Accuracy: 78.59%\n",
      "Batch 28, Loss: 0.976739, Accuracy: 78.52%\n",
      "Batch 29, Loss: 0.995396, Accuracy: 78.39%\n",
      "Batch 30, Loss: 0.969500, Accuracy: 78.39%\n",
      "Batch 31, Loss: 0.947153, Accuracy: 78.43%\n",
      "Batch 32, Loss: 0.911771, Accuracy: 78.56%\n",
      "Batch 33, Loss: 0.970773, Accuracy: 78.55%\n",
      "Batch 34, Loss: 0.865690, Accuracy: 78.86%\n",
      "Batch 35, Loss: 0.920795, Accuracy: 78.97%\n",
      "Batch 36, Loss: 0.992948, Accuracy: 78.86%\n",
      "Batch 37, Loss: 1.014644, Accuracy: 78.67%\n",
      "Batch 38, Loss: 1.013519, Accuracy: 78.50%\n",
      "Batch 39, Loss: 0.970452, Accuracy: 78.49%\n",
      "Batch 40, Loss: 0.937376, Accuracy: 78.52%\n",
      "Batch 41, Loss: 0.925270, Accuracy: 78.62%\n",
      "Batch 42, Loss: 0.971877, Accuracy: 78.65%\n",
      "Batch 43, Loss: 0.932766, Accuracy: 78.71%\n",
      "Batch 44, Loss: 1.004038, Accuracy: 78.59%\n",
      "Batch 45, Loss: 1.009512, Accuracy: 78.44%\n",
      "Batch 46, Loss: 0.903534, Accuracy: 78.57%\n",
      "Batch 47, Loss: 0.924332, Accuracy: 78.62%\n",
      "Batch 48, Loss: 0.957659, Accuracy: 78.65%\n",
      "Batch 49, Loss: 1.005473, Accuracy: 78.48%\n",
      "Batch 50, Loss: 0.857218, Accuracy: 78.72%\n",
      "Batch 51, Loss: 1.044504, Accuracy: 78.49%\n",
      "Batch 52, Loss: 0.951847, Accuracy: 78.49%\n",
      "Batch 53, Loss: 0.942789, Accuracy: 78.57%\n",
      "Batch 54, Loss: 0.886450, Accuracy: 78.70%\n",
      "Batch 55, Loss: 0.950265, Accuracy: 78.72%\n",
      "Batch 56, Loss: 0.886661, Accuracy: 78.85%\n",
      "Batch 57, Loss: 0.969053, Accuracy: 78.84%\n",
      "Batch 58, Loss: 0.933573, Accuracy: 78.88%\n",
      "Batch 59, Loss: 0.935539, Accuracy: 78.92%\n",
      "Batch 60, Loss: 0.915098, Accuracy: 78.98%\n",
      "Batch 61, Loss: 0.972091, Accuracy: 78.94%\n",
      "Batch 62, Loss: 0.963693, Accuracy: 78.93%\n",
      "Batch 63, Loss: 0.989502, Accuracy: 78.87%\n",
      "Batch 64, Loss: 0.862766, Accuracy: 79.03%\n",
      "Batch 65, Loss: 1.019038, Accuracy: 78.92%\n",
      "Batch 66, Loss: 0.963812, Accuracy: 78.88%\n",
      "Batch 67, Loss: 0.962380, Accuracy: 78.87%\n",
      "Batch 68, Loss: 1.079795, Accuracy: 78.68%\n",
      "Batch 69, Loss: 1.006333, Accuracy: 78.58%\n",
      "Batch 70, Loss: 0.927865, Accuracy: 78.64%\n",
      "Batch 71, Loss: 0.941146, Accuracy: 78.68%\n",
      "Batch 72, Loss: 0.910613, Accuracy: 78.73%\n",
      "Batch 73, Loss: 0.974406, Accuracy: 78.68%\n",
      "Batch 74, Loss: 0.980990, Accuracy: 78.61%\n",
      "Batch 75, Loss: 0.865831, Accuracy: 78.75%\n",
      "Batch 76, Loss: 0.944884, Accuracy: 78.76%\n",
      "Batch 77, Loss: 0.888071, Accuracy: 78.84%\n",
      "Batch 78, Loss: 1.007149, Accuracy: 78.75%\n",
      "Batch 79, Loss: 0.932558, Accuracy: 78.78%\n",
      "Batch 80, Loss: 0.949056, Accuracy: 78.77%\n",
      "Batch 81, Loss: 0.978790, Accuracy: 78.74%\n",
      "Batch 82, Loss: 0.932235, Accuracy: 78.75%\n",
      "Batch 83, Loss: 1.075717, Accuracy: 78.60%\n",
      "Batch 84, Loss: 1.001775, Accuracy: 78.55%\n",
      "Batch 85, Loss: 0.968879, Accuracy: 78.53%\n",
      "Batch 86, Loss: 0.938437, Accuracy: 78.54%\n",
      "Batch 87, Loss: 1.009558, Accuracy: 78.48%\n",
      "Batch 88, Loss: 0.875626, Accuracy: 78.59%\n",
      "Batch 89, Loss: 0.997565, Accuracy: 78.53%\n",
      "Batch 90, Loss: 0.992879, Accuracy: 78.47%\n",
      "Batch 91, Loss: 0.968583, Accuracy: 78.45%\n",
      "Batch 92, Loss: 1.022976, Accuracy: 78.36%\n",
      "Batch 93, Loss: 0.873865, Accuracy: 78.46%\n",
      "Batch 94, Loss: 0.953098, Accuracy: 78.47%\n",
      "Batch 95, Loss: 0.946889, Accuracy: 78.49%\n",
      "Batch 96, Loss: 1.021515, Accuracy: 78.42%\n",
      "Batch 97, Loss: 0.969122, Accuracy: 78.40%\n",
      "Batch 98, Loss: 0.966201, Accuracy: 78.40%\n",
      "Batch 99, Loss: 1.015480, Accuracy: 78.35%\n",
      "Batch 100, Loss: 0.985055, Accuracy: 78.31%\n",
      "Batch 101, Loss: 0.940796, Accuracy: 78.33%\n",
      "Batch 102, Loss: 0.962454, Accuracy: 78.32%\n",
      "Batch 103, Loss: 0.955033, Accuracy: 78.32%\n",
      "Batch 104, Loss: 1.010372, Accuracy: 78.28%\n",
      "Batch 105, Loss: 0.990600, Accuracy: 78.24%\n",
      "Batch 106, Loss: 0.940617, Accuracy: 78.26%\n",
      "Batch 107, Loss: 0.959218, Accuracy: 78.26%\n",
      "Batch 108, Loss: 1.040636, Accuracy: 78.17%\n",
      "Batch 109, Loss: 0.965906, Accuracy: 78.17%\n",
      "Batch 110, Loss: 0.943114, Accuracy: 78.17%\n",
      "Batch 111, Loss: 0.932421, Accuracy: 78.21%\n",
      "Batch 112, Loss: 0.893691, Accuracy: 78.26%\n",
      "Batch 113, Loss: 0.949925, Accuracy: 78.28%\n",
      "Batch 114, Loss: 0.877764, Accuracy: 78.34%\n",
      "Batch 115, Loss: 0.905996, Accuracy: 78.38%\n",
      "Batch 116, Loss: 0.963628, Accuracy: 78.38%\n",
      "Batch 117, Loss: 0.921835, Accuracy: 78.41%\n",
      "Batch 118, Loss: 0.949814, Accuracy: 78.42%\n",
      "Batch 119, Loss: 1.060674, Accuracy: 78.34%\n",
      "Batch 120, Loss: 0.940647, Accuracy: 78.33%\n",
      "Batch 121, Loss: 0.911478, Accuracy: 78.38%\n",
      "Batch 122, Loss: 0.906623, Accuracy: 78.43%\n",
      "Batch 123, Loss: 1.041599, Accuracy: 78.35%\n",
      "Batch 124, Loss: 1.051447, Accuracy: 78.29%\n",
      "Batch 125, Loss: 0.931725, Accuracy: 78.30%\n",
      "Batch 126, Loss: 0.972010, Accuracy: 78.30%\n",
      "Batch 127, Loss: 1.033326, Accuracy: 78.25%\n",
      "Batch 128, Loss: 0.965274, Accuracy: 78.23%\n",
      "Batch 129, Loss: 1.039469, Accuracy: 78.16%\n",
      "Batch 130, Loss: 0.963024, Accuracy: 78.16%\n",
      "Batch 131, Loss: 1.031622, Accuracy: 78.10%\n",
      "Batch 132, Loss: 0.944995, Accuracy: 78.12%\n",
      "Batch 133, Loss: 1.079381, Accuracy: 78.03%\n",
      "Batch 134, Loss: 0.969188, Accuracy: 78.02%\n",
      "Batch 135, Loss: 0.978464, Accuracy: 78.00%\n",
      "Batch 136, Loss: 1.044190, Accuracy: 77.93%\n",
      "Batch 137, Loss: 0.956422, Accuracy: 77.94%\n",
      "Batch 138, Loss: 0.966157, Accuracy: 77.94%\n",
      "Batch 139, Loss: 0.907087, Accuracy: 78.00%\n",
      "Batch 140, Loss: 0.939228, Accuracy: 78.02%\n",
      "Batch 141, Loss: 0.953509, Accuracy: 78.01%\n",
      "Batch 142, Loss: 0.943316, Accuracy: 78.03%\n",
      "Batch 143, Loss: 1.057215, Accuracy: 77.95%\n",
      "Batch 144, Loss: 1.014822, Accuracy: 77.91%\n",
      "Batch 145, Loss: 0.952262, Accuracy: 77.92%\n",
      "Batch 146, Loss: 1.002142, Accuracy: 77.89%\n",
      "Batch 147, Loss: 0.942339, Accuracy: 77.91%\n",
      "Batch 148, Loss: 0.919678, Accuracy: 77.93%\n",
      "Batch 149, Loss: 0.936907, Accuracy: 77.97%\n",
      "Batch 150, Loss: 0.975552, Accuracy: 77.96%\n",
      "Batch 151, Loss: 0.999813, Accuracy: 77.93%\n",
      "Batch 152, Loss: 0.953108, Accuracy: 77.93%\n",
      "Batch 153, Loss: 1.001418, Accuracy: 77.90%\n",
      "Batch 154, Loss: 0.995662, Accuracy: 77.86%\n",
      "Batch 155, Loss: 0.927181, Accuracy: 77.90%\n",
      "Batch 156, Loss: 0.922163, Accuracy: 77.93%\n",
      "Batch 157, Loss: 0.969757, Accuracy: 77.93%\n",
      "Batch 158, Loss: 0.913342, Accuracy: 77.97%\n",
      "Batch 159, Loss: 0.954449, Accuracy: 77.97%\n",
      "Batch 160, Loss: 0.940067, Accuracy: 77.99%\n",
      "Batch 161, Loss: 1.024364, Accuracy: 77.95%\n",
      "Batch 162, Loss: 1.012195, Accuracy: 77.91%\n",
      "Batch 163, Loss: 0.944538, Accuracy: 77.92%\n",
      "Batch 164, Loss: 1.052597, Accuracy: 77.86%\n",
      "Batch 165, Loss: 0.936367, Accuracy: 77.87%\n",
      "Batch 166, Loss: 0.947565, Accuracy: 77.88%\n",
      "Batch 167, Loss: 0.952551, Accuracy: 77.89%\n",
      "Batch 168, Loss: 0.986145, Accuracy: 77.87%\n",
      "Batch 169, Loss: 1.056459, Accuracy: 77.82%\n",
      "Batch 170, Loss: 0.933748, Accuracy: 77.85%\n",
      "Batch 171, Loss: 0.994059, Accuracy: 77.83%\n",
      "Batch 172, Loss: 0.957864, Accuracy: 77.83%\n",
      "Batch 173, Loss: 0.938566, Accuracy: 77.85%\n",
      "Batch 174, Loss: 0.933489, Accuracy: 77.86%\n",
      "Batch 175, Loss: 1.027092, Accuracy: 77.82%\n",
      "Batch 176, Loss: 0.896013, Accuracy: 77.86%\n",
      "Batch 177, Loss: 0.907292, Accuracy: 77.90%\n",
      "Batch 178, Loss: 0.949593, Accuracy: 77.90%\n",
      "Batch 179, Loss: 0.854483, Accuracy: 77.97%\n",
      "Batch 180, Loss: 0.956389, Accuracy: 77.96%\n",
      "Batch 181, Loss: 0.878709, Accuracy: 78.01%\n",
      "Batch 182, Loss: 0.967573, Accuracy: 78.01%\n",
      "Batch 183, Loss: 0.945408, Accuracy: 78.03%\n",
      "Batch 184, Loss: 0.956273, Accuracy: 78.04%\n",
      "Batch 185, Loss: 0.927098, Accuracy: 78.07%\n",
      "Batch 186, Loss: 0.985826, Accuracy: 78.05%\n",
      "Batch 187, Loss: 0.941388, Accuracy: 78.06%\n",
      "Batch 188, Loss: 0.967206, Accuracy: 78.06%\n",
      "Batch 189, Loss: 0.966040, Accuracy: 78.04%\n",
      "Batch 190, Loss: 0.954228, Accuracy: 78.05%\n",
      "Batch 191, Loss: 0.876388, Accuracy: 78.09%\n",
      "Batch 192, Loss: 0.947739, Accuracy: 78.11%\n",
      "Batch 193, Loss: 0.910927, Accuracy: 78.14%\n",
      "Batch 194, Loss: 0.929130, Accuracy: 78.17%\n",
      "Batch 195, Loss: 0.958058, Accuracy: 78.17%\n",
      "Batch 196, Loss: 0.913106, Accuracy: 78.20%\n",
      "Batch 197, Loss: 1.011958, Accuracy: 78.17%\n",
      "Batch 198, Loss: 0.947166, Accuracy: 78.18%\n",
      "Batch 199, Loss: 0.998066, Accuracy: 78.16%\n",
      "Batch 200, Loss: 0.913829, Accuracy: 78.19%\n",
      "Batch 201, Loss: 0.933248, Accuracy: 78.20%\n",
      "Batch 202, Loss: 0.870245, Accuracy: 78.26%\n",
      "Batch 203, Loss: 0.970878, Accuracy: 78.26%\n",
      "Batch 204, Loss: 0.939540, Accuracy: 78.26%\n",
      "Batch 205, Loss: 0.974149, Accuracy: 78.26%\n",
      "Batch 206, Loss: 0.942317, Accuracy: 78.28%\n",
      "Batch 207, Loss: 1.038856, Accuracy: 78.25%\n",
      "Batch 208, Loss: 0.923665, Accuracy: 78.26%\n",
      "Batch 209, Loss: 1.035007, Accuracy: 78.22%\n",
      "Batch 210, Loss: 0.948931, Accuracy: 78.23%\n",
      "Batch 211, Loss: 1.023300, Accuracy: 78.19%\n",
      "Batch 212, Loss: 0.969491, Accuracy: 78.19%\n",
      "Batch 213, Loss: 0.943812, Accuracy: 78.19%\n",
      "Training - Epoch 90, Loss: 0.960600, Accuracy: 78.19%\n",
      "Validation Batch 1, Loss: 0.928846, Accuracy: 82.81%\n",
      "Validation Batch 2, Loss: 0.969716, Accuracy: 79.69%\n",
      "Validation Batch 3, Loss: 1.013948, Accuracy: 77.60%\n",
      "Validation Batch 4, Loss: 0.964764, Accuracy: 77.73%\n",
      "Validation Batch 5, Loss: 0.940769, Accuracy: 78.44%\n",
      "Validation Batch 6, Loss: 0.911105, Accuracy: 79.43%\n",
      "Validation Batch 7, Loss: 0.984116, Accuracy: 78.79%\n",
      "Validation Batch 8, Loss: 1.012639, Accuracy: 77.93%\n",
      "Validation Batch 9, Loss: 1.010227, Accuracy: 77.43%\n",
      "Validation Batch 10, Loss: 0.997897, Accuracy: 77.19%\n",
      "Validation Batch 11, Loss: 0.930107, Accuracy: 77.56%\n",
      "Validation Batch 12, Loss: 0.909112, Accuracy: 78.26%\n",
      "Validation Batch 13, Loss: 1.042285, Accuracy: 77.52%\n",
      "Validation Batch 14, Loss: 0.995950, Accuracy: 77.34%\n",
      "Validation Batch 15, Loss: 0.961293, Accuracy: 77.40%\n",
      "Validation Batch 16, Loss: 0.931565, Accuracy: 77.64%\n",
      "Validation Batch 17, Loss: 0.982385, Accuracy: 77.48%\n",
      "Validation Batch 18, Loss: 0.954792, Accuracy: 77.60%\n",
      "Validation Batch 19, Loss: 0.993714, Accuracy: 77.55%\n",
      "Validation Batch 20, Loss: 0.963551, Accuracy: 77.42%\n",
      "Validation Batch 21, Loss: 0.979093, Accuracy: 77.38%\n",
      "Validation Batch 22, Loss: 1.003713, Accuracy: 77.20%\n",
      "Validation Batch 23, Loss: 1.063310, Accuracy: 76.70%\n",
      "Validation Batch 24, Loss: 1.046499, Accuracy: 76.37%\n",
      "Validation Batch 25, Loss: 0.964371, Accuracy: 76.38%\n",
      "Validation Batch 26, Loss: 0.983302, Accuracy: 76.32%\n",
      "Validation Batch 27, Loss: 0.932872, Accuracy: 76.45%\n",
      "Validation - Epoch 90, Loss: 0.976739, Accuracy: 76.45%\n",
      "Patienceâ€”11\n",
      "Epoch 91\n",
      "Batch 1, Loss: 0.916912, Accuracy: 81.25%\n",
      "Batch 2, Loss: 0.963165, Accuracy: 79.69%\n",
      "Batch 3, Loss: 1.033040, Accuracy: 77.60%\n",
      "Batch 4, Loss: 0.957666, Accuracy: 77.73%\n",
      "Batch 5, Loss: 0.938920, Accuracy: 78.44%\n",
      "Batch 6, Loss: 0.935259, Accuracy: 78.65%\n",
      "Batch 7, Loss: 0.864660, Accuracy: 79.91%\n",
      "Batch 8, Loss: 0.953169, Accuracy: 79.88%\n",
      "Batch 9, Loss: 1.040435, Accuracy: 78.82%\n",
      "Batch 10, Loss: 0.945095, Accuracy: 78.75%\n",
      "Batch 11, Loss: 0.962153, Accuracy: 78.69%\n",
      "Batch 12, Loss: 0.956293, Accuracy: 78.65%\n",
      "Batch 13, Loss: 1.016153, Accuracy: 78.12%\n",
      "Batch 14, Loss: 0.958530, Accuracy: 78.12%\n",
      "Batch 15, Loss: 0.986445, Accuracy: 78.02%\n",
      "Batch 16, Loss: 0.990178, Accuracy: 77.93%\n",
      "Batch 17, Loss: 1.020738, Accuracy: 77.48%\n",
      "Batch 18, Loss: 0.960818, Accuracy: 77.60%\n",
      "Batch 19, Loss: 0.974650, Accuracy: 77.55%\n",
      "Batch 20, Loss: 0.941433, Accuracy: 77.66%\n",
      "Batch 21, Loss: 1.021786, Accuracy: 77.38%\n",
      "Batch 22, Loss: 1.028565, Accuracy: 77.13%\n",
      "Batch 23, Loss: 0.852482, Accuracy: 77.72%\n",
      "Batch 24, Loss: 0.989385, Accuracy: 77.54%\n",
      "Batch 25, Loss: 0.945364, Accuracy: 77.62%\n",
      "Batch 26, Loss: 0.909631, Accuracy: 77.88%\n",
      "Batch 27, Loss: 0.929581, Accuracy: 77.95%\n",
      "Batch 28, Loss: 0.906002, Accuracy: 78.24%\n",
      "Batch 29, Loss: 0.974432, Accuracy: 78.12%\n",
      "Batch 30, Loss: 0.883100, Accuracy: 78.39%\n",
      "Batch 31, Loss: 0.976187, Accuracy: 78.33%\n",
      "Batch 32, Loss: 0.958309, Accuracy: 78.42%\n",
      "Batch 33, Loss: 0.969114, Accuracy: 78.41%\n",
      "Batch 34, Loss: 0.937273, Accuracy: 78.49%\n",
      "Batch 35, Loss: 0.937700, Accuracy: 78.53%\n",
      "Batch 36, Loss: 0.930889, Accuracy: 78.60%\n",
      "Batch 37, Loss: 1.021331, Accuracy: 78.42%\n",
      "Batch 38, Loss: 0.966101, Accuracy: 78.37%\n",
      "Batch 39, Loss: 1.083382, Accuracy: 78.04%\n",
      "Batch 40, Loss: 1.010769, Accuracy: 77.89%\n",
      "Batch 41, Loss: 1.033408, Accuracy: 77.74%\n",
      "Batch 42, Loss: 1.017574, Accuracy: 77.68%\n",
      "Batch 43, Loss: 1.059998, Accuracy: 77.43%\n",
      "Batch 44, Loss: 0.998676, Accuracy: 77.38%\n",
      "Batch 45, Loss: 0.979464, Accuracy: 77.33%\n",
      "Batch 46, Loss: 1.091768, Accuracy: 77.07%\n",
      "Batch 47, Loss: 0.953212, Accuracy: 77.13%\n",
      "Batch 48, Loss: 0.888815, Accuracy: 77.31%\n",
      "Batch 49, Loss: 1.004964, Accuracy: 77.23%\n",
      "Batch 50, Loss: 0.991867, Accuracy: 77.12%\n",
      "Batch 51, Loss: 0.930381, Accuracy: 77.21%\n",
      "Batch 52, Loss: 0.898387, Accuracy: 77.37%\n",
      "Batch 53, Loss: 0.982445, Accuracy: 77.33%\n",
      "Batch 54, Loss: 0.889874, Accuracy: 77.46%\n",
      "Batch 55, Loss: 0.956062, Accuracy: 77.44%\n",
      "Batch 56, Loss: 0.873036, Accuracy: 77.62%\n",
      "Batch 57, Loss: 0.930315, Accuracy: 77.69%\n",
      "Batch 58, Loss: 0.889618, Accuracy: 77.83%\n",
      "Batch 59, Loss: 0.923917, Accuracy: 77.91%\n",
      "Batch 60, Loss: 0.898444, Accuracy: 78.05%\n",
      "Batch 61, Loss: 0.873028, Accuracy: 78.20%\n",
      "Batch 62, Loss: 0.951619, Accuracy: 78.18%\n",
      "Batch 63, Loss: 1.004992, Accuracy: 78.10%\n",
      "Batch 64, Loss: 0.908364, Accuracy: 78.20%\n",
      "Batch 65, Loss: 0.963736, Accuracy: 78.17%\n",
      "Batch 66, Loss: 0.888387, Accuracy: 78.27%\n",
      "Batch 67, Loss: 0.925254, Accuracy: 78.31%\n",
      "Batch 68, Loss: 1.057677, Accuracy: 78.17%\n",
      "Batch 69, Loss: 0.907289, Accuracy: 78.28%\n",
      "Batch 70, Loss: 0.919442, Accuracy: 78.35%\n",
      "Batch 71, Loss: 0.984917, Accuracy: 78.32%\n",
      "Batch 72, Loss: 0.946057, Accuracy: 78.34%\n",
      "Batch 73, Loss: 0.949142, Accuracy: 78.36%\n",
      "Batch 74, Loss: 0.966153, Accuracy: 78.40%\n",
      "Batch 75, Loss: 0.917367, Accuracy: 78.48%\n",
      "Batch 76, Loss: 0.888081, Accuracy: 78.58%\n",
      "Batch 77, Loss: 1.023195, Accuracy: 78.47%\n",
      "Batch 78, Loss: 0.888723, Accuracy: 78.59%\n",
      "Batch 79, Loss: 0.943244, Accuracy: 78.60%\n",
      "Batch 80, Loss: 0.915512, Accuracy: 78.65%\n",
      "Batch 81, Loss: 0.969315, Accuracy: 78.63%\n",
      "Batch 82, Loss: 0.946476, Accuracy: 78.66%\n",
      "Batch 83, Loss: 0.960040, Accuracy: 78.65%\n",
      "Batch 84, Loss: 0.835534, Accuracy: 78.79%\n",
      "Batch 85, Loss: 0.971622, Accuracy: 78.79%\n",
      "Batch 86, Loss: 0.999041, Accuracy: 78.72%\n",
      "Batch 87, Loss: 0.980006, Accuracy: 78.68%\n",
      "Batch 88, Loss: 0.969142, Accuracy: 78.66%\n",
      "Batch 89, Loss: 0.938636, Accuracy: 78.67%\n",
      "Batch 90, Loss: 0.983304, Accuracy: 78.65%\n",
      "Batch 91, Loss: 1.031396, Accuracy: 78.57%\n",
      "Batch 92, Loss: 0.999679, Accuracy: 78.53%\n",
      "Batch 93, Loss: 1.004817, Accuracy: 78.46%\n",
      "Batch 94, Loss: 1.017836, Accuracy: 78.39%\n",
      "Batch 95, Loss: 0.985666, Accuracy: 78.36%\n",
      "Batch 96, Loss: 0.904961, Accuracy: 78.42%\n",
      "Batch 97, Loss: 1.011007, Accuracy: 78.35%\n",
      "Batch 98, Loss: 0.978305, Accuracy: 78.32%\n",
      "Batch 99, Loss: 0.996033, Accuracy: 78.25%\n",
      "Batch 100, Loss: 1.012684, Accuracy: 78.19%\n",
      "Batch 101, Loss: 1.014973, Accuracy: 78.14%\n",
      "Batch 102, Loss: 0.906376, Accuracy: 78.19%\n",
      "Batch 103, Loss: 0.978539, Accuracy: 78.19%\n",
      "Batch 104, Loss: 0.932379, Accuracy: 78.22%\n",
      "Batch 105, Loss: 0.979542, Accuracy: 78.20%\n",
      "Batch 106, Loss: 0.920390, Accuracy: 78.26%\n",
      "Batch 107, Loss: 1.035016, Accuracy: 78.18%\n",
      "Batch 108, Loss: 0.911173, Accuracy: 78.23%\n",
      "Batch 109, Loss: 0.869653, Accuracy: 78.31%\n",
      "Batch 110, Loss: 0.924075, Accuracy: 78.32%\n",
      "Batch 111, Loss: 1.011394, Accuracy: 78.28%\n",
      "Batch 112, Loss: 0.947116, Accuracy: 78.29%\n",
      "Batch 113, Loss: 0.966936, Accuracy: 78.29%\n",
      "Batch 114, Loss: 0.963534, Accuracy: 78.28%\n",
      "Batch 115, Loss: 0.944407, Accuracy: 78.30%\n",
      "Batch 116, Loss: 1.001956, Accuracy: 78.27%\n",
      "Batch 117, Loss: 0.958974, Accuracy: 78.29%\n",
      "Batch 118, Loss: 0.975314, Accuracy: 78.27%\n",
      "Batch 119, Loss: 0.902576, Accuracy: 78.32%\n",
      "Batch 120, Loss: 0.998940, Accuracy: 78.28%\n",
      "Batch 121, Loss: 1.067683, Accuracy: 78.19%\n",
      "Batch 122, Loss: 1.004277, Accuracy: 78.12%\n",
      "Batch 123, Loss: 1.013973, Accuracy: 78.06%\n",
      "Batch 124, Loss: 0.916451, Accuracy: 78.10%\n",
      "Batch 125, Loss: 0.924367, Accuracy: 78.14%\n",
      "Batch 126, Loss: 0.962250, Accuracy: 78.15%\n",
      "Batch 127, Loss: 0.969927, Accuracy: 78.15%\n",
      "Batch 128, Loss: 1.025155, Accuracy: 78.10%\n",
      "Batch 129, Loss: 0.978810, Accuracy: 78.09%\n",
      "Batch 130, Loss: 0.937667, Accuracy: 78.10%\n",
      "Batch 131, Loss: 0.997671, Accuracy: 78.08%\n",
      "Batch 132, Loss: 0.994762, Accuracy: 78.05%\n",
      "Batch 133, Loss: 1.006722, Accuracy: 78.03%\n",
      "Batch 134, Loss: 0.986342, Accuracy: 78.02%\n",
      "Batch 135, Loss: 0.937926, Accuracy: 78.04%\n",
      "Batch 136, Loss: 0.987882, Accuracy: 78.03%\n",
      "Batch 137, Loss: 1.069829, Accuracy: 77.97%\n",
      "Batch 138, Loss: 0.873712, Accuracy: 78.02%\n",
      "Batch 139, Loss: 0.992194, Accuracy: 78.01%\n",
      "Batch 140, Loss: 0.929973, Accuracy: 78.02%\n",
      "Batch 141, Loss: 0.959634, Accuracy: 78.03%\n",
      "Batch 142, Loss: 0.982083, Accuracy: 78.00%\n",
      "Batch 143, Loss: 0.964024, Accuracy: 78.00%\n",
      "Batch 144, Loss: 0.958638, Accuracy: 78.01%\n",
      "Batch 145, Loss: 0.899928, Accuracy: 78.05%\n",
      "Batch 146, Loss: 0.976544, Accuracy: 78.03%\n",
      "Batch 147, Loss: 0.904425, Accuracy: 78.06%\n",
      "Batch 148, Loss: 0.940285, Accuracy: 78.07%\n",
      "Batch 149, Loss: 0.946149, Accuracy: 78.09%\n",
      "Batch 150, Loss: 0.930891, Accuracy: 78.11%\n",
      "Batch 151, Loss: 0.922295, Accuracy: 78.14%\n",
      "Batch 152, Loss: 0.937129, Accuracy: 78.16%\n",
      "Batch 153, Loss: 0.978381, Accuracy: 78.14%\n",
      "Batch 154, Loss: 0.917689, Accuracy: 78.17%\n",
      "Batch 155, Loss: 1.000543, Accuracy: 78.14%\n",
      "Batch 156, Loss: 0.996404, Accuracy: 78.11%\n",
      "Batch 157, Loss: 0.946723, Accuracy: 78.12%\n",
      "Batch 158, Loss: 0.973293, Accuracy: 78.11%\n",
      "Batch 159, Loss: 0.875911, Accuracy: 78.16%\n",
      "Batch 160, Loss: 1.008177, Accuracy: 78.12%\n",
      "Batch 161, Loss: 0.936903, Accuracy: 78.13%\n",
      "Batch 162, Loss: 0.932283, Accuracy: 78.15%\n",
      "Batch 163, Loss: 1.015582, Accuracy: 78.12%\n",
      "Batch 164, Loss: 0.977800, Accuracy: 78.11%\n",
      "Batch 165, Loss: 0.926196, Accuracy: 78.12%\n",
      "Batch 166, Loss: 0.933658, Accuracy: 78.14%\n",
      "Batch 167, Loss: 0.914527, Accuracy: 78.17%\n",
      "Batch 168, Loss: 0.998678, Accuracy: 78.15%\n",
      "Batch 169, Loss: 1.007324, Accuracy: 78.13%\n",
      "Batch 170, Loss: 0.987562, Accuracy: 78.12%\n",
      "Batch 171, Loss: 0.923874, Accuracy: 78.14%\n",
      "Batch 172, Loss: 0.937831, Accuracy: 78.15%\n",
      "Batch 173, Loss: 0.990358, Accuracy: 78.14%\n",
      "Batch 174, Loss: 0.972273, Accuracy: 78.12%\n",
      "Batch 175, Loss: 0.922420, Accuracy: 78.15%\n",
      "Batch 176, Loss: 1.003665, Accuracy: 78.12%\n",
      "Batch 177, Loss: 0.923897, Accuracy: 78.14%\n",
      "Batch 178, Loss: 0.949421, Accuracy: 78.14%\n",
      "Batch 179, Loss: 0.961294, Accuracy: 78.15%\n",
      "Batch 180, Loss: 0.884570, Accuracy: 78.19%\n",
      "Batch 181, Loss: 0.901571, Accuracy: 78.22%\n",
      "Batch 182, Loss: 1.023023, Accuracy: 78.18%\n",
      "Batch 183, Loss: 0.836809, Accuracy: 78.24%\n",
      "Batch 184, Loss: 1.026047, Accuracy: 78.20%\n",
      "Batch 185, Loss: 1.016709, Accuracy: 78.17%\n",
      "Batch 186, Loss: 0.945935, Accuracy: 78.16%\n",
      "Batch 187, Loss: 0.889183, Accuracy: 78.20%\n",
      "Batch 188, Loss: 0.962939, Accuracy: 78.21%\n",
      "Batch 189, Loss: 0.960049, Accuracy: 78.21%\n",
      "Batch 190, Loss: 1.018776, Accuracy: 78.17%\n",
      "Batch 191, Loss: 0.970678, Accuracy: 78.17%\n",
      "Batch 192, Loss: 0.951994, Accuracy: 78.16%\n",
      "Batch 193, Loss: 0.996224, Accuracy: 78.14%\n",
      "Batch 194, Loss: 1.016440, Accuracy: 78.12%\n",
      "Batch 195, Loss: 0.964305, Accuracy: 78.12%\n",
      "Batch 196, Loss: 0.924442, Accuracy: 78.14%\n",
      "Batch 197, Loss: 0.940974, Accuracy: 78.16%\n",
      "Batch 198, Loss: 0.873263, Accuracy: 78.20%\n",
      "Batch 199, Loss: 0.930690, Accuracy: 78.22%\n",
      "Batch 200, Loss: 0.973539, Accuracy: 78.21%\n",
      "Batch 201, Loss: 0.971475, Accuracy: 78.20%\n",
      "Batch 202, Loss: 0.998809, Accuracy: 78.19%\n",
      "Batch 203, Loss: 1.009333, Accuracy: 78.16%\n",
      "Batch 204, Loss: 0.902113, Accuracy: 78.20%\n",
      "Batch 205, Loss: 0.953593, Accuracy: 78.21%\n",
      "Batch 206, Loss: 0.920894, Accuracy: 78.23%\n",
      "Batch 207, Loss: 0.954382, Accuracy: 78.23%\n",
      "Batch 208, Loss: 0.970071, Accuracy: 78.22%\n",
      "Batch 209, Loss: 0.866534, Accuracy: 78.25%\n",
      "Batch 210, Loss: 1.007572, Accuracy: 78.24%\n",
      "Batch 211, Loss: 0.910252, Accuracy: 78.27%\n",
      "Batch 212, Loss: 1.005456, Accuracy: 78.24%\n",
      "Batch 213, Loss: 0.877134, Accuracy: 78.28%\n",
      "Training - Epoch 91, Loss: 0.959476, Accuracy: 78.28%\n",
      "Validation Batch 1, Loss: 0.923471, Accuracy: 82.81%\n",
      "Validation Batch 2, Loss: 0.952611, Accuracy: 80.47%\n",
      "Validation Batch 3, Loss: 1.006958, Accuracy: 78.12%\n",
      "Validation Batch 4, Loss: 0.949562, Accuracy: 78.91%\n",
      "Validation Batch 5, Loss: 0.925877, Accuracy: 79.69%\n",
      "Validation Batch 6, Loss: 0.895690, Accuracy: 80.47%\n",
      "Validation Batch 7, Loss: 0.969070, Accuracy: 80.13%\n",
      "Validation Batch 8, Loss: 0.996682, Accuracy: 79.30%\n",
      "Validation Batch 9, Loss: 0.997863, Accuracy: 78.82%\n",
      "Validation Batch 10, Loss: 0.986643, Accuracy: 78.44%\n",
      "Validation Batch 11, Loss: 0.916600, Accuracy: 78.98%\n",
      "Validation Batch 12, Loss: 0.896159, Accuracy: 79.56%\n",
      "Validation Batch 13, Loss: 1.017634, Accuracy: 78.73%\n",
      "Validation Batch 14, Loss: 0.978723, Accuracy: 78.57%\n",
      "Validation Batch 15, Loss: 0.962838, Accuracy: 78.54%\n",
      "Validation Batch 16, Loss: 0.917819, Accuracy: 78.91%\n",
      "Validation Batch 17, Loss: 0.966248, Accuracy: 78.86%\n",
      "Validation Batch 18, Loss: 0.951196, Accuracy: 78.91%\n",
      "Validation Batch 19, Loss: 0.981742, Accuracy: 78.78%\n",
      "Validation Batch 20, Loss: 0.942086, Accuracy: 78.91%\n",
      "Validation Batch 21, Loss: 0.959245, Accuracy: 78.94%\n",
      "Validation Batch 22, Loss: 0.985414, Accuracy: 78.76%\n",
      "Validation Batch 23, Loss: 1.039510, Accuracy: 78.40%\n",
      "Validation Batch 24, Loss: 1.038118, Accuracy: 77.99%\n",
      "Validation Batch 25, Loss: 0.942066, Accuracy: 78.06%\n",
      "Validation Batch 26, Loss: 0.975494, Accuracy: 78.00%\n",
      "Validation Batch 27, Loss: 0.919650, Accuracy: 78.16%\n",
      "Validation - Epoch 91, Loss: 0.962777, Accuracy: 78.16%\n",
      "Patienceâ€”0\n",
      "Epoch 92\n",
      "Batch 1, Loss: 0.965607, Accuracy: 75.00%\n",
      "Batch 2, Loss: 0.980047, Accuracy: 75.78%\n",
      "Batch 3, Loss: 0.953890, Accuracy: 76.56%\n",
      "Batch 4, Loss: 0.923769, Accuracy: 77.73%\n",
      "Batch 5, Loss: 0.974057, Accuracy: 77.81%\n",
      "Batch 6, Loss: 0.888855, Accuracy: 79.17%\n",
      "Batch 7, Loss: 0.993498, Accuracy: 78.57%\n",
      "Batch 8, Loss: 0.962931, Accuracy: 78.52%\n",
      "Batch 9, Loss: 0.931550, Accuracy: 78.99%\n",
      "Batch 10, Loss: 0.923827, Accuracy: 79.22%\n",
      "Batch 11, Loss: 0.994633, Accuracy: 78.84%\n",
      "Batch 12, Loss: 0.975005, Accuracy: 78.65%\n",
      "Batch 13, Loss: 0.948118, Accuracy: 78.73%\n",
      "Batch 14, Loss: 0.915074, Accuracy: 79.13%\n",
      "Batch 15, Loss: 0.930624, Accuracy: 79.38%\n",
      "Batch 16, Loss: 0.987226, Accuracy: 79.00%\n",
      "Batch 17, Loss: 0.978908, Accuracy: 78.77%\n",
      "Batch 18, Loss: 1.020216, Accuracy: 78.39%\n",
      "Batch 19, Loss: 0.923361, Accuracy: 78.54%\n",
      "Batch 20, Loss: 1.065663, Accuracy: 77.97%\n",
      "Batch 21, Loss: 0.978605, Accuracy: 77.98%\n",
      "Batch 22, Loss: 0.900699, Accuracy: 78.27%\n",
      "Batch 23, Loss: 0.953295, Accuracy: 78.40%\n",
      "Batch 24, Loss: 0.929617, Accuracy: 78.58%\n",
      "Batch 25, Loss: 0.995014, Accuracy: 78.44%\n",
      "Batch 26, Loss: 0.920745, Accuracy: 78.55%\n",
      "Batch 27, Loss: 1.026003, Accuracy: 78.30%\n",
      "Batch 28, Loss: 0.901429, Accuracy: 78.52%\n",
      "Batch 29, Loss: 0.993330, Accuracy: 78.39%\n",
      "Batch 30, Loss: 0.969590, Accuracy: 78.39%\n",
      "Batch 31, Loss: 0.961468, Accuracy: 78.38%\n",
      "Batch 32, Loss: 0.843144, Accuracy: 78.71%\n",
      "Batch 33, Loss: 0.970051, Accuracy: 78.65%\n",
      "Batch 34, Loss: 0.956928, Accuracy: 78.68%\n",
      "Batch 35, Loss: 0.939737, Accuracy: 78.75%\n",
      "Batch 36, Loss: 1.019714, Accuracy: 78.56%\n",
      "Batch 37, Loss: 0.904941, Accuracy: 78.63%\n",
      "Batch 38, Loss: 1.004599, Accuracy: 78.50%\n",
      "Batch 39, Loss: 1.020827, Accuracy: 78.37%\n",
      "Batch 40, Loss: 0.890822, Accuracy: 78.55%\n",
      "Batch 41, Loss: 0.982368, Accuracy: 78.47%\n",
      "Batch 42, Loss: 1.005315, Accuracy: 78.31%\n",
      "Batch 43, Loss: 0.938834, Accuracy: 78.31%\n",
      "Batch 44, Loss: 0.990833, Accuracy: 78.23%\n",
      "Batch 45, Loss: 0.975061, Accuracy: 78.23%\n",
      "Batch 46, Loss: 0.986105, Accuracy: 78.19%\n",
      "Batch 47, Loss: 0.860746, Accuracy: 78.36%\n",
      "Batch 48, Loss: 0.908329, Accuracy: 78.52%\n",
      "Batch 49, Loss: 0.890412, Accuracy: 78.67%\n",
      "Batch 50, Loss: 0.923139, Accuracy: 78.72%\n",
      "Batch 51, Loss: 0.994408, Accuracy: 78.65%\n",
      "Batch 52, Loss: 0.915779, Accuracy: 78.73%\n",
      "Batch 53, Loss: 0.962047, Accuracy: 78.71%\n",
      "Batch 54, Loss: 1.022415, Accuracy: 78.59%\n",
      "Batch 55, Loss: 0.916155, Accuracy: 78.66%\n",
      "Batch 56, Loss: 0.957539, Accuracy: 78.66%\n",
      "Batch 57, Loss: 0.942376, Accuracy: 78.67%\n",
      "Batch 58, Loss: 1.020406, Accuracy: 78.58%\n",
      "Batch 59, Loss: 0.955681, Accuracy: 78.58%\n",
      "Batch 60, Loss: 0.972902, Accuracy: 78.57%\n",
      "Batch 61, Loss: 1.011912, Accuracy: 78.48%\n",
      "Batch 62, Loss: 1.017310, Accuracy: 78.38%\n",
      "Batch 63, Loss: 0.951785, Accuracy: 78.42%\n",
      "Batch 64, Loss: 0.991229, Accuracy: 78.37%\n",
      "Batch 65, Loss: 0.988306, Accuracy: 78.34%\n",
      "Batch 66, Loss: 0.901388, Accuracy: 78.43%\n",
      "Batch 67, Loss: 0.994426, Accuracy: 78.43%\n",
      "Batch 68, Loss: 0.913898, Accuracy: 78.52%\n",
      "Batch 69, Loss: 0.911036, Accuracy: 78.58%\n",
      "Batch 70, Loss: 0.967760, Accuracy: 78.55%\n",
      "Batch 71, Loss: 1.026748, Accuracy: 78.46%\n",
      "Batch 72, Loss: 0.966049, Accuracy: 78.43%\n",
      "Batch 73, Loss: 0.951536, Accuracy: 78.45%\n",
      "Batch 74, Loss: 0.952925, Accuracy: 78.44%\n",
      "Batch 75, Loss: 1.022420, Accuracy: 78.35%\n",
      "Batch 76, Loss: 0.925152, Accuracy: 78.41%\n",
      "Batch 77, Loss: 1.012818, Accuracy: 78.33%\n",
      "Batch 78, Loss: 0.965968, Accuracy: 78.35%\n",
      "Batch 79, Loss: 0.921161, Accuracy: 78.42%\n",
      "Batch 80, Loss: 0.939667, Accuracy: 78.44%\n",
      "Batch 81, Loss: 1.033147, Accuracy: 78.34%\n",
      "Batch 82, Loss: 0.879727, Accuracy: 78.43%\n",
      "Batch 83, Loss: 0.931346, Accuracy: 78.48%\n",
      "Batch 84, Loss: 0.940136, Accuracy: 78.50%\n",
      "Batch 85, Loss: 0.963045, Accuracy: 78.47%\n",
      "Batch 86, Loss: 0.943606, Accuracy: 78.49%\n",
      "Batch 87, Loss: 0.997845, Accuracy: 78.43%\n",
      "Batch 88, Loss: 0.945832, Accuracy: 78.44%\n",
      "Batch 89, Loss: 0.953593, Accuracy: 78.44%\n",
      "Batch 90, Loss: 0.943681, Accuracy: 78.44%\n",
      "Batch 91, Loss: 0.907440, Accuracy: 78.49%\n",
      "Batch 92, Loss: 0.945858, Accuracy: 78.50%\n",
      "Batch 93, Loss: 1.030015, Accuracy: 78.41%\n",
      "Batch 94, Loss: 0.925524, Accuracy: 78.44%\n",
      "Batch 95, Loss: 0.870839, Accuracy: 78.54%\n",
      "Batch 96, Loss: 0.989368, Accuracy: 78.50%\n",
      "Batch 97, Loss: 0.939333, Accuracy: 78.53%\n",
      "Batch 98, Loss: 0.908578, Accuracy: 78.59%\n",
      "Batch 99, Loss: 1.040266, Accuracy: 78.50%\n",
      "Batch 100, Loss: 0.936808, Accuracy: 78.53%\n",
      "Batch 101, Loss: 0.912932, Accuracy: 78.57%\n",
      "Batch 102, Loss: 1.044525, Accuracy: 78.49%\n",
      "Batch 103, Loss: 0.940562, Accuracy: 78.52%\n",
      "Batch 104, Loss: 0.926031, Accuracy: 78.56%\n",
      "Batch 105, Loss: 0.974998, Accuracy: 78.56%\n",
      "Batch 106, Loss: 0.876032, Accuracy: 78.66%\n",
      "Batch 107, Loss: 0.904802, Accuracy: 78.68%\n",
      "Batch 108, Loss: 0.945650, Accuracy: 78.69%\n",
      "Batch 109, Loss: 0.989533, Accuracy: 78.67%\n",
      "Batch 110, Loss: 0.982865, Accuracy: 78.65%\n",
      "Batch 111, Loss: 0.933474, Accuracy: 78.66%\n",
      "Batch 112, Loss: 0.983033, Accuracy: 78.64%\n",
      "Batch 113, Loss: 0.992296, Accuracy: 78.61%\n",
      "Batch 114, Loss: 0.965135, Accuracy: 78.60%\n",
      "Batch 115, Loss: 0.920042, Accuracy: 78.64%\n",
      "Batch 116, Loss: 0.964351, Accuracy: 78.64%\n",
      "Batch 117, Loss: 0.933097, Accuracy: 78.65%\n",
      "Batch 118, Loss: 0.932003, Accuracy: 78.67%\n",
      "Batch 119, Loss: 1.026326, Accuracy: 78.61%\n",
      "Batch 120, Loss: 0.973403, Accuracy: 78.59%\n",
      "Batch 121, Loss: 0.966230, Accuracy: 78.59%\n",
      "Batch 122, Loss: 0.953525, Accuracy: 78.60%\n",
      "Batch 123, Loss: 0.961927, Accuracy: 78.60%\n",
      "Batch 124, Loss: 0.956564, Accuracy: 78.59%\n",
      "Batch 125, Loss: 0.969563, Accuracy: 78.60%\n",
      "Batch 126, Loss: 0.932938, Accuracy: 78.62%\n",
      "Batch 127, Loss: 0.962487, Accuracy: 78.60%\n",
      "Batch 128, Loss: 0.989852, Accuracy: 78.58%\n",
      "Batch 129, Loss: 0.976482, Accuracy: 78.55%\n",
      "Batch 130, Loss: 0.986713, Accuracy: 78.52%\n",
      "Batch 131, Loss: 0.990659, Accuracy: 78.49%\n",
      "Batch 132, Loss: 0.998179, Accuracy: 78.46%\n",
      "Batch 133, Loss: 0.974414, Accuracy: 78.44%\n",
      "Batch 134, Loss: 0.912176, Accuracy: 78.47%\n",
      "Batch 135, Loss: 0.891284, Accuracy: 78.52%\n",
      "Batch 136, Loss: 0.912390, Accuracy: 78.55%\n",
      "Batch 137, Loss: 0.890191, Accuracy: 78.60%\n",
      "Batch 138, Loss: 0.973885, Accuracy: 78.59%\n",
      "Batch 139, Loss: 0.931364, Accuracy: 78.61%\n",
      "Batch 140, Loss: 0.979706, Accuracy: 78.58%\n",
      "Batch 141, Loss: 0.897223, Accuracy: 78.62%\n",
      "Batch 142, Loss: 0.895315, Accuracy: 78.66%\n",
      "Batch 143, Loss: 0.936914, Accuracy: 78.69%\n",
      "Batch 144, Loss: 0.963882, Accuracy: 78.70%\n",
      "Batch 145, Loss: 1.005968, Accuracy: 78.67%\n",
      "Batch 146, Loss: 0.945904, Accuracy: 78.67%\n",
      "Batch 147, Loss: 1.048147, Accuracy: 78.61%\n",
      "Batch 148, Loss: 0.951922, Accuracy: 78.62%\n",
      "Batch 149, Loss: 1.017056, Accuracy: 78.58%\n",
      "Batch 150, Loss: 0.883540, Accuracy: 78.62%\n",
      "Batch 151, Loss: 0.930466, Accuracy: 78.64%\n",
      "Batch 152, Loss: 0.974583, Accuracy: 78.63%\n",
      "Batch 153, Loss: 0.911882, Accuracy: 78.68%\n",
      "Batch 154, Loss: 1.009459, Accuracy: 78.65%\n",
      "Batch 155, Loss: 0.970564, Accuracy: 78.65%\n",
      "Batch 156, Loss: 0.838854, Accuracy: 78.74%\n",
      "Batch 157, Loss: 0.912564, Accuracy: 78.76%\n",
      "Batch 158, Loss: 0.866918, Accuracy: 78.82%\n",
      "Batch 159, Loss: 0.983505, Accuracy: 78.82%\n",
      "Batch 160, Loss: 0.920803, Accuracy: 78.86%\n",
      "Batch 161, Loss: 0.953049, Accuracy: 78.86%\n",
      "Batch 162, Loss: 0.981269, Accuracy: 78.85%\n",
      "Batch 163, Loss: 0.939685, Accuracy: 78.87%\n",
      "Batch 164, Loss: 0.940487, Accuracy: 78.88%\n",
      "Batch 165, Loss: 0.982046, Accuracy: 78.85%\n",
      "Batch 166, Loss: 0.945698, Accuracy: 78.86%\n",
      "Batch 167, Loss: 1.009766, Accuracy: 78.82%\n",
      "Batch 168, Loss: 1.031456, Accuracy: 78.78%\n",
      "Batch 169, Loss: 0.974771, Accuracy: 78.76%\n",
      "Batch 170, Loss: 0.984251, Accuracy: 78.75%\n",
      "Batch 171, Loss: 1.062396, Accuracy: 78.68%\n",
      "Batch 172, Loss: 0.977158, Accuracy: 78.67%\n",
      "Batch 173, Loss: 0.970759, Accuracy: 78.66%\n",
      "Batch 174, Loss: 0.931128, Accuracy: 78.68%\n",
      "Batch 175, Loss: 0.910148, Accuracy: 78.71%\n",
      "Batch 176, Loss: 0.919855, Accuracy: 78.73%\n",
      "Batch 177, Loss: 0.908890, Accuracy: 78.76%\n",
      "Batch 178, Loss: 1.006415, Accuracy: 78.73%\n",
      "Batch 179, Loss: 0.917224, Accuracy: 78.75%\n",
      "Batch 180, Loss: 1.002279, Accuracy: 78.72%\n",
      "Batch 181, Loss: 0.945449, Accuracy: 78.73%\n",
      "Batch 182, Loss: 0.958100, Accuracy: 78.73%\n",
      "Batch 183, Loss: 0.971659, Accuracy: 78.70%\n",
      "Batch 184, Loss: 0.936945, Accuracy: 78.70%\n",
      "Batch 185, Loss: 1.025798, Accuracy: 78.66%\n",
      "Batch 186, Loss: 0.921844, Accuracy: 78.68%\n",
      "Batch 187, Loss: 0.981588, Accuracy: 78.66%\n",
      "Batch 188, Loss: 1.007449, Accuracy: 78.63%\n",
      "Batch 189, Loss: 1.024937, Accuracy: 78.60%\n",
      "Batch 190, Loss: 0.934322, Accuracy: 78.62%\n",
      "Batch 191, Loss: 0.900772, Accuracy: 78.65%\n",
      "Batch 192, Loss: 0.933311, Accuracy: 78.66%\n",
      "Batch 193, Loss: 0.941209, Accuracy: 78.67%\n",
      "Batch 194, Loss: 0.956420, Accuracy: 78.66%\n",
      "Batch 195, Loss: 0.930795, Accuracy: 78.69%\n",
      "Batch 196, Loss: 0.932487, Accuracy: 78.70%\n",
      "Batch 197, Loss: 0.917649, Accuracy: 78.72%\n",
      "Batch 198, Loss: 1.085633, Accuracy: 78.65%\n",
      "Batch 199, Loss: 0.963093, Accuracy: 78.64%\n",
      "Batch 200, Loss: 0.937747, Accuracy: 78.65%\n",
      "Batch 201, Loss: 1.030388, Accuracy: 78.61%\n",
      "Batch 202, Loss: 0.963735, Accuracy: 78.60%\n",
      "Batch 203, Loss: 0.873195, Accuracy: 78.65%\n",
      "Batch 204, Loss: 0.967600, Accuracy: 78.65%\n",
      "Batch 205, Loss: 1.097083, Accuracy: 78.57%\n",
      "Batch 206, Loss: 0.956980, Accuracy: 78.56%\n",
      "Batch 207, Loss: 1.006546, Accuracy: 78.53%\n",
      "Batch 208, Loss: 0.950681, Accuracy: 78.54%\n",
      "Batch 209, Loss: 0.885204, Accuracy: 78.57%\n",
      "Batch 210, Loss: 0.972766, Accuracy: 78.56%\n",
      "Batch 211, Loss: 0.969619, Accuracy: 78.55%\n",
      "Batch 212, Loss: 0.951270, Accuracy: 78.57%\n",
      "Batch 213, Loss: 0.921148, Accuracy: 78.59%\n",
      "Training - Epoch 92, Loss: 0.957732, Accuracy: 78.59%\n",
      "Validation Batch 1, Loss: 0.888217, Accuracy: 84.38%\n",
      "Validation Batch 2, Loss: 0.894642, Accuracy: 85.16%\n",
      "Validation Batch 3, Loss: 0.976021, Accuracy: 82.29%\n",
      "Validation Batch 4, Loss: 0.926912, Accuracy: 82.03%\n",
      "Validation Batch 5, Loss: 0.893994, Accuracy: 82.81%\n",
      "Validation Batch 6, Loss: 0.831356, Accuracy: 84.38%\n",
      "Validation Batch 7, Loss: 0.927567, Accuracy: 83.71%\n",
      "Validation Batch 8, Loss: 0.940631, Accuracy: 83.40%\n",
      "Validation Batch 9, Loss: 0.951116, Accuracy: 82.81%\n",
      "Validation Batch 10, Loss: 0.947448, Accuracy: 82.50%\n",
      "Validation Batch 11, Loss: 0.904701, Accuracy: 82.67%\n",
      "Validation Batch 12, Loss: 0.869940, Accuracy: 82.94%\n",
      "Validation Batch 13, Loss: 0.940909, Accuracy: 82.81%\n",
      "Validation Batch 14, Loss: 0.929275, Accuracy: 82.70%\n",
      "Validation Batch 15, Loss: 0.949755, Accuracy: 82.40%\n",
      "Validation Batch 16, Loss: 0.889586, Accuracy: 82.62%\n",
      "Validation Batch 17, Loss: 0.930200, Accuracy: 82.63%\n",
      "Validation Batch 18, Loss: 0.927048, Accuracy: 82.47%\n",
      "Validation Batch 19, Loss: 0.943111, Accuracy: 82.32%\n",
      "Validation Batch 20, Loss: 0.869999, Accuracy: 82.66%\n",
      "Validation Batch 21, Loss: 0.929215, Accuracy: 82.59%\n",
      "Validation Batch 22, Loss: 0.916704, Accuracy: 82.53%\n",
      "Validation Batch 23, Loss: 0.954218, Accuracy: 82.34%\n",
      "Validation Batch 24, Loss: 0.988379, Accuracy: 82.03%\n",
      "Validation Batch 25, Loss: 0.902800, Accuracy: 82.12%\n",
      "Validation Batch 26, Loss: 0.949831, Accuracy: 82.03%\n",
      "Validation Batch 27, Loss: 0.868853, Accuracy: 82.15%\n",
      "Validation - Epoch 92, Loss: 0.920090, Accuracy: 82.15%\n",
      "Patienceâ€”0\n",
      "Epoch 93\n",
      "Batch 1, Loss: 0.952809, Accuracy: 78.12%\n",
      "Batch 2, Loss: 0.995969, Accuracy: 76.56%\n",
      "Batch 3, Loss: 1.017103, Accuracy: 74.48%\n",
      "Batch 4, Loss: 0.911106, Accuracy: 76.95%\n",
      "Batch 5, Loss: 0.961289, Accuracy: 77.19%\n",
      "Batch 6, Loss: 0.901902, Accuracy: 78.65%\n",
      "Batch 7, Loss: 0.963889, Accuracy: 78.79%\n",
      "Batch 8, Loss: 1.027145, Accuracy: 77.93%\n",
      "Batch 9, Loss: 0.894835, Accuracy: 78.82%\n",
      "Batch 10, Loss: 0.939109, Accuracy: 78.75%\n",
      "Batch 11, Loss: 0.910145, Accuracy: 78.98%\n",
      "Batch 12, Loss: 0.944359, Accuracy: 79.17%\n",
      "Batch 13, Loss: 1.045792, Accuracy: 78.37%\n",
      "Batch 14, Loss: 1.015711, Accuracy: 78.01%\n",
      "Batch 15, Loss: 0.912313, Accuracy: 78.23%\n",
      "Batch 16, Loss: 0.967787, Accuracy: 78.22%\n",
      "Batch 17, Loss: 0.897497, Accuracy: 78.68%\n",
      "Batch 18, Loss: 0.959336, Accuracy: 78.65%\n",
      "Batch 19, Loss: 0.950275, Accuracy: 78.70%\n",
      "Batch 20, Loss: 0.933558, Accuracy: 78.83%\n",
      "Batch 21, Loss: 0.945390, Accuracy: 78.87%\n",
      "Batch 22, Loss: 0.970486, Accuracy: 78.84%\n",
      "Batch 23, Loss: 0.919395, Accuracy: 78.94%\n",
      "Batch 24, Loss: 1.013585, Accuracy: 78.71%\n",
      "Batch 25, Loss: 0.995921, Accuracy: 78.44%\n",
      "Batch 26, Loss: 0.983917, Accuracy: 78.31%\n",
      "Batch 27, Loss: 0.936334, Accuracy: 78.41%\n",
      "Batch 28, Loss: 1.040905, Accuracy: 78.07%\n",
      "Batch 29, Loss: 0.940869, Accuracy: 78.12%\n",
      "Batch 30, Loss: 1.027927, Accuracy: 77.86%\n",
      "Batch 31, Loss: 0.960242, Accuracy: 77.92%\n",
      "Batch 32, Loss: 0.974687, Accuracy: 77.93%\n",
      "Batch 33, Loss: 0.949828, Accuracy: 77.98%\n",
      "Batch 34, Loss: 1.005191, Accuracy: 77.80%\n",
      "Batch 35, Loss: 0.952825, Accuracy: 77.86%\n",
      "Batch 36, Loss: 0.888037, Accuracy: 78.08%\n",
      "Batch 37, Loss: 1.028616, Accuracy: 77.83%\n",
      "Batch 38, Loss: 0.907280, Accuracy: 78.00%\n",
      "Batch 39, Loss: 0.942515, Accuracy: 78.08%\n",
      "Batch 40, Loss: 0.887003, Accuracy: 78.28%\n",
      "Batch 41, Loss: 0.878883, Accuracy: 78.47%\n",
      "Batch 42, Loss: 0.973735, Accuracy: 78.46%\n",
      "Batch 43, Loss: 0.926384, Accuracy: 78.52%\n",
      "Batch 44, Loss: 1.028972, Accuracy: 78.34%\n",
      "Batch 45, Loss: 1.025989, Accuracy: 78.16%\n",
      "Batch 46, Loss: 0.922546, Accuracy: 78.19%\n",
      "Batch 47, Loss: 0.941103, Accuracy: 78.22%\n",
      "Batch 48, Loss: 0.984698, Accuracy: 78.19%\n",
      "Batch 49, Loss: 0.968423, Accuracy: 78.19%\n",
      "Batch 50, Loss: 0.898523, Accuracy: 78.31%\n",
      "Batch 51, Loss: 0.950653, Accuracy: 78.40%\n",
      "Batch 52, Loss: 1.000743, Accuracy: 78.34%\n",
      "Batch 53, Loss: 0.895886, Accuracy: 78.48%\n",
      "Batch 54, Loss: 1.024251, Accuracy: 78.36%\n",
      "Batch 55, Loss: 1.038604, Accuracy: 78.24%\n",
      "Batch 56, Loss: 0.933567, Accuracy: 78.26%\n",
      "Batch 57, Loss: 0.969989, Accuracy: 78.23%\n",
      "Batch 58, Loss: 1.014694, Accuracy: 78.12%\n",
      "Batch 59, Loss: 0.899027, Accuracy: 78.26%\n",
      "Batch 60, Loss: 1.012278, Accuracy: 78.15%\n",
      "Batch 61, Loss: 1.042406, Accuracy: 78.00%\n",
      "Batch 62, Loss: 0.901662, Accuracy: 78.12%\n",
      "Batch 63, Loss: 0.955397, Accuracy: 78.10%\n",
      "Batch 64, Loss: 1.010574, Accuracy: 78.00%\n",
      "Batch 65, Loss: 0.926659, Accuracy: 78.05%\n",
      "Batch 66, Loss: 0.963504, Accuracy: 78.05%\n",
      "Batch 67, Loss: 0.921457, Accuracy: 78.10%\n",
      "Batch 68, Loss: 0.949235, Accuracy: 78.10%\n",
      "Batch 69, Loss: 0.949280, Accuracy: 78.12%\n",
      "Batch 70, Loss: 0.987562, Accuracy: 78.08%\n",
      "Batch 71, Loss: 0.934427, Accuracy: 78.12%\n",
      "Batch 72, Loss: 0.972771, Accuracy: 78.10%\n",
      "Batch 73, Loss: 0.919171, Accuracy: 78.17%\n",
      "Batch 74, Loss: 0.902532, Accuracy: 78.25%\n",
      "Batch 75, Loss: 1.039642, Accuracy: 78.15%\n",
      "Batch 76, Loss: 1.019830, Accuracy: 78.04%\n",
      "Batch 77, Loss: 1.004768, Accuracy: 77.98%\n",
      "Batch 78, Loss: 0.896806, Accuracy: 78.06%\n",
      "Batch 79, Loss: 0.924120, Accuracy: 78.12%\n",
      "Batch 80, Loss: 0.964353, Accuracy: 78.11%\n",
      "Batch 81, Loss: 0.949763, Accuracy: 78.11%\n",
      "Batch 82, Loss: 0.992691, Accuracy: 78.07%\n",
      "Batch 83, Loss: 0.927224, Accuracy: 78.11%\n",
      "Batch 84, Loss: 1.052909, Accuracy: 77.99%\n",
      "Batch 85, Loss: 0.948929, Accuracy: 78.00%\n",
      "Batch 86, Loss: 1.029327, Accuracy: 77.93%\n",
      "Batch 87, Loss: 1.020177, Accuracy: 77.84%\n",
      "Batch 88, Loss: 0.976113, Accuracy: 77.82%\n",
      "Batch 89, Loss: 0.985245, Accuracy: 77.83%\n",
      "Batch 90, Loss: 1.001858, Accuracy: 77.78%\n",
      "Batch 91, Loss: 0.957442, Accuracy: 77.78%\n",
      "Batch 92, Loss: 0.919095, Accuracy: 77.82%\n",
      "Batch 93, Loss: 0.897840, Accuracy: 77.89%\n",
      "Batch 94, Loss: 0.923026, Accuracy: 77.93%\n",
      "Batch 95, Loss: 0.859603, Accuracy: 78.06%\n",
      "Batch 96, Loss: 0.966322, Accuracy: 78.06%\n",
      "Batch 97, Loss: 0.892789, Accuracy: 78.14%\n",
      "Batch 98, Loss: 0.968997, Accuracy: 78.11%\n",
      "Batch 99, Loss: 0.812907, Accuracy: 78.28%\n",
      "Batch 100, Loss: 0.871648, Accuracy: 78.39%\n",
      "Batch 101, Loss: 0.960557, Accuracy: 78.40%\n",
      "Batch 102, Loss: 0.961801, Accuracy: 78.39%\n",
      "Batch 103, Loss: 1.031928, Accuracy: 78.32%\n",
      "Batch 104, Loss: 0.934468, Accuracy: 78.37%\n",
      "Batch 105, Loss: 0.951149, Accuracy: 78.39%\n",
      "Batch 106, Loss: 1.010253, Accuracy: 78.35%\n",
      "Batch 107, Loss: 0.950287, Accuracy: 78.34%\n",
      "Batch 108, Loss: 0.986343, Accuracy: 78.33%\n",
      "Batch 109, Loss: 0.901247, Accuracy: 78.38%\n",
      "Batch 110, Loss: 1.022749, Accuracy: 78.31%\n",
      "Batch 111, Loss: 0.990628, Accuracy: 78.28%\n",
      "Batch 112, Loss: 0.875545, Accuracy: 78.36%\n",
      "Batch 113, Loss: 1.036182, Accuracy: 78.29%\n",
      "Batch 114, Loss: 0.949425, Accuracy: 78.29%\n",
      "Batch 115, Loss: 0.882409, Accuracy: 78.36%\n",
      "Batch 116, Loss: 0.948229, Accuracy: 78.37%\n",
      "Batch 117, Loss: 0.963686, Accuracy: 78.38%\n",
      "Batch 118, Loss: 0.916753, Accuracy: 78.40%\n",
      "Batch 119, Loss: 1.042519, Accuracy: 78.34%\n",
      "Batch 120, Loss: 0.962289, Accuracy: 78.33%\n",
      "Batch 121, Loss: 0.921547, Accuracy: 78.36%\n",
      "Batch 122, Loss: 1.117851, Accuracy: 78.21%\n",
      "Batch 123, Loss: 0.952159, Accuracy: 78.23%\n",
      "Batch 124, Loss: 0.904986, Accuracy: 78.28%\n",
      "Batch 125, Loss: 0.975327, Accuracy: 78.26%\n",
      "Batch 126, Loss: 0.861332, Accuracy: 78.35%\n",
      "Batch 127, Loss: 0.870911, Accuracy: 78.42%\n",
      "Batch 128, Loss: 0.920544, Accuracy: 78.47%\n",
      "Batch 129, Loss: 0.970773, Accuracy: 78.45%\n",
      "Batch 130, Loss: 1.002339, Accuracy: 78.41%\n",
      "Batch 131, Loss: 0.939512, Accuracy: 78.44%\n",
      "Batch 132, Loss: 0.983003, Accuracy: 78.40%\n",
      "Batch 133, Loss: 0.915198, Accuracy: 78.44%\n",
      "Batch 134, Loss: 0.996181, Accuracy: 78.42%\n",
      "Batch 135, Loss: 0.979995, Accuracy: 78.41%\n",
      "Batch 136, Loss: 1.095601, Accuracy: 78.32%\n",
      "Batch 137, Loss: 0.938126, Accuracy: 78.33%\n",
      "Batch 138, Loss: 0.959230, Accuracy: 78.33%\n",
      "Batch 139, Loss: 0.998076, Accuracy: 78.29%\n",
      "Batch 140, Loss: 0.995450, Accuracy: 78.25%\n",
      "Batch 141, Loss: 0.898273, Accuracy: 78.29%\n",
      "Batch 142, Loss: 0.852570, Accuracy: 78.37%\n",
      "Batch 143, Loss: 1.019584, Accuracy: 78.33%\n",
      "Batch 144, Loss: 1.031926, Accuracy: 78.28%\n",
      "Batch 145, Loss: 0.923153, Accuracy: 78.31%\n",
      "Batch 146, Loss: 0.946764, Accuracy: 78.33%\n",
      "Batch 147, Loss: 0.915541, Accuracy: 78.36%\n",
      "Batch 148, Loss: 0.919454, Accuracy: 78.39%\n",
      "Batch 149, Loss: 0.940436, Accuracy: 78.40%\n",
      "Batch 150, Loss: 0.971619, Accuracy: 78.40%\n",
      "Batch 151, Loss: 0.978207, Accuracy: 78.38%\n",
      "Batch 152, Loss: 1.006376, Accuracy: 78.35%\n",
      "Batch 153, Loss: 1.015357, Accuracy: 78.30%\n",
      "Batch 154, Loss: 0.946697, Accuracy: 78.31%\n",
      "Batch 155, Loss: 0.899990, Accuracy: 78.36%\n",
      "Batch 156, Loss: 0.896450, Accuracy: 78.41%\n",
      "Batch 157, Loss: 0.953615, Accuracy: 78.40%\n",
      "Batch 158, Loss: 0.987580, Accuracy: 78.38%\n",
      "Batch 159, Loss: 0.888006, Accuracy: 78.43%\n",
      "Batch 160, Loss: 0.936928, Accuracy: 78.44%\n",
      "Batch 161, Loss: 0.993779, Accuracy: 78.42%\n",
      "Batch 162, Loss: 0.980041, Accuracy: 78.41%\n",
      "Batch 163, Loss: 0.980201, Accuracy: 78.38%\n",
      "Batch 164, Loss: 1.017750, Accuracy: 78.34%\n",
      "Batch 165, Loss: 0.974144, Accuracy: 78.33%\n",
      "Batch 166, Loss: 0.974478, Accuracy: 78.34%\n",
      "Batch 167, Loss: 0.956101, Accuracy: 78.34%\n",
      "Batch 168, Loss: 0.938523, Accuracy: 78.36%\n",
      "Batch 169, Loss: 0.957806, Accuracy: 78.36%\n",
      "Batch 170, Loss: 0.947719, Accuracy: 78.36%\n",
      "Batch 171, Loss: 0.967415, Accuracy: 78.35%\n",
      "Batch 172, Loss: 0.993399, Accuracy: 78.33%\n",
      "Batch 173, Loss: 0.988310, Accuracy: 78.31%\n",
      "Batch 174, Loss: 0.937073, Accuracy: 78.32%\n",
      "Batch 175, Loss: 0.948218, Accuracy: 78.33%\n",
      "Batch 176, Loss: 1.038014, Accuracy: 78.28%\n",
      "Batch 177, Loss: 0.938228, Accuracy: 78.31%\n",
      "Batch 178, Loss: 1.019783, Accuracy: 78.26%\n",
      "Batch 179, Loss: 1.017612, Accuracy: 78.20%\n",
      "Batch 180, Loss: 0.945484, Accuracy: 78.20%\n",
      "Batch 181, Loss: 0.994605, Accuracy: 78.18%\n",
      "Batch 182, Loss: 0.918157, Accuracy: 78.20%\n",
      "Batch 183, Loss: 0.863170, Accuracy: 78.26%\n",
      "Batch 184, Loss: 0.943139, Accuracy: 78.28%\n",
      "Batch 185, Loss: 0.983194, Accuracy: 78.27%\n",
      "Batch 186, Loss: 0.955716, Accuracy: 78.28%\n",
      "Batch 187, Loss: 0.957526, Accuracy: 78.28%\n",
      "Batch 188, Loss: 0.962586, Accuracy: 78.27%\n",
      "Batch 189, Loss: 0.956236, Accuracy: 78.27%\n",
      "Batch 190, Loss: 0.962476, Accuracy: 78.26%\n",
      "Batch 191, Loss: 1.080109, Accuracy: 78.20%\n",
      "Batch 192, Loss: 0.891719, Accuracy: 78.23%\n",
      "Batch 193, Loss: 0.972597, Accuracy: 78.22%\n",
      "Batch 194, Loss: 1.032311, Accuracy: 78.18%\n",
      "Batch 195, Loss: 1.003068, Accuracy: 78.15%\n",
      "Batch 196, Loss: 0.922211, Accuracy: 78.17%\n",
      "Batch 197, Loss: 0.974772, Accuracy: 78.17%\n",
      "Batch 198, Loss: 0.949249, Accuracy: 78.18%\n",
      "Batch 199, Loss: 1.026992, Accuracy: 78.14%\n",
      "Batch 200, Loss: 0.976629, Accuracy: 78.13%\n",
      "Batch 201, Loss: 0.966340, Accuracy: 78.12%\n",
      "Batch 202, Loss: 0.875980, Accuracy: 78.16%\n",
      "Batch 203, Loss: 0.937261, Accuracy: 78.17%\n",
      "Batch 204, Loss: 0.859303, Accuracy: 78.22%\n",
      "Batch 205, Loss: 0.986033, Accuracy: 78.19%\n",
      "Batch 206, Loss: 0.953351, Accuracy: 78.20%\n",
      "Batch 207, Loss: 0.940675, Accuracy: 78.21%\n",
      "Batch 208, Loss: 0.925342, Accuracy: 78.22%\n",
      "Batch 209, Loss: 1.003728, Accuracy: 78.20%\n",
      "Batch 210, Loss: 0.911468, Accuracy: 78.22%\n",
      "Batch 211, Loss: 1.025385, Accuracy: 78.18%\n",
      "Batch 212, Loss: 0.905855, Accuracy: 78.21%\n",
      "Batch 213, Loss: 0.949293, Accuracy: 78.22%\n",
      "Training - Epoch 93, Loss: 0.959566, Accuracy: 78.22%\n",
      "Validation Batch 1, Loss: 0.931269, Accuracy: 81.25%\n",
      "Validation Batch 2, Loss: 0.939140, Accuracy: 81.25%\n",
      "Validation Batch 3, Loss: 1.011330, Accuracy: 78.65%\n",
      "Validation Batch 4, Loss: 0.963459, Accuracy: 78.52%\n",
      "Validation Batch 5, Loss: 0.929433, Accuracy: 78.75%\n",
      "Validation Batch 6, Loss: 0.867410, Accuracy: 80.21%\n",
      "Validation Batch 7, Loss: 0.963722, Accuracy: 79.91%\n",
      "Validation Batch 8, Loss: 0.976832, Accuracy: 79.30%\n",
      "Validation Batch 9, Loss: 0.984113, Accuracy: 79.17%\n",
      "Validation Batch 10, Loss: 0.971138, Accuracy: 78.91%\n",
      "Validation Batch 11, Loss: 0.910390, Accuracy: 79.40%\n",
      "Validation Batch 12, Loss: 0.892070, Accuracy: 80.08%\n",
      "Validation Batch 13, Loss: 1.000517, Accuracy: 79.57%\n",
      "Validation Batch 14, Loss: 0.983536, Accuracy: 79.24%\n",
      "Validation Batch 15, Loss: 0.965437, Accuracy: 79.17%\n",
      "Validation Batch 16, Loss: 0.922172, Accuracy: 79.39%\n",
      "Validation Batch 17, Loss: 0.957438, Accuracy: 79.41%\n",
      "Validation Batch 18, Loss: 0.965064, Accuracy: 79.25%\n",
      "Validation Batch 19, Loss: 0.986799, Accuracy: 79.11%\n",
      "Validation Batch 20, Loss: 0.952934, Accuracy: 79.06%\n",
      "Validation Batch 21, Loss: 0.961581, Accuracy: 79.09%\n",
      "Validation Batch 22, Loss: 0.974949, Accuracy: 79.05%\n",
      "Validation Batch 23, Loss: 1.032913, Accuracy: 78.60%\n",
      "Validation Batch 24, Loss: 1.037555, Accuracy: 78.19%\n",
      "Validation Batch 25, Loss: 0.947042, Accuracy: 78.25%\n",
      "Validation Batch 26, Loss: 0.963018, Accuracy: 78.25%\n",
      "Validation Batch 27, Loss: 0.901623, Accuracy: 78.39%\n",
      "Validation - Epoch 93, Loss: 0.958996, Accuracy: 78.39%\n",
      "Patienceâ€”1\n",
      "Epoch 94\n",
      "Batch 1, Loss: 0.979506, Accuracy: 76.56%\n",
      "Batch 2, Loss: 0.930347, Accuracy: 78.12%\n",
      "Batch 3, Loss: 1.005572, Accuracy: 76.04%\n",
      "Batch 4, Loss: 0.936015, Accuracy: 76.56%\n",
      "Batch 5, Loss: 0.911723, Accuracy: 77.81%\n",
      "Batch 6, Loss: 1.061229, Accuracy: 76.30%\n",
      "Batch 7, Loss: 0.866167, Accuracy: 77.90%\n",
      "Batch 8, Loss: 0.928629, Accuracy: 78.32%\n",
      "Batch 9, Loss: 0.939752, Accuracy: 78.47%\n",
      "Batch 10, Loss: 0.903211, Accuracy: 78.75%\n",
      "Batch 11, Loss: 0.874303, Accuracy: 79.69%\n",
      "Batch 12, Loss: 0.863801, Accuracy: 80.47%\n",
      "Batch 13, Loss: 1.011252, Accuracy: 79.81%\n",
      "Batch 14, Loss: 1.009972, Accuracy: 79.24%\n",
      "Batch 15, Loss: 0.971065, Accuracy: 79.17%\n",
      "Batch 16, Loss: 1.008951, Accuracy: 78.61%\n",
      "Batch 17, Loss: 0.974942, Accuracy: 78.49%\n",
      "Batch 18, Loss: 0.902075, Accuracy: 78.82%\n",
      "Batch 19, Loss: 0.979031, Accuracy: 78.62%\n",
      "Batch 20, Loss: 0.993303, Accuracy: 78.36%\n",
      "Batch 21, Loss: 0.995805, Accuracy: 78.20%\n",
      "Batch 22, Loss: 0.906814, Accuracy: 78.48%\n",
      "Batch 23, Loss: 0.961376, Accuracy: 78.40%\n",
      "Batch 24, Loss: 0.967874, Accuracy: 78.39%\n",
      "Batch 25, Loss: 0.923794, Accuracy: 78.56%\n",
      "Batch 26, Loss: 0.864124, Accuracy: 78.85%\n",
      "Batch 27, Loss: 0.905187, Accuracy: 79.05%\n",
      "Batch 28, Loss: 0.985071, Accuracy: 78.96%\n",
      "Batch 29, Loss: 0.867752, Accuracy: 79.31%\n",
      "Batch 30, Loss: 0.915027, Accuracy: 79.43%\n",
      "Batch 31, Loss: 0.974222, Accuracy: 79.33%\n",
      "Batch 32, Loss: 0.940148, Accuracy: 79.39%\n",
      "Batch 33, Loss: 0.914175, Accuracy: 79.50%\n",
      "Batch 34, Loss: 0.997302, Accuracy: 79.27%\n",
      "Batch 35, Loss: 1.033431, Accuracy: 79.06%\n",
      "Batch 36, Loss: 0.977020, Accuracy: 78.95%\n",
      "Batch 37, Loss: 0.990466, Accuracy: 78.80%\n",
      "Batch 38, Loss: 0.985802, Accuracy: 78.70%\n",
      "Batch 39, Loss: 0.871178, Accuracy: 78.93%\n",
      "Batch 40, Loss: 1.009470, Accuracy: 78.79%\n",
      "Batch 41, Loss: 0.903749, Accuracy: 78.93%\n",
      "Batch 42, Loss: 0.999858, Accuracy: 78.83%\n",
      "Batch 43, Loss: 0.946099, Accuracy: 78.82%\n",
      "Batch 44, Loss: 0.924738, Accuracy: 78.91%\n",
      "Batch 45, Loss: 0.979184, Accuracy: 78.82%\n",
      "Batch 46, Loss: 0.959487, Accuracy: 78.77%\n",
      "Batch 47, Loss: 0.953268, Accuracy: 78.79%\n",
      "Batch 48, Loss: 0.974266, Accuracy: 78.78%\n",
      "Batch 49, Loss: 1.040124, Accuracy: 78.64%\n",
      "Batch 50, Loss: 0.934578, Accuracy: 78.69%\n",
      "Batch 51, Loss: 0.938956, Accuracy: 78.71%\n",
      "Batch 52, Loss: 1.011937, Accuracy: 78.58%\n",
      "Batch 53, Loss: 0.993053, Accuracy: 78.51%\n",
      "Batch 54, Loss: 0.918096, Accuracy: 78.56%\n",
      "Batch 55, Loss: 0.934511, Accuracy: 78.58%\n",
      "Batch 56, Loss: 0.967828, Accuracy: 78.54%\n",
      "Batch 57, Loss: 0.888627, Accuracy: 78.67%\n",
      "Batch 58, Loss: 0.986212, Accuracy: 78.61%\n",
      "Batch 59, Loss: 0.939605, Accuracy: 78.63%\n",
      "Batch 60, Loss: 0.973262, Accuracy: 78.62%\n",
      "Batch 61, Loss: 1.016728, Accuracy: 78.51%\n",
      "Batch 62, Loss: 1.014803, Accuracy: 78.40%\n",
      "Batch 63, Loss: 0.989944, Accuracy: 78.35%\n",
      "Batch 64, Loss: 0.932449, Accuracy: 78.39%\n",
      "Batch 65, Loss: 1.006716, Accuracy: 78.34%\n",
      "Batch 66, Loss: 0.989149, Accuracy: 78.31%\n",
      "Batch 67, Loss: 0.986267, Accuracy: 78.24%\n",
      "Batch 68, Loss: 1.011825, Accuracy: 78.15%\n",
      "Batch 69, Loss: 0.893848, Accuracy: 78.22%\n",
      "Batch 70, Loss: 0.927981, Accuracy: 78.28%\n",
      "Batch 71, Loss: 0.913299, Accuracy: 78.37%\n",
      "Batch 72, Loss: 0.872543, Accuracy: 78.49%\n",
      "Batch 73, Loss: 0.911598, Accuracy: 78.57%\n",
      "Batch 74, Loss: 1.006045, Accuracy: 78.51%\n",
      "Batch 75, Loss: 1.011607, Accuracy: 78.42%\n",
      "Batch 76, Loss: 0.917409, Accuracy: 78.47%\n",
      "Batch 77, Loss: 0.886530, Accuracy: 78.57%\n",
      "Batch 78, Loss: 0.979197, Accuracy: 78.55%\n",
      "Batch 79, Loss: 0.945988, Accuracy: 78.56%\n",
      "Batch 80, Loss: 0.923715, Accuracy: 78.63%\n",
      "Batch 81, Loss: 0.978318, Accuracy: 78.59%\n",
      "Batch 82, Loss: 1.050233, Accuracy: 78.49%\n",
      "Batch 83, Loss: 0.986369, Accuracy: 78.43%\n",
      "Batch 84, Loss: 0.951186, Accuracy: 78.44%\n",
      "Batch 85, Loss: 0.964110, Accuracy: 78.44%\n",
      "Batch 86, Loss: 0.927115, Accuracy: 78.45%\n",
      "Batch 87, Loss: 0.934859, Accuracy: 78.48%\n",
      "Batch 88, Loss: 0.940217, Accuracy: 78.52%\n",
      "Batch 89, Loss: 1.019880, Accuracy: 78.46%\n",
      "Batch 90, Loss: 0.977168, Accuracy: 78.42%\n",
      "Batch 91, Loss: 0.924500, Accuracy: 78.47%\n",
      "Batch 92, Loss: 1.026559, Accuracy: 78.36%\n",
      "Batch 93, Loss: 0.926141, Accuracy: 78.39%\n",
      "Batch 94, Loss: 0.980825, Accuracy: 78.37%\n",
      "Batch 95, Loss: 0.980868, Accuracy: 78.36%\n",
      "Batch 96, Loss: 0.937190, Accuracy: 78.37%\n",
      "Batch 97, Loss: 0.918879, Accuracy: 78.40%\n",
      "Batch 98, Loss: 0.981537, Accuracy: 78.40%\n",
      "Batch 99, Loss: 0.899473, Accuracy: 78.46%\n",
      "Batch 100, Loss: 0.964695, Accuracy: 78.45%\n",
      "Batch 101, Loss: 1.030284, Accuracy: 78.37%\n",
      "Batch 102, Loss: 0.954565, Accuracy: 78.37%\n",
      "Batch 103, Loss: 0.940062, Accuracy: 78.40%\n",
      "Batch 104, Loss: 0.891397, Accuracy: 78.46%\n",
      "Batch 105, Loss: 1.015935, Accuracy: 78.39%\n",
      "Batch 106, Loss: 0.870059, Accuracy: 78.46%\n",
      "Batch 107, Loss: 0.903763, Accuracy: 78.50%\n",
      "Batch 108, Loss: 0.952791, Accuracy: 78.50%\n",
      "Batch 109, Loss: 0.996605, Accuracy: 78.47%\n",
      "Batch 110, Loss: 0.969140, Accuracy: 78.45%\n",
      "Batch 111, Loss: 0.947469, Accuracy: 78.46%\n",
      "Batch 112, Loss: 0.993740, Accuracy: 78.42%\n",
      "Batch 113, Loss: 0.920912, Accuracy: 78.44%\n",
      "Batch 114, Loss: 0.903017, Accuracy: 78.50%\n",
      "Batch 115, Loss: 0.939743, Accuracy: 78.52%\n",
      "Batch 116, Loss: 0.937803, Accuracy: 78.54%\n",
      "Batch 117, Loss: 0.910394, Accuracy: 78.59%\n",
      "Batch 118, Loss: 0.890408, Accuracy: 78.65%\n",
      "Batch 119, Loss: 0.960500, Accuracy: 78.64%\n",
      "Batch 120, Loss: 0.968757, Accuracy: 78.63%\n",
      "Batch 121, Loss: 0.904958, Accuracy: 78.67%\n",
      "Batch 122, Loss: 0.946696, Accuracy: 78.68%\n",
      "Batch 123, Loss: 0.979570, Accuracy: 78.65%\n",
      "Batch 124, Loss: 1.066342, Accuracy: 78.55%\n",
      "Batch 125, Loss: 1.030222, Accuracy: 78.49%\n",
      "Batch 126, Loss: 0.951398, Accuracy: 78.50%\n",
      "Batch 127, Loss: 0.958560, Accuracy: 78.51%\n",
      "Batch 128, Loss: 0.903170, Accuracy: 78.55%\n",
      "Batch 129, Loss: 1.058266, Accuracy: 78.46%\n",
      "Batch 130, Loss: 1.026799, Accuracy: 78.40%\n",
      "Batch 131, Loss: 0.909433, Accuracy: 78.45%\n",
      "Batch 132, Loss: 0.911298, Accuracy: 78.49%\n",
      "Batch 133, Loss: 0.935536, Accuracy: 78.51%\n",
      "Batch 134, Loss: 0.875517, Accuracy: 78.58%\n",
      "Batch 135, Loss: 0.976052, Accuracy: 78.58%\n",
      "Batch 136, Loss: 0.952376, Accuracy: 78.60%\n",
      "Batch 137, Loss: 0.893989, Accuracy: 78.65%\n",
      "Batch 138, Loss: 0.958692, Accuracy: 78.65%\n",
      "Batch 139, Loss: 0.956609, Accuracy: 78.63%\n",
      "Batch 140, Loss: 0.943800, Accuracy: 78.64%\n",
      "Batch 141, Loss: 0.917658, Accuracy: 78.67%\n",
      "Batch 142, Loss: 1.007321, Accuracy: 78.64%\n",
      "Batch 143, Loss: 0.970426, Accuracy: 78.63%\n",
      "Batch 144, Loss: 0.980279, Accuracy: 78.61%\n",
      "Batch 145, Loss: 1.037416, Accuracy: 78.56%\n",
      "Batch 146, Loss: 0.923678, Accuracy: 78.57%\n",
      "Batch 147, Loss: 0.969767, Accuracy: 78.58%\n",
      "Batch 148, Loss: 0.992818, Accuracy: 78.56%\n",
      "Batch 149, Loss: 0.980659, Accuracy: 78.54%\n",
      "Batch 150, Loss: 0.902298, Accuracy: 78.57%\n",
      "Batch 151, Loss: 1.030329, Accuracy: 78.54%\n",
      "Batch 152, Loss: 0.966861, Accuracy: 78.54%\n",
      "Batch 153, Loss: 0.958912, Accuracy: 78.54%\n",
      "Batch 154, Loss: 0.966954, Accuracy: 78.53%\n",
      "Batch 155, Loss: 0.938491, Accuracy: 78.55%\n",
      "Batch 156, Loss: 0.917454, Accuracy: 78.58%\n",
      "Batch 157, Loss: 0.912330, Accuracy: 78.61%\n",
      "Batch 158, Loss: 0.970311, Accuracy: 78.60%\n",
      "Batch 159, Loss: 0.961075, Accuracy: 78.60%\n",
      "Batch 160, Loss: 0.997075, Accuracy: 78.57%\n",
      "Batch 161, Loss: 0.947678, Accuracy: 78.58%\n",
      "Batch 162, Loss: 1.009849, Accuracy: 78.54%\n",
      "Batch 163, Loss: 0.930018, Accuracy: 78.55%\n",
      "Batch 164, Loss: 0.919387, Accuracy: 78.57%\n",
      "Batch 165, Loss: 0.962198, Accuracy: 78.57%\n",
      "Batch 166, Loss: 0.961105, Accuracy: 78.57%\n",
      "Batch 167, Loss: 1.003721, Accuracy: 78.53%\n",
      "Batch 168, Loss: 0.968637, Accuracy: 78.52%\n",
      "Batch 169, Loss: 0.905967, Accuracy: 78.55%\n",
      "Batch 170, Loss: 0.986496, Accuracy: 78.54%\n",
      "Batch 171, Loss: 0.909266, Accuracy: 78.57%\n",
      "Batch 172, Loss: 0.932858, Accuracy: 78.59%\n",
      "Batch 173, Loss: 0.986867, Accuracy: 78.57%\n",
      "Batch 174, Loss: 1.002394, Accuracy: 78.55%\n",
      "Batch 175, Loss: 0.909999, Accuracy: 78.58%\n",
      "Batch 176, Loss: 0.920928, Accuracy: 78.60%\n",
      "Batch 177, Loss: 1.032048, Accuracy: 78.56%\n",
      "Batch 178, Loss: 0.997796, Accuracy: 78.54%\n",
      "Batch 179, Loss: 0.913588, Accuracy: 78.58%\n",
      "Batch 180, Loss: 0.975962, Accuracy: 78.56%\n",
      "Batch 181, Loss: 0.958021, Accuracy: 78.56%\n",
      "Batch 182, Loss: 0.989517, Accuracy: 78.54%\n",
      "Batch 183, Loss: 0.888875, Accuracy: 78.57%\n",
      "Batch 184, Loss: 0.953041, Accuracy: 78.57%\n",
      "Batch 185, Loss: 0.940841, Accuracy: 78.57%\n",
      "Batch 186, Loss: 0.898919, Accuracy: 78.61%\n",
      "Batch 187, Loss: 0.986978, Accuracy: 78.59%\n",
      "Batch 188, Loss: 1.001774, Accuracy: 78.57%\n",
      "Batch 189, Loss: 0.884051, Accuracy: 78.60%\n",
      "Batch 190, Loss: 0.962685, Accuracy: 78.59%\n",
      "Batch 191, Loss: 0.932609, Accuracy: 78.60%\n",
      "Batch 192, Loss: 0.920271, Accuracy: 78.62%\n",
      "Batch 193, Loss: 0.954732, Accuracy: 78.62%\n",
      "Batch 194, Loss: 0.952267, Accuracy: 78.62%\n",
      "Batch 195, Loss: 0.893588, Accuracy: 78.65%\n",
      "Batch 196, Loss: 0.989468, Accuracy: 78.63%\n",
      "Batch 197, Loss: 0.975328, Accuracy: 78.62%\n",
      "Batch 198, Loss: 1.014552, Accuracy: 78.58%\n",
      "Batch 199, Loss: 0.980200, Accuracy: 78.57%\n",
      "Batch 200, Loss: 0.977635, Accuracy: 78.58%\n",
      "Batch 201, Loss: 0.930796, Accuracy: 78.59%\n",
      "Batch 202, Loss: 0.991475, Accuracy: 78.57%\n",
      "Batch 203, Loss: 0.926313, Accuracy: 78.59%\n",
      "Batch 204, Loss: 0.953241, Accuracy: 78.60%\n",
      "Batch 205, Loss: 0.985896, Accuracy: 78.59%\n",
      "Batch 206, Loss: 0.872198, Accuracy: 78.63%\n",
      "Batch 207, Loss: 1.019840, Accuracy: 78.62%\n",
      "Batch 208, Loss: 0.955871, Accuracy: 78.63%\n",
      "Batch 209, Loss: 0.956492, Accuracy: 78.64%\n",
      "Batch 210, Loss: 1.039038, Accuracy: 78.59%\n",
      "Batch 211, Loss: 0.959063, Accuracy: 78.58%\n",
      "Batch 212, Loss: 0.945667, Accuracy: 78.59%\n",
      "Batch 213, Loss: 0.921997, Accuracy: 78.61%\n",
      "Training - Epoch 94, Loss: 0.956060, Accuracy: 78.61%\n",
      "Validation Batch 1, Loss: 0.909275, Accuracy: 84.38%\n",
      "Validation Batch 2, Loss: 0.964218, Accuracy: 80.47%\n",
      "Validation Batch 3, Loss: 1.008211, Accuracy: 78.12%\n",
      "Validation Batch 4, Loss: 0.934705, Accuracy: 78.52%\n",
      "Validation Batch 5, Loss: 0.921885, Accuracy: 79.38%\n",
      "Validation Batch 6, Loss: 0.898279, Accuracy: 80.21%\n",
      "Validation Batch 7, Loss: 0.981545, Accuracy: 79.69%\n",
      "Validation Batch 8, Loss: 1.001755, Accuracy: 79.10%\n",
      "Validation Batch 9, Loss: 1.004523, Accuracy: 78.47%\n",
      "Validation Batch 10, Loss: 0.988948, Accuracy: 78.12%\n",
      "Validation Batch 11, Loss: 0.929709, Accuracy: 78.41%\n",
      "Validation Batch 12, Loss: 0.899798, Accuracy: 78.91%\n",
      "Validation Batch 13, Loss: 1.021160, Accuracy: 78.25%\n",
      "Validation Batch 14, Loss: 0.985787, Accuracy: 78.01%\n",
      "Validation Batch 15, Loss: 0.956486, Accuracy: 78.02%\n",
      "Validation Batch 16, Loss: 0.913211, Accuracy: 78.42%\n",
      "Validation Batch 17, Loss: 0.980149, Accuracy: 78.22%\n",
      "Validation Batch 18, Loss: 0.938995, Accuracy: 78.39%\n",
      "Validation Batch 19, Loss: 0.979663, Accuracy: 78.29%\n",
      "Validation Batch 20, Loss: 0.934306, Accuracy: 78.28%\n",
      "Validation Batch 21, Loss: 0.968437, Accuracy: 78.35%\n",
      "Validation Batch 22, Loss: 0.991911, Accuracy: 78.12%\n",
      "Validation Batch 23, Loss: 1.037913, Accuracy: 77.72%\n",
      "Validation Batch 24, Loss: 1.025398, Accuracy: 77.41%\n",
      "Validation Batch 25, Loss: 0.949569, Accuracy: 77.50%\n",
      "Validation Batch 26, Loss: 0.961026, Accuracy: 77.52%\n",
      "Validation Batch 27, Loss: 0.922781, Accuracy: 77.69%\n",
      "Validation - Epoch 94, Loss: 0.963320, Accuracy: 77.69%\n",
      "Patienceâ€”2\n",
      "Epoch 95\n",
      "Batch 1, Loss: 1.016876, Accuracy: 73.44%\n",
      "Batch 2, Loss: 0.996102, Accuracy: 74.22%\n",
      "Batch 3, Loss: 0.941809, Accuracy: 76.04%\n",
      "Batch 4, Loss: 0.884238, Accuracy: 78.52%\n",
      "Batch 5, Loss: 0.965947, Accuracy: 78.12%\n",
      "Batch 6, Loss: 0.941277, Accuracy: 78.39%\n",
      "Batch 7, Loss: 1.020823, Accuracy: 77.46%\n",
      "Batch 8, Loss: 0.951121, Accuracy: 77.73%\n",
      "Batch 9, Loss: 0.854423, Accuracy: 78.99%\n",
      "Batch 10, Loss: 0.917335, Accuracy: 79.38%\n",
      "Batch 11, Loss: 0.937005, Accuracy: 79.40%\n",
      "Batch 12, Loss: 0.899508, Accuracy: 79.82%\n",
      "Batch 13, Loss: 0.879850, Accuracy: 80.53%\n",
      "Batch 14, Loss: 1.030218, Accuracy: 79.80%\n",
      "Batch 15, Loss: 0.932560, Accuracy: 79.90%\n",
      "Batch 16, Loss: 0.924949, Accuracy: 80.08%\n",
      "Batch 17, Loss: 0.942620, Accuracy: 80.15%\n",
      "Batch 18, Loss: 0.925036, Accuracy: 80.30%\n",
      "Batch 19, Loss: 0.949785, Accuracy: 80.26%\n",
      "Batch 20, Loss: 0.960218, Accuracy: 80.23%\n",
      "Batch 21, Loss: 0.945954, Accuracy: 80.28%\n",
      "Batch 22, Loss: 0.906199, Accuracy: 80.47%\n",
      "Batch 23, Loss: 1.060335, Accuracy: 79.96%\n",
      "Batch 24, Loss: 0.980713, Accuracy: 79.75%\n",
      "Batch 25, Loss: 0.944030, Accuracy: 79.69%\n",
      "Batch 26, Loss: 0.928508, Accuracy: 79.69%\n",
      "Batch 27, Loss: 1.067132, Accuracy: 79.28%\n",
      "Batch 28, Loss: 0.974948, Accuracy: 79.13%\n",
      "Batch 29, Loss: 0.939419, Accuracy: 79.15%\n",
      "Batch 30, Loss: 0.934188, Accuracy: 79.22%\n",
      "Batch 31, Loss: 0.879071, Accuracy: 79.44%\n",
      "Batch 32, Loss: 1.034240, Accuracy: 79.05%\n",
      "Batch 33, Loss: 0.967973, Accuracy: 79.07%\n",
      "Batch 34, Loss: 0.889209, Accuracy: 79.32%\n",
      "Batch 35, Loss: 0.967330, Accuracy: 79.33%\n",
      "Batch 36, Loss: 0.930558, Accuracy: 79.43%\n",
      "Batch 37, Loss: 1.014689, Accuracy: 79.27%\n",
      "Batch 38, Loss: 1.014971, Accuracy: 79.03%\n",
      "Batch 39, Loss: 1.063434, Accuracy: 78.69%\n",
      "Batch 40, Loss: 0.931882, Accuracy: 78.75%\n",
      "Batch 41, Loss: 0.937981, Accuracy: 78.73%\n",
      "Batch 42, Loss: 0.988830, Accuracy: 78.61%\n",
      "Batch 43, Loss: 0.985458, Accuracy: 78.52%\n",
      "Batch 44, Loss: 0.948789, Accuracy: 78.59%\n",
      "Batch 45, Loss: 1.008357, Accuracy: 78.44%\n",
      "Batch 46, Loss: 0.902197, Accuracy: 78.53%\n",
      "Batch 47, Loss: 0.885049, Accuracy: 78.66%\n",
      "Batch 48, Loss: 0.884286, Accuracy: 78.81%\n",
      "Batch 49, Loss: 1.004025, Accuracy: 78.70%\n",
      "Batch 50, Loss: 0.913667, Accuracy: 78.78%\n",
      "Batch 51, Loss: 1.002580, Accuracy: 78.68%\n",
      "Batch 52, Loss: 1.037412, Accuracy: 78.52%\n",
      "Batch 53, Loss: 0.857361, Accuracy: 78.71%\n",
      "Batch 54, Loss: 1.001229, Accuracy: 78.62%\n",
      "Batch 55, Loss: 0.970994, Accuracy: 78.61%\n",
      "Batch 56, Loss: 0.887098, Accuracy: 78.71%\n",
      "Batch 57, Loss: 1.023748, Accuracy: 78.59%\n",
      "Batch 58, Loss: 0.993280, Accuracy: 78.48%\n",
      "Batch 59, Loss: 0.943131, Accuracy: 78.52%\n",
      "Batch 60, Loss: 0.889382, Accuracy: 78.65%\n",
      "Batch 61, Loss: 1.039897, Accuracy: 78.51%\n",
      "Batch 62, Loss: 0.938063, Accuracy: 78.53%\n",
      "Batch 63, Loss: 0.964076, Accuracy: 78.50%\n",
      "Batch 64, Loss: 0.970233, Accuracy: 78.47%\n",
      "Batch 65, Loss: 0.925534, Accuracy: 78.51%\n",
      "Batch 66, Loss: 0.908647, Accuracy: 78.57%\n",
      "Batch 67, Loss: 0.916266, Accuracy: 78.64%\n",
      "Batch 68, Loss: 0.923960, Accuracy: 78.70%\n",
      "Batch 69, Loss: 0.863648, Accuracy: 78.85%\n",
      "Batch 70, Loss: 0.941973, Accuracy: 78.88%\n",
      "Batch 71, Loss: 0.933820, Accuracy: 78.90%\n",
      "Batch 72, Loss: 0.990023, Accuracy: 78.84%\n",
      "Batch 73, Loss: 0.920175, Accuracy: 78.87%\n",
      "Batch 74, Loss: 0.917442, Accuracy: 78.93%\n",
      "Batch 75, Loss: 0.959354, Accuracy: 78.94%\n",
      "Batch 76, Loss: 0.951577, Accuracy: 78.95%\n",
      "Batch 77, Loss: 0.861937, Accuracy: 79.06%\n",
      "Batch 78, Loss: 0.978269, Accuracy: 79.03%\n",
      "Batch 79, Loss: 0.994442, Accuracy: 79.00%\n",
      "Batch 80, Loss: 0.859814, Accuracy: 79.10%\n",
      "Batch 81, Loss: 1.006619, Accuracy: 79.05%\n",
      "Batch 82, Loss: 0.926745, Accuracy: 79.08%\n",
      "Batch 83, Loss: 1.000612, Accuracy: 78.99%\n",
      "Batch 84, Loss: 1.042388, Accuracy: 78.91%\n",
      "Batch 85, Loss: 0.942491, Accuracy: 78.93%\n",
      "Batch 86, Loss: 0.946000, Accuracy: 78.94%\n",
      "Batch 87, Loss: 0.923412, Accuracy: 78.99%\n",
      "Batch 88, Loss: 0.907934, Accuracy: 79.03%\n",
      "Batch 89, Loss: 0.908209, Accuracy: 79.06%\n",
      "Batch 90, Loss: 0.906841, Accuracy: 79.11%\n",
      "Batch 91, Loss: 0.984942, Accuracy: 79.09%\n",
      "Batch 92, Loss: 0.980983, Accuracy: 79.04%\n",
      "Batch 93, Loss: 0.965434, Accuracy: 79.03%\n",
      "Batch 94, Loss: 0.996848, Accuracy: 78.99%\n",
      "Batch 95, Loss: 0.876238, Accuracy: 79.08%\n",
      "Batch 96, Loss: 0.927181, Accuracy: 79.12%\n",
      "Batch 97, Loss: 0.938527, Accuracy: 79.12%\n",
      "Batch 98, Loss: 0.932307, Accuracy: 79.15%\n",
      "Batch 99, Loss: 0.939898, Accuracy: 79.15%\n",
      "Batch 100, Loss: 0.898334, Accuracy: 79.20%\n",
      "Batch 101, Loss: 0.926478, Accuracy: 79.24%\n",
      "Batch 102, Loss: 0.948778, Accuracy: 79.24%\n",
      "Batch 103, Loss: 0.934612, Accuracy: 79.25%\n",
      "Batch 104, Loss: 0.959972, Accuracy: 79.24%\n",
      "Batch 105, Loss: 0.930982, Accuracy: 79.26%\n",
      "Batch 106, Loss: 0.963740, Accuracy: 79.26%\n",
      "Batch 107, Loss: 0.914832, Accuracy: 79.29%\n",
      "Batch 108, Loss: 0.895294, Accuracy: 79.35%\n",
      "Batch 109, Loss: 0.960153, Accuracy: 79.36%\n",
      "Batch 110, Loss: 0.977939, Accuracy: 79.32%\n",
      "Batch 111, Loss: 0.938130, Accuracy: 79.34%\n",
      "Batch 112, Loss: 0.984048, Accuracy: 79.28%\n",
      "Batch 113, Loss: 1.050081, Accuracy: 79.19%\n",
      "Batch 114, Loss: 0.969071, Accuracy: 79.19%\n",
      "Batch 115, Loss: 0.945197, Accuracy: 79.21%\n",
      "Batch 116, Loss: 0.899476, Accuracy: 79.27%\n",
      "Batch 117, Loss: 0.887741, Accuracy: 79.33%\n",
      "Batch 118, Loss: 1.034700, Accuracy: 79.26%\n",
      "Batch 119, Loss: 0.961450, Accuracy: 79.24%\n",
      "Batch 120, Loss: 0.995741, Accuracy: 79.19%\n",
      "Batch 121, Loss: 1.025231, Accuracy: 79.13%\n",
      "Batch 122, Loss: 0.952884, Accuracy: 79.15%\n",
      "Batch 123, Loss: 0.951900, Accuracy: 79.15%\n",
      "Batch 124, Loss: 1.044123, Accuracy: 79.08%\n",
      "Batch 125, Loss: 0.994206, Accuracy: 79.05%\n",
      "Batch 126, Loss: 0.907639, Accuracy: 79.09%\n",
      "Batch 127, Loss: 0.983694, Accuracy: 79.07%\n",
      "Batch 128, Loss: 0.999384, Accuracy: 79.05%\n",
      "Batch 129, Loss: 0.880685, Accuracy: 79.11%\n",
      "Batch 130, Loss: 0.937660, Accuracy: 79.11%\n",
      "Batch 131, Loss: 1.019896, Accuracy: 79.04%\n",
      "Batch 132, Loss: 0.932048, Accuracy: 79.07%\n",
      "Batch 133, Loss: 0.883911, Accuracy: 79.12%\n",
      "Batch 134, Loss: 1.051201, Accuracy: 79.05%\n",
      "Batch 135, Loss: 1.032094, Accuracy: 78.98%\n",
      "Batch 136, Loss: 0.911866, Accuracy: 79.02%\n",
      "Batch 137, Loss: 0.945882, Accuracy: 79.03%\n",
      "Batch 138, Loss: 0.923399, Accuracy: 79.04%\n",
      "Batch 139, Loss: 0.994475, Accuracy: 79.01%\n",
      "Batch 140, Loss: 1.076748, Accuracy: 78.92%\n",
      "Batch 141, Loss: 0.976543, Accuracy: 78.89%\n",
      "Batch 142, Loss: 0.875346, Accuracy: 78.95%\n",
      "Batch 143, Loss: 0.928924, Accuracy: 78.97%\n",
      "Batch 144, Loss: 0.884921, Accuracy: 79.01%\n",
      "Batch 145, Loss: 0.963234, Accuracy: 79.00%\n",
      "Batch 146, Loss: 0.956720, Accuracy: 78.99%\n",
      "Batch 147, Loss: 1.020464, Accuracy: 78.94%\n",
      "Batch 148, Loss: 0.920596, Accuracy: 78.96%\n",
      "Batch 149, Loss: 0.975068, Accuracy: 78.95%\n",
      "Batch 150, Loss: 0.924978, Accuracy: 78.97%\n",
      "Batch 151, Loss: 0.901548, Accuracy: 79.00%\n",
      "Batch 152, Loss: 0.910692, Accuracy: 79.03%\n",
      "Batch 153, Loss: 0.883846, Accuracy: 79.08%\n",
      "Batch 154, Loss: 0.934627, Accuracy: 79.09%\n",
      "Batch 155, Loss: 0.989558, Accuracy: 79.06%\n",
      "Batch 156, Loss: 1.018695, Accuracy: 79.03%\n",
      "Batch 157, Loss: 1.020129, Accuracy: 78.98%\n",
      "Batch 158, Loss: 0.933416, Accuracy: 78.99%\n",
      "Batch 159, Loss: 0.914150, Accuracy: 79.01%\n",
      "Batch 160, Loss: 0.915117, Accuracy: 79.02%\n",
      "Batch 161, Loss: 0.907656, Accuracy: 79.06%\n",
      "Batch 162, Loss: 0.931014, Accuracy: 79.07%\n",
      "Batch 163, Loss: 0.958538, Accuracy: 79.06%\n",
      "Batch 164, Loss: 1.071953, Accuracy: 78.98%\n",
      "Batch 165, Loss: 1.017997, Accuracy: 78.94%\n",
      "Batch 166, Loss: 0.957325, Accuracy: 78.93%\n",
      "Batch 167, Loss: 0.967337, Accuracy: 78.93%\n",
      "Batch 168, Loss: 0.941027, Accuracy: 78.94%\n",
      "Batch 169, Loss: 1.011135, Accuracy: 78.90%\n",
      "Batch 170, Loss: 0.965265, Accuracy: 78.90%\n",
      "Batch 171, Loss: 0.969888, Accuracy: 78.88%\n",
      "Batch 172, Loss: 0.994099, Accuracy: 78.85%\n",
      "Batch 173, Loss: 0.984789, Accuracy: 78.82%\n",
      "Batch 174, Loss: 0.931333, Accuracy: 78.83%\n",
      "Batch 175, Loss: 0.981556, Accuracy: 78.81%\n",
      "Batch 176, Loss: 0.963847, Accuracy: 78.80%\n",
      "Batch 177, Loss: 0.869546, Accuracy: 78.86%\n",
      "Batch 178, Loss: 0.967178, Accuracy: 78.85%\n",
      "Batch 179, Loss: 0.935850, Accuracy: 78.87%\n",
      "Batch 180, Loss: 0.877204, Accuracy: 78.91%\n",
      "Batch 181, Loss: 0.949936, Accuracy: 78.91%\n",
      "Batch 182, Loss: 0.952484, Accuracy: 78.91%\n",
      "Batch 183, Loss: 0.923280, Accuracy: 78.93%\n",
      "Batch 184, Loss: 0.985050, Accuracy: 78.91%\n",
      "Batch 185, Loss: 0.944979, Accuracy: 78.92%\n",
      "Batch 186, Loss: 0.925736, Accuracy: 78.93%\n",
      "Batch 187, Loss: 0.922247, Accuracy: 78.94%\n",
      "Batch 188, Loss: 1.004158, Accuracy: 78.91%\n",
      "Batch 189, Loss: 0.922316, Accuracy: 78.94%\n",
      "Batch 190, Loss: 0.983997, Accuracy: 78.91%\n",
      "Batch 191, Loss: 0.986473, Accuracy: 78.90%\n",
      "Batch 192, Loss: 0.929879, Accuracy: 78.91%\n",
      "Batch 193, Loss: 1.027153, Accuracy: 78.87%\n",
      "Batch 194, Loss: 1.011939, Accuracy: 78.83%\n",
      "Batch 195, Loss: 1.061789, Accuracy: 78.77%\n",
      "Batch 196, Loss: 0.917459, Accuracy: 78.78%\n",
      "Batch 197, Loss: 1.035773, Accuracy: 78.74%\n",
      "Batch 198, Loss: 0.997410, Accuracy: 78.72%\n",
      "Batch 199, Loss: 0.948771, Accuracy: 78.73%\n",
      "Batch 200, Loss: 1.103276, Accuracy: 78.66%\n",
      "Batch 201, Loss: 1.060873, Accuracy: 78.61%\n",
      "Batch 202, Loss: 0.934136, Accuracy: 78.63%\n",
      "Batch 203, Loss: 1.040403, Accuracy: 78.58%\n",
      "Batch 204, Loss: 0.979331, Accuracy: 78.57%\n",
      "Batch 205, Loss: 0.995361, Accuracy: 78.55%\n",
      "Batch 206, Loss: 1.030735, Accuracy: 78.50%\n",
      "Batch 207, Loss: 0.883382, Accuracy: 78.53%\n",
      "Batch 208, Loss: 0.921411, Accuracy: 78.55%\n",
      "Batch 209, Loss: 0.892058, Accuracy: 78.58%\n",
      "Batch 210, Loss: 0.890280, Accuracy: 78.62%\n",
      "Batch 211, Loss: 0.950006, Accuracy: 78.62%\n",
      "Batch 212, Loss: 0.983542, Accuracy: 78.60%\n",
      "Batch 213, Loss: 0.942843, Accuracy: 78.61%\n",
      "Training - Epoch 95, Loss: 0.956063, Accuracy: 78.61%\n",
      "Validation Batch 1, Loss: 0.909490, Accuracy: 84.38%\n",
      "Validation Batch 2, Loss: 0.961531, Accuracy: 82.03%\n",
      "Validation Batch 3, Loss: 1.009448, Accuracy: 79.17%\n",
      "Validation Batch 4, Loss: 0.928998, Accuracy: 79.69%\n",
      "Validation Batch 5, Loss: 0.923898, Accuracy: 80.00%\n",
      "Validation Batch 6, Loss: 0.900103, Accuracy: 80.47%\n",
      "Validation Batch 7, Loss: 0.983349, Accuracy: 79.69%\n",
      "Validation Batch 8, Loss: 1.013248, Accuracy: 78.91%\n",
      "Validation Batch 9, Loss: 1.014634, Accuracy: 78.12%\n",
      "Validation Batch 10, Loss: 0.991193, Accuracy: 77.66%\n",
      "Validation Batch 11, Loss: 0.931722, Accuracy: 77.98%\n",
      "Validation Batch 12, Loss: 0.902675, Accuracy: 78.52%\n",
      "Validation Batch 13, Loss: 1.020733, Accuracy: 78.00%\n",
      "Validation Batch 14, Loss: 0.990086, Accuracy: 77.68%\n",
      "Validation Batch 15, Loss: 0.944592, Accuracy: 77.92%\n",
      "Validation Batch 16, Loss: 0.916419, Accuracy: 78.32%\n",
      "Validation Batch 17, Loss: 0.993253, Accuracy: 78.12%\n",
      "Validation Batch 18, Loss: 0.943668, Accuracy: 78.21%\n",
      "Validation Batch 19, Loss: 0.987041, Accuracy: 78.12%\n",
      "Validation Batch 20, Loss: 0.925480, Accuracy: 78.28%\n",
      "Validation Batch 21, Loss: 0.969906, Accuracy: 78.20%\n",
      "Validation Batch 22, Loss: 0.996241, Accuracy: 77.98%\n",
      "Validation Batch 23, Loss: 1.042539, Accuracy: 77.45%\n",
      "Validation Batch 24, Loss: 1.028753, Accuracy: 77.15%\n",
      "Validation Batch 25, Loss: 0.947971, Accuracy: 77.25%\n",
      "Validation Batch 26, Loss: 0.962508, Accuracy: 77.28%\n",
      "Validation Batch 27, Loss: 0.920860, Accuracy: 77.45%\n",
      "Validation - Epoch 95, Loss: 0.965198, Accuracy: 77.45%\n",
      "Patienceâ€”3\n",
      "Epoch 96\n",
      "Batch 1, Loss: 1.056517, Accuracy: 68.75%\n",
      "Batch 2, Loss: 0.929412, Accuracy: 75.00%\n",
      "Batch 3, Loss: 0.945373, Accuracy: 77.60%\n",
      "Batch 4, Loss: 0.937808, Accuracy: 78.12%\n",
      "Batch 5, Loss: 0.968962, Accuracy: 78.12%\n",
      "Batch 6, Loss: 0.951591, Accuracy: 78.39%\n",
      "Batch 7, Loss: 1.045191, Accuracy: 77.01%\n",
      "Batch 8, Loss: 1.016117, Accuracy: 76.56%\n",
      "Batch 9, Loss: 0.947256, Accuracy: 76.91%\n",
      "Batch 10, Loss: 0.935636, Accuracy: 77.19%\n",
      "Batch 11, Loss: 0.946001, Accuracy: 77.27%\n",
      "Batch 12, Loss: 0.988793, Accuracy: 76.95%\n",
      "Batch 13, Loss: 0.982659, Accuracy: 76.80%\n",
      "Batch 14, Loss: 0.982610, Accuracy: 76.67%\n",
      "Batch 15, Loss: 0.911226, Accuracy: 77.08%\n",
      "Batch 16, Loss: 0.948137, Accuracy: 77.25%\n",
      "Batch 17, Loss: 0.993581, Accuracy: 77.11%\n",
      "Batch 18, Loss: 0.938230, Accuracy: 77.26%\n",
      "Batch 19, Loss: 0.873840, Accuracy: 77.80%\n",
      "Batch 20, Loss: 0.936630, Accuracy: 77.97%\n",
      "Batch 21, Loss: 0.958806, Accuracy: 78.05%\n",
      "Batch 22, Loss: 0.915267, Accuracy: 78.27%\n",
      "Batch 23, Loss: 0.928039, Accuracy: 78.46%\n",
      "Batch 24, Loss: 0.913549, Accuracy: 78.65%\n",
      "Batch 25, Loss: 1.040949, Accuracy: 78.25%\n",
      "Batch 26, Loss: 0.933504, Accuracy: 78.37%\n",
      "Batch 27, Loss: 0.911837, Accuracy: 78.59%\n",
      "Batch 28, Loss: 0.898839, Accuracy: 78.79%\n",
      "Batch 29, Loss: 0.975168, Accuracy: 78.77%\n",
      "Batch 30, Loss: 1.032022, Accuracy: 78.54%\n",
      "Batch 31, Loss: 0.950197, Accuracy: 78.58%\n",
      "Batch 32, Loss: 0.948908, Accuracy: 78.56%\n",
      "Batch 33, Loss: 0.952878, Accuracy: 78.55%\n",
      "Batch 34, Loss: 0.979441, Accuracy: 78.45%\n",
      "Batch 35, Loss: 1.046339, Accuracy: 78.17%\n",
      "Batch 36, Loss: 0.922635, Accuracy: 78.26%\n",
      "Batch 37, Loss: 0.974223, Accuracy: 78.21%\n",
      "Batch 38, Loss: 0.966061, Accuracy: 78.17%\n",
      "Batch 39, Loss: 0.982421, Accuracy: 78.08%\n",
      "Batch 40, Loss: 0.964986, Accuracy: 78.09%\n",
      "Batch 41, Loss: 0.978829, Accuracy: 78.01%\n",
      "Batch 42, Loss: 0.925229, Accuracy: 78.01%\n",
      "Batch 43, Loss: 1.053990, Accuracy: 77.76%\n",
      "Batch 44, Loss: 0.977819, Accuracy: 77.73%\n",
      "Batch 45, Loss: 0.991238, Accuracy: 77.64%\n",
      "Batch 46, Loss: 1.041871, Accuracy: 77.51%\n",
      "Batch 47, Loss: 0.948643, Accuracy: 77.53%\n",
      "Batch 48, Loss: 0.912686, Accuracy: 77.64%\n",
      "Batch 49, Loss: 0.933837, Accuracy: 77.68%\n",
      "Batch 50, Loss: 1.021827, Accuracy: 77.53%\n",
      "Batch 51, Loss: 0.924491, Accuracy: 77.63%\n",
      "Batch 52, Loss: 0.914834, Accuracy: 77.70%\n",
      "Batch 53, Loss: 1.026173, Accuracy: 77.56%\n",
      "Batch 54, Loss: 0.869295, Accuracy: 77.78%\n",
      "Batch 55, Loss: 0.884559, Accuracy: 77.93%\n",
      "Batch 56, Loss: 0.908234, Accuracy: 78.01%\n",
      "Batch 57, Loss: 0.874672, Accuracy: 78.15%\n",
      "Batch 58, Loss: 0.956488, Accuracy: 78.15%\n",
      "Batch 59, Loss: 0.956301, Accuracy: 78.20%\n",
      "Batch 60, Loss: 0.958289, Accuracy: 78.23%\n",
      "Batch 61, Loss: 0.896568, Accuracy: 78.36%\n",
      "Batch 62, Loss: 0.923359, Accuracy: 78.40%\n",
      "Batch 63, Loss: 0.913481, Accuracy: 78.50%\n",
      "Batch 64, Loss: 0.943410, Accuracy: 78.49%\n",
      "Batch 65, Loss: 0.980201, Accuracy: 78.46%\n",
      "Batch 66, Loss: 0.974030, Accuracy: 78.41%\n",
      "Batch 67, Loss: 0.951046, Accuracy: 78.40%\n",
      "Batch 68, Loss: 0.859870, Accuracy: 78.56%\n",
      "Batch 69, Loss: 0.954463, Accuracy: 78.58%\n",
      "Batch 70, Loss: 0.883037, Accuracy: 78.71%\n",
      "Batch 71, Loss: 1.018922, Accuracy: 78.61%\n",
      "Batch 72, Loss: 0.942260, Accuracy: 78.62%\n",
      "Batch 73, Loss: 0.870614, Accuracy: 78.77%\n",
      "Batch 74, Loss: 0.946159, Accuracy: 78.74%\n",
      "Batch 75, Loss: 0.978861, Accuracy: 78.69%\n",
      "Batch 76, Loss: 0.951423, Accuracy: 78.70%\n",
      "Batch 77, Loss: 1.030960, Accuracy: 78.57%\n",
      "Batch 78, Loss: 0.935350, Accuracy: 78.59%\n",
      "Batch 79, Loss: 0.920878, Accuracy: 78.62%\n",
      "Batch 80, Loss: 0.972508, Accuracy: 78.61%\n",
      "Batch 81, Loss: 0.965408, Accuracy: 78.59%\n",
      "Batch 82, Loss: 0.930853, Accuracy: 78.62%\n",
      "Batch 83, Loss: 0.918552, Accuracy: 78.65%\n",
      "Batch 84, Loss: 0.977521, Accuracy: 78.63%\n",
      "Batch 85, Loss: 0.834220, Accuracy: 78.77%\n",
      "Batch 86, Loss: 0.989266, Accuracy: 78.72%\n",
      "Batch 87, Loss: 0.992769, Accuracy: 78.68%\n",
      "Batch 88, Loss: 0.888390, Accuracy: 78.76%\n",
      "Batch 89, Loss: 0.895775, Accuracy: 78.83%\n",
      "Batch 90, Loss: 1.031028, Accuracy: 78.75%\n",
      "Batch 91, Loss: 0.902012, Accuracy: 78.83%\n",
      "Batch 92, Loss: 0.969111, Accuracy: 78.84%\n",
      "Batch 93, Loss: 0.948697, Accuracy: 78.85%\n",
      "Batch 94, Loss: 0.973373, Accuracy: 78.82%\n",
      "Batch 95, Loss: 0.996456, Accuracy: 78.80%\n",
      "Batch 96, Loss: 1.032232, Accuracy: 78.73%\n",
      "Batch 97, Loss: 0.883313, Accuracy: 78.82%\n",
      "Batch 98, Loss: 0.863889, Accuracy: 78.92%\n",
      "Batch 99, Loss: 0.938222, Accuracy: 78.95%\n",
      "Batch 100, Loss: 0.919832, Accuracy: 78.98%\n",
      "Batch 101, Loss: 1.003800, Accuracy: 78.93%\n",
      "Batch 102, Loss: 0.965362, Accuracy: 78.94%\n",
      "Batch 103, Loss: 0.936980, Accuracy: 78.97%\n",
      "Batch 104, Loss: 0.983122, Accuracy: 78.95%\n",
      "Batch 105, Loss: 0.978785, Accuracy: 78.93%\n",
      "Batch 106, Loss: 0.943640, Accuracy: 78.94%\n",
      "Batch 107, Loss: 1.000695, Accuracy: 78.90%\n",
      "Batch 108, Loss: 0.993454, Accuracy: 78.86%\n",
      "Batch 109, Loss: 0.951006, Accuracy: 78.87%\n",
      "Batch 110, Loss: 0.862592, Accuracy: 78.95%\n",
      "Batch 111, Loss: 0.910986, Accuracy: 79.00%\n",
      "Batch 112, Loss: 0.949959, Accuracy: 78.99%\n",
      "Batch 113, Loss: 1.013456, Accuracy: 78.94%\n",
      "Batch 114, Loss: 0.897114, Accuracy: 78.99%\n",
      "Batch 115, Loss: 0.970429, Accuracy: 78.95%\n",
      "Batch 116, Loss: 0.971449, Accuracy: 78.95%\n",
      "Batch 117, Loss: 0.974156, Accuracy: 78.93%\n",
      "Batch 118, Loss: 1.039380, Accuracy: 78.85%\n",
      "Batch 119, Loss: 0.936407, Accuracy: 78.89%\n",
      "Batch 120, Loss: 1.033098, Accuracy: 78.80%\n",
      "Batch 121, Loss: 0.948458, Accuracy: 78.80%\n",
      "Batch 122, Loss: 0.965429, Accuracy: 78.79%\n",
      "Batch 123, Loss: 0.997426, Accuracy: 78.76%\n",
      "Batch 124, Loss: 0.944269, Accuracy: 78.78%\n",
      "Batch 125, Loss: 0.910983, Accuracy: 78.80%\n",
      "Batch 126, Loss: 0.962944, Accuracy: 78.81%\n",
      "Batch 127, Loss: 0.951819, Accuracy: 78.81%\n",
      "Batch 128, Loss: 0.947421, Accuracy: 78.81%\n",
      "Batch 129, Loss: 0.935533, Accuracy: 78.83%\n",
      "Batch 130, Loss: 0.975669, Accuracy: 78.80%\n",
      "Batch 131, Loss: 0.904623, Accuracy: 78.84%\n",
      "Batch 132, Loss: 0.980603, Accuracy: 78.81%\n",
      "Batch 133, Loss: 0.972802, Accuracy: 78.79%\n",
      "Batch 134, Loss: 1.002536, Accuracy: 78.75%\n",
      "Batch 135, Loss: 1.003580, Accuracy: 78.70%\n",
      "Batch 136, Loss: 0.953733, Accuracy: 78.71%\n",
      "Batch 137, Loss: 0.970595, Accuracy: 78.70%\n",
      "Batch 138, Loss: 1.050839, Accuracy: 78.62%\n",
      "Batch 139, Loss: 0.928929, Accuracy: 78.64%\n",
      "Batch 140, Loss: 1.042840, Accuracy: 78.56%\n",
      "Batch 141, Loss: 0.967967, Accuracy: 78.55%\n",
      "Batch 142, Loss: 0.905527, Accuracy: 78.59%\n",
      "Batch 143, Loss: 1.050886, Accuracy: 78.52%\n",
      "Batch 144, Loss: 0.930350, Accuracy: 78.54%\n",
      "Batch 145, Loss: 0.986889, Accuracy: 78.52%\n",
      "Batch 146, Loss: 0.990669, Accuracy: 78.49%\n",
      "Batch 147, Loss: 0.974707, Accuracy: 78.48%\n",
      "Batch 148, Loss: 0.928251, Accuracy: 78.48%\n",
      "Batch 149, Loss: 0.913818, Accuracy: 78.51%\n",
      "Batch 150, Loss: 0.973622, Accuracy: 78.50%\n",
      "Batch 151, Loss: 0.859401, Accuracy: 78.57%\n",
      "Batch 152, Loss: 0.925117, Accuracy: 78.59%\n",
      "Batch 153, Loss: 0.992580, Accuracy: 78.57%\n",
      "Batch 154, Loss: 0.896038, Accuracy: 78.61%\n",
      "Batch 155, Loss: 0.963849, Accuracy: 78.61%\n",
      "Batch 156, Loss: 0.958253, Accuracy: 78.61%\n",
      "Batch 157, Loss: 1.006205, Accuracy: 78.57%\n",
      "Batch 158, Loss: 0.903039, Accuracy: 78.59%\n",
      "Batch 159, Loss: 0.915494, Accuracy: 78.62%\n",
      "Batch 160, Loss: 0.973995, Accuracy: 78.60%\n",
      "Batch 161, Loss: 0.968932, Accuracy: 78.60%\n",
      "Batch 162, Loss: 0.966916, Accuracy: 78.59%\n",
      "Batch 163, Loss: 0.861718, Accuracy: 78.65%\n",
      "Batch 164, Loss: 0.924152, Accuracy: 78.66%\n",
      "Batch 165, Loss: 0.938878, Accuracy: 78.67%\n",
      "Batch 166, Loss: 0.949993, Accuracy: 78.68%\n",
      "Batch 167, Loss: 0.880175, Accuracy: 78.72%\n",
      "Batch 168, Loss: 1.007733, Accuracy: 78.69%\n",
      "Batch 169, Loss: 0.952868, Accuracy: 78.69%\n",
      "Batch 170, Loss: 0.831784, Accuracy: 78.77%\n",
      "Batch 171, Loss: 0.884784, Accuracy: 78.81%\n",
      "Batch 172, Loss: 1.000204, Accuracy: 78.78%\n",
      "Batch 173, Loss: 1.025210, Accuracy: 78.73%\n",
      "Batch 174, Loss: 0.919878, Accuracy: 78.75%\n",
      "Batch 175, Loss: 1.007861, Accuracy: 78.74%\n",
      "Batch 176, Loss: 0.945771, Accuracy: 78.76%\n",
      "Batch 177, Loss: 0.933580, Accuracy: 78.77%\n",
      "Batch 178, Loss: 0.984847, Accuracy: 78.75%\n",
      "Batch 179, Loss: 0.980889, Accuracy: 78.73%\n",
      "Batch 180, Loss: 0.952763, Accuracy: 78.73%\n",
      "Batch 181, Loss: 0.955956, Accuracy: 78.73%\n",
      "Batch 182, Loss: 0.990593, Accuracy: 78.71%\n",
      "Batch 183, Loss: 0.965272, Accuracy: 78.71%\n",
      "Batch 184, Loss: 1.026119, Accuracy: 78.66%\n",
      "Batch 185, Loss: 0.965644, Accuracy: 78.66%\n",
      "Batch 186, Loss: 0.956714, Accuracy: 78.66%\n",
      "Batch 187, Loss: 0.960592, Accuracy: 78.67%\n",
      "Batch 188, Loss: 0.851296, Accuracy: 78.72%\n",
      "Batch 189, Loss: 1.022010, Accuracy: 78.69%\n",
      "Batch 190, Loss: 1.007697, Accuracy: 78.66%\n",
      "Batch 191, Loss: 0.897432, Accuracy: 78.69%\n",
      "Batch 192, Loss: 0.949838, Accuracy: 78.69%\n",
      "Batch 193, Loss: 0.922466, Accuracy: 78.72%\n",
      "Batch 194, Loss: 0.910812, Accuracy: 78.75%\n",
      "Batch 195, Loss: 0.930847, Accuracy: 78.75%\n",
      "Batch 196, Loss: 0.970754, Accuracy: 78.74%\n",
      "Batch 197, Loss: 0.928496, Accuracy: 78.75%\n",
      "Batch 198, Loss: 0.943578, Accuracy: 78.76%\n",
      "Batch 199, Loss: 0.886860, Accuracy: 78.79%\n",
      "Batch 200, Loss: 0.931478, Accuracy: 78.80%\n",
      "Batch 201, Loss: 1.004433, Accuracy: 78.77%\n",
      "Batch 202, Loss: 0.938963, Accuracy: 78.78%\n",
      "Batch 203, Loss: 0.924538, Accuracy: 78.80%\n",
      "Batch 204, Loss: 0.899095, Accuracy: 78.84%\n",
      "Batch 205, Loss: 1.008614, Accuracy: 78.80%\n",
      "Batch 206, Loss: 0.976099, Accuracy: 78.79%\n",
      "Batch 207, Loss: 1.005265, Accuracy: 78.77%\n",
      "Batch 208, Loss: 0.865453, Accuracy: 78.82%\n",
      "Batch 209, Loss: 0.939562, Accuracy: 78.81%\n",
      "Batch 210, Loss: 0.943570, Accuracy: 78.82%\n",
      "Batch 211, Loss: 0.996321, Accuracy: 78.81%\n",
      "Batch 212, Loss: 1.021968, Accuracy: 78.79%\n",
      "Batch 213, Loss: 0.992447, Accuracy: 78.77%\n",
      "Training - Epoch 96, Loss: 0.954378, Accuracy: 78.77%\n",
      "Validation Batch 1, Loss: 0.891310, Accuracy: 85.94%\n",
      "Validation Batch 2, Loss: 0.901979, Accuracy: 85.94%\n",
      "Validation Batch 3, Loss: 0.993984, Accuracy: 81.77%\n",
      "Validation Batch 4, Loss: 0.933081, Accuracy: 81.64%\n",
      "Validation Batch 5, Loss: 0.902725, Accuracy: 82.19%\n",
      "Validation Batch 6, Loss: 0.832444, Accuracy: 83.85%\n",
      "Validation Batch 7, Loss: 0.934800, Accuracy: 83.26%\n",
      "Validation Batch 8, Loss: 0.936980, Accuracy: 83.20%\n",
      "Validation Batch 9, Loss: 0.954740, Accuracy: 82.64%\n",
      "Validation Batch 10, Loss: 0.953485, Accuracy: 82.19%\n",
      "Validation Batch 11, Loss: 0.901859, Accuracy: 82.39%\n",
      "Validation Batch 12, Loss: 0.873809, Accuracy: 82.68%\n",
      "Validation Batch 13, Loss: 0.947597, Accuracy: 82.57%\n",
      "Validation Batch 14, Loss: 0.937040, Accuracy: 82.59%\n",
      "Validation Batch 15, Loss: 0.948656, Accuracy: 82.29%\n",
      "Validation Batch 16, Loss: 0.897121, Accuracy: 82.52%\n",
      "Validation Batch 17, Loss: 0.934156, Accuracy: 82.44%\n",
      "Validation Batch 18, Loss: 0.924844, Accuracy: 82.47%\n",
      "Validation Batch 19, Loss: 0.952118, Accuracy: 82.24%\n",
      "Validation Batch 20, Loss: 0.893574, Accuracy: 82.27%\n",
      "Validation Batch 21, Loss: 0.940033, Accuracy: 82.14%\n",
      "Validation Batch 22, Loss: 0.924978, Accuracy: 82.10%\n",
      "Validation Batch 23, Loss: 0.976271, Accuracy: 81.79%\n",
      "Validation Batch 24, Loss: 0.999578, Accuracy: 81.45%\n",
      "Validation Batch 25, Loss: 0.912789, Accuracy: 81.50%\n",
      "Validation Batch 26, Loss: 0.943056, Accuracy: 81.43%\n",
      "Validation Batch 27, Loss: 0.870742, Accuracy: 81.56%\n",
      "Validation - Epoch 96, Loss: 0.926435, Accuracy: 81.56%\n",
      "Patienceâ€”4\n",
      "Epoch 97\n",
      "Batch 1, Loss: 0.905356, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.956877, Accuracy: 79.69%\n",
      "Batch 3, Loss: 0.931414, Accuracy: 80.21%\n",
      "Batch 4, Loss: 1.050327, Accuracy: 76.95%\n",
      "Batch 5, Loss: 0.942762, Accuracy: 77.81%\n",
      "Batch 6, Loss: 1.034154, Accuracy: 76.82%\n",
      "Batch 7, Loss: 0.949401, Accuracy: 77.23%\n",
      "Batch 8, Loss: 0.973771, Accuracy: 76.76%\n",
      "Batch 9, Loss: 0.948075, Accuracy: 77.26%\n",
      "Batch 10, Loss: 0.956450, Accuracy: 77.19%\n",
      "Batch 11, Loss: 0.976631, Accuracy: 77.27%\n",
      "Batch 12, Loss: 0.904050, Accuracy: 77.86%\n",
      "Batch 13, Loss: 0.913925, Accuracy: 78.37%\n",
      "Batch 14, Loss: 0.881467, Accuracy: 78.91%\n",
      "Batch 15, Loss: 0.935290, Accuracy: 79.06%\n",
      "Batch 16, Loss: 1.011606, Accuracy: 78.71%\n",
      "Batch 17, Loss: 0.954371, Accuracy: 78.68%\n",
      "Batch 18, Loss: 0.913031, Accuracy: 78.91%\n",
      "Batch 19, Loss: 0.877314, Accuracy: 79.36%\n",
      "Batch 20, Loss: 1.028318, Accuracy: 78.98%\n",
      "Batch 21, Loss: 0.928823, Accuracy: 79.09%\n",
      "Batch 22, Loss: 0.968064, Accuracy: 79.05%\n",
      "Batch 23, Loss: 0.979269, Accuracy: 78.87%\n",
      "Batch 24, Loss: 0.924847, Accuracy: 79.10%\n",
      "Batch 25, Loss: 0.897767, Accuracy: 79.31%\n",
      "Batch 26, Loss: 0.950821, Accuracy: 79.33%\n",
      "Batch 27, Loss: 0.922920, Accuracy: 79.40%\n",
      "Batch 28, Loss: 1.015732, Accuracy: 79.13%\n",
      "Batch 29, Loss: 0.903695, Accuracy: 79.31%\n",
      "Batch 30, Loss: 0.941619, Accuracy: 79.43%\n",
      "Batch 31, Loss: 0.998955, Accuracy: 79.23%\n",
      "Batch 32, Loss: 0.903148, Accuracy: 79.44%\n",
      "Batch 33, Loss: 0.895000, Accuracy: 79.59%\n",
      "Batch 34, Loss: 0.881067, Accuracy: 79.78%\n",
      "Batch 35, Loss: 0.928262, Accuracy: 79.78%\n",
      "Batch 36, Loss: 0.954638, Accuracy: 79.77%\n",
      "Batch 37, Loss: 0.996010, Accuracy: 79.65%\n",
      "Batch 38, Loss: 0.912873, Accuracy: 79.77%\n",
      "Batch 39, Loss: 0.936420, Accuracy: 79.81%\n",
      "Batch 40, Loss: 1.065069, Accuracy: 79.49%\n",
      "Batch 41, Loss: 1.014446, Accuracy: 79.31%\n",
      "Batch 42, Loss: 0.863292, Accuracy: 79.54%\n",
      "Batch 43, Loss: 0.954530, Accuracy: 79.51%\n",
      "Batch 44, Loss: 0.917684, Accuracy: 79.55%\n",
      "Batch 45, Loss: 0.938681, Accuracy: 79.58%\n",
      "Batch 46, Loss: 0.919723, Accuracy: 79.69%\n",
      "Batch 47, Loss: 1.058901, Accuracy: 79.39%\n",
      "Batch 48, Loss: 1.054514, Accuracy: 79.17%\n",
      "Batch 49, Loss: 0.952958, Accuracy: 79.15%\n",
      "Batch 50, Loss: 0.981022, Accuracy: 79.09%\n",
      "Batch 51, Loss: 0.876778, Accuracy: 79.23%\n",
      "Batch 52, Loss: 0.905364, Accuracy: 79.36%\n",
      "Batch 53, Loss: 0.898588, Accuracy: 79.45%\n",
      "Batch 54, Loss: 1.029018, Accuracy: 79.28%\n",
      "Batch 55, Loss: 0.872912, Accuracy: 79.43%\n",
      "Batch 56, Loss: 0.917018, Accuracy: 79.52%\n",
      "Batch 57, Loss: 0.887454, Accuracy: 79.61%\n",
      "Batch 58, Loss: 1.018188, Accuracy: 79.50%\n",
      "Batch 59, Loss: 0.994080, Accuracy: 79.40%\n",
      "Batch 60, Loss: 0.966693, Accuracy: 79.35%\n",
      "Batch 61, Loss: 0.900484, Accuracy: 79.46%\n",
      "Batch 62, Loss: 0.961789, Accuracy: 79.44%\n",
      "Batch 63, Loss: 0.992247, Accuracy: 79.39%\n",
      "Batch 64, Loss: 0.975088, Accuracy: 79.35%\n",
      "Batch 65, Loss: 0.970184, Accuracy: 79.30%\n",
      "Batch 66, Loss: 0.956343, Accuracy: 79.31%\n",
      "Batch 67, Loss: 0.912421, Accuracy: 79.36%\n",
      "Batch 68, Loss: 0.968885, Accuracy: 79.32%\n",
      "Batch 69, Loss: 1.000547, Accuracy: 79.23%\n",
      "Batch 70, Loss: 0.915645, Accuracy: 79.31%\n",
      "Batch 71, Loss: 0.910527, Accuracy: 79.36%\n",
      "Batch 72, Loss: 0.962184, Accuracy: 79.34%\n",
      "Batch 73, Loss: 0.950795, Accuracy: 79.32%\n",
      "Batch 74, Loss: 0.957234, Accuracy: 79.31%\n",
      "Batch 75, Loss: 0.952781, Accuracy: 79.31%\n",
      "Batch 76, Loss: 0.993938, Accuracy: 79.26%\n",
      "Batch 77, Loss: 0.963191, Accuracy: 79.24%\n",
      "Batch 78, Loss: 0.951891, Accuracy: 79.23%\n",
      "Batch 79, Loss: 1.030458, Accuracy: 79.13%\n",
      "Batch 80, Loss: 1.021700, Accuracy: 79.04%\n",
      "Batch 81, Loss: 0.936793, Accuracy: 79.09%\n",
      "Batch 82, Loss: 1.018206, Accuracy: 79.00%\n",
      "Batch 83, Loss: 0.850396, Accuracy: 79.12%\n",
      "Batch 84, Loss: 0.951925, Accuracy: 79.13%\n",
      "Batch 85, Loss: 0.956133, Accuracy: 79.12%\n",
      "Batch 86, Loss: 0.997202, Accuracy: 79.05%\n",
      "Batch 87, Loss: 0.949179, Accuracy: 79.06%\n",
      "Batch 88, Loss: 0.931851, Accuracy: 79.10%\n",
      "Batch 89, Loss: 0.984834, Accuracy: 79.06%\n",
      "Batch 90, Loss: 0.905907, Accuracy: 79.11%\n",
      "Batch 91, Loss: 0.911307, Accuracy: 79.14%\n",
      "Batch 92, Loss: 0.935511, Accuracy: 79.13%\n",
      "Batch 93, Loss: 0.968629, Accuracy: 79.12%\n",
      "Batch 94, Loss: 0.986029, Accuracy: 79.07%\n",
      "Batch 95, Loss: 1.014301, Accuracy: 79.00%\n",
      "Batch 96, Loss: 0.874162, Accuracy: 79.07%\n",
      "Batch 97, Loss: 0.980674, Accuracy: 79.03%\n",
      "Batch 98, Loss: 0.949525, Accuracy: 79.03%\n",
      "Batch 99, Loss: 0.906122, Accuracy: 79.07%\n",
      "Batch 100, Loss: 1.004224, Accuracy: 79.00%\n",
      "Batch 101, Loss: 1.013318, Accuracy: 78.94%\n",
      "Batch 102, Loss: 0.962500, Accuracy: 78.92%\n",
      "Batch 103, Loss: 0.953467, Accuracy: 78.91%\n",
      "Batch 104, Loss: 0.904469, Accuracy: 78.95%\n",
      "Batch 105, Loss: 0.993689, Accuracy: 78.93%\n",
      "Batch 106, Loss: 0.884919, Accuracy: 79.01%\n",
      "Batch 107, Loss: 0.974162, Accuracy: 79.00%\n",
      "Batch 108, Loss: 0.905137, Accuracy: 79.04%\n",
      "Batch 109, Loss: 0.914310, Accuracy: 79.07%\n",
      "Batch 110, Loss: 0.940675, Accuracy: 79.08%\n",
      "Batch 111, Loss: 0.906317, Accuracy: 79.12%\n",
      "Batch 112, Loss: 0.955969, Accuracy: 79.13%\n",
      "Batch 113, Loss: 0.977931, Accuracy: 79.09%\n",
      "Batch 114, Loss: 0.961171, Accuracy: 79.08%\n",
      "Batch 115, Loss: 0.979470, Accuracy: 79.06%\n",
      "Batch 116, Loss: 0.908262, Accuracy: 79.12%\n",
      "Batch 117, Loss: 0.968174, Accuracy: 79.09%\n",
      "Batch 118, Loss: 0.865689, Accuracy: 79.17%\n",
      "Batch 119, Loss: 0.959568, Accuracy: 79.16%\n",
      "Batch 120, Loss: 1.079783, Accuracy: 79.05%\n",
      "Batch 121, Loss: 0.951118, Accuracy: 79.05%\n",
      "Batch 122, Loss: 1.042217, Accuracy: 78.98%\n",
      "Batch 123, Loss: 0.972146, Accuracy: 78.98%\n",
      "Batch 124, Loss: 0.876564, Accuracy: 79.03%\n",
      "Batch 125, Loss: 0.946232, Accuracy: 79.04%\n",
      "Batch 126, Loss: 0.972779, Accuracy: 79.03%\n",
      "Batch 127, Loss: 1.012357, Accuracy: 78.96%\n",
      "Batch 128, Loss: 1.001175, Accuracy: 78.92%\n",
      "Batch 129, Loss: 0.918801, Accuracy: 78.95%\n",
      "Batch 130, Loss: 0.948016, Accuracy: 78.95%\n",
      "Batch 131, Loss: 0.960666, Accuracy: 78.96%\n",
      "Batch 132, Loss: 0.935165, Accuracy: 78.98%\n",
      "Batch 133, Loss: 0.969842, Accuracy: 78.96%\n",
      "Batch 134, Loss: 0.980638, Accuracy: 78.94%\n",
      "Batch 135, Loss: 0.959157, Accuracy: 78.94%\n",
      "Batch 136, Loss: 0.912022, Accuracy: 78.96%\n",
      "Batch 137, Loss: 0.991610, Accuracy: 78.92%\n",
      "Batch 138, Loss: 0.974506, Accuracy: 78.92%\n",
      "Batch 139, Loss: 1.062273, Accuracy: 78.82%\n",
      "Batch 140, Loss: 0.978319, Accuracy: 78.81%\n",
      "Batch 141, Loss: 0.947902, Accuracy: 78.81%\n",
      "Batch 142, Loss: 0.975155, Accuracy: 78.82%\n",
      "Batch 143, Loss: 0.943313, Accuracy: 78.82%\n",
      "Batch 144, Loss: 1.057659, Accuracy: 78.74%\n",
      "Batch 145, Loss: 0.925946, Accuracy: 78.77%\n",
      "Batch 146, Loss: 1.010239, Accuracy: 78.71%\n",
      "Batch 147, Loss: 0.937449, Accuracy: 78.73%\n",
      "Batch 148, Loss: 0.887839, Accuracy: 78.78%\n",
      "Batch 149, Loss: 0.894983, Accuracy: 78.82%\n",
      "Batch 150, Loss: 0.999865, Accuracy: 78.77%\n",
      "Batch 151, Loss: 0.931797, Accuracy: 78.79%\n",
      "Batch 152, Loss: 0.916172, Accuracy: 78.81%\n",
      "Batch 153, Loss: 0.867424, Accuracy: 78.88%\n",
      "Batch 154, Loss: 0.973460, Accuracy: 78.88%\n",
      "Batch 155, Loss: 0.907569, Accuracy: 78.90%\n",
      "Batch 156, Loss: 0.967048, Accuracy: 78.90%\n",
      "Batch 157, Loss: 0.940970, Accuracy: 78.92%\n",
      "Batch 158, Loss: 0.976965, Accuracy: 78.91%\n",
      "Batch 159, Loss: 0.955528, Accuracy: 78.91%\n",
      "Batch 160, Loss: 1.050704, Accuracy: 78.85%\n",
      "Batch 161, Loss: 0.993946, Accuracy: 78.83%\n",
      "Batch 162, Loss: 0.925354, Accuracy: 78.85%\n",
      "Batch 163, Loss: 0.973593, Accuracy: 78.84%\n",
      "Batch 164, Loss: 0.971574, Accuracy: 78.83%\n",
      "Batch 165, Loss: 0.903232, Accuracy: 78.87%\n",
      "Batch 166, Loss: 0.985611, Accuracy: 78.86%\n",
      "Batch 167, Loss: 0.903002, Accuracy: 78.89%\n",
      "Batch 168, Loss: 0.918968, Accuracy: 78.92%\n",
      "Batch 169, Loss: 0.886947, Accuracy: 78.96%\n",
      "Batch 170, Loss: 0.931935, Accuracy: 78.97%\n",
      "Batch 171, Loss: 0.918911, Accuracy: 78.99%\n",
      "Batch 172, Loss: 0.924080, Accuracy: 79.01%\n",
      "Batch 173, Loss: 0.957345, Accuracy: 79.00%\n",
      "Batch 174, Loss: 0.965095, Accuracy: 79.00%\n",
      "Batch 175, Loss: 1.040914, Accuracy: 78.93%\n",
      "Batch 176, Loss: 0.923215, Accuracy: 78.94%\n",
      "Batch 177, Loss: 0.896924, Accuracy: 78.98%\n",
      "Batch 178, Loss: 0.916779, Accuracy: 79.01%\n",
      "Batch 179, Loss: 0.929223, Accuracy: 79.03%\n",
      "Batch 180, Loss: 1.000857, Accuracy: 79.02%\n",
      "Batch 181, Loss: 0.950686, Accuracy: 79.02%\n",
      "Batch 182, Loss: 0.951114, Accuracy: 79.03%\n",
      "Batch 183, Loss: 0.948735, Accuracy: 79.03%\n",
      "Batch 184, Loss: 0.914916, Accuracy: 79.05%\n",
      "Batch 185, Loss: 0.896543, Accuracy: 79.09%\n",
      "Batch 186, Loss: 1.003284, Accuracy: 79.06%\n",
      "Batch 187, Loss: 0.889035, Accuracy: 79.09%\n",
      "Batch 188, Loss: 0.965403, Accuracy: 79.09%\n",
      "Batch 189, Loss: 0.996658, Accuracy: 79.06%\n",
      "Batch 190, Loss: 1.000402, Accuracy: 79.04%\n",
      "Batch 191, Loss: 0.945153, Accuracy: 79.03%\n",
      "Batch 192, Loss: 0.937046, Accuracy: 79.03%\n",
      "Batch 193, Loss: 0.914541, Accuracy: 79.04%\n",
      "Batch 194, Loss: 0.968227, Accuracy: 79.04%\n",
      "Batch 195, Loss: 0.860703, Accuracy: 79.09%\n",
      "Batch 196, Loss: 0.893042, Accuracy: 79.12%\n",
      "Batch 197, Loss: 0.945592, Accuracy: 79.13%\n",
      "Batch 198, Loss: 0.977528, Accuracy: 79.11%\n",
      "Batch 199, Loss: 0.949209, Accuracy: 79.11%\n",
      "Batch 200, Loss: 1.012072, Accuracy: 79.08%\n",
      "Batch 201, Loss: 0.947071, Accuracy: 79.08%\n",
      "Batch 202, Loss: 1.041584, Accuracy: 79.04%\n",
      "Batch 203, Loss: 0.922117, Accuracy: 79.05%\n",
      "Batch 204, Loss: 0.914853, Accuracy: 79.07%\n",
      "Batch 205, Loss: 0.978568, Accuracy: 79.06%\n",
      "Batch 206, Loss: 1.017135, Accuracy: 79.04%\n",
      "Batch 207, Loss: 0.898149, Accuracy: 79.07%\n",
      "Batch 208, Loss: 1.020043, Accuracy: 79.03%\n",
      "Batch 209, Loss: 0.948023, Accuracy: 79.03%\n",
      "Batch 210, Loss: 0.885183, Accuracy: 79.07%\n",
      "Batch 211, Loss: 0.976921, Accuracy: 79.06%\n",
      "Batch 212, Loss: 0.894680, Accuracy: 79.08%\n",
      "Batch 213, Loss: 0.979549, Accuracy: 79.07%\n",
      "Training - Epoch 97, Loss: 0.952519, Accuracy: 79.07%\n",
      "Validation Batch 1, Loss: 0.896330, Accuracy: 85.94%\n",
      "Validation Batch 2, Loss: 0.946703, Accuracy: 82.81%\n",
      "Validation Batch 3, Loss: 1.001792, Accuracy: 79.69%\n",
      "Validation Batch 4, Loss: 0.933645, Accuracy: 79.69%\n",
      "Validation Batch 5, Loss: 0.912876, Accuracy: 80.62%\n",
      "Validation Batch 6, Loss: 0.890158, Accuracy: 81.51%\n",
      "Validation Batch 7, Loss: 0.972009, Accuracy: 80.58%\n",
      "Validation Batch 8, Loss: 0.992621, Accuracy: 79.88%\n",
      "Validation Batch 9, Loss: 1.009639, Accuracy: 78.82%\n",
      "Validation Batch 10, Loss: 0.984446, Accuracy: 78.44%\n",
      "Validation Batch 11, Loss: 0.922287, Accuracy: 78.84%\n",
      "Validation Batch 12, Loss: 0.889027, Accuracy: 79.43%\n",
      "Validation Batch 13, Loss: 1.000791, Accuracy: 78.97%\n",
      "Validation Batch 14, Loss: 0.977499, Accuracy: 78.57%\n",
      "Validation Batch 15, Loss: 0.943964, Accuracy: 78.65%\n",
      "Validation Batch 16, Loss: 0.907320, Accuracy: 79.10%\n",
      "Validation Batch 17, Loss: 0.973933, Accuracy: 78.86%\n",
      "Validation Batch 18, Loss: 0.930934, Accuracy: 78.99%\n",
      "Validation Batch 19, Loss: 0.970152, Accuracy: 78.87%\n",
      "Validation Batch 20, Loss: 0.922812, Accuracy: 78.98%\n",
      "Validation Batch 21, Loss: 0.958476, Accuracy: 79.09%\n",
      "Validation Batch 22, Loss: 0.985878, Accuracy: 78.84%\n",
      "Validation Batch 23, Loss: 1.020895, Accuracy: 78.46%\n",
      "Validation Batch 24, Loss: 1.012734, Accuracy: 78.26%\n",
      "Validation Batch 25, Loss: 0.940372, Accuracy: 78.31%\n",
      "Validation Batch 26, Loss: 0.955089, Accuracy: 78.37%\n",
      "Validation Batch 27, Loss: 0.913622, Accuracy: 78.51%\n",
      "Validation - Epoch 97, Loss: 0.954297, Accuracy: 78.51%\n",
      "Patienceâ€”5\n",
      "Epoch 98\n",
      "Batch 1, Loss: 0.965941, Accuracy: 78.12%\n",
      "Batch 2, Loss: 1.025317, Accuracy: 75.00%\n",
      "Batch 3, Loss: 0.952449, Accuracy: 76.04%\n",
      "Batch 4, Loss: 0.900784, Accuracy: 78.52%\n",
      "Batch 5, Loss: 0.932458, Accuracy: 79.38%\n",
      "Batch 6, Loss: 0.920890, Accuracy: 79.69%\n",
      "Batch 7, Loss: 0.949188, Accuracy: 79.91%\n",
      "Batch 8, Loss: 0.966297, Accuracy: 79.49%\n",
      "Batch 9, Loss: 0.932869, Accuracy: 79.69%\n",
      "Batch 10, Loss: 0.948083, Accuracy: 79.69%\n",
      "Batch 11, Loss: 0.909340, Accuracy: 80.11%\n",
      "Batch 12, Loss: 0.981136, Accuracy: 79.69%\n",
      "Batch 13, Loss: 1.007934, Accuracy: 79.21%\n",
      "Batch 14, Loss: 0.934154, Accuracy: 79.35%\n",
      "Batch 15, Loss: 0.854840, Accuracy: 80.10%\n",
      "Batch 16, Loss: 0.925924, Accuracy: 80.18%\n",
      "Batch 17, Loss: 0.964388, Accuracy: 80.06%\n",
      "Batch 18, Loss: 0.958084, Accuracy: 79.95%\n",
      "Batch 19, Loss: 0.960894, Accuracy: 79.85%\n",
      "Batch 20, Loss: 0.966866, Accuracy: 79.69%\n",
      "Batch 21, Loss: 0.874352, Accuracy: 80.13%\n",
      "Batch 22, Loss: 0.962713, Accuracy: 80.04%\n",
      "Batch 23, Loss: 0.919226, Accuracy: 80.16%\n",
      "Batch 24, Loss: 1.002823, Accuracy: 79.88%\n",
      "Batch 25, Loss: 0.987091, Accuracy: 79.69%\n",
      "Batch 26, Loss: 0.920998, Accuracy: 79.93%\n",
      "Batch 27, Loss: 0.931704, Accuracy: 79.98%\n",
      "Batch 28, Loss: 0.946518, Accuracy: 79.97%\n",
      "Batch 29, Loss: 0.989439, Accuracy: 79.85%\n",
      "Batch 30, Loss: 0.949093, Accuracy: 79.84%\n",
      "Batch 31, Loss: 0.972339, Accuracy: 79.74%\n",
      "Batch 32, Loss: 0.884205, Accuracy: 79.93%\n",
      "Batch 33, Loss: 0.933300, Accuracy: 79.97%\n",
      "Batch 34, Loss: 1.047240, Accuracy: 79.69%\n",
      "Batch 35, Loss: 0.966368, Accuracy: 79.60%\n",
      "Batch 36, Loss: 0.915694, Accuracy: 79.73%\n",
      "Batch 37, Loss: 0.943366, Accuracy: 79.69%\n",
      "Batch 38, Loss: 0.960671, Accuracy: 79.65%\n",
      "Batch 39, Loss: 0.932918, Accuracy: 79.69%\n",
      "Batch 40, Loss: 0.960586, Accuracy: 79.65%\n",
      "Batch 41, Loss: 1.042558, Accuracy: 79.38%\n",
      "Batch 42, Loss: 0.921599, Accuracy: 79.43%\n",
      "Batch 43, Loss: 0.994985, Accuracy: 79.32%\n",
      "Batch 44, Loss: 0.914747, Accuracy: 79.44%\n",
      "Batch 45, Loss: 0.995668, Accuracy: 79.31%\n",
      "Batch 46, Loss: 0.943741, Accuracy: 79.31%\n",
      "Batch 47, Loss: 0.947901, Accuracy: 79.32%\n",
      "Batch 48, Loss: 1.016948, Accuracy: 79.20%\n",
      "Batch 49, Loss: 0.885905, Accuracy: 79.34%\n",
      "Batch 50, Loss: 0.929585, Accuracy: 79.34%\n",
      "Batch 51, Loss: 0.921205, Accuracy: 79.32%\n",
      "Batch 52, Loss: 0.946410, Accuracy: 79.33%\n",
      "Batch 53, Loss: 0.971279, Accuracy: 79.25%\n",
      "Batch 54, Loss: 0.979394, Accuracy: 79.17%\n",
      "Batch 55, Loss: 0.946104, Accuracy: 79.15%\n",
      "Batch 56, Loss: 0.937966, Accuracy: 79.13%\n",
      "Batch 57, Loss: 0.877845, Accuracy: 79.28%\n",
      "Batch 58, Loss: 1.006685, Accuracy: 79.15%\n",
      "Batch 59, Loss: 0.926248, Accuracy: 79.21%\n",
      "Batch 60, Loss: 0.892012, Accuracy: 79.32%\n",
      "Batch 61, Loss: 0.885956, Accuracy: 79.43%\n",
      "Batch 62, Loss: 0.947095, Accuracy: 79.44%\n",
      "Batch 63, Loss: 0.999599, Accuracy: 79.34%\n",
      "Batch 64, Loss: 0.887285, Accuracy: 79.42%\n",
      "Batch 65, Loss: 0.908268, Accuracy: 79.50%\n",
      "Batch 66, Loss: 0.939011, Accuracy: 79.50%\n",
      "Batch 67, Loss: 0.931967, Accuracy: 79.52%\n",
      "Batch 68, Loss: 0.949094, Accuracy: 79.53%\n",
      "Batch 69, Loss: 1.043063, Accuracy: 79.37%\n",
      "Batch 70, Loss: 0.927018, Accuracy: 79.40%\n",
      "Batch 71, Loss: 0.931821, Accuracy: 79.42%\n",
      "Batch 72, Loss: 0.959786, Accuracy: 79.41%\n",
      "Batch 73, Loss: 0.897545, Accuracy: 79.52%\n",
      "Batch 74, Loss: 1.017716, Accuracy: 79.39%\n",
      "Batch 75, Loss: 0.946397, Accuracy: 79.40%\n",
      "Batch 76, Loss: 0.960810, Accuracy: 79.38%\n",
      "Batch 77, Loss: 0.970636, Accuracy: 79.32%\n",
      "Batch 78, Loss: 0.898319, Accuracy: 79.39%\n",
      "Batch 79, Loss: 0.952256, Accuracy: 79.37%\n",
      "Batch 80, Loss: 0.973757, Accuracy: 79.38%\n",
      "Batch 81, Loss: 0.942077, Accuracy: 79.40%\n",
      "Batch 82, Loss: 0.913207, Accuracy: 79.44%\n",
      "Batch 83, Loss: 0.947188, Accuracy: 79.46%\n",
      "Batch 84, Loss: 1.012820, Accuracy: 79.37%\n",
      "Batch 85, Loss: 0.909328, Accuracy: 79.41%\n",
      "Batch 86, Loss: 0.972369, Accuracy: 79.36%\n",
      "Batch 87, Loss: 0.958527, Accuracy: 79.35%\n",
      "Batch 88, Loss: 1.015075, Accuracy: 79.26%\n",
      "Batch 89, Loss: 0.903548, Accuracy: 79.32%\n",
      "Batch 90, Loss: 0.888505, Accuracy: 79.39%\n",
      "Batch 91, Loss: 0.982150, Accuracy: 79.33%\n",
      "Batch 92, Loss: 0.915609, Accuracy: 79.38%\n",
      "Batch 93, Loss: 0.866838, Accuracy: 79.47%\n",
      "Batch 94, Loss: 0.925845, Accuracy: 79.47%\n",
      "Batch 95, Loss: 0.926999, Accuracy: 79.47%\n",
      "Batch 96, Loss: 1.010585, Accuracy: 79.39%\n",
      "Batch 97, Loss: 0.928831, Accuracy: 79.41%\n",
      "Batch 98, Loss: 0.914878, Accuracy: 79.45%\n",
      "Batch 99, Loss: 0.905896, Accuracy: 79.50%\n",
      "Batch 100, Loss: 0.989075, Accuracy: 79.47%\n",
      "Batch 101, Loss: 0.911221, Accuracy: 79.50%\n",
      "Batch 102, Loss: 1.000504, Accuracy: 79.46%\n",
      "Batch 103, Loss: 1.063559, Accuracy: 79.34%\n",
      "Batch 104, Loss: 0.881621, Accuracy: 79.40%\n",
      "Batch 105, Loss: 0.859060, Accuracy: 79.49%\n",
      "Batch 106, Loss: 0.890000, Accuracy: 79.54%\n",
      "Batch 107, Loss: 0.955892, Accuracy: 79.53%\n",
      "Batch 108, Loss: 0.914444, Accuracy: 79.56%\n",
      "Batch 109, Loss: 0.890196, Accuracy: 79.62%\n",
      "Batch 110, Loss: 0.868478, Accuracy: 79.70%\n",
      "Batch 111, Loss: 0.972068, Accuracy: 79.67%\n",
      "Batch 112, Loss: 0.933140, Accuracy: 79.67%\n",
      "Batch 113, Loss: 0.943792, Accuracy: 79.69%\n",
      "Batch 114, Loss: 0.961065, Accuracy: 79.66%\n",
      "Batch 115, Loss: 1.012560, Accuracy: 79.61%\n",
      "Batch 116, Loss: 0.931428, Accuracy: 79.62%\n",
      "Batch 117, Loss: 0.958831, Accuracy: 79.61%\n",
      "Batch 118, Loss: 0.969584, Accuracy: 79.58%\n",
      "Batch 119, Loss: 1.001969, Accuracy: 79.54%\n",
      "Batch 120, Loss: 1.081392, Accuracy: 79.43%\n",
      "Batch 121, Loss: 0.954393, Accuracy: 79.42%\n",
      "Batch 122, Loss: 1.006999, Accuracy: 79.37%\n",
      "Batch 123, Loss: 0.927093, Accuracy: 79.38%\n",
      "Batch 124, Loss: 0.943414, Accuracy: 79.40%\n",
      "Batch 125, Loss: 0.864540, Accuracy: 79.47%\n",
      "Batch 126, Loss: 0.915106, Accuracy: 79.50%\n",
      "Batch 127, Loss: 0.929367, Accuracy: 79.50%\n",
      "Batch 128, Loss: 0.979401, Accuracy: 79.49%\n",
      "Batch 129, Loss: 1.022707, Accuracy: 79.43%\n",
      "Batch 130, Loss: 0.885678, Accuracy: 79.48%\n",
      "Batch 131, Loss: 0.999461, Accuracy: 79.45%\n",
      "Batch 132, Loss: 1.001932, Accuracy: 79.42%\n",
      "Batch 133, Loss: 0.903824, Accuracy: 79.46%\n",
      "Batch 134, Loss: 0.909506, Accuracy: 79.51%\n",
      "Batch 135, Loss: 0.935521, Accuracy: 79.53%\n",
      "Batch 136, Loss: 0.940683, Accuracy: 79.56%\n",
      "Batch 137, Loss: 0.919514, Accuracy: 79.56%\n",
      "Batch 138, Loss: 0.904577, Accuracy: 79.60%\n",
      "Batch 139, Loss: 0.984871, Accuracy: 79.56%\n",
      "Batch 140, Loss: 0.926972, Accuracy: 79.59%\n",
      "Batch 141, Loss: 0.993229, Accuracy: 79.55%\n",
      "Batch 142, Loss: 0.960037, Accuracy: 79.54%\n",
      "Batch 143, Loss: 0.971270, Accuracy: 79.53%\n",
      "Batch 144, Loss: 0.923681, Accuracy: 79.56%\n",
      "Batch 145, Loss: 0.869209, Accuracy: 79.61%\n",
      "Batch 146, Loss: 0.907044, Accuracy: 79.63%\n",
      "Batch 147, Loss: 0.945039, Accuracy: 79.63%\n",
      "Batch 148, Loss: 0.918147, Accuracy: 79.66%\n",
      "Batch 149, Loss: 0.960135, Accuracy: 79.64%\n",
      "Batch 150, Loss: 0.914900, Accuracy: 79.67%\n",
      "Batch 151, Loss: 0.946730, Accuracy: 79.67%\n",
      "Batch 152, Loss: 0.955135, Accuracy: 79.66%\n",
      "Batch 153, Loss: 1.025027, Accuracy: 79.61%\n",
      "Batch 154, Loss: 0.980831, Accuracy: 79.58%\n",
      "Batch 155, Loss: 1.019912, Accuracy: 79.52%\n",
      "Batch 156, Loss: 0.976065, Accuracy: 79.49%\n",
      "Batch 157, Loss: 1.041060, Accuracy: 79.41%\n",
      "Batch 158, Loss: 1.012710, Accuracy: 79.37%\n",
      "Batch 159, Loss: 0.921178, Accuracy: 79.39%\n",
      "Batch 160, Loss: 0.992802, Accuracy: 79.37%\n",
      "Batch 161, Loss: 0.934957, Accuracy: 79.37%\n",
      "Batch 162, Loss: 1.002434, Accuracy: 79.33%\n",
      "Batch 163, Loss: 0.950635, Accuracy: 79.33%\n",
      "Batch 164, Loss: 0.967855, Accuracy: 79.33%\n",
      "Batch 165, Loss: 0.980418, Accuracy: 79.33%\n",
      "Batch 166, Loss: 0.942791, Accuracy: 79.33%\n",
      "Batch 167, Loss: 0.938632, Accuracy: 79.33%\n",
      "Batch 168, Loss: 0.959516, Accuracy: 79.32%\n",
      "Batch 169, Loss: 1.039986, Accuracy: 79.25%\n",
      "Batch 170, Loss: 1.017651, Accuracy: 79.22%\n",
      "Batch 171, Loss: 0.934008, Accuracy: 79.23%\n",
      "Batch 172, Loss: 0.978237, Accuracy: 79.22%\n",
      "Batch 173, Loss: 1.027663, Accuracy: 79.16%\n",
      "Batch 174, Loss: 0.945769, Accuracy: 79.16%\n",
      "Batch 175, Loss: 1.001636, Accuracy: 79.12%\n",
      "Batch 176, Loss: 1.001237, Accuracy: 79.09%\n",
      "Batch 177, Loss: 0.915814, Accuracy: 79.12%\n",
      "Batch 178, Loss: 0.976653, Accuracy: 79.11%\n",
      "Batch 179, Loss: 0.814951, Accuracy: 79.20%\n",
      "Batch 180, Loss: 1.066024, Accuracy: 79.14%\n",
      "Batch 181, Loss: 0.963012, Accuracy: 79.13%\n",
      "Batch 182, Loss: 1.025759, Accuracy: 79.09%\n",
      "Batch 183, Loss: 0.858816, Accuracy: 79.14%\n",
      "Batch 184, Loss: 0.892237, Accuracy: 79.18%\n",
      "Batch 185, Loss: 0.858602, Accuracy: 79.23%\n",
      "Batch 186, Loss: 0.938786, Accuracy: 79.24%\n",
      "Batch 187, Loss: 1.020105, Accuracy: 79.19%\n",
      "Batch 188, Loss: 1.050414, Accuracy: 79.13%\n",
      "Batch 189, Loss: 0.924237, Accuracy: 79.15%\n",
      "Batch 190, Loss: 0.873417, Accuracy: 79.19%\n",
      "Batch 191, Loss: 1.008886, Accuracy: 79.16%\n",
      "Batch 192, Loss: 0.979189, Accuracy: 79.14%\n",
      "Batch 193, Loss: 0.966429, Accuracy: 79.14%\n",
      "Batch 194, Loss: 0.872362, Accuracy: 79.18%\n",
      "Batch 195, Loss: 0.924836, Accuracy: 79.18%\n",
      "Batch 196, Loss: 0.972338, Accuracy: 79.17%\n",
      "Batch 197, Loss: 0.909760, Accuracy: 79.18%\n",
      "Batch 198, Loss: 0.902930, Accuracy: 79.21%\n",
      "Batch 199, Loss: 0.995237, Accuracy: 79.20%\n",
      "Batch 200, Loss: 0.990649, Accuracy: 79.18%\n",
      "Batch 201, Loss: 0.958257, Accuracy: 79.17%\n",
      "Batch 202, Loss: 0.936875, Accuracy: 79.18%\n",
      "Batch 203, Loss: 1.043062, Accuracy: 79.13%\n",
      "Batch 204, Loss: 0.936518, Accuracy: 79.14%\n",
      "Batch 205, Loss: 0.964077, Accuracy: 79.13%\n",
      "Batch 206, Loss: 0.949728, Accuracy: 79.13%\n",
      "Batch 207, Loss: 0.922173, Accuracy: 79.16%\n",
      "Batch 208, Loss: 0.934064, Accuracy: 79.17%\n",
      "Batch 209, Loss: 0.917811, Accuracy: 79.17%\n",
      "Batch 210, Loss: 0.951571, Accuracy: 79.17%\n",
      "Batch 211, Loss: 0.960379, Accuracy: 79.17%\n",
      "Batch 212, Loss: 0.950561, Accuracy: 79.17%\n",
      "Batch 213, Loss: 0.932413, Accuracy: 79.18%\n",
      "Training - Epoch 98, Loss: 0.950643, Accuracy: 79.18%\n",
      "Validation Batch 1, Loss: 0.896085, Accuracy: 85.94%\n",
      "Validation Batch 2, Loss: 0.938196, Accuracy: 82.81%\n",
      "Validation Batch 3, Loss: 0.992013, Accuracy: 80.21%\n",
      "Validation Batch 4, Loss: 0.924536, Accuracy: 80.08%\n",
      "Validation Batch 5, Loss: 0.906256, Accuracy: 80.94%\n",
      "Validation Batch 6, Loss: 0.882827, Accuracy: 81.77%\n",
      "Validation Batch 7, Loss: 0.964766, Accuracy: 81.03%\n",
      "Validation Batch 8, Loss: 0.997416, Accuracy: 80.08%\n",
      "Validation Batch 9, Loss: 1.000280, Accuracy: 79.34%\n",
      "Validation Batch 10, Loss: 0.979088, Accuracy: 79.06%\n",
      "Validation Batch 11, Loss: 0.920120, Accuracy: 79.40%\n",
      "Validation Batch 12, Loss: 0.891387, Accuracy: 79.82%\n",
      "Validation Batch 13, Loss: 0.994107, Accuracy: 79.33%\n",
      "Validation Batch 14, Loss: 0.969349, Accuracy: 79.13%\n",
      "Validation Batch 15, Loss: 0.940027, Accuracy: 79.27%\n",
      "Validation Batch 16, Loss: 0.902728, Accuracy: 79.69%\n",
      "Validation Batch 17, Loss: 0.976095, Accuracy: 79.50%\n",
      "Validation Batch 18, Loss: 0.928047, Accuracy: 79.60%\n",
      "Validation Batch 19, Loss: 0.968577, Accuracy: 79.44%\n",
      "Validation Batch 20, Loss: 0.899739, Accuracy: 79.61%\n",
      "Validation Batch 21, Loss: 0.949503, Accuracy: 79.69%\n",
      "Validation Batch 22, Loss: 0.978689, Accuracy: 79.47%\n",
      "Validation Batch 23, Loss: 1.006381, Accuracy: 79.14%\n",
      "Validation Batch 24, Loss: 1.009985, Accuracy: 78.84%\n",
      "Validation Batch 25, Loss: 0.923559, Accuracy: 79.00%\n",
      "Validation Batch 26, Loss: 0.952704, Accuracy: 79.03%\n",
      "Validation Batch 27, Loss: 0.905929, Accuracy: 79.15%\n",
      "Validation - Epoch 98, Loss: 0.948089, Accuracy: 79.15%\n",
      "Patienceâ€”6\n",
      "Epoch 99\n",
      "Batch 1, Loss: 0.924626, Accuracy: 84.38%\n",
      "Batch 2, Loss: 0.991556, Accuracy: 78.91%\n",
      "Batch 3, Loss: 0.989811, Accuracy: 77.08%\n",
      "Batch 4, Loss: 0.914536, Accuracy: 78.52%\n",
      "Batch 5, Loss: 0.860287, Accuracy: 80.31%\n",
      "Batch 6, Loss: 1.059811, Accuracy: 78.12%\n",
      "Batch 7, Loss: 0.944861, Accuracy: 78.35%\n",
      "Batch 8, Loss: 0.947136, Accuracy: 78.52%\n",
      "Batch 9, Loss: 0.998644, Accuracy: 77.95%\n",
      "Batch 10, Loss: 0.914324, Accuracy: 78.28%\n",
      "Batch 11, Loss: 0.848319, Accuracy: 79.26%\n",
      "Batch 12, Loss: 0.917254, Accuracy: 79.43%\n",
      "Batch 13, Loss: 0.911072, Accuracy: 79.57%\n",
      "Batch 14, Loss: 0.877374, Accuracy: 80.02%\n",
      "Batch 15, Loss: 0.920315, Accuracy: 80.10%\n",
      "Batch 16, Loss: 1.022991, Accuracy: 79.49%\n",
      "Batch 17, Loss: 0.975258, Accuracy: 79.32%\n",
      "Batch 18, Loss: 0.985761, Accuracy: 78.99%\n",
      "Batch 19, Loss: 0.967200, Accuracy: 78.87%\n",
      "Batch 20, Loss: 0.912860, Accuracy: 79.06%\n",
      "Batch 21, Loss: 0.941614, Accuracy: 79.17%\n",
      "Batch 22, Loss: 0.895279, Accuracy: 79.40%\n",
      "Batch 23, Loss: 1.006418, Accuracy: 79.14%\n",
      "Batch 24, Loss: 1.003794, Accuracy: 78.91%\n",
      "Batch 25, Loss: 0.940981, Accuracy: 79.00%\n",
      "Batch 26, Loss: 0.996403, Accuracy: 78.79%\n",
      "Batch 27, Loss: 0.944277, Accuracy: 78.88%\n",
      "Batch 28, Loss: 1.010921, Accuracy: 78.79%\n",
      "Batch 29, Loss: 0.934782, Accuracy: 78.83%\n",
      "Batch 30, Loss: 0.891889, Accuracy: 79.01%\n",
      "Batch 31, Loss: 0.883282, Accuracy: 79.23%\n",
      "Batch 32, Loss: 0.909636, Accuracy: 79.35%\n",
      "Batch 33, Loss: 0.938747, Accuracy: 79.36%\n",
      "Batch 34, Loss: 0.883816, Accuracy: 79.55%\n",
      "Batch 35, Loss: 1.004309, Accuracy: 79.33%\n",
      "Batch 36, Loss: 0.940943, Accuracy: 79.38%\n",
      "Batch 37, Loss: 0.995271, Accuracy: 79.22%\n",
      "Batch 38, Loss: 0.941698, Accuracy: 79.24%\n",
      "Batch 39, Loss: 0.909638, Accuracy: 79.33%\n",
      "Batch 40, Loss: 0.947075, Accuracy: 79.34%\n",
      "Batch 41, Loss: 0.956499, Accuracy: 79.31%\n",
      "Batch 42, Loss: 1.005369, Accuracy: 79.13%\n",
      "Batch 43, Loss: 1.012072, Accuracy: 78.96%\n",
      "Batch 44, Loss: 1.037400, Accuracy: 78.80%\n",
      "Batch 45, Loss: 0.956515, Accuracy: 78.78%\n",
      "Batch 46, Loss: 1.017752, Accuracy: 78.60%\n",
      "Batch 47, Loss: 0.908404, Accuracy: 78.72%\n",
      "Batch 48, Loss: 0.950458, Accuracy: 78.74%\n",
      "Batch 49, Loss: 0.950628, Accuracy: 78.70%\n",
      "Batch 50, Loss: 1.032244, Accuracy: 78.50%\n",
      "Batch 51, Loss: 1.051651, Accuracy: 78.37%\n",
      "Batch 52, Loss: 1.033150, Accuracy: 78.22%\n",
      "Batch 53, Loss: 0.937402, Accuracy: 78.27%\n",
      "Batch 54, Loss: 0.939054, Accuracy: 78.33%\n",
      "Batch 55, Loss: 0.920427, Accuracy: 78.38%\n",
      "Batch 56, Loss: 0.926254, Accuracy: 78.43%\n",
      "Batch 57, Loss: 0.939057, Accuracy: 78.48%\n",
      "Batch 58, Loss: 0.928776, Accuracy: 78.53%\n",
      "Batch 59, Loss: 0.943149, Accuracy: 78.58%\n",
      "Batch 60, Loss: 0.979767, Accuracy: 78.54%\n",
      "Batch 61, Loss: 0.904344, Accuracy: 78.64%\n",
      "Batch 62, Loss: 0.933813, Accuracy: 78.68%\n",
      "Batch 63, Loss: 0.949550, Accuracy: 78.67%\n",
      "Batch 64, Loss: 0.923013, Accuracy: 78.76%\n",
      "Batch 65, Loss: 1.009945, Accuracy: 78.70%\n",
      "Batch 66, Loss: 0.948790, Accuracy: 78.69%\n",
      "Batch 67, Loss: 0.862066, Accuracy: 78.82%\n",
      "Batch 68, Loss: 0.944110, Accuracy: 78.84%\n",
      "Batch 69, Loss: 0.893954, Accuracy: 78.92%\n",
      "Batch 70, Loss: 0.944954, Accuracy: 78.95%\n",
      "Batch 71, Loss: 0.924345, Accuracy: 78.98%\n",
      "Batch 72, Loss: 0.895914, Accuracy: 79.08%\n",
      "Batch 73, Loss: 1.010098, Accuracy: 78.98%\n",
      "Batch 74, Loss: 0.946438, Accuracy: 78.99%\n",
      "Batch 75, Loss: 0.904065, Accuracy: 79.06%\n",
      "Batch 76, Loss: 0.946109, Accuracy: 79.09%\n",
      "Batch 77, Loss: 1.009956, Accuracy: 79.02%\n",
      "Batch 78, Loss: 0.891995, Accuracy: 79.09%\n",
      "Batch 79, Loss: 0.946534, Accuracy: 79.09%\n",
      "Batch 80, Loss: 0.944952, Accuracy: 79.12%\n",
      "Batch 81, Loss: 0.956366, Accuracy: 79.09%\n",
      "Batch 82, Loss: 0.919475, Accuracy: 79.12%\n",
      "Batch 83, Loss: 0.910140, Accuracy: 79.18%\n",
      "Batch 84, Loss: 0.934502, Accuracy: 79.20%\n",
      "Batch 85, Loss: 0.941672, Accuracy: 79.21%\n",
      "Batch 86, Loss: 0.932335, Accuracy: 79.25%\n",
      "Batch 87, Loss: 0.890790, Accuracy: 79.31%\n",
      "Batch 88, Loss: 0.949298, Accuracy: 79.31%\n",
      "Batch 89, Loss: 1.022370, Accuracy: 79.23%\n",
      "Batch 90, Loss: 1.005106, Accuracy: 79.18%\n",
      "Batch 91, Loss: 0.980852, Accuracy: 79.16%\n",
      "Batch 92, Loss: 0.985114, Accuracy: 79.11%\n",
      "Batch 93, Loss: 1.000030, Accuracy: 79.05%\n",
      "Batch 94, Loss: 0.936469, Accuracy: 79.07%\n",
      "Batch 95, Loss: 0.924447, Accuracy: 79.11%\n",
      "Batch 96, Loss: 0.985305, Accuracy: 79.07%\n",
      "Batch 97, Loss: 1.010396, Accuracy: 79.01%\n",
      "Batch 98, Loss: 0.955177, Accuracy: 79.02%\n",
      "Batch 99, Loss: 1.021094, Accuracy: 78.96%\n",
      "Batch 100, Loss: 0.986881, Accuracy: 78.92%\n",
      "Batch 101, Loss: 0.991902, Accuracy: 78.88%\n",
      "Batch 102, Loss: 0.937090, Accuracy: 78.91%\n",
      "Batch 103, Loss: 1.081533, Accuracy: 78.76%\n",
      "Batch 104, Loss: 0.934750, Accuracy: 78.77%\n",
      "Batch 105, Loss: 0.950743, Accuracy: 78.76%\n",
      "Batch 106, Loss: 0.990501, Accuracy: 78.71%\n",
      "Batch 107, Loss: 0.900451, Accuracy: 78.75%\n",
      "Batch 108, Loss: 0.928225, Accuracy: 78.78%\n",
      "Batch 109, Loss: 0.895024, Accuracy: 78.83%\n",
      "Batch 110, Loss: 0.855057, Accuracy: 78.92%\n",
      "Batch 111, Loss: 0.928572, Accuracy: 78.94%\n",
      "Batch 112, Loss: 0.928742, Accuracy: 78.98%\n",
      "Batch 113, Loss: 0.982827, Accuracy: 78.94%\n",
      "Batch 114, Loss: 0.956190, Accuracy: 78.93%\n",
      "Batch 115, Loss: 0.982186, Accuracy: 78.90%\n",
      "Batch 116, Loss: 1.019550, Accuracy: 78.83%\n",
      "Batch 117, Loss: 0.934766, Accuracy: 78.85%\n",
      "Batch 118, Loss: 0.911575, Accuracy: 78.87%\n",
      "Batch 119, Loss: 1.003804, Accuracy: 78.82%\n",
      "Batch 120, Loss: 0.874636, Accuracy: 78.88%\n",
      "Batch 121, Loss: 0.999556, Accuracy: 78.84%\n",
      "Batch 122, Loss: 1.033544, Accuracy: 78.77%\n",
      "Batch 123, Loss: 0.953192, Accuracy: 78.77%\n",
      "Batch 124, Loss: 0.998258, Accuracy: 78.73%\n",
      "Batch 125, Loss: 0.944215, Accuracy: 78.75%\n",
      "Batch 126, Loss: 1.054764, Accuracy: 78.66%\n",
      "Batch 127, Loss: 0.875553, Accuracy: 78.72%\n",
      "Batch 128, Loss: 0.993217, Accuracy: 78.69%\n",
      "Batch 129, Loss: 0.958562, Accuracy: 78.69%\n",
      "Batch 130, Loss: 0.943920, Accuracy: 78.70%\n",
      "Batch 131, Loss: 0.970322, Accuracy: 78.69%\n",
      "Batch 132, Loss: 0.993907, Accuracy: 78.67%\n",
      "Batch 133, Loss: 0.874882, Accuracy: 78.71%\n",
      "Batch 134, Loss: 0.948550, Accuracy: 78.72%\n",
      "Batch 135, Loss: 0.922330, Accuracy: 78.75%\n",
      "Batch 136, Loss: 0.942986, Accuracy: 78.75%\n",
      "Batch 137, Loss: 0.946817, Accuracy: 78.75%\n",
      "Batch 138, Loss: 0.970476, Accuracy: 78.75%\n",
      "Batch 139, Loss: 0.993865, Accuracy: 78.71%\n",
      "Batch 140, Loss: 1.048526, Accuracy: 78.65%\n",
      "Batch 141, Loss: 0.885411, Accuracy: 78.70%\n",
      "Batch 142, Loss: 0.999477, Accuracy: 78.68%\n",
      "Batch 143, Loss: 0.974292, Accuracy: 78.66%\n",
      "Batch 144, Loss: 0.981233, Accuracy: 78.65%\n",
      "Batch 145, Loss: 0.955840, Accuracy: 78.65%\n",
      "Batch 146, Loss: 0.923972, Accuracy: 78.66%\n",
      "Batch 147, Loss: 1.036780, Accuracy: 78.59%\n",
      "Batch 148, Loss: 0.987054, Accuracy: 78.56%\n",
      "Batch 149, Loss: 0.914020, Accuracy: 78.59%\n",
      "Batch 150, Loss: 0.936032, Accuracy: 78.60%\n",
      "Batch 151, Loss: 0.973099, Accuracy: 78.60%\n",
      "Batch 152, Loss: 1.023166, Accuracy: 78.56%\n",
      "Batch 153, Loss: 1.061656, Accuracy: 78.47%\n",
      "Batch 154, Loss: 0.936051, Accuracy: 78.49%\n",
      "Batch 155, Loss: 0.976516, Accuracy: 78.48%\n",
      "Batch 156, Loss: 0.943214, Accuracy: 78.49%\n",
      "Batch 157, Loss: 0.999749, Accuracy: 78.46%\n",
      "Batch 158, Loss: 0.843867, Accuracy: 78.55%\n",
      "Batch 159, Loss: 1.002656, Accuracy: 78.52%\n",
      "Batch 160, Loss: 0.968084, Accuracy: 78.51%\n",
      "Batch 161, Loss: 0.871766, Accuracy: 78.55%\n",
      "Batch 162, Loss: 0.945866, Accuracy: 78.54%\n",
      "Batch 163, Loss: 0.947466, Accuracy: 78.55%\n",
      "Batch 164, Loss: 0.986892, Accuracy: 78.53%\n",
      "Batch 165, Loss: 0.924737, Accuracy: 78.54%\n",
      "Batch 166, Loss: 1.018584, Accuracy: 78.50%\n",
      "Batch 167, Loss: 0.997319, Accuracy: 78.48%\n",
      "Batch 168, Loss: 0.964011, Accuracy: 78.48%\n",
      "Batch 169, Loss: 1.022227, Accuracy: 78.43%\n",
      "Batch 170, Loss: 0.913307, Accuracy: 78.47%\n",
      "Batch 171, Loss: 0.956018, Accuracy: 78.47%\n",
      "Batch 172, Loss: 0.993223, Accuracy: 78.45%\n",
      "Batch 173, Loss: 0.924493, Accuracy: 78.48%\n",
      "Batch 174, Loss: 0.915192, Accuracy: 78.51%\n",
      "Batch 175, Loss: 0.960307, Accuracy: 78.50%\n",
      "Batch 176, Loss: 0.883247, Accuracy: 78.54%\n",
      "Batch 177, Loss: 0.992039, Accuracy: 78.51%\n",
      "Batch 178, Loss: 0.901108, Accuracy: 78.55%\n",
      "Batch 179, Loss: 0.864881, Accuracy: 78.61%\n",
      "Batch 180, Loss: 0.992132, Accuracy: 78.59%\n",
      "Batch 181, Loss: 0.950265, Accuracy: 78.61%\n",
      "Batch 182, Loss: 0.915123, Accuracy: 78.63%\n",
      "Batch 183, Loss: 0.984560, Accuracy: 78.63%\n",
      "Batch 184, Loss: 0.868500, Accuracy: 78.69%\n",
      "Batch 185, Loss: 1.010330, Accuracy: 78.67%\n",
      "Batch 186, Loss: 0.846545, Accuracy: 78.73%\n",
      "Batch 187, Loss: 0.882783, Accuracy: 78.78%\n",
      "Batch 188, Loss: 0.998895, Accuracy: 78.76%\n",
      "Batch 189, Loss: 0.975123, Accuracy: 78.75%\n",
      "Batch 190, Loss: 0.903980, Accuracy: 78.78%\n",
      "Batch 191, Loss: 0.948527, Accuracy: 78.79%\n",
      "Batch 192, Loss: 0.924582, Accuracy: 78.80%\n",
      "Batch 193, Loss: 1.021258, Accuracy: 78.77%\n",
      "Batch 194, Loss: 0.935239, Accuracy: 78.79%\n",
      "Batch 195, Loss: 0.893906, Accuracy: 78.81%\n",
      "Batch 196, Loss: 0.840712, Accuracy: 78.87%\n",
      "Batch 197, Loss: 1.008230, Accuracy: 78.85%\n",
      "Batch 198, Loss: 1.032992, Accuracy: 78.81%\n",
      "Batch 199, Loss: 0.955825, Accuracy: 78.81%\n",
      "Batch 200, Loss: 0.985638, Accuracy: 78.80%\n",
      "Batch 201, Loss: 0.933749, Accuracy: 78.82%\n",
      "Batch 202, Loss: 0.977822, Accuracy: 78.81%\n",
      "Batch 203, Loss: 0.979902, Accuracy: 78.80%\n",
      "Batch 204, Loss: 0.953993, Accuracy: 78.81%\n",
      "Batch 205, Loss: 0.956609, Accuracy: 78.80%\n",
      "Batch 206, Loss: 0.902814, Accuracy: 78.82%\n",
      "Batch 207, Loss: 0.990563, Accuracy: 78.80%\n",
      "Batch 208, Loss: 0.890937, Accuracy: 78.84%\n",
      "Batch 209, Loss: 0.992499, Accuracy: 78.82%\n",
      "Batch 210, Loss: 1.064210, Accuracy: 78.76%\n",
      "Batch 211, Loss: 0.994586, Accuracy: 78.75%\n",
      "Batch 212, Loss: 0.906457, Accuracy: 78.77%\n",
      "Batch 213, Loss: 0.982415, Accuracy: 78.76%\n",
      "Training - Epoch 99, Loss: 0.954346, Accuracy: 78.76%\n",
      "Validation Batch 1, Loss: 0.899851, Accuracy: 84.38%\n",
      "Validation Batch 2, Loss: 0.939795, Accuracy: 82.03%\n",
      "Validation Batch 3, Loss: 1.002370, Accuracy: 79.17%\n",
      "Validation Batch 4, Loss: 0.935798, Accuracy: 79.69%\n",
      "Validation Batch 5, Loss: 0.912370, Accuracy: 80.62%\n",
      "Validation Batch 6, Loss: 0.884253, Accuracy: 80.99%\n",
      "Validation Batch 7, Loss: 0.962682, Accuracy: 80.58%\n",
      "Validation Batch 8, Loss: 0.986397, Accuracy: 79.88%\n",
      "Validation Batch 9, Loss: 0.990986, Accuracy: 79.34%\n",
      "Validation Batch 10, Loss: 0.980726, Accuracy: 78.91%\n",
      "Validation Batch 11, Loss: 0.914713, Accuracy: 79.26%\n",
      "Validation Batch 12, Loss: 0.889136, Accuracy: 79.82%\n",
      "Validation Batch 13, Loss: 0.994431, Accuracy: 79.45%\n",
      "Validation Batch 14, Loss: 0.975042, Accuracy: 79.13%\n",
      "Validation Batch 15, Loss: 0.945086, Accuracy: 79.17%\n",
      "Validation Batch 16, Loss: 0.910194, Accuracy: 79.49%\n",
      "Validation Batch 17, Loss: 0.968611, Accuracy: 79.32%\n",
      "Validation Batch 18, Loss: 0.926052, Accuracy: 79.51%\n",
      "Validation Batch 19, Loss: 0.972615, Accuracy: 79.36%\n",
      "Validation Batch 20, Loss: 0.921706, Accuracy: 79.53%\n",
      "Validation Batch 21, Loss: 0.951871, Accuracy: 79.54%\n",
      "Validation Batch 22, Loss: 0.974132, Accuracy: 79.33%\n",
      "Validation Batch 23, Loss: 1.021640, Accuracy: 78.94%\n",
      "Validation Batch 24, Loss: 1.015695, Accuracy: 78.65%\n",
      "Validation Batch 25, Loss: 0.932715, Accuracy: 78.81%\n",
      "Validation Batch 26, Loss: 0.947949, Accuracy: 78.79%\n",
      "Validation Batch 27, Loss: 0.899668, Accuracy: 78.92%\n",
      "Validation - Epoch 99, Loss: 0.950240, Accuracy: 78.92%\n",
      "Patienceâ€”7\n",
      "Epoch 100\n",
      "Batch 1, Loss: 0.971044, Accuracy: 76.56%\n",
      "Batch 2, Loss: 0.949959, Accuracy: 78.12%\n",
      "Batch 3, Loss: 0.942383, Accuracy: 78.12%\n",
      "Batch 4, Loss: 1.014241, Accuracy: 76.17%\n",
      "Batch 5, Loss: 0.904923, Accuracy: 77.81%\n",
      "Batch 6, Loss: 1.000879, Accuracy: 77.08%\n",
      "Batch 7, Loss: 0.944036, Accuracy: 77.68%\n",
      "Batch 8, Loss: 0.963832, Accuracy: 77.73%\n",
      "Batch 9, Loss: 0.893560, Accuracy: 78.47%\n",
      "Batch 10, Loss: 0.993794, Accuracy: 78.28%\n",
      "Batch 11, Loss: 0.996921, Accuracy: 77.98%\n",
      "Batch 12, Loss: 0.923083, Accuracy: 78.12%\n",
      "Batch 13, Loss: 0.943744, Accuracy: 78.37%\n",
      "Batch 14, Loss: 0.942825, Accuracy: 78.46%\n",
      "Batch 15, Loss: 0.966108, Accuracy: 78.44%\n",
      "Batch 16, Loss: 0.911659, Accuracy: 78.71%\n",
      "Batch 17, Loss: 0.964334, Accuracy: 78.68%\n",
      "Batch 18, Loss: 0.989107, Accuracy: 78.56%\n",
      "Batch 19, Loss: 0.996210, Accuracy: 78.21%\n",
      "Batch 20, Loss: 0.920939, Accuracy: 78.44%\n",
      "Batch 21, Loss: 0.902203, Accuracy: 78.65%\n",
      "Batch 22, Loss: 1.043889, Accuracy: 78.34%\n",
      "Batch 23, Loss: 0.934276, Accuracy: 78.53%\n",
      "Batch 24, Loss: 0.975652, Accuracy: 78.39%\n",
      "Batch 25, Loss: 0.957909, Accuracy: 78.44%\n",
      "Batch 26, Loss: 0.969250, Accuracy: 78.43%\n",
      "Batch 27, Loss: 0.929175, Accuracy: 78.59%\n",
      "Batch 28, Loss: 0.995717, Accuracy: 78.35%\n",
      "Batch 29, Loss: 0.979651, Accuracy: 78.29%\n",
      "Batch 30, Loss: 0.935329, Accuracy: 78.39%\n",
      "Batch 31, Loss: 0.903802, Accuracy: 78.53%\n",
      "Batch 32, Loss: 0.928709, Accuracy: 78.61%\n",
      "Batch 33, Loss: 0.944668, Accuracy: 78.55%\n",
      "Batch 34, Loss: 0.918681, Accuracy: 78.72%\n",
      "Batch 35, Loss: 0.950315, Accuracy: 78.66%\n",
      "Batch 36, Loss: 0.897194, Accuracy: 78.86%\n",
      "Batch 37, Loss: 0.900880, Accuracy: 79.05%\n",
      "Batch 38, Loss: 0.880530, Accuracy: 79.24%\n",
      "Batch 39, Loss: 0.867746, Accuracy: 79.45%\n",
      "Batch 40, Loss: 0.962976, Accuracy: 79.45%\n",
      "Batch 41, Loss: 1.006596, Accuracy: 79.27%\n",
      "Batch 42, Loss: 0.949299, Accuracy: 79.32%\n",
      "Batch 43, Loss: 0.928003, Accuracy: 79.36%\n",
      "Batch 44, Loss: 0.941703, Accuracy: 79.37%\n",
      "Batch 45, Loss: 0.972396, Accuracy: 79.31%\n",
      "Batch 46, Loss: 0.986090, Accuracy: 79.21%\n",
      "Batch 47, Loss: 0.959937, Accuracy: 79.19%\n",
      "Batch 48, Loss: 0.963951, Accuracy: 79.17%\n",
      "Batch 49, Loss: 1.012802, Accuracy: 79.05%\n",
      "Batch 50, Loss: 0.987329, Accuracy: 78.94%\n",
      "Batch 51, Loss: 0.933399, Accuracy: 78.98%\n",
      "Batch 52, Loss: 1.022825, Accuracy: 78.85%\n",
      "Batch 53, Loss: 1.024715, Accuracy: 78.66%\n",
      "Batch 54, Loss: 1.000187, Accuracy: 78.62%\n",
      "Batch 55, Loss: 0.881284, Accuracy: 78.75%\n",
      "Batch 56, Loss: 0.920918, Accuracy: 78.77%\n",
      "Batch 57, Loss: 0.967694, Accuracy: 78.73%\n",
      "Batch 58, Loss: 0.878719, Accuracy: 78.83%\n",
      "Batch 59, Loss: 0.989734, Accuracy: 78.76%\n",
      "Batch 60, Loss: 0.984967, Accuracy: 78.70%\n",
      "Batch 61, Loss: 0.909001, Accuracy: 78.79%\n",
      "Batch 62, Loss: 0.936493, Accuracy: 78.83%\n",
      "Batch 63, Loss: 0.977574, Accuracy: 78.79%\n",
      "Batch 64, Loss: 1.048326, Accuracy: 78.64%\n",
      "Batch 65, Loss: 1.016121, Accuracy: 78.58%\n",
      "Batch 66, Loss: 0.910377, Accuracy: 78.65%\n",
      "Batch 67, Loss: 0.970007, Accuracy: 78.61%\n",
      "Batch 68, Loss: 0.909079, Accuracy: 78.65%\n",
      "Batch 69, Loss: 1.019892, Accuracy: 78.56%\n",
      "Batch 70, Loss: 0.945812, Accuracy: 78.59%\n",
      "Batch 71, Loss: 0.930541, Accuracy: 78.63%\n",
      "Batch 72, Loss: 0.993502, Accuracy: 78.60%\n",
      "Batch 73, Loss: 1.061543, Accuracy: 78.47%\n",
      "Batch 74, Loss: 0.989143, Accuracy: 78.42%\n",
      "Batch 75, Loss: 0.989616, Accuracy: 78.38%\n",
      "Batch 76, Loss: 0.938406, Accuracy: 78.41%\n",
      "Batch 77, Loss: 0.978234, Accuracy: 78.39%\n",
      "Batch 78, Loss: 0.903965, Accuracy: 78.47%\n",
      "Batch 79, Loss: 0.968047, Accuracy: 78.46%\n",
      "Batch 80, Loss: 0.957860, Accuracy: 78.48%\n",
      "Batch 81, Loss: 0.912744, Accuracy: 78.55%\n",
      "Batch 82, Loss: 0.971349, Accuracy: 78.54%\n",
      "Batch 83, Loss: 0.875798, Accuracy: 78.61%\n",
      "Batch 84, Loss: 0.930294, Accuracy: 78.66%\n",
      "Batch 85, Loss: 0.947987, Accuracy: 78.68%\n",
      "Batch 86, Loss: 0.904148, Accuracy: 78.76%\n",
      "Batch 87, Loss: 0.902477, Accuracy: 78.84%\n",
      "Batch 88, Loss: 0.968781, Accuracy: 78.84%\n",
      "Batch 89, Loss: 1.006104, Accuracy: 78.79%\n",
      "Batch 90, Loss: 0.975116, Accuracy: 78.77%\n",
      "Batch 91, Loss: 0.922769, Accuracy: 78.79%\n",
      "Batch 92, Loss: 0.915524, Accuracy: 78.84%\n",
      "Batch 93, Loss: 0.946717, Accuracy: 78.83%\n",
      "Batch 94, Loss: 0.914315, Accuracy: 78.89%\n",
      "Batch 95, Loss: 0.959449, Accuracy: 78.88%\n",
      "Batch 96, Loss: 0.928855, Accuracy: 78.92%\n",
      "Batch 97, Loss: 0.933924, Accuracy: 78.95%\n",
      "Batch 98, Loss: 0.957528, Accuracy: 78.92%\n",
      "Batch 99, Loss: 0.892498, Accuracy: 78.99%\n",
      "Batch 100, Loss: 0.896453, Accuracy: 79.06%\n",
      "Batch 101, Loss: 0.877098, Accuracy: 79.15%\n",
      "Batch 102, Loss: 1.044462, Accuracy: 79.07%\n",
      "Batch 103, Loss: 0.926294, Accuracy: 79.10%\n",
      "Batch 104, Loss: 0.935697, Accuracy: 79.13%\n",
      "Batch 105, Loss: 0.874073, Accuracy: 79.21%\n",
      "Batch 106, Loss: 1.022148, Accuracy: 79.13%\n",
      "Batch 107, Loss: 0.923117, Accuracy: 79.15%\n",
      "Batch 108, Loss: 0.996413, Accuracy: 79.12%\n",
      "Batch 109, Loss: 0.920807, Accuracy: 79.16%\n",
      "Batch 110, Loss: 0.901809, Accuracy: 79.20%\n",
      "Batch 111, Loss: 0.954550, Accuracy: 79.19%\n",
      "Batch 112, Loss: 0.982642, Accuracy: 79.16%\n",
      "Batch 113, Loss: 0.914746, Accuracy: 79.18%\n",
      "Batch 114, Loss: 0.960626, Accuracy: 79.17%\n",
      "Batch 115, Loss: 0.986872, Accuracy: 79.14%\n",
      "Batch 116, Loss: 0.942785, Accuracy: 79.14%\n",
      "Batch 117, Loss: 1.065921, Accuracy: 79.05%\n",
      "Batch 118, Loss: 0.929338, Accuracy: 79.05%\n",
      "Batch 119, Loss: 0.926483, Accuracy: 79.07%\n",
      "Batch 120, Loss: 1.003950, Accuracy: 79.01%\n",
      "Batch 121, Loss: 0.968714, Accuracy: 78.99%\n",
      "Batch 122, Loss: 0.896050, Accuracy: 79.03%\n",
      "Batch 123, Loss: 0.958353, Accuracy: 79.04%\n",
      "Batch 124, Loss: 0.935212, Accuracy: 79.04%\n",
      "Batch 125, Loss: 0.963860, Accuracy: 79.04%\n",
      "Batch 126, Loss: 0.907931, Accuracy: 79.07%\n",
      "Batch 127, Loss: 1.022467, Accuracy: 79.01%\n",
      "Batch 128, Loss: 1.009284, Accuracy: 78.97%\n",
      "Batch 129, Loss: 0.891317, Accuracy: 79.02%\n",
      "Batch 130, Loss: 1.014930, Accuracy: 78.97%\n",
      "Batch 131, Loss: 0.914630, Accuracy: 78.98%\n",
      "Batch 132, Loss: 0.952514, Accuracy: 79.00%\n",
      "Batch 133, Loss: 1.009102, Accuracy: 78.96%\n",
      "Batch 134, Loss: 0.985435, Accuracy: 78.93%\n",
      "Batch 135, Loss: 0.996479, Accuracy: 78.90%\n",
      "Batch 136, Loss: 1.005789, Accuracy: 78.86%\n",
      "Batch 137, Loss: 0.981804, Accuracy: 78.83%\n",
      "Batch 138, Loss: 1.016310, Accuracy: 78.79%\n",
      "Batch 139, Loss: 0.908339, Accuracy: 78.82%\n",
      "Batch 140, Loss: 0.953592, Accuracy: 78.84%\n",
      "Batch 141, Loss: 0.986643, Accuracy: 78.80%\n",
      "Batch 142, Loss: 0.868490, Accuracy: 78.85%\n",
      "Batch 143, Loss: 0.969971, Accuracy: 78.85%\n",
      "Batch 144, Loss: 0.903131, Accuracy: 78.90%\n",
      "Batch 145, Loss: 0.960359, Accuracy: 78.90%\n",
      "Batch 146, Loss: 0.933668, Accuracy: 78.92%\n",
      "Batch 147, Loss: 0.973335, Accuracy: 78.90%\n",
      "Batch 148, Loss: 0.982344, Accuracy: 78.89%\n",
      "Batch 149, Loss: 0.911744, Accuracy: 78.90%\n",
      "Batch 150, Loss: 1.019424, Accuracy: 78.86%\n",
      "Batch 151, Loss: 0.906213, Accuracy: 78.89%\n",
      "Batch 152, Loss: 0.901786, Accuracy: 78.93%\n",
      "Batch 153, Loss: 0.921757, Accuracy: 78.95%\n",
      "Batch 154, Loss: 0.941886, Accuracy: 78.98%\n",
      "Batch 155, Loss: 0.987128, Accuracy: 78.95%\n",
      "Batch 156, Loss: 0.904469, Accuracy: 78.99%\n",
      "Batch 157, Loss: 0.827870, Accuracy: 79.08%\n",
      "Batch 158, Loss: 0.947941, Accuracy: 79.09%\n",
      "Batch 159, Loss: 0.933484, Accuracy: 79.11%\n",
      "Batch 160, Loss: 0.990373, Accuracy: 79.08%\n",
      "Batch 161, Loss: 0.979334, Accuracy: 79.07%\n",
      "Batch 162, Loss: 0.965877, Accuracy: 79.06%\n",
      "Batch 163, Loss: 0.898725, Accuracy: 79.09%\n",
      "Batch 164, Loss: 0.974033, Accuracy: 79.08%\n",
      "Batch 165, Loss: 0.993316, Accuracy: 79.03%\n",
      "Batch 166, Loss: 0.996305, Accuracy: 79.00%\n",
      "Batch 167, Loss: 1.002464, Accuracy: 78.98%\n",
      "Batch 168, Loss: 0.984487, Accuracy: 78.94%\n",
      "Batch 169, Loss: 0.962575, Accuracy: 78.93%\n",
      "Batch 170, Loss: 1.026925, Accuracy: 78.89%\n",
      "Batch 171, Loss: 0.942640, Accuracy: 78.88%\n",
      "Batch 172, Loss: 0.933881, Accuracy: 78.89%\n",
      "Batch 173, Loss: 0.929367, Accuracy: 78.90%\n",
      "Batch 174, Loss: 0.890590, Accuracy: 78.93%\n",
      "Batch 175, Loss: 0.941114, Accuracy: 78.95%\n",
      "Batch 176, Loss: 0.907506, Accuracy: 78.98%\n",
      "Batch 177, Loss: 0.985822, Accuracy: 78.95%\n",
      "Batch 178, Loss: 0.942342, Accuracy: 78.97%\n",
      "Batch 179, Loss: 1.000103, Accuracy: 78.95%\n",
      "Batch 180, Loss: 0.921995, Accuracy: 78.96%\n",
      "Batch 181, Loss: 0.941685, Accuracy: 78.96%\n",
      "Batch 182, Loss: 0.976362, Accuracy: 78.96%\n",
      "Batch 183, Loss: 0.890679, Accuracy: 79.00%\n",
      "Batch 184, Loss: 0.948487, Accuracy: 79.01%\n",
      "Batch 185, Loss: 0.933870, Accuracy: 79.02%\n",
      "Batch 186, Loss: 0.999409, Accuracy: 78.99%\n",
      "Batch 187, Loss: 0.980457, Accuracy: 78.98%\n",
      "Batch 188, Loss: 0.967358, Accuracy: 78.97%\n",
      "Batch 189, Loss: 0.952980, Accuracy: 78.98%\n",
      "Batch 190, Loss: 0.923630, Accuracy: 78.99%\n",
      "Batch 191, Loss: 0.903543, Accuracy: 79.02%\n",
      "Batch 192, Loss: 1.078753, Accuracy: 78.96%\n",
      "Batch 193, Loss: 0.928324, Accuracy: 78.97%\n",
      "Batch 194, Loss: 0.916872, Accuracy: 78.99%\n",
      "Batch 195, Loss: 0.921626, Accuracy: 79.01%\n",
      "Batch 196, Loss: 0.944996, Accuracy: 79.01%\n",
      "Batch 197, Loss: 0.937324, Accuracy: 79.02%\n",
      "Batch 198, Loss: 0.988278, Accuracy: 79.00%\n",
      "Batch 199, Loss: 0.878564, Accuracy: 79.04%\n",
      "Batch 200, Loss: 0.946583, Accuracy: 79.03%\n",
      "Batch 201, Loss: 0.901567, Accuracy: 79.06%\n",
      "Batch 202, Loss: 0.968020, Accuracy: 79.05%\n",
      "Batch 203, Loss: 0.967999, Accuracy: 79.03%\n",
      "Batch 204, Loss: 1.020203, Accuracy: 79.01%\n",
      "Batch 205, Loss: 0.894950, Accuracy: 79.04%\n",
      "Batch 206, Loss: 0.901578, Accuracy: 79.07%\n",
      "Batch 207, Loss: 0.873658, Accuracy: 79.11%\n",
      "Batch 208, Loss: 0.950954, Accuracy: 79.12%\n",
      "Batch 209, Loss: 0.984069, Accuracy: 79.09%\n",
      "Batch 210, Loss: 0.897563, Accuracy: 79.11%\n",
      "Batch 211, Loss: 0.937323, Accuracy: 79.12%\n",
      "Batch 212, Loss: 0.948356, Accuracy: 79.12%\n",
      "Batch 213, Loss: 0.940428, Accuracy: 79.13%\n",
      "Training - Epoch 100, Loss: 0.951741, Accuracy: 79.13%\n",
      "Validation Batch 1, Loss: 0.897852, Accuracy: 84.38%\n",
      "Validation Batch 2, Loss: 0.902198, Accuracy: 85.16%\n",
      "Validation Batch 3, Loss: 0.995596, Accuracy: 81.25%\n",
      "Validation Batch 4, Loss: 0.934094, Accuracy: 81.25%\n",
      "Validation Batch 5, Loss: 0.905463, Accuracy: 81.88%\n",
      "Validation Batch 6, Loss: 0.838264, Accuracy: 83.33%\n",
      "Validation Batch 7, Loss: 0.939297, Accuracy: 82.81%\n",
      "Validation Batch 8, Loss: 0.932350, Accuracy: 82.81%\n",
      "Validation Batch 9, Loss: 0.959215, Accuracy: 82.29%\n",
      "Validation Batch 10, Loss: 0.950540, Accuracy: 82.03%\n",
      "Validation Batch 11, Loss: 0.899941, Accuracy: 82.24%\n",
      "Validation Batch 12, Loss: 0.880549, Accuracy: 82.55%\n",
      "Validation Batch 13, Loss: 0.950204, Accuracy: 82.33%\n",
      "Validation Batch 14, Loss: 0.941929, Accuracy: 82.37%\n",
      "Validation Batch 15, Loss: 0.954029, Accuracy: 82.08%\n",
      "Validation Batch 16, Loss: 0.901539, Accuracy: 82.23%\n",
      "Validation Batch 17, Loss: 0.939394, Accuracy: 82.17%\n",
      "Validation Batch 18, Loss: 0.927491, Accuracy: 82.03%\n",
      "Validation Batch 19, Loss: 0.958908, Accuracy: 81.74%\n",
      "Validation Batch 20, Loss: 0.905821, Accuracy: 81.80%\n",
      "Validation Batch 21, Loss: 0.942554, Accuracy: 81.70%\n",
      "Validation Batch 22, Loss: 0.924485, Accuracy: 81.68%\n",
      "Validation Batch 23, Loss: 0.988055, Accuracy: 81.32%\n",
      "Validation Batch 24, Loss: 0.997655, Accuracy: 80.99%\n",
      "Validation Batch 25, Loss: 0.914009, Accuracy: 81.00%\n",
      "Validation Batch 26, Loss: 0.944744, Accuracy: 80.95%\n",
      "Validation Batch 27, Loss: 0.864879, Accuracy: 81.09%\n",
      "Validation - Epoch 100, Loss: 0.929298, Accuracy: 81.09%\n",
      "Patienceâ€”8\n",
      "Epoch 101\n",
      "Batch 1, Loss: 1.044501, Accuracy: 68.75%\n",
      "Batch 2, Loss: 1.002042, Accuracy: 70.31%\n",
      "Batch 3, Loss: 0.884018, Accuracy: 76.04%\n",
      "Batch 4, Loss: 0.938408, Accuracy: 76.95%\n",
      "Batch 5, Loss: 0.956488, Accuracy: 77.50%\n",
      "Batch 6, Loss: 0.913874, Accuracy: 78.39%\n",
      "Batch 7, Loss: 1.000551, Accuracy: 77.46%\n",
      "Batch 8, Loss: 0.911817, Accuracy: 78.12%\n",
      "Batch 9, Loss: 0.934503, Accuracy: 78.47%\n",
      "Batch 10, Loss: 0.978667, Accuracy: 78.12%\n",
      "Batch 11, Loss: 0.936834, Accuracy: 78.41%\n",
      "Batch 12, Loss: 0.865412, Accuracy: 79.30%\n",
      "Batch 13, Loss: 0.914814, Accuracy: 79.57%\n",
      "Batch 14, Loss: 1.012627, Accuracy: 79.02%\n",
      "Batch 15, Loss: 0.955665, Accuracy: 79.06%\n",
      "Batch 16, Loss: 0.902168, Accuracy: 79.39%\n",
      "Batch 17, Loss: 0.987386, Accuracy: 79.14%\n",
      "Batch 18, Loss: 0.874060, Accuracy: 79.51%\n",
      "Batch 19, Loss: 0.913533, Accuracy: 79.69%\n",
      "Batch 20, Loss: 0.987403, Accuracy: 79.45%\n",
      "Batch 21, Loss: 0.913744, Accuracy: 79.61%\n",
      "Batch 22, Loss: 0.987417, Accuracy: 79.33%\n",
      "Batch 23, Loss: 0.910204, Accuracy: 79.42%\n",
      "Batch 24, Loss: 0.930291, Accuracy: 79.56%\n",
      "Batch 25, Loss: 0.890701, Accuracy: 79.81%\n",
      "Batch 26, Loss: 1.002630, Accuracy: 79.57%\n",
      "Batch 27, Loss: 0.956448, Accuracy: 79.51%\n",
      "Batch 28, Loss: 0.977233, Accuracy: 79.41%\n",
      "Batch 29, Loss: 0.918290, Accuracy: 79.58%\n",
      "Batch 30, Loss: 0.900007, Accuracy: 79.74%\n",
      "Batch 31, Loss: 0.956531, Accuracy: 79.69%\n",
      "Batch 32, Loss: 0.949180, Accuracy: 79.64%\n",
      "Batch 33, Loss: 0.957581, Accuracy: 79.55%\n",
      "Batch 34, Loss: 0.971756, Accuracy: 79.46%\n",
      "Batch 35, Loss: 0.917347, Accuracy: 79.55%\n",
      "Batch 36, Loss: 0.952233, Accuracy: 79.56%\n",
      "Batch 37, Loss: 0.984472, Accuracy: 79.48%\n",
      "Batch 38, Loss: 0.927470, Accuracy: 79.44%\n",
      "Batch 39, Loss: 0.883383, Accuracy: 79.61%\n",
      "Batch 40, Loss: 0.951732, Accuracy: 79.57%\n",
      "Batch 41, Loss: 0.952051, Accuracy: 79.57%\n",
      "Batch 42, Loss: 1.030673, Accuracy: 79.35%\n",
      "Batch 43, Loss: 0.886491, Accuracy: 79.47%\n",
      "Batch 44, Loss: 1.009384, Accuracy: 79.33%\n",
      "Batch 45, Loss: 0.932250, Accuracy: 79.41%\n",
      "Batch 46, Loss: 1.035279, Accuracy: 79.18%\n",
      "Batch 47, Loss: 0.849566, Accuracy: 79.39%\n",
      "Batch 48, Loss: 0.828387, Accuracy: 79.65%\n",
      "Batch 49, Loss: 0.885102, Accuracy: 79.78%\n",
      "Batch 50, Loss: 0.986929, Accuracy: 79.69%\n",
      "Batch 51, Loss: 0.997570, Accuracy: 79.53%\n",
      "Batch 52, Loss: 0.954970, Accuracy: 79.51%\n",
      "Batch 53, Loss: 0.923750, Accuracy: 79.54%\n",
      "Batch 54, Loss: 0.837439, Accuracy: 79.75%\n",
      "Batch 55, Loss: 0.932554, Accuracy: 79.80%\n",
      "Batch 56, Loss: 1.038099, Accuracy: 79.66%\n",
      "Batch 57, Loss: 0.977555, Accuracy: 79.61%\n",
      "Batch 58, Loss: 0.889417, Accuracy: 79.71%\n",
      "Batch 59, Loss: 0.972028, Accuracy: 79.69%\n",
      "Batch 60, Loss: 1.061802, Accuracy: 79.48%\n",
      "Batch 61, Loss: 0.951299, Accuracy: 79.46%\n",
      "Batch 62, Loss: 1.065904, Accuracy: 79.23%\n",
      "Batch 63, Loss: 0.976388, Accuracy: 79.24%\n",
      "Batch 64, Loss: 0.952121, Accuracy: 79.22%\n",
      "Batch 65, Loss: 0.883921, Accuracy: 79.33%\n",
      "Batch 66, Loss: 0.977079, Accuracy: 79.29%\n",
      "Batch 67, Loss: 1.060977, Accuracy: 79.10%\n",
      "Batch 68, Loss: 0.967581, Accuracy: 79.09%\n",
      "Batch 69, Loss: 0.960412, Accuracy: 79.08%\n",
      "Batch 70, Loss: 0.935048, Accuracy: 79.11%\n",
      "Batch 71, Loss: 0.974513, Accuracy: 79.09%\n",
      "Batch 72, Loss: 0.967909, Accuracy: 79.08%\n",
      "Batch 73, Loss: 0.971401, Accuracy: 79.07%\n",
      "Batch 74, Loss: 0.974626, Accuracy: 79.03%\n",
      "Batch 75, Loss: 0.967669, Accuracy: 79.00%\n",
      "Batch 76, Loss: 0.973803, Accuracy: 78.97%\n",
      "Batch 77, Loss: 0.844201, Accuracy: 79.12%\n",
      "Batch 78, Loss: 0.970703, Accuracy: 79.11%\n",
      "Batch 79, Loss: 0.989121, Accuracy: 79.07%\n",
      "Batch 80, Loss: 0.948392, Accuracy: 79.08%\n",
      "Batch 81, Loss: 0.877351, Accuracy: 79.19%\n",
      "Batch 82, Loss: 0.908612, Accuracy: 79.23%\n",
      "Batch 83, Loss: 0.874834, Accuracy: 79.33%\n",
      "Batch 84, Loss: 0.944186, Accuracy: 79.33%\n",
      "Batch 85, Loss: 0.858884, Accuracy: 79.45%\n",
      "Batch 86, Loss: 0.923891, Accuracy: 79.47%\n",
      "Batch 87, Loss: 0.937598, Accuracy: 79.47%\n",
      "Batch 88, Loss: 0.898425, Accuracy: 79.55%\n",
      "Batch 89, Loss: 0.965102, Accuracy: 79.53%\n",
      "Batch 90, Loss: 0.990791, Accuracy: 79.50%\n",
      "Batch 91, Loss: 0.991772, Accuracy: 79.43%\n",
      "Batch 92, Loss: 0.947135, Accuracy: 79.43%\n",
      "Batch 93, Loss: 0.934762, Accuracy: 79.44%\n",
      "Batch 94, Loss: 0.980254, Accuracy: 79.40%\n",
      "Batch 95, Loss: 0.969177, Accuracy: 79.39%\n",
      "Batch 96, Loss: 1.018971, Accuracy: 79.31%\n",
      "Batch 97, Loss: 0.990553, Accuracy: 79.27%\n",
      "Batch 98, Loss: 0.871432, Accuracy: 79.37%\n",
      "Batch 99, Loss: 0.951998, Accuracy: 79.39%\n",
      "Batch 100, Loss: 0.933877, Accuracy: 79.42%\n",
      "Batch 101, Loss: 0.908746, Accuracy: 79.46%\n",
      "Batch 102, Loss: 0.972717, Accuracy: 79.43%\n",
      "Batch 103, Loss: 0.942931, Accuracy: 79.43%\n",
      "Batch 104, Loss: 1.037593, Accuracy: 79.34%\n",
      "Batch 105, Loss: 0.919479, Accuracy: 79.35%\n",
      "Batch 106, Loss: 0.971799, Accuracy: 79.32%\n",
      "Batch 107, Loss: 0.982480, Accuracy: 79.28%\n",
      "Batch 108, Loss: 0.928208, Accuracy: 79.28%\n",
      "Batch 109, Loss: 0.915218, Accuracy: 79.29%\n",
      "Batch 110, Loss: 0.946367, Accuracy: 79.28%\n",
      "Batch 111, Loss: 0.911567, Accuracy: 79.31%\n",
      "Batch 112, Loss: 1.014562, Accuracy: 79.24%\n",
      "Batch 113, Loss: 0.905033, Accuracy: 79.27%\n",
      "Batch 114, Loss: 0.914453, Accuracy: 79.30%\n",
      "Batch 115, Loss: 1.044434, Accuracy: 79.21%\n",
      "Batch 116, Loss: 0.964909, Accuracy: 79.19%\n",
      "Batch 117, Loss: 0.981288, Accuracy: 79.17%\n",
      "Batch 118, Loss: 0.929226, Accuracy: 79.18%\n",
      "Batch 119, Loss: 0.909006, Accuracy: 79.21%\n",
      "Batch 120, Loss: 0.973885, Accuracy: 79.21%\n",
      "Batch 121, Loss: 0.944115, Accuracy: 79.21%\n",
      "Batch 122, Loss: 0.986589, Accuracy: 79.18%\n",
      "Batch 123, Loss: 1.046103, Accuracy: 79.10%\n",
      "Batch 124, Loss: 0.970409, Accuracy: 79.10%\n",
      "Batch 125, Loss: 0.934800, Accuracy: 79.11%\n",
      "Batch 126, Loss: 0.948554, Accuracy: 79.10%\n",
      "Batch 127, Loss: 1.024193, Accuracy: 79.04%\n",
      "Batch 128, Loss: 1.005997, Accuracy: 78.99%\n",
      "Batch 129, Loss: 0.928945, Accuracy: 79.01%\n",
      "Batch 130, Loss: 0.855526, Accuracy: 79.07%\n",
      "Batch 131, Loss: 0.972214, Accuracy: 79.06%\n",
      "Batch 132, Loss: 0.924954, Accuracy: 79.08%\n",
      "Batch 133, Loss: 0.984775, Accuracy: 79.06%\n",
      "Batch 134, Loss: 0.992084, Accuracy: 79.02%\n",
      "Batch 135, Loss: 1.019803, Accuracy: 78.96%\n",
      "Batch 136, Loss: 1.044733, Accuracy: 78.89%\n",
      "Batch 137, Loss: 0.932131, Accuracy: 78.91%\n",
      "Batch 138, Loss: 0.929721, Accuracy: 78.93%\n",
      "Batch 139, Loss: 0.953131, Accuracy: 78.93%\n",
      "Batch 140, Loss: 0.950201, Accuracy: 78.95%\n",
      "Batch 141, Loss: 1.003494, Accuracy: 78.92%\n",
      "Batch 142, Loss: 0.877594, Accuracy: 78.98%\n",
      "Batch 143, Loss: 0.928889, Accuracy: 79.00%\n",
      "Batch 144, Loss: 0.947652, Accuracy: 79.00%\n",
      "Batch 145, Loss: 0.924241, Accuracy: 79.03%\n",
      "Batch 146, Loss: 0.921191, Accuracy: 79.06%\n",
      "Batch 147, Loss: 0.906139, Accuracy: 79.10%\n",
      "Batch 148, Loss: 0.972914, Accuracy: 79.09%\n",
      "Batch 149, Loss: 0.954333, Accuracy: 79.08%\n",
      "Batch 150, Loss: 0.923410, Accuracy: 79.09%\n",
      "Batch 151, Loss: 0.923963, Accuracy: 79.12%\n",
      "Batch 152, Loss: 0.969415, Accuracy: 79.10%\n",
      "Batch 153, Loss: 0.959738, Accuracy: 79.11%\n",
      "Batch 154, Loss: 0.957913, Accuracy: 79.11%\n",
      "Batch 155, Loss: 0.987116, Accuracy: 79.09%\n",
      "Batch 156, Loss: 0.853305, Accuracy: 79.16%\n",
      "Batch 157, Loss: 0.962043, Accuracy: 79.16%\n",
      "Batch 158, Loss: 0.947603, Accuracy: 79.15%\n",
      "Batch 159, Loss: 0.946925, Accuracy: 79.16%\n",
      "Batch 160, Loss: 0.979403, Accuracy: 79.14%\n",
      "Batch 161, Loss: 1.003106, Accuracy: 79.11%\n",
      "Batch 162, Loss: 0.945561, Accuracy: 79.13%\n",
      "Batch 163, Loss: 0.901233, Accuracy: 79.16%\n",
      "Batch 164, Loss: 0.870036, Accuracy: 79.22%\n",
      "Batch 165, Loss: 0.943008, Accuracy: 79.22%\n",
      "Batch 166, Loss: 0.950812, Accuracy: 79.22%\n",
      "Batch 167, Loss: 0.901608, Accuracy: 79.24%\n",
      "Batch 168, Loss: 0.930826, Accuracy: 79.24%\n",
      "Batch 169, Loss: 0.943104, Accuracy: 79.24%\n",
      "Batch 170, Loss: 1.049193, Accuracy: 79.17%\n",
      "Batch 171, Loss: 0.898342, Accuracy: 79.20%\n",
      "Batch 172, Loss: 1.045309, Accuracy: 79.15%\n",
      "Batch 173, Loss: 0.897094, Accuracy: 79.18%\n",
      "Batch 174, Loss: 1.008716, Accuracy: 79.15%\n",
      "Batch 175, Loss: 1.074292, Accuracy: 79.07%\n",
      "Batch 176, Loss: 0.928304, Accuracy: 79.08%\n",
      "Batch 177, Loss: 0.916885, Accuracy: 79.10%\n",
      "Batch 178, Loss: 0.948344, Accuracy: 79.10%\n",
      "Batch 179, Loss: 0.947110, Accuracy: 79.10%\n",
      "Batch 180, Loss: 0.912758, Accuracy: 79.12%\n",
      "Batch 181, Loss: 0.951010, Accuracy: 79.12%\n",
      "Batch 182, Loss: 0.965066, Accuracy: 79.11%\n",
      "Batch 183, Loss: 0.880620, Accuracy: 79.15%\n",
      "Batch 184, Loss: 0.946986, Accuracy: 79.14%\n",
      "Batch 185, Loss: 0.922447, Accuracy: 79.16%\n",
      "Batch 186, Loss: 0.903485, Accuracy: 79.19%\n",
      "Batch 187, Loss: 0.955517, Accuracy: 79.19%\n",
      "Batch 188, Loss: 0.949959, Accuracy: 79.20%\n",
      "Batch 189, Loss: 0.940945, Accuracy: 79.20%\n",
      "Batch 190, Loss: 1.017881, Accuracy: 79.16%\n",
      "Batch 191, Loss: 0.944197, Accuracy: 79.16%\n",
      "Batch 192, Loss: 0.948216, Accuracy: 79.16%\n",
      "Batch 193, Loss: 0.959283, Accuracy: 79.16%\n",
      "Batch 194, Loss: 1.024567, Accuracy: 79.12%\n",
      "Batch 195, Loss: 0.921978, Accuracy: 79.14%\n",
      "Batch 196, Loss: 0.978766, Accuracy: 79.13%\n",
      "Batch 197, Loss: 0.944596, Accuracy: 79.14%\n",
      "Batch 198, Loss: 0.881400, Accuracy: 79.17%\n",
      "Batch 199, Loss: 0.958147, Accuracy: 79.16%\n",
      "Batch 200, Loss: 0.878638, Accuracy: 79.20%\n",
      "Batch 201, Loss: 0.928325, Accuracy: 79.21%\n",
      "Batch 202, Loss: 0.924958, Accuracy: 79.22%\n",
      "Batch 203, Loss: 0.922252, Accuracy: 79.24%\n",
      "Batch 204, Loss: 0.966043, Accuracy: 79.23%\n",
      "Batch 205, Loss: 0.952770, Accuracy: 79.23%\n",
      "Batch 206, Loss: 1.017500, Accuracy: 79.20%\n",
      "Batch 207, Loss: 1.048680, Accuracy: 79.16%\n",
      "Batch 208, Loss: 0.923243, Accuracy: 79.18%\n",
      "Batch 209, Loss: 0.897420, Accuracy: 79.20%\n",
      "Batch 210, Loss: 0.971612, Accuracy: 79.19%\n",
      "Batch 211, Loss: 0.928985, Accuracy: 79.20%\n",
      "Batch 212, Loss: 0.943664, Accuracy: 79.20%\n",
      "Batch 213, Loss: 0.955214, Accuracy: 79.20%\n",
      "Training - Epoch 101, Loss: 0.950036, Accuracy: 79.20%\n",
      "Validation Batch 1, Loss: 0.900774, Accuracy: 84.38%\n",
      "Validation Batch 2, Loss: 0.936338, Accuracy: 82.81%\n",
      "Validation Batch 3, Loss: 1.007450, Accuracy: 79.69%\n",
      "Validation Batch 4, Loss: 0.931713, Accuracy: 80.08%\n",
      "Validation Batch 5, Loss: 0.912849, Accuracy: 80.62%\n",
      "Validation Batch 6, Loss: 0.884583, Accuracy: 80.99%\n",
      "Validation Batch 7, Loss: 0.965422, Accuracy: 80.36%\n",
      "Validation Batch 8, Loss: 0.986103, Accuracy: 79.69%\n",
      "Validation Batch 9, Loss: 0.999758, Accuracy: 79.17%\n",
      "Validation Batch 10, Loss: 0.979992, Accuracy: 78.75%\n",
      "Validation Batch 11, Loss: 0.912842, Accuracy: 79.12%\n",
      "Validation Batch 12, Loss: 0.892060, Accuracy: 79.69%\n",
      "Validation Batch 13, Loss: 0.998499, Accuracy: 79.09%\n",
      "Validation Batch 14, Loss: 0.976555, Accuracy: 78.79%\n",
      "Validation Batch 15, Loss: 0.937842, Accuracy: 78.96%\n",
      "Validation Batch 16, Loss: 0.908163, Accuracy: 79.30%\n",
      "Validation Batch 17, Loss: 0.979032, Accuracy: 79.04%\n",
      "Validation Batch 18, Loss: 0.929243, Accuracy: 79.17%\n",
      "Validation Batch 19, Loss: 0.980743, Accuracy: 79.03%\n",
      "Validation Batch 20, Loss: 0.917571, Accuracy: 79.22%\n",
      "Validation Batch 21, Loss: 0.952050, Accuracy: 79.24%\n",
      "Validation Batch 22, Loss: 0.977240, Accuracy: 78.98%\n",
      "Validation Batch 23, Loss: 1.027704, Accuracy: 78.53%\n",
      "Validation Batch 24, Loss: 1.021485, Accuracy: 78.26%\n",
      "Validation Batch 25, Loss: 0.933542, Accuracy: 78.44%\n",
      "Validation Batch 26, Loss: 0.942236, Accuracy: 78.61%\n",
      "Validation Batch 27, Loss: 0.899083, Accuracy: 78.74%\n",
      "Validation - Epoch 101, Loss: 0.951514, Accuracy: 78.74%\n",
      "Patienceâ€”9\n",
      "Epoch 102\n",
      "Batch 1, Loss: 0.856161, Accuracy: 89.06%\n",
      "Batch 2, Loss: 0.932859, Accuracy: 84.38%\n",
      "Batch 3, Loss: 0.945999, Accuracy: 82.81%\n",
      "Batch 4, Loss: 0.920611, Accuracy: 82.81%\n",
      "Batch 5, Loss: 0.979396, Accuracy: 81.25%\n",
      "Batch 6, Loss: 0.876450, Accuracy: 82.03%\n",
      "Batch 7, Loss: 0.917974, Accuracy: 82.14%\n",
      "Batch 8, Loss: 1.002321, Accuracy: 81.05%\n",
      "Batch 9, Loss: 0.950457, Accuracy: 80.90%\n",
      "Batch 10, Loss: 0.919237, Accuracy: 81.09%\n",
      "Batch 11, Loss: 0.883772, Accuracy: 81.53%\n",
      "Batch 12, Loss: 0.936353, Accuracy: 81.51%\n",
      "Batch 13, Loss: 0.975573, Accuracy: 81.01%\n",
      "Batch 14, Loss: 0.904802, Accuracy: 81.25%\n",
      "Batch 15, Loss: 0.933956, Accuracy: 81.25%\n",
      "Batch 16, Loss: 0.888789, Accuracy: 81.54%\n",
      "Batch 17, Loss: 0.932736, Accuracy: 81.53%\n",
      "Batch 18, Loss: 1.053198, Accuracy: 80.82%\n",
      "Batch 19, Loss: 0.967091, Accuracy: 80.67%\n",
      "Batch 20, Loss: 0.942217, Accuracy: 80.62%\n",
      "Batch 21, Loss: 0.880443, Accuracy: 80.88%\n",
      "Batch 22, Loss: 0.942729, Accuracy: 80.82%\n",
      "Batch 23, Loss: 0.884584, Accuracy: 81.05%\n",
      "Batch 24, Loss: 0.945000, Accuracy: 81.05%\n",
      "Batch 25, Loss: 0.978311, Accuracy: 80.88%\n",
      "Batch 26, Loss: 0.965961, Accuracy: 80.77%\n",
      "Batch 27, Loss: 0.909939, Accuracy: 80.90%\n",
      "Batch 28, Loss: 0.925061, Accuracy: 80.92%\n",
      "Batch 29, Loss: 0.839323, Accuracy: 81.25%\n",
      "Batch 30, Loss: 0.984882, Accuracy: 81.04%\n",
      "Batch 31, Loss: 0.944209, Accuracy: 81.00%\n",
      "Batch 32, Loss: 0.928504, Accuracy: 81.05%\n",
      "Batch 33, Loss: 1.007814, Accuracy: 80.78%\n",
      "Batch 34, Loss: 1.023381, Accuracy: 80.56%\n",
      "Batch 35, Loss: 0.897094, Accuracy: 80.67%\n",
      "Batch 36, Loss: 0.932346, Accuracy: 80.73%\n",
      "Batch 37, Loss: 1.043120, Accuracy: 80.36%\n",
      "Batch 38, Loss: 1.023613, Accuracy: 80.14%\n",
      "Batch 39, Loss: 0.980187, Accuracy: 80.01%\n",
      "Batch 40, Loss: 0.982464, Accuracy: 79.92%\n",
      "Batch 41, Loss: 0.926931, Accuracy: 79.95%\n",
      "Batch 42, Loss: 0.857659, Accuracy: 80.17%\n",
      "Batch 43, Loss: 1.014906, Accuracy: 80.01%\n",
      "Batch 44, Loss: 0.929448, Accuracy: 80.08%\n",
      "Batch 45, Loss: 0.993598, Accuracy: 79.93%\n",
      "Batch 46, Loss: 0.948522, Accuracy: 79.96%\n",
      "Batch 47, Loss: 0.990194, Accuracy: 79.82%\n",
      "Batch 48, Loss: 0.974423, Accuracy: 79.79%\n",
      "Batch 49, Loss: 1.001337, Accuracy: 79.69%\n",
      "Batch 50, Loss: 0.944319, Accuracy: 79.69%\n",
      "Batch 51, Loss: 0.963726, Accuracy: 79.66%\n",
      "Batch 52, Loss: 1.039666, Accuracy: 79.45%\n",
      "Batch 53, Loss: 0.985827, Accuracy: 79.33%\n",
      "Batch 54, Loss: 0.902573, Accuracy: 79.43%\n",
      "Batch 55, Loss: 0.917444, Accuracy: 79.49%\n",
      "Batch 56, Loss: 1.029709, Accuracy: 79.32%\n",
      "Batch 57, Loss: 0.933980, Accuracy: 79.36%\n",
      "Batch 58, Loss: 0.978671, Accuracy: 79.31%\n",
      "Batch 59, Loss: 0.978865, Accuracy: 79.24%\n",
      "Batch 60, Loss: 0.996513, Accuracy: 79.17%\n",
      "Batch 61, Loss: 1.039611, Accuracy: 79.05%\n",
      "Batch 62, Loss: 0.947514, Accuracy: 79.08%\n",
      "Batch 63, Loss: 0.995262, Accuracy: 79.04%\n",
      "Batch 64, Loss: 0.909180, Accuracy: 79.13%\n",
      "Batch 65, Loss: 0.856832, Accuracy: 79.25%\n",
      "Batch 66, Loss: 0.998391, Accuracy: 79.17%\n",
      "Batch 67, Loss: 0.956022, Accuracy: 79.17%\n",
      "Batch 68, Loss: 0.873373, Accuracy: 79.30%\n",
      "Batch 69, Loss: 0.852321, Accuracy: 79.46%\n",
      "Batch 70, Loss: 1.003279, Accuracy: 79.38%\n",
      "Batch 71, Loss: 0.918047, Accuracy: 79.42%\n",
      "Batch 72, Loss: 0.890350, Accuracy: 79.51%\n",
      "Batch 73, Loss: 0.879307, Accuracy: 79.64%\n",
      "Batch 74, Loss: 0.896531, Accuracy: 79.75%\n",
      "Batch 75, Loss: 0.946639, Accuracy: 79.77%\n",
      "Batch 76, Loss: 0.907392, Accuracy: 79.81%\n",
      "Batch 77, Loss: 0.899660, Accuracy: 79.87%\n",
      "Batch 78, Loss: 0.954052, Accuracy: 79.87%\n",
      "Batch 79, Loss: 0.949799, Accuracy: 79.87%\n",
      "Batch 80, Loss: 0.905233, Accuracy: 79.88%\n",
      "Batch 81, Loss: 0.971731, Accuracy: 79.86%\n",
      "Batch 82, Loss: 0.879360, Accuracy: 79.97%\n",
      "Batch 83, Loss: 0.887911, Accuracy: 80.05%\n",
      "Batch 84, Loss: 1.007231, Accuracy: 79.99%\n",
      "Batch 85, Loss: 0.879816, Accuracy: 80.07%\n",
      "Batch 86, Loss: 0.999310, Accuracy: 80.00%\n",
      "Batch 87, Loss: 1.040804, Accuracy: 79.87%\n",
      "Batch 88, Loss: 0.928078, Accuracy: 79.87%\n",
      "Batch 89, Loss: 0.909729, Accuracy: 79.92%\n",
      "Batch 90, Loss: 0.932893, Accuracy: 79.93%\n",
      "Batch 91, Loss: 0.854775, Accuracy: 80.03%\n",
      "Batch 92, Loss: 1.019023, Accuracy: 79.94%\n",
      "Batch 93, Loss: 0.904962, Accuracy: 79.99%\n",
      "Batch 94, Loss: 1.025032, Accuracy: 79.90%\n",
      "Batch 95, Loss: 1.021530, Accuracy: 79.79%\n",
      "Batch 96, Loss: 0.945025, Accuracy: 79.79%\n",
      "Batch 97, Loss: 1.011550, Accuracy: 79.70%\n",
      "Batch 98, Loss: 0.953563, Accuracy: 79.69%\n",
      "Batch 99, Loss: 0.905937, Accuracy: 79.73%\n",
      "Batch 100, Loss: 0.859118, Accuracy: 79.81%\n",
      "Batch 101, Loss: 0.885541, Accuracy: 79.89%\n",
      "Batch 102, Loss: 0.970896, Accuracy: 79.84%\n",
      "Batch 103, Loss: 0.849169, Accuracy: 79.95%\n",
      "Batch 104, Loss: 0.960384, Accuracy: 79.94%\n",
      "Batch 105, Loss: 0.922705, Accuracy: 79.97%\n",
      "Batch 106, Loss: 0.930613, Accuracy: 79.97%\n",
      "Batch 107, Loss: 0.902070, Accuracy: 80.02%\n",
      "Batch 108, Loss: 0.985776, Accuracy: 79.98%\n",
      "Batch 109, Loss: 0.923047, Accuracy: 80.00%\n",
      "Batch 110, Loss: 0.968269, Accuracy: 79.99%\n",
      "Batch 111, Loss: 1.040013, Accuracy: 79.90%\n",
      "Batch 112, Loss: 0.911761, Accuracy: 79.92%\n",
      "Batch 113, Loss: 1.028084, Accuracy: 79.85%\n",
      "Batch 114, Loss: 0.908350, Accuracy: 79.88%\n",
      "Batch 115, Loss: 1.043315, Accuracy: 79.77%\n",
      "Batch 116, Loss: 0.983900, Accuracy: 79.73%\n",
      "Batch 117, Loss: 1.058419, Accuracy: 79.62%\n",
      "Batch 118, Loss: 0.991750, Accuracy: 79.57%\n",
      "Batch 119, Loss: 0.942095, Accuracy: 79.58%\n",
      "Batch 120, Loss: 0.926328, Accuracy: 79.61%\n",
      "Batch 121, Loss: 1.010966, Accuracy: 79.55%\n",
      "Batch 122, Loss: 0.973329, Accuracy: 79.53%\n",
      "Batch 123, Loss: 0.927182, Accuracy: 79.55%\n",
      "Batch 124, Loss: 1.007223, Accuracy: 79.52%\n",
      "Batch 125, Loss: 1.011364, Accuracy: 79.47%\n",
      "Batch 126, Loss: 0.992702, Accuracy: 79.45%\n",
      "Batch 127, Loss: 0.955714, Accuracy: 79.45%\n",
      "Batch 128, Loss: 0.995973, Accuracy: 79.43%\n",
      "Batch 129, Loss: 1.015938, Accuracy: 79.37%\n",
      "Batch 130, Loss: 0.979671, Accuracy: 79.34%\n",
      "Batch 131, Loss: 0.883465, Accuracy: 79.40%\n",
      "Batch 132, Loss: 0.931055, Accuracy: 79.42%\n",
      "Batch 133, Loss: 0.893514, Accuracy: 79.45%\n",
      "Batch 134, Loss: 0.917567, Accuracy: 79.49%\n",
      "Batch 135, Loss: 0.915987, Accuracy: 79.51%\n",
      "Batch 136, Loss: 0.877169, Accuracy: 79.57%\n",
      "Batch 137, Loss: 1.021035, Accuracy: 79.52%\n",
      "Batch 138, Loss: 0.970931, Accuracy: 79.50%\n",
      "Batch 139, Loss: 0.905116, Accuracy: 79.53%\n",
      "Batch 140, Loss: 0.932591, Accuracy: 79.54%\n",
      "Batch 141, Loss: 1.037369, Accuracy: 79.47%\n",
      "Batch 142, Loss: 0.967647, Accuracy: 79.45%\n",
      "Batch 143, Loss: 1.014841, Accuracy: 79.39%\n",
      "Batch 144, Loss: 0.886989, Accuracy: 79.43%\n",
      "Batch 145, Loss: 0.956729, Accuracy: 79.43%\n",
      "Batch 146, Loss: 0.917288, Accuracy: 79.45%\n",
      "Batch 147, Loss: 1.007755, Accuracy: 79.42%\n",
      "Batch 148, Loss: 0.985891, Accuracy: 79.39%\n",
      "Batch 149, Loss: 0.870519, Accuracy: 79.46%\n",
      "Batch 150, Loss: 0.946028, Accuracy: 79.48%\n",
      "Batch 151, Loss: 0.912634, Accuracy: 79.51%\n",
      "Batch 152, Loss: 1.037054, Accuracy: 79.44%\n",
      "Batch 153, Loss: 0.980969, Accuracy: 79.41%\n",
      "Batch 154, Loss: 0.925518, Accuracy: 79.42%\n",
      "Batch 155, Loss: 0.990830, Accuracy: 79.39%\n",
      "Batch 156, Loss: 0.908664, Accuracy: 79.42%\n",
      "Batch 157, Loss: 0.987937, Accuracy: 79.39%\n",
      "Batch 158, Loss: 0.981323, Accuracy: 79.37%\n",
      "Batch 159, Loss: 0.970776, Accuracy: 79.36%\n",
      "Batch 160, Loss: 0.889048, Accuracy: 79.40%\n",
      "Batch 161, Loss: 0.917169, Accuracy: 79.41%\n",
      "Batch 162, Loss: 0.881119, Accuracy: 79.44%\n",
      "Batch 163, Loss: 1.009540, Accuracy: 79.39%\n",
      "Batch 164, Loss: 0.950917, Accuracy: 79.39%\n",
      "Batch 165, Loss: 0.969440, Accuracy: 79.38%\n",
      "Batch 166, Loss: 0.961301, Accuracy: 79.36%\n",
      "Batch 167, Loss: 0.962603, Accuracy: 79.35%\n",
      "Batch 168, Loss: 0.958077, Accuracy: 79.34%\n",
      "Batch 169, Loss: 0.932691, Accuracy: 79.35%\n",
      "Batch 170, Loss: 0.944132, Accuracy: 79.37%\n",
      "Batch 171, Loss: 0.950438, Accuracy: 79.37%\n",
      "Batch 172, Loss: 0.974651, Accuracy: 79.34%\n",
      "Batch 173, Loss: 1.045971, Accuracy: 79.29%\n",
      "Batch 174, Loss: 0.967639, Accuracy: 79.27%\n",
      "Batch 175, Loss: 0.930103, Accuracy: 79.29%\n",
      "Batch 176, Loss: 0.921393, Accuracy: 79.31%\n",
      "Batch 177, Loss: 0.922420, Accuracy: 79.33%\n",
      "Batch 178, Loss: 0.922366, Accuracy: 79.33%\n",
      "Batch 179, Loss: 0.931923, Accuracy: 79.34%\n",
      "Batch 180, Loss: 0.902762, Accuracy: 79.37%\n",
      "Batch 181, Loss: 0.951589, Accuracy: 79.36%\n",
      "Batch 182, Loss: 0.939451, Accuracy: 79.36%\n",
      "Batch 183, Loss: 0.962372, Accuracy: 79.35%\n",
      "Batch 184, Loss: 0.925473, Accuracy: 79.36%\n",
      "Batch 185, Loss: 0.994107, Accuracy: 79.33%\n",
      "Batch 186, Loss: 0.917028, Accuracy: 79.36%\n",
      "Batch 187, Loss: 0.963338, Accuracy: 79.35%\n",
      "Batch 188, Loss: 0.995292, Accuracy: 79.32%\n",
      "Batch 189, Loss: 0.936951, Accuracy: 79.33%\n",
      "Batch 190, Loss: 0.948012, Accuracy: 79.34%\n",
      "Batch 191, Loss: 0.907110, Accuracy: 79.37%\n",
      "Batch 192, Loss: 0.951180, Accuracy: 79.37%\n",
      "Batch 193, Loss: 0.898401, Accuracy: 79.40%\n",
      "Batch 194, Loss: 0.953987, Accuracy: 79.39%\n",
      "Batch 195, Loss: 0.890164, Accuracy: 79.42%\n",
      "Batch 196, Loss: 0.904202, Accuracy: 79.45%\n",
      "Batch 197, Loss: 0.961710, Accuracy: 79.46%\n",
      "Batch 198, Loss: 0.949615, Accuracy: 79.46%\n",
      "Batch 199, Loss: 0.966904, Accuracy: 79.45%\n",
      "Batch 200, Loss: 0.975916, Accuracy: 79.44%\n",
      "Batch 201, Loss: 0.907957, Accuracy: 79.45%\n",
      "Batch 202, Loss: 0.992560, Accuracy: 79.43%\n",
      "Batch 203, Loss: 0.881820, Accuracy: 79.46%\n",
      "Batch 204, Loss: 0.951402, Accuracy: 79.46%\n",
      "Batch 205, Loss: 0.922894, Accuracy: 79.47%\n",
      "Batch 206, Loss: 0.919564, Accuracy: 79.48%\n",
      "Batch 207, Loss: 0.921265, Accuracy: 79.49%\n",
      "Batch 208, Loss: 0.967794, Accuracy: 79.48%\n",
      "Batch 209, Loss: 0.992246, Accuracy: 79.45%\n",
      "Batch 210, Loss: 0.944485, Accuracy: 79.46%\n",
      "Batch 211, Loss: 1.034374, Accuracy: 79.41%\n",
      "Batch 212, Loss: 0.986226, Accuracy: 79.39%\n",
      "Batch 213, Loss: 0.982422, Accuracy: 79.38%\n",
      "Training - Epoch 102, Loss: 0.949505, Accuracy: 79.38%\n",
      "Validation Batch 1, Loss: 0.900062, Accuracy: 84.38%\n",
      "Validation Batch 2, Loss: 0.940655, Accuracy: 82.03%\n",
      "Validation Batch 3, Loss: 1.003283, Accuracy: 79.17%\n",
      "Validation Batch 4, Loss: 0.932123, Accuracy: 79.69%\n",
      "Validation Batch 5, Loss: 0.915571, Accuracy: 80.31%\n",
      "Validation Batch 6, Loss: 0.881535, Accuracy: 80.99%\n",
      "Validation Batch 7, Loss: 0.962710, Accuracy: 80.58%\n",
      "Validation Batch 8, Loss: 0.984701, Accuracy: 79.88%\n",
      "Validation Batch 9, Loss: 0.997493, Accuracy: 79.17%\n",
      "Validation Batch 10, Loss: 0.976286, Accuracy: 78.75%\n",
      "Validation Batch 11, Loss: 0.914610, Accuracy: 79.26%\n",
      "Validation Batch 12, Loss: 0.888091, Accuracy: 79.82%\n",
      "Validation Batch 13, Loss: 0.994815, Accuracy: 79.45%\n",
      "Validation Batch 14, Loss: 0.972681, Accuracy: 79.13%\n",
      "Validation Batch 15, Loss: 0.941305, Accuracy: 79.27%\n",
      "Validation Batch 16, Loss: 0.907523, Accuracy: 79.59%\n",
      "Validation Batch 17, Loss: 0.974698, Accuracy: 79.32%\n",
      "Validation Batch 18, Loss: 0.922684, Accuracy: 79.51%\n",
      "Validation Batch 19, Loss: 0.973930, Accuracy: 79.36%\n",
      "Validation Batch 20, Loss: 0.918378, Accuracy: 79.45%\n",
      "Validation Batch 21, Loss: 0.955253, Accuracy: 79.46%\n",
      "Validation Batch 22, Loss: 0.980665, Accuracy: 79.19%\n",
      "Validation Batch 23, Loss: 1.028572, Accuracy: 78.74%\n",
      "Validation Batch 24, Loss: 1.014649, Accuracy: 78.52%\n",
      "Validation Batch 25, Loss: 0.939505, Accuracy: 78.69%\n",
      "Validation Batch 26, Loss: 0.942083, Accuracy: 78.79%\n",
      "Validation Batch 27, Loss: 0.901254, Accuracy: 78.92%\n",
      "Validation - Epoch 102, Loss: 0.950560, Accuracy: 78.92%\n",
      "Patienceâ€”10\n",
      "Epoch 103\n",
      "Batch 1, Loss: 0.941535, Accuracy: 79.69%\n",
      "Batch 2, Loss: 0.965467, Accuracy: 78.91%\n",
      "Batch 3, Loss: 0.990580, Accuracy: 77.60%\n",
      "Batch 4, Loss: 1.026770, Accuracy: 75.78%\n",
      "Batch 5, Loss: 0.911478, Accuracy: 77.19%\n",
      "Batch 6, Loss: 0.970837, Accuracy: 77.34%\n",
      "Batch 7, Loss: 0.904269, Accuracy: 78.35%\n",
      "Batch 8, Loss: 0.829994, Accuracy: 80.08%\n",
      "Batch 9, Loss: 0.925897, Accuracy: 80.21%\n",
      "Batch 10, Loss: 0.908943, Accuracy: 80.62%\n",
      "Batch 11, Loss: 0.891430, Accuracy: 80.97%\n",
      "Batch 12, Loss: 0.976748, Accuracy: 80.60%\n",
      "Batch 13, Loss: 0.858397, Accuracy: 81.25%\n",
      "Batch 14, Loss: 0.972183, Accuracy: 81.03%\n",
      "Batch 15, Loss: 0.926069, Accuracy: 81.04%\n",
      "Batch 16, Loss: 0.868789, Accuracy: 81.54%\n",
      "Batch 17, Loss: 1.084283, Accuracy: 80.61%\n",
      "Batch 18, Loss: 0.953848, Accuracy: 80.47%\n",
      "Batch 19, Loss: 1.018723, Accuracy: 80.10%\n",
      "Batch 20, Loss: 0.993653, Accuracy: 79.84%\n",
      "Batch 21, Loss: 0.893800, Accuracy: 80.21%\n",
      "Batch 22, Loss: 0.970572, Accuracy: 80.11%\n",
      "Batch 23, Loss: 0.957052, Accuracy: 80.03%\n",
      "Batch 24, Loss: 0.898695, Accuracy: 80.21%\n",
      "Batch 25, Loss: 0.891264, Accuracy: 80.38%\n",
      "Batch 26, Loss: 0.972651, Accuracy: 80.23%\n",
      "Batch 27, Loss: 0.936940, Accuracy: 80.27%\n",
      "Batch 28, Loss: 0.993215, Accuracy: 80.08%\n",
      "Batch 29, Loss: 0.942772, Accuracy: 80.01%\n",
      "Batch 30, Loss: 0.860514, Accuracy: 80.26%\n",
      "Batch 31, Loss: 1.007640, Accuracy: 80.04%\n",
      "Batch 32, Loss: 0.961348, Accuracy: 79.93%\n",
      "Batch 33, Loss: 0.942239, Accuracy: 79.88%\n",
      "Batch 34, Loss: 0.943593, Accuracy: 79.87%\n",
      "Batch 35, Loss: 0.973238, Accuracy: 79.82%\n",
      "Batch 36, Loss: 0.990725, Accuracy: 79.69%\n",
      "Batch 37, Loss: 0.964116, Accuracy: 79.60%\n",
      "Batch 38, Loss: 0.967353, Accuracy: 79.56%\n",
      "Batch 39, Loss: 0.918891, Accuracy: 79.69%\n",
      "Batch 40, Loss: 0.984575, Accuracy: 79.57%\n",
      "Batch 41, Loss: 1.056667, Accuracy: 79.27%\n",
      "Batch 42, Loss: 0.950275, Accuracy: 79.24%\n",
      "Batch 43, Loss: 0.962799, Accuracy: 79.22%\n",
      "Batch 44, Loss: 0.892740, Accuracy: 79.40%\n",
      "Batch 45, Loss: 0.888295, Accuracy: 79.55%\n",
      "Batch 46, Loss: 0.937927, Accuracy: 79.59%\n",
      "Batch 47, Loss: 0.933954, Accuracy: 79.62%\n",
      "Batch 48, Loss: 0.915213, Accuracy: 79.72%\n",
      "Batch 49, Loss: 0.981724, Accuracy: 79.62%\n",
      "Batch 50, Loss: 1.012185, Accuracy: 79.47%\n",
      "Batch 51, Loss: 0.966971, Accuracy: 79.44%\n",
      "Batch 52, Loss: 0.957295, Accuracy: 79.42%\n",
      "Batch 53, Loss: 0.866983, Accuracy: 79.60%\n",
      "Batch 54, Loss: 0.917748, Accuracy: 79.66%\n",
      "Batch 55, Loss: 0.884961, Accuracy: 79.80%\n",
      "Batch 56, Loss: 0.911694, Accuracy: 79.85%\n",
      "Batch 57, Loss: 0.883422, Accuracy: 79.93%\n",
      "Batch 58, Loss: 0.978767, Accuracy: 79.88%\n",
      "Batch 59, Loss: 1.004467, Accuracy: 79.71%\n",
      "Batch 60, Loss: 0.945698, Accuracy: 79.69%\n",
      "Batch 61, Loss: 0.954397, Accuracy: 79.69%\n",
      "Batch 62, Loss: 1.037425, Accuracy: 79.54%\n",
      "Batch 63, Loss: 0.998396, Accuracy: 79.46%\n",
      "Batch 64, Loss: 0.890054, Accuracy: 79.57%\n",
      "Batch 65, Loss: 1.029454, Accuracy: 79.42%\n",
      "Batch 66, Loss: 0.989769, Accuracy: 79.36%\n",
      "Batch 67, Loss: 0.975556, Accuracy: 79.31%\n",
      "Batch 68, Loss: 1.023151, Accuracy: 79.20%\n",
      "Batch 69, Loss: 0.910362, Accuracy: 79.28%\n",
      "Batch 70, Loss: 1.013838, Accuracy: 79.15%\n",
      "Batch 71, Loss: 1.018305, Accuracy: 79.05%\n",
      "Batch 72, Loss: 0.884145, Accuracy: 79.14%\n",
      "Batch 73, Loss: 0.935466, Accuracy: 79.15%\n",
      "Batch 74, Loss: 0.904409, Accuracy: 79.22%\n",
      "Batch 75, Loss: 0.926440, Accuracy: 79.27%\n",
      "Batch 76, Loss: 0.945156, Accuracy: 79.28%\n",
      "Batch 77, Loss: 0.992575, Accuracy: 79.22%\n",
      "Batch 78, Loss: 0.926184, Accuracy: 79.27%\n",
      "Batch 79, Loss: 1.013299, Accuracy: 79.19%\n",
      "Batch 80, Loss: 0.946364, Accuracy: 79.20%\n",
      "Batch 81, Loss: 0.900879, Accuracy: 79.26%\n",
      "Batch 82, Loss: 0.993357, Accuracy: 79.21%\n",
      "Batch 83, Loss: 0.931377, Accuracy: 79.24%\n",
      "Batch 84, Loss: 0.882697, Accuracy: 79.33%\n",
      "Batch 85, Loss: 0.956774, Accuracy: 79.34%\n",
      "Batch 86, Loss: 0.939558, Accuracy: 79.36%\n",
      "Batch 87, Loss: 1.021023, Accuracy: 79.27%\n",
      "Batch 88, Loss: 0.914323, Accuracy: 79.31%\n",
      "Batch 89, Loss: 1.010710, Accuracy: 79.23%\n",
      "Batch 90, Loss: 0.954370, Accuracy: 79.22%\n",
      "Batch 91, Loss: 0.937881, Accuracy: 79.22%\n",
      "Batch 92, Loss: 0.916167, Accuracy: 79.26%\n",
      "Batch 93, Loss: 0.974810, Accuracy: 79.22%\n",
      "Batch 94, Loss: 0.903521, Accuracy: 79.27%\n",
      "Batch 95, Loss: 0.987574, Accuracy: 79.23%\n",
      "Batch 96, Loss: 0.881282, Accuracy: 79.30%\n",
      "Batch 97, Loss: 0.886037, Accuracy: 79.37%\n",
      "Batch 98, Loss: 0.916988, Accuracy: 79.40%\n",
      "Batch 99, Loss: 0.961017, Accuracy: 79.39%\n",
      "Batch 100, Loss: 0.897703, Accuracy: 79.44%\n",
      "Batch 101, Loss: 0.870745, Accuracy: 79.52%\n",
      "Batch 102, Loss: 0.959477, Accuracy: 79.50%\n",
      "Batch 103, Loss: 1.033716, Accuracy: 79.43%\n",
      "Batch 104, Loss: 0.936881, Accuracy: 79.43%\n",
      "Batch 105, Loss: 0.932369, Accuracy: 79.45%\n",
      "Batch 106, Loss: 0.996932, Accuracy: 79.39%\n",
      "Batch 107, Loss: 0.888418, Accuracy: 79.47%\n",
      "Batch 108, Loss: 0.900625, Accuracy: 79.50%\n",
      "Batch 109, Loss: 0.957280, Accuracy: 79.49%\n",
      "Batch 110, Loss: 0.925447, Accuracy: 79.52%\n",
      "Batch 111, Loss: 0.950704, Accuracy: 79.49%\n",
      "Batch 112, Loss: 1.068526, Accuracy: 79.38%\n",
      "Batch 113, Loss: 0.953618, Accuracy: 79.38%\n",
      "Batch 114, Loss: 0.870903, Accuracy: 79.45%\n",
      "Batch 115, Loss: 0.981653, Accuracy: 79.42%\n",
      "Batch 116, Loss: 0.925515, Accuracy: 79.43%\n",
      "Batch 117, Loss: 0.933212, Accuracy: 79.45%\n",
      "Batch 118, Loss: 1.030244, Accuracy: 79.38%\n",
      "Batch 119, Loss: 0.891860, Accuracy: 79.45%\n",
      "Batch 120, Loss: 0.928594, Accuracy: 79.47%\n",
      "Batch 121, Loss: 0.916139, Accuracy: 79.49%\n",
      "Batch 122, Loss: 1.054019, Accuracy: 79.41%\n",
      "Batch 123, Loss: 0.957804, Accuracy: 79.40%\n",
      "Batch 124, Loss: 0.922029, Accuracy: 79.40%\n",
      "Batch 125, Loss: 0.993748, Accuracy: 79.35%\n",
      "Batch 126, Loss: 0.927593, Accuracy: 79.38%\n",
      "Batch 127, Loss: 0.968773, Accuracy: 79.36%\n",
      "Batch 128, Loss: 0.845741, Accuracy: 79.44%\n",
      "Batch 129, Loss: 0.941936, Accuracy: 79.43%\n",
      "Batch 130, Loss: 0.932544, Accuracy: 79.45%\n",
      "Batch 131, Loss: 0.949552, Accuracy: 79.45%\n",
      "Batch 132, Loss: 0.915996, Accuracy: 79.47%\n",
      "Batch 133, Loss: 0.972617, Accuracy: 79.45%\n",
      "Batch 134, Loss: 0.929041, Accuracy: 79.48%\n",
      "Batch 135, Loss: 0.921016, Accuracy: 79.49%\n",
      "Batch 136, Loss: 0.926611, Accuracy: 79.52%\n",
      "Batch 137, Loss: 0.906493, Accuracy: 79.55%\n",
      "Batch 138, Loss: 0.968614, Accuracy: 79.54%\n",
      "Batch 139, Loss: 0.988220, Accuracy: 79.51%\n",
      "Batch 140, Loss: 0.907877, Accuracy: 79.55%\n",
      "Batch 141, Loss: 0.876370, Accuracy: 79.61%\n",
      "Batch 142, Loss: 0.945913, Accuracy: 79.59%\n",
      "Batch 143, Loss: 0.975681, Accuracy: 79.58%\n",
      "Batch 144, Loss: 1.034179, Accuracy: 79.51%\n",
      "Batch 145, Loss: 0.878330, Accuracy: 79.57%\n",
      "Batch 146, Loss: 0.910000, Accuracy: 79.59%\n",
      "Batch 147, Loss: 0.986506, Accuracy: 79.56%\n",
      "Batch 148, Loss: 0.945583, Accuracy: 79.56%\n",
      "Batch 149, Loss: 0.856430, Accuracy: 79.62%\n",
      "Batch 150, Loss: 0.881584, Accuracy: 79.68%\n",
      "Batch 151, Loss: 1.008584, Accuracy: 79.64%\n",
      "Batch 152, Loss: 0.864781, Accuracy: 79.70%\n",
      "Batch 153, Loss: 0.920104, Accuracy: 79.72%\n",
      "Batch 154, Loss: 0.936491, Accuracy: 79.73%\n",
      "Batch 155, Loss: 0.952553, Accuracy: 79.72%\n",
      "Batch 156, Loss: 0.981034, Accuracy: 79.70%\n",
      "Batch 157, Loss: 0.902957, Accuracy: 79.72%\n",
      "Batch 158, Loss: 1.047644, Accuracy: 79.66%\n",
      "Batch 159, Loss: 0.936154, Accuracy: 79.66%\n",
      "Batch 160, Loss: 0.999312, Accuracy: 79.62%\n",
      "Batch 161, Loss: 0.956450, Accuracy: 79.61%\n",
      "Batch 162, Loss: 0.950297, Accuracy: 79.60%\n",
      "Batch 163, Loss: 0.926964, Accuracy: 79.62%\n",
      "Batch 164, Loss: 0.853961, Accuracy: 79.68%\n",
      "Batch 165, Loss: 0.929083, Accuracy: 79.70%\n",
      "Batch 166, Loss: 0.981650, Accuracy: 79.67%\n",
      "Batch 167, Loss: 0.898280, Accuracy: 79.69%\n",
      "Batch 168, Loss: 0.906700, Accuracy: 79.72%\n",
      "Batch 169, Loss: 0.996912, Accuracy: 79.68%\n",
      "Batch 170, Loss: 0.947270, Accuracy: 79.69%\n",
      "Batch 171, Loss: 0.948350, Accuracy: 79.70%\n",
      "Batch 172, Loss: 1.013517, Accuracy: 79.66%\n",
      "Batch 173, Loss: 0.895991, Accuracy: 79.70%\n",
      "Batch 174, Loss: 1.056118, Accuracy: 79.63%\n",
      "Batch 175, Loss: 0.969289, Accuracy: 79.62%\n",
      "Batch 176, Loss: 0.934994, Accuracy: 79.63%\n",
      "Batch 177, Loss: 0.934056, Accuracy: 79.63%\n",
      "Batch 178, Loss: 0.921322, Accuracy: 79.65%\n",
      "Batch 179, Loss: 0.956966, Accuracy: 79.65%\n",
      "Batch 180, Loss: 0.936237, Accuracy: 79.66%\n",
      "Batch 181, Loss: 0.894666, Accuracy: 79.69%\n",
      "Batch 182, Loss: 1.017759, Accuracy: 79.65%\n",
      "Batch 183, Loss: 0.963280, Accuracy: 79.64%\n",
      "Batch 184, Loss: 1.008596, Accuracy: 79.61%\n",
      "Batch 185, Loss: 0.982566, Accuracy: 79.58%\n",
      "Batch 186, Loss: 0.947141, Accuracy: 79.57%\n",
      "Batch 187, Loss: 0.946550, Accuracy: 79.56%\n",
      "Batch 188, Loss: 0.990434, Accuracy: 79.55%\n",
      "Batch 189, Loss: 1.021937, Accuracy: 79.51%\n",
      "Batch 190, Loss: 0.963588, Accuracy: 79.49%\n",
      "Batch 191, Loss: 0.945390, Accuracy: 79.49%\n",
      "Batch 192, Loss: 0.996174, Accuracy: 79.48%\n",
      "Batch 193, Loss: 0.887802, Accuracy: 79.51%\n",
      "Batch 194, Loss: 0.956551, Accuracy: 79.49%\n",
      "Batch 195, Loss: 0.951637, Accuracy: 79.50%\n",
      "Batch 196, Loss: 0.986590, Accuracy: 79.47%\n",
      "Batch 197, Loss: 0.865513, Accuracy: 79.52%\n",
      "Batch 198, Loss: 0.934852, Accuracy: 79.52%\n",
      "Batch 199, Loss: 1.014295, Accuracy: 79.49%\n",
      "Batch 200, Loss: 0.954068, Accuracy: 79.49%\n",
      "Batch 201, Loss: 0.820373, Accuracy: 79.56%\n",
      "Batch 202, Loss: 0.948745, Accuracy: 79.57%\n",
      "Batch 203, Loss: 0.959953, Accuracy: 79.56%\n",
      "Batch 204, Loss: 0.906405, Accuracy: 79.60%\n",
      "Batch 205, Loss: 0.961971, Accuracy: 79.59%\n",
      "Batch 206, Loss: 0.917104, Accuracy: 79.60%\n",
      "Batch 207, Loss: 0.933431, Accuracy: 79.60%\n",
      "Batch 208, Loss: 0.855824, Accuracy: 79.65%\n",
      "Batch 209, Loss: 0.900421, Accuracy: 79.67%\n",
      "Batch 210, Loss: 0.959258, Accuracy: 79.67%\n",
      "Batch 211, Loss: 0.946672, Accuracy: 79.67%\n",
      "Batch 212, Loss: 0.877302, Accuracy: 79.71%\n",
      "Batch 213, Loss: 0.930263, Accuracy: 79.72%\n",
      "Training - Epoch 103, Loss: 0.946049, Accuracy: 79.72%\n",
      "Validation Batch 1, Loss: 0.898996, Accuracy: 84.38%\n",
      "Validation Batch 2, Loss: 0.943420, Accuracy: 82.03%\n",
      "Validation Batch 3, Loss: 1.005227, Accuracy: 79.17%\n",
      "Validation Batch 4, Loss: 0.939870, Accuracy: 79.30%\n",
      "Validation Batch 5, Loss: 0.920511, Accuracy: 80.00%\n",
      "Validation Batch 6, Loss: 0.882605, Accuracy: 80.47%\n",
      "Validation Batch 7, Loss: 0.961561, Accuracy: 80.13%\n",
      "Validation Batch 8, Loss: 0.984577, Accuracy: 79.49%\n",
      "Validation Batch 9, Loss: 0.989954, Accuracy: 79.17%\n",
      "Validation Batch 10, Loss: 0.981043, Accuracy: 78.75%\n",
      "Validation Batch 11, Loss: 0.913133, Accuracy: 79.26%\n",
      "Validation Batch 12, Loss: 0.887644, Accuracy: 79.95%\n",
      "Validation Batch 13, Loss: 0.996937, Accuracy: 79.69%\n",
      "Validation Batch 14, Loss: 0.981959, Accuracy: 79.35%\n",
      "Validation Batch 15, Loss: 0.945174, Accuracy: 79.27%\n",
      "Validation Batch 16, Loss: 0.912688, Accuracy: 79.59%\n",
      "Validation Batch 17, Loss: 0.975940, Accuracy: 79.41%\n",
      "Validation Batch 18, Loss: 0.921658, Accuracy: 79.60%\n",
      "Validation Batch 19, Loss: 0.978374, Accuracy: 79.44%\n",
      "Validation Batch 20, Loss: 0.923653, Accuracy: 79.69%\n",
      "Validation Batch 21, Loss: 0.958751, Accuracy: 79.69%\n",
      "Validation Batch 22, Loss: 0.984193, Accuracy: 79.47%\n",
      "Validation Batch 23, Loss: 1.034922, Accuracy: 79.01%\n",
      "Validation Batch 24, Loss: 1.017367, Accuracy: 78.65%\n",
      "Validation Batch 25, Loss: 0.945250, Accuracy: 78.75%\n",
      "Validation Batch 26, Loss: 0.942141, Accuracy: 78.85%\n",
      "Validation Batch 27, Loss: 0.903374, Accuracy: 78.98%\n",
      "Validation - Epoch 103, Loss: 0.952997, Accuracy: 78.98%\n",
      "Patienceâ€”11\n",
      "Epoch 104\n",
      "Batch 1, Loss: 0.934418, Accuracy: 81.25%\n",
      "Batch 2, Loss: 0.912606, Accuracy: 82.03%\n",
      "Batch 3, Loss: 0.879794, Accuracy: 83.85%\n",
      "Batch 4, Loss: 0.995681, Accuracy: 81.25%\n",
      "Batch 5, Loss: 0.996696, Accuracy: 79.69%\n",
      "Batch 6, Loss: 0.996065, Accuracy: 78.91%\n",
      "Batch 7, Loss: 0.974580, Accuracy: 78.57%\n",
      "Batch 8, Loss: 0.958897, Accuracy: 78.71%\n",
      "Batch 9, Loss: 0.818619, Accuracy: 80.38%\n",
      "Batch 10, Loss: 0.909543, Accuracy: 80.62%\n",
      "Batch 11, Loss: 0.892378, Accuracy: 80.97%\n",
      "Batch 12, Loss: 1.006938, Accuracy: 80.34%\n",
      "Batch 13, Loss: 0.909603, Accuracy: 80.65%\n",
      "Batch 14, Loss: 0.977078, Accuracy: 80.25%\n",
      "Batch 15, Loss: 1.026469, Accuracy: 79.69%\n",
      "Batch 16, Loss: 0.891903, Accuracy: 79.88%\n",
      "Batch 17, Loss: 0.936295, Accuracy: 79.96%\n",
      "Batch 18, Loss: 0.947089, Accuracy: 79.95%\n",
      "Batch 19, Loss: 0.953548, Accuracy: 80.02%\n",
      "Batch 20, Loss: 0.870175, Accuracy: 80.47%\n",
      "Batch 21, Loss: 0.951844, Accuracy: 80.36%\n",
      "Batch 22, Loss: 1.062628, Accuracy: 79.83%\n",
      "Batch 23, Loss: 0.972639, Accuracy: 79.76%\n",
      "Batch 24, Loss: 0.954278, Accuracy: 79.82%\n",
      "Batch 25, Loss: 0.918333, Accuracy: 79.94%\n",
      "Batch 26, Loss: 0.975059, Accuracy: 79.87%\n",
      "Batch 27, Loss: 1.038582, Accuracy: 79.57%\n",
      "Batch 28, Loss: 0.951784, Accuracy: 79.58%\n",
      "Batch 29, Loss: 0.952556, Accuracy: 79.53%\n",
      "Batch 30, Loss: 0.916193, Accuracy: 79.64%\n",
      "Batch 31, Loss: 0.949905, Accuracy: 79.64%\n",
      "Batch 32, Loss: 1.023456, Accuracy: 79.39%\n",
      "Batch 33, Loss: 0.893629, Accuracy: 79.55%\n",
      "Batch 34, Loss: 0.900155, Accuracy: 79.60%\n",
      "Batch 35, Loss: 0.991036, Accuracy: 79.51%\n",
      "Batch 36, Loss: 0.860495, Accuracy: 79.77%\n",
      "Batch 37, Loss: 0.934932, Accuracy: 79.81%\n",
      "Batch 38, Loss: 0.949661, Accuracy: 79.77%\n",
      "Batch 39, Loss: 1.011039, Accuracy: 79.61%\n",
      "Batch 40, Loss: 0.837594, Accuracy: 79.88%\n",
      "Batch 41, Loss: 0.969666, Accuracy: 79.88%\n",
      "Batch 42, Loss: 0.921310, Accuracy: 79.95%\n",
      "Batch 43, Loss: 0.835040, Accuracy: 80.20%\n",
      "Batch 44, Loss: 0.939310, Accuracy: 80.22%\n",
      "Batch 45, Loss: 0.880336, Accuracy: 80.35%\n",
      "Batch 46, Loss: 0.981768, Accuracy: 80.26%\n",
      "Batch 47, Loss: 0.982118, Accuracy: 80.15%\n",
      "Batch 48, Loss: 0.898646, Accuracy: 80.24%\n",
      "Batch 49, Loss: 0.941902, Accuracy: 80.23%\n",
      "Batch 50, Loss: 1.117700, Accuracy: 79.81%\n",
      "Batch 51, Loss: 0.881428, Accuracy: 79.93%\n",
      "Batch 52, Loss: 0.928622, Accuracy: 79.96%\n",
      "Batch 53, Loss: 0.877979, Accuracy: 80.10%\n",
      "Batch 54, Loss: 0.935743, Accuracy: 80.09%\n",
      "Batch 55, Loss: 0.978569, Accuracy: 80.03%\n",
      "Batch 56, Loss: 0.908548, Accuracy: 80.08%\n",
      "Batch 57, Loss: 0.921908, Accuracy: 80.10%\n",
      "Batch 58, Loss: 0.960013, Accuracy: 80.09%\n",
      "Batch 59, Loss: 0.879569, Accuracy: 80.22%\n",
      "Batch 60, Loss: 0.920436, Accuracy: 80.26%\n",
      "Batch 61, Loss: 0.899726, Accuracy: 80.33%\n",
      "Batch 62, Loss: 0.867778, Accuracy: 80.47%\n",
      "Batch 63, Loss: 1.033192, Accuracy: 80.31%\n",
      "Batch 64, Loss: 0.971362, Accuracy: 80.25%\n",
      "Batch 65, Loss: 0.883613, Accuracy: 80.34%\n",
      "Batch 66, Loss: 0.873331, Accuracy: 80.45%\n",
      "Batch 67, Loss: 0.946938, Accuracy: 80.41%\n",
      "Batch 68, Loss: 0.904971, Accuracy: 80.47%\n",
      "Batch 69, Loss: 0.967697, Accuracy: 80.41%\n",
      "Batch 70, Loss: 0.926681, Accuracy: 80.42%\n",
      "Batch 71, Loss: 0.943696, Accuracy: 80.41%\n",
      "Batch 72, Loss: 0.970302, Accuracy: 80.36%\n",
      "Batch 73, Loss: 0.971330, Accuracy: 80.33%\n",
      "Batch 74, Loss: 0.924809, Accuracy: 80.36%\n",
      "Batch 75, Loss: 1.078191, Accuracy: 80.19%\n",
      "Batch 76, Loss: 0.841417, Accuracy: 80.32%\n",
      "Batch 77, Loss: 0.904778, Accuracy: 80.36%\n",
      "Batch 78, Loss: 0.884007, Accuracy: 80.43%\n",
      "Batch 79, Loss: 0.859421, Accuracy: 80.56%\n",
      "Batch 80, Loss: 0.967716, Accuracy: 80.55%\n",
      "Batch 81, Loss: 0.948908, Accuracy: 80.54%\n",
      "Batch 82, Loss: 0.978963, Accuracy: 80.49%\n",
      "Batch 83, Loss: 1.003496, Accuracy: 80.38%\n",
      "Batch 84, Loss: 0.941648, Accuracy: 80.41%\n",
      "Batch 85, Loss: 0.911423, Accuracy: 80.44%\n",
      "Batch 86, Loss: 0.939587, Accuracy: 80.45%\n",
      "Batch 87, Loss: 0.885829, Accuracy: 80.50%\n",
      "Batch 88, Loss: 0.986942, Accuracy: 80.45%\n",
      "Batch 89, Loss: 0.956854, Accuracy: 80.42%\n",
      "Batch 90, Loss: 0.918567, Accuracy: 80.45%\n",
      "Batch 91, Loss: 0.953009, Accuracy: 80.44%\n",
      "Batch 92, Loss: 0.970036, Accuracy: 80.38%\n",
      "Batch 93, Loss: 0.959083, Accuracy: 80.38%\n",
      "Batch 94, Loss: 1.004948, Accuracy: 80.29%\n",
      "Batch 95, Loss: 0.945881, Accuracy: 80.30%\n",
      "Batch 96, Loss: 0.954514, Accuracy: 80.29%\n",
      "Batch 97, Loss: 0.929436, Accuracy: 80.30%\n",
      "Batch 98, Loss: 0.988001, Accuracy: 80.25%\n",
      "Batch 99, Loss: 0.929016, Accuracy: 80.24%\n",
      "Batch 100, Loss: 0.988511, Accuracy: 80.20%\n",
      "Batch 101, Loss: 0.996503, Accuracy: 80.15%\n",
      "Batch 102, Loss: 0.988157, Accuracy: 80.09%\n",
      "Batch 103, Loss: 0.937161, Accuracy: 80.08%\n",
      "Batch 104, Loss: 0.937944, Accuracy: 80.08%\n",
      "Batch 105, Loss: 0.926695, Accuracy: 80.09%\n",
      "Batch 106, Loss: 0.949931, Accuracy: 80.09%\n",
      "Batch 107, Loss: 0.913379, Accuracy: 80.11%\n",
      "Batch 108, Loss: 0.889897, Accuracy: 80.16%\n",
      "Batch 109, Loss: 0.989276, Accuracy: 80.12%\n",
      "Batch 110, Loss: 1.030176, Accuracy: 80.04%\n",
      "Batch 111, Loss: 0.996122, Accuracy: 80.01%\n",
      "Batch 112, Loss: 0.924860, Accuracy: 80.05%\n",
      "Batch 113, Loss: 0.966387, Accuracy: 80.03%\n",
      "Batch 114, Loss: 0.892689, Accuracy: 80.07%\n",
      "Batch 115, Loss: 0.985453, Accuracy: 80.04%\n",
      "Batch 116, Loss: 0.895955, Accuracy: 80.09%\n",
      "Batch 117, Loss: 0.884569, Accuracy: 80.15%\n",
      "Batch 118, Loss: 0.884042, Accuracy: 80.20%\n",
      "Batch 119, Loss: 0.929669, Accuracy: 80.23%\n",
      "Batch 120, Loss: 0.904320, Accuracy: 80.27%\n",
      "Batch 121, Loss: 1.005542, Accuracy: 80.22%\n",
      "Batch 122, Loss: 0.984255, Accuracy: 80.19%\n",
      "Batch 123, Loss: 0.921778, Accuracy: 80.21%\n",
      "Batch 124, Loss: 1.043838, Accuracy: 80.13%\n",
      "Batch 125, Loss: 1.053470, Accuracy: 80.06%\n",
      "Batch 126, Loss: 0.999167, Accuracy: 80.02%\n",
      "Batch 127, Loss: 0.928336, Accuracy: 80.04%\n",
      "Batch 128, Loss: 0.907676, Accuracy: 80.07%\n",
      "Batch 129, Loss: 0.950338, Accuracy: 80.06%\n",
      "Batch 130, Loss: 0.993689, Accuracy: 80.02%\n",
      "Batch 131, Loss: 0.880667, Accuracy: 80.07%\n",
      "Batch 132, Loss: 0.956902, Accuracy: 80.04%\n",
      "Batch 133, Loss: 0.905936, Accuracy: 80.09%\n",
      "Batch 134, Loss: 0.842223, Accuracy: 80.18%\n",
      "Batch 135, Loss: 0.898868, Accuracy: 80.20%\n",
      "Batch 136, Loss: 1.007891, Accuracy: 80.15%\n",
      "Batch 137, Loss: 0.929672, Accuracy: 80.17%\n",
      "Batch 138, Loss: 0.937927, Accuracy: 80.17%\n",
      "Batch 139, Loss: 0.964286, Accuracy: 80.15%\n",
      "Batch 140, Loss: 0.941929, Accuracy: 80.15%\n",
      "Batch 141, Loss: 0.899165, Accuracy: 80.19%\n",
      "Batch 142, Loss: 0.967999, Accuracy: 80.16%\n",
      "Batch 143, Loss: 0.903526, Accuracy: 80.19%\n",
      "Batch 144, Loss: 1.018648, Accuracy: 80.12%\n",
      "Batch 145, Loss: 0.912758, Accuracy: 80.14%\n",
      "Batch 146, Loss: 0.954012, Accuracy: 80.13%\n",
      "Batch 147, Loss: 0.876498, Accuracy: 80.18%\n",
      "Batch 148, Loss: 0.882360, Accuracy: 80.23%\n",
      "Batch 149, Loss: 0.888815, Accuracy: 80.25%\n",
      "Batch 150, Loss: 0.883603, Accuracy: 80.28%\n",
      "Batch 151, Loss: 0.956358, Accuracy: 80.28%\n",
      "Batch 152, Loss: 0.986767, Accuracy: 80.25%\n",
      "Batch 153, Loss: 0.940515, Accuracy: 80.25%\n",
      "Batch 154, Loss: 0.990095, Accuracy: 80.22%\n",
      "Batch 155, Loss: 1.009357, Accuracy: 80.17%\n",
      "Batch 156, Loss: 0.924769, Accuracy: 80.19%\n",
      "Batch 157, Loss: 1.003560, Accuracy: 80.15%\n",
      "Batch 158, Loss: 0.985636, Accuracy: 80.12%\n",
      "Batch 159, Loss: 0.945264, Accuracy: 80.12%\n",
      "Batch 160, Loss: 0.938147, Accuracy: 80.13%\n",
      "Batch 161, Loss: 0.929672, Accuracy: 80.13%\n",
      "Batch 162, Loss: 1.051069, Accuracy: 80.06%\n",
      "Batch 163, Loss: 0.939218, Accuracy: 80.06%\n",
      "Batch 164, Loss: 0.981022, Accuracy: 80.04%\n",
      "Batch 165, Loss: 0.926531, Accuracy: 80.04%\n",
      "Batch 166, Loss: 0.937554, Accuracy: 80.05%\n",
      "Batch 167, Loss: 0.976722, Accuracy: 80.03%\n",
      "Batch 168, Loss: 0.870664, Accuracy: 80.08%\n",
      "Batch 169, Loss: 0.942427, Accuracy: 80.08%\n",
      "Batch 170, Loss: 0.972947, Accuracy: 80.06%\n",
      "Batch 171, Loss: 0.970132, Accuracy: 80.04%\n",
      "Batch 172, Loss: 0.914506, Accuracy: 80.06%\n",
      "Batch 173, Loss: 0.964602, Accuracy: 80.05%\n",
      "Batch 174, Loss: 1.047168, Accuracy: 79.99%\n",
      "Batch 175, Loss: 1.008891, Accuracy: 79.95%\n",
      "Batch 176, Loss: 1.037458, Accuracy: 79.89%\n",
      "Batch 177, Loss: 0.961439, Accuracy: 79.88%\n",
      "Batch 178, Loss: 0.914429, Accuracy: 79.90%\n",
      "Batch 179, Loss: 0.878036, Accuracy: 79.94%\n",
      "Batch 180, Loss: 0.971397, Accuracy: 79.92%\n",
      "Batch 181, Loss: 0.908005, Accuracy: 79.93%\n",
      "Batch 182, Loss: 0.923160, Accuracy: 79.95%\n",
      "Batch 183, Loss: 1.011985, Accuracy: 79.90%\n",
      "Batch 184, Loss: 0.974317, Accuracy: 79.89%\n",
      "Batch 185, Loss: 0.982820, Accuracy: 79.86%\n",
      "Batch 186, Loss: 0.927706, Accuracy: 79.87%\n",
      "Batch 187, Loss: 0.946721, Accuracy: 79.88%\n",
      "Batch 188, Loss: 1.001819, Accuracy: 79.85%\n",
      "Batch 189, Loss: 0.987517, Accuracy: 79.83%\n",
      "Batch 190, Loss: 1.003076, Accuracy: 79.79%\n",
      "Batch 191, Loss: 0.924000, Accuracy: 79.82%\n",
      "Batch 192, Loss: 0.965376, Accuracy: 79.80%\n",
      "Batch 193, Loss: 0.943244, Accuracy: 79.80%\n",
      "Batch 194, Loss: 1.025628, Accuracy: 79.75%\n",
      "Batch 195, Loss: 0.963260, Accuracy: 79.74%\n",
      "Batch 196, Loss: 0.964823, Accuracy: 79.73%\n",
      "Batch 197, Loss: 0.898034, Accuracy: 79.75%\n",
      "Batch 198, Loss: 0.929988, Accuracy: 79.76%\n",
      "Batch 199, Loss: 0.993968, Accuracy: 79.73%\n",
      "Batch 200, Loss: 0.935160, Accuracy: 79.74%\n",
      "Batch 201, Loss: 1.003136, Accuracy: 79.70%\n",
      "Batch 202, Loss: 0.885759, Accuracy: 79.73%\n",
      "Batch 203, Loss: 0.956599, Accuracy: 79.73%\n",
      "Batch 204, Loss: 0.941912, Accuracy: 79.72%\n",
      "Batch 205, Loss: 0.983445, Accuracy: 79.70%\n",
      "Batch 206, Loss: 0.958916, Accuracy: 79.69%\n",
      "Batch 207, Loss: 0.909794, Accuracy: 79.71%\n",
      "Batch 208, Loss: 0.960826, Accuracy: 79.70%\n",
      "Batch 209, Loss: 0.898769, Accuracy: 79.73%\n",
      "Batch 210, Loss: 0.928626, Accuracy: 79.73%\n",
      "Batch 211, Loss: 0.983314, Accuracy: 79.72%\n",
      "Batch 212, Loss: 0.910287, Accuracy: 79.73%\n",
      "Batch 213, Loss: 0.941615, Accuracy: 79.74%\n",
      "Training - Epoch 104, Loss: 0.946598, Accuracy: 79.74%\n",
      "Validation Batch 1, Loss: 0.905464, Accuracy: 84.38%\n",
      "Validation Batch 2, Loss: 0.943880, Accuracy: 82.03%\n",
      "Validation Batch 3, Loss: 1.010703, Accuracy: 79.17%\n",
      "Validation Batch 4, Loss: 0.945804, Accuracy: 79.30%\n",
      "Validation Batch 5, Loss: 0.925405, Accuracy: 80.00%\n",
      "Validation Batch 6, Loss: 0.881127, Accuracy: 80.99%\n",
      "Validation Batch 7, Loss: 0.964242, Accuracy: 80.58%\n",
      "Validation Batch 8, Loss: 0.979917, Accuracy: 80.08%\n",
      "Validation Batch 9, Loss: 0.987966, Accuracy: 79.86%\n",
      "Validation Batch 10, Loss: 0.981136, Accuracy: 79.53%\n",
      "Validation Batch 11, Loss: 0.911094, Accuracy: 79.97%\n",
      "Validation Batch 12, Loss: 0.886017, Accuracy: 80.60%\n",
      "Validation Batch 13, Loss: 0.994446, Accuracy: 80.17%\n",
      "Validation Batch 14, Loss: 0.984277, Accuracy: 79.80%\n",
      "Validation Batch 15, Loss: 0.946471, Accuracy: 79.79%\n",
      "Validation Batch 16, Loss: 0.915805, Accuracy: 80.08%\n",
      "Validation Batch 17, Loss: 0.977305, Accuracy: 79.87%\n",
      "Validation Batch 18, Loss: 0.929876, Accuracy: 80.03%\n",
      "Validation Batch 19, Loss: 0.980122, Accuracy: 79.77%\n",
      "Validation Batch 20, Loss: 0.930152, Accuracy: 79.84%\n",
      "Validation Batch 21, Loss: 0.962355, Accuracy: 79.84%\n",
      "Validation Batch 22, Loss: 0.982371, Accuracy: 79.62%\n",
      "Validation Batch 23, Loss: 1.037483, Accuracy: 79.14%\n",
      "Validation Batch 24, Loss: 1.018489, Accuracy: 78.84%\n",
      "Validation Batch 25, Loss: 0.964439, Accuracy: 78.81%\n",
      "Validation Batch 26, Loss: 0.939955, Accuracy: 78.91%\n",
      "Validation Batch 27, Loss: 0.905088, Accuracy: 79.04%\n",
      "Validation - Epoch 104, Loss: 0.955237, Accuracy: 79.04%\n",
      "Patienceâ€”12\n",
      "Epoch 105\n",
      "Batch 1, Loss: 0.879449, Accuracy: 87.50%\n",
      "Batch 2, Loss: 0.893256, Accuracy: 86.72%\n",
      "Batch 3, Loss: 0.916525, Accuracy: 85.94%\n",
      "Batch 4, Loss: 0.903297, Accuracy: 85.55%\n",
      "Batch 5, Loss: 0.991923, Accuracy: 83.12%\n",
      "Batch 6, Loss: 0.875641, Accuracy: 83.59%\n",
      "Batch 7, Loss: 0.909645, Accuracy: 83.48%\n",
      "Batch 8, Loss: 1.006680, Accuracy: 82.23%\n",
      "Batch 9, Loss: 0.925744, Accuracy: 82.29%\n",
      "Batch 10, Loss: 0.989752, Accuracy: 81.41%\n",
      "Batch 11, Loss: 1.081241, Accuracy: 79.97%\n",
      "Batch 12, Loss: 0.905411, Accuracy: 80.34%\n",
      "Batch 13, Loss: 1.013899, Accuracy: 79.81%\n",
      "Batch 14, Loss: 0.977400, Accuracy: 79.58%\n",
      "Batch 15, Loss: 0.816599, Accuracy: 80.52%\n",
      "Batch 16, Loss: 1.001661, Accuracy: 80.08%\n",
      "Batch 17, Loss: 0.949049, Accuracy: 80.06%\n",
      "Batch 18, Loss: 0.996012, Accuracy: 79.77%\n",
      "Batch 19, Loss: 0.968380, Accuracy: 79.69%\n",
      "Batch 20, Loss: 1.041841, Accuracy: 79.14%\n",
      "Batch 21, Loss: 1.000613, Accuracy: 78.87%\n",
      "Batch 22, Loss: 0.963168, Accuracy: 78.84%\n",
      "Batch 23, Loss: 0.961359, Accuracy: 78.80%\n",
      "Batch 24, Loss: 0.931048, Accuracy: 78.91%\n",
      "Batch 25, Loss: 0.944233, Accuracy: 78.94%\n",
      "Batch 26, Loss: 0.892458, Accuracy: 79.27%\n",
      "Batch 27, Loss: 0.986101, Accuracy: 79.11%\n",
      "Batch 28, Loss: 0.926378, Accuracy: 79.24%\n",
      "Batch 29, Loss: 0.923014, Accuracy: 79.31%\n",
      "Batch 30, Loss: 0.972287, Accuracy: 79.27%\n",
      "Batch 31, Loss: 0.959501, Accuracy: 79.18%\n",
      "Batch 32, Loss: 0.891251, Accuracy: 79.30%\n",
      "Batch 33, Loss: 0.941221, Accuracy: 79.31%\n",
      "Batch 34, Loss: 0.935993, Accuracy: 79.37%\n",
      "Batch 35, Loss: 0.934044, Accuracy: 79.46%\n",
      "Batch 36, Loss: 0.898132, Accuracy: 79.60%\n",
      "Batch 37, Loss: 0.969485, Accuracy: 79.56%\n",
      "Batch 38, Loss: 0.949034, Accuracy: 79.56%\n",
      "Batch 39, Loss: 0.894135, Accuracy: 79.73%\n",
      "Batch 40, Loss: 0.961278, Accuracy: 79.65%\n",
      "Batch 41, Loss: 1.022828, Accuracy: 79.46%\n",
      "Batch 42, Loss: 0.927916, Accuracy: 79.50%\n",
      "Batch 43, Loss: 0.959225, Accuracy: 79.43%\n",
      "Batch 44, Loss: 1.033723, Accuracy: 79.23%\n",
      "Batch 45, Loss: 0.980525, Accuracy: 79.10%\n",
      "Batch 46, Loss: 0.964516, Accuracy: 79.08%\n",
      "Batch 47, Loss: 0.991558, Accuracy: 79.02%\n",
      "Batch 48, Loss: 0.945341, Accuracy: 79.10%\n",
      "Batch 49, Loss: 0.900171, Accuracy: 79.24%\n",
      "Batch 50, Loss: 1.000331, Accuracy: 79.12%\n",
      "Batch 51, Loss: 0.954761, Accuracy: 79.14%\n",
      "Batch 52, Loss: 0.949796, Accuracy: 79.12%\n",
      "Batch 53, Loss: 1.004512, Accuracy: 78.98%\n",
      "Batch 54, Loss: 1.064409, Accuracy: 78.76%\n",
      "Batch 55, Loss: 0.935434, Accuracy: 78.81%\n",
      "Batch 56, Loss: 0.972183, Accuracy: 78.77%\n",
      "Batch 57, Loss: 0.965451, Accuracy: 78.76%\n",
      "Batch 58, Loss: 0.937263, Accuracy: 78.77%\n",
      "Batch 59, Loss: 0.886540, Accuracy: 78.87%\n",
      "Batch 60, Loss: 1.017100, Accuracy: 78.70%\n",
      "Batch 61, Loss: 0.983295, Accuracy: 78.66%\n",
      "Batch 62, Loss: 0.920170, Accuracy: 78.70%\n",
      "Batch 63, Loss: 0.965945, Accuracy: 78.72%\n",
      "Batch 64, Loss: 0.937288, Accuracy: 78.74%\n",
      "Batch 65, Loss: 0.917138, Accuracy: 78.77%\n",
      "Batch 66, Loss: 0.975529, Accuracy: 78.72%\n",
      "Batch 67, Loss: 0.955659, Accuracy: 78.73%\n",
      "Batch 68, Loss: 0.917860, Accuracy: 78.79%\n",
      "Batch 69, Loss: 1.015520, Accuracy: 78.71%\n",
      "Batch 70, Loss: 0.947520, Accuracy: 78.75%\n",
      "Batch 71, Loss: 0.921940, Accuracy: 78.76%\n",
      "Batch 72, Loss: 0.897425, Accuracy: 78.86%\n",
      "Batch 73, Loss: 0.897714, Accuracy: 78.94%\n",
      "Batch 74, Loss: 1.023661, Accuracy: 78.84%\n",
      "Batch 75, Loss: 0.906609, Accuracy: 78.92%\n",
      "Batch 76, Loss: 0.976595, Accuracy: 78.87%\n",
      "Batch 77, Loss: 0.922271, Accuracy: 78.92%\n",
      "Batch 78, Loss: 0.959009, Accuracy: 78.91%\n",
      "Batch 79, Loss: 0.918634, Accuracy: 78.96%\n",
      "Batch 80, Loss: 0.927215, Accuracy: 78.98%\n",
      "Batch 81, Loss: 0.900841, Accuracy: 79.05%\n",
      "Batch 82, Loss: 0.945095, Accuracy: 79.06%\n",
      "Batch 83, Loss: 0.969006, Accuracy: 79.03%\n",
      "Batch 84, Loss: 0.910727, Accuracy: 79.09%\n",
      "Batch 85, Loss: 0.952049, Accuracy: 79.08%\n",
      "Batch 86, Loss: 0.968945, Accuracy: 79.07%\n",
      "Batch 87, Loss: 0.922606, Accuracy: 79.09%\n",
      "Batch 88, Loss: 0.893283, Accuracy: 79.17%\n",
      "Batch 89, Loss: 0.969137, Accuracy: 79.16%\n",
      "Batch 90, Loss: 0.907528, Accuracy: 79.20%\n",
      "Batch 91, Loss: 0.854995, Accuracy: 79.31%\n",
      "Batch 92, Loss: 0.938729, Accuracy: 79.33%\n",
      "Batch 93, Loss: 0.964793, Accuracy: 79.32%\n",
      "Batch 94, Loss: 0.946612, Accuracy: 79.34%\n",
      "Batch 95, Loss: 1.010155, Accuracy: 79.28%\n",
      "Batch 96, Loss: 0.931925, Accuracy: 79.31%\n",
      "Batch 97, Loss: 0.860768, Accuracy: 79.40%\n",
      "Batch 98, Loss: 0.947870, Accuracy: 79.42%\n",
      "Batch 99, Loss: 0.929493, Accuracy: 79.45%\n",
      "Batch 100, Loss: 0.954675, Accuracy: 79.44%\n",
      "Batch 101, Loss: 0.974042, Accuracy: 79.39%\n",
      "Batch 102, Loss: 0.924299, Accuracy: 79.41%\n",
      "Batch 103, Loss: 0.914832, Accuracy: 79.44%\n",
      "Batch 104, Loss: 1.044752, Accuracy: 79.34%\n",
      "Batch 105, Loss: 1.042324, Accuracy: 79.23%\n",
      "Batch 106, Loss: 0.946118, Accuracy: 79.22%\n",
      "Batch 107, Loss: 1.010303, Accuracy: 79.16%\n",
      "Batch 108, Loss: 0.961872, Accuracy: 79.14%\n",
      "Batch 109, Loss: 0.988191, Accuracy: 79.10%\n",
      "Batch 110, Loss: 0.946361, Accuracy: 79.11%\n",
      "Batch 111, Loss: 0.957311, Accuracy: 79.08%\n",
      "Batch 112, Loss: 0.899098, Accuracy: 79.13%\n",
      "Batch 113, Loss: 0.893905, Accuracy: 79.18%\n",
      "Batch 114, Loss: 0.907640, Accuracy: 79.21%\n",
      "Batch 115, Loss: 0.879510, Accuracy: 79.28%\n",
      "Batch 116, Loss: 0.917678, Accuracy: 79.31%\n",
      "Batch 117, Loss: 0.980761, Accuracy: 79.29%\n",
      "Batch 118, Loss: 0.995162, Accuracy: 79.24%\n",
      "Batch 119, Loss: 0.913979, Accuracy: 79.27%\n",
      "Batch 120, Loss: 0.932305, Accuracy: 79.28%\n",
      "Batch 121, Loss: 1.060429, Accuracy: 79.18%\n",
      "Batch 122, Loss: 0.942061, Accuracy: 79.18%\n",
      "Batch 123, Loss: 0.889890, Accuracy: 79.22%\n",
      "Batch 124, Loss: 0.960017, Accuracy: 79.18%\n",
      "Batch 125, Loss: 1.011099, Accuracy: 79.12%\n",
      "Batch 126, Loss: 0.900137, Accuracy: 79.17%\n",
      "Batch 127, Loss: 0.917487, Accuracy: 79.20%\n",
      "Batch 128, Loss: 0.921373, Accuracy: 79.21%\n",
      "Batch 129, Loss: 0.933250, Accuracy: 79.24%\n",
      "Batch 130, Loss: 0.945791, Accuracy: 79.25%\n",
      "Batch 131, Loss: 0.945949, Accuracy: 79.26%\n",
      "Batch 132, Loss: 0.947258, Accuracy: 79.26%\n",
      "Batch 133, Loss: 0.953171, Accuracy: 79.26%\n",
      "Batch 134, Loss: 0.913402, Accuracy: 79.29%\n",
      "Batch 135, Loss: 0.880950, Accuracy: 79.34%\n",
      "Batch 136, Loss: 0.949034, Accuracy: 79.34%\n",
      "Batch 137, Loss: 0.968484, Accuracy: 79.33%\n",
      "Batch 138, Loss: 1.042976, Accuracy: 79.26%\n",
      "Batch 139, Loss: 0.968784, Accuracy: 79.24%\n",
      "Batch 140, Loss: 0.865067, Accuracy: 79.29%\n",
      "Batch 141, Loss: 0.968568, Accuracy: 79.27%\n",
      "Batch 142, Loss: 0.905592, Accuracy: 79.30%\n",
      "Batch 143, Loss: 0.969546, Accuracy: 79.29%\n",
      "Batch 144, Loss: 1.011697, Accuracy: 79.24%\n",
      "Batch 145, Loss: 0.904514, Accuracy: 79.28%\n",
      "Batch 146, Loss: 0.962967, Accuracy: 79.27%\n",
      "Batch 147, Loss: 0.972770, Accuracy: 79.25%\n",
      "Batch 148, Loss: 0.908007, Accuracy: 79.29%\n",
      "Batch 149, Loss: 0.957300, Accuracy: 79.27%\n",
      "Batch 150, Loss: 0.989366, Accuracy: 79.24%\n",
      "Batch 151, Loss: 0.945910, Accuracy: 79.24%\n",
      "Batch 152, Loss: 0.996128, Accuracy: 79.21%\n",
      "Batch 153, Loss: 0.936556, Accuracy: 79.24%\n",
      "Batch 154, Loss: 0.913220, Accuracy: 79.26%\n",
      "Batch 155, Loss: 0.927292, Accuracy: 79.28%\n",
      "Batch 156, Loss: 0.857611, Accuracy: 79.36%\n",
      "Batch 157, Loss: 0.881848, Accuracy: 79.40%\n",
      "Batch 158, Loss: 0.936444, Accuracy: 79.41%\n",
      "Batch 159, Loss: 0.918769, Accuracy: 79.43%\n",
      "Batch 160, Loss: 0.944134, Accuracy: 79.43%\n",
      "Batch 161, Loss: 0.877954, Accuracy: 79.47%\n",
      "Batch 162, Loss: 0.929360, Accuracy: 79.49%\n",
      "Batch 163, Loss: 1.047262, Accuracy: 79.42%\n",
      "Batch 164, Loss: 0.960885, Accuracy: 79.39%\n",
      "Batch 165, Loss: 1.016424, Accuracy: 79.36%\n",
      "Batch 166, Loss: 0.963410, Accuracy: 79.34%\n",
      "Batch 167, Loss: 0.962080, Accuracy: 79.34%\n",
      "Batch 168, Loss: 0.896201, Accuracy: 79.38%\n",
      "Batch 169, Loss: 0.990697, Accuracy: 79.35%\n",
      "Batch 170, Loss: 0.988970, Accuracy: 79.33%\n",
      "Batch 171, Loss: 0.974883, Accuracy: 79.31%\n",
      "Batch 172, Loss: 0.913332, Accuracy: 79.33%\n",
      "Batch 173, Loss: 0.924184, Accuracy: 79.33%\n",
      "Batch 174, Loss: 0.924373, Accuracy: 79.35%\n",
      "Batch 175, Loss: 0.915311, Accuracy: 79.38%\n",
      "Batch 176, Loss: 0.912817, Accuracy: 79.39%\n",
      "Batch 177, Loss: 0.956354, Accuracy: 79.39%\n",
      "Batch 178, Loss: 0.928498, Accuracy: 79.41%\n",
      "Batch 179, Loss: 0.946245, Accuracy: 79.41%\n",
      "Batch 180, Loss: 0.933532, Accuracy: 79.42%\n",
      "Batch 181, Loss: 0.873758, Accuracy: 79.45%\n",
      "Batch 182, Loss: 1.013959, Accuracy: 79.42%\n",
      "Batch 183, Loss: 0.951738, Accuracy: 79.42%\n",
      "Batch 184, Loss: 0.895351, Accuracy: 79.45%\n",
      "Batch 185, Loss: 1.002143, Accuracy: 79.42%\n",
      "Batch 186, Loss: 1.028377, Accuracy: 79.38%\n",
      "Batch 187, Loss: 0.959092, Accuracy: 79.36%\n",
      "Batch 188, Loss: 0.855118, Accuracy: 79.40%\n",
      "Batch 189, Loss: 0.919968, Accuracy: 79.41%\n",
      "Batch 190, Loss: 0.988270, Accuracy: 79.39%\n",
      "Batch 191, Loss: 0.990384, Accuracy: 79.37%\n",
      "Batch 192, Loss: 0.885801, Accuracy: 79.40%\n",
      "Batch 193, Loss: 1.079376, Accuracy: 79.33%\n",
      "Batch 194, Loss: 0.970438, Accuracy: 79.32%\n",
      "Batch 195, Loss: 0.941340, Accuracy: 79.33%\n",
      "Batch 196, Loss: 0.895330, Accuracy: 79.35%\n",
      "Batch 197, Loss: 0.902307, Accuracy: 79.38%\n",
      "Batch 198, Loss: 0.932087, Accuracy: 79.39%\n",
      "Batch 199, Loss: 0.903235, Accuracy: 79.41%\n",
      "Batch 200, Loss: 0.896688, Accuracy: 79.44%\n",
      "Batch 201, Loss: 0.923399, Accuracy: 79.45%\n",
      "Batch 202, Loss: 0.975450, Accuracy: 79.43%\n",
      "Batch 203, Loss: 0.943812, Accuracy: 79.43%\n",
      "Batch 204, Loss: 0.924442, Accuracy: 79.44%\n",
      "Batch 205, Loss: 0.889936, Accuracy: 79.47%\n",
      "Batch 206, Loss: 0.882368, Accuracy: 79.51%\n",
      "Batch 207, Loss: 0.949097, Accuracy: 79.52%\n",
      "Batch 208, Loss: 0.932952, Accuracy: 79.54%\n",
      "Batch 209, Loss: 0.992814, Accuracy: 79.52%\n",
      "Batch 210, Loss: 0.897339, Accuracy: 79.54%\n",
      "Batch 211, Loss: 0.955292, Accuracy: 79.54%\n",
      "Batch 212, Loss: 0.965647, Accuracy: 79.53%\n",
      "Batch 213, Loss: 0.885501, Accuracy: 79.55%\n",
      "Training - Epoch 105, Loss: 0.946379, Accuracy: 79.55%\n",
      "Validation Batch 1, Loss: 0.917050, Accuracy: 84.38%\n",
      "Validation Batch 2, Loss: 0.958785, Accuracy: 80.47%\n",
      "Validation Batch 3, Loss: 1.014366, Accuracy: 78.12%\n",
      "Validation Batch 4, Loss: 0.952705, Accuracy: 78.91%\n",
      "Validation Batch 5, Loss: 0.928591, Accuracy: 79.69%\n",
      "Validation Batch 6, Loss: 0.883394, Accuracy: 80.73%\n",
      "Validation Batch 7, Loss: 0.971121, Accuracy: 80.36%\n",
      "Validation Batch 8, Loss: 0.982837, Accuracy: 79.69%\n",
      "Validation Batch 9, Loss: 0.995839, Accuracy: 79.34%\n",
      "Validation Batch 10, Loss: 0.982337, Accuracy: 79.06%\n",
      "Validation Batch 11, Loss: 0.916238, Accuracy: 79.55%\n",
      "Validation Batch 12, Loss: 0.895121, Accuracy: 80.08%\n",
      "Validation Batch 13, Loss: 1.004408, Accuracy: 79.57%\n",
      "Validation Batch 14, Loss: 0.989980, Accuracy: 79.24%\n",
      "Validation Batch 15, Loss: 0.953959, Accuracy: 79.27%\n",
      "Validation Batch 16, Loss: 0.925982, Accuracy: 79.39%\n",
      "Validation Batch 17, Loss: 0.985260, Accuracy: 79.23%\n",
      "Validation Batch 18, Loss: 0.936861, Accuracy: 79.34%\n",
      "Validation Batch 19, Loss: 0.985645, Accuracy: 79.11%\n",
      "Validation Batch 20, Loss: 0.941916, Accuracy: 79.06%\n",
      "Validation Batch 21, Loss: 0.970170, Accuracy: 79.09%\n",
      "Validation Batch 22, Loss: 0.991798, Accuracy: 78.91%\n",
      "Validation Batch 23, Loss: 1.046571, Accuracy: 78.46%\n",
      "Validation Batch 24, Loss: 1.023657, Accuracy: 78.12%\n",
      "Validation Batch 25, Loss: 0.977870, Accuracy: 78.00%\n",
      "Validation Batch 26, Loss: 0.946524, Accuracy: 78.12%\n",
      "Validation Batch 27, Loss: 0.908207, Accuracy: 78.27%\n",
      "Validation - Epoch 105, Loss: 0.962489, Accuracy: 78.27%\n",
      "Patienceâ€”13\n",
      "Epoch 106\n",
      "Batch 1, Loss: 0.928567, Accuracy: 79.69%\n",
      "Batch 2, Loss: 0.846045, Accuracy: 84.38%\n",
      "Batch 3, Loss: 0.874078, Accuracy: 84.38%\n",
      "Batch 4, Loss: 0.957662, Accuracy: 82.42%\n",
      "Batch 5, Loss: 0.924610, Accuracy: 82.50%\n",
      "Batch 6, Loss: 0.973267, Accuracy: 80.99%\n",
      "Batch 7, Loss: 0.925249, Accuracy: 81.25%\n",
      "Batch 8, Loss: 0.985922, Accuracy: 80.27%\n",
      "Batch 9, Loss: 0.919122, Accuracy: 80.56%\n",
      "Batch 10, Loss: 0.927106, Accuracy: 80.78%\n",
      "Batch 11, Loss: 1.002694, Accuracy: 80.11%\n",
      "Batch 12, Loss: 0.934355, Accuracy: 80.34%\n",
      "Batch 13, Loss: 1.013358, Accuracy: 79.45%\n",
      "Batch 14, Loss: 0.963201, Accuracy: 79.35%\n",
      "Batch 15, Loss: 1.044664, Accuracy: 78.75%\n",
      "Batch 16, Loss: 0.973323, Accuracy: 78.61%\n",
      "Batch 17, Loss: 0.954319, Accuracy: 78.68%\n",
      "Batch 18, Loss: 0.934646, Accuracy: 78.73%\n",
      "Batch 19, Loss: 0.963230, Accuracy: 78.62%\n",
      "Batch 20, Loss: 0.927291, Accuracy: 78.83%\n",
      "Batch 21, Loss: 0.984696, Accuracy: 78.65%\n",
      "Batch 22, Loss: 0.942323, Accuracy: 78.69%\n",
      "Batch 23, Loss: 0.908627, Accuracy: 78.94%\n",
      "Batch 24, Loss: 0.938159, Accuracy: 79.04%\n",
      "Batch 25, Loss: 0.957504, Accuracy: 79.00%\n",
      "Batch 26, Loss: 0.936283, Accuracy: 79.15%\n",
      "Batch 27, Loss: 1.055234, Accuracy: 78.70%\n",
      "Batch 28, Loss: 1.015972, Accuracy: 78.46%\n",
      "Batch 29, Loss: 1.007957, Accuracy: 78.23%\n",
      "Batch 30, Loss: 0.906986, Accuracy: 78.44%\n",
      "Batch 31, Loss: 0.939367, Accuracy: 78.48%\n",
      "Batch 32, Loss: 0.948566, Accuracy: 78.52%\n",
      "Batch 33, Loss: 0.978832, Accuracy: 78.46%\n",
      "Batch 34, Loss: 0.915667, Accuracy: 78.54%\n",
      "Batch 35, Loss: 0.987565, Accuracy: 78.44%\n",
      "Batch 36, Loss: 0.965707, Accuracy: 78.39%\n",
      "Batch 37, Loss: 0.889617, Accuracy: 78.55%\n",
      "Batch 38, Loss: 0.994444, Accuracy: 78.41%\n",
      "Batch 39, Loss: 1.031303, Accuracy: 78.25%\n",
      "Batch 40, Loss: 0.926159, Accuracy: 78.32%\n",
      "Batch 41, Loss: 0.928909, Accuracy: 78.39%\n",
      "Batch 42, Loss: 0.953763, Accuracy: 78.39%\n",
      "Batch 43, Loss: 0.943822, Accuracy: 78.42%\n",
      "Batch 44, Loss: 1.032795, Accuracy: 78.16%\n",
      "Batch 45, Loss: 0.946244, Accuracy: 78.19%\n",
      "Batch 46, Loss: 0.983960, Accuracy: 78.16%\n",
      "Batch 47, Loss: 0.928513, Accuracy: 78.22%\n",
      "Batch 48, Loss: 0.937830, Accuracy: 78.29%\n",
      "Batch 49, Loss: 0.916663, Accuracy: 78.35%\n",
      "Batch 50, Loss: 1.111591, Accuracy: 78.00%\n",
      "Batch 51, Loss: 0.869375, Accuracy: 78.19%\n",
      "Batch 52, Loss: 0.902030, Accuracy: 78.34%\n",
      "Batch 53, Loss: 0.967694, Accuracy: 78.33%\n",
      "Batch 54, Loss: 0.920458, Accuracy: 78.39%\n",
      "Batch 55, Loss: 0.899473, Accuracy: 78.49%\n",
      "Batch 56, Loss: 0.939030, Accuracy: 78.52%\n",
      "Batch 57, Loss: 0.919067, Accuracy: 78.59%\n",
      "Batch 58, Loss: 0.969840, Accuracy: 78.56%\n",
      "Batch 59, Loss: 0.909819, Accuracy: 78.63%\n",
      "Batch 60, Loss: 0.897445, Accuracy: 78.70%\n",
      "Batch 61, Loss: 0.924904, Accuracy: 78.71%\n",
      "Batch 62, Loss: 0.891569, Accuracy: 78.81%\n",
      "Batch 63, Loss: 0.950558, Accuracy: 78.82%\n",
      "Batch 64, Loss: 0.872596, Accuracy: 78.96%\n",
      "Batch 65, Loss: 0.945764, Accuracy: 78.97%\n",
      "Batch 66, Loss: 0.901127, Accuracy: 79.02%\n",
      "Batch 67, Loss: 1.026431, Accuracy: 78.89%\n",
      "Batch 68, Loss: 0.959348, Accuracy: 78.91%\n",
      "Batch 69, Loss: 0.841587, Accuracy: 79.08%\n",
      "Batch 70, Loss: 1.045520, Accuracy: 78.95%\n",
      "Batch 71, Loss: 0.921071, Accuracy: 78.96%\n",
      "Batch 72, Loss: 0.912835, Accuracy: 79.04%\n",
      "Batch 73, Loss: 0.988759, Accuracy: 78.96%\n",
      "Batch 74, Loss: 1.026594, Accuracy: 78.84%\n",
      "Batch 75, Loss: 1.020959, Accuracy: 78.73%\n",
      "Batch 76, Loss: 0.965281, Accuracy: 78.72%\n",
      "Batch 77, Loss: 0.948021, Accuracy: 78.73%\n",
      "Batch 78, Loss: 0.944457, Accuracy: 78.75%\n",
      "Batch 79, Loss: 0.940513, Accuracy: 78.78%\n",
      "Batch 80, Loss: 0.980908, Accuracy: 78.75%\n",
      "Batch 81, Loss: 0.976113, Accuracy: 78.74%\n",
      "Batch 82, Loss: 1.007743, Accuracy: 78.68%\n",
      "Batch 83, Loss: 0.922365, Accuracy: 78.73%\n",
      "Batch 84, Loss: 0.891711, Accuracy: 78.81%\n",
      "Batch 85, Loss: 0.970620, Accuracy: 78.79%\n",
      "Batch 86, Loss: 1.063948, Accuracy: 78.65%\n",
      "Batch 87, Loss: 0.937280, Accuracy: 78.66%\n",
      "Batch 88, Loss: 0.955217, Accuracy: 78.66%\n",
      "Batch 89, Loss: 0.945737, Accuracy: 78.67%\n",
      "Batch 90, Loss: 0.925139, Accuracy: 78.70%\n",
      "Batch 91, Loss: 0.918701, Accuracy: 78.76%\n",
      "Batch 92, Loss: 0.918977, Accuracy: 78.80%\n",
      "Batch 93, Loss: 0.880698, Accuracy: 78.88%\n",
      "Batch 94, Loss: 0.900521, Accuracy: 78.92%\n",
      "Batch 95, Loss: 0.895184, Accuracy: 78.98%\n",
      "Batch 96, Loss: 0.870732, Accuracy: 79.10%\n",
      "Batch 97, Loss: 1.011257, Accuracy: 79.06%\n",
      "Batch 98, Loss: 0.937767, Accuracy: 79.07%\n",
      "Batch 99, Loss: 0.911708, Accuracy: 79.09%\n",
      "Batch 100, Loss: 0.965349, Accuracy: 79.09%\n",
      "Batch 101, Loss: 0.974039, Accuracy: 79.08%\n",
      "Batch 102, Loss: 0.972872, Accuracy: 79.04%\n",
      "Batch 103, Loss: 0.938193, Accuracy: 79.05%\n",
      "Batch 104, Loss: 0.950634, Accuracy: 79.06%\n",
      "Batch 105, Loss: 0.984220, Accuracy: 79.02%\n",
      "Batch 106, Loss: 1.038035, Accuracy: 78.94%\n",
      "Batch 107, Loss: 0.914613, Accuracy: 78.97%\n",
      "Batch 108, Loss: 0.945551, Accuracy: 78.98%\n",
      "Batch 109, Loss: 0.955215, Accuracy: 78.97%\n",
      "Batch 110, Loss: 0.968820, Accuracy: 78.93%\n",
      "Batch 111, Loss: 1.008659, Accuracy: 78.87%\n",
      "Batch 112, Loss: 0.972647, Accuracy: 78.85%\n",
      "Batch 113, Loss: 0.930866, Accuracy: 78.87%\n",
      "Batch 114, Loss: 0.935551, Accuracy: 78.89%\n",
      "Batch 115, Loss: 0.917453, Accuracy: 78.93%\n",
      "Batch 116, Loss: 0.902629, Accuracy: 78.99%\n",
      "Batch 117, Loss: 0.956444, Accuracy: 78.97%\n",
      "Batch 118, Loss: 0.920482, Accuracy: 79.00%\n",
      "Batch 119, Loss: 1.034532, Accuracy: 78.93%\n",
      "Batch 120, Loss: 0.958537, Accuracy: 78.93%\n",
      "Batch 121, Loss: 0.977718, Accuracy: 78.90%\n",
      "Batch 122, Loss: 0.909209, Accuracy: 78.94%\n",
      "Batch 123, Loss: 0.878743, Accuracy: 79.00%\n",
      "Batch 124, Loss: 0.898645, Accuracy: 79.06%\n",
      "Batch 125, Loss: 0.889868, Accuracy: 79.11%\n",
      "Batch 126, Loss: 0.985376, Accuracy: 79.08%\n",
      "Batch 127, Loss: 0.883500, Accuracy: 79.13%\n",
      "Batch 128, Loss: 0.919401, Accuracy: 79.16%\n",
      "Batch 129, Loss: 0.994584, Accuracy: 79.13%\n",
      "Batch 130, Loss: 0.950699, Accuracy: 79.12%\n",
      "Batch 131, Loss: 0.927532, Accuracy: 79.13%\n",
      "Batch 132, Loss: 0.905564, Accuracy: 79.17%\n",
      "Batch 133, Loss: 0.964072, Accuracy: 79.17%\n",
      "Batch 134, Loss: 1.000807, Accuracy: 79.13%\n",
      "Batch 135, Loss: 0.880879, Accuracy: 79.19%\n",
      "Batch 136, Loss: 0.869120, Accuracy: 79.25%\n",
      "Batch 137, Loss: 0.888685, Accuracy: 79.31%\n",
      "Batch 138, Loss: 0.954245, Accuracy: 79.30%\n",
      "Batch 139, Loss: 0.925131, Accuracy: 79.32%\n",
      "Batch 140, Loss: 0.924988, Accuracy: 79.33%\n",
      "Batch 141, Loss: 0.895414, Accuracy: 79.37%\n",
      "Batch 142, Loss: 0.895640, Accuracy: 79.40%\n",
      "Batch 143, Loss: 0.953393, Accuracy: 79.39%\n",
      "Batch 144, Loss: 1.029205, Accuracy: 79.33%\n",
      "Batch 145, Loss: 0.982588, Accuracy: 79.31%\n",
      "Batch 146, Loss: 0.957736, Accuracy: 79.30%\n",
      "Batch 147, Loss: 0.920254, Accuracy: 79.32%\n",
      "Batch 148, Loss: 0.947305, Accuracy: 79.32%\n",
      "Batch 149, Loss: 1.040305, Accuracy: 79.26%\n",
      "Batch 150, Loss: 0.984396, Accuracy: 79.26%\n",
      "Batch 151, Loss: 0.945993, Accuracy: 79.25%\n",
      "Batch 152, Loss: 0.946027, Accuracy: 79.26%\n",
      "Batch 153, Loss: 0.865794, Accuracy: 79.31%\n",
      "Batch 154, Loss: 0.888013, Accuracy: 79.34%\n",
      "Batch 155, Loss: 0.969565, Accuracy: 79.32%\n",
      "Batch 156, Loss: 0.978984, Accuracy: 79.31%\n",
      "Batch 157, Loss: 0.832193, Accuracy: 79.38%\n",
      "Batch 158, Loss: 0.931609, Accuracy: 79.39%\n",
      "Batch 159, Loss: 0.988605, Accuracy: 79.36%\n",
      "Batch 160, Loss: 0.949326, Accuracy: 79.37%\n",
      "Batch 161, Loss: 1.036294, Accuracy: 79.32%\n",
      "Batch 162, Loss: 0.865996, Accuracy: 79.36%\n",
      "Batch 163, Loss: 0.950778, Accuracy: 79.35%\n",
      "Batch 164, Loss: 0.916552, Accuracy: 79.37%\n",
      "Batch 165, Loss: 0.891490, Accuracy: 79.41%\n",
      "Batch 166, Loss: 0.960880, Accuracy: 79.40%\n",
      "Batch 167, Loss: 0.905464, Accuracy: 79.42%\n",
      "Batch 168, Loss: 0.898391, Accuracy: 79.45%\n",
      "Batch 169, Loss: 0.987143, Accuracy: 79.42%\n",
      "Batch 170, Loss: 0.938402, Accuracy: 79.42%\n",
      "Batch 171, Loss: 0.944956, Accuracy: 79.43%\n",
      "Batch 172, Loss: 0.865260, Accuracy: 79.49%\n",
      "Batch 173, Loss: 0.969451, Accuracy: 79.47%\n",
      "Batch 174, Loss: 1.019353, Accuracy: 79.43%\n",
      "Batch 175, Loss: 0.878844, Accuracy: 79.47%\n",
      "Batch 176, Loss: 0.802220, Accuracy: 79.56%\n",
      "Batch 177, Loss: 0.888120, Accuracy: 79.59%\n",
      "Batch 178, Loss: 0.939424, Accuracy: 79.60%\n",
      "Batch 179, Loss: 0.911371, Accuracy: 79.63%\n",
      "Batch 180, Loss: 0.918308, Accuracy: 79.64%\n",
      "Batch 181, Loss: 0.939168, Accuracy: 79.64%\n",
      "Batch 182, Loss: 0.993688, Accuracy: 79.61%\n",
      "Batch 183, Loss: 0.935665, Accuracy: 79.61%\n",
      "Batch 184, Loss: 0.974833, Accuracy: 79.59%\n",
      "Batch 185, Loss: 0.995341, Accuracy: 79.58%\n",
      "Batch 186, Loss: 0.992363, Accuracy: 79.55%\n",
      "Batch 187, Loss: 0.988532, Accuracy: 79.53%\n",
      "Batch 188, Loss: 0.958328, Accuracy: 79.52%\n",
      "Batch 189, Loss: 0.888629, Accuracy: 79.56%\n",
      "Batch 190, Loss: 0.855532, Accuracy: 79.61%\n",
      "Batch 191, Loss: 0.814225, Accuracy: 79.68%\n",
      "Batch 192, Loss: 0.953772, Accuracy: 79.69%\n",
      "Batch 193, Loss: 0.879050, Accuracy: 79.73%\n",
      "Batch 194, Loss: 0.938155, Accuracy: 79.73%\n",
      "Batch 195, Loss: 0.879041, Accuracy: 79.77%\n",
      "Batch 196, Loss: 0.893274, Accuracy: 79.81%\n",
      "Batch 197, Loss: 1.026052, Accuracy: 79.77%\n",
      "Batch 198, Loss: 0.905505, Accuracy: 79.79%\n",
      "Batch 199, Loss: 0.982366, Accuracy: 79.77%\n",
      "Batch 200, Loss: 0.946984, Accuracy: 79.78%\n",
      "Batch 201, Loss: 0.943879, Accuracy: 79.79%\n",
      "Batch 202, Loss: 0.955682, Accuracy: 79.77%\n",
      "Batch 203, Loss: 0.963751, Accuracy: 79.76%\n",
      "Batch 204, Loss: 0.959153, Accuracy: 79.74%\n",
      "Batch 205, Loss: 0.987497, Accuracy: 79.71%\n",
      "Batch 206, Loss: 0.944406, Accuracy: 79.70%\n",
      "Batch 207, Loss: 0.964257, Accuracy: 79.70%\n",
      "Batch 208, Loss: 0.909526, Accuracy: 79.72%\n",
      "Batch 209, Loss: 0.909871, Accuracy: 79.75%\n",
      "Batch 210, Loss: 0.927335, Accuracy: 79.75%\n",
      "Batch 211, Loss: 0.967061, Accuracy: 79.74%\n",
      "Batch 212, Loss: 0.996252, Accuracy: 79.72%\n",
      "Batch 213, Loss: 0.890396, Accuracy: 79.75%\n",
      "Training - Epoch 106, Loss: 0.943929, Accuracy: 79.75%\n",
      "Validation Batch 1, Loss: 0.877098, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.901338, Accuracy: 86.72%\n",
      "Validation Batch 3, Loss: 0.991937, Accuracy: 82.29%\n",
      "Validation Batch 4, Loss: 0.931979, Accuracy: 82.03%\n",
      "Validation Batch 5, Loss: 0.895401, Accuracy: 82.50%\n",
      "Validation Batch 6, Loss: 0.832789, Accuracy: 84.11%\n",
      "Validation Batch 7, Loss: 0.936777, Accuracy: 83.48%\n",
      "Validation Batch 8, Loss: 0.942663, Accuracy: 83.20%\n",
      "Validation Batch 9, Loss: 0.959738, Accuracy: 82.47%\n",
      "Validation Batch 10, Loss: 0.956479, Accuracy: 82.03%\n",
      "Validation Batch 11, Loss: 0.897576, Accuracy: 82.24%\n",
      "Validation Batch 12, Loss: 0.870072, Accuracy: 82.68%\n",
      "Validation Batch 13, Loss: 0.954340, Accuracy: 82.33%\n",
      "Validation Batch 14, Loss: 0.938504, Accuracy: 82.37%\n",
      "Validation Batch 15, Loss: 0.940445, Accuracy: 82.08%\n",
      "Validation Batch 16, Loss: 0.894160, Accuracy: 82.32%\n",
      "Validation Batch 17, Loss: 0.939075, Accuracy: 82.17%\n",
      "Validation Batch 18, Loss: 0.909961, Accuracy: 82.20%\n",
      "Validation Batch 19, Loss: 0.955506, Accuracy: 81.99%\n",
      "Validation Batch 20, Loss: 0.896577, Accuracy: 82.03%\n",
      "Validation Batch 21, Loss: 0.943099, Accuracy: 81.92%\n",
      "Validation Batch 22, Loss: 0.922749, Accuracy: 81.96%\n",
      "Validation Batch 23, Loss: 0.988434, Accuracy: 81.59%\n",
      "Validation Batch 24, Loss: 0.992089, Accuracy: 81.32%\n",
      "Validation Batch 25, Loss: 0.923566, Accuracy: 81.38%\n",
      "Validation Batch 26, Loss: 0.935406, Accuracy: 81.31%\n",
      "Validation Batch 27, Loss: 0.858797, Accuracy: 81.44%\n",
      "Validation - Epoch 106, Loss: 0.925428, Accuracy: 81.44%\n",
      "Patienceâ€”14\n",
      "Epoch 107\n",
      "Batch 1, Loss: 0.951402, Accuracy: 78.12%\n",
      "Batch 2, Loss: 0.919931, Accuracy: 80.47%\n",
      "Batch 3, Loss: 0.910915, Accuracy: 81.25%\n",
      "Batch 4, Loss: 0.999213, Accuracy: 79.30%\n",
      "Batch 5, Loss: 0.871312, Accuracy: 80.62%\n",
      "Batch 6, Loss: 1.020850, Accuracy: 79.17%\n",
      "Batch 7, Loss: 0.944315, Accuracy: 79.46%\n",
      "Batch 8, Loss: 0.984405, Accuracy: 79.10%\n",
      "Batch 9, Loss: 1.034777, Accuracy: 78.12%\n",
      "Batch 10, Loss: 0.909904, Accuracy: 78.59%\n",
      "Batch 11, Loss: 0.938198, Accuracy: 78.98%\n",
      "Batch 12, Loss: 0.948577, Accuracy: 78.91%\n",
      "Batch 13, Loss: 0.953154, Accuracy: 78.85%\n",
      "Batch 14, Loss: 0.979300, Accuracy: 78.79%\n",
      "Batch 15, Loss: 1.018198, Accuracy: 78.44%\n",
      "Batch 16, Loss: 0.945998, Accuracy: 78.52%\n",
      "Batch 17, Loss: 0.874896, Accuracy: 79.04%\n",
      "Batch 18, Loss: 1.005073, Accuracy: 78.73%\n",
      "Batch 19, Loss: 0.986395, Accuracy: 78.54%\n",
      "Batch 20, Loss: 0.877664, Accuracy: 78.91%\n",
      "Batch 21, Loss: 0.925040, Accuracy: 79.02%\n",
      "Batch 22, Loss: 0.854974, Accuracy: 79.40%\n",
      "Batch 23, Loss: 0.939218, Accuracy: 79.55%\n",
      "Batch 24, Loss: 0.921146, Accuracy: 79.69%\n",
      "Batch 25, Loss: 1.002579, Accuracy: 79.50%\n",
      "Batch 26, Loss: 0.968485, Accuracy: 79.45%\n",
      "Batch 27, Loss: 0.897233, Accuracy: 79.63%\n",
      "Batch 28, Loss: 0.988146, Accuracy: 79.46%\n",
      "Batch 29, Loss: 1.017784, Accuracy: 79.20%\n",
      "Batch 30, Loss: 0.957276, Accuracy: 79.22%\n",
      "Batch 31, Loss: 0.960473, Accuracy: 79.23%\n",
      "Batch 32, Loss: 0.932421, Accuracy: 79.30%\n",
      "Batch 33, Loss: 0.926179, Accuracy: 79.45%\n",
      "Batch 34, Loss: 0.962056, Accuracy: 79.41%\n",
      "Batch 35, Loss: 0.952907, Accuracy: 79.42%\n",
      "Batch 36, Loss: 1.008323, Accuracy: 79.25%\n",
      "Batch 37, Loss: 0.886219, Accuracy: 79.43%\n",
      "Batch 38, Loss: 0.911162, Accuracy: 79.52%\n",
      "Batch 39, Loss: 0.978695, Accuracy: 79.45%\n",
      "Batch 40, Loss: 1.098915, Accuracy: 79.02%\n",
      "Batch 41, Loss: 0.957850, Accuracy: 79.00%\n",
      "Batch 42, Loss: 0.903008, Accuracy: 79.13%\n",
      "Batch 43, Loss: 0.920971, Accuracy: 79.22%\n",
      "Batch 44, Loss: 0.849720, Accuracy: 79.40%\n",
      "Batch 45, Loss: 0.977196, Accuracy: 79.38%\n",
      "Batch 46, Loss: 0.944554, Accuracy: 79.38%\n",
      "Batch 47, Loss: 0.926057, Accuracy: 79.45%\n",
      "Batch 48, Loss: 0.946861, Accuracy: 79.46%\n",
      "Batch 49, Loss: 0.896023, Accuracy: 79.56%\n",
      "Batch 50, Loss: 0.960575, Accuracy: 79.53%\n",
      "Batch 51, Loss: 0.951458, Accuracy: 79.53%\n",
      "Batch 52, Loss: 0.890204, Accuracy: 79.66%\n",
      "Batch 53, Loss: 0.938049, Accuracy: 79.63%\n",
      "Batch 54, Loss: 0.953140, Accuracy: 79.63%\n",
      "Batch 55, Loss: 0.991332, Accuracy: 79.57%\n",
      "Batch 56, Loss: 0.932226, Accuracy: 79.60%\n",
      "Batch 57, Loss: 0.906898, Accuracy: 79.71%\n",
      "Batch 58, Loss: 0.971815, Accuracy: 79.66%\n",
      "Batch 59, Loss: 0.940775, Accuracy: 79.66%\n",
      "Batch 60, Loss: 0.931735, Accuracy: 79.71%\n",
      "Batch 61, Loss: 0.940419, Accuracy: 79.71%\n",
      "Batch 62, Loss: 0.919849, Accuracy: 79.74%\n",
      "Batch 63, Loss: 0.914851, Accuracy: 79.79%\n",
      "Batch 64, Loss: 0.962123, Accuracy: 79.74%\n",
      "Batch 65, Loss: 0.940576, Accuracy: 79.74%\n",
      "Batch 66, Loss: 0.955795, Accuracy: 79.69%\n",
      "Batch 67, Loss: 0.933413, Accuracy: 79.69%\n",
      "Batch 68, Loss: 0.962743, Accuracy: 79.64%\n",
      "Batch 69, Loss: 1.027665, Accuracy: 79.53%\n",
      "Batch 70, Loss: 0.985497, Accuracy: 79.46%\n",
      "Batch 71, Loss: 0.922037, Accuracy: 79.51%\n",
      "Batch 72, Loss: 0.851111, Accuracy: 79.62%\n",
      "Batch 73, Loss: 0.927794, Accuracy: 79.64%\n",
      "Batch 74, Loss: 0.919601, Accuracy: 79.71%\n",
      "Batch 75, Loss: 0.893914, Accuracy: 79.79%\n",
      "Batch 76, Loss: 0.924661, Accuracy: 79.81%\n",
      "Batch 77, Loss: 0.886544, Accuracy: 79.89%\n",
      "Batch 78, Loss: 0.967450, Accuracy: 79.85%\n",
      "Batch 79, Loss: 0.937158, Accuracy: 79.85%\n",
      "Batch 80, Loss: 0.937515, Accuracy: 79.86%\n",
      "Batch 81, Loss: 0.929296, Accuracy: 79.86%\n",
      "Batch 82, Loss: 1.012890, Accuracy: 79.78%\n",
      "Batch 83, Loss: 0.920742, Accuracy: 79.82%\n",
      "Batch 84, Loss: 0.965326, Accuracy: 79.80%\n",
      "Batch 85, Loss: 0.931134, Accuracy: 79.82%\n",
      "Batch 86, Loss: 0.972822, Accuracy: 79.81%\n",
      "Batch 87, Loss: 0.943945, Accuracy: 79.80%\n",
      "Batch 88, Loss: 0.938615, Accuracy: 79.79%\n",
      "Batch 89, Loss: 0.916617, Accuracy: 79.83%\n",
      "Batch 90, Loss: 0.900214, Accuracy: 79.88%\n",
      "Batch 91, Loss: 0.916692, Accuracy: 79.93%\n",
      "Batch 92, Loss: 0.912323, Accuracy: 79.96%\n",
      "Batch 93, Loss: 0.952080, Accuracy: 79.96%\n",
      "Batch 94, Loss: 0.892874, Accuracy: 80.02%\n",
      "Batch 95, Loss: 1.018136, Accuracy: 79.95%\n",
      "Batch 96, Loss: 0.990192, Accuracy: 79.88%\n",
      "Batch 97, Loss: 0.940823, Accuracy: 79.90%\n",
      "Batch 98, Loss: 0.996315, Accuracy: 79.85%\n",
      "Batch 99, Loss: 0.843682, Accuracy: 79.96%\n",
      "Batch 100, Loss: 0.934897, Accuracy: 79.97%\n",
      "Batch 101, Loss: 0.962365, Accuracy: 79.95%\n",
      "Batch 102, Loss: 0.893960, Accuracy: 80.01%\n",
      "Batch 103, Loss: 0.932831, Accuracy: 80.02%\n",
      "Batch 104, Loss: 0.994877, Accuracy: 79.97%\n",
      "Batch 105, Loss: 0.998877, Accuracy: 79.90%\n",
      "Batch 106, Loss: 0.923259, Accuracy: 79.91%\n",
      "Batch 107, Loss: 0.919035, Accuracy: 79.94%\n",
      "Batch 108, Loss: 0.963179, Accuracy: 79.95%\n",
      "Batch 109, Loss: 0.983475, Accuracy: 79.90%\n",
      "Batch 110, Loss: 0.959998, Accuracy: 79.90%\n",
      "Batch 111, Loss: 0.941319, Accuracy: 79.90%\n",
      "Batch 112, Loss: 0.871278, Accuracy: 79.95%\n",
      "Batch 113, Loss: 0.971678, Accuracy: 79.92%\n",
      "Batch 114, Loss: 0.918368, Accuracy: 79.95%\n",
      "Batch 115, Loss: 0.912573, Accuracy: 79.99%\n",
      "Batch 116, Loss: 0.938363, Accuracy: 79.98%\n",
      "Batch 117, Loss: 0.953807, Accuracy: 79.97%\n",
      "Batch 118, Loss: 0.980455, Accuracy: 79.93%\n",
      "Batch 119, Loss: 0.969128, Accuracy: 79.91%\n",
      "Batch 120, Loss: 0.931265, Accuracy: 79.91%\n",
      "Batch 121, Loss: 0.970663, Accuracy: 79.88%\n",
      "Batch 122, Loss: 0.959063, Accuracy: 79.88%\n",
      "Batch 123, Loss: 1.018582, Accuracy: 79.81%\n",
      "Batch 124, Loss: 0.978880, Accuracy: 79.79%\n",
      "Batch 125, Loss: 0.936010, Accuracy: 79.80%\n",
      "Batch 126, Loss: 0.943366, Accuracy: 79.79%\n",
      "Batch 127, Loss: 0.938848, Accuracy: 79.79%\n",
      "Batch 128, Loss: 0.935418, Accuracy: 79.80%\n",
      "Batch 129, Loss: 0.904638, Accuracy: 79.83%\n",
      "Batch 130, Loss: 0.860534, Accuracy: 79.89%\n",
      "Batch 131, Loss: 0.968252, Accuracy: 79.88%\n",
      "Batch 132, Loss: 0.951483, Accuracy: 79.87%\n",
      "Batch 133, Loss: 0.964874, Accuracy: 79.85%\n",
      "Batch 134, Loss: 0.993709, Accuracy: 79.80%\n",
      "Batch 135, Loss: 0.901739, Accuracy: 79.83%\n",
      "Batch 136, Loss: 0.916518, Accuracy: 79.86%\n",
      "Batch 137, Loss: 0.920703, Accuracy: 79.88%\n",
      "Batch 138, Loss: 0.967396, Accuracy: 79.87%\n",
      "Batch 139, Loss: 0.831070, Accuracy: 79.95%\n",
      "Batch 140, Loss: 0.956773, Accuracy: 79.93%\n",
      "Batch 141, Loss: 0.883698, Accuracy: 79.96%\n",
      "Batch 142, Loss: 0.942844, Accuracy: 79.95%\n",
      "Batch 143, Loss: 0.923657, Accuracy: 79.96%\n",
      "Batch 144, Loss: 0.881690, Accuracy: 80.00%\n",
      "Batch 145, Loss: 0.908510, Accuracy: 80.02%\n",
      "Batch 146, Loss: 0.942535, Accuracy: 80.02%\n",
      "Batch 147, Loss: 1.010687, Accuracy: 79.96%\n",
      "Batch 148, Loss: 0.898455, Accuracy: 80.00%\n",
      "Batch 149, Loss: 0.915067, Accuracy: 80.02%\n",
      "Batch 150, Loss: 0.905865, Accuracy: 80.05%\n",
      "Batch 151, Loss: 0.874801, Accuracy: 80.08%\n",
      "Batch 152, Loss: 0.988836, Accuracy: 80.05%\n",
      "Batch 153, Loss: 0.882927, Accuracy: 80.09%\n",
      "Batch 154, Loss: 0.993375, Accuracy: 80.05%\n",
      "Batch 155, Loss: 1.074365, Accuracy: 79.97%\n",
      "Batch 156, Loss: 0.913912, Accuracy: 79.99%\n",
      "Batch 157, Loss: 0.954150, Accuracy: 79.98%\n",
      "Batch 158, Loss: 0.949281, Accuracy: 79.96%\n",
      "Batch 159, Loss: 0.892192, Accuracy: 79.99%\n",
      "Batch 160, Loss: 0.882516, Accuracy: 80.04%\n",
      "Batch 161, Loss: 0.968899, Accuracy: 80.03%\n",
      "Batch 162, Loss: 1.000007, Accuracy: 79.99%\n",
      "Batch 163, Loss: 0.985799, Accuracy: 79.97%\n",
      "Batch 164, Loss: 0.927698, Accuracy: 79.97%\n",
      "Batch 165, Loss: 0.895979, Accuracy: 80.00%\n",
      "Batch 166, Loss: 0.963453, Accuracy: 79.99%\n",
      "Batch 167, Loss: 0.944083, Accuracy: 79.99%\n",
      "Batch 168, Loss: 0.910759, Accuracy: 79.99%\n",
      "Batch 169, Loss: 0.913906, Accuracy: 80.00%\n",
      "Batch 170, Loss: 0.930029, Accuracy: 80.01%\n",
      "Batch 171, Loss: 1.001824, Accuracy: 79.96%\n",
      "Batch 172, Loss: 0.930037, Accuracy: 79.98%\n",
      "Batch 173, Loss: 0.940205, Accuracy: 79.98%\n",
      "Batch 174, Loss: 0.945148, Accuracy: 79.98%\n",
      "Batch 175, Loss: 0.937623, Accuracy: 79.99%\n",
      "Batch 176, Loss: 0.979020, Accuracy: 79.96%\n",
      "Batch 177, Loss: 0.995419, Accuracy: 79.93%\n",
      "Batch 178, Loss: 0.985343, Accuracy: 79.91%\n",
      "Batch 179, Loss: 0.955005, Accuracy: 79.90%\n",
      "Batch 180, Loss: 0.875525, Accuracy: 79.94%\n",
      "Batch 181, Loss: 0.945946, Accuracy: 79.94%\n",
      "Batch 182, Loss: 0.958762, Accuracy: 79.95%\n",
      "Batch 183, Loss: 0.964001, Accuracy: 79.93%\n",
      "Batch 184, Loss: 0.956708, Accuracy: 79.93%\n",
      "Batch 185, Loss: 0.962467, Accuracy: 79.92%\n",
      "Batch 186, Loss: 0.940265, Accuracy: 79.91%\n",
      "Batch 187, Loss: 0.894249, Accuracy: 79.94%\n",
      "Batch 188, Loss: 1.051769, Accuracy: 79.87%\n",
      "Batch 189, Loss: 0.975842, Accuracy: 79.86%\n",
      "Batch 190, Loss: 1.044086, Accuracy: 79.80%\n",
      "Batch 191, Loss: 0.966579, Accuracy: 79.79%\n",
      "Batch 192, Loss: 1.047018, Accuracy: 79.72%\n",
      "Batch 193, Loss: 0.965744, Accuracy: 79.71%\n",
      "Batch 194, Loss: 0.887110, Accuracy: 79.74%\n",
      "Batch 195, Loss: 1.002699, Accuracy: 79.71%\n",
      "Batch 196, Loss: 0.965006, Accuracy: 79.70%\n",
      "Batch 197, Loss: 0.871057, Accuracy: 79.74%\n",
      "Batch 198, Loss: 0.881478, Accuracy: 79.77%\n",
      "Batch 199, Loss: 0.933401, Accuracy: 79.77%\n",
      "Batch 200, Loss: 0.949119, Accuracy: 79.77%\n",
      "Batch 201, Loss: 0.994223, Accuracy: 79.73%\n",
      "Batch 202, Loss: 0.943762, Accuracy: 79.73%\n",
      "Batch 203, Loss: 0.952169, Accuracy: 79.71%\n",
      "Batch 204, Loss: 0.991756, Accuracy: 79.69%\n",
      "Batch 205, Loss: 0.947054, Accuracy: 79.69%\n",
      "Batch 206, Loss: 0.943064, Accuracy: 79.69%\n",
      "Batch 207, Loss: 0.901498, Accuracy: 79.71%\n",
      "Batch 208, Loss: 0.999023, Accuracy: 79.68%\n",
      "Batch 209, Loss: 0.889045, Accuracy: 79.71%\n",
      "Batch 210, Loss: 1.015889, Accuracy: 79.67%\n",
      "Batch 211, Loss: 0.977160, Accuracy: 79.65%\n",
      "Batch 212, Loss: 0.932506, Accuracy: 79.65%\n",
      "Batch 213, Loss: 1.008794, Accuracy: 79.63%\n",
      "Training - Epoch 107, Loss: 0.945528, Accuracy: 79.63%\n",
      "Validation Batch 1, Loss: 0.903700, Accuracy: 84.38%\n",
      "Validation Batch 2, Loss: 0.948742, Accuracy: 82.03%\n",
      "Validation Batch 3, Loss: 1.009870, Accuracy: 79.17%\n",
      "Validation Batch 4, Loss: 0.942653, Accuracy: 79.30%\n",
      "Validation Batch 5, Loss: 0.921093, Accuracy: 80.00%\n",
      "Validation Batch 6, Loss: 0.880098, Accuracy: 80.99%\n",
      "Validation Batch 7, Loss: 0.965737, Accuracy: 80.58%\n",
      "Validation Batch 8, Loss: 0.982342, Accuracy: 80.08%\n",
      "Validation Batch 9, Loss: 0.998576, Accuracy: 79.51%\n",
      "Validation Batch 10, Loss: 0.980526, Accuracy: 79.22%\n",
      "Validation Batch 11, Loss: 0.912318, Accuracy: 79.69%\n",
      "Validation Batch 12, Loss: 0.891077, Accuracy: 80.21%\n",
      "Validation Batch 13, Loss: 0.980240, Accuracy: 79.93%\n",
      "Validation Batch 14, Loss: 0.978056, Accuracy: 79.69%\n",
      "Validation Batch 15, Loss: 0.945504, Accuracy: 79.69%\n",
      "Validation Batch 16, Loss: 0.919861, Accuracy: 79.88%\n",
      "Validation Batch 17, Loss: 0.983857, Accuracy: 79.60%\n",
      "Validation Batch 18, Loss: 0.926079, Accuracy: 79.60%\n",
      "Validation Batch 19, Loss: 0.976159, Accuracy: 79.44%\n",
      "Validation Batch 20, Loss: 0.914417, Accuracy: 79.69%\n",
      "Validation Batch 21, Loss: 0.958824, Accuracy: 79.69%\n",
      "Validation Batch 22, Loss: 0.982337, Accuracy: 79.47%\n",
      "Validation Batch 23, Loss: 1.024567, Accuracy: 79.01%\n",
      "Validation Batch 24, Loss: 1.011215, Accuracy: 78.78%\n",
      "Validation Batch 25, Loss: 0.954641, Accuracy: 78.94%\n",
      "Validation Batch 26, Loss: 0.939611, Accuracy: 79.09%\n",
      "Validation Batch 27, Loss: 0.896789, Accuracy: 79.21%\n",
      "Validation - Epoch 107, Loss: 0.952922, Accuracy: 79.21%\n",
      "Patienceâ€”15\n",
      "Epoch 108\n",
      "Batch 1, Loss: 0.941306, Accuracy: 81.25%\n",
      "Batch 2, Loss: 0.952680, Accuracy: 80.47%\n",
      "Batch 3, Loss: 0.984040, Accuracy: 79.17%\n",
      "Batch 4, Loss: 0.955624, Accuracy: 78.91%\n",
      "Batch 5, Loss: 0.906170, Accuracy: 80.31%\n",
      "Batch 6, Loss: 1.002774, Accuracy: 79.17%\n",
      "Batch 7, Loss: 0.965285, Accuracy: 78.57%\n",
      "Batch 8, Loss: 0.977844, Accuracy: 78.52%\n",
      "Batch 9, Loss: 0.909975, Accuracy: 79.17%\n",
      "Batch 10, Loss: 0.918699, Accuracy: 79.53%\n",
      "Batch 11, Loss: 0.912108, Accuracy: 79.83%\n",
      "Batch 12, Loss: 0.902028, Accuracy: 80.08%\n",
      "Batch 13, Loss: 0.977113, Accuracy: 79.81%\n",
      "Batch 14, Loss: 1.012970, Accuracy: 79.24%\n",
      "Batch 15, Loss: 1.005393, Accuracy: 78.96%\n",
      "Batch 16, Loss: 0.950213, Accuracy: 78.91%\n",
      "Batch 17, Loss: 0.883624, Accuracy: 79.41%\n",
      "Batch 18, Loss: 0.958946, Accuracy: 79.17%\n",
      "Batch 19, Loss: 0.885254, Accuracy: 79.52%\n",
      "Batch 20, Loss: 0.951714, Accuracy: 79.53%\n",
      "Batch 21, Loss: 1.003214, Accuracy: 79.17%\n",
      "Batch 22, Loss: 0.935794, Accuracy: 79.26%\n",
      "Batch 23, Loss: 0.960359, Accuracy: 79.14%\n",
      "Batch 24, Loss: 0.980208, Accuracy: 79.04%\n",
      "Batch 25, Loss: 0.944660, Accuracy: 79.06%\n",
      "Batch 26, Loss: 0.900629, Accuracy: 79.27%\n",
      "Batch 27, Loss: 0.950198, Accuracy: 79.28%\n",
      "Batch 28, Loss: 0.950006, Accuracy: 79.30%\n",
      "Batch 29, Loss: 0.913124, Accuracy: 79.47%\n",
      "Batch 30, Loss: 0.908398, Accuracy: 79.64%\n",
      "Batch 31, Loss: 0.907191, Accuracy: 79.79%\n",
      "Batch 32, Loss: 0.875238, Accuracy: 80.03%\n",
      "Batch 33, Loss: 0.913832, Accuracy: 80.16%\n",
      "Batch 34, Loss: 0.888724, Accuracy: 80.28%\n",
      "Batch 35, Loss: 0.885250, Accuracy: 80.40%\n",
      "Batch 36, Loss: 0.942474, Accuracy: 80.47%\n",
      "Batch 37, Loss: 0.911665, Accuracy: 80.53%\n",
      "Batch 38, Loss: 0.982350, Accuracy: 80.43%\n",
      "Batch 39, Loss: 0.889089, Accuracy: 80.57%\n",
      "Batch 40, Loss: 0.873477, Accuracy: 80.70%\n",
      "Batch 41, Loss: 0.947845, Accuracy: 80.72%\n",
      "Batch 42, Loss: 0.955095, Accuracy: 80.65%\n",
      "Batch 43, Loss: 1.032947, Accuracy: 80.41%\n",
      "Batch 44, Loss: 1.070814, Accuracy: 80.04%\n",
      "Batch 45, Loss: 0.983213, Accuracy: 79.97%\n",
      "Batch 46, Loss: 0.938585, Accuracy: 79.96%\n",
      "Batch 47, Loss: 1.030405, Accuracy: 79.72%\n",
      "Batch 48, Loss: 0.925277, Accuracy: 79.79%\n",
      "Batch 49, Loss: 0.902777, Accuracy: 79.91%\n",
      "Batch 50, Loss: 0.965825, Accuracy: 79.84%\n",
      "Batch 51, Loss: 1.003534, Accuracy: 79.75%\n",
      "Batch 52, Loss: 0.926139, Accuracy: 79.81%\n",
      "Batch 53, Loss: 0.996288, Accuracy: 79.72%\n",
      "Batch 54, Loss: 0.945545, Accuracy: 79.72%\n",
      "Batch 55, Loss: 0.888307, Accuracy: 79.80%\n",
      "Batch 56, Loss: 0.926634, Accuracy: 79.83%\n",
      "Batch 57, Loss: 0.941479, Accuracy: 79.82%\n",
      "Batch 58, Loss: 0.955822, Accuracy: 79.82%\n",
      "Batch 59, Loss: 1.026977, Accuracy: 79.69%\n",
      "Batch 60, Loss: 0.955226, Accuracy: 79.66%\n",
      "Batch 61, Loss: 0.980814, Accuracy: 79.61%\n",
      "Batch 62, Loss: 1.010757, Accuracy: 79.51%\n",
      "Batch 63, Loss: 0.922505, Accuracy: 79.56%\n",
      "Batch 64, Loss: 0.865672, Accuracy: 79.71%\n",
      "Batch 65, Loss: 0.994654, Accuracy: 79.64%\n",
      "Batch 66, Loss: 0.973116, Accuracy: 79.62%\n",
      "Batch 67, Loss: 0.976912, Accuracy: 79.57%\n",
      "Batch 68, Loss: 0.924468, Accuracy: 79.62%\n",
      "Batch 69, Loss: 0.924989, Accuracy: 79.66%\n",
      "Batch 70, Loss: 0.979047, Accuracy: 79.62%\n",
      "Batch 71, Loss: 1.020200, Accuracy: 79.49%\n",
      "Batch 72, Loss: 0.928997, Accuracy: 79.51%\n",
      "Batch 73, Loss: 0.947560, Accuracy: 79.54%\n",
      "Batch 74, Loss: 1.025220, Accuracy: 79.46%\n",
      "Batch 75, Loss: 0.908287, Accuracy: 79.50%\n",
      "Batch 76, Loss: 0.915478, Accuracy: 79.54%\n",
      "Batch 77, Loss: 0.993395, Accuracy: 79.48%\n",
      "Batch 78, Loss: 0.863814, Accuracy: 79.61%\n",
      "Batch 79, Loss: 0.970712, Accuracy: 79.59%\n",
      "Batch 80, Loss: 0.890309, Accuracy: 79.65%\n",
      "Batch 81, Loss: 0.964078, Accuracy: 79.65%\n",
      "Batch 82, Loss: 0.949737, Accuracy: 79.67%\n",
      "Batch 83, Loss: 0.947502, Accuracy: 79.67%\n",
      "Batch 84, Loss: 0.898132, Accuracy: 79.72%\n",
      "Batch 85, Loss: 0.986976, Accuracy: 79.67%\n",
      "Batch 86, Loss: 1.015701, Accuracy: 79.58%\n",
      "Batch 87, Loss: 0.944215, Accuracy: 79.58%\n",
      "Batch 88, Loss: 0.954489, Accuracy: 79.56%\n",
      "Batch 89, Loss: 0.919435, Accuracy: 79.58%\n",
      "Batch 90, Loss: 0.862107, Accuracy: 79.69%\n",
      "Batch 91, Loss: 0.945305, Accuracy: 79.67%\n",
      "Batch 92, Loss: 0.949380, Accuracy: 79.65%\n",
      "Batch 93, Loss: 0.871710, Accuracy: 79.70%\n",
      "Batch 94, Loss: 0.963424, Accuracy: 79.69%\n",
      "Batch 95, Loss: 1.016560, Accuracy: 79.61%\n",
      "Batch 96, Loss: 0.943829, Accuracy: 79.61%\n",
      "Batch 97, Loss: 0.929317, Accuracy: 79.62%\n",
      "Batch 98, Loss: 0.948898, Accuracy: 79.59%\n",
      "Batch 99, Loss: 1.011590, Accuracy: 79.51%\n",
      "Batch 100, Loss: 0.952722, Accuracy: 79.52%\n",
      "Batch 101, Loss: 0.953173, Accuracy: 79.50%\n",
      "Batch 102, Loss: 0.960572, Accuracy: 79.49%\n",
      "Batch 103, Loss: 0.950949, Accuracy: 79.49%\n",
      "Batch 104, Loss: 0.956100, Accuracy: 79.48%\n",
      "Batch 105, Loss: 0.933251, Accuracy: 79.49%\n",
      "Batch 106, Loss: 0.926281, Accuracy: 79.53%\n",
      "Batch 107, Loss: 0.922013, Accuracy: 79.57%\n",
      "Batch 108, Loss: 1.079690, Accuracy: 79.46%\n",
      "Batch 109, Loss: 1.022990, Accuracy: 79.34%\n",
      "Batch 110, Loss: 0.968144, Accuracy: 79.32%\n",
      "Batch 111, Loss: 0.961228, Accuracy: 79.29%\n",
      "Batch 112, Loss: 1.001982, Accuracy: 79.24%\n",
      "Batch 113, Loss: 0.998848, Accuracy: 79.20%\n",
      "Batch 114, Loss: 0.956391, Accuracy: 79.19%\n",
      "Batch 115, Loss: 0.890691, Accuracy: 79.25%\n",
      "Batch 116, Loss: 0.951383, Accuracy: 79.23%\n",
      "Batch 117, Loss: 0.930219, Accuracy: 79.25%\n",
      "Batch 118, Loss: 0.964814, Accuracy: 79.25%\n",
      "Batch 119, Loss: 0.905245, Accuracy: 79.29%\n",
      "Batch 120, Loss: 0.990062, Accuracy: 79.24%\n",
      "Batch 121, Loss: 0.973151, Accuracy: 79.24%\n",
      "Batch 122, Loss: 0.916240, Accuracy: 79.26%\n",
      "Batch 123, Loss: 0.971242, Accuracy: 79.26%\n",
      "Batch 124, Loss: 1.030122, Accuracy: 79.18%\n",
      "Batch 125, Loss: 0.899111, Accuracy: 79.22%\n",
      "Batch 126, Loss: 0.952918, Accuracy: 79.20%\n",
      "Batch 127, Loss: 0.864538, Accuracy: 79.27%\n",
      "Batch 128, Loss: 0.982866, Accuracy: 79.24%\n",
      "Batch 129, Loss: 1.010316, Accuracy: 79.20%\n",
      "Batch 130, Loss: 0.897888, Accuracy: 79.24%\n",
      "Batch 131, Loss: 0.876414, Accuracy: 79.31%\n",
      "Batch 132, Loss: 0.925006, Accuracy: 79.33%\n",
      "Batch 133, Loss: 0.908973, Accuracy: 79.36%\n",
      "Batch 134, Loss: 0.995248, Accuracy: 79.33%\n",
      "Batch 135, Loss: 0.930094, Accuracy: 79.32%\n",
      "Batch 136, Loss: 0.935694, Accuracy: 79.34%\n",
      "Batch 137, Loss: 0.998295, Accuracy: 79.30%\n",
      "Batch 138, Loss: 0.996601, Accuracy: 79.25%\n",
      "Batch 139, Loss: 0.967440, Accuracy: 79.25%\n",
      "Batch 140, Loss: 0.981622, Accuracy: 79.23%\n",
      "Batch 141, Loss: 0.922099, Accuracy: 79.24%\n",
      "Batch 142, Loss: 0.838985, Accuracy: 79.34%\n",
      "Batch 143, Loss: 0.959196, Accuracy: 79.34%\n",
      "Batch 144, Loss: 0.948424, Accuracy: 79.34%\n",
      "Batch 145, Loss: 1.010162, Accuracy: 79.30%\n",
      "Batch 146, Loss: 0.849414, Accuracy: 79.38%\n",
      "Batch 147, Loss: 0.941518, Accuracy: 79.39%\n",
      "Batch 148, Loss: 0.902814, Accuracy: 79.43%\n",
      "Batch 149, Loss: 0.923957, Accuracy: 79.46%\n",
      "Batch 150, Loss: 0.946959, Accuracy: 79.45%\n",
      "Batch 151, Loss: 0.876759, Accuracy: 79.50%\n",
      "Batch 152, Loss: 0.878168, Accuracy: 79.54%\n",
      "Batch 153, Loss: 0.857806, Accuracy: 79.60%\n",
      "Batch 154, Loss: 1.027967, Accuracy: 79.52%\n",
      "Batch 155, Loss: 0.824288, Accuracy: 79.60%\n",
      "Batch 156, Loss: 0.917145, Accuracy: 79.62%\n",
      "Batch 157, Loss: 0.899064, Accuracy: 79.65%\n",
      "Batch 158, Loss: 0.900969, Accuracy: 79.68%\n",
      "Batch 159, Loss: 0.976790, Accuracy: 79.65%\n",
      "Batch 160, Loss: 0.925596, Accuracy: 79.66%\n",
      "Batch 161, Loss: 0.971708, Accuracy: 79.64%\n",
      "Batch 162, Loss: 0.972572, Accuracy: 79.62%\n",
      "Batch 163, Loss: 0.952800, Accuracy: 79.62%\n",
      "Batch 164, Loss: 0.972011, Accuracy: 79.61%\n",
      "Batch 165, Loss: 0.985121, Accuracy: 79.58%\n",
      "Batch 166, Loss: 0.991771, Accuracy: 79.56%\n",
      "Batch 167, Loss: 1.014077, Accuracy: 79.51%\n",
      "Batch 168, Loss: 0.996532, Accuracy: 79.49%\n",
      "Batch 169, Loss: 0.960308, Accuracy: 79.47%\n",
      "Batch 170, Loss: 0.945990, Accuracy: 79.48%\n",
      "Batch 171, Loss: 0.874530, Accuracy: 79.51%\n",
      "Batch 172, Loss: 0.935786, Accuracy: 79.51%\n",
      "Batch 173, Loss: 0.966635, Accuracy: 79.49%\n",
      "Batch 174, Loss: 0.953215, Accuracy: 79.48%\n",
      "Batch 175, Loss: 1.006850, Accuracy: 79.45%\n",
      "Batch 176, Loss: 0.913565, Accuracy: 79.47%\n",
      "Batch 177, Loss: 0.918397, Accuracy: 79.48%\n",
      "Batch 178, Loss: 0.904227, Accuracy: 79.50%\n",
      "Batch 179, Loss: 0.892177, Accuracy: 79.54%\n",
      "Batch 180, Loss: 1.022187, Accuracy: 79.50%\n",
      "Batch 181, Loss: 0.921772, Accuracy: 79.51%\n",
      "Batch 182, Loss: 0.886232, Accuracy: 79.56%\n",
      "Batch 183, Loss: 0.976763, Accuracy: 79.53%\n",
      "Batch 184, Loss: 0.907362, Accuracy: 79.55%\n",
      "Batch 185, Loss: 0.922919, Accuracy: 79.57%\n",
      "Batch 186, Loss: 0.931550, Accuracy: 79.60%\n",
      "Batch 187, Loss: 0.929180, Accuracy: 79.61%\n",
      "Batch 188, Loss: 0.979005, Accuracy: 79.60%\n",
      "Batch 189, Loss: 0.998380, Accuracy: 79.56%\n",
      "Batch 190, Loss: 0.993308, Accuracy: 79.54%\n",
      "Batch 191, Loss: 0.965651, Accuracy: 79.52%\n",
      "Batch 192, Loss: 0.901645, Accuracy: 79.55%\n",
      "Batch 193, Loss: 0.901236, Accuracy: 79.57%\n",
      "Batch 194, Loss: 0.919698, Accuracy: 79.60%\n",
      "Batch 195, Loss: 0.949373, Accuracy: 79.59%\n",
      "Batch 196, Loss: 0.946223, Accuracy: 79.60%\n",
      "Batch 197, Loss: 0.919280, Accuracy: 79.61%\n",
      "Batch 198, Loss: 0.965079, Accuracy: 79.59%\n",
      "Batch 199, Loss: 0.908118, Accuracy: 79.61%\n",
      "Batch 200, Loss: 0.951876, Accuracy: 79.60%\n",
      "Batch 201, Loss: 0.903812, Accuracy: 79.63%\n",
      "Batch 202, Loss: 0.920478, Accuracy: 79.65%\n",
      "Batch 203, Loss: 0.933361, Accuracy: 79.66%\n",
      "Batch 204, Loss: 0.885867, Accuracy: 79.68%\n",
      "Batch 205, Loss: 1.006705, Accuracy: 79.65%\n",
      "Batch 206, Loss: 0.917104, Accuracy: 79.66%\n",
      "Batch 207, Loss: 0.898166, Accuracy: 79.69%\n",
      "Batch 208, Loss: 0.876680, Accuracy: 79.72%\n",
      "Batch 209, Loss: 0.891047, Accuracy: 79.75%\n",
      "Batch 210, Loss: 0.951563, Accuracy: 79.75%\n",
      "Batch 211, Loss: 1.017409, Accuracy: 79.71%\n",
      "Batch 212, Loss: 0.959339, Accuracy: 79.70%\n",
      "Batch 213, Loss: 0.931849, Accuracy: 79.71%\n",
      "Training - Epoch 108, Loss: 0.945374, Accuracy: 79.71%\n",
      "Validation Batch 1, Loss: 0.868600, Accuracy: 89.06%\n",
      "Validation Batch 2, Loss: 0.908259, Accuracy: 86.72%\n",
      "Validation Batch 3, Loss: 0.980094, Accuracy: 83.33%\n",
      "Validation Batch 4, Loss: 0.916545, Accuracy: 83.20%\n",
      "Validation Batch 5, Loss: 0.869391, Accuracy: 84.06%\n",
      "Validation Batch 6, Loss: 0.856498, Accuracy: 84.90%\n",
      "Validation Batch 7, Loss: 0.937645, Accuracy: 83.93%\n",
      "Validation Batch 8, Loss: 0.970492, Accuracy: 83.01%\n",
      "Validation Batch 9, Loss: 0.975500, Accuracy: 82.29%\n",
      "Validation Batch 10, Loss: 0.958882, Accuracy: 81.88%\n",
      "Validation Batch 11, Loss: 0.905775, Accuracy: 81.82%\n",
      "Validation Batch 12, Loss: 0.871194, Accuracy: 82.16%\n",
      "Validation Batch 13, Loss: 0.961172, Accuracy: 81.61%\n",
      "Validation Batch 14, Loss: 0.938418, Accuracy: 81.47%\n",
      "Validation Batch 15, Loss: 0.931632, Accuracy: 81.46%\n",
      "Validation Batch 16, Loss: 0.892223, Accuracy: 81.74%\n",
      "Validation Batch 17, Loss: 0.963239, Accuracy: 81.53%\n",
      "Validation Batch 18, Loss: 0.903060, Accuracy: 81.68%\n",
      "Validation Batch 19, Loss: 0.946147, Accuracy: 81.58%\n",
      "Validation Batch 20, Loss: 0.848993, Accuracy: 82.03%\n",
      "Validation Batch 21, Loss: 0.935509, Accuracy: 81.85%\n",
      "Validation Batch 22, Loss: 0.937100, Accuracy: 81.75%\n",
      "Validation Batch 23, Loss: 0.960177, Accuracy: 81.59%\n",
      "Validation Batch 24, Loss: 0.983157, Accuracy: 81.32%\n",
      "Validation Batch 25, Loss: 0.913264, Accuracy: 81.31%\n",
      "Validation Batch 26, Loss: 0.918688, Accuracy: 81.37%\n",
      "Validation Batch 27, Loss: 0.871936, Accuracy: 81.44%\n",
      "Validation - Epoch 108, Loss: 0.923096, Accuracy: 81.44%\n",
      "Patienceâ€”16\n",
      "Epoch 109\n",
      "Batch 1, Loss: 0.946240, Accuracy: 81.25%\n",
      "Batch 2, Loss: 0.981529, Accuracy: 78.12%\n",
      "Batch 3, Loss: 0.924029, Accuracy: 79.17%\n",
      "Batch 4, Loss: 0.980554, Accuracy: 78.12%\n",
      "Batch 5, Loss: 0.918929, Accuracy: 79.06%\n",
      "Batch 6, Loss: 0.881463, Accuracy: 80.47%\n",
      "Batch 7, Loss: 0.978403, Accuracy: 79.69%\n",
      "Batch 8, Loss: 0.960625, Accuracy: 79.49%\n",
      "Batch 9, Loss: 1.012806, Accuracy: 78.82%\n",
      "Batch 10, Loss: 0.979448, Accuracy: 78.59%\n",
      "Batch 11, Loss: 0.982286, Accuracy: 78.27%\n",
      "Batch 12, Loss: 1.048692, Accuracy: 77.60%\n",
      "Batch 13, Loss: 0.941714, Accuracy: 77.76%\n",
      "Batch 14, Loss: 0.928045, Accuracy: 78.01%\n",
      "Batch 15, Loss: 0.901171, Accuracy: 78.44%\n",
      "Batch 16, Loss: 0.861591, Accuracy: 79.10%\n",
      "Batch 17, Loss: 0.904353, Accuracy: 79.41%\n",
      "Batch 18, Loss: 0.922249, Accuracy: 79.43%\n",
      "Batch 19, Loss: 0.971544, Accuracy: 79.28%\n",
      "Batch 20, Loss: 0.944784, Accuracy: 79.30%\n",
      "Batch 21, Loss: 0.933017, Accuracy: 79.39%\n",
      "Batch 22, Loss: 0.918716, Accuracy: 79.47%\n",
      "Batch 23, Loss: 0.983598, Accuracy: 79.28%\n",
      "Batch 24, Loss: 0.954433, Accuracy: 79.30%\n",
      "Batch 25, Loss: 1.082060, Accuracy: 78.75%\n",
      "Batch 26, Loss: 0.968507, Accuracy: 78.67%\n",
      "Batch 27, Loss: 0.924396, Accuracy: 78.88%\n",
      "Batch 28, Loss: 0.898790, Accuracy: 79.02%\n",
      "Batch 29, Loss: 0.972245, Accuracy: 78.93%\n",
      "Batch 30, Loss: 0.969021, Accuracy: 78.91%\n",
      "Batch 31, Loss: 0.992080, Accuracy: 78.78%\n",
      "Batch 32, Loss: 1.081700, Accuracy: 78.37%\n",
      "Batch 33, Loss: 1.027840, Accuracy: 78.12%\n",
      "Batch 34, Loss: 0.905153, Accuracy: 78.31%\n",
      "Batch 35, Loss: 0.942788, Accuracy: 78.35%\n",
      "Batch 36, Loss: 0.939932, Accuracy: 78.34%\n",
      "Batch 37, Loss: 0.879259, Accuracy: 78.59%\n",
      "Batch 38, Loss: 0.885344, Accuracy: 78.78%\n",
      "Batch 39, Loss: 0.982212, Accuracy: 78.73%\n",
      "Batch 40, Loss: 0.951505, Accuracy: 78.75%\n",
      "Batch 41, Loss: 0.966753, Accuracy: 78.70%\n",
      "Batch 42, Loss: 0.884083, Accuracy: 78.91%\n",
      "Batch 43, Loss: 0.910991, Accuracy: 79.00%\n",
      "Batch 44, Loss: 0.897618, Accuracy: 79.12%\n",
      "Batch 45, Loss: 0.881151, Accuracy: 79.27%\n",
      "Batch 46, Loss: 0.953718, Accuracy: 79.28%\n",
      "Batch 47, Loss: 0.983821, Accuracy: 79.22%\n",
      "Batch 48, Loss: 0.973545, Accuracy: 79.17%\n",
      "Batch 49, Loss: 1.017331, Accuracy: 79.05%\n",
      "Batch 50, Loss: 1.006381, Accuracy: 78.94%\n",
      "Batch 51, Loss: 0.982548, Accuracy: 78.86%\n",
      "Batch 52, Loss: 0.919508, Accuracy: 78.94%\n",
      "Batch 53, Loss: 1.002959, Accuracy: 78.86%\n",
      "Batch 54, Loss: 1.014101, Accuracy: 78.73%\n",
      "Batch 55, Loss: 0.970483, Accuracy: 78.69%\n",
      "Batch 56, Loss: 0.927920, Accuracy: 78.74%\n",
      "Batch 57, Loss: 0.961572, Accuracy: 78.70%\n",
      "Batch 58, Loss: 0.928883, Accuracy: 78.77%\n",
      "Batch 59, Loss: 0.928882, Accuracy: 78.84%\n",
      "Batch 60, Loss: 0.870891, Accuracy: 79.01%\n",
      "Batch 61, Loss: 0.903011, Accuracy: 79.07%\n",
      "Batch 62, Loss: 0.931318, Accuracy: 79.13%\n",
      "Batch 63, Loss: 0.992595, Accuracy: 79.04%\n",
      "Batch 64, Loss: 1.038959, Accuracy: 78.83%\n",
      "Batch 65, Loss: 0.973696, Accuracy: 78.80%\n",
      "Batch 66, Loss: 0.952023, Accuracy: 78.79%\n",
      "Batch 67, Loss: 1.022042, Accuracy: 78.66%\n",
      "Batch 68, Loss: 1.059245, Accuracy: 78.52%\n",
      "Batch 69, Loss: 0.943635, Accuracy: 78.51%\n",
      "Batch 70, Loss: 0.884599, Accuracy: 78.62%\n",
      "Batch 71, Loss: 0.964450, Accuracy: 78.61%\n",
      "Batch 72, Loss: 0.975668, Accuracy: 78.58%\n",
      "Batch 73, Loss: 1.027605, Accuracy: 78.47%\n",
      "Batch 74, Loss: 0.906008, Accuracy: 78.55%\n",
      "Batch 75, Loss: 0.909863, Accuracy: 78.60%\n",
      "Batch 76, Loss: 0.932589, Accuracy: 78.64%\n",
      "Batch 77, Loss: 0.936912, Accuracy: 78.67%\n",
      "Batch 78, Loss: 0.997712, Accuracy: 78.63%\n",
      "Batch 79, Loss: 0.927376, Accuracy: 78.66%\n",
      "Batch 80, Loss: 1.004858, Accuracy: 78.61%\n",
      "Batch 81, Loss: 0.965327, Accuracy: 78.59%\n",
      "Batch 82, Loss: 0.999627, Accuracy: 78.53%\n",
      "Batch 83, Loss: 0.870166, Accuracy: 78.61%\n",
      "Batch 84, Loss: 0.997747, Accuracy: 78.53%\n",
      "Batch 85, Loss: 0.905101, Accuracy: 78.58%\n",
      "Batch 86, Loss: 0.969843, Accuracy: 78.56%\n",
      "Batch 87, Loss: 0.936689, Accuracy: 78.59%\n",
      "Batch 88, Loss: 0.872936, Accuracy: 78.69%\n",
      "Batch 89, Loss: 1.008594, Accuracy: 78.63%\n",
      "Batch 90, Loss: 1.036866, Accuracy: 78.54%\n",
      "Batch 91, Loss: 0.921602, Accuracy: 78.57%\n",
      "Batch 92, Loss: 0.900583, Accuracy: 78.63%\n",
      "Batch 93, Loss: 0.961189, Accuracy: 78.63%\n",
      "Batch 94, Loss: 0.884002, Accuracy: 78.67%\n",
      "Batch 95, Loss: 0.899999, Accuracy: 78.75%\n",
      "Batch 96, Loss: 0.991782, Accuracy: 78.69%\n",
      "Batch 97, Loss: 1.008785, Accuracy: 78.62%\n",
      "Batch 98, Loss: 0.904393, Accuracy: 78.68%\n",
      "Batch 99, Loss: 0.928177, Accuracy: 78.72%\n",
      "Batch 100, Loss: 0.927036, Accuracy: 78.78%\n",
      "Batch 101, Loss: 0.895835, Accuracy: 78.84%\n",
      "Batch 102, Loss: 0.952974, Accuracy: 78.84%\n",
      "Batch 103, Loss: 0.985625, Accuracy: 78.81%\n",
      "Batch 104, Loss: 0.897417, Accuracy: 78.88%\n",
      "Batch 105, Loss: 1.009364, Accuracy: 78.84%\n",
      "Batch 106, Loss: 0.872878, Accuracy: 78.94%\n",
      "Batch 107, Loss: 0.910526, Accuracy: 78.97%\n",
      "Batch 108, Loss: 0.949278, Accuracy: 78.96%\n",
      "Batch 109, Loss: 0.937133, Accuracy: 78.97%\n",
      "Batch 110, Loss: 0.889750, Accuracy: 79.03%\n",
      "Batch 111, Loss: 0.855662, Accuracy: 79.12%\n",
      "Batch 112, Loss: 0.921541, Accuracy: 79.16%\n",
      "Batch 113, Loss: 1.076724, Accuracy: 79.05%\n",
      "Batch 114, Loss: 0.927512, Accuracy: 79.07%\n",
      "Batch 115, Loss: 0.957376, Accuracy: 79.08%\n",
      "Batch 116, Loss: 0.966948, Accuracy: 79.07%\n",
      "Batch 117, Loss: 0.938972, Accuracy: 79.07%\n",
      "Batch 118, Loss: 0.961544, Accuracy: 79.07%\n",
      "Batch 119, Loss: 0.962927, Accuracy: 79.07%\n",
      "Batch 120, Loss: 0.913558, Accuracy: 79.10%\n",
      "Batch 121, Loss: 0.908837, Accuracy: 79.15%\n",
      "Batch 122, Loss: 0.971455, Accuracy: 79.12%\n",
      "Batch 123, Loss: 0.905888, Accuracy: 79.15%\n",
      "Batch 124, Loss: 0.910771, Accuracy: 79.17%\n",
      "Batch 125, Loss: 0.967146, Accuracy: 79.16%\n",
      "Batch 126, Loss: 0.944581, Accuracy: 79.18%\n",
      "Batch 127, Loss: 0.902346, Accuracy: 79.22%\n",
      "Batch 128, Loss: 0.934032, Accuracy: 79.24%\n",
      "Batch 129, Loss: 0.922438, Accuracy: 79.26%\n",
      "Batch 130, Loss: 0.953499, Accuracy: 79.25%\n",
      "Batch 131, Loss: 0.937374, Accuracy: 79.27%\n",
      "Batch 132, Loss: 0.978364, Accuracy: 79.24%\n",
      "Batch 133, Loss: 0.996694, Accuracy: 79.21%\n",
      "Batch 134, Loss: 0.986542, Accuracy: 79.19%\n",
      "Batch 135, Loss: 0.903996, Accuracy: 79.24%\n",
      "Batch 136, Loss: 0.982757, Accuracy: 79.22%\n",
      "Batch 137, Loss: 0.860304, Accuracy: 79.29%\n",
      "Batch 138, Loss: 0.931325, Accuracy: 79.30%\n",
      "Batch 139, Loss: 0.893359, Accuracy: 79.34%\n",
      "Batch 140, Loss: 0.961669, Accuracy: 79.33%\n",
      "Batch 141, Loss: 0.915340, Accuracy: 79.36%\n",
      "Batch 142, Loss: 0.906189, Accuracy: 79.38%\n",
      "Batch 143, Loss: 0.978483, Accuracy: 79.35%\n",
      "Batch 144, Loss: 0.883157, Accuracy: 79.39%\n",
      "Batch 145, Loss: 0.919557, Accuracy: 79.42%\n",
      "Batch 146, Loss: 0.917361, Accuracy: 79.45%\n",
      "Batch 147, Loss: 0.908314, Accuracy: 79.49%\n",
      "Batch 148, Loss: 0.885734, Accuracy: 79.54%\n",
      "Batch 149, Loss: 0.950753, Accuracy: 79.54%\n",
      "Batch 150, Loss: 0.903901, Accuracy: 79.56%\n",
      "Batch 151, Loss: 0.914992, Accuracy: 79.58%\n",
      "Batch 152, Loss: 0.972504, Accuracy: 79.56%\n",
      "Batch 153, Loss: 0.846041, Accuracy: 79.64%\n",
      "Batch 154, Loss: 0.984823, Accuracy: 79.62%\n",
      "Batch 155, Loss: 0.922524, Accuracy: 79.64%\n",
      "Batch 156, Loss: 0.967898, Accuracy: 79.63%\n",
      "Batch 157, Loss: 0.939409, Accuracy: 79.63%\n",
      "Batch 158, Loss: 0.958843, Accuracy: 79.62%\n",
      "Batch 159, Loss: 0.898025, Accuracy: 79.65%\n",
      "Batch 160, Loss: 0.971768, Accuracy: 79.63%\n",
      "Batch 161, Loss: 0.952260, Accuracy: 79.62%\n",
      "Batch 162, Loss: 0.981320, Accuracy: 79.59%\n",
      "Batch 163, Loss: 0.923442, Accuracy: 79.61%\n",
      "Batch 164, Loss: 0.877828, Accuracy: 79.66%\n",
      "Batch 165, Loss: 0.916605, Accuracy: 79.69%\n",
      "Batch 166, Loss: 0.947158, Accuracy: 79.69%\n",
      "Batch 167, Loss: 0.982780, Accuracy: 79.65%\n",
      "Batch 168, Loss: 0.931184, Accuracy: 79.66%\n",
      "Batch 169, Loss: 0.837194, Accuracy: 79.72%\n",
      "Batch 170, Loss: 0.958545, Accuracy: 79.72%\n",
      "Batch 171, Loss: 0.924951, Accuracy: 79.73%\n",
      "Batch 172, Loss: 0.909298, Accuracy: 79.74%\n",
      "Batch 173, Loss: 0.927691, Accuracy: 79.75%\n",
      "Batch 174, Loss: 0.926627, Accuracy: 79.76%\n",
      "Batch 175, Loss: 0.925213, Accuracy: 79.78%\n",
      "Batch 176, Loss: 1.009650, Accuracy: 79.73%\n",
      "Batch 177, Loss: 0.883381, Accuracy: 79.77%\n",
      "Batch 178, Loss: 0.895957, Accuracy: 79.80%\n",
      "Batch 179, Loss: 1.013027, Accuracy: 79.76%\n",
      "Batch 180, Loss: 0.899378, Accuracy: 79.80%\n",
      "Batch 181, Loss: 0.864317, Accuracy: 79.85%\n",
      "Batch 182, Loss: 0.953975, Accuracy: 79.85%\n",
      "Batch 183, Loss: 0.953251, Accuracy: 79.85%\n",
      "Batch 184, Loss: 0.894791, Accuracy: 79.87%\n",
      "Batch 185, Loss: 0.974221, Accuracy: 79.86%\n",
      "Batch 186, Loss: 0.859099, Accuracy: 79.91%\n",
      "Batch 187, Loss: 0.911861, Accuracy: 79.91%\n",
      "Batch 188, Loss: 0.945413, Accuracy: 79.91%\n",
      "Batch 189, Loss: 0.873918, Accuracy: 79.94%\n",
      "Batch 190, Loss: 0.920059, Accuracy: 79.96%\n",
      "Batch 191, Loss: 0.902441, Accuracy: 79.98%\n",
      "Batch 192, Loss: 0.934334, Accuracy: 79.99%\n",
      "Batch 193, Loss: 0.947334, Accuracy: 80.00%\n",
      "Batch 194, Loss: 0.999449, Accuracy: 79.97%\n",
      "Batch 195, Loss: 0.907802, Accuracy: 79.98%\n",
      "Batch 196, Loss: 0.948567, Accuracy: 79.97%\n",
      "Batch 197, Loss: 0.933306, Accuracy: 79.98%\n",
      "Batch 198, Loss: 0.925510, Accuracy: 79.99%\n",
      "Batch 199, Loss: 0.950919, Accuracy: 79.99%\n",
      "Batch 200, Loss: 1.073380, Accuracy: 79.92%\n",
      "Batch 201, Loss: 0.943013, Accuracy: 79.92%\n",
      "Batch 202, Loss: 0.889477, Accuracy: 79.95%\n",
      "Batch 203, Loss: 0.935205, Accuracy: 79.96%\n",
      "Batch 204, Loss: 1.022601, Accuracy: 79.91%\n",
      "Batch 205, Loss: 0.986708, Accuracy: 79.89%\n",
      "Batch 206, Loss: 0.930339, Accuracy: 79.89%\n",
      "Batch 207, Loss: 0.889374, Accuracy: 79.92%\n",
      "Batch 208, Loss: 0.853107, Accuracy: 79.97%\n",
      "Batch 209, Loss: 0.856313, Accuracy: 80.01%\n",
      "Batch 210, Loss: 0.907867, Accuracy: 80.03%\n",
      "Batch 211, Loss: 0.948240, Accuracy: 80.04%\n",
      "Batch 212, Loss: 0.910650, Accuracy: 80.05%\n",
      "Batch 213, Loss: 0.928997, Accuracy: 80.06%\n",
      "Training - Epoch 109, Loss: 0.942266, Accuracy: 80.06%\n",
      "Validation Batch 1, Loss: 0.880761, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.899164, Accuracy: 86.72%\n",
      "Validation Batch 3, Loss: 0.997271, Accuracy: 82.29%\n",
      "Validation Batch 4, Loss: 0.931062, Accuracy: 82.03%\n",
      "Validation Batch 5, Loss: 0.905563, Accuracy: 82.19%\n",
      "Validation Batch 6, Loss: 0.841429, Accuracy: 83.33%\n",
      "Validation Batch 7, Loss: 0.938207, Accuracy: 82.81%\n",
      "Validation Batch 8, Loss: 0.943579, Accuracy: 82.42%\n",
      "Validation Batch 9, Loss: 0.959413, Accuracy: 81.94%\n",
      "Validation Batch 10, Loss: 0.954290, Accuracy: 81.56%\n",
      "Validation Batch 11, Loss: 0.900690, Accuracy: 81.82%\n",
      "Validation Batch 12, Loss: 0.867313, Accuracy: 82.42%\n",
      "Validation Batch 13, Loss: 0.945252, Accuracy: 82.21%\n",
      "Validation Batch 14, Loss: 0.942377, Accuracy: 82.14%\n",
      "Validation Batch 15, Loss: 0.934940, Accuracy: 82.08%\n",
      "Validation Batch 16, Loss: 0.897527, Accuracy: 82.32%\n",
      "Validation Batch 17, Loss: 0.945727, Accuracy: 82.17%\n",
      "Validation Batch 18, Loss: 0.908276, Accuracy: 82.29%\n",
      "Validation Batch 19, Loss: 0.957556, Accuracy: 82.07%\n",
      "Validation Batch 20, Loss: 0.892203, Accuracy: 82.11%\n",
      "Validation Batch 21, Loss: 0.947192, Accuracy: 81.99%\n",
      "Validation Batch 22, Loss: 0.933448, Accuracy: 81.89%\n",
      "Validation Batch 23, Loss: 0.997880, Accuracy: 81.45%\n",
      "Validation Batch 24, Loss: 0.992872, Accuracy: 81.12%\n",
      "Validation Batch 25, Loss: 0.916112, Accuracy: 81.25%\n",
      "Validation Batch 26, Loss: 0.935062, Accuracy: 81.19%\n",
      "Validation Batch 27, Loss: 0.861111, Accuracy: 81.33%\n",
      "Validation - Epoch 109, Loss: 0.926899, Accuracy: 81.33%\n",
      "Patienceâ€”17\n",
      "Epoch 110\n",
      "Batch 1, Loss: 0.990826, Accuracy: 73.44%\n",
      "Batch 2, Loss: 0.902220, Accuracy: 78.91%\n",
      "Batch 3, Loss: 0.980875, Accuracy: 77.60%\n",
      "Batch 4, Loss: 1.039652, Accuracy: 75.78%\n",
      "Batch 5, Loss: 1.008516, Accuracy: 75.00%\n",
      "Batch 6, Loss: 0.871659, Accuracy: 77.34%\n",
      "Batch 7, Loss: 0.964105, Accuracy: 77.46%\n",
      "Batch 8, Loss: 0.953120, Accuracy: 77.73%\n",
      "Batch 9, Loss: 0.966767, Accuracy: 77.78%\n",
      "Batch 10, Loss: 0.964444, Accuracy: 77.66%\n",
      "Batch 11, Loss: 0.948135, Accuracy: 77.56%\n",
      "Batch 12, Loss: 0.820961, Accuracy: 78.78%\n",
      "Batch 13, Loss: 1.030581, Accuracy: 78.12%\n",
      "Batch 14, Loss: 0.886272, Accuracy: 78.68%\n",
      "Batch 15, Loss: 0.919234, Accuracy: 79.06%\n",
      "Batch 16, Loss: 0.959558, Accuracy: 78.91%\n",
      "Batch 17, Loss: 0.944094, Accuracy: 79.14%\n",
      "Batch 18, Loss: 0.884837, Accuracy: 79.51%\n",
      "Batch 19, Loss: 0.922587, Accuracy: 79.61%\n",
      "Batch 20, Loss: 0.901848, Accuracy: 79.84%\n",
      "Batch 21, Loss: 0.987713, Accuracy: 79.54%\n",
      "Batch 22, Loss: 0.963467, Accuracy: 79.47%\n",
      "Batch 23, Loss: 0.937269, Accuracy: 79.48%\n",
      "Batch 24, Loss: 0.875842, Accuracy: 79.88%\n",
      "Batch 25, Loss: 0.871088, Accuracy: 80.19%\n",
      "Batch 26, Loss: 0.972258, Accuracy: 80.11%\n",
      "Batch 27, Loss: 0.967748, Accuracy: 80.03%\n",
      "Batch 28, Loss: 0.963995, Accuracy: 79.97%\n",
      "Batch 29, Loss: 0.935236, Accuracy: 80.01%\n",
      "Batch 30, Loss: 0.882589, Accuracy: 80.26%\n",
      "Batch 31, Loss: 0.959388, Accuracy: 80.19%\n",
      "Batch 32, Loss: 0.997252, Accuracy: 80.03%\n",
      "Batch 33, Loss: 1.034704, Accuracy: 79.73%\n",
      "Batch 34, Loss: 0.972284, Accuracy: 79.64%\n",
      "Batch 35, Loss: 0.952743, Accuracy: 79.60%\n",
      "Batch 36, Loss: 1.020413, Accuracy: 79.30%\n",
      "Batch 37, Loss: 0.965433, Accuracy: 79.27%\n",
      "Batch 38, Loss: 0.875729, Accuracy: 79.44%\n",
      "Batch 39, Loss: 0.928021, Accuracy: 79.49%\n",
      "Batch 40, Loss: 0.895442, Accuracy: 79.65%\n",
      "Batch 41, Loss: 1.051181, Accuracy: 79.34%\n",
      "Batch 42, Loss: 1.005475, Accuracy: 79.17%\n",
      "Batch 43, Loss: 0.896316, Accuracy: 79.29%\n",
      "Batch 44, Loss: 0.965573, Accuracy: 79.23%\n",
      "Batch 45, Loss: 0.992031, Accuracy: 79.13%\n",
      "Batch 46, Loss: 0.948059, Accuracy: 79.14%\n",
      "Batch 47, Loss: 0.906084, Accuracy: 79.22%\n",
      "Batch 48, Loss: 0.954567, Accuracy: 79.23%\n",
      "Batch 49, Loss: 0.931556, Accuracy: 79.27%\n",
      "Batch 50, Loss: 1.014180, Accuracy: 79.12%\n",
      "Batch 51, Loss: 0.877456, Accuracy: 79.29%\n",
      "Batch 52, Loss: 0.982869, Accuracy: 79.30%\n",
      "Batch 53, Loss: 0.979699, Accuracy: 79.25%\n",
      "Batch 54, Loss: 0.880161, Accuracy: 79.37%\n",
      "Batch 55, Loss: 0.944064, Accuracy: 79.40%\n",
      "Batch 56, Loss: 0.927707, Accuracy: 79.46%\n",
      "Batch 57, Loss: 0.944744, Accuracy: 79.50%\n",
      "Batch 58, Loss: 0.969269, Accuracy: 79.42%\n",
      "Batch 59, Loss: 0.958349, Accuracy: 79.40%\n",
      "Batch 60, Loss: 0.939475, Accuracy: 79.40%\n",
      "Batch 61, Loss: 0.997344, Accuracy: 79.33%\n",
      "Batch 62, Loss: 0.972478, Accuracy: 79.28%\n",
      "Batch 63, Loss: 0.926673, Accuracy: 79.29%\n",
      "Batch 64, Loss: 0.896070, Accuracy: 79.42%\n",
      "Batch 65, Loss: 0.943053, Accuracy: 79.42%\n",
      "Batch 66, Loss: 0.881829, Accuracy: 79.55%\n",
      "Batch 67, Loss: 0.861769, Accuracy: 79.69%\n",
      "Batch 68, Loss: 0.938907, Accuracy: 79.66%\n",
      "Batch 69, Loss: 1.016059, Accuracy: 79.55%\n",
      "Batch 70, Loss: 0.881993, Accuracy: 79.64%\n",
      "Batch 71, Loss: 0.901254, Accuracy: 79.71%\n",
      "Batch 72, Loss: 1.050935, Accuracy: 79.54%\n",
      "Batch 73, Loss: 0.841147, Accuracy: 79.69%\n",
      "Batch 74, Loss: 0.945414, Accuracy: 79.69%\n",
      "Batch 75, Loss: 1.016714, Accuracy: 79.58%\n",
      "Batch 76, Loss: 0.942119, Accuracy: 79.61%\n",
      "Batch 77, Loss: 0.906453, Accuracy: 79.67%\n",
      "Batch 78, Loss: 0.906229, Accuracy: 79.75%\n",
      "Batch 79, Loss: 0.957160, Accuracy: 79.73%\n",
      "Batch 80, Loss: 0.958554, Accuracy: 79.73%\n",
      "Batch 81, Loss: 0.936715, Accuracy: 79.75%\n",
      "Batch 82, Loss: 0.944678, Accuracy: 79.74%\n",
      "Batch 83, Loss: 0.900642, Accuracy: 79.80%\n",
      "Batch 84, Loss: 0.949118, Accuracy: 79.78%\n",
      "Batch 85, Loss: 0.937840, Accuracy: 79.78%\n",
      "Batch 86, Loss: 1.021599, Accuracy: 79.69%\n",
      "Batch 87, Loss: 0.898904, Accuracy: 79.74%\n",
      "Batch 88, Loss: 0.983422, Accuracy: 79.69%\n",
      "Batch 89, Loss: 0.900724, Accuracy: 79.72%\n",
      "Batch 90, Loss: 0.939451, Accuracy: 79.74%\n",
      "Batch 91, Loss: 0.988357, Accuracy: 79.70%\n",
      "Batch 92, Loss: 1.017310, Accuracy: 79.62%\n",
      "Batch 93, Loss: 0.955148, Accuracy: 79.60%\n",
      "Batch 94, Loss: 0.984748, Accuracy: 79.57%\n",
      "Batch 95, Loss: 0.932540, Accuracy: 79.57%\n",
      "Batch 96, Loss: 0.908425, Accuracy: 79.62%\n",
      "Batch 97, Loss: 0.953920, Accuracy: 79.61%\n",
      "Batch 98, Loss: 0.926896, Accuracy: 79.64%\n",
      "Batch 99, Loss: 0.937697, Accuracy: 79.67%\n",
      "Batch 100, Loss: 1.019826, Accuracy: 79.59%\n",
      "Batch 101, Loss: 0.952264, Accuracy: 79.59%\n",
      "Batch 102, Loss: 1.071853, Accuracy: 79.46%\n",
      "Batch 103, Loss: 0.978141, Accuracy: 79.43%\n",
      "Batch 104, Loss: 0.950745, Accuracy: 79.45%\n",
      "Batch 105, Loss: 0.903247, Accuracy: 79.48%\n",
      "Batch 106, Loss: 0.933837, Accuracy: 79.50%\n",
      "Batch 107, Loss: 1.010109, Accuracy: 79.42%\n",
      "Batch 108, Loss: 0.931436, Accuracy: 79.44%\n",
      "Batch 109, Loss: 0.976915, Accuracy: 79.42%\n",
      "Batch 110, Loss: 0.921283, Accuracy: 79.45%\n",
      "Batch 111, Loss: 0.910432, Accuracy: 79.49%\n",
      "Batch 112, Loss: 0.898966, Accuracy: 79.53%\n",
      "Batch 113, Loss: 0.975433, Accuracy: 79.51%\n",
      "Batch 114, Loss: 1.006629, Accuracy: 79.47%\n",
      "Batch 115, Loss: 0.932143, Accuracy: 79.50%\n",
      "Batch 116, Loss: 0.993006, Accuracy: 79.46%\n",
      "Batch 117, Loss: 0.901536, Accuracy: 79.50%\n",
      "Batch 118, Loss: 0.969120, Accuracy: 79.48%\n",
      "Batch 119, Loss: 1.027537, Accuracy: 79.39%\n",
      "Batch 120, Loss: 0.903903, Accuracy: 79.44%\n",
      "Batch 121, Loss: 0.917127, Accuracy: 79.47%\n",
      "Batch 122, Loss: 0.954501, Accuracy: 79.47%\n",
      "Batch 123, Loss: 0.924900, Accuracy: 79.50%\n",
      "Batch 124, Loss: 0.924136, Accuracy: 79.51%\n",
      "Batch 125, Loss: 0.919216, Accuracy: 79.53%\n",
      "Batch 126, Loss: 0.917662, Accuracy: 79.56%\n",
      "Batch 127, Loss: 1.006362, Accuracy: 79.52%\n",
      "Batch 128, Loss: 0.945542, Accuracy: 79.52%\n",
      "Batch 129, Loss: 0.933918, Accuracy: 79.53%\n",
      "Batch 130, Loss: 0.920986, Accuracy: 79.54%\n",
      "Batch 131, Loss: 0.921395, Accuracy: 79.57%\n",
      "Batch 132, Loss: 0.882713, Accuracy: 79.62%\n",
      "Batch 133, Loss: 0.899633, Accuracy: 79.65%\n",
      "Batch 134, Loss: 0.976878, Accuracy: 79.63%\n",
      "Batch 135, Loss: 0.940787, Accuracy: 79.64%\n",
      "Batch 136, Loss: 0.940127, Accuracy: 79.64%\n",
      "Batch 137, Loss: 0.955310, Accuracy: 79.64%\n",
      "Batch 138, Loss: 0.905417, Accuracy: 79.66%\n",
      "Batch 139, Loss: 0.996249, Accuracy: 79.63%\n",
      "Batch 140, Loss: 0.986479, Accuracy: 79.61%\n",
      "Batch 141, Loss: 0.974746, Accuracy: 79.59%\n",
      "Batch 142, Loss: 0.953365, Accuracy: 79.59%\n",
      "Batch 143, Loss: 0.941669, Accuracy: 79.59%\n",
      "Batch 144, Loss: 0.944256, Accuracy: 79.61%\n",
      "Batch 145, Loss: 0.923743, Accuracy: 79.62%\n",
      "Batch 146, Loss: 0.945651, Accuracy: 79.61%\n",
      "Batch 147, Loss: 0.920188, Accuracy: 79.64%\n",
      "Batch 148, Loss: 0.855833, Accuracy: 79.72%\n",
      "Batch 149, Loss: 0.941727, Accuracy: 79.72%\n",
      "Batch 150, Loss: 0.989240, Accuracy: 79.69%\n",
      "Batch 151, Loss: 0.897132, Accuracy: 79.72%\n",
      "Batch 152, Loss: 0.895609, Accuracy: 79.74%\n",
      "Batch 153, Loss: 0.923675, Accuracy: 79.74%\n",
      "Batch 154, Loss: 1.120920, Accuracy: 79.61%\n",
      "Batch 155, Loss: 0.949461, Accuracy: 79.61%\n",
      "Batch 156, Loss: 0.933387, Accuracy: 79.62%\n",
      "Batch 157, Loss: 0.917774, Accuracy: 79.64%\n",
      "Batch 158, Loss: 0.866279, Accuracy: 79.69%\n",
      "Batch 159, Loss: 0.986801, Accuracy: 79.67%\n",
      "Batch 160, Loss: 0.981070, Accuracy: 79.65%\n",
      "Batch 161, Loss: 0.885060, Accuracy: 79.69%\n",
      "Batch 162, Loss: 0.935041, Accuracy: 79.69%\n",
      "Batch 163, Loss: 1.026934, Accuracy: 79.64%\n",
      "Batch 164, Loss: 0.931490, Accuracy: 79.64%\n",
      "Batch 165, Loss: 0.886644, Accuracy: 79.67%\n",
      "Batch 166, Loss: 0.936141, Accuracy: 79.66%\n",
      "Batch 167, Loss: 0.939375, Accuracy: 79.67%\n",
      "Batch 168, Loss: 0.957046, Accuracy: 79.67%\n",
      "Batch 169, Loss: 0.896074, Accuracy: 79.70%\n",
      "Batch 170, Loss: 0.967617, Accuracy: 79.67%\n",
      "Batch 171, Loss: 0.940807, Accuracy: 79.67%\n",
      "Batch 172, Loss: 0.919740, Accuracy: 79.68%\n",
      "Batch 173, Loss: 0.952376, Accuracy: 79.67%\n",
      "Batch 174, Loss: 0.899602, Accuracy: 79.71%\n",
      "Batch 175, Loss: 0.975856, Accuracy: 79.70%\n",
      "Batch 176, Loss: 0.909657, Accuracy: 79.73%\n",
      "Batch 177, Loss: 0.865777, Accuracy: 79.78%\n",
      "Batch 178, Loss: 1.000790, Accuracy: 79.73%\n",
      "Batch 179, Loss: 0.915764, Accuracy: 79.77%\n",
      "Batch 180, Loss: 0.924898, Accuracy: 79.77%\n",
      "Batch 181, Loss: 0.942297, Accuracy: 79.77%\n",
      "Batch 182, Loss: 0.931116, Accuracy: 79.78%\n",
      "Batch 183, Loss: 0.921174, Accuracy: 79.80%\n",
      "Batch 184, Loss: 0.957932, Accuracy: 79.78%\n",
      "Batch 185, Loss: 0.912537, Accuracy: 79.81%\n",
      "Batch 186, Loss: 0.976785, Accuracy: 79.78%\n",
      "Batch 187, Loss: 0.870838, Accuracy: 79.82%\n",
      "Batch 188, Loss: 0.978945, Accuracy: 79.80%\n",
      "Batch 189, Loss: 0.944738, Accuracy: 79.81%\n",
      "Batch 190, Loss: 0.953907, Accuracy: 79.81%\n",
      "Batch 191, Loss: 0.915607, Accuracy: 79.83%\n",
      "Batch 192, Loss: 0.960190, Accuracy: 79.82%\n",
      "Batch 193, Loss: 1.002162, Accuracy: 79.78%\n",
      "Batch 194, Loss: 0.929942, Accuracy: 79.79%\n",
      "Batch 195, Loss: 0.980503, Accuracy: 79.77%\n",
      "Batch 196, Loss: 0.926250, Accuracy: 79.78%\n",
      "Batch 197, Loss: 0.856649, Accuracy: 79.83%\n",
      "Batch 198, Loss: 0.905871, Accuracy: 79.84%\n",
      "Batch 199, Loss: 0.896724, Accuracy: 79.86%\n",
      "Batch 200, Loss: 0.986381, Accuracy: 79.84%\n",
      "Batch 201, Loss: 0.943605, Accuracy: 79.84%\n",
      "Batch 202, Loss: 0.955052, Accuracy: 79.83%\n",
      "Batch 203, Loss: 0.922079, Accuracy: 79.84%\n",
      "Batch 204, Loss: 0.931122, Accuracy: 79.85%\n",
      "Batch 205, Loss: 0.922843, Accuracy: 79.87%\n",
      "Batch 206, Loss: 0.950543, Accuracy: 79.86%\n",
      "Batch 207, Loss: 0.929932, Accuracy: 79.87%\n",
      "Batch 208, Loss: 0.950513, Accuracy: 79.87%\n",
      "Batch 209, Loss: 0.951022, Accuracy: 79.87%\n",
      "Batch 210, Loss: 0.936181, Accuracy: 79.87%\n",
      "Batch 211, Loss: 1.023543, Accuracy: 79.84%\n",
      "Batch 212, Loss: 0.937063, Accuracy: 79.84%\n",
      "Batch 213, Loss: 0.951561, Accuracy: 79.84%\n",
      "Training - Epoch 110, Loss: 0.944463, Accuracy: 79.84%\n",
      "Validation Batch 1, Loss: 0.880243, Accuracy: 85.94%\n",
      "Validation Batch 2, Loss: 0.925085, Accuracy: 83.59%\n",
      "Validation Batch 3, Loss: 0.997640, Accuracy: 80.21%\n",
      "Validation Batch 4, Loss: 0.915679, Accuracy: 80.47%\n",
      "Validation Batch 5, Loss: 0.885485, Accuracy: 81.56%\n",
      "Validation Batch 6, Loss: 0.870049, Accuracy: 82.55%\n",
      "Validation Batch 7, Loss: 0.954854, Accuracy: 81.92%\n",
      "Validation Batch 8, Loss: 0.979566, Accuracy: 81.25%\n",
      "Validation Batch 9, Loss: 0.987470, Accuracy: 80.73%\n",
      "Validation Batch 10, Loss: 0.965429, Accuracy: 80.31%\n",
      "Validation Batch 11, Loss: 0.907724, Accuracy: 80.54%\n",
      "Validation Batch 12, Loss: 0.874802, Accuracy: 80.99%\n",
      "Validation Batch 13, Loss: 0.969756, Accuracy: 80.77%\n",
      "Validation Batch 14, Loss: 0.950189, Accuracy: 80.69%\n",
      "Validation Batch 15, Loss: 0.931556, Accuracy: 80.73%\n",
      "Validation Batch 16, Loss: 0.894814, Accuracy: 81.05%\n",
      "Validation Batch 17, Loss: 0.973140, Accuracy: 80.79%\n",
      "Validation Batch 18, Loss: 0.906231, Accuracy: 80.99%\n",
      "Validation Batch 19, Loss: 0.960149, Accuracy: 80.92%\n",
      "Validation Batch 20, Loss: 0.862147, Accuracy: 81.17%\n",
      "Validation Batch 21, Loss: 0.941815, Accuracy: 81.10%\n",
      "Validation Batch 22, Loss: 0.953427, Accuracy: 81.04%\n",
      "Validation Batch 23, Loss: 0.978252, Accuracy: 80.84%\n",
      "Validation Batch 24, Loss: 0.995790, Accuracy: 80.53%\n",
      "Validation Batch 25, Loss: 0.916717, Accuracy: 80.75%\n",
      "Validation Batch 26, Loss: 0.923840, Accuracy: 80.83%\n",
      "Validation Batch 27, Loss: 0.876673, Accuracy: 80.97%\n",
      "Validation - Epoch 110, Loss: 0.932538, Accuracy: 80.97%\n",
      "Patienceâ€”18\n",
      "Epoch 111\n",
      "Batch 1, Loss: 0.863359, Accuracy: 89.06%\n",
      "Batch 2, Loss: 0.948221, Accuracy: 83.59%\n",
      "Batch 3, Loss: 0.854461, Accuracy: 85.94%\n",
      "Batch 4, Loss: 0.911686, Accuracy: 85.16%\n",
      "Batch 5, Loss: 0.973208, Accuracy: 83.75%\n",
      "Batch 6, Loss: 0.926203, Accuracy: 83.33%\n",
      "Batch 7, Loss: 1.020980, Accuracy: 81.47%\n",
      "Batch 8, Loss: 0.966612, Accuracy: 80.86%\n",
      "Batch 9, Loss: 0.948840, Accuracy: 80.73%\n",
      "Batch 10, Loss: 0.896682, Accuracy: 80.94%\n",
      "Batch 11, Loss: 0.881720, Accuracy: 81.53%\n",
      "Batch 12, Loss: 0.977277, Accuracy: 81.12%\n",
      "Batch 13, Loss: 0.906696, Accuracy: 81.37%\n",
      "Batch 14, Loss: 0.918109, Accuracy: 81.47%\n",
      "Batch 15, Loss: 0.927490, Accuracy: 81.56%\n",
      "Batch 16, Loss: 0.966718, Accuracy: 81.35%\n",
      "Batch 17, Loss: 0.897796, Accuracy: 81.62%\n",
      "Batch 18, Loss: 0.897353, Accuracy: 81.86%\n",
      "Batch 19, Loss: 1.001634, Accuracy: 81.41%\n",
      "Batch 20, Loss: 0.951498, Accuracy: 81.41%\n",
      "Batch 21, Loss: 0.990636, Accuracy: 81.03%\n",
      "Batch 22, Loss: 0.976270, Accuracy: 80.82%\n",
      "Batch 23, Loss: 0.940788, Accuracy: 80.84%\n",
      "Batch 24, Loss: 0.954197, Accuracy: 80.79%\n",
      "Batch 25, Loss: 0.967713, Accuracy: 80.62%\n",
      "Batch 26, Loss: 0.968837, Accuracy: 80.53%\n",
      "Batch 27, Loss: 0.883810, Accuracy: 80.73%\n",
      "Batch 28, Loss: 0.939216, Accuracy: 80.69%\n",
      "Batch 29, Loss: 0.906609, Accuracy: 80.82%\n",
      "Batch 30, Loss: 0.937531, Accuracy: 80.78%\n",
      "Batch 31, Loss: 0.963094, Accuracy: 80.65%\n",
      "Batch 32, Loss: 0.985248, Accuracy: 80.52%\n",
      "Batch 33, Loss: 0.905171, Accuracy: 80.63%\n",
      "Batch 34, Loss: 1.003156, Accuracy: 80.38%\n",
      "Batch 35, Loss: 0.969657, Accuracy: 80.27%\n",
      "Batch 36, Loss: 0.960346, Accuracy: 80.25%\n",
      "Batch 37, Loss: 0.947129, Accuracy: 80.24%\n",
      "Batch 38, Loss: 0.966348, Accuracy: 80.18%\n",
      "Batch 39, Loss: 0.904750, Accuracy: 80.29%\n",
      "Batch 40, Loss: 0.937543, Accuracy: 80.27%\n",
      "Batch 41, Loss: 0.867329, Accuracy: 80.49%\n",
      "Batch 42, Loss: 0.940314, Accuracy: 80.51%\n",
      "Batch 43, Loss: 0.912814, Accuracy: 80.52%\n",
      "Batch 44, Loss: 0.886609, Accuracy: 80.65%\n",
      "Batch 45, Loss: 0.929156, Accuracy: 80.69%\n",
      "Batch 46, Loss: 0.936190, Accuracy: 80.71%\n",
      "Batch 47, Loss: 0.903945, Accuracy: 80.78%\n",
      "Batch 48, Loss: 0.910381, Accuracy: 80.86%\n",
      "Batch 49, Loss: 1.002887, Accuracy: 80.71%\n",
      "Batch 50, Loss: 0.927162, Accuracy: 80.75%\n",
      "Batch 51, Loss: 0.972745, Accuracy: 80.67%\n",
      "Batch 52, Loss: 0.924943, Accuracy: 80.65%\n",
      "Batch 53, Loss: 0.854393, Accuracy: 80.81%\n",
      "Batch 54, Loss: 0.953424, Accuracy: 80.76%\n",
      "Batch 55, Loss: 1.001061, Accuracy: 80.65%\n",
      "Batch 56, Loss: 0.930389, Accuracy: 80.69%\n",
      "Batch 57, Loss: 1.032578, Accuracy: 80.56%\n",
      "Batch 58, Loss: 0.884442, Accuracy: 80.63%\n",
      "Batch 59, Loss: 0.917904, Accuracy: 80.67%\n",
      "Batch 60, Loss: 0.938416, Accuracy: 80.65%\n",
      "Batch 61, Loss: 0.868628, Accuracy: 80.79%\n",
      "Batch 62, Loss: 1.010728, Accuracy: 80.67%\n",
      "Batch 63, Loss: 0.989299, Accuracy: 80.58%\n",
      "Batch 64, Loss: 0.888733, Accuracy: 80.66%\n",
      "Batch 65, Loss: 0.920788, Accuracy: 80.67%\n",
      "Batch 66, Loss: 0.933110, Accuracy: 80.66%\n",
      "Batch 67, Loss: 0.897353, Accuracy: 80.67%\n",
      "Batch 68, Loss: 0.898607, Accuracy: 80.74%\n",
      "Batch 69, Loss: 0.862148, Accuracy: 80.87%\n",
      "Batch 70, Loss: 0.937706, Accuracy: 80.87%\n",
      "Batch 71, Loss: 0.886628, Accuracy: 80.94%\n",
      "Batch 72, Loss: 0.900628, Accuracy: 80.99%\n",
      "Batch 73, Loss: 1.033489, Accuracy: 80.86%\n",
      "Batch 74, Loss: 0.924698, Accuracy: 80.87%\n",
      "Batch 75, Loss: 0.985901, Accuracy: 80.79%\n",
      "Batch 76, Loss: 0.969633, Accuracy: 80.76%\n",
      "Batch 77, Loss: 0.969412, Accuracy: 80.72%\n",
      "Batch 78, Loss: 0.932610, Accuracy: 80.71%\n",
      "Batch 79, Loss: 1.071063, Accuracy: 80.50%\n",
      "Batch 80, Loss: 1.072906, Accuracy: 80.31%\n",
      "Batch 81, Loss: 0.900263, Accuracy: 80.36%\n",
      "Batch 82, Loss: 0.939348, Accuracy: 80.37%\n",
      "Batch 83, Loss: 0.998848, Accuracy: 80.27%\n",
      "Batch 84, Loss: 0.978869, Accuracy: 80.23%\n",
      "Batch 85, Loss: 0.899459, Accuracy: 80.26%\n",
      "Batch 86, Loss: 1.031484, Accuracy: 80.14%\n",
      "Batch 87, Loss: 0.943342, Accuracy: 80.14%\n",
      "Batch 88, Loss: 0.911977, Accuracy: 80.15%\n",
      "Batch 89, Loss: 0.936681, Accuracy: 80.16%\n",
      "Batch 90, Loss: 0.898167, Accuracy: 80.21%\n",
      "Batch 91, Loss: 0.887781, Accuracy: 80.25%\n",
      "Batch 92, Loss: 0.844952, Accuracy: 80.37%\n",
      "Batch 93, Loss: 0.876551, Accuracy: 80.44%\n",
      "Batch 94, Loss: 0.917221, Accuracy: 80.47%\n",
      "Batch 95, Loss: 0.933999, Accuracy: 80.46%\n",
      "Batch 96, Loss: 0.938451, Accuracy: 80.47%\n",
      "Batch 97, Loss: 0.910519, Accuracy: 80.49%\n",
      "Batch 98, Loss: 0.928654, Accuracy: 80.50%\n",
      "Batch 99, Loss: 0.966585, Accuracy: 80.46%\n",
      "Batch 100, Loss: 0.948139, Accuracy: 80.45%\n",
      "Batch 101, Loss: 0.900905, Accuracy: 80.49%\n",
      "Batch 102, Loss: 0.946860, Accuracy: 80.48%\n",
      "Batch 103, Loss: 0.910241, Accuracy: 80.52%\n",
      "Batch 104, Loss: 0.941525, Accuracy: 80.51%\n",
      "Batch 105, Loss: 0.870302, Accuracy: 80.58%\n",
      "Batch 106, Loss: 0.871018, Accuracy: 80.66%\n",
      "Batch 107, Loss: 0.989535, Accuracy: 80.61%\n",
      "Batch 108, Loss: 0.978171, Accuracy: 80.58%\n",
      "Batch 109, Loss: 1.008352, Accuracy: 80.52%\n",
      "Batch 110, Loss: 0.922116, Accuracy: 80.54%\n",
      "Batch 111, Loss: 0.940094, Accuracy: 80.53%\n",
      "Batch 112, Loss: 0.995829, Accuracy: 80.45%\n",
      "Batch 113, Loss: 0.911891, Accuracy: 80.49%\n",
      "Batch 114, Loss: 0.997191, Accuracy: 80.43%\n",
      "Batch 115, Loss: 0.982989, Accuracy: 80.38%\n",
      "Batch 116, Loss: 0.908065, Accuracy: 80.41%\n",
      "Batch 117, Loss: 0.986451, Accuracy: 80.37%\n",
      "Batch 118, Loss: 0.947297, Accuracy: 80.36%\n",
      "Batch 119, Loss: 0.963471, Accuracy: 80.33%\n",
      "Batch 120, Loss: 0.921262, Accuracy: 80.35%\n",
      "Batch 121, Loss: 0.912482, Accuracy: 80.37%\n",
      "Batch 122, Loss: 0.925779, Accuracy: 80.39%\n",
      "Batch 123, Loss: 0.993516, Accuracy: 80.35%\n",
      "Batch 124, Loss: 0.940986, Accuracy: 80.33%\n",
      "Batch 125, Loss: 0.957414, Accuracy: 80.31%\n",
      "Batch 126, Loss: 0.955212, Accuracy: 80.31%\n",
      "Batch 127, Loss: 0.931788, Accuracy: 80.33%\n",
      "Batch 128, Loss: 1.043432, Accuracy: 80.24%\n",
      "Batch 129, Loss: 1.019847, Accuracy: 80.20%\n",
      "Batch 130, Loss: 1.048537, Accuracy: 80.11%\n",
      "Batch 131, Loss: 0.979875, Accuracy: 80.08%\n",
      "Batch 132, Loss: 0.909862, Accuracy: 80.11%\n",
      "Batch 133, Loss: 0.941493, Accuracy: 80.10%\n",
      "Batch 134, Loss: 0.938191, Accuracy: 80.11%\n",
      "Batch 135, Loss: 1.006131, Accuracy: 80.06%\n",
      "Batch 136, Loss: 0.893725, Accuracy: 80.10%\n",
      "Batch 137, Loss: 0.975625, Accuracy: 80.08%\n",
      "Batch 138, Loss: 0.955739, Accuracy: 80.07%\n",
      "Batch 139, Loss: 0.970148, Accuracy: 80.05%\n",
      "Batch 140, Loss: 0.896783, Accuracy: 80.09%\n",
      "Batch 141, Loss: 0.921804, Accuracy: 80.11%\n",
      "Batch 142, Loss: 0.871578, Accuracy: 80.14%\n",
      "Batch 143, Loss: 0.921562, Accuracy: 80.16%\n",
      "Batch 144, Loss: 1.007585, Accuracy: 80.10%\n",
      "Batch 145, Loss: 0.963800, Accuracy: 80.10%\n",
      "Batch 146, Loss: 0.968863, Accuracy: 80.07%\n",
      "Batch 147, Loss: 0.919053, Accuracy: 80.09%\n",
      "Batch 148, Loss: 0.973551, Accuracy: 80.08%\n",
      "Batch 149, Loss: 1.006089, Accuracy: 80.02%\n",
      "Batch 150, Loss: 0.958882, Accuracy: 80.01%\n",
      "Batch 151, Loss: 1.032251, Accuracy: 79.94%\n",
      "Batch 152, Loss: 0.866867, Accuracy: 79.99%\n",
      "Batch 153, Loss: 0.922492, Accuracy: 79.99%\n",
      "Batch 154, Loss: 0.920622, Accuracy: 80.01%\n",
      "Batch 155, Loss: 1.008130, Accuracy: 79.96%\n",
      "Batch 156, Loss: 0.924198, Accuracy: 79.98%\n",
      "Batch 157, Loss: 1.005324, Accuracy: 79.95%\n",
      "Batch 158, Loss: 0.992577, Accuracy: 79.91%\n",
      "Batch 159, Loss: 0.923381, Accuracy: 79.92%\n",
      "Batch 160, Loss: 0.948055, Accuracy: 79.90%\n",
      "Batch 161, Loss: 0.875641, Accuracy: 79.95%\n",
      "Batch 162, Loss: 1.054454, Accuracy: 79.88%\n",
      "Batch 163, Loss: 0.955652, Accuracy: 79.87%\n",
      "Batch 164, Loss: 0.875006, Accuracy: 79.92%\n",
      "Batch 165, Loss: 0.996972, Accuracy: 79.89%\n",
      "Batch 166, Loss: 0.970747, Accuracy: 79.88%\n",
      "Batch 167, Loss: 0.878587, Accuracy: 79.91%\n",
      "Batch 168, Loss: 0.919700, Accuracy: 79.93%\n",
      "Batch 169, Loss: 0.974426, Accuracy: 79.91%\n",
      "Batch 170, Loss: 0.895072, Accuracy: 79.94%\n",
      "Batch 171, Loss: 0.891927, Accuracy: 79.97%\n",
      "Batch 172, Loss: 0.951004, Accuracy: 79.96%\n",
      "Batch 173, Loss: 0.969763, Accuracy: 79.93%\n",
      "Batch 174, Loss: 0.941957, Accuracy: 79.93%\n",
      "Batch 175, Loss: 0.947164, Accuracy: 79.93%\n",
      "Batch 176, Loss: 1.003241, Accuracy: 79.89%\n",
      "Batch 177, Loss: 0.947000, Accuracy: 79.89%\n",
      "Batch 178, Loss: 0.849925, Accuracy: 79.94%\n",
      "Batch 179, Loss: 0.922091, Accuracy: 79.96%\n",
      "Batch 180, Loss: 0.968483, Accuracy: 79.95%\n",
      "Batch 181, Loss: 0.885754, Accuracy: 79.98%\n",
      "Batch 182, Loss: 0.949055, Accuracy: 79.99%\n",
      "Batch 183, Loss: 0.921095, Accuracy: 80.00%\n",
      "Batch 184, Loss: 0.928409, Accuracy: 80.02%\n",
      "Batch 185, Loss: 1.018772, Accuracy: 79.97%\n",
      "Batch 186, Loss: 0.888277, Accuracy: 80.01%\n",
      "Batch 187, Loss: 0.940145, Accuracy: 80.01%\n",
      "Batch 188, Loss: 0.906997, Accuracy: 80.03%\n",
      "Batch 189, Loss: 0.892277, Accuracy: 80.05%\n",
      "Batch 190, Loss: 0.967650, Accuracy: 80.05%\n",
      "Batch 191, Loss: 0.905025, Accuracy: 80.06%\n",
      "Batch 192, Loss: 0.972021, Accuracy: 80.03%\n",
      "Batch 193, Loss: 0.960809, Accuracy: 80.00%\n",
      "Batch 194, Loss: 0.947447, Accuracy: 79.99%\n",
      "Batch 195, Loss: 0.921194, Accuracy: 80.00%\n",
      "Batch 196, Loss: 0.954795, Accuracy: 79.99%\n",
      "Batch 197, Loss: 0.952034, Accuracy: 79.99%\n",
      "Batch 198, Loss: 0.953642, Accuracy: 79.99%\n",
      "Batch 199, Loss: 1.029365, Accuracy: 79.94%\n",
      "Batch 200, Loss: 0.932246, Accuracy: 79.94%\n",
      "Batch 201, Loss: 1.037310, Accuracy: 79.88%\n",
      "Batch 202, Loss: 0.988110, Accuracy: 79.85%\n",
      "Batch 203, Loss: 0.888725, Accuracy: 79.87%\n",
      "Batch 204, Loss: 0.962164, Accuracy: 79.86%\n",
      "Batch 205, Loss: 0.958855, Accuracy: 79.86%\n",
      "Batch 206, Loss: 0.971525, Accuracy: 79.85%\n",
      "Batch 207, Loss: 0.981342, Accuracy: 79.83%\n",
      "Batch 208, Loss: 0.910055, Accuracy: 79.85%\n",
      "Batch 209, Loss: 0.964128, Accuracy: 79.84%\n",
      "Batch 210, Loss: 0.873915, Accuracy: 79.87%\n",
      "Batch 211, Loss: 0.960313, Accuracy: 79.87%\n",
      "Batch 212, Loss: 0.880983, Accuracy: 79.89%\n",
      "Batch 213, Loss: 0.943775, Accuracy: 79.90%\n",
      "Training - Epoch 111, Loss: 0.943196, Accuracy: 79.90%\n",
      "Validation Batch 1, Loss: 0.895503, Accuracy: 84.38%\n",
      "Validation Batch 2, Loss: 0.947371, Accuracy: 81.25%\n",
      "Validation Batch 3, Loss: 1.011129, Accuracy: 78.65%\n",
      "Validation Batch 4, Loss: 0.939698, Accuracy: 79.30%\n",
      "Validation Batch 5, Loss: 0.919128, Accuracy: 80.00%\n",
      "Validation Batch 6, Loss: 0.874716, Accuracy: 80.99%\n",
      "Validation Batch 7, Loss: 0.966267, Accuracy: 80.58%\n",
      "Validation Batch 8, Loss: 0.974830, Accuracy: 80.27%\n",
      "Validation Batch 9, Loss: 0.982818, Accuracy: 80.03%\n",
      "Validation Batch 10, Loss: 0.975881, Accuracy: 79.69%\n",
      "Validation Batch 11, Loss: 0.908750, Accuracy: 80.11%\n",
      "Validation Batch 12, Loss: 0.878604, Accuracy: 80.73%\n",
      "Validation Batch 13, Loss: 0.975526, Accuracy: 80.41%\n",
      "Validation Batch 14, Loss: 0.972857, Accuracy: 80.02%\n",
      "Validation Batch 15, Loss: 0.945643, Accuracy: 79.90%\n",
      "Validation Batch 16, Loss: 0.912924, Accuracy: 80.18%\n",
      "Validation Batch 17, Loss: 0.967086, Accuracy: 79.96%\n",
      "Validation Batch 18, Loss: 0.920659, Accuracy: 80.03%\n",
      "Validation Batch 19, Loss: 0.978231, Accuracy: 79.85%\n",
      "Validation Batch 20, Loss: 0.917135, Accuracy: 80.00%\n",
      "Validation Batch 21, Loss: 0.959481, Accuracy: 79.99%\n",
      "Validation Batch 22, Loss: 0.968603, Accuracy: 79.83%\n",
      "Validation Batch 23, Loss: 1.022942, Accuracy: 79.48%\n",
      "Validation Batch 24, Loss: 1.005123, Accuracy: 79.23%\n",
      "Validation Batch 25, Loss: 0.955373, Accuracy: 79.25%\n",
      "Validation Batch 26, Loss: 0.929105, Accuracy: 79.39%\n",
      "Validation Batch 27, Loss: 0.895169, Accuracy: 79.51%\n",
      "Validation - Epoch 111, Loss: 0.948169, Accuracy: 79.51%\n",
      "Patienceâ€”19\n",
      "Epoch 112\n",
      "Batch 1, Loss: 0.893035, Accuracy: 85.94%\n",
      "Batch 2, Loss: 1.051567, Accuracy: 78.12%\n",
      "Batch 3, Loss: 0.874416, Accuracy: 80.73%\n",
      "Batch 4, Loss: 0.889164, Accuracy: 82.42%\n",
      "Batch 5, Loss: 0.952105, Accuracy: 82.19%\n",
      "Batch 6, Loss: 0.974797, Accuracy: 81.51%\n",
      "Batch 7, Loss: 0.993813, Accuracy: 80.36%\n",
      "Batch 8, Loss: 0.865243, Accuracy: 81.45%\n",
      "Batch 9, Loss: 0.915818, Accuracy: 81.60%\n",
      "Batch 10, Loss: 0.973256, Accuracy: 81.09%\n",
      "Batch 11, Loss: 0.848467, Accuracy: 81.96%\n",
      "Batch 12, Loss: 0.907588, Accuracy: 82.16%\n",
      "Batch 13, Loss: 0.988109, Accuracy: 81.73%\n",
      "Batch 14, Loss: 0.946500, Accuracy: 81.47%\n",
      "Batch 15, Loss: 0.891789, Accuracy: 81.77%\n",
      "Batch 16, Loss: 0.927110, Accuracy: 81.74%\n",
      "Batch 17, Loss: 0.907071, Accuracy: 81.89%\n",
      "Batch 18, Loss: 0.905268, Accuracy: 82.03%\n",
      "Batch 19, Loss: 0.851619, Accuracy: 82.40%\n",
      "Batch 20, Loss: 0.914882, Accuracy: 82.34%\n",
      "Batch 21, Loss: 0.986893, Accuracy: 81.92%\n",
      "Batch 22, Loss: 0.885268, Accuracy: 82.10%\n",
      "Batch 23, Loss: 0.940259, Accuracy: 82.00%\n",
      "Batch 24, Loss: 0.881303, Accuracy: 82.16%\n",
      "Batch 25, Loss: 0.962153, Accuracy: 82.00%\n",
      "Batch 26, Loss: 0.923576, Accuracy: 81.97%\n",
      "Batch 27, Loss: 1.023950, Accuracy: 81.60%\n",
      "Batch 28, Loss: 0.942067, Accuracy: 81.53%\n",
      "Batch 29, Loss: 0.967000, Accuracy: 81.47%\n",
      "Batch 30, Loss: 0.936293, Accuracy: 81.41%\n",
      "Batch 31, Loss: 1.014255, Accuracy: 81.15%\n",
      "Batch 32, Loss: 0.968678, Accuracy: 80.96%\n",
      "Batch 33, Loss: 0.873807, Accuracy: 81.16%\n",
      "Batch 34, Loss: 0.902240, Accuracy: 81.20%\n",
      "Batch 35, Loss: 0.970673, Accuracy: 81.03%\n",
      "Batch 36, Loss: 0.905838, Accuracy: 81.12%\n",
      "Batch 37, Loss: 0.938854, Accuracy: 81.08%\n",
      "Batch 38, Loss: 0.938049, Accuracy: 81.09%\n",
      "Batch 39, Loss: 0.957918, Accuracy: 81.01%\n",
      "Batch 40, Loss: 0.900676, Accuracy: 81.09%\n",
      "Batch 41, Loss: 0.868792, Accuracy: 81.29%\n",
      "Batch 42, Loss: 0.991714, Accuracy: 81.10%\n",
      "Batch 43, Loss: 0.885554, Accuracy: 81.18%\n",
      "Batch 44, Loss: 0.920672, Accuracy: 81.21%\n",
      "Batch 45, Loss: 0.909748, Accuracy: 81.25%\n",
      "Batch 46, Loss: 0.895692, Accuracy: 81.35%\n",
      "Batch 47, Loss: 1.030178, Accuracy: 81.18%\n",
      "Batch 48, Loss: 0.999146, Accuracy: 81.02%\n",
      "Batch 49, Loss: 0.881435, Accuracy: 81.15%\n",
      "Batch 50, Loss: 1.009035, Accuracy: 81.06%\n",
      "Batch 51, Loss: 0.843459, Accuracy: 81.25%\n",
      "Batch 52, Loss: 0.908101, Accuracy: 81.25%\n",
      "Batch 53, Loss: 0.948735, Accuracy: 81.25%\n",
      "Batch 54, Loss: 0.863282, Accuracy: 81.39%\n",
      "Batch 55, Loss: 0.973618, Accuracy: 81.28%\n",
      "Batch 56, Loss: 0.878512, Accuracy: 81.36%\n",
      "Batch 57, Loss: 0.904692, Accuracy: 81.41%\n",
      "Batch 58, Loss: 1.006164, Accuracy: 81.25%\n",
      "Batch 59, Loss: 0.914399, Accuracy: 81.28%\n",
      "Batch 60, Loss: 0.866751, Accuracy: 81.41%\n",
      "Batch 61, Loss: 1.029960, Accuracy: 81.22%\n",
      "Batch 62, Loss: 0.913209, Accuracy: 81.28%\n",
      "Batch 63, Loss: 0.896763, Accuracy: 81.32%\n",
      "Batch 64, Loss: 0.944573, Accuracy: 81.30%\n",
      "Batch 65, Loss: 0.981946, Accuracy: 81.23%\n",
      "Batch 66, Loss: 0.883327, Accuracy: 81.30%\n",
      "Batch 67, Loss: 0.988256, Accuracy: 81.23%\n",
      "Batch 68, Loss: 0.950780, Accuracy: 81.18%\n",
      "Batch 69, Loss: 0.946826, Accuracy: 81.14%\n",
      "Batch 70, Loss: 0.938617, Accuracy: 81.12%\n",
      "Batch 71, Loss: 0.975358, Accuracy: 81.03%\n",
      "Batch 72, Loss: 0.980137, Accuracy: 80.95%\n",
      "Batch 73, Loss: 0.950156, Accuracy: 80.93%\n",
      "Batch 74, Loss: 0.992867, Accuracy: 80.83%\n",
      "Batch 75, Loss: 0.935780, Accuracy: 80.85%\n",
      "Batch 76, Loss: 0.898047, Accuracy: 80.90%\n",
      "Batch 77, Loss: 0.986132, Accuracy: 80.84%\n",
      "Batch 78, Loss: 1.038760, Accuracy: 80.71%\n",
      "Batch 79, Loss: 0.944072, Accuracy: 80.70%\n",
      "Batch 80, Loss: 0.933348, Accuracy: 80.70%\n",
      "Batch 81, Loss: 0.971763, Accuracy: 80.67%\n",
      "Batch 82, Loss: 0.958042, Accuracy: 80.64%\n",
      "Batch 83, Loss: 1.035541, Accuracy: 80.50%\n",
      "Batch 84, Loss: 0.887183, Accuracy: 80.56%\n",
      "Batch 85, Loss: 0.987483, Accuracy: 80.50%\n",
      "Batch 86, Loss: 1.051798, Accuracy: 80.38%\n",
      "Batch 87, Loss: 1.043613, Accuracy: 80.24%\n",
      "Batch 88, Loss: 0.950045, Accuracy: 80.24%\n",
      "Batch 89, Loss: 0.977863, Accuracy: 80.18%\n",
      "Batch 90, Loss: 1.013645, Accuracy: 80.09%\n",
      "Batch 91, Loss: 1.001684, Accuracy: 80.03%\n",
      "Batch 92, Loss: 0.870331, Accuracy: 80.11%\n",
      "Batch 93, Loss: 1.018010, Accuracy: 80.01%\n",
      "Batch 94, Loss: 0.859067, Accuracy: 80.09%\n",
      "Batch 95, Loss: 0.900417, Accuracy: 80.13%\n",
      "Batch 96, Loss: 0.983755, Accuracy: 80.09%\n",
      "Batch 97, Loss: 1.045721, Accuracy: 79.99%\n",
      "Batch 98, Loss: 1.020355, Accuracy: 79.93%\n",
      "Batch 99, Loss: 0.905860, Accuracy: 79.97%\n",
      "Batch 100, Loss: 0.952558, Accuracy: 79.95%\n",
      "Batch 101, Loss: 1.035593, Accuracy: 79.86%\n",
      "Batch 102, Loss: 0.923896, Accuracy: 79.89%\n",
      "Batch 103, Loss: 0.915420, Accuracy: 79.92%\n",
      "Batch 104, Loss: 1.020193, Accuracy: 79.82%\n",
      "Batch 105, Loss: 1.009227, Accuracy: 79.75%\n",
      "Batch 106, Loss: 0.953474, Accuracy: 79.75%\n",
      "Batch 107, Loss: 0.951848, Accuracy: 79.72%\n",
      "Batch 108, Loss: 0.829803, Accuracy: 79.83%\n",
      "Batch 109, Loss: 0.940500, Accuracy: 79.85%\n",
      "Batch 110, Loss: 1.015016, Accuracy: 79.79%\n",
      "Batch 111, Loss: 0.950054, Accuracy: 79.79%\n",
      "Batch 112, Loss: 0.878534, Accuracy: 79.84%\n",
      "Batch 113, Loss: 0.942407, Accuracy: 79.84%\n",
      "Batch 114, Loss: 0.905902, Accuracy: 79.85%\n",
      "Batch 115, Loss: 0.945074, Accuracy: 79.85%\n",
      "Batch 116, Loss: 0.899377, Accuracy: 79.89%\n",
      "Batch 117, Loss: 0.929734, Accuracy: 79.90%\n",
      "Batch 118, Loss: 0.948325, Accuracy: 79.91%\n",
      "Batch 119, Loss: 0.899063, Accuracy: 79.94%\n",
      "Batch 120, Loss: 0.903975, Accuracy: 79.97%\n",
      "Batch 121, Loss: 0.970934, Accuracy: 79.95%\n",
      "Batch 122, Loss: 0.902979, Accuracy: 79.97%\n",
      "Batch 123, Loss: 0.903219, Accuracy: 80.02%\n",
      "Batch 124, Loss: 0.979038, Accuracy: 80.00%\n",
      "Batch 125, Loss: 0.911742, Accuracy: 80.03%\n",
      "Batch 126, Loss: 0.904889, Accuracy: 80.05%\n",
      "Batch 127, Loss: 0.934455, Accuracy: 80.04%\n",
      "Batch 128, Loss: 0.973084, Accuracy: 80.02%\n",
      "Batch 129, Loss: 0.947533, Accuracy: 80.01%\n",
      "Batch 130, Loss: 0.958242, Accuracy: 79.99%\n",
      "Batch 131, Loss: 0.853374, Accuracy: 80.06%\n",
      "Batch 132, Loss: 1.004048, Accuracy: 80.01%\n",
      "Batch 133, Loss: 0.929995, Accuracy: 80.02%\n",
      "Batch 134, Loss: 0.882726, Accuracy: 80.06%\n",
      "Batch 135, Loss: 0.941079, Accuracy: 80.07%\n",
      "Batch 136, Loss: 0.951225, Accuracy: 80.06%\n",
      "Batch 137, Loss: 0.963998, Accuracy: 80.04%\n",
      "Batch 138, Loss: 0.924352, Accuracy: 80.05%\n",
      "Batch 139, Loss: 0.897702, Accuracy: 80.08%\n",
      "Batch 140, Loss: 0.961400, Accuracy: 80.07%\n",
      "Batch 141, Loss: 1.107621, Accuracy: 79.94%\n",
      "Batch 142, Loss: 0.960081, Accuracy: 79.94%\n",
      "Batch 143, Loss: 0.985711, Accuracy: 79.91%\n",
      "Batch 144, Loss: 0.902474, Accuracy: 79.94%\n",
      "Batch 145, Loss: 0.987575, Accuracy: 79.91%\n",
      "Batch 146, Loss: 0.974258, Accuracy: 79.87%\n",
      "Batch 147, Loss: 0.912042, Accuracy: 79.90%\n",
      "Batch 148, Loss: 0.922636, Accuracy: 79.92%\n",
      "Batch 149, Loss: 0.942501, Accuracy: 79.92%\n",
      "Batch 150, Loss: 0.935315, Accuracy: 79.93%\n",
      "Batch 151, Loss: 0.898272, Accuracy: 79.95%\n",
      "Batch 152, Loss: 0.944981, Accuracy: 79.94%\n",
      "Batch 153, Loss: 1.020748, Accuracy: 79.90%\n",
      "Batch 154, Loss: 0.921565, Accuracy: 79.92%\n",
      "Batch 155, Loss: 0.958822, Accuracy: 79.91%\n",
      "Batch 156, Loss: 0.908570, Accuracy: 79.95%\n",
      "Batch 157, Loss: 0.890730, Accuracy: 80.00%\n",
      "Batch 158, Loss: 0.933904, Accuracy: 79.99%\n",
      "Batch 159, Loss: 0.986848, Accuracy: 79.96%\n",
      "Batch 160, Loss: 0.932102, Accuracy: 79.98%\n",
      "Batch 161, Loss: 0.908877, Accuracy: 80.00%\n",
      "Batch 162, Loss: 0.937982, Accuracy: 80.01%\n",
      "Batch 163, Loss: 0.959947, Accuracy: 79.99%\n",
      "Batch 164, Loss: 0.932951, Accuracy: 80.00%\n",
      "Batch 165, Loss: 1.024939, Accuracy: 79.94%\n",
      "Batch 166, Loss: 0.854210, Accuracy: 80.01%\n",
      "Batch 167, Loss: 0.918872, Accuracy: 80.02%\n",
      "Batch 168, Loss: 0.860782, Accuracy: 80.07%\n",
      "Batch 169, Loss: 0.983372, Accuracy: 80.04%\n",
      "Batch 170, Loss: 0.926364, Accuracy: 80.05%\n",
      "Batch 171, Loss: 0.995155, Accuracy: 80.02%\n",
      "Batch 172, Loss: 0.900509, Accuracy: 80.04%\n",
      "Batch 173, Loss: 0.900531, Accuracy: 80.06%\n",
      "Batch 174, Loss: 0.932982, Accuracy: 80.07%\n",
      "Batch 175, Loss: 0.967316, Accuracy: 80.07%\n",
      "Batch 176, Loss: 0.877479, Accuracy: 80.10%\n",
      "Batch 177, Loss: 0.951675, Accuracy: 80.10%\n",
      "Batch 178, Loss: 0.897506, Accuracy: 80.12%\n",
      "Batch 179, Loss: 0.897584, Accuracy: 80.14%\n",
      "Batch 180, Loss: 0.999815, Accuracy: 80.10%\n",
      "Batch 181, Loss: 0.920922, Accuracy: 80.12%\n",
      "Batch 182, Loss: 0.951496, Accuracy: 80.12%\n",
      "Batch 183, Loss: 0.982266, Accuracy: 80.09%\n",
      "Batch 184, Loss: 0.889520, Accuracy: 80.11%\n",
      "Batch 185, Loss: 0.983824, Accuracy: 80.08%\n",
      "Batch 186, Loss: 0.839949, Accuracy: 80.13%\n",
      "Batch 187, Loss: 0.965009, Accuracy: 80.11%\n",
      "Batch 188, Loss: 0.904879, Accuracy: 80.14%\n",
      "Batch 189, Loss: 0.890811, Accuracy: 80.17%\n",
      "Batch 190, Loss: 1.014497, Accuracy: 80.12%\n",
      "Batch 191, Loss: 0.915732, Accuracy: 80.14%\n",
      "Batch 192, Loss: 0.983347, Accuracy: 80.12%\n",
      "Batch 193, Loss: 0.999879, Accuracy: 80.08%\n",
      "Batch 194, Loss: 0.846323, Accuracy: 80.14%\n",
      "Batch 195, Loss: 0.866559, Accuracy: 80.18%\n",
      "Batch 196, Loss: 0.855560, Accuracy: 80.22%\n",
      "Batch 197, Loss: 0.992309, Accuracy: 80.20%\n",
      "Batch 198, Loss: 0.982064, Accuracy: 80.17%\n",
      "Batch 199, Loss: 0.994186, Accuracy: 80.14%\n",
      "Batch 200, Loss: 0.938183, Accuracy: 80.14%\n",
      "Batch 201, Loss: 0.990969, Accuracy: 80.12%\n",
      "Batch 202, Loss: 0.941950, Accuracy: 80.11%\n",
      "Batch 203, Loss: 0.931882, Accuracy: 80.12%\n",
      "Batch 204, Loss: 0.999482, Accuracy: 80.09%\n",
      "Batch 205, Loss: 0.995067, Accuracy: 80.07%\n",
      "Batch 206, Loss: 0.973879, Accuracy: 80.04%\n",
      "Batch 207, Loss: 0.923648, Accuracy: 80.06%\n",
      "Batch 208, Loss: 0.941213, Accuracy: 80.06%\n",
      "Batch 209, Loss: 0.917512, Accuracy: 80.07%\n",
      "Batch 210, Loss: 0.949090, Accuracy: 80.07%\n",
      "Batch 211, Loss: 0.906656, Accuracy: 80.09%\n",
      "Batch 212, Loss: 0.922632, Accuracy: 80.09%\n",
      "Batch 213, Loss: 0.949470, Accuracy: 80.09%\n",
      "Training - Epoch 112, Loss: 0.941523, Accuracy: 80.09%\n",
      "Validation Batch 1, Loss: 0.865550, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.899552, Accuracy: 86.72%\n",
      "Validation Batch 3, Loss: 0.989403, Accuracy: 82.29%\n",
      "Validation Batch 4, Loss: 0.921250, Accuracy: 82.03%\n",
      "Validation Batch 5, Loss: 0.880328, Accuracy: 82.50%\n",
      "Validation Batch 6, Loss: 0.844557, Accuracy: 83.85%\n",
      "Validation Batch 7, Loss: 0.928639, Accuracy: 83.48%\n",
      "Validation Batch 8, Loss: 0.952625, Accuracy: 82.81%\n",
      "Validation Batch 9, Loss: 0.955209, Accuracy: 82.29%\n",
      "Validation Batch 10, Loss: 0.951818, Accuracy: 82.03%\n",
      "Validation Batch 11, Loss: 0.894734, Accuracy: 82.24%\n",
      "Validation Batch 12, Loss: 0.865023, Accuracy: 82.68%\n",
      "Validation Batch 13, Loss: 0.937992, Accuracy: 82.57%\n",
      "Validation Batch 14, Loss: 0.925557, Accuracy: 82.59%\n",
      "Validation Batch 15, Loss: 0.921780, Accuracy: 82.60%\n",
      "Validation Batch 16, Loss: 0.887974, Accuracy: 82.81%\n",
      "Validation Batch 17, Loss: 0.939196, Accuracy: 82.63%\n",
      "Validation Batch 18, Loss: 0.897485, Accuracy: 82.73%\n",
      "Validation Batch 19, Loss: 0.942754, Accuracy: 82.65%\n",
      "Validation Batch 20, Loss: 0.857731, Accuracy: 82.89%\n",
      "Validation Batch 21, Loss: 0.935840, Accuracy: 82.74%\n",
      "Validation Batch 22, Loss: 0.916683, Accuracy: 82.74%\n",
      "Validation Batch 23, Loss: 0.952918, Accuracy: 82.61%\n",
      "Validation Batch 24, Loss: 0.974710, Accuracy: 82.29%\n",
      "Validation Batch 25, Loss: 0.902553, Accuracy: 82.38%\n",
      "Validation Batch 26, Loss: 0.918906, Accuracy: 82.39%\n",
      "Validation Batch 27, Loss: 0.846750, Accuracy: 82.56%\n",
      "Validation - Epoch 112, Loss: 0.915093, Accuracy: 82.56%\n",
      "Patienceâ€”0\n",
      "Epoch 113\n",
      "Batch 1, Loss: 1.018103, Accuracy: 71.88%\n",
      "Batch 2, Loss: 1.001317, Accuracy: 71.88%\n",
      "Batch 3, Loss: 0.970131, Accuracy: 72.92%\n",
      "Batch 4, Loss: 0.939875, Accuracy: 75.00%\n",
      "Batch 5, Loss: 0.958630, Accuracy: 75.62%\n",
      "Batch 6, Loss: 0.876561, Accuracy: 77.86%\n",
      "Batch 7, Loss: 1.016423, Accuracy: 77.01%\n",
      "Batch 8, Loss: 0.944795, Accuracy: 77.34%\n",
      "Batch 9, Loss: 0.956887, Accuracy: 77.60%\n",
      "Batch 10, Loss: 0.803964, Accuracy: 79.22%\n",
      "Batch 11, Loss: 0.874624, Accuracy: 80.11%\n",
      "Batch 12, Loss: 0.890834, Accuracy: 80.60%\n",
      "Batch 13, Loss: 0.969788, Accuracy: 80.41%\n",
      "Batch 14, Loss: 0.963954, Accuracy: 80.13%\n",
      "Batch 15, Loss: 0.911485, Accuracy: 80.31%\n",
      "Batch 16, Loss: 0.957310, Accuracy: 80.18%\n",
      "Batch 17, Loss: 0.858726, Accuracy: 80.70%\n",
      "Batch 18, Loss: 0.869325, Accuracy: 81.16%\n",
      "Batch 19, Loss: 0.975272, Accuracy: 80.92%\n",
      "Batch 20, Loss: 0.977396, Accuracy: 80.70%\n",
      "Batch 21, Loss: 0.964632, Accuracy: 80.65%\n",
      "Batch 22, Loss: 0.902475, Accuracy: 80.89%\n",
      "Batch 23, Loss: 1.014004, Accuracy: 80.43%\n",
      "Batch 24, Loss: 0.919295, Accuracy: 80.47%\n",
      "Batch 25, Loss: 0.959380, Accuracy: 80.31%\n",
      "Batch 26, Loss: 0.959741, Accuracy: 80.23%\n",
      "Batch 27, Loss: 0.962000, Accuracy: 80.15%\n",
      "Batch 28, Loss: 1.016167, Accuracy: 79.91%\n",
      "Batch 29, Loss: 0.941094, Accuracy: 79.90%\n",
      "Batch 30, Loss: 0.907873, Accuracy: 80.00%\n",
      "Batch 31, Loss: 0.971565, Accuracy: 79.89%\n",
      "Batch 32, Loss: 0.864307, Accuracy: 80.18%\n",
      "Batch 33, Loss: 0.875284, Accuracy: 80.35%\n",
      "Batch 34, Loss: 0.885688, Accuracy: 80.56%\n",
      "Batch 35, Loss: 0.994072, Accuracy: 80.40%\n",
      "Batch 36, Loss: 0.941248, Accuracy: 80.38%\n",
      "Batch 37, Loss: 0.870569, Accuracy: 80.57%\n",
      "Batch 38, Loss: 0.992860, Accuracy: 80.43%\n",
      "Batch 39, Loss: 0.930442, Accuracy: 80.45%\n",
      "Batch 40, Loss: 0.885041, Accuracy: 80.59%\n",
      "Batch 41, Loss: 0.993340, Accuracy: 80.45%\n",
      "Batch 42, Loss: 0.905305, Accuracy: 80.54%\n",
      "Batch 43, Loss: 0.882484, Accuracy: 80.70%\n",
      "Batch 44, Loss: 0.906132, Accuracy: 80.75%\n",
      "Batch 45, Loss: 0.994062, Accuracy: 80.62%\n",
      "Batch 46, Loss: 0.978318, Accuracy: 80.50%\n",
      "Batch 47, Loss: 0.920726, Accuracy: 80.55%\n",
      "Batch 48, Loss: 0.860811, Accuracy: 80.73%\n",
      "Batch 49, Loss: 0.887966, Accuracy: 80.84%\n",
      "Batch 50, Loss: 0.922007, Accuracy: 80.88%\n",
      "Batch 51, Loss: 1.005287, Accuracy: 80.70%\n",
      "Batch 52, Loss: 0.993468, Accuracy: 80.56%\n",
      "Batch 53, Loss: 1.020553, Accuracy: 80.40%\n",
      "Batch 54, Loss: 0.945928, Accuracy: 80.35%\n",
      "Batch 55, Loss: 0.916614, Accuracy: 80.37%\n",
      "Batch 56, Loss: 0.958005, Accuracy: 80.30%\n",
      "Batch 57, Loss: 0.977751, Accuracy: 80.21%\n",
      "Batch 58, Loss: 0.924068, Accuracy: 80.28%\n",
      "Batch 59, Loss: 0.928753, Accuracy: 80.32%\n",
      "Batch 60, Loss: 0.922535, Accuracy: 80.36%\n",
      "Batch 61, Loss: 0.959661, Accuracy: 80.30%\n",
      "Batch 62, Loss: 0.977303, Accuracy: 80.19%\n",
      "Batch 63, Loss: 0.913795, Accuracy: 80.26%\n",
      "Batch 64, Loss: 0.915169, Accuracy: 80.27%\n",
      "Batch 65, Loss: 0.916649, Accuracy: 80.31%\n",
      "Batch 66, Loss: 0.963342, Accuracy: 80.26%\n",
      "Batch 67, Loss: 0.980552, Accuracy: 80.20%\n",
      "Batch 68, Loss: 0.951916, Accuracy: 80.19%\n",
      "Batch 69, Loss: 0.907802, Accuracy: 80.23%\n",
      "Batch 70, Loss: 0.913532, Accuracy: 80.29%\n",
      "Batch 71, Loss: 0.955840, Accuracy: 80.26%\n",
      "Batch 72, Loss: 0.973886, Accuracy: 80.21%\n",
      "Batch 73, Loss: 0.994783, Accuracy: 80.16%\n",
      "Batch 74, Loss: 0.969021, Accuracy: 80.13%\n",
      "Batch 75, Loss: 0.966000, Accuracy: 80.08%\n",
      "Batch 76, Loss: 0.943794, Accuracy: 80.08%\n",
      "Batch 77, Loss: 1.034220, Accuracy: 79.95%\n",
      "Batch 78, Loss: 0.918357, Accuracy: 80.01%\n",
      "Batch 79, Loss: 0.980493, Accuracy: 79.92%\n",
      "Batch 80, Loss: 0.957942, Accuracy: 79.88%\n",
      "Batch 81, Loss: 0.967014, Accuracy: 79.86%\n",
      "Batch 82, Loss: 0.953396, Accuracy: 79.84%\n",
      "Batch 83, Loss: 0.936604, Accuracy: 79.86%\n",
      "Batch 84, Loss: 0.938297, Accuracy: 79.87%\n",
      "Batch 85, Loss: 0.949158, Accuracy: 79.87%\n",
      "Batch 86, Loss: 0.863181, Accuracy: 79.96%\n",
      "Batch 87, Loss: 0.960287, Accuracy: 79.92%\n",
      "Batch 88, Loss: 0.946963, Accuracy: 79.92%\n",
      "Batch 89, Loss: 0.971419, Accuracy: 79.88%\n",
      "Batch 90, Loss: 1.003770, Accuracy: 79.81%\n",
      "Batch 91, Loss: 0.990079, Accuracy: 79.77%\n",
      "Batch 92, Loss: 0.928767, Accuracy: 79.77%\n",
      "Batch 93, Loss: 0.980806, Accuracy: 79.72%\n",
      "Batch 94, Loss: 0.950279, Accuracy: 79.72%\n",
      "Batch 95, Loss: 0.960719, Accuracy: 79.70%\n",
      "Batch 96, Loss: 0.915859, Accuracy: 79.74%\n",
      "Batch 97, Loss: 0.888205, Accuracy: 79.82%\n",
      "Batch 98, Loss: 1.017369, Accuracy: 79.75%\n",
      "Batch 99, Loss: 0.904977, Accuracy: 79.80%\n",
      "Batch 100, Loss: 0.902466, Accuracy: 79.84%\n",
      "Batch 101, Loss: 0.916582, Accuracy: 79.87%\n",
      "Batch 102, Loss: 0.919082, Accuracy: 79.89%\n",
      "Batch 103, Loss: 0.935731, Accuracy: 79.87%\n",
      "Batch 104, Loss: 0.954739, Accuracy: 79.87%\n",
      "Batch 105, Loss: 0.879596, Accuracy: 79.94%\n",
      "Batch 106, Loss: 0.908375, Accuracy: 79.97%\n",
      "Batch 107, Loss: 0.895090, Accuracy: 80.02%\n",
      "Batch 108, Loss: 0.968780, Accuracy: 80.01%\n",
      "Batch 109, Loss: 0.926803, Accuracy: 80.00%\n",
      "Batch 110, Loss: 0.906763, Accuracy: 80.03%\n",
      "Batch 111, Loss: 0.929073, Accuracy: 80.03%\n",
      "Batch 112, Loss: 0.833042, Accuracy: 80.12%\n",
      "Batch 113, Loss: 0.894577, Accuracy: 80.16%\n",
      "Batch 114, Loss: 1.036045, Accuracy: 80.07%\n",
      "Batch 115, Loss: 0.938092, Accuracy: 80.08%\n",
      "Batch 116, Loss: 0.901184, Accuracy: 80.12%\n",
      "Batch 117, Loss: 0.921706, Accuracy: 80.15%\n",
      "Batch 118, Loss: 0.863764, Accuracy: 80.22%\n",
      "Batch 119, Loss: 0.917182, Accuracy: 80.24%\n",
      "Batch 120, Loss: 0.963207, Accuracy: 80.21%\n",
      "Batch 121, Loss: 0.956278, Accuracy: 80.19%\n",
      "Batch 122, Loss: 0.960152, Accuracy: 80.17%\n",
      "Batch 123, Loss: 1.020589, Accuracy: 80.12%\n",
      "Batch 124, Loss: 0.911567, Accuracy: 80.14%\n",
      "Batch 125, Loss: 0.898986, Accuracy: 80.19%\n",
      "Batch 126, Loss: 0.882947, Accuracy: 80.25%\n",
      "Batch 127, Loss: 0.919273, Accuracy: 80.27%\n",
      "Batch 128, Loss: 0.892717, Accuracy: 80.30%\n",
      "Batch 129, Loss: 0.891878, Accuracy: 80.34%\n",
      "Batch 130, Loss: 0.923860, Accuracy: 80.35%\n",
      "Batch 131, Loss: 0.883929, Accuracy: 80.39%\n",
      "Batch 132, Loss: 0.926875, Accuracy: 80.41%\n",
      "Batch 133, Loss: 0.899985, Accuracy: 80.44%\n",
      "Batch 134, Loss: 0.929903, Accuracy: 80.45%\n",
      "Batch 135, Loss: 0.957255, Accuracy: 80.43%\n",
      "Batch 136, Loss: 0.879669, Accuracy: 80.48%\n",
      "Batch 137, Loss: 0.881701, Accuracy: 80.52%\n",
      "Batch 138, Loss: 0.962147, Accuracy: 80.50%\n",
      "Batch 139, Loss: 0.906764, Accuracy: 80.53%\n",
      "Batch 140, Loss: 0.950302, Accuracy: 80.52%\n",
      "Batch 141, Loss: 0.975945, Accuracy: 80.51%\n",
      "Batch 142, Loss: 0.989015, Accuracy: 80.46%\n",
      "Batch 143, Loss: 0.881747, Accuracy: 80.51%\n",
      "Batch 144, Loss: 0.887047, Accuracy: 80.56%\n",
      "Batch 145, Loss: 0.929434, Accuracy: 80.55%\n",
      "Batch 146, Loss: 0.902609, Accuracy: 80.57%\n",
      "Batch 147, Loss: 0.958153, Accuracy: 80.54%\n",
      "Batch 148, Loss: 1.007076, Accuracy: 80.48%\n",
      "Batch 149, Loss: 0.963939, Accuracy: 80.46%\n",
      "Batch 150, Loss: 0.944199, Accuracy: 80.47%\n",
      "Batch 151, Loss: 0.881963, Accuracy: 80.53%\n",
      "Batch 152, Loss: 1.001938, Accuracy: 80.48%\n",
      "Batch 153, Loss: 1.042476, Accuracy: 80.40%\n",
      "Batch 154, Loss: 0.980364, Accuracy: 80.37%\n",
      "Batch 155, Loss: 1.005593, Accuracy: 80.32%\n",
      "Batch 156, Loss: 0.954659, Accuracy: 80.32%\n",
      "Batch 157, Loss: 0.933028, Accuracy: 80.31%\n",
      "Batch 158, Loss: 0.867916, Accuracy: 80.36%\n",
      "Batch 159, Loss: 0.975215, Accuracy: 80.33%\n",
      "Batch 160, Loss: 0.888483, Accuracy: 80.35%\n",
      "Batch 161, Loss: 0.926392, Accuracy: 80.37%\n",
      "Batch 162, Loss: 0.954074, Accuracy: 80.35%\n",
      "Batch 163, Loss: 0.968926, Accuracy: 80.33%\n",
      "Batch 164, Loss: 0.880758, Accuracy: 80.36%\n",
      "Batch 165, Loss: 1.002255, Accuracy: 80.32%\n",
      "Batch 166, Loss: 0.922855, Accuracy: 80.34%\n",
      "Batch 167, Loss: 0.987700, Accuracy: 80.30%\n",
      "Batch 168, Loss: 1.036777, Accuracy: 80.24%\n",
      "Batch 169, Loss: 0.996648, Accuracy: 80.21%\n",
      "Batch 170, Loss: 0.952486, Accuracy: 80.20%\n",
      "Batch 171, Loss: 0.846196, Accuracy: 80.26%\n",
      "Batch 172, Loss: 0.850894, Accuracy: 80.31%\n",
      "Batch 173, Loss: 0.907799, Accuracy: 80.32%\n",
      "Batch 174, Loss: 1.048518, Accuracy: 80.26%\n",
      "Batch 175, Loss: 0.971784, Accuracy: 80.23%\n",
      "Batch 176, Loss: 0.976198, Accuracy: 80.21%\n",
      "Batch 177, Loss: 1.046781, Accuracy: 80.15%\n",
      "Batch 178, Loss: 0.927327, Accuracy: 80.14%\n",
      "Batch 179, Loss: 0.881612, Accuracy: 80.18%\n",
      "Batch 180, Loss: 0.966037, Accuracy: 80.16%\n",
      "Batch 181, Loss: 1.001625, Accuracy: 80.12%\n",
      "Batch 182, Loss: 0.875586, Accuracy: 80.16%\n",
      "Batch 183, Loss: 0.915330, Accuracy: 80.18%\n",
      "Batch 184, Loss: 0.964876, Accuracy: 80.17%\n",
      "Batch 185, Loss: 0.936669, Accuracy: 80.18%\n",
      "Batch 186, Loss: 0.851946, Accuracy: 80.23%\n",
      "Batch 187, Loss: 0.960330, Accuracy: 80.21%\n",
      "Batch 188, Loss: 0.931221, Accuracy: 80.21%\n",
      "Batch 189, Loss: 0.870393, Accuracy: 80.25%\n",
      "Batch 190, Loss: 0.879565, Accuracy: 80.29%\n",
      "Batch 191, Loss: 0.922930, Accuracy: 80.29%\n",
      "Batch 192, Loss: 0.967105, Accuracy: 80.28%\n",
      "Batch 193, Loss: 0.956357, Accuracy: 80.27%\n",
      "Batch 194, Loss: 0.929224, Accuracy: 80.28%\n",
      "Batch 195, Loss: 1.009265, Accuracy: 80.23%\n",
      "Batch 196, Loss: 0.933137, Accuracy: 80.23%\n",
      "Batch 197, Loss: 0.989782, Accuracy: 80.21%\n",
      "Batch 198, Loss: 0.932612, Accuracy: 80.22%\n",
      "Batch 199, Loss: 0.991095, Accuracy: 80.20%\n",
      "Batch 200, Loss: 1.006532, Accuracy: 80.15%\n",
      "Batch 201, Loss: 0.900190, Accuracy: 80.16%\n",
      "Batch 202, Loss: 0.997628, Accuracy: 80.14%\n",
      "Batch 203, Loss: 0.898483, Accuracy: 80.16%\n",
      "Batch 204, Loss: 0.985663, Accuracy: 80.14%\n",
      "Batch 205, Loss: 0.917558, Accuracy: 80.15%\n",
      "Batch 206, Loss: 0.925852, Accuracy: 80.16%\n",
      "Batch 207, Loss: 0.924981, Accuracy: 80.17%\n",
      "Batch 208, Loss: 0.906009, Accuracy: 80.18%\n",
      "Batch 209, Loss: 0.961726, Accuracy: 80.17%\n",
      "Batch 210, Loss: 0.937169, Accuracy: 80.17%\n",
      "Batch 211, Loss: 0.964968, Accuracy: 80.16%\n",
      "Batch 212, Loss: 0.957081, Accuracy: 80.14%\n",
      "Batch 213, Loss: 0.896632, Accuracy: 80.17%\n",
      "Training - Epoch 113, Loss: 0.940044, Accuracy: 80.17%\n",
      "Validation Batch 1, Loss: 0.864234, Accuracy: 89.06%\n",
      "Validation Batch 2, Loss: 0.897830, Accuracy: 87.50%\n",
      "Validation Batch 3, Loss: 0.979530, Accuracy: 83.85%\n",
      "Validation Batch 4, Loss: 0.918969, Accuracy: 83.20%\n",
      "Validation Batch 5, Loss: 0.871226, Accuracy: 84.06%\n",
      "Validation Batch 6, Loss: 0.847312, Accuracy: 84.90%\n",
      "Validation Batch 7, Loss: 0.923981, Accuracy: 84.60%\n",
      "Validation Batch 8, Loss: 0.954765, Accuracy: 83.79%\n",
      "Validation Batch 9, Loss: 0.960764, Accuracy: 82.99%\n",
      "Validation Batch 10, Loss: 0.949109, Accuracy: 82.66%\n",
      "Validation Batch 11, Loss: 0.890189, Accuracy: 82.81%\n",
      "Validation Batch 12, Loss: 0.863442, Accuracy: 83.20%\n",
      "Validation Batch 13, Loss: 0.939248, Accuracy: 82.81%\n",
      "Validation Batch 14, Loss: 0.923372, Accuracy: 82.70%\n",
      "Validation Batch 15, Loss: 0.923937, Accuracy: 82.60%\n",
      "Validation Batch 16, Loss: 0.887529, Accuracy: 82.81%\n",
      "Validation Batch 17, Loss: 0.944240, Accuracy: 82.63%\n",
      "Validation Batch 18, Loss: 0.893045, Accuracy: 82.81%\n",
      "Validation Batch 19, Loss: 0.940832, Accuracy: 82.73%\n",
      "Validation Batch 20, Loss: 0.856264, Accuracy: 82.89%\n",
      "Validation Batch 21, Loss: 0.931400, Accuracy: 82.81%\n",
      "Validation Batch 22, Loss: 0.911584, Accuracy: 82.95%\n",
      "Validation Batch 23, Loss: 0.940783, Accuracy: 82.95%\n",
      "Validation Batch 24, Loss: 0.967836, Accuracy: 82.75%\n",
      "Validation Batch 25, Loss: 0.900721, Accuracy: 82.75%\n",
      "Validation Batch 26, Loss: 0.914233, Accuracy: 82.75%\n",
      "Validation Batch 27, Loss: 0.834370, Accuracy: 82.97%\n",
      "Validation - Epoch 113, Loss: 0.912250, Accuracy: 82.97%\n",
      "Patienceâ€”0\n",
      "Epoch 114\n",
      "Batch 1, Loss: 0.992192, Accuracy: 73.44%\n",
      "Batch 2, Loss: 0.987110, Accuracy: 73.44%\n",
      "Batch 3, Loss: 0.927112, Accuracy: 76.04%\n",
      "Batch 4, Loss: 0.946635, Accuracy: 77.34%\n",
      "Batch 5, Loss: 0.909389, Accuracy: 78.44%\n",
      "Batch 6, Loss: 0.849793, Accuracy: 80.47%\n",
      "Batch 7, Loss: 0.971449, Accuracy: 79.91%\n",
      "Batch 8, Loss: 0.936066, Accuracy: 79.88%\n",
      "Batch 9, Loss: 0.935410, Accuracy: 80.03%\n",
      "Batch 10, Loss: 0.918651, Accuracy: 80.31%\n",
      "Batch 11, Loss: 0.871651, Accuracy: 81.11%\n",
      "Batch 12, Loss: 0.960303, Accuracy: 80.73%\n",
      "Batch 13, Loss: 0.966935, Accuracy: 80.53%\n",
      "Batch 14, Loss: 0.899662, Accuracy: 80.92%\n",
      "Batch 15, Loss: 0.929289, Accuracy: 81.04%\n",
      "Batch 16, Loss: 0.903711, Accuracy: 81.25%\n",
      "Batch 17, Loss: 0.911326, Accuracy: 81.43%\n",
      "Batch 18, Loss: 0.898280, Accuracy: 81.60%\n",
      "Batch 19, Loss: 1.022267, Accuracy: 81.17%\n",
      "Batch 20, Loss: 0.909608, Accuracy: 81.33%\n",
      "Batch 21, Loss: 1.047386, Accuracy: 80.80%\n",
      "Batch 22, Loss: 0.973825, Accuracy: 80.61%\n",
      "Batch 23, Loss: 0.926882, Accuracy: 80.64%\n",
      "Batch 24, Loss: 0.950481, Accuracy: 80.53%\n",
      "Batch 25, Loss: 0.895388, Accuracy: 80.75%\n",
      "Batch 26, Loss: 0.917843, Accuracy: 80.95%\n",
      "Batch 27, Loss: 0.897677, Accuracy: 81.08%\n",
      "Batch 28, Loss: 0.938140, Accuracy: 81.03%\n",
      "Batch 29, Loss: 1.004851, Accuracy: 80.71%\n",
      "Batch 30, Loss: 0.963403, Accuracy: 80.62%\n",
      "Batch 31, Loss: 0.962423, Accuracy: 80.49%\n",
      "Batch 32, Loss: 0.976040, Accuracy: 80.37%\n",
      "Batch 33, Loss: 0.952606, Accuracy: 80.30%\n",
      "Batch 34, Loss: 0.907447, Accuracy: 80.38%\n",
      "Batch 35, Loss: 0.917662, Accuracy: 80.45%\n",
      "Batch 36, Loss: 1.032352, Accuracy: 80.16%\n",
      "Batch 37, Loss: 0.946030, Accuracy: 80.11%\n",
      "Batch 38, Loss: 0.950515, Accuracy: 80.10%\n",
      "Batch 39, Loss: 1.008570, Accuracy: 79.93%\n",
      "Batch 40, Loss: 0.846679, Accuracy: 80.20%\n",
      "Batch 41, Loss: 0.995368, Accuracy: 80.07%\n",
      "Batch 42, Loss: 0.892140, Accuracy: 80.17%\n",
      "Batch 43, Loss: 0.995011, Accuracy: 80.05%\n",
      "Batch 44, Loss: 0.891758, Accuracy: 80.15%\n",
      "Batch 45, Loss: 0.868929, Accuracy: 80.31%\n",
      "Batch 46, Loss: 0.897010, Accuracy: 80.40%\n",
      "Batch 47, Loss: 0.937450, Accuracy: 80.39%\n",
      "Batch 48, Loss: 0.953588, Accuracy: 80.37%\n",
      "Batch 49, Loss: 0.897394, Accuracy: 80.42%\n",
      "Batch 50, Loss: 0.935915, Accuracy: 80.44%\n",
      "Batch 51, Loss: 0.902770, Accuracy: 80.51%\n",
      "Batch 52, Loss: 0.888417, Accuracy: 80.62%\n",
      "Batch 53, Loss: 0.953685, Accuracy: 80.63%\n",
      "Batch 54, Loss: 0.928106, Accuracy: 80.67%\n",
      "Batch 55, Loss: 0.981432, Accuracy: 80.62%\n",
      "Batch 56, Loss: 0.919827, Accuracy: 80.64%\n",
      "Batch 57, Loss: 1.045006, Accuracy: 80.46%\n",
      "Batch 58, Loss: 0.962886, Accuracy: 80.39%\n",
      "Batch 59, Loss: 1.005741, Accuracy: 80.30%\n",
      "Batch 60, Loss: 0.883837, Accuracy: 80.39%\n",
      "Batch 61, Loss: 0.904279, Accuracy: 80.48%\n",
      "Batch 62, Loss: 0.976967, Accuracy: 80.42%\n",
      "Batch 63, Loss: 0.905224, Accuracy: 80.48%\n",
      "Batch 64, Loss: 1.045044, Accuracy: 80.27%\n",
      "Batch 65, Loss: 0.902069, Accuracy: 80.31%\n",
      "Batch 66, Loss: 0.966217, Accuracy: 80.30%\n",
      "Batch 67, Loss: 1.050326, Accuracy: 80.13%\n",
      "Batch 68, Loss: 0.888844, Accuracy: 80.22%\n",
      "Batch 69, Loss: 0.911831, Accuracy: 80.28%\n",
      "Batch 70, Loss: 0.852001, Accuracy: 80.40%\n",
      "Batch 71, Loss: 0.927953, Accuracy: 80.44%\n",
      "Batch 72, Loss: 0.967136, Accuracy: 80.38%\n",
      "Batch 73, Loss: 0.923792, Accuracy: 80.39%\n",
      "Batch 74, Loss: 0.878937, Accuracy: 80.49%\n",
      "Batch 75, Loss: 0.925764, Accuracy: 80.52%\n",
      "Batch 76, Loss: 0.975682, Accuracy: 80.47%\n",
      "Batch 77, Loss: 0.907885, Accuracy: 80.50%\n",
      "Batch 78, Loss: 1.006690, Accuracy: 80.43%\n",
      "Batch 79, Loss: 1.024311, Accuracy: 80.32%\n",
      "Batch 80, Loss: 0.851695, Accuracy: 80.43%\n",
      "Batch 81, Loss: 0.925628, Accuracy: 80.46%\n",
      "Batch 82, Loss: 0.986600, Accuracy: 80.41%\n",
      "Batch 83, Loss: 0.955475, Accuracy: 80.40%\n",
      "Batch 84, Loss: 0.956675, Accuracy: 80.39%\n",
      "Batch 85, Loss: 0.918999, Accuracy: 80.42%\n",
      "Batch 86, Loss: 0.962514, Accuracy: 80.40%\n",
      "Batch 87, Loss: 0.956207, Accuracy: 80.39%\n",
      "Batch 88, Loss: 0.879906, Accuracy: 80.47%\n",
      "Batch 89, Loss: 0.971027, Accuracy: 80.42%\n",
      "Batch 90, Loss: 0.915968, Accuracy: 80.45%\n",
      "Batch 91, Loss: 0.918327, Accuracy: 80.46%\n",
      "Batch 92, Loss: 0.874554, Accuracy: 80.52%\n",
      "Batch 93, Loss: 0.889373, Accuracy: 80.58%\n",
      "Batch 94, Loss: 1.014245, Accuracy: 80.50%\n",
      "Batch 95, Loss: 0.941255, Accuracy: 80.49%\n",
      "Batch 96, Loss: 0.960505, Accuracy: 80.47%\n",
      "Batch 97, Loss: 0.837369, Accuracy: 80.61%\n",
      "Batch 98, Loss: 0.955489, Accuracy: 80.58%\n",
      "Batch 99, Loss: 0.979934, Accuracy: 80.54%\n",
      "Batch 100, Loss: 0.905780, Accuracy: 80.58%\n",
      "Batch 101, Loss: 0.949487, Accuracy: 80.55%\n",
      "Batch 102, Loss: 0.863938, Accuracy: 80.62%\n",
      "Batch 103, Loss: 0.974579, Accuracy: 80.58%\n",
      "Batch 104, Loss: 0.978536, Accuracy: 80.54%\n",
      "Batch 105, Loss: 1.001515, Accuracy: 80.46%\n",
      "Batch 106, Loss: 0.891512, Accuracy: 80.50%\n",
      "Batch 107, Loss: 0.920657, Accuracy: 80.52%\n",
      "Batch 108, Loss: 0.936232, Accuracy: 80.53%\n",
      "Batch 109, Loss: 0.891608, Accuracy: 80.58%\n",
      "Batch 110, Loss: 0.960046, Accuracy: 80.55%\n",
      "Batch 111, Loss: 0.905509, Accuracy: 80.57%\n",
      "Batch 112, Loss: 0.986373, Accuracy: 80.51%\n",
      "Batch 113, Loss: 1.018421, Accuracy: 80.42%\n",
      "Batch 114, Loss: 0.961023, Accuracy: 80.39%\n",
      "Batch 115, Loss: 1.005015, Accuracy: 80.33%\n",
      "Batch 116, Loss: 0.968158, Accuracy: 80.29%\n",
      "Batch 117, Loss: 0.880779, Accuracy: 80.34%\n",
      "Batch 118, Loss: 0.946981, Accuracy: 80.31%\n",
      "Batch 119, Loss: 0.928042, Accuracy: 80.33%\n",
      "Batch 120, Loss: 0.944866, Accuracy: 80.31%\n",
      "Batch 121, Loss: 0.933972, Accuracy: 80.32%\n",
      "Batch 122, Loss: 0.938467, Accuracy: 80.32%\n",
      "Batch 123, Loss: 0.893211, Accuracy: 80.34%\n",
      "Batch 124, Loss: 0.896377, Accuracy: 80.36%\n",
      "Batch 125, Loss: 0.984898, Accuracy: 80.33%\n",
      "Batch 126, Loss: 0.925906, Accuracy: 80.33%\n",
      "Batch 127, Loss: 0.858727, Accuracy: 80.40%\n",
      "Batch 128, Loss: 0.920170, Accuracy: 80.43%\n",
      "Batch 129, Loss: 0.969520, Accuracy: 80.41%\n",
      "Batch 130, Loss: 0.882760, Accuracy: 80.46%\n",
      "Batch 131, Loss: 0.989689, Accuracy: 80.43%\n",
      "Batch 132, Loss: 1.043721, Accuracy: 80.35%\n",
      "Batch 133, Loss: 0.960965, Accuracy: 80.32%\n",
      "Batch 134, Loss: 0.885836, Accuracy: 80.36%\n",
      "Batch 135, Loss: 1.002076, Accuracy: 80.30%\n",
      "Batch 136, Loss: 0.935260, Accuracy: 80.30%\n",
      "Batch 137, Loss: 0.865140, Accuracy: 80.36%\n",
      "Batch 138, Loss: 1.012212, Accuracy: 80.31%\n",
      "Batch 139, Loss: 0.910089, Accuracy: 80.33%\n",
      "Batch 140, Loss: 0.857719, Accuracy: 80.39%\n",
      "Batch 141, Loss: 1.002566, Accuracy: 80.35%\n",
      "Batch 142, Loss: 0.942287, Accuracy: 80.36%\n",
      "Batch 143, Loss: 0.947013, Accuracy: 80.34%\n",
      "Batch 144, Loss: 1.047501, Accuracy: 80.26%\n",
      "Batch 145, Loss: 0.896069, Accuracy: 80.28%\n",
      "Batch 146, Loss: 0.937917, Accuracy: 80.28%\n",
      "Batch 147, Loss: 0.888650, Accuracy: 80.31%\n",
      "Batch 148, Loss: 0.935827, Accuracy: 80.31%\n",
      "Batch 149, Loss: 1.030015, Accuracy: 80.25%\n",
      "Batch 150, Loss: 0.931894, Accuracy: 80.27%\n",
      "Batch 151, Loss: 0.900322, Accuracy: 80.29%\n",
      "Batch 152, Loss: 0.957641, Accuracy: 80.27%\n",
      "Batch 153, Loss: 1.007836, Accuracy: 80.24%\n",
      "Batch 154, Loss: 0.925509, Accuracy: 80.26%\n",
      "Batch 155, Loss: 0.923507, Accuracy: 80.26%\n",
      "Batch 156, Loss: 1.016642, Accuracy: 80.20%\n",
      "Batch 157, Loss: 0.975247, Accuracy: 80.19%\n",
      "Batch 158, Loss: 1.007008, Accuracy: 80.13%\n",
      "Batch 159, Loss: 0.955674, Accuracy: 80.13%\n",
      "Batch 160, Loss: 0.951775, Accuracy: 80.12%\n",
      "Batch 161, Loss: 0.947649, Accuracy: 80.11%\n",
      "Batch 162, Loss: 0.999016, Accuracy: 80.07%\n",
      "Batch 163, Loss: 0.914877, Accuracy: 80.09%\n",
      "Batch 164, Loss: 0.996798, Accuracy: 80.05%\n",
      "Batch 165, Loss: 0.970041, Accuracy: 80.02%\n",
      "Batch 166, Loss: 0.987372, Accuracy: 79.99%\n",
      "Batch 167, Loss: 1.056840, Accuracy: 79.92%\n",
      "Batch 168, Loss: 0.995596, Accuracy: 79.89%\n",
      "Batch 169, Loss: 0.928260, Accuracy: 79.90%\n",
      "Batch 170, Loss: 0.856782, Accuracy: 79.95%\n",
      "Batch 171, Loss: 0.826864, Accuracy: 80.03%\n",
      "Batch 172, Loss: 0.943056, Accuracy: 80.01%\n",
      "Batch 173, Loss: 0.992412, Accuracy: 79.99%\n",
      "Batch 174, Loss: 0.937266, Accuracy: 79.98%\n",
      "Batch 175, Loss: 0.865792, Accuracy: 80.03%\n",
      "Batch 176, Loss: 0.914805, Accuracy: 80.04%\n",
      "Batch 177, Loss: 0.926756, Accuracy: 80.05%\n",
      "Batch 178, Loss: 0.993008, Accuracy: 80.02%\n",
      "Batch 179, Loss: 0.823331, Accuracy: 80.09%\n",
      "Batch 180, Loss: 0.853551, Accuracy: 80.14%\n",
      "Batch 181, Loss: 0.980345, Accuracy: 80.11%\n",
      "Batch 182, Loss: 0.938586, Accuracy: 80.11%\n",
      "Batch 183, Loss: 0.943594, Accuracy: 80.11%\n",
      "Batch 184, Loss: 0.957179, Accuracy: 80.10%\n",
      "Batch 185, Loss: 0.983860, Accuracy: 80.08%\n",
      "Batch 186, Loss: 0.858264, Accuracy: 80.13%\n",
      "Batch 187, Loss: 0.918940, Accuracy: 80.15%\n",
      "Batch 188, Loss: 0.942363, Accuracy: 80.14%\n",
      "Batch 189, Loss: 0.968591, Accuracy: 80.14%\n",
      "Batch 190, Loss: 0.870146, Accuracy: 80.19%\n",
      "Batch 191, Loss: 1.000857, Accuracy: 80.15%\n",
      "Batch 192, Loss: 0.927264, Accuracy: 80.16%\n",
      "Batch 193, Loss: 0.852446, Accuracy: 80.21%\n",
      "Batch 194, Loss: 0.894084, Accuracy: 80.24%\n",
      "Batch 195, Loss: 0.955572, Accuracy: 80.23%\n",
      "Batch 196, Loss: 0.942237, Accuracy: 80.23%\n",
      "Batch 197, Loss: 0.884580, Accuracy: 80.26%\n",
      "Batch 198, Loss: 0.966495, Accuracy: 80.25%\n",
      "Batch 199, Loss: 0.946823, Accuracy: 80.24%\n",
      "Batch 200, Loss: 0.951163, Accuracy: 80.23%\n",
      "Batch 201, Loss: 1.060500, Accuracy: 80.19%\n",
      "Batch 202, Loss: 0.938335, Accuracy: 80.20%\n",
      "Batch 203, Loss: 0.943558, Accuracy: 80.20%\n",
      "Batch 204, Loss: 0.847821, Accuracy: 80.25%\n",
      "Batch 205, Loss: 0.868889, Accuracy: 80.27%\n",
      "Batch 206, Loss: 0.906325, Accuracy: 80.29%\n",
      "Batch 207, Loss: 0.908736, Accuracy: 80.31%\n",
      "Batch 208, Loss: 0.903172, Accuracy: 80.33%\n",
      "Batch 209, Loss: 0.937428, Accuracy: 80.33%\n",
      "Batch 210, Loss: 0.923234, Accuracy: 80.33%\n",
      "Batch 211, Loss: 0.995462, Accuracy: 80.31%\n",
      "Batch 212, Loss: 0.844487, Accuracy: 80.35%\n",
      "Batch 213, Loss: 0.877462, Accuracy: 80.38%\n",
      "Training - Epoch 114, Loss: 0.939096, Accuracy: 80.38%\n",
      "Validation Batch 1, Loss: 0.869478, Accuracy: 89.06%\n",
      "Validation Batch 2, Loss: 0.908093, Accuracy: 86.72%\n",
      "Validation Batch 3, Loss: 0.981272, Accuracy: 83.33%\n",
      "Validation Batch 4, Loss: 0.909874, Accuracy: 83.59%\n",
      "Validation Batch 5, Loss: 0.861267, Accuracy: 84.38%\n",
      "Validation Batch 6, Loss: 0.855733, Accuracy: 85.16%\n",
      "Validation Batch 7, Loss: 0.928080, Accuracy: 84.60%\n",
      "Validation Batch 8, Loss: 0.966044, Accuracy: 83.59%\n",
      "Validation Batch 9, Loss: 0.966281, Accuracy: 82.64%\n",
      "Validation Batch 10, Loss: 0.946010, Accuracy: 82.50%\n",
      "Validation Batch 11, Loss: 0.891338, Accuracy: 82.81%\n",
      "Validation Batch 12, Loss: 0.862644, Accuracy: 83.20%\n",
      "Validation Batch 13, Loss: 0.943502, Accuracy: 82.69%\n",
      "Validation Batch 14, Loss: 0.927276, Accuracy: 82.59%\n",
      "Validation Batch 15, Loss: 0.918306, Accuracy: 82.60%\n",
      "Validation Batch 16, Loss: 0.888930, Accuracy: 82.81%\n",
      "Validation Batch 17, Loss: 0.956022, Accuracy: 82.54%\n",
      "Validation Batch 18, Loss: 0.895107, Accuracy: 82.73%\n",
      "Validation Batch 19, Loss: 0.945178, Accuracy: 82.57%\n",
      "Validation Batch 20, Loss: 0.836023, Accuracy: 83.05%\n",
      "Validation Batch 21, Loss: 0.924037, Accuracy: 82.89%\n",
      "Validation Batch 22, Loss: 0.914681, Accuracy: 83.03%\n",
      "Validation Batch 23, Loss: 0.938696, Accuracy: 82.88%\n",
      "Validation Batch 24, Loss: 0.971578, Accuracy: 82.68%\n",
      "Validation Batch 25, Loss: 0.905626, Accuracy: 82.75%\n",
      "Validation Batch 26, Loss: 0.910898, Accuracy: 82.75%\n",
      "Validation Batch 27, Loss: 0.850001, Accuracy: 82.91%\n",
      "Validation - Epoch 114, Loss: 0.913777, Accuracy: 82.91%\n",
      "Patienceâ€”1\n",
      "Epoch 115\n",
      "Batch 1, Loss: 0.978015, Accuracy: 76.56%\n",
      "Batch 2, Loss: 0.905263, Accuracy: 80.47%\n",
      "Batch 3, Loss: 0.928380, Accuracy: 81.25%\n",
      "Batch 4, Loss: 0.904634, Accuracy: 81.64%\n",
      "Batch 5, Loss: 0.945277, Accuracy: 81.25%\n",
      "Batch 6, Loss: 0.888158, Accuracy: 82.03%\n",
      "Batch 7, Loss: 1.024820, Accuracy: 80.36%\n",
      "Batch 8, Loss: 0.950283, Accuracy: 80.47%\n",
      "Batch 9, Loss: 0.904204, Accuracy: 80.90%\n",
      "Batch 10, Loss: 0.983535, Accuracy: 80.47%\n",
      "Batch 11, Loss: 1.031382, Accuracy: 79.40%\n",
      "Batch 12, Loss: 0.863648, Accuracy: 80.21%\n",
      "Batch 13, Loss: 0.950097, Accuracy: 80.17%\n",
      "Batch 14, Loss: 0.959448, Accuracy: 80.02%\n",
      "Batch 15, Loss: 0.993295, Accuracy: 79.69%\n",
      "Batch 16, Loss: 0.991884, Accuracy: 79.49%\n",
      "Batch 17, Loss: 0.938842, Accuracy: 79.50%\n",
      "Batch 18, Loss: 1.013198, Accuracy: 79.17%\n",
      "Batch 19, Loss: 0.941577, Accuracy: 79.28%\n",
      "Batch 20, Loss: 0.892978, Accuracy: 79.61%\n",
      "Batch 21, Loss: 0.856413, Accuracy: 80.06%\n",
      "Batch 22, Loss: 0.875485, Accuracy: 80.47%\n",
      "Batch 23, Loss: 0.901075, Accuracy: 80.64%\n",
      "Batch 24, Loss: 0.927968, Accuracy: 80.60%\n",
      "Batch 25, Loss: 0.975587, Accuracy: 80.31%\n",
      "Batch 26, Loss: 0.972530, Accuracy: 80.17%\n",
      "Batch 27, Loss: 1.014723, Accuracy: 79.92%\n",
      "Batch 28, Loss: 0.931011, Accuracy: 79.97%\n",
      "Batch 29, Loss: 0.986998, Accuracy: 79.85%\n",
      "Batch 30, Loss: 0.996085, Accuracy: 79.69%\n",
      "Batch 31, Loss: 0.938831, Accuracy: 79.69%\n",
      "Batch 32, Loss: 0.920480, Accuracy: 79.79%\n",
      "Batch 33, Loss: 1.005729, Accuracy: 79.55%\n",
      "Batch 34, Loss: 0.930361, Accuracy: 79.60%\n",
      "Batch 35, Loss: 0.950561, Accuracy: 79.55%\n",
      "Batch 36, Loss: 0.939736, Accuracy: 79.56%\n",
      "Batch 37, Loss: 0.850313, Accuracy: 79.81%\n",
      "Batch 38, Loss: 0.931368, Accuracy: 79.89%\n",
      "Batch 39, Loss: 0.873938, Accuracy: 80.09%\n",
      "Batch 40, Loss: 0.853930, Accuracy: 80.35%\n",
      "Batch 41, Loss: 0.926096, Accuracy: 80.41%\n",
      "Batch 42, Loss: 0.967219, Accuracy: 80.32%\n",
      "Batch 43, Loss: 1.067323, Accuracy: 80.01%\n",
      "Batch 44, Loss: 0.982307, Accuracy: 79.90%\n",
      "Batch 45, Loss: 0.955122, Accuracy: 79.86%\n",
      "Batch 46, Loss: 0.815251, Accuracy: 80.13%\n",
      "Batch 47, Loss: 0.969294, Accuracy: 80.05%\n",
      "Batch 48, Loss: 0.948622, Accuracy: 80.05%\n",
      "Batch 49, Loss: 1.022203, Accuracy: 79.88%\n",
      "Batch 50, Loss: 0.984356, Accuracy: 79.78%\n",
      "Batch 51, Loss: 1.025063, Accuracy: 79.63%\n",
      "Batch 52, Loss: 0.934323, Accuracy: 79.60%\n",
      "Batch 53, Loss: 0.972457, Accuracy: 79.57%\n",
      "Batch 54, Loss: 0.910458, Accuracy: 79.63%\n",
      "Batch 55, Loss: 0.964430, Accuracy: 79.60%\n",
      "Batch 56, Loss: 0.986074, Accuracy: 79.55%\n",
      "Batch 57, Loss: 0.949724, Accuracy: 79.55%\n",
      "Batch 58, Loss: 0.969429, Accuracy: 79.53%\n",
      "Batch 59, Loss: 0.945017, Accuracy: 79.53%\n",
      "Batch 60, Loss: 0.902898, Accuracy: 79.64%\n",
      "Batch 61, Loss: 0.857284, Accuracy: 79.79%\n",
      "Batch 62, Loss: 0.921363, Accuracy: 79.84%\n",
      "Batch 63, Loss: 0.924976, Accuracy: 79.89%\n",
      "Batch 64, Loss: 0.929206, Accuracy: 79.93%\n",
      "Batch 65, Loss: 0.947036, Accuracy: 79.90%\n",
      "Batch 66, Loss: 0.998556, Accuracy: 79.81%\n",
      "Batch 67, Loss: 0.886151, Accuracy: 79.87%\n",
      "Batch 68, Loss: 0.905690, Accuracy: 79.94%\n",
      "Batch 69, Loss: 0.983689, Accuracy: 79.89%\n",
      "Batch 70, Loss: 0.984262, Accuracy: 79.82%\n",
      "Batch 71, Loss: 0.996928, Accuracy: 79.75%\n",
      "Batch 72, Loss: 0.845303, Accuracy: 79.88%\n",
      "Batch 73, Loss: 0.891766, Accuracy: 79.94%\n",
      "Batch 74, Loss: 0.967464, Accuracy: 79.92%\n",
      "Batch 75, Loss: 0.895490, Accuracy: 80.00%\n",
      "Batch 76, Loss: 0.910031, Accuracy: 80.06%\n",
      "Batch 77, Loss: 0.957461, Accuracy: 80.03%\n",
      "Batch 78, Loss: 0.959334, Accuracy: 80.01%\n",
      "Batch 79, Loss: 0.869601, Accuracy: 80.10%\n",
      "Batch 80, Loss: 0.941939, Accuracy: 80.08%\n",
      "Batch 81, Loss: 0.886800, Accuracy: 80.17%\n",
      "Batch 82, Loss: 0.932337, Accuracy: 80.16%\n",
      "Batch 83, Loss: 0.919747, Accuracy: 80.20%\n",
      "Batch 84, Loss: 0.941150, Accuracy: 80.19%\n",
      "Batch 85, Loss: 0.907912, Accuracy: 80.20%\n",
      "Batch 86, Loss: 0.894446, Accuracy: 80.27%\n",
      "Batch 87, Loss: 0.947747, Accuracy: 80.26%\n",
      "Batch 88, Loss: 0.935951, Accuracy: 80.26%\n",
      "Batch 89, Loss: 0.924325, Accuracy: 80.28%\n",
      "Batch 90, Loss: 0.885271, Accuracy: 80.35%\n",
      "Batch 91, Loss: 0.913123, Accuracy: 80.39%\n",
      "Batch 92, Loss: 0.980188, Accuracy: 80.35%\n",
      "Batch 93, Loss: 0.897779, Accuracy: 80.41%\n",
      "Batch 94, Loss: 0.949506, Accuracy: 80.40%\n",
      "Batch 95, Loss: 0.902419, Accuracy: 80.43%\n",
      "Batch 96, Loss: 0.968184, Accuracy: 80.40%\n",
      "Batch 97, Loss: 1.025668, Accuracy: 80.32%\n",
      "Batch 98, Loss: 0.931370, Accuracy: 80.33%\n",
      "Batch 99, Loss: 0.920196, Accuracy: 80.35%\n",
      "Batch 100, Loss: 0.847032, Accuracy: 80.45%\n",
      "Batch 101, Loss: 1.016581, Accuracy: 80.38%\n",
      "Batch 102, Loss: 0.912586, Accuracy: 80.41%\n",
      "Batch 103, Loss: 0.985959, Accuracy: 80.35%\n",
      "Batch 104, Loss: 0.989106, Accuracy: 80.30%\n",
      "Batch 105, Loss: 0.975044, Accuracy: 80.27%\n",
      "Batch 106, Loss: 0.880196, Accuracy: 80.32%\n",
      "Batch 107, Loss: 0.920067, Accuracy: 80.33%\n",
      "Batch 108, Loss: 0.911725, Accuracy: 80.35%\n",
      "Batch 109, Loss: 0.991187, Accuracy: 80.30%\n",
      "Batch 110, Loss: 0.980708, Accuracy: 80.27%\n",
      "Batch 111, Loss: 0.913435, Accuracy: 80.29%\n",
      "Batch 112, Loss: 0.934488, Accuracy: 80.30%\n",
      "Batch 113, Loss: 1.037255, Accuracy: 80.20%\n",
      "Batch 114, Loss: 0.936002, Accuracy: 80.19%\n",
      "Batch 115, Loss: 0.921088, Accuracy: 80.22%\n",
      "Batch 116, Loss: 0.930116, Accuracy: 80.24%\n",
      "Batch 117, Loss: 0.939968, Accuracy: 80.25%\n",
      "Batch 118, Loss: 0.889744, Accuracy: 80.31%\n",
      "Batch 119, Loss: 0.928947, Accuracy: 80.33%\n",
      "Batch 120, Loss: 0.922961, Accuracy: 80.35%\n",
      "Batch 121, Loss: 0.942458, Accuracy: 80.33%\n",
      "Batch 122, Loss: 0.947160, Accuracy: 80.33%\n",
      "Batch 123, Loss: 0.919010, Accuracy: 80.35%\n",
      "Batch 124, Loss: 0.912833, Accuracy: 80.37%\n",
      "Batch 125, Loss: 0.924817, Accuracy: 80.39%\n",
      "Batch 126, Loss: 0.903578, Accuracy: 80.43%\n",
      "Batch 127, Loss: 0.943373, Accuracy: 80.44%\n",
      "Batch 128, Loss: 0.894891, Accuracy: 80.48%\n",
      "Batch 129, Loss: 0.957122, Accuracy: 80.46%\n",
      "Batch 130, Loss: 0.895034, Accuracy: 80.50%\n",
      "Batch 131, Loss: 0.909311, Accuracy: 80.52%\n",
      "Batch 132, Loss: 0.998847, Accuracy: 80.48%\n",
      "Batch 133, Loss: 0.933135, Accuracy: 80.47%\n",
      "Batch 134, Loss: 0.915203, Accuracy: 80.48%\n",
      "Batch 135, Loss: 0.859200, Accuracy: 80.53%\n",
      "Batch 136, Loss: 0.897968, Accuracy: 80.56%\n",
      "Batch 137, Loss: 0.958049, Accuracy: 80.54%\n",
      "Batch 138, Loss: 0.932390, Accuracy: 80.55%\n",
      "Batch 139, Loss: 0.925015, Accuracy: 80.55%\n",
      "Batch 140, Loss: 1.005534, Accuracy: 80.50%\n",
      "Batch 141, Loss: 0.931879, Accuracy: 80.50%\n",
      "Batch 142, Loss: 0.948326, Accuracy: 80.48%\n",
      "Batch 143, Loss: 0.956645, Accuracy: 80.47%\n",
      "Batch 144, Loss: 0.916786, Accuracy: 80.49%\n",
      "Batch 145, Loss: 0.853772, Accuracy: 80.55%\n",
      "Batch 146, Loss: 0.892121, Accuracy: 80.59%\n",
      "Batch 147, Loss: 0.951327, Accuracy: 80.58%\n",
      "Batch 148, Loss: 0.937469, Accuracy: 80.58%\n",
      "Batch 149, Loss: 0.920165, Accuracy: 80.60%\n",
      "Batch 150, Loss: 0.903370, Accuracy: 80.62%\n",
      "Batch 151, Loss: 0.932609, Accuracy: 80.63%\n",
      "Batch 152, Loss: 0.877178, Accuracy: 80.67%\n",
      "Batch 153, Loss: 0.943343, Accuracy: 80.67%\n",
      "Batch 154, Loss: 0.915770, Accuracy: 80.68%\n",
      "Batch 155, Loss: 0.956336, Accuracy: 80.67%\n",
      "Batch 156, Loss: 0.916644, Accuracy: 80.68%\n",
      "Batch 157, Loss: 0.938291, Accuracy: 80.68%\n",
      "Batch 158, Loss: 0.912052, Accuracy: 80.71%\n",
      "Batch 159, Loss: 1.036022, Accuracy: 80.64%\n",
      "Batch 160, Loss: 0.930665, Accuracy: 80.64%\n",
      "Batch 161, Loss: 0.975659, Accuracy: 80.62%\n",
      "Batch 162, Loss: 0.902766, Accuracy: 80.64%\n",
      "Batch 163, Loss: 0.924039, Accuracy: 80.64%\n",
      "Batch 164, Loss: 0.940203, Accuracy: 80.63%\n",
      "Batch 165, Loss: 0.916716, Accuracy: 80.64%\n",
      "Batch 166, Loss: 0.897598, Accuracy: 80.66%\n",
      "Batch 167, Loss: 0.916160, Accuracy: 80.67%\n",
      "Batch 168, Loss: 0.864020, Accuracy: 80.72%\n",
      "Batch 169, Loss: 0.895741, Accuracy: 80.73%\n",
      "Batch 170, Loss: 0.916027, Accuracy: 80.75%\n",
      "Batch 171, Loss: 0.912497, Accuracy: 80.77%\n",
      "Batch 172, Loss: 0.957881, Accuracy: 80.75%\n",
      "Batch 173, Loss: 0.919298, Accuracy: 80.76%\n",
      "Batch 174, Loss: 0.976927, Accuracy: 80.74%\n",
      "Batch 175, Loss: 0.992171, Accuracy: 80.71%\n",
      "Batch 176, Loss: 0.961289, Accuracy: 80.68%\n",
      "Batch 177, Loss: 0.918034, Accuracy: 80.69%\n",
      "Batch 178, Loss: 0.988782, Accuracy: 80.66%\n",
      "Batch 179, Loss: 1.005315, Accuracy: 80.62%\n",
      "Batch 180, Loss: 1.015000, Accuracy: 80.58%\n",
      "Batch 181, Loss: 0.976111, Accuracy: 80.56%\n",
      "Batch 182, Loss: 0.990946, Accuracy: 80.53%\n",
      "Batch 183, Loss: 0.871429, Accuracy: 80.57%\n",
      "Batch 184, Loss: 0.849496, Accuracy: 80.61%\n",
      "Batch 185, Loss: 0.914690, Accuracy: 80.62%\n",
      "Batch 186, Loss: 0.897583, Accuracy: 80.65%\n",
      "Batch 187, Loss: 0.945897, Accuracy: 80.65%\n",
      "Batch 188, Loss: 0.990924, Accuracy: 80.62%\n",
      "Batch 189, Loss: 0.849222, Accuracy: 80.68%\n",
      "Batch 190, Loss: 0.859640, Accuracy: 80.72%\n",
      "Batch 191, Loss: 1.000419, Accuracy: 80.69%\n",
      "Batch 192, Loss: 1.027401, Accuracy: 80.64%\n",
      "Batch 193, Loss: 0.920664, Accuracy: 80.64%\n",
      "Batch 194, Loss: 0.961768, Accuracy: 80.64%\n",
      "Batch 195, Loss: 0.942317, Accuracy: 80.64%\n",
      "Batch 196, Loss: 1.030196, Accuracy: 80.59%\n",
      "Batch 197, Loss: 0.866558, Accuracy: 80.62%\n",
      "Batch 198, Loss: 0.941737, Accuracy: 80.62%\n",
      "Batch 199, Loss: 0.995007, Accuracy: 80.59%\n",
      "Batch 200, Loss: 0.953590, Accuracy: 80.59%\n",
      "Batch 201, Loss: 0.953162, Accuracy: 80.57%\n",
      "Batch 202, Loss: 0.960208, Accuracy: 80.56%\n",
      "Batch 203, Loss: 1.012147, Accuracy: 80.53%\n",
      "Batch 204, Loss: 0.972595, Accuracy: 80.51%\n",
      "Batch 205, Loss: 1.008395, Accuracy: 80.47%\n",
      "Batch 206, Loss: 0.970816, Accuracy: 80.45%\n",
      "Batch 207, Loss: 0.921499, Accuracy: 80.46%\n",
      "Batch 208, Loss: 0.938133, Accuracy: 80.46%\n",
      "Batch 209, Loss: 0.897678, Accuracy: 80.48%\n",
      "Batch 210, Loss: 0.903441, Accuracy: 80.50%\n",
      "Batch 211, Loss: 0.884810, Accuracy: 80.52%\n",
      "Batch 212, Loss: 0.905267, Accuracy: 80.54%\n",
      "Batch 213, Loss: 0.997985, Accuracy: 80.50%\n",
      "Training - Epoch 115, Loss: 0.938381, Accuracy: 80.50%\n",
      "Validation Batch 1, Loss: 0.870745, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.896998, Accuracy: 86.72%\n",
      "Validation Batch 3, Loss: 0.989131, Accuracy: 82.81%\n",
      "Validation Batch 4, Loss: 0.923813, Accuracy: 82.42%\n",
      "Validation Batch 5, Loss: 0.887550, Accuracy: 83.12%\n",
      "Validation Batch 6, Loss: 0.841990, Accuracy: 84.38%\n",
      "Validation Batch 7, Loss: 0.932243, Accuracy: 83.93%\n",
      "Validation Batch 8, Loss: 0.949456, Accuracy: 83.20%\n",
      "Validation Batch 9, Loss: 0.949415, Accuracy: 82.64%\n",
      "Validation Batch 10, Loss: 0.953112, Accuracy: 82.34%\n",
      "Validation Batch 11, Loss: 0.891451, Accuracy: 82.53%\n",
      "Validation Batch 12, Loss: 0.858966, Accuracy: 82.94%\n",
      "Validation Batch 13, Loss: 0.941361, Accuracy: 82.81%\n",
      "Validation Batch 14, Loss: 0.929503, Accuracy: 82.70%\n",
      "Validation Batch 15, Loss: 0.920815, Accuracy: 82.71%\n",
      "Validation Batch 16, Loss: 0.893043, Accuracy: 82.91%\n",
      "Validation Batch 17, Loss: 0.944043, Accuracy: 82.72%\n",
      "Validation Batch 18, Loss: 0.897275, Accuracy: 82.90%\n",
      "Validation Batch 19, Loss: 0.945230, Accuracy: 82.81%\n",
      "Validation Batch 20, Loss: 0.871581, Accuracy: 82.97%\n",
      "Validation Batch 21, Loss: 0.937785, Accuracy: 82.81%\n",
      "Validation Batch 22, Loss: 0.929875, Accuracy: 82.67%\n",
      "Validation Batch 23, Loss: 0.969605, Accuracy: 82.40%\n",
      "Validation Batch 24, Loss: 0.978206, Accuracy: 82.10%\n",
      "Validation Batch 25, Loss: 0.914008, Accuracy: 82.06%\n",
      "Validation Batch 26, Loss: 0.919139, Accuracy: 82.09%\n",
      "Validation Batch 27, Loss: 0.856952, Accuracy: 82.21%\n",
      "Validation - Epoch 115, Loss: 0.918270, Accuracy: 82.21%\n",
      "Patienceâ€”2\n",
      "Epoch 116\n",
      "Batch 1, Loss: 0.952194, Accuracy: 79.69%\n",
      "Batch 2, Loss: 0.868599, Accuracy: 84.38%\n",
      "Batch 3, Loss: 0.939211, Accuracy: 83.33%\n",
      "Batch 4, Loss: 0.933004, Accuracy: 82.81%\n",
      "Batch 5, Loss: 0.879312, Accuracy: 83.44%\n",
      "Batch 6, Loss: 1.001614, Accuracy: 82.03%\n",
      "Batch 7, Loss: 0.959876, Accuracy: 81.70%\n",
      "Batch 8, Loss: 0.918521, Accuracy: 81.64%\n",
      "Batch 9, Loss: 0.956852, Accuracy: 81.25%\n",
      "Batch 10, Loss: 0.894974, Accuracy: 81.56%\n",
      "Batch 11, Loss: 0.919527, Accuracy: 81.53%\n",
      "Batch 12, Loss: 0.960955, Accuracy: 81.25%\n",
      "Batch 13, Loss: 0.893964, Accuracy: 81.49%\n",
      "Batch 14, Loss: 0.896557, Accuracy: 81.81%\n",
      "Batch 15, Loss: 1.039132, Accuracy: 81.04%\n",
      "Batch 16, Loss: 0.983241, Accuracy: 80.76%\n",
      "Batch 17, Loss: 0.906992, Accuracy: 80.88%\n",
      "Batch 18, Loss: 0.919319, Accuracy: 80.99%\n",
      "Batch 19, Loss: 0.937664, Accuracy: 81.00%\n",
      "Batch 20, Loss: 0.952637, Accuracy: 80.86%\n",
      "Batch 21, Loss: 1.049406, Accuracy: 80.36%\n",
      "Batch 22, Loss: 0.991879, Accuracy: 79.97%\n",
      "Batch 23, Loss: 0.974642, Accuracy: 79.89%\n",
      "Batch 24, Loss: 0.898933, Accuracy: 80.14%\n",
      "Batch 25, Loss: 0.915883, Accuracy: 80.19%\n",
      "Batch 26, Loss: 0.923108, Accuracy: 80.23%\n",
      "Batch 27, Loss: 0.900608, Accuracy: 80.38%\n",
      "Batch 28, Loss: 0.965063, Accuracy: 80.36%\n",
      "Batch 29, Loss: 1.041532, Accuracy: 80.01%\n",
      "Batch 30, Loss: 0.879585, Accuracy: 80.26%\n",
      "Batch 31, Loss: 1.010600, Accuracy: 80.04%\n",
      "Batch 32, Loss: 0.954795, Accuracy: 79.98%\n",
      "Batch 33, Loss: 0.908115, Accuracy: 80.16%\n",
      "Batch 34, Loss: 0.938502, Accuracy: 80.15%\n",
      "Batch 35, Loss: 0.956515, Accuracy: 80.09%\n",
      "Batch 36, Loss: 0.972608, Accuracy: 80.03%\n",
      "Batch 37, Loss: 0.870826, Accuracy: 80.19%\n",
      "Batch 38, Loss: 0.899823, Accuracy: 80.30%\n",
      "Batch 39, Loss: 0.929528, Accuracy: 80.33%\n",
      "Batch 40, Loss: 1.000837, Accuracy: 80.20%\n",
      "Batch 41, Loss: 1.011869, Accuracy: 79.99%\n",
      "Batch 42, Loss: 0.988708, Accuracy: 79.84%\n",
      "Batch 43, Loss: 0.946030, Accuracy: 79.83%\n",
      "Batch 44, Loss: 0.951759, Accuracy: 79.79%\n",
      "Batch 45, Loss: 0.949051, Accuracy: 79.79%\n",
      "Batch 46, Loss: 0.871942, Accuracy: 79.93%\n",
      "Batch 47, Loss: 0.983286, Accuracy: 79.79%\n",
      "Batch 48, Loss: 0.915355, Accuracy: 79.85%\n",
      "Batch 49, Loss: 1.013489, Accuracy: 79.72%\n",
      "Batch 50, Loss: 0.936349, Accuracy: 79.75%\n",
      "Batch 51, Loss: 0.966771, Accuracy: 79.72%\n",
      "Batch 52, Loss: 0.985743, Accuracy: 79.63%\n",
      "Batch 53, Loss: 0.991368, Accuracy: 79.51%\n",
      "Batch 54, Loss: 0.961382, Accuracy: 79.48%\n",
      "Batch 55, Loss: 0.908661, Accuracy: 79.55%\n",
      "Batch 56, Loss: 0.934419, Accuracy: 79.58%\n",
      "Batch 57, Loss: 0.969611, Accuracy: 79.55%\n",
      "Batch 58, Loss: 0.827339, Accuracy: 79.80%\n",
      "Batch 59, Loss: 0.914692, Accuracy: 79.87%\n",
      "Batch 60, Loss: 1.063680, Accuracy: 79.69%\n",
      "Batch 61, Loss: 0.949272, Accuracy: 79.69%\n",
      "Batch 62, Loss: 0.899224, Accuracy: 79.79%\n",
      "Batch 63, Loss: 0.900870, Accuracy: 79.86%\n",
      "Batch 64, Loss: 0.948064, Accuracy: 79.86%\n",
      "Batch 65, Loss: 0.971703, Accuracy: 79.81%\n",
      "Batch 66, Loss: 0.921775, Accuracy: 79.85%\n",
      "Batch 67, Loss: 0.985894, Accuracy: 79.78%\n",
      "Batch 68, Loss: 0.953767, Accuracy: 79.76%\n",
      "Batch 69, Loss: 0.823711, Accuracy: 79.91%\n",
      "Batch 70, Loss: 1.033464, Accuracy: 79.78%\n",
      "Batch 71, Loss: 0.904157, Accuracy: 79.82%\n",
      "Batch 72, Loss: 0.950064, Accuracy: 79.82%\n",
      "Batch 73, Loss: 0.949211, Accuracy: 79.82%\n",
      "Batch 74, Loss: 0.881122, Accuracy: 79.90%\n",
      "Batch 75, Loss: 0.947553, Accuracy: 79.90%\n",
      "Batch 76, Loss: 0.991504, Accuracy: 79.83%\n",
      "Batch 77, Loss: 0.966396, Accuracy: 79.79%\n",
      "Batch 78, Loss: 0.902657, Accuracy: 79.85%\n",
      "Batch 79, Loss: 0.916231, Accuracy: 79.89%\n",
      "Batch 80, Loss: 0.919713, Accuracy: 79.94%\n",
      "Batch 81, Loss: 0.941346, Accuracy: 79.94%\n",
      "Batch 82, Loss: 0.926345, Accuracy: 79.95%\n",
      "Batch 83, Loss: 0.970976, Accuracy: 79.91%\n",
      "Batch 84, Loss: 0.896653, Accuracy: 79.97%\n",
      "Batch 85, Loss: 0.947602, Accuracy: 79.94%\n",
      "Batch 86, Loss: 0.899518, Accuracy: 80.01%\n",
      "Batch 87, Loss: 0.993593, Accuracy: 79.96%\n",
      "Batch 88, Loss: 0.989899, Accuracy: 79.92%\n",
      "Batch 89, Loss: 0.950909, Accuracy: 79.92%\n",
      "Batch 90, Loss: 0.886610, Accuracy: 79.97%\n",
      "Batch 91, Loss: 0.996486, Accuracy: 79.89%\n",
      "Batch 92, Loss: 0.959025, Accuracy: 79.87%\n",
      "Batch 93, Loss: 0.928301, Accuracy: 79.89%\n",
      "Batch 94, Loss: 0.925906, Accuracy: 79.92%\n",
      "Batch 95, Loss: 0.874864, Accuracy: 80.00%\n",
      "Batch 96, Loss: 0.939526, Accuracy: 79.98%\n",
      "Batch 97, Loss: 0.985294, Accuracy: 79.93%\n",
      "Batch 98, Loss: 0.926589, Accuracy: 79.96%\n",
      "Batch 99, Loss: 0.934758, Accuracy: 79.97%\n",
      "Batch 100, Loss: 0.970600, Accuracy: 79.91%\n",
      "Batch 101, Loss: 0.976092, Accuracy: 79.86%\n",
      "Batch 102, Loss: 0.842715, Accuracy: 79.96%\n",
      "Batch 103, Loss: 0.927214, Accuracy: 79.98%\n",
      "Batch 104, Loss: 0.882633, Accuracy: 80.03%\n",
      "Batch 105, Loss: 0.946054, Accuracy: 80.03%\n",
      "Batch 106, Loss: 0.913432, Accuracy: 80.07%\n",
      "Batch 107, Loss: 0.886687, Accuracy: 80.13%\n",
      "Batch 108, Loss: 0.890487, Accuracy: 80.18%\n",
      "Batch 109, Loss: 0.946148, Accuracy: 80.17%\n",
      "Batch 110, Loss: 0.988096, Accuracy: 80.13%\n",
      "Batch 111, Loss: 0.902453, Accuracy: 80.17%\n",
      "Batch 112, Loss: 0.963888, Accuracy: 80.16%\n",
      "Batch 113, Loss: 0.885275, Accuracy: 80.23%\n",
      "Batch 114, Loss: 0.914280, Accuracy: 80.26%\n",
      "Batch 115, Loss: 0.951044, Accuracy: 80.26%\n",
      "Batch 116, Loss: 0.999891, Accuracy: 80.20%\n",
      "Batch 117, Loss: 0.897911, Accuracy: 80.24%\n",
      "Batch 118, Loss: 0.914709, Accuracy: 80.27%\n",
      "Batch 119, Loss: 0.930317, Accuracy: 80.28%\n",
      "Batch 120, Loss: 0.973121, Accuracy: 80.26%\n",
      "Batch 121, Loss: 0.922607, Accuracy: 80.28%\n",
      "Batch 122, Loss: 0.909439, Accuracy: 80.32%\n",
      "Batch 123, Loss: 0.919559, Accuracy: 80.35%\n",
      "Batch 124, Loss: 0.874679, Accuracy: 80.39%\n",
      "Batch 125, Loss: 0.939510, Accuracy: 80.39%\n",
      "Batch 126, Loss: 0.960489, Accuracy: 80.37%\n",
      "Batch 127, Loss: 0.926095, Accuracy: 80.38%\n",
      "Batch 128, Loss: 0.948498, Accuracy: 80.36%\n",
      "Batch 129, Loss: 0.918435, Accuracy: 80.38%\n",
      "Batch 130, Loss: 1.028200, Accuracy: 80.30%\n",
      "Batch 131, Loss: 0.942800, Accuracy: 80.28%\n",
      "Batch 132, Loss: 0.897583, Accuracy: 80.34%\n",
      "Batch 133, Loss: 0.884021, Accuracy: 80.39%\n",
      "Batch 134, Loss: 0.976757, Accuracy: 80.38%\n",
      "Batch 135, Loss: 0.873489, Accuracy: 80.43%\n",
      "Batch 136, Loss: 0.957433, Accuracy: 80.40%\n",
      "Batch 137, Loss: 0.992902, Accuracy: 80.36%\n",
      "Batch 138, Loss: 0.907459, Accuracy: 80.37%\n",
      "Batch 139, Loss: 1.005453, Accuracy: 80.32%\n",
      "Batch 140, Loss: 0.879979, Accuracy: 80.37%\n",
      "Batch 141, Loss: 0.937139, Accuracy: 80.37%\n",
      "Batch 142, Loss: 0.969742, Accuracy: 80.35%\n",
      "Batch 143, Loss: 0.954193, Accuracy: 80.33%\n",
      "Batch 144, Loss: 0.959499, Accuracy: 80.32%\n",
      "Batch 145, Loss: 0.923689, Accuracy: 80.32%\n",
      "Batch 146, Loss: 0.994776, Accuracy: 80.29%\n",
      "Batch 147, Loss: 0.940596, Accuracy: 80.28%\n",
      "Batch 148, Loss: 0.872245, Accuracy: 80.33%\n",
      "Batch 149, Loss: 1.000564, Accuracy: 80.29%\n",
      "Batch 150, Loss: 0.950295, Accuracy: 80.27%\n",
      "Batch 151, Loss: 1.011960, Accuracy: 80.22%\n",
      "Batch 152, Loss: 0.919625, Accuracy: 80.23%\n",
      "Batch 153, Loss: 0.908578, Accuracy: 80.26%\n",
      "Batch 154, Loss: 0.988653, Accuracy: 80.23%\n",
      "Batch 155, Loss: 0.914187, Accuracy: 80.25%\n",
      "Batch 156, Loss: 0.956160, Accuracy: 80.23%\n",
      "Batch 157, Loss: 0.961518, Accuracy: 80.21%\n",
      "Batch 158, Loss: 0.956810, Accuracy: 80.21%\n",
      "Batch 159, Loss: 0.932638, Accuracy: 80.22%\n",
      "Batch 160, Loss: 0.985577, Accuracy: 80.20%\n",
      "Batch 161, Loss: 0.913789, Accuracy: 80.22%\n",
      "Batch 162, Loss: 0.913878, Accuracy: 80.25%\n",
      "Batch 163, Loss: 0.889349, Accuracy: 80.28%\n",
      "Batch 164, Loss: 1.029994, Accuracy: 80.22%\n",
      "Batch 165, Loss: 0.900882, Accuracy: 80.25%\n",
      "Batch 166, Loss: 0.892750, Accuracy: 80.28%\n",
      "Batch 167, Loss: 0.935884, Accuracy: 80.28%\n",
      "Batch 168, Loss: 0.937711, Accuracy: 80.28%\n",
      "Batch 169, Loss: 1.016162, Accuracy: 80.22%\n",
      "Batch 170, Loss: 0.975081, Accuracy: 80.20%\n",
      "Batch 171, Loss: 0.873953, Accuracy: 80.24%\n",
      "Batch 172, Loss: 0.946349, Accuracy: 80.23%\n",
      "Batch 173, Loss: 0.934487, Accuracy: 80.23%\n",
      "Batch 174, Loss: 0.896380, Accuracy: 80.25%\n",
      "Batch 175, Loss: 0.939460, Accuracy: 80.25%\n",
      "Batch 176, Loss: 0.909290, Accuracy: 80.26%\n",
      "Batch 177, Loss: 0.887660, Accuracy: 80.31%\n",
      "Batch 178, Loss: 0.910866, Accuracy: 80.33%\n",
      "Batch 179, Loss: 0.967871, Accuracy: 80.32%\n",
      "Batch 180, Loss: 0.922317, Accuracy: 80.32%\n",
      "Batch 181, Loss: 0.925880, Accuracy: 80.33%\n",
      "Batch 182, Loss: 0.920418, Accuracy: 80.34%\n",
      "Batch 183, Loss: 0.897914, Accuracy: 80.36%\n",
      "Batch 184, Loss: 0.925797, Accuracy: 80.38%\n",
      "Batch 185, Loss: 0.910997, Accuracy: 80.40%\n",
      "Batch 186, Loss: 0.984131, Accuracy: 80.37%\n",
      "Batch 187, Loss: 0.970167, Accuracy: 80.35%\n",
      "Batch 188, Loss: 0.953295, Accuracy: 80.34%\n",
      "Batch 189, Loss: 0.949742, Accuracy: 80.33%\n",
      "Batch 190, Loss: 0.950663, Accuracy: 80.33%\n",
      "Batch 191, Loss: 0.873619, Accuracy: 80.36%\n",
      "Batch 192, Loss: 0.954264, Accuracy: 80.35%\n",
      "Batch 193, Loss: 0.994305, Accuracy: 80.33%\n",
      "Batch 194, Loss: 0.913671, Accuracy: 80.33%\n",
      "Batch 195, Loss: 0.952514, Accuracy: 80.31%\n",
      "Batch 196, Loss: 0.898264, Accuracy: 80.33%\n",
      "Batch 197, Loss: 0.880472, Accuracy: 80.37%\n",
      "Batch 198, Loss: 0.876628, Accuracy: 80.41%\n",
      "Batch 199, Loss: 0.972347, Accuracy: 80.39%\n",
      "Batch 200, Loss: 0.925248, Accuracy: 80.40%\n",
      "Batch 201, Loss: 0.957018, Accuracy: 80.39%\n",
      "Batch 202, Loss: 0.866152, Accuracy: 80.41%\n",
      "Batch 203, Loss: 0.895234, Accuracy: 80.43%\n",
      "Batch 204, Loss: 0.922442, Accuracy: 80.45%\n",
      "Batch 205, Loss: 0.916404, Accuracy: 80.46%\n",
      "Batch 206, Loss: 0.982002, Accuracy: 80.44%\n",
      "Batch 207, Loss: 0.971638, Accuracy: 80.42%\n",
      "Batch 208, Loss: 0.956814, Accuracy: 80.41%\n",
      "Batch 209, Loss: 0.958696, Accuracy: 80.40%\n",
      "Batch 210, Loss: 0.953022, Accuracy: 80.39%\n",
      "Batch 211, Loss: 0.972810, Accuracy: 80.37%\n",
      "Batch 212, Loss: 0.953686, Accuracy: 80.36%\n",
      "Batch 213, Loss: 0.957634, Accuracy: 80.34%\n",
      "Training - Epoch 116, Loss: 0.939314, Accuracy: 80.34%\n",
      "Validation Batch 1, Loss: 0.865808, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.899921, Accuracy: 86.72%\n",
      "Validation Batch 3, Loss: 0.987796, Accuracy: 82.81%\n",
      "Validation Batch 4, Loss: 0.922994, Accuracy: 82.42%\n",
      "Validation Batch 5, Loss: 0.882518, Accuracy: 83.12%\n",
      "Validation Batch 6, Loss: 0.837068, Accuracy: 84.64%\n",
      "Validation Batch 7, Loss: 0.932413, Accuracy: 83.93%\n",
      "Validation Batch 8, Loss: 0.950312, Accuracy: 83.20%\n",
      "Validation Batch 9, Loss: 0.950016, Accuracy: 82.64%\n",
      "Validation Batch 10, Loss: 0.950440, Accuracy: 82.34%\n",
      "Validation Batch 11, Loss: 0.888493, Accuracy: 82.67%\n",
      "Validation Batch 12, Loss: 0.858405, Accuracy: 83.07%\n",
      "Validation Batch 13, Loss: 0.938071, Accuracy: 82.81%\n",
      "Validation Batch 14, Loss: 0.925709, Accuracy: 82.81%\n",
      "Validation Batch 15, Loss: 0.916506, Accuracy: 82.81%\n",
      "Validation Batch 16, Loss: 0.890533, Accuracy: 83.01%\n",
      "Validation Batch 17, Loss: 0.941588, Accuracy: 82.81%\n",
      "Validation Batch 18, Loss: 0.902268, Accuracy: 82.90%\n",
      "Validation Batch 19, Loss: 0.943173, Accuracy: 82.81%\n",
      "Validation Batch 20, Loss: 0.863991, Accuracy: 83.05%\n",
      "Validation Batch 21, Loss: 0.934023, Accuracy: 82.89%\n",
      "Validation Batch 22, Loss: 0.921232, Accuracy: 82.88%\n",
      "Validation Batch 23, Loss: 0.956951, Accuracy: 82.68%\n",
      "Validation Batch 24, Loss: 0.977195, Accuracy: 82.36%\n",
      "Validation Batch 25, Loss: 0.917788, Accuracy: 82.31%\n",
      "Validation Batch 26, Loss: 0.918195, Accuracy: 82.33%\n",
      "Validation Batch 27, Loss: 0.849383, Accuracy: 82.44%\n",
      "Validation - Epoch 116, Loss: 0.915659, Accuracy: 82.44%\n",
      "Patienceâ€”3\n",
      "Epoch 117\n",
      "Batch 1, Loss: 0.926070, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.907392, Accuracy: 82.81%\n",
      "Batch 3, Loss: 0.990789, Accuracy: 80.73%\n",
      "Batch 4, Loss: 0.974874, Accuracy: 80.08%\n",
      "Batch 5, Loss: 0.939151, Accuracy: 79.69%\n",
      "Batch 6, Loss: 0.928333, Accuracy: 79.69%\n",
      "Batch 7, Loss: 0.972004, Accuracy: 79.46%\n",
      "Batch 8, Loss: 0.964571, Accuracy: 79.30%\n",
      "Batch 9, Loss: 0.905600, Accuracy: 79.86%\n",
      "Batch 10, Loss: 0.910384, Accuracy: 80.16%\n",
      "Batch 11, Loss: 0.942875, Accuracy: 80.11%\n",
      "Batch 12, Loss: 0.938838, Accuracy: 80.08%\n",
      "Batch 13, Loss: 0.900588, Accuracy: 80.41%\n",
      "Batch 14, Loss: 0.920698, Accuracy: 80.58%\n",
      "Batch 15, Loss: 0.967767, Accuracy: 80.10%\n",
      "Batch 16, Loss: 0.972648, Accuracy: 79.88%\n",
      "Batch 17, Loss: 0.933918, Accuracy: 79.96%\n",
      "Batch 18, Loss: 0.865100, Accuracy: 80.38%\n",
      "Batch 19, Loss: 0.904542, Accuracy: 80.51%\n",
      "Batch 20, Loss: 0.914836, Accuracy: 80.62%\n",
      "Batch 21, Loss: 0.902407, Accuracy: 80.80%\n",
      "Batch 22, Loss: 0.892612, Accuracy: 81.04%\n",
      "Batch 23, Loss: 0.918241, Accuracy: 81.05%\n",
      "Batch 24, Loss: 0.864147, Accuracy: 81.32%\n",
      "Batch 25, Loss: 0.938526, Accuracy: 81.38%\n",
      "Batch 26, Loss: 0.961588, Accuracy: 81.19%\n",
      "Batch 27, Loss: 0.959758, Accuracy: 81.08%\n",
      "Batch 28, Loss: 0.938781, Accuracy: 81.08%\n",
      "Batch 29, Loss: 0.975598, Accuracy: 80.93%\n",
      "Batch 30, Loss: 0.923349, Accuracy: 80.99%\n",
      "Batch 31, Loss: 0.893869, Accuracy: 81.15%\n",
      "Batch 32, Loss: 0.956144, Accuracy: 81.05%\n",
      "Batch 33, Loss: 0.890993, Accuracy: 81.16%\n",
      "Batch 34, Loss: 0.921159, Accuracy: 81.20%\n",
      "Batch 35, Loss: 0.936911, Accuracy: 81.12%\n",
      "Batch 36, Loss: 0.983857, Accuracy: 80.95%\n",
      "Batch 37, Loss: 0.874186, Accuracy: 81.12%\n",
      "Batch 38, Loss: 0.953971, Accuracy: 81.04%\n",
      "Batch 39, Loss: 0.928028, Accuracy: 81.05%\n",
      "Batch 40, Loss: 0.950507, Accuracy: 80.98%\n",
      "Batch 41, Loss: 0.932426, Accuracy: 80.98%\n",
      "Batch 42, Loss: 0.950998, Accuracy: 80.95%\n",
      "Batch 43, Loss: 0.945116, Accuracy: 80.96%\n",
      "Batch 44, Loss: 0.895337, Accuracy: 81.04%\n",
      "Batch 45, Loss: 0.949019, Accuracy: 80.97%\n",
      "Batch 46, Loss: 0.877247, Accuracy: 81.11%\n",
      "Batch 47, Loss: 1.009182, Accuracy: 80.95%\n",
      "Batch 48, Loss: 0.908223, Accuracy: 81.02%\n",
      "Batch 49, Loss: 0.922454, Accuracy: 81.03%\n",
      "Batch 50, Loss: 0.840702, Accuracy: 81.22%\n",
      "Batch 51, Loss: 0.859440, Accuracy: 81.37%\n",
      "Batch 52, Loss: 0.925600, Accuracy: 81.37%\n",
      "Batch 53, Loss: 1.059104, Accuracy: 81.10%\n",
      "Batch 54, Loss: 0.907259, Accuracy: 81.19%\n",
      "Batch 55, Loss: 0.979143, Accuracy: 81.11%\n",
      "Batch 56, Loss: 0.914962, Accuracy: 81.14%\n",
      "Batch 57, Loss: 0.983288, Accuracy: 81.03%\n",
      "Batch 58, Loss: 0.966030, Accuracy: 80.95%\n",
      "Batch 59, Loss: 0.927969, Accuracy: 80.93%\n",
      "Batch 60, Loss: 0.936283, Accuracy: 80.94%\n",
      "Batch 61, Loss: 0.961896, Accuracy: 80.89%\n",
      "Batch 62, Loss: 0.956843, Accuracy: 80.87%\n",
      "Batch 63, Loss: 0.935484, Accuracy: 80.85%\n",
      "Batch 64, Loss: 0.925397, Accuracy: 80.86%\n",
      "Batch 65, Loss: 0.934000, Accuracy: 80.87%\n",
      "Batch 66, Loss: 1.018555, Accuracy: 80.75%\n",
      "Batch 67, Loss: 0.980072, Accuracy: 80.69%\n",
      "Batch 68, Loss: 0.948639, Accuracy: 80.65%\n",
      "Batch 69, Loss: 0.988306, Accuracy: 80.57%\n",
      "Batch 70, Loss: 0.950074, Accuracy: 80.56%\n",
      "Batch 71, Loss: 0.992383, Accuracy: 80.50%\n",
      "Batch 72, Loss: 0.911551, Accuracy: 80.51%\n",
      "Batch 73, Loss: 0.939808, Accuracy: 80.52%\n",
      "Batch 74, Loss: 0.966975, Accuracy: 80.45%\n",
      "Batch 75, Loss: 0.923794, Accuracy: 80.48%\n",
      "Batch 76, Loss: 0.924086, Accuracy: 80.49%\n",
      "Batch 77, Loss: 0.838404, Accuracy: 80.64%\n",
      "Batch 78, Loss: 1.020872, Accuracy: 80.53%\n",
      "Batch 79, Loss: 0.995819, Accuracy: 80.44%\n",
      "Batch 80, Loss: 0.892777, Accuracy: 80.49%\n",
      "Batch 81, Loss: 0.901744, Accuracy: 80.54%\n",
      "Batch 82, Loss: 0.932732, Accuracy: 80.53%\n",
      "Batch 83, Loss: 0.942534, Accuracy: 80.52%\n",
      "Batch 84, Loss: 0.848551, Accuracy: 80.62%\n",
      "Batch 85, Loss: 0.919683, Accuracy: 80.64%\n",
      "Batch 86, Loss: 0.930398, Accuracy: 80.63%\n",
      "Batch 87, Loss: 0.987425, Accuracy: 80.59%\n",
      "Batch 88, Loss: 0.904144, Accuracy: 80.63%\n",
      "Batch 89, Loss: 0.881033, Accuracy: 80.71%\n",
      "Batch 90, Loss: 0.950669, Accuracy: 80.68%\n",
      "Batch 91, Loss: 0.895511, Accuracy: 80.73%\n",
      "Batch 92, Loss: 0.824455, Accuracy: 80.86%\n",
      "Batch 93, Loss: 0.929686, Accuracy: 80.85%\n",
      "Batch 94, Loss: 0.940462, Accuracy: 80.85%\n",
      "Batch 95, Loss: 0.901525, Accuracy: 80.89%\n",
      "Batch 96, Loss: 0.877642, Accuracy: 80.94%\n",
      "Batch 97, Loss: 1.000991, Accuracy: 80.88%\n",
      "Batch 98, Loss: 0.945942, Accuracy: 80.88%\n",
      "Batch 99, Loss: 0.922925, Accuracy: 80.90%\n",
      "Batch 100, Loss: 0.914113, Accuracy: 80.92%\n",
      "Batch 101, Loss: 0.883827, Accuracy: 80.96%\n",
      "Batch 102, Loss: 0.981380, Accuracy: 80.91%\n",
      "Batch 103, Loss: 0.965491, Accuracy: 80.89%\n",
      "Batch 104, Loss: 1.022235, Accuracy: 80.78%\n",
      "Batch 105, Loss: 0.878679, Accuracy: 80.86%\n",
      "Batch 106, Loss: 1.023277, Accuracy: 80.76%\n",
      "Batch 107, Loss: 0.897200, Accuracy: 80.80%\n",
      "Batch 108, Loss: 0.886159, Accuracy: 80.84%\n",
      "Batch 109, Loss: 0.934729, Accuracy: 80.85%\n",
      "Batch 110, Loss: 1.005909, Accuracy: 80.78%\n",
      "Batch 111, Loss: 0.918856, Accuracy: 80.80%\n",
      "Batch 112, Loss: 0.947885, Accuracy: 80.79%\n",
      "Batch 113, Loss: 0.971764, Accuracy: 80.77%\n",
      "Batch 114, Loss: 0.923776, Accuracy: 80.78%\n",
      "Batch 115, Loss: 0.981279, Accuracy: 80.76%\n",
      "Batch 116, Loss: 0.912480, Accuracy: 80.78%\n",
      "Batch 117, Loss: 0.944433, Accuracy: 80.77%\n",
      "Batch 118, Loss: 0.973940, Accuracy: 80.75%\n",
      "Batch 119, Loss: 1.003438, Accuracy: 80.69%\n",
      "Batch 120, Loss: 0.937250, Accuracy: 80.69%\n",
      "Batch 121, Loss: 0.966116, Accuracy: 80.67%\n",
      "Batch 122, Loss: 0.892616, Accuracy: 80.71%\n",
      "Batch 123, Loss: 0.916088, Accuracy: 80.72%\n",
      "Batch 124, Loss: 0.919074, Accuracy: 80.72%\n",
      "Batch 125, Loss: 0.948492, Accuracy: 80.71%\n",
      "Batch 126, Loss: 0.934200, Accuracy: 80.72%\n",
      "Batch 127, Loss: 0.897109, Accuracy: 80.76%\n",
      "Batch 128, Loss: 0.954754, Accuracy: 80.74%\n",
      "Batch 129, Loss: 0.902396, Accuracy: 80.77%\n",
      "Batch 130, Loss: 0.951088, Accuracy: 80.75%\n",
      "Batch 131, Loss: 1.039677, Accuracy: 80.67%\n",
      "Batch 132, Loss: 0.873748, Accuracy: 80.71%\n",
      "Batch 133, Loss: 0.996617, Accuracy: 80.65%\n",
      "Batch 134, Loss: 0.867824, Accuracy: 80.70%\n",
      "Batch 135, Loss: 0.917731, Accuracy: 80.71%\n",
      "Batch 136, Loss: 0.902737, Accuracy: 80.73%\n",
      "Batch 137, Loss: 0.998454, Accuracy: 80.68%\n",
      "Batch 138, Loss: 0.964122, Accuracy: 80.66%\n",
      "Batch 139, Loss: 0.957053, Accuracy: 80.65%\n",
      "Batch 140, Loss: 0.900703, Accuracy: 80.68%\n",
      "Batch 141, Loss: 0.986957, Accuracy: 80.65%\n",
      "Batch 142, Loss: 0.945134, Accuracy: 80.63%\n",
      "Batch 143, Loss: 0.918195, Accuracy: 80.65%\n",
      "Batch 144, Loss: 0.918814, Accuracy: 80.65%\n",
      "Batch 145, Loss: 1.034220, Accuracy: 80.58%\n",
      "Batch 146, Loss: 1.112414, Accuracy: 80.46%\n",
      "Batch 147, Loss: 0.933085, Accuracy: 80.46%\n",
      "Batch 148, Loss: 0.926370, Accuracy: 80.48%\n",
      "Batch 149, Loss: 0.884595, Accuracy: 80.51%\n",
      "Batch 150, Loss: 0.980844, Accuracy: 80.47%\n",
      "Batch 151, Loss: 0.976618, Accuracy: 80.44%\n",
      "Batch 152, Loss: 0.925803, Accuracy: 80.45%\n",
      "Batch 153, Loss: 0.932035, Accuracy: 80.45%\n",
      "Batch 154, Loss: 1.027869, Accuracy: 80.40%\n",
      "Batch 155, Loss: 0.824757, Accuracy: 80.47%\n",
      "Batch 156, Loss: 0.977129, Accuracy: 80.45%\n",
      "Batch 157, Loss: 0.962325, Accuracy: 80.44%\n",
      "Batch 158, Loss: 1.054264, Accuracy: 80.37%\n",
      "Batch 159, Loss: 0.991514, Accuracy: 80.34%\n",
      "Batch 160, Loss: 0.894924, Accuracy: 80.37%\n",
      "Batch 161, Loss: 0.952200, Accuracy: 80.37%\n",
      "Batch 162, Loss: 0.895906, Accuracy: 80.39%\n",
      "Batch 163, Loss: 0.901753, Accuracy: 80.42%\n",
      "Batch 164, Loss: 0.892873, Accuracy: 80.44%\n",
      "Batch 165, Loss: 0.951599, Accuracy: 80.44%\n",
      "Batch 166, Loss: 1.022585, Accuracy: 80.37%\n",
      "Batch 167, Loss: 0.949036, Accuracy: 80.37%\n",
      "Batch 168, Loss: 0.972707, Accuracy: 80.34%\n",
      "Batch 169, Loss: 0.968815, Accuracy: 80.31%\n",
      "Batch 170, Loss: 0.983185, Accuracy: 80.28%\n",
      "Batch 171, Loss: 0.943334, Accuracy: 80.27%\n",
      "Batch 172, Loss: 0.994314, Accuracy: 80.24%\n",
      "Batch 173, Loss: 0.947527, Accuracy: 80.25%\n",
      "Batch 174, Loss: 1.016456, Accuracy: 80.19%\n",
      "Batch 175, Loss: 0.983113, Accuracy: 80.17%\n",
      "Batch 176, Loss: 0.909578, Accuracy: 80.19%\n",
      "Batch 177, Loss: 0.944108, Accuracy: 80.19%\n",
      "Batch 178, Loss: 0.950153, Accuracy: 80.19%\n",
      "Batch 179, Loss: 0.961724, Accuracy: 80.17%\n",
      "Batch 180, Loss: 0.940618, Accuracy: 80.16%\n",
      "Batch 181, Loss: 0.885799, Accuracy: 80.19%\n",
      "Batch 182, Loss: 0.852535, Accuracy: 80.24%\n",
      "Batch 183, Loss: 0.973258, Accuracy: 80.21%\n",
      "Batch 184, Loss: 1.010275, Accuracy: 80.16%\n",
      "Batch 185, Loss: 0.963706, Accuracy: 80.14%\n",
      "Batch 186, Loss: 0.964690, Accuracy: 80.13%\n",
      "Batch 187, Loss: 0.913024, Accuracy: 80.14%\n",
      "Batch 188, Loss: 0.858553, Accuracy: 80.18%\n",
      "Batch 189, Loss: 0.994799, Accuracy: 80.14%\n",
      "Batch 190, Loss: 0.966967, Accuracy: 80.12%\n",
      "Batch 191, Loss: 0.963986, Accuracy: 80.11%\n",
      "Batch 192, Loss: 0.913732, Accuracy: 80.14%\n",
      "Batch 193, Loss: 0.939738, Accuracy: 80.14%\n",
      "Batch 194, Loss: 0.893630, Accuracy: 80.17%\n",
      "Batch 195, Loss: 0.879267, Accuracy: 80.21%\n",
      "Batch 196, Loss: 0.949429, Accuracy: 80.21%\n",
      "Batch 197, Loss: 0.982509, Accuracy: 80.19%\n",
      "Batch 198, Loss: 0.970163, Accuracy: 80.18%\n",
      "Batch 199, Loss: 0.892796, Accuracy: 80.21%\n",
      "Batch 200, Loss: 1.025461, Accuracy: 80.16%\n",
      "Batch 201, Loss: 0.898309, Accuracy: 80.19%\n",
      "Batch 202, Loss: 0.876039, Accuracy: 80.21%\n",
      "Batch 203, Loss: 0.957248, Accuracy: 80.21%\n",
      "Batch 204, Loss: 0.920758, Accuracy: 80.22%\n",
      "Batch 205, Loss: 0.939935, Accuracy: 80.23%\n",
      "Batch 206, Loss: 0.927522, Accuracy: 80.23%\n",
      "Batch 207, Loss: 0.923143, Accuracy: 80.24%\n",
      "Batch 208, Loss: 0.868214, Accuracy: 80.27%\n",
      "Batch 209, Loss: 0.961312, Accuracy: 80.27%\n",
      "Batch 210, Loss: 0.971598, Accuracy: 80.25%\n",
      "Batch 211, Loss: 0.927409, Accuracy: 80.26%\n",
      "Batch 212, Loss: 0.901059, Accuracy: 80.28%\n",
      "Batch 213, Loss: 1.013421, Accuracy: 80.24%\n",
      "Training - Epoch 117, Loss: 0.939707, Accuracy: 80.24%\n",
      "Validation Batch 1, Loss: 0.891542, Accuracy: 85.94%\n",
      "Validation Batch 2, Loss: 0.949007, Accuracy: 82.81%\n",
      "Validation Batch 3, Loss: 1.009850, Accuracy: 79.69%\n",
      "Validation Batch 4, Loss: 0.938041, Accuracy: 80.08%\n",
      "Validation Batch 5, Loss: 0.919873, Accuracy: 80.62%\n",
      "Validation Batch 6, Loss: 0.872286, Accuracy: 81.51%\n",
      "Validation Batch 7, Loss: 0.963466, Accuracy: 81.03%\n",
      "Validation Batch 8, Loss: 0.971345, Accuracy: 80.66%\n",
      "Validation Batch 9, Loss: 0.978511, Accuracy: 80.38%\n",
      "Validation Batch 10, Loss: 0.967340, Accuracy: 80.00%\n",
      "Validation Batch 11, Loss: 0.904877, Accuracy: 80.40%\n",
      "Validation Batch 12, Loss: 0.875940, Accuracy: 80.99%\n",
      "Validation Batch 13, Loss: 0.977144, Accuracy: 80.65%\n",
      "Validation Batch 14, Loss: 0.965492, Accuracy: 80.47%\n",
      "Validation Batch 15, Loss: 0.931742, Accuracy: 80.52%\n",
      "Validation Batch 16, Loss: 0.910852, Accuracy: 80.76%\n",
      "Validation Batch 17, Loss: 0.975652, Accuracy: 80.51%\n",
      "Validation Batch 18, Loss: 0.931195, Accuracy: 80.47%\n",
      "Validation Batch 19, Loss: 0.977099, Accuracy: 80.26%\n",
      "Validation Batch 20, Loss: 0.905625, Accuracy: 80.39%\n",
      "Validation Batch 21, Loss: 0.951283, Accuracy: 80.36%\n",
      "Validation Batch 22, Loss: 0.978473, Accuracy: 80.11%\n",
      "Validation Batch 23, Loss: 1.017729, Accuracy: 79.82%\n",
      "Validation Batch 24, Loss: 1.001954, Accuracy: 79.62%\n",
      "Validation Batch 25, Loss: 0.957495, Accuracy: 79.56%\n",
      "Validation Batch 26, Loss: 0.925484, Accuracy: 79.69%\n",
      "Validation Batch 27, Loss: 0.886355, Accuracy: 79.80%\n",
      "Validation - Epoch 117, Loss: 0.945765, Accuracy: 79.80%\n",
      "Patienceâ€”4\n",
      "Epoch 118\n",
      "Batch 1, Loss: 0.928389, Accuracy: 81.25%\n",
      "Batch 2, Loss: 0.911039, Accuracy: 82.03%\n",
      "Batch 3, Loss: 0.946281, Accuracy: 81.25%\n",
      "Batch 4, Loss: 0.978906, Accuracy: 80.08%\n",
      "Batch 5, Loss: 0.924647, Accuracy: 80.31%\n",
      "Batch 6, Loss: 0.888720, Accuracy: 81.25%\n",
      "Batch 7, Loss: 0.925022, Accuracy: 81.25%\n",
      "Batch 8, Loss: 0.932778, Accuracy: 81.25%\n",
      "Batch 9, Loss: 0.999535, Accuracy: 80.56%\n",
      "Batch 10, Loss: 0.900317, Accuracy: 81.09%\n",
      "Batch 11, Loss: 0.934641, Accuracy: 81.11%\n",
      "Batch 12, Loss: 0.982358, Accuracy: 80.73%\n",
      "Batch 13, Loss: 1.009033, Accuracy: 80.17%\n",
      "Batch 14, Loss: 0.973394, Accuracy: 79.91%\n",
      "Batch 15, Loss: 0.921515, Accuracy: 80.00%\n",
      "Batch 16, Loss: 0.902201, Accuracy: 80.27%\n",
      "Batch 17, Loss: 0.961389, Accuracy: 80.24%\n",
      "Batch 18, Loss: 0.920618, Accuracy: 80.38%\n",
      "Batch 19, Loss: 0.864543, Accuracy: 80.84%\n",
      "Batch 20, Loss: 0.932981, Accuracy: 80.86%\n",
      "Batch 21, Loss: 0.954517, Accuracy: 80.73%\n",
      "Batch 22, Loss: 1.022676, Accuracy: 80.33%\n",
      "Batch 23, Loss: 0.962165, Accuracy: 80.30%\n",
      "Batch 24, Loss: 0.906224, Accuracy: 80.40%\n",
      "Batch 25, Loss: 0.936243, Accuracy: 80.44%\n",
      "Batch 26, Loss: 0.921650, Accuracy: 80.47%\n",
      "Batch 27, Loss: 0.829895, Accuracy: 80.90%\n",
      "Batch 28, Loss: 0.911630, Accuracy: 81.08%\n",
      "Batch 29, Loss: 0.930558, Accuracy: 81.09%\n",
      "Batch 30, Loss: 0.932249, Accuracy: 81.09%\n",
      "Batch 31, Loss: 0.878153, Accuracy: 81.30%\n",
      "Batch 32, Loss: 0.975878, Accuracy: 81.15%\n",
      "Batch 33, Loss: 0.904274, Accuracy: 81.20%\n",
      "Batch 34, Loss: 0.883491, Accuracy: 81.34%\n",
      "Batch 35, Loss: 0.949599, Accuracy: 81.29%\n",
      "Batch 36, Loss: 0.939196, Accuracy: 81.29%\n",
      "Batch 37, Loss: 0.860562, Accuracy: 81.46%\n",
      "Batch 38, Loss: 0.986751, Accuracy: 81.29%\n",
      "Batch 39, Loss: 0.930906, Accuracy: 81.29%\n",
      "Batch 40, Loss: 0.917086, Accuracy: 81.33%\n",
      "Batch 41, Loss: 0.852332, Accuracy: 81.52%\n",
      "Batch 42, Loss: 0.939468, Accuracy: 81.47%\n",
      "Batch 43, Loss: 0.903829, Accuracy: 81.54%\n",
      "Batch 44, Loss: 0.960319, Accuracy: 81.50%\n",
      "Batch 45, Loss: 1.013719, Accuracy: 81.32%\n",
      "Batch 46, Loss: 0.934736, Accuracy: 81.28%\n",
      "Batch 47, Loss: 0.956984, Accuracy: 81.18%\n",
      "Batch 48, Loss: 0.950721, Accuracy: 81.15%\n",
      "Batch 49, Loss: 0.969574, Accuracy: 81.09%\n",
      "Batch 50, Loss: 0.974512, Accuracy: 80.97%\n",
      "Batch 51, Loss: 1.063641, Accuracy: 80.70%\n",
      "Batch 52, Loss: 0.856854, Accuracy: 80.86%\n",
      "Batch 53, Loss: 0.868757, Accuracy: 80.98%\n",
      "Batch 54, Loss: 0.912993, Accuracy: 81.02%\n",
      "Batch 55, Loss: 0.963905, Accuracy: 80.97%\n",
      "Batch 56, Loss: 0.935473, Accuracy: 80.97%\n",
      "Batch 57, Loss: 0.889454, Accuracy: 81.03%\n",
      "Batch 58, Loss: 0.949673, Accuracy: 81.01%\n",
      "Batch 59, Loss: 0.937414, Accuracy: 81.01%\n",
      "Batch 60, Loss: 0.984968, Accuracy: 80.91%\n",
      "Batch 61, Loss: 0.940829, Accuracy: 80.89%\n",
      "Batch 62, Loss: 0.919842, Accuracy: 80.95%\n",
      "Batch 63, Loss: 0.967636, Accuracy: 80.90%\n",
      "Batch 64, Loss: 0.879030, Accuracy: 80.98%\n",
      "Batch 65, Loss: 0.975027, Accuracy: 80.91%\n",
      "Batch 66, Loss: 0.868113, Accuracy: 81.01%\n",
      "Batch 67, Loss: 0.934855, Accuracy: 81.02%\n",
      "Batch 68, Loss: 0.967029, Accuracy: 81.00%\n",
      "Batch 69, Loss: 0.979344, Accuracy: 80.91%\n",
      "Batch 70, Loss: 0.846543, Accuracy: 81.05%\n",
      "Batch 71, Loss: 0.911653, Accuracy: 81.10%\n",
      "Batch 72, Loss: 0.865664, Accuracy: 81.18%\n",
      "Batch 73, Loss: 0.914430, Accuracy: 81.21%\n",
      "Batch 74, Loss: 0.931233, Accuracy: 81.21%\n",
      "Batch 75, Loss: 0.892586, Accuracy: 81.27%\n",
      "Batch 76, Loss: 0.887070, Accuracy: 81.31%\n",
      "Batch 77, Loss: 0.852377, Accuracy: 81.41%\n",
      "Batch 78, Loss: 0.907046, Accuracy: 81.45%\n",
      "Batch 79, Loss: 0.901664, Accuracy: 81.47%\n",
      "Batch 80, Loss: 0.951723, Accuracy: 81.43%\n",
      "Batch 81, Loss: 0.944407, Accuracy: 81.40%\n",
      "Batch 82, Loss: 0.945035, Accuracy: 81.38%\n",
      "Batch 83, Loss: 0.947038, Accuracy: 81.34%\n",
      "Batch 84, Loss: 0.909931, Accuracy: 81.36%\n",
      "Batch 85, Loss: 0.935698, Accuracy: 81.36%\n",
      "Batch 86, Loss: 0.941179, Accuracy: 81.34%\n",
      "Batch 87, Loss: 0.974242, Accuracy: 81.30%\n",
      "Batch 88, Loss: 0.897220, Accuracy: 81.34%\n",
      "Batch 89, Loss: 0.915002, Accuracy: 81.36%\n",
      "Batch 90, Loss: 0.850080, Accuracy: 81.44%\n",
      "Batch 91, Loss: 0.961433, Accuracy: 81.39%\n",
      "Batch 92, Loss: 0.904899, Accuracy: 81.42%\n",
      "Batch 93, Loss: 0.910311, Accuracy: 81.45%\n",
      "Batch 94, Loss: 0.902950, Accuracy: 81.48%\n",
      "Batch 95, Loss: 0.960728, Accuracy: 81.45%\n",
      "Batch 96, Loss: 0.913430, Accuracy: 81.45%\n",
      "Batch 97, Loss: 0.992408, Accuracy: 81.38%\n",
      "Batch 98, Loss: 0.899128, Accuracy: 81.43%\n",
      "Batch 99, Loss: 0.952717, Accuracy: 81.39%\n",
      "Batch 100, Loss: 1.010608, Accuracy: 81.30%\n",
      "Batch 101, Loss: 0.986683, Accuracy: 81.23%\n",
      "Batch 102, Loss: 0.904409, Accuracy: 81.27%\n",
      "Batch 103, Loss: 0.969734, Accuracy: 81.22%\n",
      "Batch 104, Loss: 0.958100, Accuracy: 81.20%\n",
      "Batch 105, Loss: 0.970734, Accuracy: 81.18%\n",
      "Batch 106, Loss: 0.961079, Accuracy: 81.15%\n",
      "Batch 107, Loss: 0.875002, Accuracy: 81.19%\n",
      "Batch 108, Loss: 0.916068, Accuracy: 81.21%\n",
      "Batch 109, Loss: 0.900733, Accuracy: 81.22%\n",
      "Batch 110, Loss: 0.887046, Accuracy: 81.25%\n",
      "Batch 111, Loss: 0.963150, Accuracy: 81.24%\n",
      "Batch 112, Loss: 0.938581, Accuracy: 81.21%\n",
      "Batch 113, Loss: 0.984916, Accuracy: 81.17%\n",
      "Batch 114, Loss: 1.007810, Accuracy: 81.10%\n",
      "Batch 115, Loss: 0.867290, Accuracy: 81.17%\n",
      "Batch 116, Loss: 0.959136, Accuracy: 81.14%\n",
      "Batch 117, Loss: 0.985584, Accuracy: 81.10%\n",
      "Batch 118, Loss: 0.844297, Accuracy: 81.18%\n",
      "Batch 119, Loss: 0.925704, Accuracy: 81.18%\n",
      "Batch 120, Loss: 0.914334, Accuracy: 81.18%\n",
      "Batch 121, Loss: 0.911833, Accuracy: 81.20%\n",
      "Batch 122, Loss: 0.918133, Accuracy: 81.22%\n",
      "Batch 123, Loss: 0.912187, Accuracy: 81.24%\n",
      "Batch 124, Loss: 0.872259, Accuracy: 81.29%\n",
      "Batch 125, Loss: 0.948183, Accuracy: 81.28%\n",
      "Batch 126, Loss: 0.958068, Accuracy: 81.25%\n",
      "Batch 127, Loss: 0.947525, Accuracy: 81.21%\n",
      "Batch 128, Loss: 0.960197, Accuracy: 81.20%\n",
      "Batch 129, Loss: 0.978915, Accuracy: 81.15%\n",
      "Batch 130, Loss: 0.910221, Accuracy: 81.15%\n",
      "Batch 131, Loss: 0.913841, Accuracy: 81.18%\n",
      "Batch 132, Loss: 1.104181, Accuracy: 81.03%\n",
      "Batch 133, Loss: 0.976658, Accuracy: 80.99%\n",
      "Batch 134, Loss: 0.955438, Accuracy: 80.98%\n",
      "Batch 135, Loss: 0.977533, Accuracy: 80.95%\n",
      "Batch 136, Loss: 0.899859, Accuracy: 80.96%\n",
      "Batch 137, Loss: 0.977932, Accuracy: 80.93%\n",
      "Batch 138, Loss: 0.917334, Accuracy: 80.94%\n",
      "Batch 139, Loss: 0.965396, Accuracy: 80.92%\n",
      "Batch 140, Loss: 0.952562, Accuracy: 80.90%\n",
      "Batch 141, Loss: 0.931615, Accuracy: 80.90%\n",
      "Batch 142, Loss: 0.918894, Accuracy: 80.90%\n",
      "Batch 143, Loss: 1.068728, Accuracy: 80.78%\n",
      "Batch 144, Loss: 0.904218, Accuracy: 80.81%\n",
      "Batch 145, Loss: 0.912478, Accuracy: 80.81%\n",
      "Batch 146, Loss: 0.936037, Accuracy: 80.81%\n",
      "Batch 147, Loss: 1.016657, Accuracy: 80.75%\n",
      "Batch 148, Loss: 0.908509, Accuracy: 80.77%\n",
      "Batch 149, Loss: 0.920130, Accuracy: 80.79%\n",
      "Batch 150, Loss: 0.971800, Accuracy: 80.75%\n",
      "Batch 151, Loss: 1.031903, Accuracy: 80.68%\n",
      "Batch 152, Loss: 0.921386, Accuracy: 80.69%\n",
      "Batch 153, Loss: 0.981313, Accuracy: 80.67%\n",
      "Batch 154, Loss: 0.884828, Accuracy: 80.70%\n",
      "Batch 155, Loss: 0.917135, Accuracy: 80.70%\n",
      "Batch 156, Loss: 0.923596, Accuracy: 80.70%\n",
      "Batch 157, Loss: 0.884262, Accuracy: 80.74%\n",
      "Batch 158, Loss: 1.048647, Accuracy: 80.68%\n",
      "Batch 159, Loss: 0.898035, Accuracy: 80.70%\n",
      "Batch 160, Loss: 1.028383, Accuracy: 80.63%\n",
      "Batch 161, Loss: 0.980243, Accuracy: 80.60%\n",
      "Batch 162, Loss: 0.884235, Accuracy: 80.63%\n",
      "Batch 163, Loss: 0.850268, Accuracy: 80.68%\n",
      "Batch 164, Loss: 0.971200, Accuracy: 80.65%\n",
      "Batch 165, Loss: 0.892059, Accuracy: 80.67%\n",
      "Batch 166, Loss: 0.945425, Accuracy: 80.67%\n",
      "Batch 167, Loss: 0.940836, Accuracy: 80.67%\n",
      "Batch 168, Loss: 0.985404, Accuracy: 80.65%\n",
      "Batch 169, Loss: 0.976309, Accuracy: 80.62%\n",
      "Batch 170, Loss: 0.963641, Accuracy: 80.61%\n",
      "Batch 171, Loss: 0.970077, Accuracy: 80.60%\n",
      "Batch 172, Loss: 0.912913, Accuracy: 80.61%\n",
      "Batch 173, Loss: 0.951698, Accuracy: 80.60%\n",
      "Batch 174, Loss: 0.972542, Accuracy: 80.56%\n",
      "Batch 175, Loss: 0.934617, Accuracy: 80.57%\n",
      "Batch 176, Loss: 0.901688, Accuracy: 80.58%\n",
      "Batch 177, Loss: 0.947470, Accuracy: 80.58%\n",
      "Batch 178, Loss: 1.060267, Accuracy: 80.50%\n",
      "Batch 179, Loss: 0.953276, Accuracy: 80.49%\n",
      "Batch 180, Loss: 0.913487, Accuracy: 80.51%\n",
      "Batch 181, Loss: 0.992961, Accuracy: 80.47%\n",
      "Batch 182, Loss: 0.980811, Accuracy: 80.44%\n",
      "Batch 183, Loss: 0.904020, Accuracy: 80.46%\n",
      "Batch 184, Loss: 0.922921, Accuracy: 80.48%\n",
      "Batch 185, Loss: 0.972133, Accuracy: 80.44%\n",
      "Batch 186, Loss: 0.856539, Accuracy: 80.49%\n",
      "Batch 187, Loss: 0.913042, Accuracy: 80.50%\n",
      "Batch 188, Loss: 0.874566, Accuracy: 80.54%\n",
      "Batch 189, Loss: 0.885463, Accuracy: 80.56%\n",
      "Batch 190, Loss: 0.971579, Accuracy: 80.55%\n",
      "Batch 191, Loss: 0.994932, Accuracy: 80.51%\n",
      "Batch 192, Loss: 0.930591, Accuracy: 80.52%\n",
      "Batch 193, Loss: 0.932377, Accuracy: 80.52%\n",
      "Batch 194, Loss: 0.991971, Accuracy: 80.49%\n",
      "Batch 195, Loss: 0.961596, Accuracy: 80.49%\n",
      "Batch 196, Loss: 0.895449, Accuracy: 80.52%\n",
      "Batch 197, Loss: 1.006305, Accuracy: 80.48%\n",
      "Batch 198, Loss: 0.921098, Accuracy: 80.48%\n",
      "Batch 199, Loss: 0.892691, Accuracy: 80.51%\n",
      "Batch 200, Loss: 0.871876, Accuracy: 80.55%\n",
      "Batch 201, Loss: 0.957288, Accuracy: 80.54%\n",
      "Batch 202, Loss: 0.881885, Accuracy: 80.58%\n",
      "Batch 203, Loss: 0.890499, Accuracy: 80.60%\n",
      "Batch 204, Loss: 0.927733, Accuracy: 80.61%\n",
      "Batch 205, Loss: 0.834484, Accuracy: 80.66%\n",
      "Batch 206, Loss: 0.998490, Accuracy: 80.63%\n",
      "Batch 207, Loss: 0.929265, Accuracy: 80.62%\n",
      "Batch 208, Loss: 0.904400, Accuracy: 80.63%\n",
      "Batch 209, Loss: 1.017540, Accuracy: 80.59%\n",
      "Batch 210, Loss: 0.960377, Accuracy: 80.59%\n",
      "Batch 211, Loss: 0.982632, Accuracy: 80.57%\n",
      "Batch 212, Loss: 0.934221, Accuracy: 80.56%\n",
      "Batch 213, Loss: 0.998467, Accuracy: 80.54%\n",
      "Training - Epoch 118, Loss: 0.936801, Accuracy: 80.54%\n",
      "Validation Batch 1, Loss: 0.859573, Accuracy: 89.06%\n",
      "Validation Batch 2, Loss: 0.903100, Accuracy: 86.72%\n",
      "Validation Batch 3, Loss: 0.977896, Accuracy: 83.33%\n",
      "Validation Batch 4, Loss: 0.913527, Accuracy: 83.20%\n",
      "Validation Batch 5, Loss: 0.858458, Accuracy: 84.69%\n",
      "Validation Batch 6, Loss: 0.850355, Accuracy: 85.42%\n",
      "Validation Batch 7, Loss: 0.928333, Accuracy: 84.82%\n",
      "Validation Batch 8, Loss: 0.961436, Accuracy: 83.79%\n",
      "Validation Batch 9, Loss: 0.948618, Accuracy: 82.99%\n",
      "Validation Batch 10, Loss: 0.940267, Accuracy: 82.66%\n",
      "Validation Batch 11, Loss: 0.891783, Accuracy: 82.95%\n",
      "Validation Batch 12, Loss: 0.856290, Accuracy: 83.46%\n",
      "Validation Batch 13, Loss: 0.929886, Accuracy: 83.41%\n",
      "Validation Batch 14, Loss: 0.917329, Accuracy: 83.37%\n",
      "Validation Batch 15, Loss: 0.919215, Accuracy: 83.33%\n",
      "Validation Batch 16, Loss: 0.885274, Accuracy: 83.50%\n",
      "Validation Batch 17, Loss: 0.945886, Accuracy: 83.27%\n",
      "Validation Batch 18, Loss: 0.890549, Accuracy: 83.42%\n",
      "Validation Batch 19, Loss: 0.939994, Accuracy: 83.22%\n",
      "Validation Batch 20, Loss: 0.832232, Accuracy: 83.67%\n",
      "Validation Batch 21, Loss: 0.914186, Accuracy: 83.63%\n",
      "Validation Batch 22, Loss: 0.911169, Accuracy: 83.59%\n",
      "Validation Batch 23, Loss: 0.929039, Accuracy: 83.49%\n",
      "Validation Batch 24, Loss: 0.961578, Accuracy: 83.27%\n",
      "Validation Batch 25, Loss: 0.906415, Accuracy: 83.31%\n",
      "Validation Batch 26, Loss: 0.908657, Accuracy: 83.29%\n",
      "Validation Batch 27, Loss: 0.839932, Accuracy: 83.50%\n",
      "Validation - Epoch 118, Loss: 0.908184, Accuracy: 83.50%\n",
      "Patienceâ€”0\n",
      "Epoch 119\n",
      "Batch 1, Loss: 0.967155, Accuracy: 78.12%\n",
      "Batch 2, Loss: 0.912962, Accuracy: 80.47%\n",
      "Batch 3, Loss: 0.911120, Accuracy: 81.25%\n",
      "Batch 4, Loss: 0.900508, Accuracy: 82.03%\n",
      "Batch 5, Loss: 1.009616, Accuracy: 80.31%\n",
      "Batch 6, Loss: 0.954492, Accuracy: 80.21%\n",
      "Batch 7, Loss: 0.925495, Accuracy: 80.13%\n",
      "Batch 8, Loss: 1.049029, Accuracy: 78.71%\n",
      "Batch 9, Loss: 0.889712, Accuracy: 79.34%\n",
      "Batch 10, Loss: 0.919935, Accuracy: 79.53%\n",
      "Batch 11, Loss: 0.917743, Accuracy: 79.83%\n",
      "Batch 12, Loss: 0.939601, Accuracy: 79.95%\n",
      "Batch 13, Loss: 1.011544, Accuracy: 79.33%\n",
      "Batch 14, Loss: 0.921013, Accuracy: 79.46%\n",
      "Batch 15, Loss: 0.945801, Accuracy: 79.58%\n",
      "Batch 16, Loss: 0.966355, Accuracy: 79.49%\n",
      "Batch 17, Loss: 0.932755, Accuracy: 79.60%\n",
      "Batch 18, Loss: 0.931274, Accuracy: 79.77%\n",
      "Batch 19, Loss: 0.935420, Accuracy: 79.77%\n",
      "Batch 20, Loss: 0.989874, Accuracy: 79.61%\n",
      "Batch 21, Loss: 0.928279, Accuracy: 79.69%\n",
      "Batch 22, Loss: 0.929383, Accuracy: 79.76%\n",
      "Batch 23, Loss: 0.984658, Accuracy: 79.55%\n",
      "Batch 24, Loss: 0.985839, Accuracy: 79.49%\n",
      "Batch 25, Loss: 0.993983, Accuracy: 79.25%\n",
      "Batch 26, Loss: 0.951594, Accuracy: 79.27%\n",
      "Batch 27, Loss: 0.920951, Accuracy: 79.34%\n",
      "Batch 28, Loss: 0.919432, Accuracy: 79.46%\n",
      "Batch 29, Loss: 0.905308, Accuracy: 79.63%\n",
      "Batch 30, Loss: 0.916811, Accuracy: 79.69%\n",
      "Batch 31, Loss: 0.963502, Accuracy: 79.69%\n",
      "Batch 32, Loss: 0.942882, Accuracy: 79.69%\n",
      "Batch 33, Loss: 0.935817, Accuracy: 79.73%\n",
      "Batch 34, Loss: 0.942034, Accuracy: 79.73%\n",
      "Batch 35, Loss: 0.940543, Accuracy: 79.73%\n",
      "Batch 36, Loss: 1.046773, Accuracy: 79.47%\n",
      "Batch 37, Loss: 0.920961, Accuracy: 79.56%\n",
      "Batch 38, Loss: 0.965487, Accuracy: 79.48%\n",
      "Batch 39, Loss: 0.874345, Accuracy: 79.69%\n",
      "Batch 40, Loss: 0.928551, Accuracy: 79.73%\n",
      "Batch 41, Loss: 0.999104, Accuracy: 79.61%\n",
      "Batch 42, Loss: 0.946416, Accuracy: 79.65%\n",
      "Batch 43, Loss: 0.953626, Accuracy: 79.65%\n",
      "Batch 44, Loss: 1.016343, Accuracy: 79.47%\n",
      "Batch 45, Loss: 0.931835, Accuracy: 79.55%\n",
      "Batch 46, Loss: 0.879666, Accuracy: 79.65%\n",
      "Batch 47, Loss: 0.912104, Accuracy: 79.72%\n",
      "Batch 48, Loss: 0.911743, Accuracy: 79.79%\n",
      "Batch 49, Loss: 0.979288, Accuracy: 79.69%\n",
      "Batch 50, Loss: 0.948360, Accuracy: 79.69%\n",
      "Batch 51, Loss: 0.955444, Accuracy: 79.66%\n",
      "Batch 52, Loss: 0.896360, Accuracy: 79.75%\n",
      "Batch 53, Loss: 0.930687, Accuracy: 79.78%\n",
      "Batch 54, Loss: 0.897293, Accuracy: 79.86%\n",
      "Batch 55, Loss: 0.904947, Accuracy: 79.91%\n",
      "Batch 56, Loss: 0.900752, Accuracy: 79.97%\n",
      "Batch 57, Loss: 1.012282, Accuracy: 79.85%\n",
      "Batch 58, Loss: 0.979723, Accuracy: 79.82%\n",
      "Batch 59, Loss: 0.973844, Accuracy: 79.74%\n",
      "Batch 60, Loss: 0.901240, Accuracy: 79.79%\n",
      "Batch 61, Loss: 0.872832, Accuracy: 79.92%\n",
      "Batch 62, Loss: 0.887706, Accuracy: 80.02%\n",
      "Batch 63, Loss: 0.905586, Accuracy: 80.06%\n",
      "Batch 64, Loss: 1.032504, Accuracy: 79.96%\n",
      "Batch 65, Loss: 0.918557, Accuracy: 80.00%\n",
      "Batch 66, Loss: 1.053747, Accuracy: 79.81%\n",
      "Batch 67, Loss: 0.990350, Accuracy: 79.71%\n",
      "Batch 68, Loss: 0.920142, Accuracy: 79.73%\n",
      "Batch 69, Loss: 0.965142, Accuracy: 79.73%\n",
      "Batch 70, Loss: 0.886920, Accuracy: 79.84%\n",
      "Batch 71, Loss: 0.978657, Accuracy: 79.80%\n",
      "Batch 72, Loss: 0.979949, Accuracy: 79.75%\n",
      "Batch 73, Loss: 0.969879, Accuracy: 79.71%\n",
      "Batch 74, Loss: 0.906559, Accuracy: 79.77%\n",
      "Batch 75, Loss: 0.907299, Accuracy: 79.83%\n",
      "Batch 76, Loss: 0.978374, Accuracy: 79.79%\n",
      "Batch 77, Loss: 0.907289, Accuracy: 79.83%\n",
      "Batch 78, Loss: 0.917670, Accuracy: 79.89%\n",
      "Batch 79, Loss: 0.891101, Accuracy: 79.96%\n",
      "Batch 80, Loss: 0.924237, Accuracy: 80.00%\n",
      "Batch 81, Loss: 0.864493, Accuracy: 80.11%\n",
      "Batch 82, Loss: 0.963849, Accuracy: 80.09%\n",
      "Batch 83, Loss: 0.898577, Accuracy: 80.16%\n",
      "Batch 84, Loss: 0.980845, Accuracy: 80.12%\n",
      "Batch 85, Loss: 0.902419, Accuracy: 80.17%\n",
      "Batch 86, Loss: 0.932533, Accuracy: 80.18%\n",
      "Batch 87, Loss: 0.935931, Accuracy: 80.19%\n",
      "Batch 88, Loss: 0.900609, Accuracy: 80.26%\n",
      "Batch 89, Loss: 0.936426, Accuracy: 80.25%\n",
      "Batch 90, Loss: 0.915409, Accuracy: 80.28%\n",
      "Batch 91, Loss: 1.009795, Accuracy: 80.19%\n",
      "Batch 92, Loss: 0.962548, Accuracy: 80.16%\n",
      "Batch 93, Loss: 0.904417, Accuracy: 80.19%\n",
      "Batch 94, Loss: 0.983685, Accuracy: 80.12%\n",
      "Batch 95, Loss: 0.976772, Accuracy: 80.08%\n",
      "Batch 96, Loss: 0.947795, Accuracy: 80.06%\n",
      "Batch 97, Loss: 0.910529, Accuracy: 80.07%\n",
      "Batch 98, Loss: 0.999723, Accuracy: 80.02%\n",
      "Batch 99, Loss: 0.972330, Accuracy: 79.99%\n",
      "Batch 100, Loss: 0.849447, Accuracy: 80.08%\n",
      "Batch 101, Loss: 0.914934, Accuracy: 80.11%\n",
      "Batch 102, Loss: 0.956709, Accuracy: 80.09%\n",
      "Batch 103, Loss: 0.953572, Accuracy: 80.07%\n",
      "Batch 104, Loss: 0.847876, Accuracy: 80.15%\n",
      "Batch 105, Loss: 0.959326, Accuracy: 80.12%\n",
      "Batch 106, Loss: 1.014123, Accuracy: 80.04%\n",
      "Batch 107, Loss: 1.023292, Accuracy: 79.96%\n",
      "Batch 108, Loss: 0.879424, Accuracy: 80.02%\n",
      "Batch 109, Loss: 0.940697, Accuracy: 80.02%\n",
      "Batch 110, Loss: 1.007650, Accuracy: 79.96%\n",
      "Batch 111, Loss: 0.987804, Accuracy: 79.91%\n",
      "Batch 112, Loss: 0.970921, Accuracy: 79.88%\n",
      "Batch 113, Loss: 0.902028, Accuracy: 79.92%\n",
      "Batch 114, Loss: 0.919174, Accuracy: 79.96%\n",
      "Batch 115, Loss: 0.973646, Accuracy: 79.95%\n",
      "Batch 116, Loss: 0.867918, Accuracy: 80.01%\n",
      "Batch 117, Loss: 0.941881, Accuracy: 80.01%\n",
      "Batch 118, Loss: 0.936545, Accuracy: 80.01%\n",
      "Batch 119, Loss: 0.943042, Accuracy: 80.00%\n",
      "Batch 120, Loss: 0.945265, Accuracy: 80.01%\n",
      "Batch 121, Loss: 0.909435, Accuracy: 80.02%\n",
      "Batch 122, Loss: 0.941901, Accuracy: 80.02%\n",
      "Batch 123, Loss: 0.904674, Accuracy: 80.06%\n",
      "Batch 124, Loss: 0.914649, Accuracy: 80.08%\n",
      "Batch 125, Loss: 1.008051, Accuracy: 80.01%\n",
      "Batch 126, Loss: 0.839979, Accuracy: 80.10%\n",
      "Batch 127, Loss: 0.897900, Accuracy: 80.13%\n",
      "Batch 128, Loss: 0.914278, Accuracy: 80.15%\n",
      "Batch 129, Loss: 0.912882, Accuracy: 80.17%\n",
      "Batch 130, Loss: 0.979585, Accuracy: 80.16%\n",
      "Batch 131, Loss: 0.859069, Accuracy: 80.24%\n",
      "Batch 132, Loss: 0.992252, Accuracy: 80.20%\n",
      "Batch 133, Loss: 1.003629, Accuracy: 80.13%\n",
      "Batch 134, Loss: 0.888581, Accuracy: 80.18%\n",
      "Batch 135, Loss: 0.903022, Accuracy: 80.21%\n",
      "Batch 136, Loss: 0.920281, Accuracy: 80.23%\n",
      "Batch 137, Loss: 0.849654, Accuracy: 80.29%\n",
      "Batch 138, Loss: 0.952436, Accuracy: 80.29%\n",
      "Batch 139, Loss: 1.036496, Accuracy: 80.22%\n",
      "Batch 140, Loss: 0.938264, Accuracy: 80.21%\n",
      "Batch 141, Loss: 0.905647, Accuracy: 80.23%\n",
      "Batch 142, Loss: 0.955658, Accuracy: 80.23%\n",
      "Batch 143, Loss: 0.977733, Accuracy: 80.20%\n",
      "Batch 144, Loss: 1.038256, Accuracy: 80.12%\n",
      "Batch 145, Loss: 0.959130, Accuracy: 80.11%\n",
      "Batch 146, Loss: 0.948498, Accuracy: 80.12%\n",
      "Batch 147, Loss: 0.963054, Accuracy: 80.09%\n",
      "Batch 148, Loss: 0.849650, Accuracy: 80.15%\n",
      "Batch 149, Loss: 0.928679, Accuracy: 80.16%\n",
      "Batch 150, Loss: 0.923382, Accuracy: 80.18%\n",
      "Batch 151, Loss: 0.899849, Accuracy: 80.19%\n",
      "Batch 152, Loss: 0.884592, Accuracy: 80.23%\n",
      "Batch 153, Loss: 0.946877, Accuracy: 80.22%\n",
      "Batch 154, Loss: 1.002004, Accuracy: 80.17%\n",
      "Batch 155, Loss: 0.901086, Accuracy: 80.20%\n",
      "Batch 156, Loss: 0.966491, Accuracy: 80.19%\n",
      "Batch 157, Loss: 0.977530, Accuracy: 80.17%\n",
      "Batch 158, Loss: 0.937161, Accuracy: 80.16%\n",
      "Batch 159, Loss: 0.886465, Accuracy: 80.19%\n",
      "Batch 160, Loss: 0.982732, Accuracy: 80.16%\n",
      "Batch 161, Loss: 0.907630, Accuracy: 80.17%\n",
      "Batch 162, Loss: 0.932547, Accuracy: 80.18%\n",
      "Batch 163, Loss: 0.927978, Accuracy: 80.19%\n",
      "Batch 164, Loss: 0.910545, Accuracy: 80.20%\n",
      "Batch 165, Loss: 0.944814, Accuracy: 80.20%\n",
      "Batch 166, Loss: 1.025848, Accuracy: 80.15%\n",
      "Batch 167, Loss: 0.994761, Accuracy: 80.10%\n",
      "Batch 168, Loss: 0.995441, Accuracy: 80.07%\n",
      "Batch 169, Loss: 0.940891, Accuracy: 80.07%\n",
      "Batch 170, Loss: 0.907954, Accuracy: 80.09%\n",
      "Batch 171, Loss: 0.987408, Accuracy: 80.05%\n",
      "Batch 172, Loss: 0.890725, Accuracy: 80.09%\n",
      "Batch 173, Loss: 0.941936, Accuracy: 80.09%\n",
      "Batch 174, Loss: 0.914466, Accuracy: 80.12%\n",
      "Batch 175, Loss: 0.968942, Accuracy: 80.10%\n",
      "Batch 176, Loss: 0.866530, Accuracy: 80.14%\n",
      "Batch 177, Loss: 0.913100, Accuracy: 80.16%\n",
      "Batch 178, Loss: 0.895009, Accuracy: 80.19%\n",
      "Batch 179, Loss: 0.947376, Accuracy: 80.18%\n",
      "Batch 180, Loss: 0.905837, Accuracy: 80.20%\n",
      "Batch 181, Loss: 1.005051, Accuracy: 80.17%\n",
      "Batch 182, Loss: 0.971312, Accuracy: 80.15%\n",
      "Batch 183, Loss: 0.889191, Accuracy: 80.17%\n",
      "Batch 184, Loss: 0.893055, Accuracy: 80.19%\n",
      "Batch 185, Loss: 0.951208, Accuracy: 80.19%\n",
      "Batch 186, Loss: 0.878217, Accuracy: 80.22%\n",
      "Batch 187, Loss: 0.899293, Accuracy: 80.25%\n",
      "Batch 188, Loss: 0.908421, Accuracy: 80.26%\n",
      "Batch 189, Loss: 0.830947, Accuracy: 80.32%\n",
      "Batch 190, Loss: 0.932115, Accuracy: 80.31%\n",
      "Batch 191, Loss: 0.934345, Accuracy: 80.32%\n",
      "Batch 192, Loss: 0.890137, Accuracy: 80.35%\n",
      "Batch 193, Loss: 1.010629, Accuracy: 80.31%\n",
      "Batch 194, Loss: 0.973747, Accuracy: 80.28%\n",
      "Batch 195, Loss: 0.907191, Accuracy: 80.30%\n",
      "Batch 196, Loss: 0.913745, Accuracy: 80.31%\n",
      "Batch 197, Loss: 1.034604, Accuracy: 80.26%\n",
      "Batch 198, Loss: 0.902275, Accuracy: 80.30%\n",
      "Batch 199, Loss: 0.871343, Accuracy: 80.33%\n",
      "Batch 200, Loss: 0.949741, Accuracy: 80.32%\n",
      "Batch 201, Loss: 0.962844, Accuracy: 80.30%\n",
      "Batch 202, Loss: 1.001244, Accuracy: 80.27%\n",
      "Batch 203, Loss: 0.958288, Accuracy: 80.26%\n",
      "Batch 204, Loss: 0.884412, Accuracy: 80.29%\n",
      "Batch 205, Loss: 0.921355, Accuracy: 80.30%\n",
      "Batch 206, Loss: 0.957554, Accuracy: 80.29%\n",
      "Batch 207, Loss: 0.902627, Accuracy: 80.30%\n",
      "Batch 208, Loss: 0.861753, Accuracy: 80.33%\n",
      "Batch 209, Loss: 0.870271, Accuracy: 80.37%\n",
      "Batch 210, Loss: 0.974059, Accuracy: 80.36%\n",
      "Batch 211, Loss: 0.907348, Accuracy: 80.36%\n",
      "Batch 212, Loss: 0.928303, Accuracy: 80.37%\n",
      "Batch 213, Loss: 0.956623, Accuracy: 80.35%\n",
      "Training - Epoch 119, Loss: 0.938228, Accuracy: 80.35%\n",
      "Validation Batch 1, Loss: 0.887914, Accuracy: 85.94%\n",
      "Validation Batch 2, Loss: 0.918784, Accuracy: 83.59%\n",
      "Validation Batch 3, Loss: 1.008129, Accuracy: 80.21%\n",
      "Validation Batch 4, Loss: 0.938793, Accuracy: 80.47%\n",
      "Validation Batch 5, Loss: 0.922984, Accuracy: 80.62%\n",
      "Validation Batch 6, Loss: 0.851643, Accuracy: 82.03%\n",
      "Validation Batch 7, Loss: 0.945877, Accuracy: 81.70%\n",
      "Validation Batch 8, Loss: 0.962463, Accuracy: 81.25%\n",
      "Validation Batch 9, Loss: 0.963902, Accuracy: 80.90%\n",
      "Validation Batch 10, Loss: 0.961777, Accuracy: 80.62%\n",
      "Validation Batch 11, Loss: 0.899019, Accuracy: 80.97%\n",
      "Validation Batch 12, Loss: 0.876314, Accuracy: 81.64%\n",
      "Validation Batch 13, Loss: 0.965191, Accuracy: 81.25%\n",
      "Validation Batch 14, Loss: 0.963613, Accuracy: 80.92%\n",
      "Validation Batch 15, Loss: 0.933997, Accuracy: 81.04%\n",
      "Validation Batch 16, Loss: 0.920813, Accuracy: 81.15%\n",
      "Validation Batch 17, Loss: 0.963279, Accuracy: 80.97%\n",
      "Validation Batch 18, Loss: 0.915513, Accuracy: 80.90%\n",
      "Validation Batch 19, Loss: 0.973822, Accuracy: 80.59%\n",
      "Validation Batch 20, Loss: 0.915615, Accuracy: 80.62%\n",
      "Validation Batch 21, Loss: 0.958332, Accuracy: 80.58%\n",
      "Validation Batch 22, Loss: 0.961065, Accuracy: 80.47%\n",
      "Validation Batch 23, Loss: 1.018935, Accuracy: 80.03%\n",
      "Validation Batch 24, Loss: 0.992688, Accuracy: 79.75%\n",
      "Validation Batch 25, Loss: 0.950079, Accuracy: 79.69%\n",
      "Validation Batch 26, Loss: 0.922628, Accuracy: 79.75%\n",
      "Validation Batch 27, Loss: 0.869679, Accuracy: 79.92%\n",
      "Validation - Epoch 119, Loss: 0.939365, Accuracy: 79.92%\n",
      "Patienceâ€”1\n",
      "Epoch 120\n",
      "Batch 1, Loss: 0.925546, Accuracy: 81.25%\n",
      "Batch 2, Loss: 1.031099, Accuracy: 75.78%\n",
      "Batch 3, Loss: 0.930196, Accuracy: 77.08%\n",
      "Batch 4, Loss: 0.951117, Accuracy: 77.34%\n",
      "Batch 5, Loss: 0.987561, Accuracy: 76.88%\n",
      "Batch 6, Loss: 0.934559, Accuracy: 77.34%\n",
      "Batch 7, Loss: 0.939328, Accuracy: 77.90%\n",
      "Batch 8, Loss: 0.826597, Accuracy: 79.69%\n",
      "Batch 9, Loss: 0.892464, Accuracy: 80.38%\n",
      "Batch 10, Loss: 0.917301, Accuracy: 80.47%\n",
      "Batch 11, Loss: 0.931450, Accuracy: 80.68%\n",
      "Batch 12, Loss: 0.920862, Accuracy: 80.86%\n",
      "Batch 13, Loss: 0.851118, Accuracy: 81.73%\n",
      "Batch 14, Loss: 0.943112, Accuracy: 81.58%\n",
      "Batch 15, Loss: 0.923142, Accuracy: 81.56%\n",
      "Batch 16, Loss: 0.946519, Accuracy: 81.45%\n",
      "Batch 17, Loss: 0.907771, Accuracy: 81.53%\n",
      "Batch 18, Loss: 0.906532, Accuracy: 81.68%\n",
      "Batch 19, Loss: 0.944934, Accuracy: 81.58%\n",
      "Batch 20, Loss: 0.902497, Accuracy: 81.64%\n",
      "Batch 21, Loss: 1.013556, Accuracy: 81.18%\n",
      "Batch 22, Loss: 0.905265, Accuracy: 81.25%\n",
      "Batch 23, Loss: 0.892450, Accuracy: 81.45%\n",
      "Batch 24, Loss: 0.988899, Accuracy: 81.25%\n",
      "Batch 25, Loss: 0.904644, Accuracy: 81.31%\n",
      "Batch 26, Loss: 0.945198, Accuracy: 81.25%\n",
      "Batch 27, Loss: 0.935197, Accuracy: 81.25%\n",
      "Batch 28, Loss: 0.955239, Accuracy: 81.14%\n",
      "Batch 29, Loss: 0.875214, Accuracy: 81.41%\n",
      "Batch 30, Loss: 0.859795, Accuracy: 81.61%\n",
      "Batch 31, Loss: 0.992452, Accuracy: 81.35%\n",
      "Batch 32, Loss: 0.884713, Accuracy: 81.49%\n",
      "Batch 33, Loss: 0.875846, Accuracy: 81.68%\n",
      "Batch 34, Loss: 0.911124, Accuracy: 81.76%\n",
      "Batch 35, Loss: 0.910294, Accuracy: 81.83%\n",
      "Batch 36, Loss: 0.923988, Accuracy: 81.81%\n",
      "Batch 37, Loss: 0.884599, Accuracy: 81.97%\n",
      "Batch 38, Loss: 0.841316, Accuracy: 82.20%\n",
      "Batch 39, Loss: 0.952037, Accuracy: 82.09%\n",
      "Batch 40, Loss: 0.859282, Accuracy: 82.23%\n",
      "Batch 41, Loss: 0.885415, Accuracy: 82.36%\n",
      "Batch 42, Loss: 0.951592, Accuracy: 82.22%\n",
      "Batch 43, Loss: 0.966296, Accuracy: 82.12%\n",
      "Batch 44, Loss: 0.887427, Accuracy: 82.21%\n",
      "Batch 45, Loss: 0.939801, Accuracy: 82.15%\n",
      "Batch 46, Loss: 0.949904, Accuracy: 82.10%\n",
      "Batch 47, Loss: 0.920965, Accuracy: 82.15%\n",
      "Batch 48, Loss: 0.853596, Accuracy: 82.32%\n",
      "Batch 49, Loss: 0.816464, Accuracy: 82.56%\n",
      "Batch 50, Loss: 0.918571, Accuracy: 82.53%\n",
      "Batch 51, Loss: 1.023540, Accuracy: 82.29%\n",
      "Batch 52, Loss: 0.931147, Accuracy: 82.27%\n",
      "Batch 53, Loss: 0.950861, Accuracy: 82.19%\n",
      "Batch 54, Loss: 0.879789, Accuracy: 82.29%\n",
      "Batch 55, Loss: 0.892287, Accuracy: 82.36%\n",
      "Batch 56, Loss: 0.903491, Accuracy: 82.39%\n",
      "Batch 57, Loss: 0.904314, Accuracy: 82.43%\n",
      "Batch 58, Loss: 0.877645, Accuracy: 82.52%\n",
      "Batch 59, Loss: 0.876633, Accuracy: 82.60%\n",
      "Batch 60, Loss: 0.983076, Accuracy: 82.50%\n",
      "Batch 61, Loss: 1.037956, Accuracy: 82.30%\n",
      "Batch 62, Loss: 0.915044, Accuracy: 82.33%\n",
      "Batch 63, Loss: 0.887245, Accuracy: 82.39%\n",
      "Batch 64, Loss: 0.905053, Accuracy: 82.40%\n",
      "Batch 65, Loss: 0.962727, Accuracy: 82.33%\n",
      "Batch 66, Loss: 0.943714, Accuracy: 82.32%\n",
      "Batch 67, Loss: 0.949419, Accuracy: 82.25%\n",
      "Batch 68, Loss: 0.862663, Accuracy: 82.35%\n",
      "Batch 69, Loss: 0.869731, Accuracy: 82.40%\n",
      "Batch 70, Loss: 0.862358, Accuracy: 82.50%\n",
      "Batch 71, Loss: 0.969430, Accuracy: 82.42%\n",
      "Batch 72, Loss: 0.965523, Accuracy: 82.36%\n",
      "Batch 73, Loss: 0.924311, Accuracy: 82.36%\n",
      "Batch 74, Loss: 0.871601, Accuracy: 82.43%\n",
      "Batch 75, Loss: 0.906264, Accuracy: 82.46%\n",
      "Batch 76, Loss: 0.981704, Accuracy: 82.40%\n",
      "Batch 77, Loss: 0.872784, Accuracy: 82.47%\n",
      "Batch 78, Loss: 0.911053, Accuracy: 82.47%\n",
      "Batch 79, Loss: 0.901653, Accuracy: 82.50%\n",
      "Batch 80, Loss: 0.909841, Accuracy: 82.52%\n",
      "Batch 81, Loss: 0.994872, Accuracy: 82.45%\n",
      "Batch 82, Loss: 0.933898, Accuracy: 82.43%\n",
      "Batch 83, Loss: 0.909169, Accuracy: 82.45%\n",
      "Batch 84, Loss: 0.976841, Accuracy: 82.40%\n",
      "Batch 85, Loss: 1.017248, Accuracy: 82.28%\n",
      "Batch 86, Loss: 0.969458, Accuracy: 82.21%\n",
      "Batch 87, Loss: 0.967618, Accuracy: 82.17%\n",
      "Batch 88, Loss: 0.995830, Accuracy: 82.10%\n",
      "Batch 89, Loss: 0.882788, Accuracy: 82.15%\n",
      "Batch 90, Loss: 0.927457, Accuracy: 82.15%\n",
      "Batch 91, Loss: 0.861260, Accuracy: 82.23%\n",
      "Batch 92, Loss: 0.919682, Accuracy: 82.25%\n",
      "Batch 93, Loss: 0.945792, Accuracy: 82.22%\n",
      "Batch 94, Loss: 0.999136, Accuracy: 82.15%\n",
      "Batch 95, Loss: 0.936173, Accuracy: 82.14%\n",
      "Batch 96, Loss: 0.908609, Accuracy: 82.15%\n",
      "Batch 97, Loss: 0.894658, Accuracy: 82.18%\n",
      "Batch 98, Loss: 0.939931, Accuracy: 82.17%\n",
      "Batch 99, Loss: 0.934575, Accuracy: 82.20%\n",
      "Batch 100, Loss: 0.867688, Accuracy: 82.27%\n",
      "Batch 101, Loss: 0.929347, Accuracy: 82.27%\n",
      "Batch 102, Loss: 0.947340, Accuracy: 82.23%\n",
      "Batch 103, Loss: 0.944383, Accuracy: 82.22%\n",
      "Batch 104, Loss: 0.940445, Accuracy: 82.20%\n",
      "Batch 105, Loss: 0.929089, Accuracy: 82.19%\n",
      "Batch 106, Loss: 0.873744, Accuracy: 82.22%\n",
      "Batch 107, Loss: 0.934287, Accuracy: 82.21%\n",
      "Batch 108, Loss: 0.876262, Accuracy: 82.25%\n",
      "Batch 109, Loss: 0.903473, Accuracy: 82.27%\n",
      "Batch 110, Loss: 0.969711, Accuracy: 82.22%\n",
      "Batch 111, Loss: 0.878893, Accuracy: 82.28%\n",
      "Batch 112, Loss: 0.951003, Accuracy: 82.25%\n",
      "Batch 113, Loss: 0.866789, Accuracy: 82.31%\n",
      "Batch 114, Loss: 0.905053, Accuracy: 82.32%\n",
      "Batch 115, Loss: 0.898566, Accuracy: 82.32%\n",
      "Batch 116, Loss: 0.879127, Accuracy: 82.37%\n",
      "Batch 117, Loss: 0.852211, Accuracy: 82.43%\n",
      "Batch 118, Loss: 0.892119, Accuracy: 82.45%\n",
      "Batch 119, Loss: 0.983318, Accuracy: 82.41%\n",
      "Batch 120, Loss: 0.962337, Accuracy: 82.34%\n",
      "Batch 121, Loss: 0.941106, Accuracy: 82.31%\n",
      "Batch 122, Loss: 0.990584, Accuracy: 82.25%\n",
      "Batch 123, Loss: 0.905835, Accuracy: 82.27%\n",
      "Batch 124, Loss: 0.984030, Accuracy: 82.20%\n",
      "Batch 125, Loss: 0.902022, Accuracy: 82.21%\n",
      "Batch 126, Loss: 0.882220, Accuracy: 82.23%\n",
      "Batch 127, Loss: 0.963645, Accuracy: 82.20%\n",
      "Batch 128, Loss: 0.964013, Accuracy: 82.17%\n",
      "Batch 129, Loss: 0.898845, Accuracy: 82.19%\n",
      "Batch 130, Loss: 0.858435, Accuracy: 82.25%\n",
      "Batch 131, Loss: 0.938922, Accuracy: 82.23%\n",
      "Batch 132, Loss: 0.917215, Accuracy: 82.22%\n",
      "Batch 133, Loss: 0.851723, Accuracy: 82.28%\n",
      "Batch 134, Loss: 0.921707, Accuracy: 82.26%\n",
      "Batch 135, Loss: 0.890176, Accuracy: 82.29%\n",
      "Batch 136, Loss: 0.897786, Accuracy: 82.30%\n",
      "Batch 137, Loss: 0.951782, Accuracy: 82.27%\n",
      "Batch 138, Loss: 0.865409, Accuracy: 82.31%\n",
      "Batch 139, Loss: 0.919164, Accuracy: 82.32%\n",
      "Batch 140, Loss: 0.877107, Accuracy: 82.34%\n",
      "Batch 141, Loss: 0.900698, Accuracy: 82.36%\n",
      "Batch 142, Loss: 0.957774, Accuracy: 82.33%\n",
      "Batch 143, Loss: 0.884100, Accuracy: 82.35%\n",
      "Batch 144, Loss: 0.908384, Accuracy: 82.35%\n",
      "Batch 145, Loss: 0.881598, Accuracy: 82.37%\n",
      "Batch 146, Loss: 0.918071, Accuracy: 82.37%\n",
      "Batch 147, Loss: 0.868639, Accuracy: 82.41%\n",
      "Batch 148, Loss: 0.845025, Accuracy: 82.46%\n",
      "Batch 149, Loss: 1.002635, Accuracy: 82.41%\n",
      "Batch 150, Loss: 0.862872, Accuracy: 82.46%\n",
      "Batch 151, Loss: 1.015728, Accuracy: 82.39%\n",
      "Batch 152, Loss: 0.938200, Accuracy: 82.38%\n",
      "Batch 153, Loss: 0.886029, Accuracy: 82.40%\n",
      "Batch 154, Loss: 0.948812, Accuracy: 82.37%\n",
      "Batch 155, Loss: 0.849586, Accuracy: 82.41%\n",
      "Batch 156, Loss: 0.864901, Accuracy: 82.45%\n",
      "Batch 157, Loss: 0.905461, Accuracy: 82.46%\n",
      "Batch 158, Loss: 0.945115, Accuracy: 82.45%\n",
      "Batch 159, Loss: 0.936302, Accuracy: 82.44%\n",
      "Batch 160, Loss: 0.941017, Accuracy: 82.42%\n",
      "Batch 161, Loss: 0.887994, Accuracy: 82.44%\n",
      "Batch 162, Loss: 0.893091, Accuracy: 82.47%\n",
      "Batch 163, Loss: 0.910548, Accuracy: 82.48%\n",
      "Batch 164, Loss: 0.913996, Accuracy: 82.48%\n",
      "Batch 165, Loss: 0.901171, Accuracy: 82.48%\n",
      "Batch 166, Loss: 0.916157, Accuracy: 82.47%\n",
      "Batch 167, Loss: 0.965045, Accuracy: 82.45%\n",
      "Batch 168, Loss: 0.824311, Accuracy: 82.51%\n",
      "Batch 169, Loss: 0.896761, Accuracy: 82.52%\n",
      "Batch 170, Loss: 0.882606, Accuracy: 82.54%\n",
      "Batch 171, Loss: 0.964551, Accuracy: 82.51%\n",
      "Batch 172, Loss: 0.926913, Accuracy: 82.51%\n",
      "Batch 173, Loss: 0.843880, Accuracy: 82.56%\n",
      "Batch 174, Loss: 0.871609, Accuracy: 82.59%\n",
      "Batch 175, Loss: 0.963609, Accuracy: 82.56%\n",
      "Batch 176, Loss: 0.955257, Accuracy: 82.54%\n",
      "Batch 177, Loss: 0.906436, Accuracy: 82.55%\n",
      "Batch 178, Loss: 1.011161, Accuracy: 82.50%\n",
      "Batch 179, Loss: 0.824403, Accuracy: 82.54%\n",
      "Batch 180, Loss: 0.923775, Accuracy: 82.53%\n",
      "Batch 181, Loss: 1.023711, Accuracy: 82.48%\n",
      "Batch 182, Loss: 0.902504, Accuracy: 82.48%\n",
      "Batch 183, Loss: 0.891256, Accuracy: 82.47%\n",
      "Batch 184, Loss: 0.972070, Accuracy: 82.44%\n",
      "Batch 185, Loss: 0.892911, Accuracy: 82.45%\n",
      "Batch 186, Loss: 0.903442, Accuracy: 82.45%\n",
      "Batch 187, Loss: 0.896123, Accuracy: 82.46%\n",
      "Batch 188, Loss: 0.873689, Accuracy: 82.49%\n",
      "Batch 189, Loss: 0.956294, Accuracy: 82.47%\n",
      "Batch 190, Loss: 0.903979, Accuracy: 82.48%\n",
      "Batch 191, Loss: 0.860339, Accuracy: 82.50%\n",
      "Batch 192, Loss: 0.875196, Accuracy: 82.54%\n",
      "Batch 193, Loss: 0.900310, Accuracy: 82.55%\n",
      "Batch 194, Loss: 0.931633, Accuracy: 82.55%\n",
      "Batch 195, Loss: 0.898390, Accuracy: 82.55%\n",
      "Batch 196, Loss: 0.920377, Accuracy: 82.54%\n",
      "Batch 197, Loss: 0.830187, Accuracy: 82.58%\n",
      "Batch 198, Loss: 0.891642, Accuracy: 82.60%\n",
      "Batch 199, Loss: 0.931684, Accuracy: 82.59%\n",
      "Batch 200, Loss: 0.912021, Accuracy: 82.59%\n",
      "Batch 201, Loss: 0.915033, Accuracy: 82.59%\n",
      "Batch 202, Loss: 0.864479, Accuracy: 82.61%\n",
      "Batch 203, Loss: 0.860498, Accuracy: 82.64%\n",
      "Batch 204, Loss: 1.004774, Accuracy: 82.60%\n",
      "Batch 205, Loss: 0.919997, Accuracy: 82.60%\n",
      "Batch 206, Loss: 0.911851, Accuracy: 82.60%\n",
      "Batch 207, Loss: 0.917359, Accuracy: 82.60%\n",
      "Batch 208, Loss: 0.982354, Accuracy: 82.58%\n",
      "Batch 209, Loss: 0.937804, Accuracy: 82.57%\n",
      "Batch 210, Loss: 0.893042, Accuracy: 82.58%\n",
      "Batch 211, Loss: 0.828855, Accuracy: 82.63%\n",
      "Batch 212, Loss: 0.875079, Accuracy: 82.65%\n",
      "Batch 213, Loss: 0.976295, Accuracy: 82.62%\n",
      "Training - Epoch 120, Loss: 0.917695, Accuracy: 82.62%\n",
      "Validation Batch 1, Loss: 0.843380, Accuracy: 90.62%\n",
      "Validation Batch 2, Loss: 0.843733, Accuracy: 91.41%\n",
      "Validation Batch 3, Loss: 0.957379, Accuracy: 86.98%\n",
      "Validation Batch 4, Loss: 0.890449, Accuracy: 86.72%\n",
      "Validation Batch 5, Loss: 0.870796, Accuracy: 86.88%\n",
      "Validation Batch 6, Loss: 0.809847, Accuracy: 88.28%\n",
      "Validation Batch 7, Loss: 0.897097, Accuracy: 87.50%\n",
      "Validation Batch 8, Loss: 0.921003, Accuracy: 86.91%\n",
      "Validation Batch 9, Loss: 0.907184, Accuracy: 86.63%\n",
      "Validation Batch 10, Loss: 0.936219, Accuracy: 85.78%\n",
      "Validation Batch 11, Loss: 0.855153, Accuracy: 86.08%\n",
      "Validation Batch 12, Loss: 0.840969, Accuracy: 86.46%\n",
      "Validation Batch 13, Loss: 0.874369, Accuracy: 86.54%\n",
      "Validation Batch 14, Loss: 0.920099, Accuracy: 86.27%\n",
      "Validation Batch 15, Loss: 0.910685, Accuracy: 86.04%\n",
      "Validation Batch 16, Loss: 0.891257, Accuracy: 86.04%\n",
      "Validation Batch 17, Loss: 0.914103, Accuracy: 85.94%\n",
      "Validation Batch 18, Loss: 0.848346, Accuracy: 86.20%\n",
      "Validation Batch 19, Loss: 0.903154, Accuracy: 86.10%\n",
      "Validation Batch 20, Loss: 0.885484, Accuracy: 86.09%\n",
      "Validation Batch 21, Loss: 0.927738, Accuracy: 85.79%\n",
      "Validation Batch 22, Loss: 0.889047, Accuracy: 85.80%\n",
      "Validation Batch 23, Loss: 0.955657, Accuracy: 85.33%\n",
      "Validation Batch 24, Loss: 0.963662, Accuracy: 84.96%\n",
      "Validation Batch 25, Loss: 0.881669, Accuracy: 85.06%\n",
      "Validation Batch 26, Loss: 0.896464, Accuracy: 85.04%\n",
      "Validation Batch 27, Loss: 0.817607, Accuracy: 85.20%\n",
      "Validation - Epoch 120, Loss: 0.890835, Accuracy: 85.20%\n",
      "Patienceâ€”0\n",
      "Epoch 121\n",
      "Batch 1, Loss: 0.922755, Accuracy: 81.25%\n",
      "Batch 2, Loss: 0.884093, Accuracy: 83.59%\n",
      "Batch 3, Loss: 0.911231, Accuracy: 82.81%\n",
      "Batch 4, Loss: 0.879206, Accuracy: 84.38%\n",
      "Batch 5, Loss: 0.887824, Accuracy: 84.38%\n",
      "Batch 6, Loss: 0.947187, Accuracy: 83.33%\n",
      "Batch 7, Loss: 0.963507, Accuracy: 82.59%\n",
      "Batch 8, Loss: 0.893071, Accuracy: 83.01%\n",
      "Batch 9, Loss: 0.980945, Accuracy: 82.12%\n",
      "Batch 10, Loss: 0.887590, Accuracy: 82.50%\n",
      "Batch 11, Loss: 0.931178, Accuracy: 81.96%\n",
      "Batch 12, Loss: 0.846506, Accuracy: 82.68%\n",
      "Batch 13, Loss: 0.913308, Accuracy: 82.69%\n",
      "Batch 14, Loss: 0.919004, Accuracy: 82.70%\n",
      "Batch 15, Loss: 0.917010, Accuracy: 82.81%\n",
      "Batch 16, Loss: 0.954810, Accuracy: 82.71%\n",
      "Batch 17, Loss: 0.960874, Accuracy: 82.54%\n",
      "Batch 18, Loss: 0.815917, Accuracy: 83.07%\n",
      "Batch 19, Loss: 0.903667, Accuracy: 83.14%\n",
      "Batch 20, Loss: 0.915674, Accuracy: 83.05%\n",
      "Batch 21, Loss: 0.985186, Accuracy: 82.74%\n",
      "Batch 22, Loss: 0.895228, Accuracy: 82.88%\n",
      "Batch 23, Loss: 0.928811, Accuracy: 82.88%\n",
      "Batch 24, Loss: 0.901069, Accuracy: 83.01%\n",
      "Batch 25, Loss: 0.874920, Accuracy: 83.25%\n",
      "Batch 26, Loss: 0.965283, Accuracy: 82.99%\n",
      "Batch 27, Loss: 0.944023, Accuracy: 82.81%\n",
      "Batch 28, Loss: 0.944941, Accuracy: 82.76%\n",
      "Batch 29, Loss: 0.950699, Accuracy: 82.60%\n",
      "Batch 30, Loss: 0.934294, Accuracy: 82.50%\n",
      "Batch 31, Loss: 0.855742, Accuracy: 82.71%\n",
      "Batch 32, Loss: 0.915083, Accuracy: 82.71%\n",
      "Batch 33, Loss: 0.955213, Accuracy: 82.58%\n",
      "Batch 34, Loss: 0.928222, Accuracy: 82.54%\n",
      "Batch 35, Loss: 0.872740, Accuracy: 82.68%\n",
      "Batch 36, Loss: 0.891742, Accuracy: 82.81%\n",
      "Batch 37, Loss: 0.950951, Accuracy: 82.69%\n",
      "Batch 38, Loss: 0.903596, Accuracy: 82.73%\n",
      "Batch 39, Loss: 0.914598, Accuracy: 82.73%\n",
      "Batch 40, Loss: 0.836342, Accuracy: 82.89%\n",
      "Batch 41, Loss: 0.956546, Accuracy: 82.74%\n",
      "Batch 42, Loss: 0.939223, Accuracy: 82.70%\n",
      "Batch 43, Loss: 0.981765, Accuracy: 82.56%\n",
      "Batch 44, Loss: 0.869606, Accuracy: 82.67%\n",
      "Batch 45, Loss: 0.986511, Accuracy: 82.50%\n",
      "Batch 46, Loss: 0.920151, Accuracy: 82.54%\n",
      "Batch 47, Loss: 0.956793, Accuracy: 82.45%\n",
      "Batch 48, Loss: 0.892070, Accuracy: 82.55%\n",
      "Batch 49, Loss: 0.874425, Accuracy: 82.68%\n",
      "Batch 50, Loss: 0.868623, Accuracy: 82.75%\n",
      "Batch 51, Loss: 0.953465, Accuracy: 82.66%\n",
      "Batch 52, Loss: 0.867050, Accuracy: 82.78%\n",
      "Batch 53, Loss: 0.838156, Accuracy: 82.90%\n",
      "Batch 54, Loss: 0.859653, Accuracy: 82.99%\n",
      "Batch 55, Loss: 0.930665, Accuracy: 82.93%\n",
      "Batch 56, Loss: 0.905153, Accuracy: 82.92%\n",
      "Batch 57, Loss: 0.974540, Accuracy: 82.81%\n",
      "Batch 58, Loss: 0.918866, Accuracy: 82.76%\n",
      "Batch 59, Loss: 0.930709, Accuracy: 82.73%\n",
      "Batch 60, Loss: 0.863645, Accuracy: 82.84%\n",
      "Batch 61, Loss: 0.945100, Accuracy: 82.79%\n",
      "Batch 62, Loss: 0.875868, Accuracy: 82.84%\n",
      "Batch 63, Loss: 0.944838, Accuracy: 82.79%\n",
      "Batch 64, Loss: 0.945480, Accuracy: 82.71%\n",
      "Batch 65, Loss: 0.959908, Accuracy: 82.62%\n",
      "Batch 66, Loss: 0.953179, Accuracy: 82.55%\n",
      "Batch 67, Loss: 0.932759, Accuracy: 82.56%\n",
      "Batch 68, Loss: 0.909209, Accuracy: 82.58%\n",
      "Batch 69, Loss: 0.883101, Accuracy: 82.65%\n",
      "Batch 70, Loss: 0.877921, Accuracy: 82.70%\n",
      "Batch 71, Loss: 0.908168, Accuracy: 82.72%\n",
      "Batch 72, Loss: 0.863864, Accuracy: 82.81%\n",
      "Batch 73, Loss: 0.934159, Accuracy: 82.77%\n",
      "Batch 74, Loss: 0.885460, Accuracy: 82.81%\n",
      "Batch 75, Loss: 0.864701, Accuracy: 82.88%\n",
      "Batch 76, Loss: 0.897880, Accuracy: 82.89%\n",
      "Batch 77, Loss: 0.888552, Accuracy: 82.91%\n",
      "Batch 78, Loss: 0.913916, Accuracy: 82.91%\n",
      "Batch 79, Loss: 0.922223, Accuracy: 82.91%\n",
      "Batch 80, Loss: 0.938142, Accuracy: 82.87%\n",
      "Batch 81, Loss: 0.969658, Accuracy: 82.81%\n",
      "Batch 82, Loss: 0.891902, Accuracy: 82.83%\n",
      "Batch 83, Loss: 0.936887, Accuracy: 82.81%\n",
      "Batch 84, Loss: 0.969325, Accuracy: 82.72%\n",
      "Batch 85, Loss: 0.884724, Accuracy: 82.76%\n",
      "Batch 86, Loss: 0.916273, Accuracy: 82.78%\n",
      "Batch 87, Loss: 1.042318, Accuracy: 82.61%\n",
      "Batch 88, Loss: 0.900628, Accuracy: 82.63%\n",
      "Batch 89, Loss: 0.904505, Accuracy: 82.64%\n",
      "Batch 90, Loss: 1.012697, Accuracy: 82.53%\n",
      "Batch 91, Loss: 0.902059, Accuracy: 82.55%\n",
      "Batch 92, Loss: 0.996168, Accuracy: 82.46%\n",
      "Batch 93, Loss: 0.919364, Accuracy: 82.48%\n",
      "Batch 94, Loss: 0.834901, Accuracy: 82.58%\n",
      "Batch 95, Loss: 0.917183, Accuracy: 82.58%\n",
      "Batch 96, Loss: 0.924879, Accuracy: 82.60%\n",
      "Batch 97, Loss: 0.907324, Accuracy: 82.60%\n",
      "Batch 98, Loss: 0.956691, Accuracy: 82.56%\n",
      "Batch 99, Loss: 0.956427, Accuracy: 82.53%\n",
      "Batch 100, Loss: 0.925081, Accuracy: 82.52%\n",
      "Batch 101, Loss: 0.987997, Accuracy: 82.46%\n",
      "Batch 102, Loss: 0.828881, Accuracy: 82.54%\n",
      "Batch 103, Loss: 0.913744, Accuracy: 82.55%\n",
      "Batch 104, Loss: 0.851165, Accuracy: 82.65%\n",
      "Batch 105, Loss: 0.970280, Accuracy: 82.59%\n",
      "Batch 106, Loss: 0.899380, Accuracy: 82.64%\n",
      "Batch 107, Loss: 0.908874, Accuracy: 82.65%\n",
      "Batch 108, Loss: 0.905226, Accuracy: 82.67%\n",
      "Batch 109, Loss: 0.920384, Accuracy: 82.65%\n",
      "Batch 110, Loss: 0.955775, Accuracy: 82.63%\n",
      "Batch 111, Loss: 0.948780, Accuracy: 82.59%\n",
      "Batch 112, Loss: 1.018290, Accuracy: 82.49%\n",
      "Batch 113, Loss: 0.855125, Accuracy: 82.56%\n",
      "Batch 114, Loss: 0.952341, Accuracy: 82.54%\n",
      "Batch 115, Loss: 0.914582, Accuracy: 82.55%\n",
      "Batch 116, Loss: 0.960081, Accuracy: 82.53%\n",
      "Batch 117, Loss: 0.885591, Accuracy: 82.56%\n",
      "Batch 118, Loss: 1.009906, Accuracy: 82.47%\n",
      "Batch 119, Loss: 0.821437, Accuracy: 82.55%\n",
      "Batch 120, Loss: 0.935983, Accuracy: 82.54%\n",
      "Batch 121, Loss: 0.938633, Accuracy: 82.52%\n",
      "Batch 122, Loss: 0.855875, Accuracy: 82.57%\n",
      "Batch 123, Loss: 0.837151, Accuracy: 82.65%\n",
      "Batch 124, Loss: 0.922952, Accuracy: 82.62%\n",
      "Batch 125, Loss: 0.912830, Accuracy: 82.62%\n",
      "Batch 126, Loss: 0.985982, Accuracy: 82.54%\n",
      "Batch 127, Loss: 0.904769, Accuracy: 82.54%\n",
      "Batch 128, Loss: 0.950200, Accuracy: 82.52%\n",
      "Batch 129, Loss: 0.831651, Accuracy: 82.59%\n",
      "Batch 130, Loss: 0.956495, Accuracy: 82.55%\n",
      "Batch 131, Loss: 0.907321, Accuracy: 82.56%\n",
      "Batch 132, Loss: 0.901991, Accuracy: 82.56%\n",
      "Batch 133, Loss: 0.942399, Accuracy: 82.55%\n",
      "Batch 134, Loss: 0.894341, Accuracy: 82.58%\n",
      "Batch 135, Loss: 0.940164, Accuracy: 82.57%\n",
      "Batch 136, Loss: 0.936591, Accuracy: 82.57%\n",
      "Batch 137, Loss: 0.916108, Accuracy: 82.57%\n",
      "Batch 138, Loss: 0.985332, Accuracy: 82.52%\n",
      "Batch 139, Loss: 0.925776, Accuracy: 82.52%\n",
      "Batch 140, Loss: 0.941678, Accuracy: 82.49%\n",
      "Batch 141, Loss: 0.940267, Accuracy: 82.47%\n",
      "Batch 142, Loss: 0.899886, Accuracy: 82.48%\n",
      "Batch 143, Loss: 0.821217, Accuracy: 82.54%\n",
      "Batch 144, Loss: 0.957686, Accuracy: 82.51%\n",
      "Batch 145, Loss: 0.858238, Accuracy: 82.55%\n",
      "Batch 146, Loss: 0.933425, Accuracy: 82.54%\n",
      "Batch 147, Loss: 0.909162, Accuracy: 82.55%\n",
      "Batch 148, Loss: 0.964679, Accuracy: 82.53%\n",
      "Batch 149, Loss: 0.911947, Accuracy: 82.53%\n",
      "Batch 150, Loss: 0.943037, Accuracy: 82.53%\n",
      "Batch 151, Loss: 0.906915, Accuracy: 82.54%\n",
      "Batch 152, Loss: 0.969634, Accuracy: 82.51%\n",
      "Batch 153, Loss: 0.906781, Accuracy: 82.52%\n",
      "Batch 154, Loss: 0.905717, Accuracy: 82.53%\n",
      "Batch 155, Loss: 0.886134, Accuracy: 82.56%\n",
      "Batch 156, Loss: 0.871000, Accuracy: 82.60%\n",
      "Batch 157, Loss: 0.819827, Accuracy: 82.66%\n",
      "Batch 158, Loss: 0.899132, Accuracy: 82.66%\n",
      "Batch 159, Loss: 0.923188, Accuracy: 82.66%\n",
      "Batch 160, Loss: 0.831716, Accuracy: 82.71%\n",
      "Batch 161, Loss: 0.909183, Accuracy: 82.72%\n",
      "Batch 162, Loss: 0.870820, Accuracy: 82.74%\n",
      "Batch 163, Loss: 0.825995, Accuracy: 82.79%\n",
      "Batch 164, Loss: 0.975610, Accuracy: 82.75%\n",
      "Batch 165, Loss: 0.951643, Accuracy: 82.73%\n",
      "Batch 166, Loss: 0.937389, Accuracy: 82.72%\n",
      "Batch 167, Loss: 0.978495, Accuracy: 82.68%\n",
      "Batch 168, Loss: 0.918201, Accuracy: 82.69%\n",
      "Batch 169, Loss: 0.878918, Accuracy: 82.71%\n",
      "Batch 170, Loss: 1.020835, Accuracy: 82.66%\n",
      "Batch 171, Loss: 0.971656, Accuracy: 82.62%\n",
      "Batch 172, Loss: 0.920057, Accuracy: 82.61%\n",
      "Batch 173, Loss: 0.907718, Accuracy: 82.62%\n",
      "Batch 174, Loss: 0.935152, Accuracy: 82.61%\n",
      "Batch 175, Loss: 0.929439, Accuracy: 82.59%\n",
      "Batch 176, Loss: 0.904686, Accuracy: 82.60%\n",
      "Batch 177, Loss: 0.943413, Accuracy: 82.58%\n",
      "Batch 178, Loss: 0.922427, Accuracy: 82.58%\n",
      "Batch 179, Loss: 0.926916, Accuracy: 82.58%\n",
      "Batch 180, Loss: 0.907269, Accuracy: 82.59%\n",
      "Batch 181, Loss: 0.918459, Accuracy: 82.58%\n",
      "Batch 182, Loss: 0.936747, Accuracy: 82.57%\n",
      "Batch 183, Loss: 0.898970, Accuracy: 82.59%\n",
      "Batch 184, Loss: 0.906022, Accuracy: 82.62%\n",
      "Batch 185, Loss: 0.974939, Accuracy: 82.58%\n",
      "Batch 186, Loss: 0.924859, Accuracy: 82.57%\n",
      "Batch 187, Loss: 0.907959, Accuracy: 82.58%\n",
      "Batch 188, Loss: 0.877612, Accuracy: 82.60%\n",
      "Batch 189, Loss: 0.913320, Accuracy: 82.60%\n",
      "Batch 190, Loss: 0.911449, Accuracy: 82.59%\n",
      "Batch 191, Loss: 0.943009, Accuracy: 82.57%\n",
      "Batch 192, Loss: 0.924369, Accuracy: 82.56%\n",
      "Batch 193, Loss: 0.839838, Accuracy: 82.61%\n",
      "Batch 194, Loss: 0.949385, Accuracy: 82.60%\n",
      "Batch 195, Loss: 0.888269, Accuracy: 82.61%\n",
      "Batch 196, Loss: 1.006319, Accuracy: 82.57%\n",
      "Batch 197, Loss: 0.984121, Accuracy: 82.53%\n",
      "Batch 198, Loss: 0.858319, Accuracy: 82.55%\n",
      "Batch 199, Loss: 0.877315, Accuracy: 82.57%\n",
      "Batch 200, Loss: 0.922062, Accuracy: 82.57%\n",
      "Batch 201, Loss: 0.898149, Accuracy: 82.59%\n",
      "Batch 202, Loss: 0.903870, Accuracy: 82.60%\n",
      "Batch 203, Loss: 0.911263, Accuracy: 82.60%\n",
      "Batch 204, Loss: 0.970819, Accuracy: 82.57%\n",
      "Batch 205, Loss: 0.853544, Accuracy: 82.61%\n",
      "Batch 206, Loss: 0.927663, Accuracy: 82.61%\n",
      "Batch 207, Loss: 0.869185, Accuracy: 82.64%\n",
      "Batch 208, Loss: 0.890480, Accuracy: 82.64%\n",
      "Batch 209, Loss: 0.874900, Accuracy: 82.66%\n",
      "Batch 210, Loss: 0.926683, Accuracy: 82.66%\n",
      "Batch 211, Loss: 0.909936, Accuracy: 82.66%\n",
      "Batch 212, Loss: 1.034030, Accuracy: 82.59%\n",
      "Batch 213, Loss: 0.947988, Accuracy: 82.58%\n",
      "Training - Epoch 121, Loss: 0.917814, Accuracy: 82.58%\n",
      "Validation Batch 1, Loss: 0.878898, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.893719, Accuracy: 85.16%\n",
      "Validation Batch 3, Loss: 1.003036, Accuracy: 81.25%\n",
      "Validation Batch 4, Loss: 0.919667, Accuracy: 81.64%\n",
      "Validation Batch 5, Loss: 0.917154, Accuracy: 81.88%\n",
      "Validation Batch 6, Loss: 0.851801, Accuracy: 83.07%\n",
      "Validation Batch 7, Loss: 0.934198, Accuracy: 82.81%\n",
      "Validation Batch 8, Loss: 0.967310, Accuracy: 82.23%\n",
      "Validation Batch 9, Loss: 0.957927, Accuracy: 81.94%\n",
      "Validation Batch 10, Loss: 0.960094, Accuracy: 81.56%\n",
      "Validation Batch 11, Loss: 0.896478, Accuracy: 81.82%\n",
      "Validation Batch 12, Loss: 0.875131, Accuracy: 82.29%\n",
      "Validation Batch 13, Loss: 0.962926, Accuracy: 81.85%\n",
      "Validation Batch 14, Loss: 0.960860, Accuracy: 81.58%\n",
      "Validation Batch 15, Loss: 0.919405, Accuracy: 81.67%\n",
      "Validation Batch 16, Loss: 0.911680, Accuracy: 81.84%\n",
      "Validation Batch 17, Loss: 0.966489, Accuracy: 81.62%\n",
      "Validation Batch 18, Loss: 0.904800, Accuracy: 81.68%\n",
      "Validation Batch 19, Loss: 0.975841, Accuracy: 81.33%\n",
      "Validation Batch 20, Loss: 0.906754, Accuracy: 81.41%\n",
      "Validation Batch 21, Loss: 0.952816, Accuracy: 81.32%\n",
      "Validation Batch 22, Loss: 0.955656, Accuracy: 81.11%\n",
      "Validation Batch 23, Loss: 1.008510, Accuracy: 80.77%\n",
      "Validation Batch 24, Loss: 0.993385, Accuracy: 80.53%\n",
      "Validation Batch 25, Loss: 0.941252, Accuracy: 80.50%\n",
      "Validation Batch 26, Loss: 0.916403, Accuracy: 80.65%\n",
      "Validation Batch 27, Loss: 0.859034, Accuracy: 80.86%\n",
      "Validation - Epoch 121, Loss: 0.933008, Accuracy: 80.86%\n",
      "Patienceâ€”1\n",
      "Epoch 122\n",
      "Batch 1, Loss: 0.901758, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.952604, Accuracy: 81.25%\n",
      "Batch 3, Loss: 0.981807, Accuracy: 79.69%\n",
      "Batch 4, Loss: 0.953809, Accuracy: 79.69%\n",
      "Batch 5, Loss: 0.873821, Accuracy: 81.25%\n",
      "Batch 6, Loss: 0.882527, Accuracy: 82.03%\n",
      "Batch 7, Loss: 0.914084, Accuracy: 82.37%\n",
      "Batch 8, Loss: 0.928720, Accuracy: 82.42%\n",
      "Batch 9, Loss: 0.986401, Accuracy: 81.25%\n",
      "Batch 10, Loss: 0.972549, Accuracy: 80.78%\n",
      "Batch 11, Loss: 0.977226, Accuracy: 80.11%\n",
      "Batch 12, Loss: 0.852567, Accuracy: 80.86%\n",
      "Batch 13, Loss: 0.851636, Accuracy: 81.73%\n",
      "Batch 14, Loss: 0.907150, Accuracy: 81.81%\n",
      "Batch 15, Loss: 0.944341, Accuracy: 81.46%\n",
      "Batch 16, Loss: 0.909829, Accuracy: 81.54%\n",
      "Batch 17, Loss: 0.927998, Accuracy: 81.53%\n",
      "Batch 18, Loss: 0.900503, Accuracy: 81.60%\n",
      "Batch 19, Loss: 0.969368, Accuracy: 81.33%\n",
      "Batch 20, Loss: 0.913291, Accuracy: 81.48%\n",
      "Batch 21, Loss: 0.945287, Accuracy: 81.47%\n",
      "Batch 22, Loss: 0.934409, Accuracy: 81.46%\n",
      "Batch 23, Loss: 0.852529, Accuracy: 81.79%\n",
      "Batch 24, Loss: 0.792010, Accuracy: 82.36%\n",
      "Batch 25, Loss: 0.931101, Accuracy: 82.38%\n",
      "Batch 26, Loss: 0.837817, Accuracy: 82.75%\n",
      "Batch 27, Loss: 0.971687, Accuracy: 82.52%\n",
      "Batch 28, Loss: 0.900292, Accuracy: 82.65%\n",
      "Batch 29, Loss: 0.932928, Accuracy: 82.60%\n",
      "Batch 30, Loss: 0.939544, Accuracy: 82.50%\n",
      "Batch 31, Loss: 0.938010, Accuracy: 82.36%\n",
      "Batch 32, Loss: 0.958813, Accuracy: 82.18%\n",
      "Batch 33, Loss: 0.916495, Accuracy: 82.20%\n",
      "Batch 34, Loss: 0.945026, Accuracy: 82.08%\n",
      "Batch 35, Loss: 0.902843, Accuracy: 82.10%\n",
      "Batch 36, Loss: 0.845315, Accuracy: 82.34%\n",
      "Batch 37, Loss: 0.919736, Accuracy: 82.35%\n",
      "Batch 38, Loss: 0.869873, Accuracy: 82.48%\n",
      "Batch 39, Loss: 0.873798, Accuracy: 82.65%\n",
      "Batch 40, Loss: 0.870535, Accuracy: 82.81%\n",
      "Batch 41, Loss: 0.947578, Accuracy: 82.77%\n",
      "Batch 42, Loss: 0.838437, Accuracy: 82.92%\n",
      "Batch 43, Loss: 0.888946, Accuracy: 82.99%\n",
      "Batch 44, Loss: 0.860729, Accuracy: 83.13%\n",
      "Batch 45, Loss: 0.942320, Accuracy: 82.99%\n",
      "Batch 46, Loss: 0.918339, Accuracy: 82.98%\n",
      "Batch 47, Loss: 0.958576, Accuracy: 82.95%\n",
      "Batch 48, Loss: 0.885132, Accuracy: 83.04%\n",
      "Batch 49, Loss: 0.871047, Accuracy: 83.13%\n",
      "Batch 50, Loss: 0.972966, Accuracy: 83.03%\n",
      "Batch 51, Loss: 0.943881, Accuracy: 82.97%\n",
      "Batch 52, Loss: 0.908314, Accuracy: 82.96%\n",
      "Batch 53, Loss: 0.881562, Accuracy: 83.05%\n",
      "Batch 54, Loss: 0.884139, Accuracy: 83.10%\n",
      "Batch 55, Loss: 0.942906, Accuracy: 83.04%\n",
      "Batch 56, Loss: 0.892487, Accuracy: 83.06%\n",
      "Batch 57, Loss: 0.873329, Accuracy: 83.17%\n",
      "Batch 58, Loss: 0.890217, Accuracy: 83.22%\n",
      "Batch 59, Loss: 0.853059, Accuracy: 83.32%\n",
      "Batch 60, Loss: 0.956516, Accuracy: 83.23%\n",
      "Batch 61, Loss: 0.913996, Accuracy: 83.22%\n",
      "Batch 62, Loss: 0.895028, Accuracy: 83.27%\n",
      "Batch 63, Loss: 0.879099, Accuracy: 83.31%\n",
      "Batch 64, Loss: 0.950403, Accuracy: 83.25%\n",
      "Batch 65, Loss: 0.904495, Accuracy: 83.25%\n",
      "Batch 66, Loss: 0.941356, Accuracy: 83.21%\n",
      "Batch 67, Loss: 0.906390, Accuracy: 83.26%\n",
      "Batch 68, Loss: 0.840202, Accuracy: 83.36%\n",
      "Batch 69, Loss: 0.978854, Accuracy: 83.24%\n",
      "Batch 70, Loss: 0.848786, Accuracy: 83.35%\n",
      "Batch 71, Loss: 0.927084, Accuracy: 83.34%\n",
      "Batch 72, Loss: 0.845233, Accuracy: 83.44%\n",
      "Batch 73, Loss: 0.841334, Accuracy: 83.56%\n",
      "Batch 74, Loss: 0.954560, Accuracy: 83.49%\n",
      "Batch 75, Loss: 0.972960, Accuracy: 83.40%\n",
      "Batch 76, Loss: 0.980110, Accuracy: 83.31%\n",
      "Batch 77, Loss: 0.979277, Accuracy: 83.20%\n",
      "Batch 78, Loss: 0.938308, Accuracy: 83.15%\n",
      "Batch 79, Loss: 0.835983, Accuracy: 83.25%\n",
      "Batch 80, Loss: 0.938041, Accuracy: 83.22%\n",
      "Batch 81, Loss: 0.898564, Accuracy: 83.24%\n",
      "Batch 82, Loss: 0.924417, Accuracy: 83.21%\n",
      "Batch 83, Loss: 0.934016, Accuracy: 83.19%\n",
      "Batch 84, Loss: 0.930608, Accuracy: 83.18%\n",
      "Batch 85, Loss: 0.944803, Accuracy: 83.14%\n",
      "Batch 86, Loss: 0.961245, Accuracy: 83.10%\n",
      "Batch 87, Loss: 0.879476, Accuracy: 83.14%\n",
      "Batch 88, Loss: 0.848111, Accuracy: 83.22%\n",
      "Batch 89, Loss: 0.926517, Accuracy: 83.20%\n",
      "Batch 90, Loss: 0.890668, Accuracy: 83.23%\n",
      "Batch 91, Loss: 0.960509, Accuracy: 83.19%\n",
      "Batch 92, Loss: 0.974661, Accuracy: 83.12%\n",
      "Batch 93, Loss: 0.887099, Accuracy: 83.17%\n",
      "Batch 94, Loss: 0.913823, Accuracy: 83.16%\n",
      "Batch 95, Loss: 0.827359, Accuracy: 83.27%\n",
      "Batch 96, Loss: 0.914192, Accuracy: 83.25%\n",
      "Batch 97, Loss: 0.925864, Accuracy: 83.23%\n",
      "Batch 98, Loss: 0.907399, Accuracy: 83.24%\n",
      "Batch 99, Loss: 0.949778, Accuracy: 83.21%\n",
      "Batch 100, Loss: 0.959925, Accuracy: 83.16%\n",
      "Batch 101, Loss: 0.946527, Accuracy: 83.14%\n",
      "Batch 102, Loss: 0.904600, Accuracy: 83.15%\n",
      "Batch 103, Loss: 0.911198, Accuracy: 83.16%\n",
      "Batch 104, Loss: 0.885288, Accuracy: 83.19%\n",
      "Batch 105, Loss: 0.867784, Accuracy: 83.21%\n",
      "Batch 106, Loss: 0.912721, Accuracy: 83.23%\n",
      "Batch 107, Loss: 0.994406, Accuracy: 83.15%\n",
      "Batch 108, Loss: 0.952145, Accuracy: 83.10%\n",
      "Batch 109, Loss: 0.881854, Accuracy: 83.13%\n",
      "Batch 110, Loss: 0.951755, Accuracy: 83.08%\n",
      "Batch 111, Loss: 0.954776, Accuracy: 83.04%\n",
      "Batch 112, Loss: 0.877077, Accuracy: 83.06%\n",
      "Batch 113, Loss: 0.856639, Accuracy: 83.13%\n",
      "Batch 114, Loss: 0.896540, Accuracy: 83.14%\n",
      "Batch 115, Loss: 0.845491, Accuracy: 83.21%\n",
      "Batch 116, Loss: 0.884256, Accuracy: 83.23%\n",
      "Batch 117, Loss: 0.882279, Accuracy: 83.27%\n",
      "Batch 118, Loss: 0.914591, Accuracy: 83.26%\n",
      "Batch 119, Loss: 0.923145, Accuracy: 83.25%\n",
      "Batch 120, Loss: 0.916313, Accuracy: 83.24%\n",
      "Batch 121, Loss: 0.935647, Accuracy: 83.23%\n",
      "Batch 122, Loss: 0.994763, Accuracy: 83.13%\n",
      "Batch 123, Loss: 0.849372, Accuracy: 83.19%\n",
      "Batch 124, Loss: 0.911927, Accuracy: 83.19%\n",
      "Batch 125, Loss: 0.934254, Accuracy: 83.16%\n",
      "Batch 126, Loss: 0.999931, Accuracy: 83.11%\n",
      "Batch 127, Loss: 0.927992, Accuracy: 83.08%\n",
      "Batch 128, Loss: 0.868501, Accuracy: 83.12%\n",
      "Batch 129, Loss: 0.919766, Accuracy: 83.12%\n",
      "Batch 130, Loss: 0.914786, Accuracy: 83.12%\n",
      "Batch 131, Loss: 0.920956, Accuracy: 83.11%\n",
      "Batch 132, Loss: 0.916566, Accuracy: 83.10%\n",
      "Batch 133, Loss: 0.942197, Accuracy: 83.07%\n",
      "Batch 134, Loss: 0.914167, Accuracy: 83.06%\n",
      "Batch 135, Loss: 0.914656, Accuracy: 83.04%\n",
      "Batch 136, Loss: 0.813454, Accuracy: 83.12%\n",
      "Batch 137, Loss: 0.890039, Accuracy: 83.13%\n",
      "Batch 138, Loss: 0.847377, Accuracy: 83.19%\n",
      "Batch 139, Loss: 0.909525, Accuracy: 83.18%\n",
      "Batch 140, Loss: 0.916903, Accuracy: 83.17%\n",
      "Batch 141, Loss: 0.856063, Accuracy: 83.21%\n",
      "Batch 142, Loss: 0.919641, Accuracy: 83.19%\n",
      "Batch 143, Loss: 0.971687, Accuracy: 83.14%\n",
      "Batch 144, Loss: 0.913691, Accuracy: 83.15%\n",
      "Batch 145, Loss: 0.888572, Accuracy: 83.17%\n",
      "Batch 146, Loss: 0.961158, Accuracy: 83.12%\n",
      "Batch 147, Loss: 0.924351, Accuracy: 83.12%\n",
      "Batch 148, Loss: 0.936849, Accuracy: 83.12%\n",
      "Batch 149, Loss: 0.946258, Accuracy: 83.10%\n",
      "Batch 150, Loss: 0.892394, Accuracy: 83.10%\n",
      "Batch 151, Loss: 0.869970, Accuracy: 83.13%\n",
      "Batch 152, Loss: 0.942077, Accuracy: 83.12%\n",
      "Batch 153, Loss: 0.964146, Accuracy: 83.09%\n",
      "Batch 154, Loss: 0.958987, Accuracy: 83.06%\n",
      "Batch 155, Loss: 0.859757, Accuracy: 83.08%\n",
      "Batch 156, Loss: 0.857591, Accuracy: 83.13%\n",
      "Batch 157, Loss: 0.854922, Accuracy: 83.17%\n",
      "Batch 158, Loss: 0.903062, Accuracy: 83.18%\n",
      "Batch 159, Loss: 0.905388, Accuracy: 83.19%\n",
      "Batch 160, Loss: 0.928172, Accuracy: 83.17%\n",
      "Batch 161, Loss: 0.886267, Accuracy: 83.19%\n",
      "Batch 162, Loss: 1.023884, Accuracy: 83.12%\n",
      "Batch 163, Loss: 0.948974, Accuracy: 83.09%\n",
      "Batch 164, Loss: 0.862244, Accuracy: 83.13%\n",
      "Batch 165, Loss: 0.937841, Accuracy: 83.11%\n",
      "Batch 166, Loss: 0.923621, Accuracy: 83.10%\n",
      "Batch 167, Loss: 0.919806, Accuracy: 83.10%\n",
      "Batch 168, Loss: 0.926195, Accuracy: 83.09%\n",
      "Batch 169, Loss: 0.875912, Accuracy: 83.11%\n",
      "Batch 170, Loss: 0.901117, Accuracy: 83.12%\n",
      "Batch 171, Loss: 0.936617, Accuracy: 83.10%\n",
      "Batch 172, Loss: 0.898547, Accuracy: 83.11%\n",
      "Batch 173, Loss: 0.916772, Accuracy: 83.11%\n",
      "Batch 174, Loss: 0.916017, Accuracy: 83.11%\n",
      "Batch 175, Loss: 0.868348, Accuracy: 83.13%\n",
      "Batch 176, Loss: 0.892092, Accuracy: 83.15%\n",
      "Batch 177, Loss: 0.937248, Accuracy: 83.14%\n",
      "Batch 178, Loss: 0.901490, Accuracy: 83.15%\n",
      "Batch 179, Loss: 0.916656, Accuracy: 83.14%\n",
      "Batch 180, Loss: 0.835470, Accuracy: 83.19%\n",
      "Batch 181, Loss: 0.892480, Accuracy: 83.20%\n",
      "Batch 182, Loss: 0.880188, Accuracy: 83.22%\n",
      "Batch 183, Loss: 0.982569, Accuracy: 83.17%\n",
      "Batch 184, Loss: 0.882252, Accuracy: 83.19%\n",
      "Batch 185, Loss: 0.988060, Accuracy: 83.14%\n",
      "Batch 186, Loss: 0.921998, Accuracy: 83.14%\n",
      "Batch 187, Loss: 0.873292, Accuracy: 83.16%\n",
      "Batch 188, Loss: 0.902993, Accuracy: 83.18%\n",
      "Batch 189, Loss: 0.915280, Accuracy: 83.17%\n",
      "Batch 190, Loss: 0.885482, Accuracy: 83.18%\n",
      "Batch 191, Loss: 0.799844, Accuracy: 83.24%\n",
      "Batch 192, Loss: 0.977647, Accuracy: 83.19%\n",
      "Batch 193, Loss: 0.927982, Accuracy: 83.18%\n",
      "Batch 194, Loss: 0.981919, Accuracy: 83.13%\n",
      "Batch 195, Loss: 0.925871, Accuracy: 83.12%\n",
      "Batch 196, Loss: 0.905679, Accuracy: 83.13%\n",
      "Batch 197, Loss: 0.910178, Accuracy: 83.13%\n",
      "Batch 198, Loss: 0.898066, Accuracy: 83.14%\n",
      "Batch 199, Loss: 0.878392, Accuracy: 83.15%\n",
      "Batch 200, Loss: 0.914397, Accuracy: 83.15%\n",
      "Batch 201, Loss: 0.934475, Accuracy: 83.13%\n",
      "Batch 202, Loss: 0.876476, Accuracy: 83.16%\n",
      "Batch 203, Loss: 0.977962, Accuracy: 83.13%\n",
      "Batch 204, Loss: 0.913387, Accuracy: 83.13%\n",
      "Batch 205, Loss: 0.971085, Accuracy: 83.10%\n",
      "Batch 206, Loss: 0.914806, Accuracy: 83.09%\n",
      "Batch 207, Loss: 0.915951, Accuracy: 83.08%\n",
      "Batch 208, Loss: 0.919947, Accuracy: 83.07%\n",
      "Batch 209, Loss: 0.930873, Accuracy: 83.06%\n",
      "Batch 210, Loss: 0.914373, Accuracy: 83.07%\n",
      "Batch 211, Loss: 0.917719, Accuracy: 83.06%\n",
      "Batch 212, Loss: 0.926737, Accuracy: 83.04%\n",
      "Batch 213, Loss: 0.871654, Accuracy: 83.06%\n",
      "Training - Epoch 122, Loss: 0.912831, Accuracy: 83.06%\n",
      "Validation Batch 1, Loss: 0.886802, Accuracy: 85.94%\n",
      "Validation Batch 2, Loss: 0.927205, Accuracy: 83.59%\n",
      "Validation Batch 3, Loss: 1.006818, Accuracy: 80.21%\n",
      "Validation Batch 4, Loss: 0.931467, Accuracy: 80.47%\n",
      "Validation Batch 5, Loss: 0.916032, Accuracy: 80.94%\n",
      "Validation Batch 6, Loss: 0.875294, Accuracy: 82.03%\n",
      "Validation Batch 7, Loss: 0.957347, Accuracy: 81.47%\n",
      "Validation Batch 8, Loss: 0.975905, Accuracy: 80.86%\n",
      "Validation Batch 9, Loss: 0.984035, Accuracy: 80.38%\n",
      "Validation Batch 10, Loss: 0.964698, Accuracy: 80.00%\n",
      "Validation Batch 11, Loss: 0.905602, Accuracy: 80.40%\n",
      "Validation Batch 12, Loss: 0.875703, Accuracy: 80.99%\n",
      "Validation Batch 13, Loss: 0.975153, Accuracy: 80.53%\n",
      "Validation Batch 14, Loss: 0.964972, Accuracy: 80.36%\n",
      "Validation Batch 15, Loss: 0.927550, Accuracy: 80.42%\n",
      "Validation Batch 16, Loss: 0.907804, Accuracy: 80.66%\n",
      "Validation Batch 17, Loss: 0.981797, Accuracy: 80.33%\n",
      "Validation Batch 18, Loss: 0.913078, Accuracy: 80.47%\n",
      "Validation Batch 19, Loss: 0.970478, Accuracy: 80.26%\n",
      "Validation Batch 20, Loss: 0.895742, Accuracy: 80.39%\n",
      "Validation Batch 21, Loss: 0.947241, Accuracy: 80.36%\n",
      "Validation Batch 22, Loss: 0.974378, Accuracy: 80.18%\n",
      "Validation Batch 23, Loss: 1.010422, Accuracy: 79.76%\n",
      "Validation Batch 24, Loss: 0.997386, Accuracy: 79.56%\n",
      "Validation Batch 25, Loss: 0.951882, Accuracy: 79.50%\n",
      "Validation Batch 26, Loss: 0.921017, Accuracy: 79.63%\n",
      "Validation Batch 27, Loss: 0.883490, Accuracy: 79.80%\n",
      "Validation - Epoch 122, Loss: 0.941826, Accuracy: 79.80%\n",
      "Patienceâ€”2\n",
      "Epoch 123\n",
      "Batch 1, Loss: 0.875497, Accuracy: 89.06%\n",
      "Batch 2, Loss: 0.972183, Accuracy: 83.59%\n",
      "Batch 3, Loss: 0.866377, Accuracy: 84.90%\n",
      "Batch 4, Loss: 0.944391, Accuracy: 83.98%\n",
      "Batch 5, Loss: 0.902602, Accuracy: 84.38%\n",
      "Batch 6, Loss: 0.889272, Accuracy: 84.64%\n",
      "Batch 7, Loss: 0.889968, Accuracy: 84.60%\n",
      "Batch 8, Loss: 0.839240, Accuracy: 85.16%\n",
      "Batch 9, Loss: 0.909620, Accuracy: 84.72%\n",
      "Batch 10, Loss: 0.843224, Accuracy: 85.31%\n",
      "Batch 11, Loss: 0.971683, Accuracy: 84.38%\n",
      "Batch 12, Loss: 0.899401, Accuracy: 84.38%\n",
      "Batch 13, Loss: 0.920729, Accuracy: 84.13%\n",
      "Batch 14, Loss: 0.964732, Accuracy: 83.48%\n",
      "Batch 15, Loss: 0.992927, Accuracy: 82.92%\n",
      "Batch 16, Loss: 0.868579, Accuracy: 83.30%\n",
      "Batch 17, Loss: 0.951882, Accuracy: 83.00%\n",
      "Batch 18, Loss: 0.946619, Accuracy: 82.81%\n",
      "Batch 19, Loss: 0.942300, Accuracy: 82.73%\n",
      "Batch 20, Loss: 0.943163, Accuracy: 82.50%\n",
      "Batch 21, Loss: 0.862945, Accuracy: 82.81%\n",
      "Batch 22, Loss: 0.940285, Accuracy: 82.67%\n",
      "Batch 23, Loss: 0.835686, Accuracy: 83.02%\n",
      "Batch 24, Loss: 0.935649, Accuracy: 82.94%\n",
      "Batch 25, Loss: 0.921507, Accuracy: 82.88%\n",
      "Batch 26, Loss: 0.912723, Accuracy: 82.87%\n",
      "Batch 27, Loss: 0.936101, Accuracy: 82.81%\n",
      "Batch 28, Loss: 0.830537, Accuracy: 83.15%\n",
      "Batch 29, Loss: 0.895485, Accuracy: 83.14%\n",
      "Batch 30, Loss: 0.904246, Accuracy: 83.23%\n",
      "Batch 31, Loss: 0.912152, Accuracy: 83.27%\n",
      "Batch 32, Loss: 0.914902, Accuracy: 83.30%\n",
      "Batch 33, Loss: 0.978321, Accuracy: 83.05%\n",
      "Batch 34, Loss: 0.904555, Accuracy: 83.13%\n",
      "Batch 35, Loss: 0.853068, Accuracy: 83.30%\n",
      "Batch 36, Loss: 0.921070, Accuracy: 83.29%\n",
      "Batch 37, Loss: 0.922211, Accuracy: 83.28%\n",
      "Batch 38, Loss: 0.901851, Accuracy: 83.26%\n",
      "Batch 39, Loss: 0.897733, Accuracy: 83.25%\n",
      "Batch 40, Loss: 0.905019, Accuracy: 83.24%\n",
      "Batch 41, Loss: 0.955296, Accuracy: 83.12%\n",
      "Batch 42, Loss: 0.903769, Accuracy: 83.15%\n",
      "Batch 43, Loss: 0.843245, Accuracy: 83.32%\n",
      "Batch 44, Loss: 0.835254, Accuracy: 83.49%\n",
      "Batch 45, Loss: 0.938162, Accuracy: 83.40%\n",
      "Batch 46, Loss: 0.862732, Accuracy: 83.49%\n",
      "Batch 47, Loss: 0.872990, Accuracy: 83.58%\n",
      "Batch 48, Loss: 0.924729, Accuracy: 83.53%\n",
      "Batch 49, Loss: 1.020841, Accuracy: 83.32%\n",
      "Batch 50, Loss: 0.938438, Accuracy: 83.25%\n",
      "Batch 51, Loss: 0.942580, Accuracy: 83.15%\n",
      "Batch 52, Loss: 0.990249, Accuracy: 82.99%\n",
      "Batch 53, Loss: 0.967862, Accuracy: 82.90%\n",
      "Batch 54, Loss: 0.898500, Accuracy: 82.93%\n",
      "Batch 55, Loss: 0.911439, Accuracy: 82.93%\n",
      "Batch 56, Loss: 0.938989, Accuracy: 82.87%\n",
      "Batch 57, Loss: 0.823982, Accuracy: 83.00%\n",
      "Batch 58, Loss: 0.891778, Accuracy: 83.08%\n",
      "Batch 59, Loss: 0.886567, Accuracy: 83.10%\n",
      "Batch 60, Loss: 0.912510, Accuracy: 83.12%\n",
      "Batch 61, Loss: 0.954634, Accuracy: 83.04%\n",
      "Batch 62, Loss: 0.891708, Accuracy: 83.09%\n",
      "Batch 63, Loss: 0.936840, Accuracy: 83.04%\n",
      "Batch 64, Loss: 0.925364, Accuracy: 83.03%\n",
      "Batch 65, Loss: 0.850699, Accuracy: 83.15%\n",
      "Batch 66, Loss: 0.885784, Accuracy: 83.19%\n",
      "Batch 67, Loss: 0.883431, Accuracy: 83.23%\n",
      "Batch 68, Loss: 0.864631, Accuracy: 83.32%\n",
      "Batch 69, Loss: 0.812791, Accuracy: 83.45%\n",
      "Batch 70, Loss: 0.839133, Accuracy: 83.53%\n",
      "Batch 71, Loss: 1.071954, Accuracy: 83.30%\n",
      "Batch 72, Loss: 0.919653, Accuracy: 83.27%\n",
      "Batch 73, Loss: 0.886823, Accuracy: 83.33%\n",
      "Batch 74, Loss: 0.928668, Accuracy: 83.28%\n",
      "Batch 75, Loss: 0.860047, Accuracy: 83.35%\n",
      "Batch 76, Loss: 0.900812, Accuracy: 83.35%\n",
      "Batch 77, Loss: 0.857241, Accuracy: 83.42%\n",
      "Batch 78, Loss: 0.870154, Accuracy: 83.45%\n",
      "Batch 79, Loss: 1.009735, Accuracy: 83.31%\n",
      "Batch 80, Loss: 0.869808, Accuracy: 83.36%\n",
      "Batch 81, Loss: 0.867540, Accuracy: 83.41%\n",
      "Batch 82, Loss: 0.998243, Accuracy: 83.29%\n",
      "Batch 83, Loss: 0.907822, Accuracy: 83.30%\n",
      "Batch 84, Loss: 0.958111, Accuracy: 83.26%\n",
      "Batch 85, Loss: 0.901792, Accuracy: 83.25%\n",
      "Batch 86, Loss: 0.924271, Accuracy: 83.25%\n",
      "Batch 87, Loss: 0.991597, Accuracy: 83.14%\n",
      "Batch 88, Loss: 0.917809, Accuracy: 83.11%\n",
      "Batch 89, Loss: 0.931015, Accuracy: 83.09%\n",
      "Batch 90, Loss: 0.906769, Accuracy: 83.11%\n",
      "Batch 91, Loss: 0.874299, Accuracy: 83.16%\n",
      "Batch 92, Loss: 0.918513, Accuracy: 83.15%\n",
      "Batch 93, Loss: 0.907870, Accuracy: 83.15%\n",
      "Batch 94, Loss: 0.939553, Accuracy: 83.13%\n",
      "Batch 95, Loss: 0.985700, Accuracy: 83.04%\n",
      "Batch 96, Loss: 0.912518, Accuracy: 83.04%\n",
      "Batch 97, Loss: 0.866971, Accuracy: 83.10%\n",
      "Batch 98, Loss: 0.898116, Accuracy: 83.12%\n",
      "Batch 99, Loss: 0.902660, Accuracy: 83.13%\n",
      "Batch 100, Loss: 0.933788, Accuracy: 83.11%\n",
      "Batch 101, Loss: 0.921714, Accuracy: 83.09%\n",
      "Batch 102, Loss: 0.993235, Accuracy: 83.03%\n",
      "Batch 103, Loss: 0.836286, Accuracy: 83.09%\n",
      "Batch 104, Loss: 0.946120, Accuracy: 83.04%\n",
      "Batch 105, Loss: 0.944029, Accuracy: 83.01%\n",
      "Batch 106, Loss: 0.958523, Accuracy: 82.96%\n",
      "Batch 107, Loss: 0.900162, Accuracy: 82.97%\n",
      "Batch 108, Loss: 0.869963, Accuracy: 83.03%\n",
      "Batch 109, Loss: 0.920810, Accuracy: 83.03%\n",
      "Batch 110, Loss: 0.951598, Accuracy: 82.98%\n",
      "Batch 111, Loss: 0.937690, Accuracy: 82.95%\n",
      "Batch 112, Loss: 0.931273, Accuracy: 82.94%\n",
      "Batch 113, Loss: 0.908332, Accuracy: 82.95%\n",
      "Batch 114, Loss: 0.971457, Accuracy: 82.91%\n",
      "Batch 115, Loss: 0.881528, Accuracy: 82.95%\n",
      "Batch 116, Loss: 0.841347, Accuracy: 83.01%\n",
      "Batch 117, Loss: 0.891965, Accuracy: 83.03%\n",
      "Batch 118, Loss: 0.881749, Accuracy: 83.08%\n",
      "Batch 119, Loss: 0.979698, Accuracy: 83.01%\n",
      "Batch 120, Loss: 0.864543, Accuracy: 83.05%\n",
      "Batch 121, Loss: 0.957672, Accuracy: 83.01%\n",
      "Batch 122, Loss: 0.993040, Accuracy: 82.93%\n",
      "Batch 123, Loss: 0.965304, Accuracy: 82.89%\n",
      "Batch 124, Loss: 0.954440, Accuracy: 82.84%\n",
      "Batch 125, Loss: 0.949767, Accuracy: 82.81%\n",
      "Batch 126, Loss: 0.910789, Accuracy: 82.81%\n",
      "Batch 127, Loss: 0.952643, Accuracy: 82.79%\n",
      "Batch 128, Loss: 0.937350, Accuracy: 82.76%\n",
      "Batch 129, Loss: 0.945181, Accuracy: 82.74%\n",
      "Batch 130, Loss: 0.947814, Accuracy: 82.72%\n",
      "Batch 131, Loss: 0.869809, Accuracy: 82.74%\n",
      "Batch 132, Loss: 0.949411, Accuracy: 82.72%\n",
      "Batch 133, Loss: 1.009686, Accuracy: 82.65%\n",
      "Batch 134, Loss: 0.874161, Accuracy: 82.70%\n",
      "Batch 135, Loss: 0.892095, Accuracy: 82.71%\n",
      "Batch 136, Loss: 0.869341, Accuracy: 82.76%\n",
      "Batch 137, Loss: 0.918570, Accuracy: 82.76%\n",
      "Batch 138, Loss: 0.949288, Accuracy: 82.73%\n",
      "Batch 139, Loss: 0.900592, Accuracy: 82.73%\n",
      "Batch 140, Loss: 0.862452, Accuracy: 82.77%\n",
      "Batch 141, Loss: 0.991176, Accuracy: 82.71%\n",
      "Batch 142, Loss: 0.889248, Accuracy: 82.72%\n",
      "Batch 143, Loss: 0.979167, Accuracy: 82.69%\n",
      "Batch 144, Loss: 0.891362, Accuracy: 82.70%\n",
      "Batch 145, Loss: 0.937385, Accuracy: 82.69%\n",
      "Batch 146, Loss: 0.863034, Accuracy: 82.74%\n",
      "Batch 147, Loss: 0.894809, Accuracy: 82.76%\n",
      "Batch 148, Loss: 0.925124, Accuracy: 82.76%\n",
      "Batch 149, Loss: 1.019787, Accuracy: 82.69%\n",
      "Batch 150, Loss: 0.937523, Accuracy: 82.68%\n",
      "Batch 151, Loss: 0.912156, Accuracy: 82.69%\n",
      "Batch 152, Loss: 0.894409, Accuracy: 82.71%\n",
      "Batch 153, Loss: 0.897441, Accuracy: 82.73%\n",
      "Batch 154, Loss: 0.928516, Accuracy: 82.71%\n",
      "Batch 155, Loss: 0.930014, Accuracy: 82.71%\n",
      "Batch 156, Loss: 0.833820, Accuracy: 82.76%\n",
      "Batch 157, Loss: 0.927371, Accuracy: 82.76%\n",
      "Batch 158, Loss: 0.903969, Accuracy: 82.76%\n",
      "Batch 159, Loss: 0.884727, Accuracy: 82.78%\n",
      "Batch 160, Loss: 0.891054, Accuracy: 82.80%\n",
      "Batch 161, Loss: 0.870663, Accuracy: 82.82%\n",
      "Batch 162, Loss: 0.866029, Accuracy: 82.85%\n",
      "Batch 163, Loss: 0.919174, Accuracy: 82.84%\n",
      "Batch 164, Loss: 0.945823, Accuracy: 82.82%\n",
      "Batch 165, Loss: 0.941931, Accuracy: 82.80%\n",
      "Batch 166, Loss: 0.899189, Accuracy: 82.81%\n",
      "Batch 167, Loss: 0.870337, Accuracy: 82.83%\n",
      "Batch 168, Loss: 0.914719, Accuracy: 82.83%\n",
      "Batch 169, Loss: 0.886131, Accuracy: 82.85%\n",
      "Batch 170, Loss: 0.923978, Accuracy: 82.85%\n",
      "Batch 171, Loss: 0.916274, Accuracy: 82.85%\n",
      "Batch 172, Loss: 0.808200, Accuracy: 82.91%\n",
      "Batch 173, Loss: 0.954474, Accuracy: 82.89%\n",
      "Batch 174, Loss: 1.011285, Accuracy: 82.82%\n",
      "Batch 175, Loss: 0.904821, Accuracy: 82.82%\n",
      "Batch 176, Loss: 0.883214, Accuracy: 82.84%\n",
      "Batch 177, Loss: 0.899666, Accuracy: 82.85%\n",
      "Batch 178, Loss: 0.907504, Accuracy: 82.86%\n",
      "Batch 179, Loss: 0.878281, Accuracy: 82.87%\n",
      "Batch 180, Loss: 0.931704, Accuracy: 82.86%\n",
      "Batch 181, Loss: 0.847618, Accuracy: 82.90%\n",
      "Batch 182, Loss: 0.931618, Accuracy: 82.89%\n",
      "Batch 183, Loss: 0.917310, Accuracy: 82.89%\n",
      "Batch 184, Loss: 0.911437, Accuracy: 82.88%\n",
      "Batch 185, Loss: 0.939452, Accuracy: 82.85%\n",
      "Batch 186, Loss: 0.918082, Accuracy: 82.85%\n",
      "Batch 187, Loss: 0.883878, Accuracy: 82.88%\n",
      "Batch 188, Loss: 0.870878, Accuracy: 82.90%\n",
      "Batch 189, Loss: 0.960057, Accuracy: 82.87%\n",
      "Batch 190, Loss: 0.905567, Accuracy: 82.87%\n",
      "Batch 191, Loss: 0.815480, Accuracy: 82.93%\n",
      "Batch 192, Loss: 0.880420, Accuracy: 82.94%\n",
      "Batch 193, Loss: 0.928175, Accuracy: 82.93%\n",
      "Batch 194, Loss: 0.885835, Accuracy: 82.97%\n",
      "Batch 195, Loss: 0.944419, Accuracy: 82.93%\n",
      "Batch 196, Loss: 0.950643, Accuracy: 82.91%\n",
      "Batch 197, Loss: 0.968672, Accuracy: 82.88%\n",
      "Batch 198, Loss: 0.989593, Accuracy: 82.83%\n",
      "Batch 199, Loss: 0.865734, Accuracy: 82.85%\n",
      "Batch 200, Loss: 0.941495, Accuracy: 82.84%\n",
      "Batch 201, Loss: 0.824229, Accuracy: 82.89%\n",
      "Batch 202, Loss: 0.983320, Accuracy: 82.84%\n",
      "Batch 203, Loss: 0.876278, Accuracy: 82.85%\n",
      "Batch 204, Loss: 0.928407, Accuracy: 82.84%\n",
      "Batch 205, Loss: 0.915011, Accuracy: 82.84%\n",
      "Batch 206, Loss: 0.875619, Accuracy: 82.87%\n",
      "Batch 207, Loss: 0.929644, Accuracy: 82.86%\n",
      "Batch 208, Loss: 0.817862, Accuracy: 82.91%\n",
      "Batch 209, Loss: 0.846924, Accuracy: 82.95%\n",
      "Batch 210, Loss: 0.925610, Accuracy: 82.94%\n",
      "Batch 211, Loss: 0.950480, Accuracy: 82.93%\n",
      "Batch 212, Loss: 0.845557, Accuracy: 82.96%\n",
      "Batch 213, Loss: 0.973222, Accuracy: 82.93%\n",
      "Training - Epoch 123, Loss: 0.913149, Accuracy: 82.93%\n",
      "Validation Batch 1, Loss: 0.853281, Accuracy: 90.62%\n",
      "Validation Batch 2, Loss: 0.858681, Accuracy: 90.62%\n",
      "Validation Batch 3, Loss: 0.981328, Accuracy: 85.42%\n",
      "Validation Batch 4, Loss: 0.898701, Accuracy: 85.16%\n",
      "Validation Batch 5, Loss: 0.891515, Accuracy: 85.00%\n",
      "Validation Batch 6, Loss: 0.842497, Accuracy: 85.68%\n",
      "Validation Batch 7, Loss: 0.920696, Accuracy: 85.27%\n",
      "Validation Batch 8, Loss: 0.951933, Accuracy: 84.38%\n",
      "Validation Batch 9, Loss: 0.933260, Accuracy: 84.03%\n",
      "Validation Batch 10, Loss: 0.955430, Accuracy: 83.44%\n",
      "Validation Batch 11, Loss: 0.878723, Accuracy: 83.66%\n",
      "Validation Batch 12, Loss: 0.859723, Accuracy: 84.11%\n",
      "Validation Batch 13, Loss: 0.932184, Accuracy: 83.89%\n",
      "Validation Batch 14, Loss: 0.934957, Accuracy: 83.71%\n",
      "Validation Batch 15, Loss: 0.913042, Accuracy: 83.65%\n",
      "Validation Batch 16, Loss: 0.902256, Accuracy: 83.69%\n",
      "Validation Batch 17, Loss: 0.947611, Accuracy: 83.46%\n",
      "Validation Batch 18, Loss: 0.880220, Accuracy: 83.51%\n",
      "Validation Batch 19, Loss: 0.943276, Accuracy: 83.22%\n",
      "Validation Batch 20, Loss: 0.893256, Accuracy: 83.20%\n",
      "Validation Batch 21, Loss: 0.943342, Accuracy: 83.04%\n",
      "Validation Batch 22, Loss: 0.934168, Accuracy: 82.88%\n",
      "Validation Batch 23, Loss: 0.986856, Accuracy: 82.61%\n",
      "Validation Batch 24, Loss: 0.973629, Accuracy: 82.36%\n",
      "Validation Batch 25, Loss: 0.919655, Accuracy: 82.38%\n",
      "Validation Batch 26, Loss: 0.909963, Accuracy: 82.39%\n",
      "Validation Batch 27, Loss: 0.844493, Accuracy: 82.56%\n",
      "Validation - Epoch 123, Loss: 0.914247, Accuracy: 82.56%\n",
      "Patienceâ€”3\n",
      "Epoch 124\n",
      "Batch 1, Loss: 0.949692, Accuracy: 79.69%\n",
      "Batch 2, Loss: 0.878434, Accuracy: 82.81%\n",
      "Batch 3, Loss: 0.953109, Accuracy: 81.77%\n",
      "Batch 4, Loss: 0.966543, Accuracy: 80.86%\n",
      "Batch 5, Loss: 0.815244, Accuracy: 83.12%\n",
      "Batch 6, Loss: 0.910991, Accuracy: 82.81%\n",
      "Batch 7, Loss: 0.921372, Accuracy: 82.59%\n",
      "Batch 8, Loss: 0.895023, Accuracy: 83.01%\n",
      "Batch 9, Loss: 0.916852, Accuracy: 82.81%\n",
      "Batch 10, Loss: 0.970371, Accuracy: 82.34%\n",
      "Batch 11, Loss: 0.873236, Accuracy: 82.81%\n",
      "Batch 12, Loss: 0.893732, Accuracy: 82.94%\n",
      "Batch 13, Loss: 0.873600, Accuracy: 83.05%\n",
      "Batch 14, Loss: 0.940743, Accuracy: 82.92%\n",
      "Batch 15, Loss: 0.947867, Accuracy: 82.60%\n",
      "Batch 16, Loss: 0.940480, Accuracy: 82.52%\n",
      "Batch 17, Loss: 0.923920, Accuracy: 82.54%\n",
      "Batch 18, Loss: 0.949720, Accuracy: 82.38%\n",
      "Batch 19, Loss: 0.929095, Accuracy: 82.32%\n",
      "Batch 20, Loss: 0.870054, Accuracy: 82.66%\n",
      "Batch 21, Loss: 0.881145, Accuracy: 82.89%\n",
      "Batch 22, Loss: 0.909638, Accuracy: 82.95%\n",
      "Batch 23, Loss: 0.911638, Accuracy: 83.02%\n",
      "Batch 24, Loss: 0.884416, Accuracy: 83.07%\n",
      "Batch 25, Loss: 0.855217, Accuracy: 83.38%\n",
      "Batch 26, Loss: 0.920192, Accuracy: 83.41%\n",
      "Batch 27, Loss: 0.832379, Accuracy: 83.68%\n",
      "Batch 28, Loss: 0.990199, Accuracy: 83.37%\n",
      "Batch 29, Loss: 0.865061, Accuracy: 83.57%\n",
      "Batch 30, Loss: 0.943074, Accuracy: 83.39%\n",
      "Batch 31, Loss: 0.928274, Accuracy: 83.32%\n",
      "Batch 32, Loss: 0.924963, Accuracy: 83.25%\n",
      "Batch 33, Loss: 0.932261, Accuracy: 83.19%\n",
      "Batch 34, Loss: 0.888671, Accuracy: 83.32%\n",
      "Batch 35, Loss: 0.860860, Accuracy: 83.44%\n",
      "Batch 36, Loss: 0.882744, Accuracy: 83.55%\n",
      "Batch 37, Loss: 0.966366, Accuracy: 83.40%\n",
      "Batch 38, Loss: 0.910771, Accuracy: 83.39%\n",
      "Batch 39, Loss: 0.936958, Accuracy: 83.41%\n",
      "Batch 40, Loss: 0.900967, Accuracy: 83.44%\n",
      "Batch 41, Loss: 0.943838, Accuracy: 83.35%\n",
      "Batch 42, Loss: 0.868843, Accuracy: 83.44%\n",
      "Batch 43, Loss: 0.958246, Accuracy: 83.28%\n",
      "Batch 44, Loss: 0.977415, Accuracy: 83.10%\n",
      "Batch 45, Loss: 0.900931, Accuracy: 83.09%\n",
      "Batch 46, Loss: 0.878610, Accuracy: 83.19%\n",
      "Batch 47, Loss: 0.930942, Accuracy: 83.14%\n",
      "Batch 48, Loss: 0.903553, Accuracy: 83.14%\n",
      "Batch 49, Loss: 0.880982, Accuracy: 83.16%\n",
      "Batch 50, Loss: 0.934298, Accuracy: 83.12%\n",
      "Batch 51, Loss: 0.897450, Accuracy: 83.12%\n",
      "Batch 52, Loss: 0.909809, Accuracy: 83.08%\n",
      "Batch 53, Loss: 0.947663, Accuracy: 83.02%\n",
      "Batch 54, Loss: 0.884422, Accuracy: 83.04%\n",
      "Batch 55, Loss: 0.951591, Accuracy: 82.98%\n",
      "Batch 56, Loss: 0.907822, Accuracy: 82.98%\n",
      "Batch 57, Loss: 0.965485, Accuracy: 82.87%\n",
      "Batch 58, Loss: 0.917134, Accuracy: 82.87%\n",
      "Batch 59, Loss: 0.941183, Accuracy: 82.87%\n",
      "Batch 60, Loss: 0.926559, Accuracy: 82.81%\n",
      "Batch 61, Loss: 0.901119, Accuracy: 82.86%\n",
      "Batch 62, Loss: 0.907595, Accuracy: 82.89%\n",
      "Batch 63, Loss: 0.878016, Accuracy: 82.96%\n",
      "Batch 64, Loss: 0.906226, Accuracy: 82.96%\n",
      "Batch 65, Loss: 0.916391, Accuracy: 82.98%\n",
      "Batch 66, Loss: 0.830385, Accuracy: 83.12%\n",
      "Batch 67, Loss: 0.909631, Accuracy: 83.12%\n",
      "Batch 68, Loss: 0.873551, Accuracy: 83.16%\n",
      "Batch 69, Loss: 0.923857, Accuracy: 83.15%\n",
      "Batch 70, Loss: 0.890283, Accuracy: 83.19%\n",
      "Batch 71, Loss: 0.942439, Accuracy: 83.14%\n",
      "Batch 72, Loss: 0.868949, Accuracy: 83.20%\n",
      "Batch 73, Loss: 0.923762, Accuracy: 83.18%\n",
      "Batch 74, Loss: 0.954580, Accuracy: 83.11%\n",
      "Batch 75, Loss: 0.876229, Accuracy: 83.17%\n",
      "Batch 76, Loss: 0.867593, Accuracy: 83.20%\n",
      "Batch 77, Loss: 0.858838, Accuracy: 83.28%\n",
      "Batch 78, Loss: 0.966695, Accuracy: 83.19%\n",
      "Batch 79, Loss: 0.892171, Accuracy: 83.23%\n",
      "Batch 80, Loss: 0.877813, Accuracy: 83.28%\n",
      "Batch 81, Loss: 0.887514, Accuracy: 83.31%\n",
      "Batch 82, Loss: 0.938722, Accuracy: 83.27%\n",
      "Batch 83, Loss: 0.850095, Accuracy: 83.36%\n",
      "Batch 84, Loss: 0.914921, Accuracy: 83.33%\n",
      "Batch 85, Loss: 0.943304, Accuracy: 83.31%\n",
      "Batch 86, Loss: 0.945018, Accuracy: 83.27%\n",
      "Batch 87, Loss: 0.912438, Accuracy: 83.26%\n",
      "Batch 88, Loss: 0.891056, Accuracy: 83.29%\n",
      "Batch 89, Loss: 0.918831, Accuracy: 83.30%\n",
      "Batch 90, Loss: 0.934643, Accuracy: 83.32%\n",
      "Batch 91, Loss: 0.964989, Accuracy: 83.22%\n",
      "Batch 92, Loss: 0.896578, Accuracy: 83.24%\n",
      "Batch 93, Loss: 0.838150, Accuracy: 83.32%\n",
      "Batch 94, Loss: 0.887345, Accuracy: 83.38%\n",
      "Batch 95, Loss: 0.925003, Accuracy: 83.36%\n",
      "Batch 96, Loss: 0.994022, Accuracy: 83.25%\n",
      "Batch 97, Loss: 0.887139, Accuracy: 83.26%\n",
      "Batch 98, Loss: 0.915434, Accuracy: 83.24%\n",
      "Batch 99, Loss: 0.913981, Accuracy: 83.24%\n",
      "Batch 100, Loss: 0.891378, Accuracy: 83.27%\n",
      "Batch 101, Loss: 0.907922, Accuracy: 83.28%\n",
      "Batch 102, Loss: 0.902799, Accuracy: 83.27%\n",
      "Batch 103, Loss: 0.878157, Accuracy: 83.30%\n",
      "Batch 104, Loss: 0.932804, Accuracy: 83.28%\n",
      "Batch 105, Loss: 0.902402, Accuracy: 83.27%\n",
      "Batch 106, Loss: 0.915204, Accuracy: 83.27%\n",
      "Batch 107, Loss: 0.915759, Accuracy: 83.27%\n",
      "Batch 108, Loss: 0.929095, Accuracy: 83.26%\n",
      "Batch 109, Loss: 0.892908, Accuracy: 83.29%\n",
      "Batch 110, Loss: 1.095593, Accuracy: 83.10%\n",
      "Batch 111, Loss: 0.929695, Accuracy: 83.08%\n",
      "Batch 112, Loss: 0.945014, Accuracy: 83.04%\n",
      "Batch 113, Loss: 0.885198, Accuracy: 83.08%\n",
      "Batch 114, Loss: 0.895496, Accuracy: 83.10%\n",
      "Batch 115, Loss: 0.852982, Accuracy: 83.17%\n",
      "Batch 116, Loss: 0.882998, Accuracy: 83.19%\n",
      "Batch 117, Loss: 0.901603, Accuracy: 83.20%\n",
      "Batch 118, Loss: 0.969607, Accuracy: 83.16%\n",
      "Batch 119, Loss: 0.917452, Accuracy: 83.15%\n",
      "Batch 120, Loss: 0.991989, Accuracy: 83.09%\n",
      "Batch 121, Loss: 0.837627, Accuracy: 83.15%\n",
      "Batch 122, Loss: 0.873805, Accuracy: 83.20%\n",
      "Batch 123, Loss: 0.897876, Accuracy: 83.21%\n",
      "Batch 124, Loss: 0.939438, Accuracy: 83.18%\n",
      "Batch 125, Loss: 0.861451, Accuracy: 83.22%\n",
      "Batch 126, Loss: 0.859825, Accuracy: 83.27%\n",
      "Batch 127, Loss: 0.919787, Accuracy: 83.27%\n",
      "Batch 128, Loss: 0.921710, Accuracy: 83.26%\n",
      "Batch 129, Loss: 0.848027, Accuracy: 83.31%\n",
      "Batch 130, Loss: 0.911595, Accuracy: 83.32%\n",
      "Batch 131, Loss: 0.869165, Accuracy: 83.35%\n",
      "Batch 132, Loss: 0.983180, Accuracy: 83.29%\n",
      "Batch 133, Loss: 0.867084, Accuracy: 83.32%\n",
      "Batch 134, Loss: 0.915007, Accuracy: 83.31%\n",
      "Batch 135, Loss: 0.878713, Accuracy: 83.34%\n",
      "Batch 136, Loss: 0.860210, Accuracy: 83.39%\n",
      "Batch 137, Loss: 0.959210, Accuracy: 83.35%\n",
      "Batch 138, Loss: 0.894988, Accuracy: 83.36%\n",
      "Batch 139, Loss: 0.868114, Accuracy: 83.39%\n",
      "Batch 140, Loss: 0.889875, Accuracy: 83.40%\n",
      "Batch 141, Loss: 0.879997, Accuracy: 83.42%\n",
      "Batch 142, Loss: 0.895554, Accuracy: 83.44%\n",
      "Batch 143, Loss: 0.851461, Accuracy: 83.49%\n",
      "Batch 144, Loss: 0.896240, Accuracy: 83.50%\n",
      "Batch 145, Loss: 1.002677, Accuracy: 83.44%\n",
      "Batch 146, Loss: 0.909343, Accuracy: 83.43%\n",
      "Batch 147, Loss: 0.972801, Accuracy: 83.39%\n",
      "Batch 148, Loss: 0.945340, Accuracy: 83.36%\n",
      "Batch 149, Loss: 1.009446, Accuracy: 83.26%\n",
      "Batch 150, Loss: 0.976235, Accuracy: 83.22%\n",
      "Batch 151, Loss: 0.900975, Accuracy: 83.23%\n",
      "Batch 152, Loss: 0.962430, Accuracy: 83.18%\n",
      "Batch 153, Loss: 0.889587, Accuracy: 83.19%\n",
      "Batch 154, Loss: 0.957734, Accuracy: 83.17%\n",
      "Batch 155, Loss: 0.931902, Accuracy: 83.15%\n",
      "Batch 156, Loss: 0.903939, Accuracy: 83.15%\n",
      "Batch 157, Loss: 0.929360, Accuracy: 83.15%\n",
      "Batch 158, Loss: 0.915324, Accuracy: 83.15%\n",
      "Batch 159, Loss: 0.949770, Accuracy: 83.13%\n",
      "Batch 160, Loss: 0.877574, Accuracy: 83.15%\n",
      "Batch 161, Loss: 0.859487, Accuracy: 83.19%\n",
      "Batch 162, Loss: 0.915890, Accuracy: 83.19%\n",
      "Batch 163, Loss: 0.852868, Accuracy: 83.22%\n",
      "Batch 164, Loss: 0.922211, Accuracy: 83.21%\n",
      "Batch 165, Loss: 0.884513, Accuracy: 83.23%\n",
      "Batch 166, Loss: 0.930177, Accuracy: 83.22%\n",
      "Batch 167, Loss: 0.855621, Accuracy: 83.26%\n",
      "Batch 168, Loss: 0.896219, Accuracy: 83.28%\n",
      "Batch 169, Loss: 0.974889, Accuracy: 83.24%\n",
      "Batch 170, Loss: 0.985112, Accuracy: 83.20%\n",
      "Batch 171, Loss: 0.944414, Accuracy: 83.18%\n",
      "Batch 172, Loss: 0.950107, Accuracy: 83.15%\n",
      "Batch 173, Loss: 0.971008, Accuracy: 83.10%\n",
      "Batch 174, Loss: 0.933077, Accuracy: 83.09%\n",
      "Batch 175, Loss: 0.903135, Accuracy: 83.09%\n",
      "Batch 176, Loss: 0.915851, Accuracy: 83.10%\n",
      "Batch 177, Loss: 0.846272, Accuracy: 83.14%\n",
      "Batch 178, Loss: 0.893600, Accuracy: 83.15%\n",
      "Batch 179, Loss: 0.885863, Accuracy: 83.16%\n",
      "Batch 180, Loss: 0.902657, Accuracy: 83.16%\n",
      "Batch 181, Loss: 0.941262, Accuracy: 83.15%\n",
      "Batch 182, Loss: 0.832563, Accuracy: 83.20%\n",
      "Batch 183, Loss: 0.941903, Accuracy: 83.18%\n",
      "Batch 184, Loss: 0.902424, Accuracy: 83.20%\n",
      "Batch 185, Loss: 0.873315, Accuracy: 83.22%\n",
      "Batch 186, Loss: 0.845577, Accuracy: 83.27%\n",
      "Batch 187, Loss: 0.867217, Accuracy: 83.29%\n",
      "Batch 188, Loss: 0.854801, Accuracy: 83.32%\n",
      "Batch 189, Loss: 0.936317, Accuracy: 83.31%\n",
      "Batch 190, Loss: 0.884402, Accuracy: 83.32%\n",
      "Batch 191, Loss: 0.901693, Accuracy: 83.33%\n",
      "Batch 192, Loss: 0.912874, Accuracy: 83.33%\n",
      "Batch 193, Loss: 0.893519, Accuracy: 83.32%\n",
      "Batch 194, Loss: 0.871967, Accuracy: 83.34%\n",
      "Batch 195, Loss: 0.968768, Accuracy: 83.31%\n",
      "Batch 196, Loss: 0.965532, Accuracy: 83.28%\n",
      "Batch 197, Loss: 0.894960, Accuracy: 83.29%\n",
      "Batch 198, Loss: 0.943219, Accuracy: 83.27%\n",
      "Batch 199, Loss: 0.853332, Accuracy: 83.30%\n",
      "Batch 200, Loss: 0.912000, Accuracy: 83.30%\n",
      "Batch 201, Loss: 0.879672, Accuracy: 83.31%\n",
      "Batch 202, Loss: 0.924782, Accuracy: 83.31%\n",
      "Batch 203, Loss: 0.888578, Accuracy: 83.32%\n",
      "Batch 204, Loss: 0.878146, Accuracy: 83.34%\n",
      "Batch 205, Loss: 0.896730, Accuracy: 83.35%\n",
      "Batch 206, Loss: 0.985603, Accuracy: 83.32%\n",
      "Batch 207, Loss: 0.865655, Accuracy: 83.34%\n",
      "Batch 208, Loss: 0.879210, Accuracy: 83.35%\n",
      "Batch 209, Loss: 0.902144, Accuracy: 83.36%\n",
      "Batch 210, Loss: 0.886270, Accuracy: 83.37%\n",
      "Batch 211, Loss: 0.972700, Accuracy: 83.34%\n",
      "Batch 212, Loss: 0.933339, Accuracy: 83.34%\n",
      "Batch 213, Loss: 0.916409, Accuracy: 83.33%\n",
      "Training - Epoch 124, Loss: 0.910696, Accuracy: 83.33%\n",
      "Validation Batch 1, Loss: 0.848929, Accuracy: 90.62%\n",
      "Validation Batch 2, Loss: 0.883816, Accuracy: 88.28%\n",
      "Validation Batch 3, Loss: 0.972215, Accuracy: 84.38%\n",
      "Validation Batch 4, Loss: 0.902654, Accuracy: 83.98%\n",
      "Validation Batch 5, Loss: 0.847678, Accuracy: 85.31%\n",
      "Validation Batch 6, Loss: 0.847989, Accuracy: 85.94%\n",
      "Validation Batch 7, Loss: 0.914789, Accuracy: 85.49%\n",
      "Validation Batch 8, Loss: 0.963386, Accuracy: 84.38%\n",
      "Validation Batch 9, Loss: 0.944177, Accuracy: 83.68%\n",
      "Validation Batch 10, Loss: 0.936271, Accuracy: 83.44%\n",
      "Validation Batch 11, Loss: 0.889718, Accuracy: 83.52%\n",
      "Validation Batch 12, Loss: 0.855629, Accuracy: 83.98%\n",
      "Validation Batch 13, Loss: 0.935200, Accuracy: 83.77%\n",
      "Validation Batch 14, Loss: 0.917272, Accuracy: 83.71%\n",
      "Validation Batch 15, Loss: 0.912751, Accuracy: 83.65%\n",
      "Validation Batch 16, Loss: 0.885004, Accuracy: 83.79%\n",
      "Validation Batch 17, Loss: 0.957649, Accuracy: 83.46%\n",
      "Validation Batch 18, Loss: 0.883795, Accuracy: 83.59%\n",
      "Validation Batch 19, Loss: 0.942144, Accuracy: 83.39%\n",
      "Validation Batch 20, Loss: 0.829768, Accuracy: 83.83%\n",
      "Validation Batch 21, Loss: 0.907768, Accuracy: 83.78%\n",
      "Validation Batch 22, Loss: 0.916407, Accuracy: 83.74%\n",
      "Validation Batch 23, Loss: 0.924799, Accuracy: 83.63%\n",
      "Validation Batch 24, Loss: 0.957215, Accuracy: 83.46%\n",
      "Validation Batch 25, Loss: 0.909527, Accuracy: 83.44%\n",
      "Validation Batch 26, Loss: 0.905456, Accuracy: 83.47%\n",
      "Validation Batch 27, Loss: 0.823980, Accuracy: 83.68%\n",
      "Validation - Epoch 124, Loss: 0.904296, Accuracy: 83.68%\n",
      "Patienceâ€”4\n",
      "Epoch 125\n",
      "Batch 1, Loss: 0.908521, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.972486, Accuracy: 79.69%\n",
      "Batch 3, Loss: 0.966440, Accuracy: 78.65%\n",
      "Batch 4, Loss: 0.897287, Accuracy: 80.08%\n",
      "Batch 5, Loss: 0.909575, Accuracy: 80.62%\n",
      "Batch 6, Loss: 0.915178, Accuracy: 80.99%\n",
      "Batch 7, Loss: 0.874120, Accuracy: 81.92%\n",
      "Batch 8, Loss: 0.885074, Accuracy: 82.42%\n",
      "Batch 9, Loss: 0.831875, Accuracy: 83.33%\n",
      "Batch 10, Loss: 0.856294, Accuracy: 83.91%\n",
      "Batch 11, Loss: 0.967497, Accuracy: 83.24%\n",
      "Batch 12, Loss: 0.995063, Accuracy: 82.16%\n",
      "Batch 13, Loss: 0.867030, Accuracy: 82.69%\n",
      "Batch 14, Loss: 0.967279, Accuracy: 82.14%\n",
      "Batch 15, Loss: 0.870625, Accuracy: 82.60%\n",
      "Batch 16, Loss: 0.914571, Accuracy: 82.62%\n",
      "Batch 17, Loss: 0.916732, Accuracy: 82.63%\n",
      "Batch 18, Loss: 0.873520, Accuracy: 82.99%\n",
      "Batch 19, Loss: 0.863113, Accuracy: 83.22%\n",
      "Batch 20, Loss: 0.848814, Accuracy: 83.59%\n",
      "Batch 21, Loss: 0.955510, Accuracy: 83.48%\n",
      "Batch 22, Loss: 0.947824, Accuracy: 83.31%\n",
      "Batch 23, Loss: 0.863997, Accuracy: 83.49%\n",
      "Batch 24, Loss: 0.901357, Accuracy: 83.53%\n",
      "Batch 25, Loss: 0.910816, Accuracy: 83.56%\n",
      "Batch 26, Loss: 0.892231, Accuracy: 83.65%\n",
      "Batch 27, Loss: 0.960415, Accuracy: 83.45%\n",
      "Batch 28, Loss: 0.866324, Accuracy: 83.59%\n",
      "Batch 29, Loss: 0.929539, Accuracy: 83.46%\n",
      "Batch 30, Loss: 0.847915, Accuracy: 83.65%\n",
      "Batch 31, Loss: 0.892259, Accuracy: 83.67%\n",
      "Batch 32, Loss: 1.000111, Accuracy: 83.35%\n",
      "Batch 33, Loss: 0.985972, Accuracy: 83.05%\n",
      "Batch 34, Loss: 0.911528, Accuracy: 83.04%\n",
      "Batch 35, Loss: 0.968338, Accuracy: 82.90%\n",
      "Batch 36, Loss: 0.917490, Accuracy: 82.86%\n",
      "Batch 37, Loss: 0.873220, Accuracy: 82.98%\n",
      "Batch 38, Loss: 0.855150, Accuracy: 83.14%\n",
      "Batch 39, Loss: 0.860705, Accuracy: 83.29%\n",
      "Batch 40, Loss: 0.923357, Accuracy: 83.24%\n",
      "Batch 41, Loss: 0.878880, Accuracy: 83.31%\n",
      "Batch 42, Loss: 0.932926, Accuracy: 83.22%\n",
      "Batch 43, Loss: 0.892460, Accuracy: 83.25%\n",
      "Batch 44, Loss: 0.934869, Accuracy: 83.17%\n",
      "Batch 45, Loss: 0.917163, Accuracy: 83.12%\n",
      "Batch 46, Loss: 0.850623, Accuracy: 83.25%\n",
      "Batch 47, Loss: 0.918956, Accuracy: 83.24%\n",
      "Batch 48, Loss: 0.890826, Accuracy: 83.30%\n",
      "Batch 49, Loss: 0.845045, Accuracy: 83.45%\n",
      "Batch 50, Loss: 0.921183, Accuracy: 83.44%\n",
      "Batch 51, Loss: 0.889755, Accuracy: 83.49%\n",
      "Batch 52, Loss: 0.916248, Accuracy: 83.47%\n",
      "Batch 53, Loss: 0.882894, Accuracy: 83.52%\n",
      "Batch 54, Loss: 0.939370, Accuracy: 83.48%\n",
      "Batch 55, Loss: 0.902726, Accuracy: 83.47%\n",
      "Batch 56, Loss: 0.993359, Accuracy: 83.29%\n",
      "Batch 57, Loss: 0.818046, Accuracy: 83.47%\n",
      "Batch 58, Loss: 0.906582, Accuracy: 83.46%\n",
      "Batch 59, Loss: 0.948681, Accuracy: 83.40%\n",
      "Batch 60, Loss: 1.010729, Accuracy: 83.23%\n",
      "Batch 61, Loss: 0.910784, Accuracy: 83.22%\n",
      "Batch 62, Loss: 0.858523, Accuracy: 83.32%\n",
      "Batch 63, Loss: 0.906669, Accuracy: 83.33%\n",
      "Batch 64, Loss: 0.910746, Accuracy: 83.35%\n",
      "Batch 65, Loss: 0.872947, Accuracy: 83.37%\n",
      "Batch 66, Loss: 0.890186, Accuracy: 83.36%\n",
      "Batch 67, Loss: 1.031669, Accuracy: 83.16%\n",
      "Batch 68, Loss: 0.948090, Accuracy: 83.13%\n",
      "Batch 69, Loss: 1.088125, Accuracy: 82.84%\n",
      "Batch 70, Loss: 0.868620, Accuracy: 82.90%\n",
      "Batch 71, Loss: 0.927607, Accuracy: 82.88%\n",
      "Batch 72, Loss: 0.923869, Accuracy: 82.90%\n",
      "Batch 73, Loss: 0.883378, Accuracy: 82.94%\n",
      "Batch 74, Loss: 0.917203, Accuracy: 82.92%\n",
      "Batch 75, Loss: 0.862743, Accuracy: 83.00%\n",
      "Batch 76, Loss: 0.888148, Accuracy: 83.06%\n",
      "Batch 77, Loss: 0.860017, Accuracy: 83.14%\n",
      "Batch 78, Loss: 0.906316, Accuracy: 83.13%\n",
      "Batch 79, Loss: 0.904067, Accuracy: 83.15%\n",
      "Batch 80, Loss: 0.910110, Accuracy: 83.14%\n",
      "Batch 81, Loss: 0.961680, Accuracy: 83.08%\n",
      "Batch 82, Loss: 0.920550, Accuracy: 83.06%\n",
      "Batch 83, Loss: 0.964648, Accuracy: 83.02%\n",
      "Batch 84, Loss: 0.932489, Accuracy: 82.96%\n",
      "Batch 85, Loss: 0.884721, Accuracy: 83.00%\n",
      "Batch 86, Loss: 0.859938, Accuracy: 83.07%\n",
      "Batch 87, Loss: 0.951077, Accuracy: 83.03%\n",
      "Batch 88, Loss: 0.939536, Accuracy: 82.97%\n",
      "Batch 89, Loss: 0.922309, Accuracy: 82.99%\n",
      "Batch 90, Loss: 0.893519, Accuracy: 83.00%\n",
      "Batch 91, Loss: 0.888715, Accuracy: 83.04%\n",
      "Batch 92, Loss: 0.855868, Accuracy: 83.08%\n",
      "Batch 93, Loss: 0.976665, Accuracy: 83.00%\n",
      "Batch 94, Loss: 0.886737, Accuracy: 83.00%\n",
      "Batch 95, Loss: 0.845954, Accuracy: 83.06%\n",
      "Batch 96, Loss: 0.886632, Accuracy: 83.09%\n",
      "Batch 97, Loss: 0.889710, Accuracy: 83.12%\n",
      "Batch 98, Loss: 0.851111, Accuracy: 83.18%\n",
      "Batch 99, Loss: 0.993958, Accuracy: 83.10%\n",
      "Batch 100, Loss: 0.879224, Accuracy: 83.14%\n",
      "Batch 101, Loss: 0.926063, Accuracy: 83.12%\n",
      "Batch 102, Loss: 0.911465, Accuracy: 83.12%\n",
      "Batch 103, Loss: 0.895673, Accuracy: 83.15%\n",
      "Batch 104, Loss: 0.918992, Accuracy: 83.16%\n",
      "Batch 105, Loss: 0.910844, Accuracy: 83.15%\n",
      "Batch 106, Loss: 0.946237, Accuracy: 83.12%\n",
      "Batch 107, Loss: 0.855327, Accuracy: 83.16%\n",
      "Batch 108, Loss: 0.950115, Accuracy: 83.15%\n",
      "Batch 109, Loss: 0.902069, Accuracy: 83.16%\n",
      "Batch 110, Loss: 0.858504, Accuracy: 83.21%\n",
      "Batch 111, Loss: 0.966230, Accuracy: 83.16%\n",
      "Batch 112, Loss: 0.873097, Accuracy: 83.19%\n",
      "Batch 113, Loss: 0.887006, Accuracy: 83.20%\n",
      "Batch 114, Loss: 0.881025, Accuracy: 83.22%\n",
      "Batch 115, Loss: 0.909327, Accuracy: 83.22%\n",
      "Batch 116, Loss: 0.879381, Accuracy: 83.24%\n",
      "Batch 117, Loss: 0.837949, Accuracy: 83.29%\n",
      "Batch 118, Loss: 0.882349, Accuracy: 83.32%\n",
      "Batch 119, Loss: 0.812522, Accuracy: 83.39%\n",
      "Batch 120, Loss: 0.929052, Accuracy: 83.39%\n",
      "Batch 121, Loss: 0.873057, Accuracy: 83.42%\n",
      "Batch 122, Loss: 0.876269, Accuracy: 83.47%\n",
      "Batch 123, Loss: 0.883708, Accuracy: 83.49%\n",
      "Batch 124, Loss: 0.903367, Accuracy: 83.49%\n",
      "Batch 125, Loss: 0.867210, Accuracy: 83.54%\n",
      "Batch 126, Loss: 0.928002, Accuracy: 83.53%\n",
      "Batch 127, Loss: 0.915349, Accuracy: 83.51%\n",
      "Batch 128, Loss: 0.824382, Accuracy: 83.58%\n",
      "Batch 129, Loss: 0.827280, Accuracy: 83.65%\n",
      "Batch 130, Loss: 0.909051, Accuracy: 83.64%\n",
      "Batch 131, Loss: 0.980915, Accuracy: 83.59%\n",
      "Batch 132, Loss: 0.879105, Accuracy: 83.62%\n",
      "Batch 133, Loss: 0.889339, Accuracy: 83.63%\n",
      "Batch 134, Loss: 0.961219, Accuracy: 83.61%\n",
      "Batch 135, Loss: 0.935743, Accuracy: 83.58%\n",
      "Batch 136, Loss: 0.876187, Accuracy: 83.59%\n",
      "Batch 137, Loss: 0.872728, Accuracy: 83.62%\n",
      "Batch 138, Loss: 0.896137, Accuracy: 83.63%\n",
      "Batch 139, Loss: 0.871088, Accuracy: 83.67%\n",
      "Batch 140, Loss: 1.001053, Accuracy: 83.60%\n",
      "Batch 141, Loss: 0.923102, Accuracy: 83.60%\n",
      "Batch 142, Loss: 0.877255, Accuracy: 83.63%\n",
      "Batch 143, Loss: 0.861152, Accuracy: 83.65%\n",
      "Batch 144, Loss: 0.987994, Accuracy: 83.59%\n",
      "Batch 145, Loss: 0.823545, Accuracy: 83.64%\n",
      "Batch 146, Loss: 0.936955, Accuracy: 83.63%\n",
      "Batch 147, Loss: 0.872324, Accuracy: 83.66%\n",
      "Batch 148, Loss: 0.905384, Accuracy: 83.68%\n",
      "Batch 149, Loss: 0.913306, Accuracy: 83.67%\n",
      "Batch 150, Loss: 0.897399, Accuracy: 83.67%\n",
      "Batch 151, Loss: 0.992976, Accuracy: 83.60%\n",
      "Batch 152, Loss: 0.978524, Accuracy: 83.55%\n",
      "Batch 153, Loss: 0.940517, Accuracy: 83.54%\n",
      "Batch 154, Loss: 0.806836, Accuracy: 83.60%\n",
      "Batch 155, Loss: 0.877747, Accuracy: 83.63%\n",
      "Batch 156, Loss: 0.896469, Accuracy: 83.64%\n",
      "Batch 157, Loss: 0.910993, Accuracy: 83.64%\n",
      "Batch 158, Loss: 0.894792, Accuracy: 83.65%\n",
      "Batch 159, Loss: 0.871352, Accuracy: 83.68%\n",
      "Batch 160, Loss: 0.913812, Accuracy: 83.68%\n",
      "Batch 161, Loss: 0.946259, Accuracy: 83.66%\n",
      "Batch 162, Loss: 0.976241, Accuracy: 83.60%\n",
      "Batch 163, Loss: 0.888015, Accuracy: 83.62%\n",
      "Batch 164, Loss: 0.916778, Accuracy: 83.61%\n",
      "Batch 165, Loss: 0.821187, Accuracy: 83.67%\n",
      "Batch 166, Loss: 0.904553, Accuracy: 83.68%\n",
      "Batch 167, Loss: 0.859513, Accuracy: 83.71%\n",
      "Batch 168, Loss: 0.809728, Accuracy: 83.77%\n",
      "Batch 169, Loss: 0.923422, Accuracy: 83.76%\n",
      "Batch 170, Loss: 0.926724, Accuracy: 83.75%\n",
      "Batch 171, Loss: 0.903607, Accuracy: 83.75%\n",
      "Batch 172, Loss: 0.899767, Accuracy: 83.76%\n",
      "Batch 173, Loss: 0.942181, Accuracy: 83.73%\n",
      "Batch 174, Loss: 0.938257, Accuracy: 83.72%\n",
      "Batch 175, Loss: 0.899904, Accuracy: 83.72%\n",
      "Batch 176, Loss: 0.925046, Accuracy: 83.72%\n",
      "Batch 177, Loss: 0.882431, Accuracy: 83.73%\n",
      "Batch 178, Loss: 0.873201, Accuracy: 83.75%\n",
      "Batch 179, Loss: 0.959062, Accuracy: 83.71%\n",
      "Batch 180, Loss: 0.834621, Accuracy: 83.75%\n",
      "Batch 181, Loss: 0.941473, Accuracy: 83.73%\n",
      "Batch 182, Loss: 0.957876, Accuracy: 83.71%\n",
      "Batch 183, Loss: 0.929612, Accuracy: 83.69%\n",
      "Batch 184, Loss: 0.908996, Accuracy: 83.70%\n",
      "Batch 185, Loss: 0.938706, Accuracy: 83.67%\n",
      "Batch 186, Loss: 0.926411, Accuracy: 83.66%\n",
      "Batch 187, Loss: 0.899072, Accuracy: 83.66%\n",
      "Batch 188, Loss: 0.895245, Accuracy: 83.66%\n",
      "Batch 189, Loss: 0.938223, Accuracy: 83.65%\n",
      "Batch 190, Loss: 0.963880, Accuracy: 83.61%\n",
      "Batch 191, Loss: 0.934842, Accuracy: 83.59%\n",
      "Batch 192, Loss: 0.932889, Accuracy: 83.58%\n",
      "Batch 193, Loss: 0.935109, Accuracy: 83.57%\n",
      "Batch 194, Loss: 0.855488, Accuracy: 83.59%\n",
      "Batch 195, Loss: 0.935147, Accuracy: 83.57%\n",
      "Batch 196, Loss: 0.924478, Accuracy: 83.55%\n",
      "Batch 197, Loss: 0.918778, Accuracy: 83.56%\n",
      "Batch 198, Loss: 0.898208, Accuracy: 83.56%\n",
      "Batch 199, Loss: 0.886989, Accuracy: 83.57%\n",
      "Batch 200, Loss: 0.909216, Accuracy: 83.57%\n",
      "Batch 201, Loss: 0.954025, Accuracy: 83.55%\n",
      "Batch 202, Loss: 0.863816, Accuracy: 83.58%\n",
      "Batch 203, Loss: 0.859401, Accuracy: 83.60%\n",
      "Batch 204, Loss: 0.928786, Accuracy: 83.59%\n",
      "Batch 205, Loss: 0.932966, Accuracy: 83.57%\n",
      "Batch 206, Loss: 0.860621, Accuracy: 83.60%\n",
      "Batch 207, Loss: 0.995546, Accuracy: 83.56%\n",
      "Batch 208, Loss: 0.924079, Accuracy: 83.55%\n",
      "Batch 209, Loss: 0.921641, Accuracy: 83.55%\n",
      "Batch 210, Loss: 0.872465, Accuracy: 83.56%\n",
      "Batch 211, Loss: 0.795140, Accuracy: 83.62%\n",
      "Batch 212, Loss: 0.875595, Accuracy: 83.64%\n",
      "Batch 213, Loss: 0.916012, Accuracy: 83.64%\n",
      "Training - Epoch 125, Loss: 0.906852, Accuracy: 83.64%\n",
      "Validation Batch 1, Loss: 0.854029, Accuracy: 90.62%\n",
      "Validation Batch 2, Loss: 0.879511, Accuracy: 88.28%\n",
      "Validation Batch 3, Loss: 0.994730, Accuracy: 83.33%\n",
      "Validation Batch 4, Loss: 0.911234, Accuracy: 83.20%\n",
      "Validation Batch 5, Loss: 0.876594, Accuracy: 83.75%\n",
      "Validation Batch 6, Loss: 0.849123, Accuracy: 84.90%\n",
      "Validation Batch 7, Loss: 0.922947, Accuracy: 84.38%\n",
      "Validation Batch 8, Loss: 0.960872, Accuracy: 83.59%\n",
      "Validation Batch 9, Loss: 0.959753, Accuracy: 82.99%\n",
      "Validation Batch 10, Loss: 0.946996, Accuracy: 82.66%\n",
      "Validation Batch 11, Loss: 0.890446, Accuracy: 82.81%\n",
      "Validation Batch 12, Loss: 0.858538, Accuracy: 83.33%\n",
      "Validation Batch 13, Loss: 0.945656, Accuracy: 82.93%\n",
      "Validation Batch 14, Loss: 0.931482, Accuracy: 82.81%\n",
      "Validation Batch 15, Loss: 0.903846, Accuracy: 83.02%\n",
      "Validation Batch 16, Loss: 0.889223, Accuracy: 83.20%\n",
      "Validation Batch 17, Loss: 0.962770, Accuracy: 82.81%\n",
      "Validation Batch 18, Loss: 0.886867, Accuracy: 82.99%\n",
      "Validation Batch 19, Loss: 0.944444, Accuracy: 82.81%\n",
      "Validation Batch 20, Loss: 0.860459, Accuracy: 82.97%\n",
      "Validation Batch 21, Loss: 0.930531, Accuracy: 82.81%\n",
      "Validation Batch 22, Loss: 0.939374, Accuracy: 82.60%\n",
      "Validation Batch 23, Loss: 0.963286, Accuracy: 82.34%\n",
      "Validation Batch 24, Loss: 0.970168, Accuracy: 82.16%\n",
      "Validation Batch 25, Loss: 0.925398, Accuracy: 82.12%\n",
      "Validation Batch 26, Loss: 0.911210, Accuracy: 82.15%\n",
      "Validation Batch 27, Loss: 0.842887, Accuracy: 82.33%\n",
      "Validation - Epoch 125, Loss: 0.915273, Accuracy: 82.33%\n",
      "Patienceâ€”5\n",
      "Epoch 126\n",
      "Batch 1, Loss: 0.831090, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.886134, Accuracy: 87.50%\n",
      "Batch 3, Loss: 0.942243, Accuracy: 84.38%\n",
      "Batch 4, Loss: 0.921451, Accuracy: 83.98%\n",
      "Batch 5, Loss: 0.886979, Accuracy: 84.38%\n",
      "Batch 6, Loss: 0.862146, Accuracy: 84.90%\n",
      "Batch 7, Loss: 0.861126, Accuracy: 85.49%\n",
      "Batch 8, Loss: 0.893058, Accuracy: 85.35%\n",
      "Batch 9, Loss: 0.887837, Accuracy: 85.59%\n",
      "Batch 10, Loss: 1.047223, Accuracy: 83.75%\n",
      "Batch 11, Loss: 0.899947, Accuracy: 83.81%\n",
      "Batch 12, Loss: 0.899163, Accuracy: 83.85%\n",
      "Batch 13, Loss: 0.929796, Accuracy: 83.65%\n",
      "Batch 14, Loss: 0.837613, Accuracy: 84.04%\n",
      "Batch 15, Loss: 0.908275, Accuracy: 84.06%\n",
      "Batch 16, Loss: 0.950467, Accuracy: 83.79%\n",
      "Batch 17, Loss: 0.886200, Accuracy: 83.82%\n",
      "Batch 18, Loss: 0.907978, Accuracy: 83.77%\n",
      "Batch 19, Loss: 0.895253, Accuracy: 83.80%\n",
      "Batch 20, Loss: 0.877317, Accuracy: 83.91%\n",
      "Batch 21, Loss: 0.909760, Accuracy: 83.93%\n",
      "Batch 22, Loss: 0.888532, Accuracy: 83.95%\n",
      "Batch 23, Loss: 0.961421, Accuracy: 83.70%\n",
      "Batch 24, Loss: 0.892873, Accuracy: 83.79%\n",
      "Batch 25, Loss: 0.928602, Accuracy: 83.62%\n",
      "Batch 26, Loss: 0.920933, Accuracy: 83.53%\n",
      "Batch 27, Loss: 0.861772, Accuracy: 83.74%\n",
      "Batch 28, Loss: 0.946483, Accuracy: 83.59%\n",
      "Batch 29, Loss: 0.957773, Accuracy: 83.41%\n",
      "Batch 30, Loss: 0.931466, Accuracy: 83.23%\n",
      "Batch 31, Loss: 0.993287, Accuracy: 82.96%\n",
      "Batch 32, Loss: 0.917914, Accuracy: 82.96%\n",
      "Batch 33, Loss: 0.916251, Accuracy: 82.95%\n",
      "Batch 34, Loss: 0.984034, Accuracy: 82.72%\n",
      "Batch 35, Loss: 0.934398, Accuracy: 82.68%\n",
      "Batch 36, Loss: 0.901001, Accuracy: 82.73%\n",
      "Batch 37, Loss: 0.903603, Accuracy: 82.81%\n",
      "Batch 38, Loss: 0.929815, Accuracy: 82.73%\n",
      "Batch 39, Loss: 0.888010, Accuracy: 82.77%\n",
      "Batch 40, Loss: 0.922913, Accuracy: 82.77%\n",
      "Batch 41, Loss: 0.867476, Accuracy: 82.93%\n",
      "Batch 42, Loss: 0.876402, Accuracy: 83.04%\n",
      "Batch 43, Loss: 0.906949, Accuracy: 83.03%\n",
      "Batch 44, Loss: 0.938035, Accuracy: 82.99%\n",
      "Batch 45, Loss: 0.923514, Accuracy: 82.99%\n",
      "Batch 46, Loss: 0.913180, Accuracy: 83.02%\n",
      "Batch 47, Loss: 0.914625, Accuracy: 83.01%\n",
      "Batch 48, Loss: 0.918498, Accuracy: 83.01%\n",
      "Batch 49, Loss: 0.852805, Accuracy: 83.13%\n",
      "Batch 50, Loss: 0.922575, Accuracy: 83.12%\n",
      "Batch 51, Loss: 0.889183, Accuracy: 83.18%\n",
      "Batch 52, Loss: 0.936821, Accuracy: 83.11%\n",
      "Batch 53, Loss: 0.890578, Accuracy: 83.17%\n",
      "Batch 54, Loss: 0.881754, Accuracy: 83.22%\n",
      "Batch 55, Loss: 0.914954, Accuracy: 83.21%\n",
      "Batch 56, Loss: 0.940581, Accuracy: 83.15%\n",
      "Batch 57, Loss: 0.910467, Accuracy: 83.14%\n",
      "Batch 58, Loss: 0.828691, Accuracy: 83.30%\n",
      "Batch 59, Loss: 0.873754, Accuracy: 83.37%\n",
      "Batch 60, Loss: 0.876691, Accuracy: 83.44%\n",
      "Batch 61, Loss: 0.841383, Accuracy: 83.53%\n",
      "Batch 62, Loss: 0.921102, Accuracy: 83.52%\n",
      "Batch 63, Loss: 0.943283, Accuracy: 83.48%\n",
      "Batch 64, Loss: 0.929289, Accuracy: 83.45%\n",
      "Batch 65, Loss: 0.911515, Accuracy: 83.44%\n",
      "Batch 66, Loss: 0.951323, Accuracy: 83.36%\n",
      "Batch 67, Loss: 0.852661, Accuracy: 83.44%\n",
      "Batch 68, Loss: 0.855465, Accuracy: 83.50%\n",
      "Batch 69, Loss: 0.898180, Accuracy: 83.54%\n",
      "Batch 70, Loss: 0.890042, Accuracy: 83.55%\n",
      "Batch 71, Loss: 0.948612, Accuracy: 83.49%\n",
      "Batch 72, Loss: 0.950170, Accuracy: 83.44%\n",
      "Batch 73, Loss: 0.914895, Accuracy: 83.41%\n",
      "Batch 74, Loss: 0.887052, Accuracy: 83.42%\n",
      "Batch 75, Loss: 0.893017, Accuracy: 83.40%\n",
      "Batch 76, Loss: 0.864065, Accuracy: 83.47%\n",
      "Batch 77, Loss: 0.927268, Accuracy: 83.46%\n",
      "Batch 78, Loss: 0.907458, Accuracy: 83.47%\n",
      "Batch 79, Loss: 0.864585, Accuracy: 83.54%\n",
      "Batch 80, Loss: 0.883901, Accuracy: 83.61%\n",
      "Batch 81, Loss: 0.890643, Accuracy: 83.64%\n",
      "Batch 82, Loss: 0.914563, Accuracy: 83.65%\n",
      "Batch 83, Loss: 0.874877, Accuracy: 83.70%\n",
      "Batch 84, Loss: 0.990162, Accuracy: 83.58%\n",
      "Batch 85, Loss: 0.911943, Accuracy: 83.57%\n",
      "Batch 86, Loss: 0.860777, Accuracy: 83.63%\n",
      "Batch 87, Loss: 0.857376, Accuracy: 83.71%\n",
      "Batch 88, Loss: 0.910486, Accuracy: 83.70%\n",
      "Batch 89, Loss: 0.880243, Accuracy: 83.74%\n",
      "Batch 90, Loss: 0.947071, Accuracy: 83.70%\n",
      "Batch 91, Loss: 0.858857, Accuracy: 83.74%\n",
      "Batch 92, Loss: 0.904864, Accuracy: 83.75%\n",
      "Batch 93, Loss: 0.932544, Accuracy: 83.72%\n",
      "Batch 94, Loss: 0.906690, Accuracy: 83.71%\n",
      "Batch 95, Loss: 0.864042, Accuracy: 83.75%\n",
      "Batch 96, Loss: 0.927768, Accuracy: 83.76%\n",
      "Batch 97, Loss: 0.938935, Accuracy: 83.75%\n",
      "Batch 98, Loss: 0.886161, Accuracy: 83.77%\n",
      "Batch 99, Loss: 0.889342, Accuracy: 83.79%\n",
      "Batch 100, Loss: 0.877294, Accuracy: 83.81%\n",
      "Batch 101, Loss: 0.901405, Accuracy: 83.79%\n",
      "Batch 102, Loss: 0.889999, Accuracy: 83.81%\n",
      "Batch 103, Loss: 0.916189, Accuracy: 83.78%\n",
      "Batch 104, Loss: 0.910246, Accuracy: 83.79%\n",
      "Batch 105, Loss: 0.850634, Accuracy: 83.85%\n",
      "Batch 106, Loss: 0.947014, Accuracy: 83.83%\n",
      "Batch 107, Loss: 0.899239, Accuracy: 83.83%\n",
      "Batch 108, Loss: 0.886166, Accuracy: 83.85%\n",
      "Batch 109, Loss: 0.837560, Accuracy: 83.92%\n",
      "Batch 110, Loss: 0.922359, Accuracy: 83.91%\n",
      "Batch 111, Loss: 0.915135, Accuracy: 83.90%\n",
      "Batch 112, Loss: 0.888585, Accuracy: 83.90%\n",
      "Batch 113, Loss: 0.902634, Accuracy: 83.89%\n",
      "Batch 114, Loss: 0.957973, Accuracy: 83.85%\n",
      "Batch 115, Loss: 0.932619, Accuracy: 83.83%\n",
      "Batch 116, Loss: 0.859555, Accuracy: 83.86%\n",
      "Batch 117, Loss: 0.918118, Accuracy: 83.85%\n",
      "Batch 118, Loss: 0.950901, Accuracy: 83.82%\n",
      "Batch 119, Loss: 0.945042, Accuracy: 83.78%\n",
      "Batch 120, Loss: 0.937337, Accuracy: 83.76%\n",
      "Batch 121, Loss: 0.925410, Accuracy: 83.77%\n",
      "Batch 122, Loss: 0.974042, Accuracy: 83.71%\n",
      "Batch 123, Loss: 0.955667, Accuracy: 83.66%\n",
      "Batch 124, Loss: 0.957758, Accuracy: 83.62%\n",
      "Batch 125, Loss: 0.939939, Accuracy: 83.60%\n",
      "Batch 126, Loss: 0.904568, Accuracy: 83.59%\n",
      "Batch 127, Loss: 0.925763, Accuracy: 83.58%\n",
      "Batch 128, Loss: 1.013625, Accuracy: 83.47%\n",
      "Batch 129, Loss: 0.897896, Accuracy: 83.48%\n",
      "Batch 130, Loss: 0.950502, Accuracy: 83.44%\n",
      "Batch 131, Loss: 0.898432, Accuracy: 83.44%\n",
      "Batch 132, Loss: 0.956421, Accuracy: 83.40%\n",
      "Batch 133, Loss: 0.883683, Accuracy: 83.44%\n",
      "Batch 134, Loss: 0.862859, Accuracy: 83.48%\n",
      "Batch 135, Loss: 0.915448, Accuracy: 83.47%\n",
      "Batch 136, Loss: 0.916851, Accuracy: 83.47%\n",
      "Batch 137, Loss: 0.877503, Accuracy: 83.50%\n",
      "Batch 138, Loss: 0.967342, Accuracy: 83.45%\n",
      "Batch 139, Loss: 0.870779, Accuracy: 83.46%\n",
      "Batch 140, Loss: 0.939103, Accuracy: 83.45%\n",
      "Batch 141, Loss: 0.931427, Accuracy: 83.42%\n",
      "Batch 142, Loss: 0.876276, Accuracy: 83.45%\n",
      "Batch 143, Loss: 0.857795, Accuracy: 83.48%\n",
      "Batch 144, Loss: 0.924817, Accuracy: 83.46%\n",
      "Batch 145, Loss: 0.894394, Accuracy: 83.48%\n",
      "Batch 146, Loss: 0.862279, Accuracy: 83.51%\n",
      "Batch 147, Loss: 0.860653, Accuracy: 83.55%\n",
      "Batch 148, Loss: 0.945411, Accuracy: 83.52%\n",
      "Batch 149, Loss: 0.854806, Accuracy: 83.55%\n",
      "Batch 150, Loss: 0.947919, Accuracy: 83.52%\n",
      "Batch 151, Loss: 0.916933, Accuracy: 83.52%\n",
      "Batch 152, Loss: 0.929022, Accuracy: 83.49%\n",
      "Batch 153, Loss: 0.889801, Accuracy: 83.51%\n",
      "Batch 154, Loss: 0.874538, Accuracy: 83.53%\n",
      "Batch 155, Loss: 0.890537, Accuracy: 83.54%\n",
      "Batch 156, Loss: 0.892419, Accuracy: 83.55%\n",
      "Batch 157, Loss: 0.873265, Accuracy: 83.58%\n",
      "Batch 158, Loss: 0.903385, Accuracy: 83.57%\n",
      "Batch 159, Loss: 0.883247, Accuracy: 83.59%\n",
      "Batch 160, Loss: 0.979120, Accuracy: 83.54%\n",
      "Batch 161, Loss: 0.914658, Accuracy: 83.52%\n",
      "Batch 162, Loss: 0.869317, Accuracy: 83.54%\n",
      "Batch 163, Loss: 0.867328, Accuracy: 83.56%\n",
      "Batch 164, Loss: 0.886412, Accuracy: 83.59%\n",
      "Batch 165, Loss: 0.969689, Accuracy: 83.55%\n",
      "Batch 166, Loss: 0.896968, Accuracy: 83.57%\n",
      "Batch 167, Loss: 0.936653, Accuracy: 83.55%\n",
      "Batch 168, Loss: 0.855564, Accuracy: 83.58%\n",
      "Batch 169, Loss: 0.945742, Accuracy: 83.57%\n",
      "Batch 170, Loss: 0.926557, Accuracy: 83.56%\n",
      "Batch 171, Loss: 0.867204, Accuracy: 83.58%\n",
      "Batch 172, Loss: 0.850153, Accuracy: 83.62%\n",
      "Batch 173, Loss: 0.919330, Accuracy: 83.62%\n",
      "Batch 174, Loss: 0.874881, Accuracy: 83.63%\n",
      "Batch 175, Loss: 0.908960, Accuracy: 83.62%\n",
      "Batch 176, Loss: 0.900259, Accuracy: 83.63%\n",
      "Batch 177, Loss: 0.907650, Accuracy: 83.62%\n",
      "Batch 178, Loss: 0.926172, Accuracy: 83.61%\n",
      "Batch 179, Loss: 0.884957, Accuracy: 83.63%\n",
      "Batch 180, Loss: 0.946558, Accuracy: 83.61%\n",
      "Batch 181, Loss: 0.928633, Accuracy: 83.58%\n",
      "Batch 182, Loss: 0.895683, Accuracy: 83.59%\n",
      "Batch 183, Loss: 0.871220, Accuracy: 83.60%\n",
      "Batch 184, Loss: 0.979546, Accuracy: 83.57%\n",
      "Batch 185, Loss: 0.911445, Accuracy: 83.57%\n",
      "Batch 186, Loss: 0.953672, Accuracy: 83.54%\n",
      "Batch 187, Loss: 0.800712, Accuracy: 83.61%\n",
      "Batch 188, Loss: 0.886616, Accuracy: 83.62%\n",
      "Batch 189, Loss: 0.844339, Accuracy: 83.66%\n",
      "Batch 190, Loss: 0.995424, Accuracy: 83.61%\n",
      "Batch 191, Loss: 0.910227, Accuracy: 83.61%\n",
      "Batch 192, Loss: 0.869524, Accuracy: 83.63%\n",
      "Batch 193, Loss: 0.913395, Accuracy: 83.62%\n",
      "Batch 194, Loss: 0.879455, Accuracy: 83.63%\n",
      "Batch 195, Loss: 0.865356, Accuracy: 83.66%\n",
      "Batch 196, Loss: 0.871471, Accuracy: 83.68%\n",
      "Batch 197, Loss: 0.908229, Accuracy: 83.69%\n",
      "Batch 198, Loss: 0.919604, Accuracy: 83.69%\n",
      "Batch 199, Loss: 0.888353, Accuracy: 83.70%\n",
      "Batch 200, Loss: 1.024458, Accuracy: 83.64%\n",
      "Batch 201, Loss: 0.868959, Accuracy: 83.66%\n",
      "Batch 202, Loss: 0.898679, Accuracy: 83.66%\n",
      "Batch 203, Loss: 0.860875, Accuracy: 83.68%\n",
      "Batch 204, Loss: 0.883951, Accuracy: 83.69%\n",
      "Batch 205, Loss: 0.935975, Accuracy: 83.67%\n",
      "Batch 206, Loss: 0.927758, Accuracy: 83.65%\n",
      "Batch 207, Loss: 0.900956, Accuracy: 83.65%\n",
      "Batch 208, Loss: 0.893027, Accuracy: 83.65%\n",
      "Batch 209, Loss: 0.891766, Accuracy: 83.66%\n",
      "Batch 210, Loss: 1.004489, Accuracy: 83.62%\n",
      "Batch 211, Loss: 0.900242, Accuracy: 83.60%\n",
      "Batch 212, Loss: 0.859802, Accuracy: 83.63%\n",
      "Batch 213, Loss: 0.892086, Accuracy: 83.63%\n",
      "Training - Epoch 126, Loss: 0.906671, Accuracy: 83.63%\n",
      "Validation Batch 1, Loss: 0.871532, Accuracy: 85.94%\n",
      "Validation Batch 2, Loss: 0.913510, Accuracy: 84.38%\n",
      "Validation Batch 3, Loss: 1.002360, Accuracy: 80.73%\n",
      "Validation Batch 4, Loss: 0.917523, Accuracy: 81.25%\n",
      "Validation Batch 5, Loss: 0.893697, Accuracy: 81.88%\n",
      "Validation Batch 6, Loss: 0.873411, Accuracy: 82.55%\n",
      "Validation Batch 7, Loss: 0.954495, Accuracy: 81.92%\n",
      "Validation Batch 8, Loss: 0.973539, Accuracy: 81.45%\n",
      "Validation Batch 9, Loss: 0.980902, Accuracy: 80.73%\n",
      "Validation Batch 10, Loss: 0.955981, Accuracy: 80.62%\n",
      "Validation Batch 11, Loss: 0.897510, Accuracy: 80.97%\n",
      "Validation Batch 12, Loss: 0.870168, Accuracy: 81.51%\n",
      "Validation Batch 13, Loss: 0.965610, Accuracy: 81.01%\n",
      "Validation Batch 14, Loss: 0.953115, Accuracy: 80.80%\n",
      "Validation Batch 15, Loss: 0.914501, Accuracy: 80.94%\n",
      "Validation Batch 16, Loss: 0.896391, Accuracy: 81.25%\n",
      "Validation Batch 17, Loss: 0.982572, Accuracy: 80.79%\n",
      "Validation Batch 18, Loss: 0.906967, Accuracy: 80.99%\n",
      "Validation Batch 19, Loss: 0.955954, Accuracy: 80.92%\n",
      "Validation Batch 20, Loss: 0.872827, Accuracy: 81.17%\n",
      "Validation Batch 21, Loss: 0.938084, Accuracy: 81.10%\n",
      "Validation Batch 22, Loss: 0.960366, Accuracy: 80.82%\n",
      "Validation Batch 23, Loss: 0.994487, Accuracy: 80.57%\n",
      "Validation Batch 24, Loss: 0.991741, Accuracy: 80.34%\n",
      "Validation Batch 25, Loss: 0.938007, Accuracy: 80.38%\n",
      "Validation Batch 26, Loss: 0.916497, Accuracy: 80.47%\n",
      "Validation Batch 27, Loss: 0.857277, Accuracy: 80.74%\n",
      "Validation - Epoch 126, Loss: 0.931445, Accuracy: 80.74%\n",
      "Patienceâ€”6\n",
      "Epoch 127\n",
      "Batch 1, Loss: 0.888696, Accuracy: 84.38%\n",
      "Batch 2, Loss: 0.884749, Accuracy: 84.38%\n",
      "Batch 3, Loss: 0.963754, Accuracy: 81.25%\n",
      "Batch 4, Loss: 0.890590, Accuracy: 82.42%\n",
      "Batch 5, Loss: 0.987623, Accuracy: 80.94%\n",
      "Batch 6, Loss: 0.923737, Accuracy: 81.25%\n",
      "Batch 7, Loss: 0.900020, Accuracy: 81.70%\n",
      "Batch 8, Loss: 0.986344, Accuracy: 81.05%\n",
      "Batch 9, Loss: 0.948700, Accuracy: 80.90%\n",
      "Batch 10, Loss: 0.866645, Accuracy: 81.41%\n",
      "Batch 11, Loss: 0.970441, Accuracy: 81.11%\n",
      "Batch 12, Loss: 0.916003, Accuracy: 81.12%\n",
      "Batch 13, Loss: 0.946521, Accuracy: 80.89%\n",
      "Batch 14, Loss: 0.877644, Accuracy: 81.25%\n",
      "Batch 15, Loss: 0.913988, Accuracy: 81.25%\n",
      "Batch 16, Loss: 0.914034, Accuracy: 81.35%\n",
      "Batch 17, Loss: 0.881530, Accuracy: 81.62%\n",
      "Batch 18, Loss: 0.909134, Accuracy: 81.68%\n",
      "Batch 19, Loss: 0.924810, Accuracy: 81.74%\n",
      "Batch 20, Loss: 0.967053, Accuracy: 81.41%\n",
      "Batch 21, Loss: 0.982572, Accuracy: 81.18%\n",
      "Batch 22, Loss: 0.914607, Accuracy: 81.25%\n",
      "Batch 23, Loss: 0.965208, Accuracy: 81.11%\n",
      "Batch 24, Loss: 0.854935, Accuracy: 81.45%\n",
      "Batch 25, Loss: 0.975179, Accuracy: 81.12%\n",
      "Batch 26, Loss: 0.946474, Accuracy: 81.13%\n",
      "Batch 27, Loss: 0.871503, Accuracy: 81.31%\n",
      "Batch 28, Loss: 0.860394, Accuracy: 81.58%\n",
      "Batch 29, Loss: 0.900885, Accuracy: 81.73%\n",
      "Batch 30, Loss: 0.854021, Accuracy: 82.03%\n",
      "Batch 31, Loss: 0.942057, Accuracy: 81.96%\n",
      "Batch 32, Loss: 0.999568, Accuracy: 81.69%\n",
      "Batch 33, Loss: 0.824683, Accuracy: 82.05%\n",
      "Batch 34, Loss: 0.961513, Accuracy: 81.94%\n",
      "Batch 35, Loss: 0.905528, Accuracy: 81.96%\n",
      "Batch 36, Loss: 0.886933, Accuracy: 82.07%\n",
      "Batch 37, Loss: 0.911488, Accuracy: 82.05%\n",
      "Batch 38, Loss: 0.907332, Accuracy: 82.11%\n",
      "Batch 39, Loss: 0.931750, Accuracy: 82.05%\n",
      "Batch 40, Loss: 0.853952, Accuracy: 82.19%\n",
      "Batch 41, Loss: 0.928725, Accuracy: 82.16%\n",
      "Batch 42, Loss: 0.876333, Accuracy: 82.25%\n",
      "Batch 43, Loss: 0.890976, Accuracy: 82.34%\n",
      "Batch 44, Loss: 0.882214, Accuracy: 82.42%\n",
      "Batch 45, Loss: 0.912608, Accuracy: 82.40%\n",
      "Batch 46, Loss: 0.969259, Accuracy: 82.30%\n",
      "Batch 47, Loss: 0.911920, Accuracy: 82.31%\n",
      "Batch 48, Loss: 0.946398, Accuracy: 82.19%\n",
      "Batch 49, Loss: 0.895413, Accuracy: 82.24%\n",
      "Batch 50, Loss: 0.955734, Accuracy: 82.16%\n",
      "Batch 51, Loss: 0.824432, Accuracy: 82.35%\n",
      "Batch 52, Loss: 0.943713, Accuracy: 82.30%\n",
      "Batch 53, Loss: 0.879115, Accuracy: 82.40%\n",
      "Batch 54, Loss: 0.887290, Accuracy: 82.44%\n",
      "Batch 55, Loss: 0.912764, Accuracy: 82.44%\n",
      "Batch 56, Loss: 0.943376, Accuracy: 82.37%\n",
      "Batch 57, Loss: 0.922363, Accuracy: 82.35%\n",
      "Batch 58, Loss: 0.903175, Accuracy: 82.35%\n",
      "Batch 59, Loss: 0.822719, Accuracy: 82.55%\n",
      "Batch 60, Loss: 0.928225, Accuracy: 82.55%\n",
      "Batch 61, Loss: 0.898876, Accuracy: 82.56%\n",
      "Batch 62, Loss: 0.984785, Accuracy: 82.46%\n",
      "Batch 63, Loss: 0.840742, Accuracy: 82.59%\n",
      "Batch 64, Loss: 0.923332, Accuracy: 82.59%\n",
      "Batch 65, Loss: 0.976860, Accuracy: 82.45%\n",
      "Batch 66, Loss: 0.891167, Accuracy: 82.48%\n",
      "Batch 67, Loss: 0.837642, Accuracy: 82.63%\n",
      "Batch 68, Loss: 0.961800, Accuracy: 82.54%\n",
      "Batch 69, Loss: 0.871627, Accuracy: 82.61%\n",
      "Batch 70, Loss: 0.873460, Accuracy: 82.66%\n",
      "Batch 71, Loss: 0.882857, Accuracy: 82.75%\n",
      "Batch 72, Loss: 0.878079, Accuracy: 82.81%\n",
      "Batch 73, Loss: 0.983706, Accuracy: 82.71%\n",
      "Batch 74, Loss: 0.907268, Accuracy: 82.73%\n",
      "Batch 75, Loss: 0.884659, Accuracy: 82.77%\n",
      "Batch 76, Loss: 0.852190, Accuracy: 82.85%\n",
      "Batch 77, Loss: 0.907975, Accuracy: 82.85%\n",
      "Batch 78, Loss: 0.875433, Accuracy: 82.91%\n",
      "Batch 79, Loss: 0.879049, Accuracy: 82.97%\n",
      "Batch 80, Loss: 0.965417, Accuracy: 82.89%\n",
      "Batch 81, Loss: 0.933159, Accuracy: 82.85%\n",
      "Batch 82, Loss: 0.956650, Accuracy: 82.79%\n",
      "Batch 83, Loss: 0.928154, Accuracy: 82.77%\n",
      "Batch 84, Loss: 0.902409, Accuracy: 82.81%\n",
      "Batch 85, Loss: 0.881955, Accuracy: 82.87%\n",
      "Batch 86, Loss: 0.913202, Accuracy: 82.89%\n",
      "Batch 87, Loss: 0.878739, Accuracy: 82.92%\n",
      "Batch 88, Loss: 0.873658, Accuracy: 82.97%\n",
      "Batch 89, Loss: 1.009069, Accuracy: 82.87%\n",
      "Batch 90, Loss: 0.867442, Accuracy: 82.92%\n",
      "Batch 91, Loss: 0.916127, Accuracy: 82.92%\n",
      "Batch 92, Loss: 0.920916, Accuracy: 82.91%\n",
      "Batch 93, Loss: 0.907819, Accuracy: 82.91%\n",
      "Batch 94, Loss: 1.001248, Accuracy: 82.81%\n",
      "Batch 95, Loss: 0.888694, Accuracy: 82.83%\n",
      "Batch 96, Loss: 0.932490, Accuracy: 82.80%\n",
      "Batch 97, Loss: 0.917376, Accuracy: 82.80%\n",
      "Batch 98, Loss: 0.912161, Accuracy: 82.80%\n",
      "Batch 99, Loss: 0.840889, Accuracy: 82.88%\n",
      "Batch 100, Loss: 0.901785, Accuracy: 82.91%\n",
      "Batch 101, Loss: 0.981999, Accuracy: 82.83%\n",
      "Batch 102, Loss: 0.888375, Accuracy: 82.87%\n",
      "Batch 103, Loss: 0.917203, Accuracy: 82.87%\n",
      "Batch 104, Loss: 0.955444, Accuracy: 82.84%\n",
      "Batch 105, Loss: 0.951970, Accuracy: 82.80%\n",
      "Batch 106, Loss: 0.915497, Accuracy: 82.78%\n",
      "Batch 107, Loss: 0.941613, Accuracy: 82.77%\n",
      "Batch 108, Loss: 0.891956, Accuracy: 82.77%\n",
      "Batch 109, Loss: 0.957469, Accuracy: 82.73%\n",
      "Batch 110, Loss: 0.863934, Accuracy: 82.78%\n",
      "Batch 111, Loss: 0.944495, Accuracy: 82.74%\n",
      "Batch 112, Loss: 0.851008, Accuracy: 82.81%\n",
      "Batch 113, Loss: 0.912130, Accuracy: 82.83%\n",
      "Batch 114, Loss: 1.015968, Accuracy: 82.72%\n",
      "Batch 115, Loss: 0.907960, Accuracy: 82.73%\n",
      "Batch 116, Loss: 0.950968, Accuracy: 82.69%\n",
      "Batch 117, Loss: 0.894333, Accuracy: 82.72%\n",
      "Batch 118, Loss: 0.905305, Accuracy: 82.73%\n",
      "Batch 119, Loss: 0.916712, Accuracy: 82.75%\n",
      "Batch 120, Loss: 0.867146, Accuracy: 82.80%\n",
      "Batch 121, Loss: 0.910416, Accuracy: 82.80%\n",
      "Batch 122, Loss: 0.852194, Accuracy: 82.85%\n",
      "Batch 123, Loss: 0.819738, Accuracy: 82.93%\n",
      "Batch 124, Loss: 0.915650, Accuracy: 82.91%\n",
      "Batch 125, Loss: 0.847973, Accuracy: 82.96%\n",
      "Batch 126, Loss: 0.840621, Accuracy: 83.04%\n",
      "Batch 127, Loss: 0.859920, Accuracy: 83.06%\n",
      "Batch 128, Loss: 0.875274, Accuracy: 83.08%\n",
      "Batch 129, Loss: 0.929388, Accuracy: 83.05%\n",
      "Batch 130, Loss: 0.970279, Accuracy: 83.02%\n",
      "Batch 131, Loss: 0.829915, Accuracy: 83.09%\n",
      "Batch 132, Loss: 0.945421, Accuracy: 83.07%\n",
      "Batch 133, Loss: 0.840910, Accuracy: 83.13%\n",
      "Batch 134, Loss: 0.912388, Accuracy: 83.13%\n",
      "Batch 135, Loss: 0.907457, Accuracy: 83.14%\n",
      "Batch 136, Loss: 0.840025, Accuracy: 83.19%\n",
      "Batch 137, Loss: 0.899839, Accuracy: 83.19%\n",
      "Batch 138, Loss: 0.930739, Accuracy: 83.19%\n",
      "Batch 139, Loss: 0.926433, Accuracy: 83.16%\n",
      "Batch 140, Loss: 0.895002, Accuracy: 83.17%\n",
      "Batch 141, Loss: 0.933888, Accuracy: 83.16%\n",
      "Batch 142, Loss: 0.900773, Accuracy: 83.15%\n",
      "Batch 143, Loss: 0.926569, Accuracy: 83.15%\n",
      "Batch 144, Loss: 0.954073, Accuracy: 83.13%\n",
      "Batch 145, Loss: 0.890134, Accuracy: 83.15%\n",
      "Batch 146, Loss: 0.930942, Accuracy: 83.13%\n",
      "Batch 147, Loss: 0.835610, Accuracy: 83.18%\n",
      "Batch 148, Loss: 0.906403, Accuracy: 83.19%\n",
      "Batch 149, Loss: 0.923358, Accuracy: 83.18%\n",
      "Batch 150, Loss: 0.879587, Accuracy: 83.21%\n",
      "Batch 151, Loss: 1.017505, Accuracy: 83.13%\n",
      "Batch 152, Loss: 0.869205, Accuracy: 83.16%\n",
      "Batch 153, Loss: 0.899795, Accuracy: 83.17%\n",
      "Batch 154, Loss: 0.845269, Accuracy: 83.21%\n",
      "Batch 155, Loss: 0.787567, Accuracy: 83.30%\n",
      "Batch 156, Loss: 0.934145, Accuracy: 83.28%\n",
      "Batch 157, Loss: 0.885559, Accuracy: 83.30%\n",
      "Batch 158, Loss: 0.892578, Accuracy: 83.32%\n",
      "Batch 159, Loss: 0.923194, Accuracy: 83.31%\n",
      "Batch 160, Loss: 0.951080, Accuracy: 83.27%\n",
      "Batch 161, Loss: 0.935117, Accuracy: 83.25%\n",
      "Batch 162, Loss: 0.976788, Accuracy: 83.21%\n",
      "Batch 163, Loss: 0.838943, Accuracy: 83.26%\n",
      "Batch 164, Loss: 0.924244, Accuracy: 83.25%\n",
      "Batch 165, Loss: 0.932135, Accuracy: 83.23%\n",
      "Batch 166, Loss: 0.879869, Accuracy: 83.25%\n",
      "Batch 167, Loss: 0.876728, Accuracy: 83.27%\n",
      "Batch 168, Loss: 0.871796, Accuracy: 83.30%\n",
      "Batch 169, Loss: 0.875107, Accuracy: 83.32%\n",
      "Batch 170, Loss: 0.914161, Accuracy: 83.32%\n",
      "Batch 171, Loss: 0.881963, Accuracy: 83.33%\n",
      "Batch 172, Loss: 0.871254, Accuracy: 83.36%\n",
      "Batch 173, Loss: 0.945418, Accuracy: 83.34%\n",
      "Batch 174, Loss: 0.855720, Accuracy: 83.37%\n",
      "Batch 175, Loss: 0.885821, Accuracy: 83.38%\n",
      "Batch 176, Loss: 0.879856, Accuracy: 83.39%\n",
      "Batch 177, Loss: 0.872504, Accuracy: 83.42%\n",
      "Batch 178, Loss: 0.890668, Accuracy: 83.44%\n",
      "Batch 179, Loss: 0.949669, Accuracy: 83.41%\n",
      "Batch 180, Loss: 0.951330, Accuracy: 83.39%\n",
      "Batch 181, Loss: 0.901806, Accuracy: 83.39%\n",
      "Batch 182, Loss: 0.945422, Accuracy: 83.37%\n",
      "Batch 183, Loss: 0.862525, Accuracy: 83.40%\n",
      "Batch 184, Loss: 0.953452, Accuracy: 83.36%\n",
      "Batch 185, Loss: 0.939326, Accuracy: 83.34%\n",
      "Batch 186, Loss: 0.898849, Accuracy: 83.34%\n",
      "Batch 187, Loss: 0.940314, Accuracy: 83.32%\n",
      "Batch 188, Loss: 0.836824, Accuracy: 83.37%\n",
      "Batch 189, Loss: 0.963097, Accuracy: 83.33%\n",
      "Batch 190, Loss: 0.930157, Accuracy: 83.31%\n",
      "Batch 191, Loss: 0.921248, Accuracy: 83.31%\n",
      "Batch 192, Loss: 0.910317, Accuracy: 83.31%\n",
      "Batch 193, Loss: 0.940103, Accuracy: 83.30%\n",
      "Batch 194, Loss: 0.887090, Accuracy: 83.31%\n",
      "Batch 195, Loss: 0.950013, Accuracy: 83.29%\n",
      "Batch 196, Loss: 0.916635, Accuracy: 83.29%\n",
      "Batch 197, Loss: 0.916368, Accuracy: 83.28%\n",
      "Batch 198, Loss: 0.980805, Accuracy: 83.25%\n",
      "Batch 199, Loss: 0.833640, Accuracy: 83.30%\n",
      "Batch 200, Loss: 0.937587, Accuracy: 83.29%\n",
      "Batch 201, Loss: 1.009732, Accuracy: 83.24%\n",
      "Batch 202, Loss: 0.943821, Accuracy: 83.22%\n",
      "Batch 203, Loss: 0.901132, Accuracy: 83.22%\n",
      "Batch 204, Loss: 0.887421, Accuracy: 83.23%\n",
      "Batch 205, Loss: 0.929085, Accuracy: 83.22%\n",
      "Batch 206, Loss: 0.889165, Accuracy: 83.23%\n",
      "Batch 207, Loss: 0.914541, Accuracy: 83.24%\n",
      "Batch 208, Loss: 0.968174, Accuracy: 83.20%\n",
      "Batch 209, Loss: 0.909966, Accuracy: 83.20%\n",
      "Batch 210, Loss: 0.932567, Accuracy: 83.20%\n",
      "Batch 211, Loss: 0.919950, Accuracy: 83.19%\n",
      "Batch 212, Loss: 0.857256, Accuracy: 83.22%\n",
      "Batch 213, Loss: 0.879804, Accuracy: 83.24%\n",
      "Training - Epoch 127, Loss: 0.909839, Accuracy: 83.24%\n",
      "Validation Batch 1, Loss: 0.833828, Accuracy: 90.62%\n",
      "Validation Batch 2, Loss: 0.825323, Accuracy: 91.41%\n",
      "Validation Batch 3, Loss: 0.915139, Accuracy: 88.54%\n",
      "Validation Batch 4, Loss: 0.865814, Accuracy: 88.28%\n",
      "Validation Batch 5, Loss: 0.799118, Accuracy: 89.69%\n",
      "Validation Batch 6, Loss: 0.804106, Accuracy: 90.36%\n",
      "Validation Batch 7, Loss: 0.860037, Accuracy: 89.96%\n",
      "Validation Batch 8, Loss: 0.910009, Accuracy: 89.06%\n",
      "Validation Batch 9, Loss: 0.853186, Accuracy: 89.06%\n",
      "Validation Batch 10, Loss: 0.898307, Accuracy: 88.75%\n",
      "Validation Batch 11, Loss: 0.840794, Accuracy: 88.78%\n",
      "Validation Batch 12, Loss: 0.829005, Accuracy: 88.93%\n",
      "Validation Batch 13, Loss: 0.856518, Accuracy: 88.82%\n",
      "Validation Batch 14, Loss: 0.879420, Accuracy: 88.73%\n",
      "Validation Batch 15, Loss: 0.881918, Accuracy: 88.54%\n",
      "Validation Batch 16, Loss: 0.863753, Accuracy: 88.48%\n",
      "Validation Batch 17, Loss: 0.902439, Accuracy: 88.14%\n",
      "Validation Batch 18, Loss: 0.823348, Accuracy: 88.37%\n",
      "Validation Batch 19, Loss: 0.887468, Accuracy: 88.24%\n",
      "Validation Batch 20, Loss: 0.813949, Accuracy: 88.44%\n",
      "Validation Batch 21, Loss: 0.875523, Accuracy: 88.32%\n",
      "Validation Batch 22, Loss: 0.854633, Accuracy: 88.35%\n",
      "Validation Batch 23, Loss: 0.869376, Accuracy: 88.38%\n",
      "Validation Batch 24, Loss: 0.927813, Accuracy: 88.09%\n",
      "Validation Batch 25, Loss: 0.849160, Accuracy: 88.12%\n",
      "Validation Batch 26, Loss: 0.858123, Accuracy: 88.16%\n",
      "Validation Batch 27, Loss: 0.788845, Accuracy: 88.31%\n",
      "Validation - Epoch 127, Loss: 0.858035, Accuracy: 88.31%\n",
      "Patienceâ€”0\n",
      "Epoch 128\n",
      "Batch 1, Loss: 0.947313, Accuracy: 76.56%\n",
      "Batch 2, Loss: 0.892435, Accuracy: 81.25%\n",
      "Batch 3, Loss: 0.862694, Accuracy: 83.33%\n",
      "Batch 4, Loss: 0.894147, Accuracy: 83.98%\n",
      "Batch 5, Loss: 0.837624, Accuracy: 85.31%\n",
      "Batch 6, Loss: 0.888464, Accuracy: 85.16%\n",
      "Batch 7, Loss: 0.905011, Accuracy: 85.04%\n",
      "Batch 8, Loss: 0.874151, Accuracy: 85.35%\n",
      "Batch 9, Loss: 0.945430, Accuracy: 84.38%\n",
      "Batch 10, Loss: 0.910232, Accuracy: 84.38%\n",
      "Batch 11, Loss: 0.953542, Accuracy: 83.95%\n",
      "Batch 12, Loss: 0.890113, Accuracy: 83.98%\n",
      "Batch 13, Loss: 0.943657, Accuracy: 83.53%\n",
      "Batch 14, Loss: 0.934641, Accuracy: 83.37%\n",
      "Batch 15, Loss: 0.867714, Accuracy: 83.65%\n",
      "Batch 16, Loss: 0.883663, Accuracy: 83.69%\n",
      "Batch 17, Loss: 0.853909, Accuracy: 84.01%\n",
      "Batch 18, Loss: 0.891316, Accuracy: 84.11%\n",
      "Batch 19, Loss: 0.888641, Accuracy: 84.05%\n",
      "Batch 20, Loss: 0.990801, Accuracy: 83.52%\n",
      "Batch 21, Loss: 0.888604, Accuracy: 83.63%\n",
      "Batch 22, Loss: 0.843859, Accuracy: 83.95%\n",
      "Batch 23, Loss: 0.898008, Accuracy: 83.97%\n",
      "Batch 24, Loss: 0.855296, Accuracy: 84.24%\n",
      "Batch 25, Loss: 0.926598, Accuracy: 84.12%\n",
      "Batch 26, Loss: 0.929624, Accuracy: 83.95%\n",
      "Batch 27, Loss: 0.914169, Accuracy: 83.91%\n",
      "Batch 28, Loss: 0.905351, Accuracy: 83.82%\n",
      "Batch 29, Loss: 0.896887, Accuracy: 83.84%\n",
      "Batch 30, Loss: 0.927219, Accuracy: 83.70%\n",
      "Batch 31, Loss: 0.911897, Accuracy: 83.72%\n",
      "Batch 32, Loss: 0.932854, Accuracy: 83.59%\n",
      "Batch 33, Loss: 0.907882, Accuracy: 83.57%\n",
      "Batch 34, Loss: 0.880315, Accuracy: 83.69%\n",
      "Batch 35, Loss: 0.857839, Accuracy: 83.93%\n",
      "Batch 36, Loss: 0.911880, Accuracy: 83.98%\n",
      "Batch 37, Loss: 0.908826, Accuracy: 83.95%\n",
      "Batch 38, Loss: 0.875777, Accuracy: 84.05%\n",
      "Batch 39, Loss: 0.935199, Accuracy: 83.93%\n",
      "Batch 40, Loss: 0.906514, Accuracy: 83.91%\n",
      "Batch 41, Loss: 0.864658, Accuracy: 83.99%\n",
      "Batch 42, Loss: 0.930884, Accuracy: 83.93%\n",
      "Batch 43, Loss: 0.878855, Accuracy: 83.98%\n",
      "Batch 44, Loss: 0.933929, Accuracy: 83.88%\n",
      "Batch 45, Loss: 0.970116, Accuracy: 83.75%\n",
      "Batch 46, Loss: 0.894391, Accuracy: 83.80%\n",
      "Batch 47, Loss: 0.923359, Accuracy: 83.78%\n",
      "Batch 48, Loss: 0.905215, Accuracy: 83.79%\n",
      "Batch 49, Loss: 0.893416, Accuracy: 83.80%\n",
      "Batch 50, Loss: 0.857659, Accuracy: 83.91%\n",
      "Batch 51, Loss: 0.856978, Accuracy: 83.98%\n",
      "Batch 52, Loss: 0.927753, Accuracy: 83.95%\n",
      "Batch 53, Loss: 0.914456, Accuracy: 83.93%\n",
      "Batch 54, Loss: 0.833502, Accuracy: 84.03%\n",
      "Batch 55, Loss: 0.863872, Accuracy: 84.12%\n",
      "Batch 56, Loss: 0.879201, Accuracy: 84.15%\n",
      "Batch 57, Loss: 0.929149, Accuracy: 84.10%\n",
      "Batch 58, Loss: 0.878393, Accuracy: 84.13%\n",
      "Batch 59, Loss: 0.916966, Accuracy: 84.08%\n",
      "Batch 60, Loss: 0.939238, Accuracy: 83.98%\n",
      "Batch 61, Loss: 0.929620, Accuracy: 83.94%\n",
      "Batch 62, Loss: 1.026018, Accuracy: 83.72%\n",
      "Batch 63, Loss: 0.922070, Accuracy: 83.68%\n",
      "Batch 64, Loss: 0.840388, Accuracy: 83.79%\n",
      "Batch 65, Loss: 0.904234, Accuracy: 83.77%\n",
      "Batch 66, Loss: 0.982462, Accuracy: 83.64%\n",
      "Batch 67, Loss: 0.962942, Accuracy: 83.56%\n",
      "Batch 68, Loss: 0.903836, Accuracy: 83.57%\n",
      "Batch 69, Loss: 0.891534, Accuracy: 83.58%\n",
      "Batch 70, Loss: 0.854674, Accuracy: 83.68%\n",
      "Batch 71, Loss: 0.851258, Accuracy: 83.78%\n",
      "Batch 72, Loss: 0.873378, Accuracy: 83.85%\n",
      "Batch 73, Loss: 0.962460, Accuracy: 83.78%\n",
      "Batch 74, Loss: 0.951343, Accuracy: 83.72%\n",
      "Batch 75, Loss: 0.930477, Accuracy: 83.69%\n",
      "Batch 76, Loss: 0.901842, Accuracy: 83.70%\n",
      "Batch 77, Loss: 0.943148, Accuracy: 83.64%\n",
      "Batch 78, Loss: 0.866355, Accuracy: 83.69%\n",
      "Batch 79, Loss: 0.944316, Accuracy: 83.64%\n",
      "Batch 80, Loss: 0.956344, Accuracy: 83.57%\n",
      "Batch 81, Loss: 0.966000, Accuracy: 83.51%\n",
      "Batch 82, Loss: 0.913005, Accuracy: 83.48%\n",
      "Batch 83, Loss: 0.960599, Accuracy: 83.41%\n",
      "Batch 84, Loss: 0.901187, Accuracy: 83.41%\n",
      "Batch 85, Loss: 0.863686, Accuracy: 83.47%\n",
      "Batch 86, Loss: 0.841807, Accuracy: 83.58%\n",
      "Batch 87, Loss: 0.972132, Accuracy: 83.49%\n",
      "Batch 88, Loss: 0.840739, Accuracy: 83.56%\n",
      "Batch 89, Loss: 0.941731, Accuracy: 83.51%\n",
      "Batch 90, Loss: 0.928640, Accuracy: 83.51%\n",
      "Batch 91, Loss: 0.931087, Accuracy: 83.50%\n",
      "Batch 92, Loss: 0.890151, Accuracy: 83.51%\n",
      "Batch 93, Loss: 0.867358, Accuracy: 83.55%\n",
      "Batch 94, Loss: 0.899715, Accuracy: 83.56%\n",
      "Batch 95, Loss: 0.923690, Accuracy: 83.55%\n",
      "Batch 96, Loss: 0.866653, Accuracy: 83.61%\n",
      "Batch 97, Loss: 0.841927, Accuracy: 83.68%\n",
      "Batch 98, Loss: 0.882508, Accuracy: 83.71%\n",
      "Batch 99, Loss: 0.910810, Accuracy: 83.71%\n",
      "Batch 100, Loss: 0.940044, Accuracy: 83.67%\n",
      "Batch 101, Loss: 0.929722, Accuracy: 83.65%\n",
      "Batch 102, Loss: 0.916850, Accuracy: 83.66%\n",
      "Batch 103, Loss: 0.862873, Accuracy: 83.69%\n",
      "Batch 104, Loss: 0.887008, Accuracy: 83.71%\n",
      "Batch 105, Loss: 0.884983, Accuracy: 83.74%\n",
      "Batch 106, Loss: 0.955976, Accuracy: 83.68%\n",
      "Batch 107, Loss: 0.928272, Accuracy: 83.67%\n",
      "Batch 108, Loss: 0.961539, Accuracy: 83.59%\n",
      "Batch 109, Loss: 0.892560, Accuracy: 83.60%\n",
      "Batch 110, Loss: 0.988515, Accuracy: 83.49%\n",
      "Batch 111, Loss: 0.859930, Accuracy: 83.53%\n",
      "Batch 112, Loss: 0.912417, Accuracy: 83.54%\n",
      "Batch 113, Loss: 0.912565, Accuracy: 83.53%\n",
      "Batch 114, Loss: 0.892170, Accuracy: 83.57%\n",
      "Batch 115, Loss: 0.884341, Accuracy: 83.60%\n",
      "Batch 116, Loss: 0.852352, Accuracy: 83.66%\n",
      "Batch 117, Loss: 0.951186, Accuracy: 83.63%\n",
      "Batch 118, Loss: 0.902785, Accuracy: 83.63%\n",
      "Batch 119, Loss: 0.952169, Accuracy: 83.60%\n",
      "Batch 120, Loss: 0.890177, Accuracy: 83.61%\n",
      "Batch 121, Loss: 0.888870, Accuracy: 83.63%\n",
      "Batch 122, Loss: 0.850869, Accuracy: 83.67%\n",
      "Batch 123, Loss: 0.861189, Accuracy: 83.71%\n",
      "Batch 124, Loss: 0.968204, Accuracy: 83.67%\n",
      "Batch 125, Loss: 0.906431, Accuracy: 83.67%\n",
      "Batch 126, Loss: 0.917954, Accuracy: 83.67%\n",
      "Batch 127, Loss: 0.921285, Accuracy: 83.65%\n",
      "Batch 128, Loss: 0.888419, Accuracy: 83.67%\n",
      "Batch 129, Loss: 0.963217, Accuracy: 83.62%\n",
      "Batch 130, Loss: 0.865001, Accuracy: 83.67%\n",
      "Batch 131, Loss: 0.985659, Accuracy: 83.60%\n",
      "Batch 132, Loss: 0.864835, Accuracy: 83.64%\n",
      "Batch 133, Loss: 0.916736, Accuracy: 83.63%\n",
      "Batch 134, Loss: 0.883208, Accuracy: 83.65%\n",
      "Batch 135, Loss: 0.901342, Accuracy: 83.65%\n",
      "Batch 136, Loss: 0.878864, Accuracy: 83.66%\n",
      "Batch 137, Loss: 0.877251, Accuracy: 83.68%\n",
      "Batch 138, Loss: 0.897346, Accuracy: 83.68%\n",
      "Batch 139, Loss: 1.007149, Accuracy: 83.61%\n",
      "Batch 140, Loss: 0.904819, Accuracy: 83.62%\n",
      "Batch 141, Loss: 0.903737, Accuracy: 83.61%\n",
      "Batch 142, Loss: 0.987935, Accuracy: 83.55%\n",
      "Batch 143, Loss: 0.908348, Accuracy: 83.56%\n",
      "Batch 144, Loss: 0.962261, Accuracy: 83.54%\n",
      "Batch 145, Loss: 0.919865, Accuracy: 83.53%\n",
      "Batch 146, Loss: 0.890488, Accuracy: 83.55%\n",
      "Batch 147, Loss: 0.881116, Accuracy: 83.58%\n",
      "Batch 148, Loss: 0.841701, Accuracy: 83.63%\n",
      "Batch 149, Loss: 0.967276, Accuracy: 83.58%\n",
      "Batch 150, Loss: 0.879654, Accuracy: 83.58%\n",
      "Batch 151, Loss: 0.966782, Accuracy: 83.55%\n",
      "Batch 152, Loss: 0.846753, Accuracy: 83.59%\n",
      "Batch 153, Loss: 0.949376, Accuracy: 83.57%\n",
      "Batch 154, Loss: 0.879455, Accuracy: 83.58%\n",
      "Batch 155, Loss: 0.851484, Accuracy: 83.61%\n",
      "Batch 156, Loss: 0.884342, Accuracy: 83.63%\n",
      "Batch 157, Loss: 0.808664, Accuracy: 83.71%\n",
      "Batch 158, Loss: 0.883323, Accuracy: 83.71%\n",
      "Batch 159, Loss: 0.948157, Accuracy: 83.68%\n",
      "Batch 160, Loss: 0.852287, Accuracy: 83.72%\n",
      "Batch 161, Loss: 0.952401, Accuracy: 83.71%\n",
      "Batch 162, Loss: 0.887437, Accuracy: 83.72%\n",
      "Batch 163, Loss: 0.948774, Accuracy: 83.70%\n",
      "Batch 164, Loss: 0.880949, Accuracy: 83.72%\n",
      "Batch 165, Loss: 0.887185, Accuracy: 83.73%\n",
      "Batch 166, Loss: 0.890773, Accuracy: 83.73%\n",
      "Batch 167, Loss: 0.891304, Accuracy: 83.74%\n",
      "Batch 168, Loss: 0.894991, Accuracy: 83.75%\n",
      "Batch 169, Loss: 0.888338, Accuracy: 83.76%\n",
      "Batch 170, Loss: 0.871628, Accuracy: 83.78%\n",
      "Batch 171, Loss: 0.848396, Accuracy: 83.81%\n",
      "Batch 172, Loss: 0.865183, Accuracy: 83.84%\n",
      "Batch 173, Loss: 1.006868, Accuracy: 83.77%\n",
      "Batch 174, Loss: 0.809353, Accuracy: 83.83%\n",
      "Batch 175, Loss: 0.970231, Accuracy: 83.79%\n",
      "Batch 176, Loss: 0.948573, Accuracy: 83.77%\n",
      "Batch 177, Loss: 0.840748, Accuracy: 83.81%\n",
      "Batch 178, Loss: 0.895731, Accuracy: 83.82%\n",
      "Batch 179, Loss: 0.933004, Accuracy: 83.80%\n",
      "Batch 180, Loss: 0.887972, Accuracy: 83.81%\n",
      "Batch 181, Loss: 0.935442, Accuracy: 83.81%\n",
      "Batch 182, Loss: 0.918987, Accuracy: 83.80%\n",
      "Batch 183, Loss: 0.808294, Accuracy: 83.85%\n",
      "Batch 184, Loss: 0.915354, Accuracy: 83.86%\n",
      "Batch 185, Loss: 0.896124, Accuracy: 83.86%\n",
      "Batch 186, Loss: 0.856976, Accuracy: 83.89%\n",
      "Batch 187, Loss: 0.852567, Accuracy: 83.92%\n",
      "Batch 188, Loss: 0.936447, Accuracy: 83.90%\n",
      "Batch 189, Loss: 0.943331, Accuracy: 83.87%\n",
      "Batch 190, Loss: 0.948225, Accuracy: 83.85%\n",
      "Batch 191, Loss: 0.951559, Accuracy: 83.82%\n",
      "Batch 192, Loss: 1.005743, Accuracy: 83.76%\n",
      "Batch 193, Loss: 0.919307, Accuracy: 83.75%\n",
      "Batch 194, Loss: 0.941653, Accuracy: 83.73%\n",
      "Batch 195, Loss: 0.856551, Accuracy: 83.76%\n",
      "Batch 196, Loss: 0.859044, Accuracy: 83.79%\n",
      "Batch 197, Loss: 0.872189, Accuracy: 83.80%\n",
      "Batch 198, Loss: 0.875749, Accuracy: 83.82%\n",
      "Batch 199, Loss: 0.908320, Accuracy: 83.81%\n",
      "Batch 200, Loss: 0.982444, Accuracy: 83.77%\n",
      "Batch 201, Loss: 0.929844, Accuracy: 83.75%\n",
      "Batch 202, Loss: 0.886683, Accuracy: 83.76%\n",
      "Batch 203, Loss: 0.855917, Accuracy: 83.79%\n",
      "Batch 204, Loss: 0.841086, Accuracy: 83.82%\n",
      "Batch 205, Loss: 0.832366, Accuracy: 83.86%\n",
      "Batch 206, Loss: 0.871248, Accuracy: 83.88%\n",
      "Batch 207, Loss: 0.988163, Accuracy: 83.85%\n",
      "Batch 208, Loss: 0.832433, Accuracy: 83.89%\n",
      "Batch 209, Loss: 0.910560, Accuracy: 83.89%\n",
      "Batch 210, Loss: 0.934731, Accuracy: 83.88%\n",
      "Batch 211, Loss: 0.900955, Accuracy: 83.87%\n",
      "Batch 212, Loss: 0.970821, Accuracy: 83.84%\n",
      "Batch 213, Loss: 0.916894, Accuracy: 83.83%\n",
      "Training - Epoch 128, Loss: 0.905071, Accuracy: 83.83%\n",
      "Validation Batch 1, Loss: 0.864237, Accuracy: 87.50%\n",
      "Validation Batch 2, Loss: 0.890624, Accuracy: 86.72%\n",
      "Validation Batch 3, Loss: 1.000630, Accuracy: 82.29%\n",
      "Validation Batch 4, Loss: 0.909559, Accuracy: 82.42%\n",
      "Validation Batch 5, Loss: 0.879768, Accuracy: 82.81%\n",
      "Validation Batch 6, Loss: 0.856549, Accuracy: 84.11%\n",
      "Validation Batch 7, Loss: 0.940323, Accuracy: 83.48%\n",
      "Validation Batch 8, Loss: 0.961705, Accuracy: 82.81%\n",
      "Validation Batch 9, Loss: 0.969946, Accuracy: 82.12%\n",
      "Validation Batch 10, Loss: 0.948994, Accuracy: 81.88%\n",
      "Validation Batch 11, Loss: 0.894667, Accuracy: 82.10%\n",
      "Validation Batch 12, Loss: 0.860446, Accuracy: 82.55%\n",
      "Validation Batch 13, Loss: 0.951911, Accuracy: 82.09%\n",
      "Validation Batch 14, Loss: 0.934106, Accuracy: 82.03%\n",
      "Validation Batch 15, Loss: 0.907369, Accuracy: 82.19%\n",
      "Validation Batch 16, Loss: 0.889462, Accuracy: 82.42%\n",
      "Validation Batch 17, Loss: 0.970043, Accuracy: 82.17%\n",
      "Validation Batch 18, Loss: 0.894310, Accuracy: 82.38%\n",
      "Validation Batch 19, Loss: 0.950824, Accuracy: 82.24%\n",
      "Validation Batch 20, Loss: 0.878034, Accuracy: 82.42%\n",
      "Validation Batch 21, Loss: 0.934133, Accuracy: 82.29%\n",
      "Validation Batch 22, Loss: 0.947654, Accuracy: 82.10%\n",
      "Validation Batch 23, Loss: 0.980363, Accuracy: 81.79%\n",
      "Validation Batch 24, Loss: 0.973941, Accuracy: 81.64%\n",
      "Validation Batch 25, Loss: 0.940762, Accuracy: 81.50%\n",
      "Validation Batch 26, Loss: 0.912911, Accuracy: 81.55%\n",
      "Validation Batch 27, Loss: 0.854187, Accuracy: 81.74%\n",
      "Validation - Epoch 128, Loss: 0.922128, Accuracy: 81.74%\n",
      "Patienceâ€”1\n",
      "Epoch 129\n",
      "Batch 1, Loss: 0.886418, Accuracy: 85.94%\n",
      "Batch 2, Loss: 0.938595, Accuracy: 82.81%\n",
      "Batch 3, Loss: 0.918290, Accuracy: 82.29%\n",
      "Batch 4, Loss: 0.948614, Accuracy: 81.25%\n",
      "Batch 5, Loss: 0.985730, Accuracy: 80.00%\n",
      "Batch 6, Loss: 0.931731, Accuracy: 80.21%\n",
      "Batch 7, Loss: 0.877441, Accuracy: 81.03%\n",
      "Batch 8, Loss: 0.828699, Accuracy: 82.42%\n",
      "Batch 9, Loss: 0.891244, Accuracy: 82.81%\n",
      "Batch 10, Loss: 0.948273, Accuracy: 82.34%\n",
      "Batch 11, Loss: 0.878854, Accuracy: 82.81%\n",
      "Batch 12, Loss: 0.895063, Accuracy: 83.07%\n",
      "Batch 13, Loss: 0.956718, Accuracy: 82.81%\n",
      "Batch 14, Loss: 0.983027, Accuracy: 82.25%\n",
      "Batch 15, Loss: 0.906143, Accuracy: 82.29%\n",
      "Batch 16, Loss: 0.909856, Accuracy: 82.32%\n",
      "Batch 17, Loss: 0.879256, Accuracy: 82.54%\n",
      "Batch 18, Loss: 0.886343, Accuracy: 82.64%\n",
      "Batch 19, Loss: 0.906193, Accuracy: 82.73%\n",
      "Batch 20, Loss: 0.841689, Accuracy: 83.12%\n",
      "Batch 21, Loss: 0.917551, Accuracy: 83.11%\n",
      "Batch 22, Loss: 0.907742, Accuracy: 83.10%\n",
      "Batch 23, Loss: 0.966186, Accuracy: 82.88%\n",
      "Batch 24, Loss: 0.853242, Accuracy: 83.14%\n",
      "Batch 25, Loss: 0.864245, Accuracy: 83.38%\n",
      "Batch 26, Loss: 0.916419, Accuracy: 83.35%\n",
      "Batch 27, Loss: 0.960755, Accuracy: 83.22%\n",
      "Batch 28, Loss: 0.941747, Accuracy: 83.09%\n",
      "Batch 29, Loss: 0.862568, Accuracy: 83.30%\n",
      "Batch 30, Loss: 0.877614, Accuracy: 83.39%\n",
      "Batch 31, Loss: 0.856601, Accuracy: 83.62%\n",
      "Batch 32, Loss: 0.864477, Accuracy: 83.79%\n",
      "Batch 33, Loss: 0.891214, Accuracy: 83.81%\n",
      "Batch 34, Loss: 0.900169, Accuracy: 83.78%\n",
      "Batch 35, Loss: 0.863374, Accuracy: 83.97%\n",
      "Batch 36, Loss: 0.885900, Accuracy: 83.98%\n",
      "Batch 37, Loss: 0.934882, Accuracy: 83.87%\n",
      "Batch 38, Loss: 0.865067, Accuracy: 83.96%\n",
      "Batch 39, Loss: 0.888710, Accuracy: 84.01%\n",
      "Batch 40, Loss: 0.842045, Accuracy: 84.22%\n",
      "Batch 41, Loss: 0.905717, Accuracy: 84.22%\n",
      "Batch 42, Loss: 0.877461, Accuracy: 84.34%\n",
      "Batch 43, Loss: 0.919327, Accuracy: 84.27%\n",
      "Batch 44, Loss: 0.866004, Accuracy: 84.34%\n",
      "Batch 45, Loss: 0.877015, Accuracy: 84.38%\n",
      "Batch 46, Loss: 0.951886, Accuracy: 84.27%\n",
      "Batch 47, Loss: 0.920652, Accuracy: 84.21%\n",
      "Batch 48, Loss: 0.942017, Accuracy: 84.15%\n",
      "Batch 49, Loss: 0.907924, Accuracy: 84.12%\n",
      "Batch 50, Loss: 0.858287, Accuracy: 84.22%\n",
      "Batch 51, Loss: 0.902557, Accuracy: 84.19%\n",
      "Batch 52, Loss: 0.904714, Accuracy: 84.19%\n",
      "Batch 53, Loss: 0.904379, Accuracy: 84.23%\n",
      "Batch 54, Loss: 0.843689, Accuracy: 84.35%\n",
      "Batch 55, Loss: 0.888493, Accuracy: 84.38%\n",
      "Batch 56, Loss: 0.961803, Accuracy: 84.24%\n",
      "Batch 57, Loss: 0.899629, Accuracy: 84.24%\n",
      "Batch 58, Loss: 0.885705, Accuracy: 84.29%\n",
      "Batch 59, Loss: 0.900101, Accuracy: 84.30%\n",
      "Batch 60, Loss: 0.923744, Accuracy: 84.24%\n",
      "Batch 61, Loss: 0.856136, Accuracy: 84.35%\n",
      "Batch 62, Loss: 0.950726, Accuracy: 84.25%\n",
      "Batch 63, Loss: 0.971701, Accuracy: 84.13%\n",
      "Batch 64, Loss: 0.867324, Accuracy: 84.20%\n",
      "Batch 65, Loss: 0.928192, Accuracy: 84.16%\n",
      "Batch 66, Loss: 0.941022, Accuracy: 84.09%\n",
      "Batch 67, Loss: 0.921346, Accuracy: 84.05%\n",
      "Batch 68, Loss: 0.790705, Accuracy: 84.21%\n",
      "Batch 69, Loss: 0.895862, Accuracy: 84.19%\n",
      "Batch 70, Loss: 0.895723, Accuracy: 84.20%\n",
      "Batch 71, Loss: 0.860362, Accuracy: 84.26%\n",
      "Batch 72, Loss: 0.872783, Accuracy: 84.29%\n",
      "Batch 73, Loss: 0.921027, Accuracy: 84.25%\n",
      "Batch 74, Loss: 0.855425, Accuracy: 84.31%\n",
      "Batch 75, Loss: 0.856273, Accuracy: 84.38%\n",
      "Batch 76, Loss: 0.897160, Accuracy: 84.35%\n",
      "Batch 77, Loss: 0.900262, Accuracy: 84.38%\n",
      "Batch 78, Loss: 0.919811, Accuracy: 84.31%\n",
      "Batch 79, Loss: 0.910532, Accuracy: 84.32%\n",
      "Batch 80, Loss: 0.842058, Accuracy: 84.39%\n",
      "Batch 81, Loss: 0.964183, Accuracy: 84.30%\n",
      "Batch 82, Loss: 0.947064, Accuracy: 84.24%\n",
      "Batch 83, Loss: 0.949425, Accuracy: 84.17%\n",
      "Batch 84, Loss: 0.922294, Accuracy: 84.13%\n",
      "Batch 85, Loss: 0.949880, Accuracy: 84.04%\n",
      "Batch 86, Loss: 0.856293, Accuracy: 84.10%\n",
      "Batch 87, Loss: 0.898284, Accuracy: 84.12%\n",
      "Batch 88, Loss: 0.918155, Accuracy: 84.11%\n",
      "Batch 89, Loss: 0.906560, Accuracy: 84.08%\n",
      "Batch 90, Loss: 0.852800, Accuracy: 84.15%\n",
      "Batch 91, Loss: 0.856910, Accuracy: 84.20%\n",
      "Batch 92, Loss: 0.846978, Accuracy: 84.26%\n",
      "Batch 93, Loss: 0.876275, Accuracy: 84.29%\n",
      "Batch 94, Loss: 0.907638, Accuracy: 84.28%\n",
      "Batch 95, Loss: 0.839165, Accuracy: 84.36%\n",
      "Batch 96, Loss: 0.897352, Accuracy: 84.38%\n",
      "Batch 97, Loss: 0.980018, Accuracy: 84.28%\n",
      "Batch 98, Loss: 0.924189, Accuracy: 84.26%\n",
      "Batch 99, Loss: 0.903153, Accuracy: 84.25%\n",
      "Batch 100, Loss: 0.921729, Accuracy: 84.22%\n",
      "Batch 101, Loss: 0.867836, Accuracy: 84.25%\n",
      "Batch 102, Loss: 0.973011, Accuracy: 84.18%\n",
      "Batch 103, Loss: 0.933894, Accuracy: 84.15%\n",
      "Batch 104, Loss: 0.895718, Accuracy: 84.15%\n",
      "Batch 105, Loss: 0.836374, Accuracy: 84.21%\n",
      "Batch 106, Loss: 0.935183, Accuracy: 84.15%\n",
      "Batch 107, Loss: 0.871470, Accuracy: 84.19%\n",
      "Batch 108, Loss: 0.850942, Accuracy: 84.23%\n",
      "Batch 109, Loss: 0.921171, Accuracy: 84.23%\n",
      "Batch 110, Loss: 0.962644, Accuracy: 84.18%\n",
      "Batch 111, Loss: 0.947943, Accuracy: 84.14%\n",
      "Batch 112, Loss: 0.883970, Accuracy: 84.18%\n",
      "Batch 113, Loss: 0.923894, Accuracy: 84.15%\n",
      "Batch 114, Loss: 0.895532, Accuracy: 84.17%\n",
      "Batch 115, Loss: 0.863636, Accuracy: 84.20%\n",
      "Batch 116, Loss: 0.951967, Accuracy: 84.15%\n",
      "Batch 117, Loss: 0.967189, Accuracy: 84.08%\n",
      "Batch 118, Loss: 0.904726, Accuracy: 84.08%\n",
      "Batch 119, Loss: 0.969136, Accuracy: 84.02%\n",
      "Batch 120, Loss: 0.900032, Accuracy: 84.02%\n",
      "Batch 121, Loss: 0.886299, Accuracy: 84.03%\n",
      "Batch 122, Loss: 0.848454, Accuracy: 84.07%\n",
      "Batch 123, Loss: 0.848681, Accuracy: 84.11%\n",
      "Batch 124, Loss: 0.904932, Accuracy: 84.11%\n",
      "Batch 125, Loss: 0.847403, Accuracy: 84.16%\n",
      "Batch 126, Loss: 0.910929, Accuracy: 84.15%\n",
      "Batch 127, Loss: 0.952081, Accuracy: 84.12%\n",
      "Batch 128, Loss: 0.911795, Accuracy: 84.12%\n",
      "Batch 129, Loss: 0.898288, Accuracy: 84.13%\n",
      "Batch 130, Loss: 0.839076, Accuracy: 84.17%\n",
      "Batch 131, Loss: 0.961719, Accuracy: 84.12%\n",
      "Batch 132, Loss: 0.866351, Accuracy: 84.15%\n",
      "Batch 133, Loss: 0.903278, Accuracy: 84.14%\n",
      "Batch 134, Loss: 0.904911, Accuracy: 84.13%\n",
      "Batch 135, Loss: 0.892623, Accuracy: 84.14%\n",
      "Batch 136, Loss: 0.821501, Accuracy: 84.21%\n",
      "Batch 137, Loss: 0.876784, Accuracy: 84.24%\n",
      "Batch 138, Loss: 0.896208, Accuracy: 84.25%\n",
      "Batch 139, Loss: 0.935333, Accuracy: 84.24%\n",
      "Batch 140, Loss: 0.836833, Accuracy: 84.30%\n",
      "Batch 141, Loss: 0.903434, Accuracy: 84.30%\n",
      "Batch 142, Loss: 0.915409, Accuracy: 84.29%\n",
      "Batch 143, Loss: 0.936357, Accuracy: 84.28%\n",
      "Batch 144, Loss: 0.878440, Accuracy: 84.29%\n",
      "Batch 145, Loss: 0.977184, Accuracy: 84.23%\n",
      "Batch 146, Loss: 0.881377, Accuracy: 84.25%\n",
      "Batch 147, Loss: 0.862532, Accuracy: 84.27%\n",
      "Batch 148, Loss: 0.915356, Accuracy: 84.27%\n",
      "Batch 149, Loss: 0.907021, Accuracy: 84.27%\n",
      "Batch 150, Loss: 0.916703, Accuracy: 84.25%\n",
      "Batch 151, Loss: 0.948103, Accuracy: 84.21%\n",
      "Batch 152, Loss: 0.873773, Accuracy: 84.23%\n",
      "Batch 153, Loss: 0.880523, Accuracy: 84.25%\n",
      "Batch 154, Loss: 0.856553, Accuracy: 84.28%\n",
      "Batch 155, Loss: 0.927450, Accuracy: 84.27%\n",
      "Batch 156, Loss: 0.913120, Accuracy: 84.27%\n",
      "Batch 157, Loss: 0.910433, Accuracy: 84.28%\n",
      "Batch 158, Loss: 0.880156, Accuracy: 84.30%\n",
      "Batch 159, Loss: 0.896418, Accuracy: 84.29%\n",
      "Batch 160, Loss: 0.941020, Accuracy: 84.26%\n",
      "Batch 161, Loss: 0.882441, Accuracy: 84.27%\n",
      "Batch 162, Loss: 0.921656, Accuracy: 84.26%\n",
      "Batch 163, Loss: 0.873688, Accuracy: 84.27%\n",
      "Batch 164, Loss: 0.901191, Accuracy: 84.27%\n",
      "Batch 165, Loss: 0.886353, Accuracy: 84.28%\n",
      "Batch 166, Loss: 0.900833, Accuracy: 84.29%\n",
      "Batch 167, Loss: 0.887049, Accuracy: 84.29%\n",
      "Batch 168, Loss: 0.926511, Accuracy: 84.27%\n",
      "Batch 169, Loss: 0.871794, Accuracy: 84.31%\n",
      "Batch 170, Loss: 0.953284, Accuracy: 84.27%\n",
      "Batch 171, Loss: 0.924467, Accuracy: 84.26%\n",
      "Batch 172, Loss: 0.946605, Accuracy: 84.23%\n",
      "Batch 173, Loss: 0.878405, Accuracy: 84.25%\n",
      "Batch 174, Loss: 0.901039, Accuracy: 84.25%\n",
      "Batch 175, Loss: 0.826073, Accuracy: 84.29%\n",
      "Batch 176, Loss: 0.901113, Accuracy: 84.30%\n",
      "Batch 177, Loss: 0.923693, Accuracy: 84.30%\n",
      "Batch 178, Loss: 0.952013, Accuracy: 84.27%\n",
      "Batch 179, Loss: 0.902764, Accuracy: 84.28%\n",
      "Batch 180, Loss: 0.998707, Accuracy: 84.23%\n",
      "Batch 181, Loss: 0.964518, Accuracy: 84.19%\n",
      "Batch 182, Loss: 0.895513, Accuracy: 84.20%\n",
      "Batch 183, Loss: 0.876090, Accuracy: 84.21%\n",
      "Batch 184, Loss: 0.866217, Accuracy: 84.22%\n",
      "Batch 185, Loss: 0.939544, Accuracy: 84.20%\n",
      "Batch 186, Loss: 0.926035, Accuracy: 84.19%\n",
      "Batch 187, Loss: 0.877764, Accuracy: 84.20%\n",
      "Batch 188, Loss: 0.852695, Accuracy: 84.23%\n",
      "Batch 189, Loss: 0.850988, Accuracy: 84.26%\n",
      "Batch 190, Loss: 0.911281, Accuracy: 84.25%\n",
      "Batch 191, Loss: 0.872496, Accuracy: 84.27%\n",
      "Batch 192, Loss: 0.958445, Accuracy: 84.23%\n",
      "Batch 193, Loss: 0.898182, Accuracy: 84.24%\n",
      "Batch 194, Loss: 0.909775, Accuracy: 84.24%\n",
      "Batch 195, Loss: 0.887479, Accuracy: 84.25%\n",
      "Batch 196, Loss: 0.877236, Accuracy: 84.26%\n",
      "Batch 197, Loss: 0.936945, Accuracy: 84.24%\n",
      "Batch 198, Loss: 0.949111, Accuracy: 84.20%\n",
      "Batch 199, Loss: 0.951526, Accuracy: 84.19%\n",
      "Batch 200, Loss: 0.937325, Accuracy: 84.18%\n",
      "Batch 201, Loss: 0.872929, Accuracy: 84.19%\n",
      "Batch 202, Loss: 0.890146, Accuracy: 84.20%\n",
      "Batch 203, Loss: 0.952857, Accuracy: 84.17%\n",
      "Batch 204, Loss: 0.900732, Accuracy: 84.16%\n",
      "Batch 205, Loss: 0.875513, Accuracy: 84.17%\n",
      "Batch 206, Loss: 0.896493, Accuracy: 84.17%\n",
      "Batch 207, Loss: 0.898592, Accuracy: 84.17%\n",
      "Batch 208, Loss: 0.919240, Accuracy: 84.17%\n",
      "Batch 209, Loss: 0.960857, Accuracy: 84.14%\n",
      "Batch 210, Loss: 0.875893, Accuracy: 84.15%\n",
      "Batch 211, Loss: 0.870153, Accuracy: 84.16%\n",
      "Batch 212, Loss: 0.947118, Accuracy: 84.14%\n",
      "Batch 213, Loss: 0.907194, Accuracy: 84.14%\n",
      "Training - Epoch 129, Loss: 0.902630, Accuracy: 84.14%\n",
      "Validation Batch 1, Loss: 0.855411, Accuracy: 89.06%\n",
      "Validation Batch 2, Loss: 0.887932, Accuracy: 86.72%\n",
      "Validation Batch 3, Loss: 0.974241, Accuracy: 83.33%\n",
      "Validation Batch 4, Loss: 0.899851, Accuracy: 83.59%\n",
      "Validation Batch 5, Loss: 0.848491, Accuracy: 85.00%\n",
      "Validation Batch 6, Loss: 0.856445, Accuracy: 85.68%\n",
      "Validation Batch 7, Loss: 0.933682, Accuracy: 84.82%\n",
      "Validation Batch 8, Loss: 0.964427, Accuracy: 83.98%\n",
      "Validation Batch 9, Loss: 0.955238, Accuracy: 83.33%\n",
      "Validation Batch 10, Loss: 0.937902, Accuracy: 82.97%\n",
      "Validation Batch 11, Loss: 0.875973, Accuracy: 83.38%\n",
      "Validation Batch 12, Loss: 0.854555, Accuracy: 83.85%\n",
      "Validation Batch 13, Loss: 0.935680, Accuracy: 83.65%\n",
      "Validation Batch 14, Loss: 0.918121, Accuracy: 83.48%\n",
      "Validation Batch 15, Loss: 0.916427, Accuracy: 83.44%\n",
      "Validation Batch 16, Loss: 0.882427, Accuracy: 83.59%\n",
      "Validation Batch 17, Loss: 0.965051, Accuracy: 83.27%\n",
      "Validation Batch 18, Loss: 0.888462, Accuracy: 83.33%\n",
      "Validation Batch 19, Loss: 0.946443, Accuracy: 83.14%\n",
      "Validation Batch 20, Loss: 0.828723, Accuracy: 83.52%\n",
      "Validation Batch 21, Loss: 0.900801, Accuracy: 83.63%\n",
      "Validation Batch 22, Loss: 0.922377, Accuracy: 83.52%\n",
      "Validation Batch 23, Loss: 0.926489, Accuracy: 83.42%\n",
      "Validation Batch 24, Loss: 0.957336, Accuracy: 83.27%\n",
      "Validation Batch 25, Loss: 0.909324, Accuracy: 83.25%\n",
      "Validation Batch 26, Loss: 0.904461, Accuracy: 83.29%\n",
      "Validation Batch 27, Loss: 0.837757, Accuracy: 83.50%\n",
      "Validation - Epoch 129, Loss: 0.906816, Accuracy: 83.50%\n",
      "Patienceâ€”2\n",
      "Epoch 130\n",
      "Batch 1, Loss: 0.894720, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.829951, Accuracy: 87.50%\n",
      "Batch 3, Loss: 0.893293, Accuracy: 85.94%\n",
      "Batch 4, Loss: 0.889435, Accuracy: 85.55%\n",
      "Batch 5, Loss: 0.844917, Accuracy: 86.56%\n",
      "Batch 6, Loss: 0.853276, Accuracy: 87.24%\n",
      "Batch 7, Loss: 0.902944, Accuracy: 87.05%\n",
      "Batch 8, Loss: 0.914698, Accuracy: 86.52%\n",
      "Batch 9, Loss: 0.791864, Accuracy: 87.50%\n",
      "Batch 10, Loss: 0.969099, Accuracy: 86.41%\n",
      "Batch 11, Loss: 0.884986, Accuracy: 86.36%\n",
      "Batch 12, Loss: 0.896463, Accuracy: 86.20%\n",
      "Batch 13, Loss: 0.948106, Accuracy: 85.82%\n",
      "Batch 14, Loss: 0.776694, Accuracy: 86.72%\n",
      "Batch 15, Loss: 0.905018, Accuracy: 86.56%\n",
      "Batch 16, Loss: 0.817345, Accuracy: 87.01%\n",
      "Batch 17, Loss: 0.976871, Accuracy: 86.40%\n",
      "Batch 18, Loss: 0.886844, Accuracy: 86.28%\n",
      "Batch 19, Loss: 0.907052, Accuracy: 86.18%\n",
      "Batch 20, Loss: 0.943851, Accuracy: 85.86%\n",
      "Batch 21, Loss: 0.890055, Accuracy: 85.79%\n",
      "Batch 22, Loss: 0.894034, Accuracy: 85.80%\n",
      "Batch 23, Loss: 0.938533, Accuracy: 85.53%\n",
      "Batch 24, Loss: 0.901674, Accuracy: 85.48%\n",
      "Batch 25, Loss: 0.861658, Accuracy: 85.62%\n",
      "Batch 26, Loss: 0.895009, Accuracy: 85.64%\n",
      "Batch 27, Loss: 0.954497, Accuracy: 85.36%\n",
      "Batch 28, Loss: 0.919554, Accuracy: 85.27%\n",
      "Batch 29, Loss: 0.932301, Accuracy: 85.18%\n",
      "Batch 30, Loss: 0.917117, Accuracy: 85.05%\n",
      "Batch 31, Loss: 0.937201, Accuracy: 84.93%\n",
      "Batch 32, Loss: 0.961530, Accuracy: 84.77%\n",
      "Batch 33, Loss: 0.860865, Accuracy: 84.90%\n",
      "Batch 34, Loss: 0.937094, Accuracy: 84.74%\n",
      "Batch 35, Loss: 0.915541, Accuracy: 84.69%\n",
      "Batch 36, Loss: 0.899783, Accuracy: 84.68%\n",
      "Batch 37, Loss: 0.890835, Accuracy: 84.67%\n",
      "Batch 38, Loss: 0.884745, Accuracy: 84.75%\n",
      "Batch 39, Loss: 0.814068, Accuracy: 84.98%\n",
      "Batch 40, Loss: 0.947233, Accuracy: 84.84%\n",
      "Batch 41, Loss: 0.859032, Accuracy: 84.95%\n",
      "Batch 42, Loss: 0.938658, Accuracy: 84.86%\n",
      "Batch 43, Loss: 0.936464, Accuracy: 84.74%\n",
      "Batch 44, Loss: 0.891267, Accuracy: 84.80%\n",
      "Batch 45, Loss: 0.916472, Accuracy: 84.76%\n",
      "Batch 46, Loss: 0.907155, Accuracy: 84.75%\n",
      "Batch 47, Loss: 0.860519, Accuracy: 84.84%\n",
      "Batch 48, Loss: 0.931273, Accuracy: 84.77%\n",
      "Batch 49, Loss: 0.836448, Accuracy: 84.92%\n",
      "Batch 50, Loss: 0.907649, Accuracy: 84.88%\n",
      "Batch 51, Loss: 0.946708, Accuracy: 84.77%\n",
      "Batch 52, Loss: 0.883189, Accuracy: 84.83%\n",
      "Batch 53, Loss: 0.874241, Accuracy: 84.85%\n",
      "Batch 54, Loss: 0.911762, Accuracy: 84.81%\n",
      "Batch 55, Loss: 0.872476, Accuracy: 84.89%\n",
      "Batch 56, Loss: 0.900755, Accuracy: 84.88%\n",
      "Batch 57, Loss: 0.918022, Accuracy: 84.84%\n",
      "Batch 58, Loss: 0.938340, Accuracy: 84.75%\n",
      "Batch 59, Loss: 0.937472, Accuracy: 84.67%\n",
      "Batch 60, Loss: 0.823615, Accuracy: 84.82%\n",
      "Batch 61, Loss: 0.843405, Accuracy: 84.94%\n",
      "Batch 62, Loss: 0.884598, Accuracy: 84.93%\n",
      "Batch 63, Loss: 0.841288, Accuracy: 85.02%\n",
      "Batch 64, Loss: 0.825306, Accuracy: 85.16%\n",
      "Batch 65, Loss: 0.970117, Accuracy: 85.02%\n",
      "Batch 66, Loss: 0.917678, Accuracy: 84.99%\n",
      "Batch 67, Loss: 0.905302, Accuracy: 85.00%\n",
      "Batch 68, Loss: 0.832095, Accuracy: 85.09%\n",
      "Batch 69, Loss: 0.821467, Accuracy: 85.19%\n",
      "Batch 70, Loss: 0.920908, Accuracy: 85.18%\n",
      "Batch 71, Loss: 0.914830, Accuracy: 85.15%\n",
      "Batch 72, Loss: 0.856278, Accuracy: 85.20%\n",
      "Batch 73, Loss: 0.836073, Accuracy: 85.30%\n",
      "Batch 74, Loss: 0.951509, Accuracy: 85.20%\n",
      "Batch 75, Loss: 0.857366, Accuracy: 85.25%\n",
      "Batch 76, Loss: 0.861935, Accuracy: 85.30%\n",
      "Batch 77, Loss: 0.987422, Accuracy: 85.17%\n",
      "Batch 78, Loss: 0.855107, Accuracy: 85.24%\n",
      "Batch 79, Loss: 0.954266, Accuracy: 85.15%\n",
      "Batch 80, Loss: 0.917802, Accuracy: 85.10%\n",
      "Batch 81, Loss: 0.846620, Accuracy: 85.17%\n",
      "Batch 82, Loss: 0.961166, Accuracy: 85.08%\n",
      "Batch 83, Loss: 0.999461, Accuracy: 84.94%\n",
      "Batch 84, Loss: 0.935238, Accuracy: 84.88%\n",
      "Batch 85, Loss: 0.880855, Accuracy: 84.89%\n",
      "Batch 86, Loss: 0.946424, Accuracy: 84.83%\n",
      "Batch 87, Loss: 0.948862, Accuracy: 84.75%\n",
      "Batch 88, Loss: 0.931911, Accuracy: 84.71%\n",
      "Batch 89, Loss: 0.923106, Accuracy: 84.69%\n",
      "Batch 90, Loss: 0.883354, Accuracy: 84.70%\n",
      "Batch 91, Loss: 0.943683, Accuracy: 84.63%\n",
      "Batch 92, Loss: 0.876850, Accuracy: 84.63%\n",
      "Batch 93, Loss: 0.880277, Accuracy: 84.66%\n",
      "Batch 94, Loss: 0.900073, Accuracy: 84.64%\n",
      "Batch 95, Loss: 0.893186, Accuracy: 84.64%\n",
      "Batch 96, Loss: 0.915699, Accuracy: 84.64%\n",
      "Batch 97, Loss: 0.959161, Accuracy: 84.55%\n",
      "Batch 98, Loss: 0.984897, Accuracy: 84.45%\n",
      "Batch 99, Loss: 0.935774, Accuracy: 84.42%\n",
      "Batch 100, Loss: 0.928724, Accuracy: 84.39%\n",
      "Batch 101, Loss: 0.891563, Accuracy: 84.39%\n",
      "Batch 102, Loss: 0.907038, Accuracy: 84.39%\n",
      "Batch 103, Loss: 0.902328, Accuracy: 84.38%\n",
      "Batch 104, Loss: 0.919244, Accuracy: 84.34%\n",
      "Batch 105, Loss: 0.896531, Accuracy: 84.35%\n",
      "Batch 106, Loss: 1.002313, Accuracy: 84.24%\n",
      "Batch 107, Loss: 0.903183, Accuracy: 84.23%\n",
      "Batch 108, Loss: 0.957384, Accuracy: 84.17%\n",
      "Batch 109, Loss: 0.868385, Accuracy: 84.20%\n",
      "Batch 110, Loss: 0.915131, Accuracy: 84.20%\n",
      "Batch 111, Loss: 0.902555, Accuracy: 84.19%\n",
      "Batch 112, Loss: 0.926604, Accuracy: 84.17%\n",
      "Batch 113, Loss: 0.852573, Accuracy: 84.22%\n",
      "Batch 114, Loss: 0.845431, Accuracy: 84.28%\n",
      "Batch 115, Loss: 0.886838, Accuracy: 84.29%\n",
      "Batch 116, Loss: 0.911371, Accuracy: 84.28%\n",
      "Batch 117, Loss: 0.915664, Accuracy: 84.27%\n",
      "Batch 118, Loss: 0.844263, Accuracy: 84.32%\n",
      "Batch 119, Loss: 0.938151, Accuracy: 84.27%\n",
      "Batch 120, Loss: 0.893439, Accuracy: 84.28%\n",
      "Batch 121, Loss: 0.905531, Accuracy: 84.28%\n",
      "Batch 122, Loss: 0.927836, Accuracy: 84.26%\n",
      "Batch 123, Loss: 0.861986, Accuracy: 84.29%\n",
      "Batch 124, Loss: 0.951019, Accuracy: 84.25%\n",
      "Batch 125, Loss: 0.908481, Accuracy: 84.25%\n",
      "Batch 126, Loss: 0.960016, Accuracy: 84.21%\n",
      "Batch 127, Loss: 0.937306, Accuracy: 84.18%\n",
      "Batch 128, Loss: 0.869120, Accuracy: 84.20%\n",
      "Batch 129, Loss: 0.886891, Accuracy: 84.22%\n",
      "Batch 130, Loss: 0.884114, Accuracy: 84.24%\n",
      "Batch 131, Loss: 0.878231, Accuracy: 84.27%\n",
      "Batch 132, Loss: 0.925283, Accuracy: 84.24%\n",
      "Batch 133, Loss: 0.944577, Accuracy: 84.21%\n",
      "Batch 134, Loss: 0.840095, Accuracy: 84.25%\n",
      "Batch 135, Loss: 0.921669, Accuracy: 84.21%\n",
      "Batch 136, Loss: 0.883379, Accuracy: 84.24%\n",
      "Batch 137, Loss: 0.872982, Accuracy: 84.25%\n",
      "Batch 138, Loss: 0.893812, Accuracy: 84.26%\n",
      "Batch 139, Loss: 0.872516, Accuracy: 84.29%\n",
      "Batch 140, Loss: 0.918793, Accuracy: 84.26%\n",
      "Batch 141, Loss: 0.959773, Accuracy: 84.22%\n",
      "Batch 142, Loss: 0.883996, Accuracy: 84.24%\n",
      "Batch 143, Loss: 0.942756, Accuracy: 84.22%\n",
      "Batch 144, Loss: 0.970525, Accuracy: 84.19%\n",
      "Batch 145, Loss: 0.903955, Accuracy: 84.19%\n",
      "Batch 146, Loss: 0.915844, Accuracy: 84.19%\n",
      "Batch 147, Loss: 0.965918, Accuracy: 84.14%\n",
      "Batch 148, Loss: 0.864558, Accuracy: 84.16%\n",
      "Batch 149, Loss: 0.918372, Accuracy: 84.14%\n",
      "Batch 150, Loss: 0.918331, Accuracy: 84.14%\n",
      "Batch 151, Loss: 0.922828, Accuracy: 84.12%\n",
      "Batch 152, Loss: 0.889860, Accuracy: 84.13%\n",
      "Batch 153, Loss: 0.970916, Accuracy: 84.09%\n",
      "Batch 154, Loss: 0.808728, Accuracy: 84.16%\n",
      "Batch 155, Loss: 0.892963, Accuracy: 84.17%\n",
      "Batch 156, Loss: 0.940268, Accuracy: 84.14%\n",
      "Batch 157, Loss: 0.910963, Accuracy: 84.14%\n",
      "Batch 158, Loss: 0.921132, Accuracy: 84.13%\n",
      "Batch 159, Loss: 0.887958, Accuracy: 84.15%\n",
      "Batch 160, Loss: 0.909136, Accuracy: 84.16%\n",
      "Batch 161, Loss: 0.933112, Accuracy: 84.13%\n",
      "Batch 162, Loss: 0.914815, Accuracy: 84.11%\n",
      "Batch 163, Loss: 0.982439, Accuracy: 84.06%\n",
      "Batch 164, Loss: 0.837118, Accuracy: 84.10%\n",
      "Batch 165, Loss: 0.882660, Accuracy: 84.12%\n",
      "Batch 166, Loss: 0.987708, Accuracy: 84.07%\n",
      "Batch 167, Loss: 0.837153, Accuracy: 84.10%\n",
      "Batch 168, Loss: 0.907162, Accuracy: 84.11%\n",
      "Batch 169, Loss: 0.912194, Accuracy: 84.10%\n",
      "Batch 170, Loss: 0.932995, Accuracy: 84.08%\n",
      "Batch 171, Loss: 0.941155, Accuracy: 84.06%\n",
      "Batch 172, Loss: 0.871925, Accuracy: 84.08%\n",
      "Batch 173, Loss: 0.867052, Accuracy: 84.10%\n",
      "Batch 174, Loss: 0.855289, Accuracy: 84.13%\n",
      "Batch 175, Loss: 0.984869, Accuracy: 84.09%\n",
      "Batch 176, Loss: 0.894978, Accuracy: 84.10%\n",
      "Batch 177, Loss: 0.858410, Accuracy: 84.13%\n",
      "Batch 178, Loss: 0.865394, Accuracy: 84.16%\n",
      "Batch 179, Loss: 0.935516, Accuracy: 84.14%\n",
      "Batch 180, Loss: 0.965287, Accuracy: 84.10%\n",
      "Batch 181, Loss: 0.911053, Accuracy: 84.08%\n",
      "Batch 182, Loss: 0.944576, Accuracy: 84.05%\n",
      "Batch 183, Loss: 0.846869, Accuracy: 84.08%\n",
      "Batch 184, Loss: 0.902777, Accuracy: 84.09%\n",
      "Batch 185, Loss: 0.873762, Accuracy: 84.10%\n",
      "Batch 186, Loss: 0.930958, Accuracy: 84.08%\n",
      "Batch 187, Loss: 0.847651, Accuracy: 84.11%\n",
      "Batch 188, Loss: 0.928388, Accuracy: 84.09%\n",
      "Batch 189, Loss: 0.893754, Accuracy: 84.09%\n",
      "Batch 190, Loss: 0.958155, Accuracy: 84.07%\n",
      "Batch 191, Loss: 0.860562, Accuracy: 84.11%\n",
      "Batch 192, Loss: 0.792686, Accuracy: 84.17%\n",
      "Batch 193, Loss: 0.892732, Accuracy: 84.17%\n",
      "Batch 194, Loss: 0.987914, Accuracy: 84.13%\n",
      "Batch 195, Loss: 0.872020, Accuracy: 84.14%\n",
      "Batch 196, Loss: 0.887756, Accuracy: 84.16%\n",
      "Batch 197, Loss: 1.013716, Accuracy: 84.11%\n",
      "Batch 198, Loss: 0.865744, Accuracy: 84.12%\n",
      "Batch 199, Loss: 0.884584, Accuracy: 84.12%\n",
      "Batch 200, Loss: 0.885389, Accuracy: 84.12%\n",
      "Batch 201, Loss: 0.924116, Accuracy: 84.12%\n",
      "Batch 202, Loss: 0.922316, Accuracy: 84.11%\n",
      "Batch 203, Loss: 0.874359, Accuracy: 84.12%\n",
      "Batch 204, Loss: 0.880258, Accuracy: 84.13%\n",
      "Batch 205, Loss: 0.917939, Accuracy: 84.12%\n",
      "Batch 206, Loss: 0.932724, Accuracy: 84.12%\n",
      "Batch 207, Loss: 0.827181, Accuracy: 84.16%\n",
      "Batch 208, Loss: 0.937106, Accuracy: 84.13%\n",
      "Batch 209, Loss: 0.887178, Accuracy: 84.14%\n",
      "Batch 210, Loss: 0.850585, Accuracy: 84.17%\n",
      "Batch 211, Loss: 0.910169, Accuracy: 84.16%\n",
      "Batch 212, Loss: 0.866525, Accuracy: 84.18%\n",
      "Batch 213, Loss: 0.907004, Accuracy: 84.17%\n",
      "Training - Epoch 130, Loss: 0.902866, Accuracy: 84.17%\n",
      "Validation Batch 1, Loss: 0.848511, Accuracy: 90.62%\n",
      "Validation Batch 2, Loss: 0.872892, Accuracy: 89.06%\n",
      "Validation Batch 3, Loss: 0.974672, Accuracy: 84.90%\n",
      "Validation Batch 4, Loss: 0.895625, Accuracy: 84.77%\n",
      "Validation Batch 5, Loss: 0.846512, Accuracy: 85.62%\n",
      "Validation Batch 6, Loss: 0.845420, Accuracy: 86.20%\n",
      "Validation Batch 7, Loss: 0.916083, Accuracy: 85.49%\n",
      "Validation Batch 8, Loss: 0.961885, Accuracy: 84.38%\n",
      "Validation Batch 9, Loss: 0.951910, Accuracy: 83.68%\n",
      "Validation Batch 10, Loss: 0.936396, Accuracy: 83.28%\n",
      "Validation Batch 11, Loss: 0.873564, Accuracy: 83.66%\n",
      "Validation Batch 12, Loss: 0.854235, Accuracy: 84.11%\n",
      "Validation Batch 13, Loss: 0.922642, Accuracy: 83.77%\n",
      "Validation Batch 14, Loss: 0.917560, Accuracy: 83.71%\n",
      "Validation Batch 15, Loss: 0.910269, Accuracy: 83.65%\n",
      "Validation Batch 16, Loss: 0.877460, Accuracy: 83.79%\n",
      "Validation Batch 17, Loss: 0.955161, Accuracy: 83.46%\n",
      "Validation Batch 18, Loss: 0.881768, Accuracy: 83.59%\n",
      "Validation Batch 19, Loss: 0.942940, Accuracy: 83.39%\n",
      "Validation Batch 20, Loss: 0.832517, Accuracy: 83.75%\n",
      "Validation Batch 21, Loss: 0.900255, Accuracy: 83.78%\n",
      "Validation Batch 22, Loss: 0.914058, Accuracy: 83.66%\n",
      "Validation Batch 23, Loss: 0.922420, Accuracy: 83.63%\n",
      "Validation Batch 24, Loss: 0.948104, Accuracy: 83.46%\n",
      "Validation Batch 25, Loss: 0.903446, Accuracy: 83.44%\n",
      "Validation Batch 26, Loss: 0.901301, Accuracy: 83.41%\n",
      "Validation Batch 27, Loss: 0.823462, Accuracy: 83.62%\n",
      "Validation - Epoch 130, Loss: 0.901151, Accuracy: 83.62%\n",
      "Patienceâ€”3\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 130  \n",
    "patience = 20  \n",
    "best_val_loss = float('inf')  \n",
    "best_model = None  \n",
    "\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "train_accuracy_list = list()\n",
    "val_accuracy_list = list()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)  \n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(images)  \n",
    "        loss = criterion(outputs, labels)  \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Batch {batch_idx+1}, Loss: {loss.item():.6f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Training - Epoch {epoch+1}, Loss: {train_loss:.6f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()  \n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for batch_idx, (images, labels) in enumerate(val_loader):\n",
    "            images, labels = images.to(device), labels.to(device)  \n",
    "\n",
    "            outputs = model(images) \n",
    "            loss = criterion(outputs, labels) \n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            print(f\"Validation Batch {batch_idx+1}, Loss: {loss.item():.6f}, Accuracy: {100 * val_correct / val_total:.2f}%\")\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    print(f\"Validation - Epoch {epoch+1}, Loss: {val_loss:.6f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accuracy_list.append(val_accuracy)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    print(f'Patienceâ€”{patience_counter}')\n",
    "\n",
    "    # Check for early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "model = best_model\n",
    "torch.save(model.state_dict(), f'{experiment}-{model_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'{experiment}-{model_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_accuracy_list, label='Train Accuracy')\n",
    "\n",
    "# Plotting the second graph\n",
    "plt.plot(val_accuracy_list, label='Val Accuracy')\n",
    "\n",
    "# Adding labels and a legend\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Train vs Val Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_loss_list, label='Train Loss')\n",
    "\n",
    "# Plotting the second graph\n",
    "plt.plot(val_loss_list, label='Val Loss')\n",
    "\n",
    "# Adding labels and a legend\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train vs Val Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch 1, Loss: 0.863547, Accuracy: 85.94%\n",
      "Test Batch 2, Loss: 0.833212, Accuracy: 88.28%\n",
      "Test Batch 3, Loss: 0.852718, Accuracy: 88.54%\n",
      "Test Batch 4, Loss: 0.882028, Accuracy: 87.89%\n",
      "Test Batch 5, Loss: 0.807814, Accuracy: 89.38%\n",
      "Test Batch 6, Loss: 0.923640, Accuracy: 88.28%\n",
      "Test Batch 7, Loss: 0.871746, Accuracy: 87.95%\n",
      "Test Batch 8, Loss: 0.910185, Accuracy: 87.50%\n",
      "Test Batch 9, Loss: 0.832938, Accuracy: 88.02%\n",
      "Test Batch 10, Loss: 0.832936, Accuracy: 88.44%\n",
      "Test Batch 11, Loss: 0.896060, Accuracy: 88.21%\n",
      "Test Batch 12, Loss: 0.854661, Accuracy: 88.41%\n",
      "Test Batch 13, Loss: 0.847278, Accuracy: 88.46%\n",
      "Test Batch 14, Loss: 0.873774, Accuracy: 88.28%\n",
      "Test Batch 15, Loss: 0.925148, Accuracy: 87.81%\n",
      "Test Batch 16, Loss: 0.866219, Accuracy: 87.79%\n",
      "Test Batch 17, Loss: 0.856494, Accuracy: 87.87%\n",
      "Test Batch 18, Loss: 0.904995, Accuracy: 87.59%\n",
      "Test Batch 19, Loss: 0.907451, Accuracy: 87.34%\n",
      "Test Batch 20, Loss: 0.858174, Accuracy: 87.42%\n",
      "Test Batch 21, Loss: 0.843727, Accuracy: 87.57%\n",
      "Test Batch 22, Loss: 0.854366, Accuracy: 87.71%\n",
      "Test Batch 23, Loss: 0.909421, Accuracy: 87.57%\n",
      "Test Batch 24, Loss: 0.847454, Accuracy: 87.63%\n",
      "Test Batch 25, Loss: 0.879234, Accuracy: 87.50%\n",
      "Test Batch 26, Loss: 0.892608, Accuracy: 87.38%\n",
      "Test Batch 27, Loss: 0.943141, Accuracy: 87.21%\n",
      "Test Set - Loss: 0.872999, Accuracy: 87.21%\n",
      "Confusion Matrix:\n",
      "[[ 48  25  29  16]\n",
      " [  0 349  45  45]\n",
      " [  0   9 563   7]\n",
      " [  0   8  34 527]]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        print(f\"Test Batch {batch_idx+1}, Loss: {loss.item():.6f}, Accuracy: {100 * test_correct / test_total:.2f}%\")\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"Test Set - Loss: {test_loss:.6f}, Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "# Calculate percentages for each element\n",
    "cm_percentage = (cm / np.sum(cm)) * 100\n",
    "\n",
    "# Plot confusion matrix with count and percentages\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=['MILD', 'MODERATE', 'NORMAL', 'SEVERE'],\n",
    "            yticklabels=['MILD', 'MODERATE', 'NORMAL', 'SEVERE'],\n",
    "            ax=ax)\n",
    "\n",
    "# Annotate each box with count and percentage\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm[0])):\n",
    "        ax.text(j + 0.5, i + 0.5, f'\\n\\n{cm_percentage[i, j]:.2f}%',\n",
    "                ha='center', va='center', color='black')\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Epoch 1\n",
      "Batch 1, Loss: 0.890514, Accuracy: 85.94%\n",
      "Batch 2, Loss: 0.968432, Accuracy: 82.03%\n",
      "Batch 3, Loss: 0.909088, Accuracy: 82.81%\n",
      "Batch 4, Loss: 0.880478, Accuracy: 83.98%\n",
      "Batch 5, Loss: 0.890358, Accuracy: 84.06%\n",
      "Batch 6, Loss: 0.820727, Accuracy: 85.42%\n",
      "Batch 7, Loss: 0.840786, Accuracy: 86.38%\n",
      "Batch 8, Loss: 0.873823, Accuracy: 86.52%\n",
      "Batch 9, Loss: 0.916289, Accuracy: 86.11%\n",
      "Batch 10, Loss: 0.915615, Accuracy: 85.78%\n",
      "Batch 11, Loss: 0.961150, Accuracy: 85.09%\n",
      "Batch 12, Loss: 0.906034, Accuracy: 84.90%\n",
      "Batch 13, Loss: 0.898553, Accuracy: 84.86%\n",
      "Batch 14, Loss: 0.884389, Accuracy: 85.04%\n",
      "Batch 15, Loss: 0.879476, Accuracy: 85.10%\n",
      "Batch 16, Loss: 0.940196, Accuracy: 84.77%\n",
      "Batch 17, Loss: 0.942966, Accuracy: 84.28%\n",
      "Batch 18, Loss: 0.852851, Accuracy: 84.55%\n",
      "Batch 19, Loss: 0.866116, Accuracy: 84.70%\n",
      "Batch 20, Loss: 0.892155, Accuracy: 84.84%\n",
      "Batch 21, Loss: 0.865259, Accuracy: 85.04%\n",
      "Batch 22, Loss: 0.902459, Accuracy: 85.01%\n",
      "Batch 23, Loss: 0.930854, Accuracy: 84.85%\n",
      "Batch 24, Loss: 0.927141, Accuracy: 84.70%\n",
      "Batch 25, Loss: 0.940538, Accuracy: 84.50%\n",
      "Batch 26, Loss: 0.916637, Accuracy: 84.44%\n",
      "Batch 27, Loss: 0.946887, Accuracy: 84.26%\n",
      "Batch 28, Loss: 0.945142, Accuracy: 84.10%\n",
      "Batch 29, Loss: 0.913901, Accuracy: 84.00%\n",
      "Batch 30, Loss: 0.908088, Accuracy: 83.96%\n",
      "Batch 31, Loss: 0.872271, Accuracy: 84.07%\n",
      "Batch 32, Loss: 0.882858, Accuracy: 84.13%\n",
      "Batch 33, Loss: 0.894850, Accuracy: 84.09%\n",
      "Batch 34, Loss: 0.914279, Accuracy: 84.01%\n",
      "Batch 35, Loss: 0.910928, Accuracy: 83.97%\n",
      "Batch 36, Loss: 0.867690, Accuracy: 84.07%\n",
      "Batch 37, Loss: 0.824244, Accuracy: 84.29%\n",
      "Batch 38, Loss: 0.923607, Accuracy: 84.25%\n",
      "Batch 39, Loss: 0.933984, Accuracy: 84.17%\n",
      "Batch 40, Loss: 0.886705, Accuracy: 84.22%\n",
      "Batch 41, Loss: 0.954211, Accuracy: 84.07%\n",
      "Batch 42, Loss: 0.911579, Accuracy: 84.04%\n",
      "Batch 43, Loss: 0.893193, Accuracy: 84.08%\n",
      "Batch 44, Loss: 0.889928, Accuracy: 84.13%\n",
      "Batch 45, Loss: 0.874269, Accuracy: 84.17%\n",
      "Batch 46, Loss: 0.944126, Accuracy: 84.07%\n",
      "Batch 47, Loss: 0.856331, Accuracy: 84.18%\n",
      "Batch 48, Loss: 0.864350, Accuracy: 84.21%\n",
      "Batch 49, Loss: 0.880796, Accuracy: 84.22%\n",
      "Batch 50, Loss: 0.963513, Accuracy: 84.06%\n",
      "Batch 51, Loss: 0.874918, Accuracy: 84.16%\n",
      "Batch 52, Loss: 0.899467, Accuracy: 84.16%\n",
      "Batch 53, Loss: 0.905896, Accuracy: 84.14%\n",
      "Batch 54, Loss: 0.878416, Accuracy: 84.17%\n",
      "Batch 55, Loss: 0.911576, Accuracy: 84.18%\n",
      "Batch 56, Loss: 0.930157, Accuracy: 84.12%\n",
      "Batch 57, Loss: 0.927988, Accuracy: 84.10%\n",
      "Batch 58, Loss: 0.894111, Accuracy: 84.13%\n",
      "Batch 59, Loss: 0.964034, Accuracy: 84.03%\n",
      "Batch 60, Loss: 0.984396, Accuracy: 83.88%\n",
      "Batch 61, Loss: 0.871632, Accuracy: 83.94%\n",
      "Batch 62, Loss: 0.929034, Accuracy: 83.90%\n",
      "Batch 63, Loss: 0.899137, Accuracy: 83.90%\n",
      "Batch 64, Loss: 0.986415, Accuracy: 83.76%\n",
      "Batch 65, Loss: 0.947703, Accuracy: 83.70%\n",
      "Batch 66, Loss: 0.885408, Accuracy: 83.74%\n",
      "Batch 67, Loss: 0.910440, Accuracy: 83.72%\n",
      "Batch 68, Loss: 0.880096, Accuracy: 83.75%\n",
      "Batch 69, Loss: 0.851523, Accuracy: 83.85%\n",
      "Batch 70, Loss: 0.959295, Accuracy: 83.79%\n",
      "Batch 71, Loss: 0.857013, Accuracy: 83.87%\n",
      "Batch 72, Loss: 0.897192, Accuracy: 83.88%\n",
      "Batch 73, Loss: 0.888466, Accuracy: 83.90%\n",
      "Batch 74, Loss: 0.898186, Accuracy: 83.91%\n",
      "Batch 75, Loss: 0.898913, Accuracy: 83.92%\n",
      "Batch 76, Loss: 0.954411, Accuracy: 83.84%\n",
      "Batch 77, Loss: 0.976246, Accuracy: 83.77%\n",
      "Batch 78, Loss: 0.910955, Accuracy: 83.75%\n",
      "Batch 79, Loss: 0.969663, Accuracy: 83.66%\n",
      "Batch 80, Loss: 0.920717, Accuracy: 83.63%\n",
      "Batch 81, Loss: 0.950171, Accuracy: 83.56%\n",
      "Batch 82, Loss: 0.898885, Accuracy: 83.59%\n",
      "Batch 83, Loss: 0.953522, Accuracy: 83.55%\n",
      "Batch 84, Loss: 0.958283, Accuracy: 83.50%\n",
      "Batch 85, Loss: 0.879869, Accuracy: 83.55%\n",
      "Batch 86, Loss: 0.854994, Accuracy: 83.61%\n",
      "Batch 87, Loss: 0.902626, Accuracy: 83.60%\n",
      "Batch 88, Loss: 0.898974, Accuracy: 83.61%\n",
      "Batch 89, Loss: 0.907524, Accuracy: 83.60%\n",
      "Batch 90, Loss: 0.918646, Accuracy: 83.59%\n",
      "Batch 91, Loss: 0.980248, Accuracy: 83.50%\n",
      "Batch 92, Loss: 0.878939, Accuracy: 83.54%\n",
      "Batch 93, Loss: 0.940914, Accuracy: 83.50%\n",
      "Batch 94, Loss: 0.909501, Accuracy: 83.51%\n",
      "Batch 95, Loss: 0.846139, Accuracy: 83.57%\n",
      "Batch 96, Loss: 0.923307, Accuracy: 83.56%\n",
      "Batch 97, Loss: 0.912642, Accuracy: 83.55%\n",
      "Batch 98, Loss: 0.949067, Accuracy: 83.50%\n",
      "Batch 99, Loss: 0.905209, Accuracy: 83.49%\n",
      "Batch 100, Loss: 0.865143, Accuracy: 83.53%\n",
      "Batch 101, Loss: 0.908973, Accuracy: 83.52%\n",
      "Batch 102, Loss: 0.946126, Accuracy: 83.47%\n",
      "Batch 103, Loss: 0.942938, Accuracy: 83.43%\n",
      "Batch 104, Loss: 0.926373, Accuracy: 83.41%\n",
      "Batch 105, Loss: 0.920012, Accuracy: 83.41%\n",
      "Batch 106, Loss: 0.925182, Accuracy: 83.39%\n",
      "Batch 107, Loss: 1.013712, Accuracy: 83.28%\n",
      "Batch 108, Loss: 0.973128, Accuracy: 83.23%\n",
      "Batch 109, Loss: 0.871136, Accuracy: 83.27%\n",
      "Batch 110, Loss: 0.877621, Accuracy: 83.31%\n",
      "Batch 111, Loss: 0.952726, Accuracy: 83.26%\n",
      "Batch 112, Loss: 0.827888, Accuracy: 83.34%\n",
      "Batch 113, Loss: 1.019196, Accuracy: 83.24%\n",
      "Batch 114, Loss: 0.967263, Accuracy: 83.20%\n",
      "Batch 115, Loss: 0.920830, Accuracy: 83.17%\n",
      "Batch 116, Loss: 0.849593, Accuracy: 83.22%\n",
      "Batch 117, Loss: 0.898765, Accuracy: 83.25%\n",
      "Batch 118, Loss: 0.943475, Accuracy: 83.24%\n",
      "Batch 119, Loss: 0.991503, Accuracy: 83.14%\n",
      "Batch 120, Loss: 0.879164, Accuracy: 83.15%\n",
      "Batch 121, Loss: 0.867888, Accuracy: 83.19%\n",
      "Batch 122, Loss: 0.923234, Accuracy: 83.18%\n",
      "Batch 123, Loss: 0.958413, Accuracy: 83.16%\n",
      "Batch 124, Loss: 0.839352, Accuracy: 83.22%\n",
      "Batch 125, Loss: 0.972204, Accuracy: 83.17%\n",
      "Batch 126, Loss: 0.920268, Accuracy: 83.17%\n",
      "Batch 127, Loss: 0.968760, Accuracy: 83.12%\n",
      "Batch 128, Loss: 0.935653, Accuracy: 83.11%\n",
      "Batch 129, Loss: 0.936301, Accuracy: 83.08%\n",
      "Batch 130, Loss: 0.900159, Accuracy: 83.10%\n",
      "Batch 131, Loss: 0.948106, Accuracy: 83.07%\n",
      "Batch 132, Loss: 0.839111, Accuracy: 83.13%\n",
      "Batch 133, Loss: 0.885712, Accuracy: 83.15%\n",
      "Batch 134, Loss: 0.970507, Accuracy: 83.10%\n",
      "Batch 135, Loss: 0.909715, Accuracy: 83.08%\n",
      "Batch 136, Loss: 0.842200, Accuracy: 83.13%\n",
      "Batch 137, Loss: 0.966162, Accuracy: 83.09%\n",
      "Batch 138, Loss: 0.881300, Accuracy: 83.12%\n",
      "Batch 139, Loss: 0.978186, Accuracy: 83.06%\n",
      "Batch 140, Loss: 0.931731, Accuracy: 83.05%\n",
      "Batch 141, Loss: 0.867761, Accuracy: 83.09%\n",
      "Batch 142, Loss: 0.981919, Accuracy: 83.03%\n",
      "Batch 143, Loss: 0.956800, Accuracy: 83.00%\n",
      "Batch 144, Loss: 0.949255, Accuracy: 82.99%\n",
      "Batch 145, Loss: 0.898290, Accuracy: 82.98%\n",
      "Batch 146, Loss: 0.845766, Accuracy: 83.03%\n",
      "Batch 147, Loss: 0.871812, Accuracy: 83.06%\n",
      "Batch 148, Loss: 0.893102, Accuracy: 83.08%\n",
      "Batch 149, Loss: 0.898358, Accuracy: 83.10%\n",
      "Batch 150, Loss: 0.930989, Accuracy: 83.09%\n",
      "Batch 151, Loss: 0.980860, Accuracy: 83.03%\n",
      "Batch 152, Loss: 0.940643, Accuracy: 83.01%\n",
      "Batch 153, Loss: 0.899957, Accuracy: 83.03%\n",
      "Batch 154, Loss: 0.905412, Accuracy: 83.04%\n",
      "Batch 155, Loss: 0.920724, Accuracy: 83.03%\n",
      "Batch 156, Loss: 0.906781, Accuracy: 83.04%\n",
      "Batch 157, Loss: 0.847514, Accuracy: 83.09%\n",
      "Batch 158, Loss: 0.926811, Accuracy: 83.08%\n",
      "Batch 159, Loss: 0.907694, Accuracy: 83.08%\n",
      "Batch 160, Loss: 0.927887, Accuracy: 83.08%\n",
      "Batch 161, Loss: 0.938451, Accuracy: 83.06%\n",
      "Batch 162, Loss: 0.914038, Accuracy: 83.07%\n",
      "Batch 163, Loss: 0.928581, Accuracy: 83.07%\n",
      "Batch 164, Loss: 0.935809, Accuracy: 83.06%\n",
      "Batch 165, Loss: 0.902871, Accuracy: 83.06%\n",
      "Batch 166, Loss: 0.916053, Accuracy: 83.05%\n",
      "Batch 167, Loss: 0.950265, Accuracy: 83.03%\n",
      "Batch 168, Loss: 0.891429, Accuracy: 83.05%\n",
      "Batch 169, Loss: 0.953710, Accuracy: 83.03%\n",
      "Batch 170, Loss: 0.902200, Accuracy: 83.02%\n",
      "Batch 171, Loss: 0.945681, Accuracy: 83.00%\n",
      "Batch 172, Loss: 0.885543, Accuracy: 83.02%\n",
      "Batch 173, Loss: 0.837663, Accuracy: 83.07%\n",
      "Batch 174, Loss: 0.853035, Accuracy: 83.12%\n",
      "Batch 175, Loss: 0.924920, Accuracy: 83.11%\n",
      "Batch 176, Loss: 1.045657, Accuracy: 83.03%\n",
      "Batch 177, Loss: 0.901715, Accuracy: 83.04%\n",
      "Batch 178, Loss: 0.916280, Accuracy: 83.04%\n",
      "Batch 179, Loss: 0.876840, Accuracy: 83.07%\n",
      "Batch 180, Loss: 0.886282, Accuracy: 83.07%\n",
      "Batch 181, Loss: 0.945471, Accuracy: 83.05%\n",
      "Batch 182, Loss: 0.897931, Accuracy: 83.06%\n",
      "Batch 183, Loss: 0.967075, Accuracy: 83.03%\n",
      "Batch 184, Loss: 0.913039, Accuracy: 83.03%\n",
      "Batch 185, Loss: 0.957359, Accuracy: 83.01%\n",
      "Batch 186, Loss: 0.864055, Accuracy: 83.03%\n",
      "Batch 187, Loss: 0.869686, Accuracy: 83.05%\n",
      "Batch 188, Loss: 0.923885, Accuracy: 83.05%\n",
      "Batch 189, Loss: 0.877160, Accuracy: 83.07%\n",
      "Batch 190, Loss: 0.893969, Accuracy: 83.08%\n",
      "Batch 191, Loss: 0.854903, Accuracy: 83.11%\n",
      "Batch 192, Loss: 0.886348, Accuracy: 83.12%\n",
      "Batch 193, Loss: 0.847895, Accuracy: 83.15%\n",
      "Batch 194, Loss: 0.920413, Accuracy: 83.15%\n",
      "Batch 195, Loss: 0.845423, Accuracy: 83.19%\n",
      "Batch 196, Loss: 0.889744, Accuracy: 83.20%\n",
      "Batch 197, Loss: 0.933440, Accuracy: 83.19%\n",
      "Batch 198, Loss: 0.883111, Accuracy: 83.20%\n",
      "Batch 199, Loss: 0.883963, Accuracy: 83.21%\n",
      "Batch 200, Loss: 0.910984, Accuracy: 83.23%\n",
      "Batch 201, Loss: 0.853003, Accuracy: 83.26%\n",
      "Batch 202, Loss: 0.895377, Accuracy: 83.27%\n",
      "Batch 203, Loss: 0.884493, Accuracy: 83.28%\n",
      "Batch 204, Loss: 0.934466, Accuracy: 83.26%\n",
      "Batch 205, Loss: 0.923075, Accuracy: 83.25%\n",
      "Batch 206, Loss: 0.907906, Accuracy: 83.25%\n",
      "Batch 207, Loss: 0.889346, Accuracy: 83.27%\n",
      "Batch 208, Loss: 0.936717, Accuracy: 83.25%\n",
      "Batch 209, Loss: 0.875447, Accuracy: 83.26%\n",
      "Batch 210, Loss: 0.909381, Accuracy: 83.26%\n",
      "Batch 211, Loss: 0.851863, Accuracy: 83.29%\n",
      "Batch 212, Loss: 0.877730, Accuracy: 83.31%\n",
      "Batch 213, Loss: 0.943592, Accuracy: 83.29%\n",
      "Training - Epoch 1, Loss: 0.910389, Accuracy: 83.29%\n",
      "Validation Batch 1, Loss: 0.837365, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.845203, Accuracy: 91.41%\n",
      "Validation Batch 3, Loss: 0.962481, Accuracy: 86.98%\n",
      "Validation Batch 4, Loss: 0.882090, Accuracy: 87.11%\n",
      "Validation Batch 5, Loss: 0.835268, Accuracy: 87.50%\n",
      "Validation Batch 6, Loss: 0.829609, Accuracy: 88.28%\n",
      "Validation Batch 7, Loss: 0.893895, Accuracy: 87.72%\n",
      "Validation Batch 8, Loss: 0.948958, Accuracy: 86.52%\n",
      "Validation Batch 9, Loss: 0.925144, Accuracy: 85.94%\n",
      "Validation Batch 10, Loss: 0.928287, Accuracy: 85.47%\n",
      "Validation Batch 11, Loss: 0.860069, Accuracy: 85.65%\n",
      "Validation Batch 12, Loss: 0.845820, Accuracy: 85.94%\n",
      "Validation Batch 13, Loss: 0.895917, Accuracy: 85.94%\n",
      "Validation Batch 14, Loss: 0.905909, Accuracy: 85.71%\n",
      "Validation Batch 15, Loss: 0.899751, Accuracy: 85.62%\n",
      "Validation Batch 16, Loss: 0.875463, Accuracy: 85.64%\n",
      "Validation Batch 17, Loss: 0.941196, Accuracy: 85.29%\n",
      "Validation Batch 18, Loss: 0.869586, Accuracy: 85.33%\n",
      "Validation Batch 19, Loss: 0.928866, Accuracy: 85.12%\n",
      "Validation Batch 20, Loss: 0.828760, Accuracy: 85.39%\n",
      "Validation Batch 21, Loss: 0.891350, Accuracy: 85.42%\n",
      "Validation Batch 22, Loss: 0.886900, Accuracy: 85.44%\n",
      "Validation Batch 23, Loss: 0.910865, Accuracy: 85.39%\n",
      "Validation Batch 24, Loss: 0.947748, Accuracy: 85.16%\n",
      "Validation Batch 25, Loss: 0.889766, Accuracy: 85.06%\n",
      "Validation Batch 26, Loss: 0.891093, Accuracy: 85.04%\n",
      "Validation Batch 27, Loss: 0.800459, Accuracy: 85.26%\n",
      "Validation - Epoch 1, Loss: 0.887327, Accuracy: 85.26%\n",
      "Patienceâ€”4\n",
      "Epoch 2\n",
      "Batch 1, Loss: 0.910371, Accuracy: 84.38%\n",
      "Batch 2, Loss: 0.965442, Accuracy: 81.25%\n",
      "Batch 3, Loss: 0.853432, Accuracy: 83.85%\n",
      "Batch 4, Loss: 0.888295, Accuracy: 84.38%\n",
      "Batch 5, Loss: 0.927356, Accuracy: 83.75%\n",
      "Batch 6, Loss: 0.941248, Accuracy: 83.33%\n",
      "Batch 7, Loss: 0.922916, Accuracy: 83.26%\n",
      "Batch 8, Loss: 0.917510, Accuracy: 83.01%\n",
      "Batch 9, Loss: 0.928429, Accuracy: 82.81%\n",
      "Batch 10, Loss: 0.846708, Accuracy: 83.59%\n",
      "Batch 11, Loss: 0.940954, Accuracy: 83.24%\n",
      "Batch 12, Loss: 0.966219, Accuracy: 82.81%\n",
      "Batch 13, Loss: 0.847242, Accuracy: 83.41%\n",
      "Batch 14, Loss: 0.888822, Accuracy: 83.59%\n",
      "Batch 15, Loss: 0.873864, Accuracy: 83.75%\n",
      "Batch 16, Loss: 0.873303, Accuracy: 84.18%\n",
      "Batch 17, Loss: 0.923007, Accuracy: 84.10%\n",
      "Batch 18, Loss: 0.920395, Accuracy: 84.03%\n",
      "Batch 19, Loss: 0.920247, Accuracy: 83.96%\n",
      "Batch 20, Loss: 0.899172, Accuracy: 83.91%\n",
      "Batch 21, Loss: 0.957477, Accuracy: 83.63%\n",
      "Batch 22, Loss: 0.862405, Accuracy: 83.81%\n",
      "Batch 23, Loss: 0.926707, Accuracy: 83.70%\n",
      "Batch 24, Loss: 0.820475, Accuracy: 84.11%\n",
      "Batch 25, Loss: 0.875235, Accuracy: 84.19%\n",
      "Batch 26, Loss: 0.901750, Accuracy: 84.25%\n",
      "Batch 27, Loss: 0.820475, Accuracy: 84.61%\n",
      "Batch 28, Loss: 0.899507, Accuracy: 84.60%\n",
      "Batch 29, Loss: 0.913529, Accuracy: 84.54%\n",
      "Batch 30, Loss: 0.869733, Accuracy: 84.64%\n",
      "Batch 31, Loss: 0.848352, Accuracy: 84.83%\n",
      "Batch 32, Loss: 0.901411, Accuracy: 84.81%\n",
      "Batch 33, Loss: 0.932222, Accuracy: 84.66%\n",
      "Batch 34, Loss: 0.914085, Accuracy: 84.56%\n",
      "Batch 35, Loss: 0.962131, Accuracy: 84.33%\n",
      "Batch 36, Loss: 0.885229, Accuracy: 84.33%\n",
      "Batch 37, Loss: 0.899083, Accuracy: 84.33%\n",
      "Batch 38, Loss: 0.923104, Accuracy: 84.29%\n",
      "Batch 39, Loss: 0.851962, Accuracy: 84.46%\n",
      "Batch 40, Loss: 0.941362, Accuracy: 84.34%\n",
      "Batch 41, Loss: 0.882355, Accuracy: 84.45%\n",
      "Batch 42, Loss: 0.822098, Accuracy: 84.60%\n",
      "Batch 43, Loss: 0.838237, Accuracy: 84.74%\n",
      "Batch 44, Loss: 0.853559, Accuracy: 84.84%\n",
      "Batch 45, Loss: 0.878967, Accuracy: 84.86%\n",
      "Batch 46, Loss: 0.988411, Accuracy: 84.61%\n",
      "Batch 47, Loss: 0.908997, Accuracy: 84.61%\n",
      "Batch 48, Loss: 0.857103, Accuracy: 84.70%\n",
      "Batch 49, Loss: 0.899956, Accuracy: 84.66%\n",
      "Batch 50, Loss: 0.948900, Accuracy: 84.53%\n",
      "Batch 51, Loss: 0.889135, Accuracy: 84.50%\n",
      "Batch 52, Loss: 0.871890, Accuracy: 84.59%\n",
      "Batch 53, Loss: 0.885656, Accuracy: 84.61%\n",
      "Batch 54, Loss: 0.920947, Accuracy: 84.55%\n",
      "Batch 55, Loss: 0.928002, Accuracy: 84.49%\n",
      "Batch 56, Loss: 0.850462, Accuracy: 84.60%\n",
      "Batch 57, Loss: 0.915604, Accuracy: 84.57%\n",
      "Batch 58, Loss: 0.955434, Accuracy: 84.46%\n",
      "Batch 59, Loss: 0.864959, Accuracy: 84.51%\n",
      "Batch 60, Loss: 0.953158, Accuracy: 84.43%\n",
      "Batch 61, Loss: 0.934354, Accuracy: 84.38%\n",
      "Batch 62, Loss: 0.877718, Accuracy: 84.43%\n",
      "Batch 63, Loss: 0.917709, Accuracy: 84.40%\n",
      "Batch 64, Loss: 0.869543, Accuracy: 84.47%\n",
      "Batch 65, Loss: 0.947068, Accuracy: 84.40%\n",
      "Batch 66, Loss: 0.886644, Accuracy: 84.42%\n",
      "Batch 67, Loss: 0.988290, Accuracy: 84.28%\n",
      "Batch 68, Loss: 0.900121, Accuracy: 84.28%\n",
      "Batch 69, Loss: 0.965593, Accuracy: 84.17%\n",
      "Batch 70, Loss: 0.879736, Accuracy: 84.20%\n",
      "Batch 71, Loss: 0.780469, Accuracy: 84.40%\n",
      "Batch 72, Loss: 0.968942, Accuracy: 84.31%\n",
      "Batch 73, Loss: 0.895982, Accuracy: 84.31%\n",
      "Batch 74, Loss: 0.883429, Accuracy: 84.33%\n",
      "Batch 75, Loss: 0.936314, Accuracy: 84.25%\n",
      "Batch 76, Loss: 0.912294, Accuracy: 84.27%\n",
      "Batch 77, Loss: 0.865016, Accuracy: 84.31%\n",
      "Batch 78, Loss: 0.885039, Accuracy: 84.33%\n",
      "Batch 79, Loss: 0.885248, Accuracy: 84.36%\n",
      "Batch 80, Loss: 0.923460, Accuracy: 84.32%\n",
      "Batch 81, Loss: 0.878420, Accuracy: 84.36%\n",
      "Batch 82, Loss: 0.980855, Accuracy: 84.26%\n",
      "Batch 83, Loss: 0.897105, Accuracy: 84.28%\n",
      "Batch 84, Loss: 0.941038, Accuracy: 84.24%\n",
      "Batch 85, Loss: 0.877095, Accuracy: 84.26%\n",
      "Batch 86, Loss: 0.892573, Accuracy: 84.28%\n",
      "Batch 87, Loss: 0.927240, Accuracy: 84.27%\n",
      "Batch 88, Loss: 0.913041, Accuracy: 84.27%\n",
      "Batch 89, Loss: 0.894328, Accuracy: 84.25%\n",
      "Batch 90, Loss: 0.885315, Accuracy: 84.27%\n",
      "Batch 91, Loss: 0.870418, Accuracy: 84.31%\n",
      "Batch 92, Loss: 0.884162, Accuracy: 84.34%\n",
      "Batch 93, Loss: 0.938568, Accuracy: 84.31%\n",
      "Batch 94, Loss: 0.856548, Accuracy: 84.34%\n",
      "Batch 95, Loss: 0.938785, Accuracy: 84.33%\n",
      "Batch 96, Loss: 0.894299, Accuracy: 84.29%\n",
      "Batch 97, Loss: 0.910395, Accuracy: 84.28%\n",
      "Batch 98, Loss: 0.955739, Accuracy: 84.20%\n",
      "Batch 99, Loss: 0.961715, Accuracy: 84.12%\n",
      "Batch 100, Loss: 0.902739, Accuracy: 84.14%\n",
      "Batch 101, Loss: 0.897705, Accuracy: 84.14%\n",
      "Batch 102, Loss: 0.949109, Accuracy: 84.11%\n",
      "Batch 103, Loss: 0.929688, Accuracy: 84.09%\n",
      "Batch 104, Loss: 0.928509, Accuracy: 84.07%\n",
      "Batch 105, Loss: 0.850113, Accuracy: 84.12%\n",
      "Batch 106, Loss: 0.879361, Accuracy: 84.15%\n",
      "Batch 107, Loss: 0.856922, Accuracy: 84.19%\n",
      "Batch 108, Loss: 0.881042, Accuracy: 84.19%\n",
      "Batch 109, Loss: 0.908035, Accuracy: 84.16%\n",
      "Batch 110, Loss: 0.937567, Accuracy: 84.11%\n",
      "Batch 111, Loss: 0.978228, Accuracy: 84.04%\n",
      "Batch 112, Loss: 0.877414, Accuracy: 84.07%\n",
      "Batch 113, Loss: 0.861762, Accuracy: 84.11%\n",
      "Batch 114, Loss: 0.901810, Accuracy: 84.13%\n",
      "Batch 115, Loss: 0.821548, Accuracy: 84.18%\n",
      "Batch 116, Loss: 0.901230, Accuracy: 84.19%\n",
      "Batch 117, Loss: 0.911980, Accuracy: 84.16%\n",
      "Batch 118, Loss: 0.866872, Accuracy: 84.19%\n",
      "Batch 119, Loss: 0.924731, Accuracy: 84.16%\n",
      "Batch 120, Loss: 0.900603, Accuracy: 84.18%\n",
      "Batch 121, Loss: 0.902841, Accuracy: 84.19%\n",
      "Batch 122, Loss: 0.888071, Accuracy: 84.21%\n",
      "Batch 123, Loss: 0.936355, Accuracy: 84.17%\n",
      "Batch 124, Loss: 0.907535, Accuracy: 84.17%\n",
      "Batch 125, Loss: 0.895334, Accuracy: 84.17%\n",
      "Batch 126, Loss: 0.937782, Accuracy: 84.15%\n",
      "Batch 127, Loss: 0.899376, Accuracy: 84.14%\n",
      "Batch 128, Loss: 0.896866, Accuracy: 84.14%\n",
      "Batch 129, Loss: 0.906310, Accuracy: 84.14%\n",
      "Batch 130, Loss: 0.926417, Accuracy: 84.12%\n",
      "Batch 131, Loss: 0.969993, Accuracy: 84.06%\n",
      "Batch 132, Loss: 0.890415, Accuracy: 84.07%\n",
      "Batch 133, Loss: 0.866425, Accuracy: 84.09%\n",
      "Batch 134, Loss: 0.883986, Accuracy: 84.11%\n",
      "Batch 135, Loss: 0.805071, Accuracy: 84.19%\n",
      "Batch 136, Loss: 0.889844, Accuracy: 84.20%\n",
      "Batch 137, Loss: 0.851912, Accuracy: 84.23%\n",
      "Batch 138, Loss: 0.912347, Accuracy: 84.22%\n",
      "Batch 139, Loss: 0.973332, Accuracy: 84.16%\n",
      "Batch 140, Loss: 0.862865, Accuracy: 84.21%\n",
      "Batch 141, Loss: 0.913877, Accuracy: 84.20%\n",
      "Batch 142, Loss: 0.881670, Accuracy: 84.22%\n",
      "Batch 143, Loss: 0.856007, Accuracy: 84.27%\n",
      "Batch 144, Loss: 0.963532, Accuracy: 84.21%\n",
      "Batch 145, Loss: 0.906058, Accuracy: 84.20%\n",
      "Batch 146, Loss: 0.908517, Accuracy: 84.20%\n",
      "Batch 147, Loss: 0.858290, Accuracy: 84.24%\n",
      "Batch 148, Loss: 1.100658, Accuracy: 84.10%\n",
      "Batch 149, Loss: 0.916231, Accuracy: 84.09%\n",
      "Batch 150, Loss: 0.893359, Accuracy: 84.10%\n",
      "Batch 151, Loss: 0.971070, Accuracy: 84.06%\n",
      "Batch 152, Loss: 0.940597, Accuracy: 84.04%\n",
      "Batch 153, Loss: 0.911986, Accuracy: 84.04%\n",
      "Batch 154, Loss: 0.904214, Accuracy: 84.05%\n",
      "Batch 155, Loss: 0.865519, Accuracy: 84.07%\n",
      "Batch 156, Loss: 0.902782, Accuracy: 84.06%\n",
      "Batch 157, Loss: 0.919364, Accuracy: 84.06%\n",
      "Batch 158, Loss: 0.913784, Accuracy: 84.05%\n",
      "Batch 159, Loss: 0.843246, Accuracy: 84.09%\n",
      "Batch 160, Loss: 0.935844, Accuracy: 84.07%\n",
      "Batch 161, Loss: 0.928834, Accuracy: 84.05%\n",
      "Batch 162, Loss: 0.857248, Accuracy: 84.10%\n",
      "Batch 163, Loss: 0.868291, Accuracy: 84.12%\n",
      "Batch 164, Loss: 0.923752, Accuracy: 84.10%\n",
      "Batch 165, Loss: 0.905625, Accuracy: 84.09%\n",
      "Batch 166, Loss: 0.839725, Accuracy: 84.12%\n",
      "Batch 167, Loss: 0.832838, Accuracy: 84.17%\n",
      "Batch 168, Loss: 0.888792, Accuracy: 84.17%\n",
      "Batch 169, Loss: 0.940719, Accuracy: 84.15%\n",
      "Batch 170, Loss: 0.819723, Accuracy: 84.21%\n",
      "Batch 171, Loss: 0.914062, Accuracy: 84.20%\n",
      "Batch 172, Loss: 0.963350, Accuracy: 84.17%\n",
      "Batch 173, Loss: 0.905887, Accuracy: 84.17%\n",
      "Batch 174, Loss: 0.947772, Accuracy: 84.14%\n",
      "Batch 175, Loss: 0.905380, Accuracy: 84.14%\n",
      "Batch 176, Loss: 0.963149, Accuracy: 84.12%\n",
      "Batch 177, Loss: 0.961895, Accuracy: 84.08%\n",
      "Batch 178, Loss: 0.901108, Accuracy: 84.09%\n",
      "Batch 179, Loss: 0.964838, Accuracy: 84.06%\n",
      "Batch 180, Loss: 0.994303, Accuracy: 84.01%\n",
      "Batch 181, Loss: 0.894890, Accuracy: 84.02%\n",
      "Batch 182, Loss: 0.902943, Accuracy: 84.03%\n",
      "Batch 183, Loss: 0.851638, Accuracy: 84.06%\n",
      "Batch 184, Loss: 0.838614, Accuracy: 84.09%\n",
      "Batch 185, Loss: 0.918387, Accuracy: 84.08%\n",
      "Batch 186, Loss: 0.926359, Accuracy: 84.06%\n",
      "Batch 187, Loss: 0.874734, Accuracy: 84.08%\n",
      "Batch 188, Loss: 0.825525, Accuracy: 84.13%\n",
      "Batch 189, Loss: 0.884657, Accuracy: 84.14%\n",
      "Batch 190, Loss: 0.951118, Accuracy: 84.11%\n",
      "Batch 191, Loss: 0.927872, Accuracy: 84.10%\n",
      "Batch 192, Loss: 0.917678, Accuracy: 84.09%\n",
      "Batch 193, Loss: 0.948543, Accuracy: 84.07%\n",
      "Batch 194, Loss: 0.911558, Accuracy: 84.06%\n",
      "Batch 195, Loss: 0.871345, Accuracy: 84.08%\n",
      "Batch 196, Loss: 0.863741, Accuracy: 84.10%\n",
      "Batch 197, Loss: 0.938117, Accuracy: 84.09%\n",
      "Batch 198, Loss: 0.879800, Accuracy: 84.11%\n",
      "Batch 199, Loss: 0.870427, Accuracy: 84.12%\n",
      "Batch 200, Loss: 0.960706, Accuracy: 84.09%\n",
      "Batch 201, Loss: 0.884918, Accuracy: 84.09%\n",
      "Batch 202, Loss: 0.940383, Accuracy: 84.07%\n",
      "Batch 203, Loss: 0.905906, Accuracy: 84.07%\n",
      "Batch 204, Loss: 0.889598, Accuracy: 84.08%\n",
      "Batch 205, Loss: 0.892893, Accuracy: 84.09%\n",
      "Batch 206, Loss: 0.921030, Accuracy: 84.08%\n",
      "Batch 207, Loss: 0.811866, Accuracy: 84.13%\n",
      "Batch 208, Loss: 0.980189, Accuracy: 84.09%\n",
      "Batch 209, Loss: 0.892703, Accuracy: 84.08%\n",
      "Batch 210, Loss: 0.894831, Accuracy: 84.09%\n",
      "Batch 211, Loss: 0.880120, Accuracy: 84.11%\n",
      "Batch 212, Loss: 0.923921, Accuracy: 84.09%\n",
      "Batch 213, Loss: 0.901432, Accuracy: 84.10%\n",
      "Training - Epoch 2, Loss: 0.903559, Accuracy: 84.10%\n",
      "Validation Batch 1, Loss: 0.837612, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.844961, Accuracy: 91.41%\n",
      "Validation Batch 3, Loss: 0.961722, Accuracy: 86.98%\n",
      "Validation Batch 4, Loss: 0.881998, Accuracy: 86.72%\n",
      "Validation Batch 5, Loss: 0.833884, Accuracy: 87.19%\n",
      "Validation Batch 6, Loss: 0.828894, Accuracy: 88.02%\n",
      "Validation Batch 7, Loss: 0.892943, Accuracy: 87.50%\n",
      "Validation Batch 8, Loss: 0.948152, Accuracy: 86.33%\n",
      "Validation Batch 9, Loss: 0.924048, Accuracy: 85.59%\n",
      "Validation Batch 10, Loss: 0.927163, Accuracy: 85.16%\n",
      "Validation Batch 11, Loss: 0.859355, Accuracy: 85.37%\n",
      "Validation Batch 12, Loss: 0.845518, Accuracy: 85.68%\n",
      "Validation Batch 13, Loss: 0.894467, Accuracy: 85.70%\n",
      "Validation Batch 14, Loss: 0.904750, Accuracy: 85.49%\n",
      "Validation Batch 15, Loss: 0.899866, Accuracy: 85.42%\n",
      "Validation Batch 16, Loss: 0.875299, Accuracy: 85.45%\n",
      "Validation Batch 17, Loss: 0.940496, Accuracy: 85.11%\n",
      "Validation Batch 18, Loss: 0.868933, Accuracy: 85.24%\n",
      "Validation Batch 19, Loss: 0.928182, Accuracy: 85.12%\n",
      "Validation Batch 20, Loss: 0.828238, Accuracy: 85.39%\n",
      "Validation Batch 21, Loss: 0.890647, Accuracy: 85.42%\n",
      "Validation Batch 22, Loss: 0.885264, Accuracy: 85.44%\n",
      "Validation Batch 23, Loss: 0.908951, Accuracy: 85.39%\n",
      "Validation Batch 24, Loss: 0.946913, Accuracy: 85.16%\n",
      "Validation Batch 25, Loss: 0.888523, Accuracy: 85.06%\n",
      "Validation Batch 26, Loss: 0.889894, Accuracy: 85.04%\n",
      "Validation Batch 27, Loss: 0.798922, Accuracy: 85.26%\n",
      "Validation - Epoch 2, Loss: 0.886504, Accuracy: 85.26%\n",
      "Patienceâ€”5\n",
      "Epoch 3\n",
      "Batch 1, Loss: 0.916176, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.881386, Accuracy: 85.16%\n",
      "Batch 3, Loss: 0.948752, Accuracy: 83.85%\n",
      "Batch 4, Loss: 0.929815, Accuracy: 83.20%\n",
      "Batch 5, Loss: 0.834933, Accuracy: 84.69%\n",
      "Batch 6, Loss: 0.847823, Accuracy: 85.68%\n",
      "Batch 7, Loss: 0.882221, Accuracy: 85.94%\n",
      "Batch 8, Loss: 0.892667, Accuracy: 85.94%\n",
      "Batch 9, Loss: 0.961523, Accuracy: 85.07%\n",
      "Batch 10, Loss: 0.872986, Accuracy: 85.16%\n",
      "Batch 11, Loss: 0.878146, Accuracy: 85.23%\n",
      "Batch 12, Loss: 0.867648, Accuracy: 85.55%\n",
      "Batch 13, Loss: 0.979298, Accuracy: 84.98%\n",
      "Batch 14, Loss: 0.869131, Accuracy: 85.16%\n",
      "Batch 15, Loss: 0.934474, Accuracy: 85.00%\n",
      "Batch 16, Loss: 0.929956, Accuracy: 84.57%\n",
      "Batch 17, Loss: 0.898892, Accuracy: 84.56%\n",
      "Batch 18, Loss: 0.863736, Accuracy: 84.72%\n",
      "Batch 19, Loss: 0.874723, Accuracy: 84.87%\n",
      "Batch 20, Loss: 0.882949, Accuracy: 85.00%\n",
      "Batch 21, Loss: 0.922412, Accuracy: 84.82%\n",
      "Batch 22, Loss: 0.891253, Accuracy: 84.87%\n",
      "Batch 23, Loss: 0.903720, Accuracy: 84.78%\n",
      "Batch 24, Loss: 0.896861, Accuracy: 84.70%\n",
      "Batch 25, Loss: 0.927700, Accuracy: 84.56%\n",
      "Batch 26, Loss: 0.887572, Accuracy: 84.62%\n",
      "Batch 27, Loss: 0.898651, Accuracy: 84.66%\n",
      "Batch 28, Loss: 0.942545, Accuracy: 84.49%\n",
      "Batch 29, Loss: 0.894784, Accuracy: 84.48%\n",
      "Batch 30, Loss: 1.000095, Accuracy: 84.11%\n",
      "Batch 31, Loss: 0.863637, Accuracy: 84.27%\n",
      "Batch 32, Loss: 0.972813, Accuracy: 83.98%\n",
      "Batch 33, Loss: 0.890300, Accuracy: 84.04%\n",
      "Batch 34, Loss: 0.905559, Accuracy: 84.05%\n",
      "Batch 35, Loss: 0.854302, Accuracy: 84.15%\n",
      "Batch 36, Loss: 0.934463, Accuracy: 84.11%\n",
      "Batch 37, Loss: 0.900296, Accuracy: 84.16%\n",
      "Batch 38, Loss: 0.951910, Accuracy: 84.00%\n",
      "Batch 39, Loss: 0.914454, Accuracy: 84.01%\n",
      "Batch 40, Loss: 0.891072, Accuracy: 84.06%\n",
      "Batch 41, Loss: 0.909874, Accuracy: 84.03%\n",
      "Batch 42, Loss: 0.981844, Accuracy: 83.82%\n",
      "Batch 43, Loss: 0.970812, Accuracy: 83.65%\n",
      "Batch 44, Loss: 0.933781, Accuracy: 83.63%\n",
      "Batch 45, Loss: 0.909885, Accuracy: 83.65%\n",
      "Batch 46, Loss: 0.921293, Accuracy: 83.66%\n",
      "Batch 47, Loss: 0.938393, Accuracy: 83.61%\n",
      "Batch 48, Loss: 0.894233, Accuracy: 83.63%\n",
      "Batch 49, Loss: 0.903110, Accuracy: 83.64%\n",
      "Batch 50, Loss: 0.868629, Accuracy: 83.72%\n",
      "Batch 51, Loss: 0.894526, Accuracy: 83.70%\n",
      "Batch 52, Loss: 0.919312, Accuracy: 83.65%\n",
      "Batch 53, Loss: 0.929566, Accuracy: 83.61%\n",
      "Batch 54, Loss: 0.875369, Accuracy: 83.68%\n",
      "Batch 55, Loss: 0.928253, Accuracy: 83.64%\n",
      "Batch 56, Loss: 0.856260, Accuracy: 83.76%\n",
      "Batch 57, Loss: 0.923036, Accuracy: 83.72%\n",
      "Batch 58, Loss: 0.942871, Accuracy: 83.65%\n",
      "Batch 59, Loss: 0.912193, Accuracy: 83.63%\n",
      "Batch 60, Loss: 0.901510, Accuracy: 83.62%\n",
      "Batch 61, Loss: 0.885140, Accuracy: 83.63%\n",
      "Batch 62, Loss: 0.892144, Accuracy: 83.64%\n",
      "Batch 63, Loss: 0.856822, Accuracy: 83.73%\n",
      "Batch 64, Loss: 0.907207, Accuracy: 83.74%\n",
      "Batch 65, Loss: 0.920644, Accuracy: 83.73%\n",
      "Batch 66, Loss: 0.842951, Accuracy: 83.83%\n",
      "Batch 67, Loss: 0.857632, Accuracy: 83.89%\n",
      "Batch 68, Loss: 0.908088, Accuracy: 83.89%\n",
      "Batch 69, Loss: 1.026525, Accuracy: 83.70%\n",
      "Batch 70, Loss: 0.837252, Accuracy: 83.79%\n",
      "Batch 71, Loss: 0.876730, Accuracy: 83.85%\n",
      "Batch 72, Loss: 0.927082, Accuracy: 83.81%\n",
      "Batch 73, Loss: 0.940202, Accuracy: 83.75%\n",
      "Batch 74, Loss: 0.884675, Accuracy: 83.78%\n",
      "Batch 75, Loss: 0.903583, Accuracy: 83.79%\n",
      "Batch 76, Loss: 0.888408, Accuracy: 83.82%\n",
      "Batch 77, Loss: 0.914493, Accuracy: 83.81%\n",
      "Batch 78, Loss: 0.857967, Accuracy: 83.85%\n",
      "Batch 79, Loss: 0.919462, Accuracy: 83.84%\n",
      "Batch 80, Loss: 0.866233, Accuracy: 83.89%\n",
      "Batch 81, Loss: 0.875221, Accuracy: 83.91%\n",
      "Batch 82, Loss: 0.823568, Accuracy: 84.01%\n",
      "Batch 83, Loss: 0.883770, Accuracy: 84.05%\n",
      "Batch 84, Loss: 0.949582, Accuracy: 83.98%\n",
      "Batch 85, Loss: 0.843855, Accuracy: 84.06%\n",
      "Batch 86, Loss: 0.866974, Accuracy: 84.14%\n",
      "Batch 87, Loss: 0.835992, Accuracy: 84.21%\n",
      "Batch 88, Loss: 0.939959, Accuracy: 84.16%\n",
      "Batch 89, Loss: 0.843120, Accuracy: 84.23%\n",
      "Batch 90, Loss: 0.857391, Accuracy: 84.31%\n",
      "Batch 91, Loss: 0.934314, Accuracy: 84.25%\n",
      "Batch 92, Loss: 0.875144, Accuracy: 84.31%\n",
      "Batch 93, Loss: 0.897264, Accuracy: 84.31%\n",
      "Batch 94, Loss: 0.859298, Accuracy: 84.34%\n",
      "Batch 95, Loss: 0.869621, Accuracy: 84.41%\n",
      "Batch 96, Loss: 0.913054, Accuracy: 84.39%\n",
      "Batch 97, Loss: 0.879342, Accuracy: 84.42%\n",
      "Batch 98, Loss: 0.890181, Accuracy: 84.44%\n",
      "Batch 99, Loss: 0.868303, Accuracy: 84.47%\n",
      "Batch 100, Loss: 0.921105, Accuracy: 84.44%\n",
      "Batch 101, Loss: 0.936975, Accuracy: 84.39%\n",
      "Batch 102, Loss: 0.896559, Accuracy: 84.38%\n",
      "Batch 103, Loss: 0.989176, Accuracy: 84.28%\n",
      "Batch 104, Loss: 0.900286, Accuracy: 84.27%\n",
      "Batch 105, Loss: 0.845261, Accuracy: 84.33%\n",
      "Batch 106, Loss: 0.898179, Accuracy: 84.33%\n",
      "Batch 107, Loss: 0.902691, Accuracy: 84.33%\n",
      "Batch 108, Loss: 0.862165, Accuracy: 84.38%\n",
      "Batch 109, Loss: 0.873159, Accuracy: 84.40%\n",
      "Batch 110, Loss: 0.948425, Accuracy: 84.38%\n",
      "Batch 111, Loss: 0.912218, Accuracy: 84.36%\n",
      "Batch 112, Loss: 0.917336, Accuracy: 84.36%\n",
      "Batch 113, Loss: 0.872254, Accuracy: 84.39%\n",
      "Batch 114, Loss: 0.943062, Accuracy: 84.36%\n",
      "Batch 115, Loss: 0.828457, Accuracy: 84.42%\n",
      "Batch 116, Loss: 0.893209, Accuracy: 84.42%\n",
      "Batch 117, Loss: 0.916294, Accuracy: 84.40%\n",
      "Batch 118, Loss: 0.927116, Accuracy: 84.38%\n",
      "Batch 119, Loss: 0.921595, Accuracy: 84.36%\n",
      "Batch 120, Loss: 0.961235, Accuracy: 84.28%\n",
      "Batch 121, Loss: 0.888879, Accuracy: 84.28%\n",
      "Batch 122, Loss: 0.846368, Accuracy: 84.34%\n",
      "Batch 123, Loss: 0.910463, Accuracy: 84.34%\n",
      "Batch 124, Loss: 0.889478, Accuracy: 84.35%\n",
      "Batch 125, Loss: 0.959804, Accuracy: 84.30%\n",
      "Batch 126, Loss: 0.892237, Accuracy: 84.30%\n",
      "Batch 127, Loss: 0.896157, Accuracy: 84.31%\n",
      "Batch 128, Loss: 1.006053, Accuracy: 84.24%\n",
      "Batch 129, Loss: 0.966721, Accuracy: 84.18%\n",
      "Batch 130, Loss: 0.906394, Accuracy: 84.18%\n",
      "Batch 131, Loss: 0.876286, Accuracy: 84.20%\n",
      "Batch 132, Loss: 0.915676, Accuracy: 84.19%\n",
      "Batch 133, Loss: 0.999340, Accuracy: 84.12%\n",
      "Batch 134, Loss: 0.944326, Accuracy: 84.07%\n",
      "Batch 135, Loss: 0.862797, Accuracy: 84.10%\n",
      "Batch 136, Loss: 0.899574, Accuracy: 84.10%\n",
      "Batch 137, Loss: 0.910391, Accuracy: 84.10%\n",
      "Batch 138, Loss: 0.889597, Accuracy: 84.11%\n",
      "Batch 139, Loss: 0.922128, Accuracy: 84.11%\n",
      "Batch 140, Loss: 0.974953, Accuracy: 84.05%\n",
      "Batch 141, Loss: 0.912491, Accuracy: 84.05%\n",
      "Batch 142, Loss: 0.897600, Accuracy: 84.03%\n",
      "Batch 143, Loss: 0.970821, Accuracy: 83.98%\n",
      "Batch 144, Loss: 0.884798, Accuracy: 84.00%\n",
      "Batch 145, Loss: 0.840561, Accuracy: 84.04%\n",
      "Batch 146, Loss: 0.979980, Accuracy: 83.99%\n",
      "Batch 147, Loss: 0.952300, Accuracy: 83.95%\n",
      "Batch 148, Loss: 0.883860, Accuracy: 83.96%\n",
      "Batch 149, Loss: 0.864458, Accuracy: 84.00%\n",
      "Batch 150, Loss: 0.854725, Accuracy: 84.02%\n",
      "Batch 151, Loss: 0.998060, Accuracy: 83.95%\n",
      "Batch 152, Loss: 0.967220, Accuracy: 83.90%\n",
      "Batch 153, Loss: 0.864339, Accuracy: 83.93%\n",
      "Batch 154, Loss: 0.884551, Accuracy: 83.93%\n",
      "Batch 155, Loss: 0.936420, Accuracy: 83.90%\n",
      "Batch 156, Loss: 0.927872, Accuracy: 83.87%\n",
      "Batch 157, Loss: 0.908521, Accuracy: 83.87%\n",
      "Batch 158, Loss: 0.880053, Accuracy: 83.89%\n",
      "Batch 159, Loss: 0.950584, Accuracy: 83.85%\n",
      "Batch 160, Loss: 0.892860, Accuracy: 83.86%\n",
      "Batch 161, Loss: 0.836275, Accuracy: 83.90%\n",
      "Batch 162, Loss: 0.943762, Accuracy: 83.86%\n",
      "Batch 163, Loss: 0.889499, Accuracy: 83.87%\n",
      "Batch 164, Loss: 0.911938, Accuracy: 83.87%\n",
      "Batch 165, Loss: 0.861841, Accuracy: 83.90%\n",
      "Batch 166, Loss: 0.890966, Accuracy: 83.91%\n",
      "Batch 167, Loss: 0.923371, Accuracy: 83.91%\n",
      "Batch 168, Loss: 0.884943, Accuracy: 83.92%\n",
      "Batch 169, Loss: 0.905936, Accuracy: 83.91%\n",
      "Batch 170, Loss: 0.880721, Accuracy: 83.92%\n",
      "Batch 171, Loss: 0.883790, Accuracy: 83.95%\n",
      "Batch 172, Loss: 0.897895, Accuracy: 83.96%\n",
      "Batch 173, Loss: 0.850551, Accuracy: 83.99%\n",
      "Batch 174, Loss: 0.983446, Accuracy: 83.93%\n",
      "Batch 175, Loss: 0.905331, Accuracy: 83.93%\n",
      "Batch 176, Loss: 0.843016, Accuracy: 83.97%\n",
      "Batch 177, Loss: 0.916192, Accuracy: 83.96%\n",
      "Batch 178, Loss: 0.898526, Accuracy: 83.97%\n",
      "Batch 179, Loss: 0.947457, Accuracy: 83.94%\n",
      "Batch 180, Loss: 0.888796, Accuracy: 83.95%\n",
      "Batch 181, Loss: 0.902922, Accuracy: 83.95%\n",
      "Batch 182, Loss: 0.945654, Accuracy: 83.93%\n",
      "Batch 183, Loss: 0.886755, Accuracy: 83.94%\n",
      "Batch 184, Loss: 0.923394, Accuracy: 83.92%\n",
      "Batch 185, Loss: 0.983865, Accuracy: 83.87%\n",
      "Batch 186, Loss: 0.960264, Accuracy: 83.83%\n",
      "Batch 187, Loss: 0.955072, Accuracy: 83.81%\n",
      "Batch 188, Loss: 0.969569, Accuracy: 83.77%\n",
      "Batch 189, Loss: 0.868262, Accuracy: 83.79%\n",
      "Batch 190, Loss: 0.932642, Accuracy: 83.78%\n",
      "Batch 191, Loss: 0.855243, Accuracy: 83.80%\n",
      "Batch 192, Loss: 0.937745, Accuracy: 83.77%\n",
      "Batch 193, Loss: 0.981046, Accuracy: 83.72%\n",
      "Batch 194, Loss: 0.866085, Accuracy: 83.75%\n",
      "Batch 195, Loss: 0.919620, Accuracy: 83.74%\n",
      "Batch 196, Loss: 0.994653, Accuracy: 83.70%\n",
      "Batch 197, Loss: 0.917162, Accuracy: 83.70%\n",
      "Batch 198, Loss: 0.911627, Accuracy: 83.70%\n",
      "Batch 199, Loss: 0.897618, Accuracy: 83.71%\n",
      "Batch 200, Loss: 0.888438, Accuracy: 83.73%\n",
      "Batch 201, Loss: 0.813408, Accuracy: 83.77%\n",
      "Batch 202, Loss: 0.908184, Accuracy: 83.76%\n",
      "Batch 203, Loss: 0.886199, Accuracy: 83.77%\n",
      "Batch 204, Loss: 0.850764, Accuracy: 83.80%\n",
      "Batch 205, Loss: 0.929257, Accuracy: 83.80%\n",
      "Batch 206, Loss: 0.905534, Accuracy: 83.79%\n",
      "Batch 207, Loss: 1.013775, Accuracy: 83.73%\n",
      "Batch 208, Loss: 0.913647, Accuracy: 83.72%\n",
      "Batch 209, Loss: 0.897877, Accuracy: 83.73%\n",
      "Batch 210, Loss: 0.935333, Accuracy: 83.72%\n",
      "Batch 211, Loss: 0.918042, Accuracy: 83.72%\n",
      "Batch 212, Loss: 0.976027, Accuracy: 83.68%\n",
      "Batch 213, Loss: 0.886541, Accuracy: 83.69%\n",
      "Training - Epoch 3, Loss: 0.906472, Accuracy: 83.69%\n",
      "Validation Batch 1, Loss: 0.837368, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.846123, Accuracy: 91.41%\n",
      "Validation Batch 3, Loss: 0.962610, Accuracy: 86.98%\n",
      "Validation Batch 4, Loss: 0.883616, Accuracy: 86.33%\n",
      "Validation Batch 5, Loss: 0.836550, Accuracy: 86.88%\n",
      "Validation Batch 6, Loss: 0.830030, Accuracy: 87.76%\n",
      "Validation Batch 7, Loss: 0.895635, Accuracy: 87.28%\n",
      "Validation Batch 8, Loss: 0.948780, Accuracy: 86.13%\n",
      "Validation Batch 9, Loss: 0.925299, Accuracy: 85.59%\n",
      "Validation Batch 10, Loss: 0.929556, Accuracy: 85.16%\n",
      "Validation Batch 11, Loss: 0.862355, Accuracy: 85.37%\n",
      "Validation Batch 12, Loss: 0.845487, Accuracy: 85.68%\n",
      "Validation Batch 13, Loss: 0.896287, Accuracy: 85.70%\n",
      "Validation Batch 14, Loss: 0.906139, Accuracy: 85.49%\n",
      "Validation Batch 15, Loss: 0.900251, Accuracy: 85.42%\n",
      "Validation Batch 16, Loss: 0.875838, Accuracy: 85.45%\n",
      "Validation Batch 17, Loss: 0.941008, Accuracy: 85.11%\n",
      "Validation Batch 18, Loss: 0.869586, Accuracy: 85.24%\n",
      "Validation Batch 19, Loss: 0.929057, Accuracy: 85.03%\n",
      "Validation Batch 20, Loss: 0.830317, Accuracy: 85.31%\n",
      "Validation Batch 21, Loss: 0.892624, Accuracy: 85.34%\n",
      "Validation Batch 22, Loss: 0.888069, Accuracy: 85.30%\n",
      "Validation Batch 23, Loss: 0.911138, Accuracy: 85.19%\n",
      "Validation Batch 24, Loss: 0.947567, Accuracy: 84.96%\n",
      "Validation Batch 25, Loss: 0.891420, Accuracy: 84.88%\n",
      "Validation Batch 26, Loss: 0.892809, Accuracy: 84.86%\n",
      "Validation Batch 27, Loss: 0.802209, Accuracy: 85.09%\n",
      "Validation - Epoch 3, Loss: 0.888064, Accuracy: 85.09%\n",
      "Patienceâ€”6\n",
      "Epoch 4\n",
      "Batch 1, Loss: 0.981170, Accuracy: 75.00%\n",
      "Batch 2, Loss: 0.915532, Accuracy: 78.12%\n",
      "Batch 3, Loss: 0.916352, Accuracy: 80.21%\n",
      "Batch 4, Loss: 0.862099, Accuracy: 82.42%\n",
      "Batch 5, Loss: 0.929445, Accuracy: 81.88%\n",
      "Batch 6, Loss: 0.991878, Accuracy: 80.47%\n",
      "Batch 7, Loss: 0.902526, Accuracy: 80.80%\n",
      "Batch 8, Loss: 0.913902, Accuracy: 81.25%\n",
      "Batch 9, Loss: 0.900927, Accuracy: 81.60%\n",
      "Batch 10, Loss: 0.865218, Accuracy: 82.19%\n",
      "Batch 11, Loss: 0.922171, Accuracy: 82.24%\n",
      "Batch 12, Loss: 0.928848, Accuracy: 82.16%\n",
      "Batch 13, Loss: 0.918825, Accuracy: 82.21%\n",
      "Batch 14, Loss: 0.889351, Accuracy: 82.37%\n",
      "Batch 15, Loss: 0.892473, Accuracy: 82.60%\n",
      "Batch 16, Loss: 0.899853, Accuracy: 82.71%\n",
      "Batch 17, Loss: 0.859291, Accuracy: 83.00%\n",
      "Batch 18, Loss: 0.989809, Accuracy: 82.64%\n",
      "Batch 19, Loss: 0.971407, Accuracy: 82.40%\n",
      "Batch 20, Loss: 0.906865, Accuracy: 82.50%\n",
      "Batch 21, Loss: 0.903134, Accuracy: 82.59%\n",
      "Batch 22, Loss: 0.877829, Accuracy: 82.74%\n",
      "Batch 23, Loss: 0.948047, Accuracy: 82.61%\n",
      "Batch 24, Loss: 0.916671, Accuracy: 82.55%\n",
      "Batch 25, Loss: 0.856849, Accuracy: 82.75%\n",
      "Batch 26, Loss: 0.929820, Accuracy: 82.63%\n",
      "Batch 27, Loss: 0.825175, Accuracy: 82.93%\n",
      "Batch 28, Loss: 0.900064, Accuracy: 82.98%\n",
      "Batch 29, Loss: 0.945077, Accuracy: 82.87%\n",
      "Batch 30, Loss: 0.865238, Accuracy: 83.02%\n",
      "Batch 31, Loss: 0.870179, Accuracy: 83.17%\n",
      "Batch 32, Loss: 0.901905, Accuracy: 83.25%\n",
      "Batch 33, Loss: 0.882540, Accuracy: 83.38%\n",
      "Batch 34, Loss: 0.867719, Accuracy: 83.55%\n",
      "Batch 35, Loss: 0.920762, Accuracy: 83.57%\n",
      "Batch 36, Loss: 0.905729, Accuracy: 83.59%\n",
      "Batch 37, Loss: 0.878620, Accuracy: 83.66%\n",
      "Batch 38, Loss: 0.861321, Accuracy: 83.80%\n",
      "Batch 39, Loss: 0.862199, Accuracy: 83.93%\n",
      "Batch 40, Loss: 0.919987, Accuracy: 83.95%\n",
      "Batch 41, Loss: 0.910528, Accuracy: 83.92%\n",
      "Batch 42, Loss: 0.968951, Accuracy: 83.74%\n",
      "Batch 43, Loss: 0.909831, Accuracy: 83.68%\n",
      "Batch 44, Loss: 0.954291, Accuracy: 83.52%\n",
      "Batch 45, Loss: 0.865771, Accuracy: 83.65%\n",
      "Batch 46, Loss: 0.902368, Accuracy: 83.66%\n",
      "Batch 47, Loss: 0.841378, Accuracy: 83.81%\n",
      "Batch 48, Loss: 0.920173, Accuracy: 83.82%\n",
      "Batch 49, Loss: 0.859148, Accuracy: 83.93%\n",
      "Batch 50, Loss: 0.910115, Accuracy: 83.91%\n",
      "Batch 51, Loss: 0.875233, Accuracy: 83.98%\n",
      "Batch 52, Loss: 0.914801, Accuracy: 83.95%\n",
      "Batch 53, Loss: 0.932306, Accuracy: 83.90%\n",
      "Batch 54, Loss: 0.883440, Accuracy: 83.97%\n",
      "Batch 55, Loss: 0.892304, Accuracy: 84.01%\n",
      "Batch 56, Loss: 0.865491, Accuracy: 84.07%\n",
      "Batch 57, Loss: 0.951254, Accuracy: 83.96%\n",
      "Batch 58, Loss: 0.889713, Accuracy: 84.02%\n",
      "Batch 59, Loss: 0.902774, Accuracy: 84.03%\n",
      "Batch 60, Loss: 0.882287, Accuracy: 84.06%\n",
      "Batch 61, Loss: 0.850923, Accuracy: 84.14%\n",
      "Batch 62, Loss: 0.985051, Accuracy: 83.97%\n",
      "Batch 63, Loss: 0.914727, Accuracy: 83.98%\n",
      "Batch 64, Loss: 0.942011, Accuracy: 83.89%\n",
      "Batch 65, Loss: 0.888603, Accuracy: 83.92%\n",
      "Batch 66, Loss: 0.957218, Accuracy: 83.85%\n",
      "Batch 67, Loss: 0.893934, Accuracy: 83.89%\n",
      "Batch 68, Loss: 0.928329, Accuracy: 83.85%\n",
      "Batch 69, Loss: 0.922654, Accuracy: 83.83%\n",
      "Batch 70, Loss: 0.977336, Accuracy: 83.75%\n",
      "Batch 71, Loss: 0.961603, Accuracy: 83.65%\n",
      "Batch 72, Loss: 0.892051, Accuracy: 83.68%\n",
      "Batch 73, Loss: 0.891417, Accuracy: 83.69%\n",
      "Batch 74, Loss: 0.885081, Accuracy: 83.72%\n",
      "Batch 75, Loss: 0.952496, Accuracy: 83.62%\n",
      "Batch 76, Loss: 0.908746, Accuracy: 83.63%\n",
      "Batch 77, Loss: 0.990752, Accuracy: 83.52%\n",
      "Batch 78, Loss: 0.878589, Accuracy: 83.57%\n",
      "Batch 79, Loss: 0.893302, Accuracy: 83.60%\n",
      "Batch 80, Loss: 0.891726, Accuracy: 83.61%\n",
      "Batch 81, Loss: 0.966282, Accuracy: 83.55%\n",
      "Batch 82, Loss: 0.848234, Accuracy: 83.61%\n",
      "Batch 83, Loss: 0.881805, Accuracy: 83.64%\n",
      "Batch 84, Loss: 0.854755, Accuracy: 83.72%\n",
      "Batch 85, Loss: 0.845177, Accuracy: 83.81%\n",
      "Batch 86, Loss: 0.879581, Accuracy: 83.83%\n",
      "Batch 87, Loss: 0.842585, Accuracy: 83.91%\n",
      "Batch 88, Loss: 0.862863, Accuracy: 83.97%\n",
      "Batch 89, Loss: 0.893688, Accuracy: 83.97%\n",
      "Batch 90, Loss: 0.978320, Accuracy: 83.87%\n",
      "Batch 91, Loss: 0.910296, Accuracy: 83.86%\n",
      "Batch 92, Loss: 0.859209, Accuracy: 83.90%\n",
      "Batch 93, Loss: 0.945963, Accuracy: 83.84%\n",
      "Batch 94, Loss: 1.028225, Accuracy: 83.71%\n",
      "Batch 95, Loss: 0.884107, Accuracy: 83.73%\n",
      "Batch 96, Loss: 0.887019, Accuracy: 83.74%\n",
      "Batch 97, Loss: 0.918204, Accuracy: 83.71%\n",
      "Batch 98, Loss: 0.979785, Accuracy: 83.64%\n",
      "Batch 99, Loss: 0.910334, Accuracy: 83.65%\n",
      "Batch 100, Loss: 0.897746, Accuracy: 83.66%\n",
      "Batch 101, Loss: 0.877359, Accuracy: 83.69%\n",
      "Batch 102, Loss: 0.885163, Accuracy: 83.72%\n",
      "Batch 103, Loss: 0.834879, Accuracy: 83.80%\n",
      "Batch 104, Loss: 0.959422, Accuracy: 83.74%\n",
      "Batch 105, Loss: 0.906653, Accuracy: 83.74%\n",
      "Batch 106, Loss: 0.869454, Accuracy: 83.80%\n",
      "Batch 107, Loss: 0.837357, Accuracy: 83.85%\n",
      "Batch 108, Loss: 0.876593, Accuracy: 83.88%\n",
      "Batch 109, Loss: 0.897211, Accuracy: 83.89%\n",
      "Batch 110, Loss: 0.963375, Accuracy: 83.84%\n",
      "Batch 111, Loss: 0.945378, Accuracy: 83.78%\n",
      "Batch 112, Loss: 0.921376, Accuracy: 83.78%\n",
      "Batch 113, Loss: 0.861292, Accuracy: 83.82%\n",
      "Batch 114, Loss: 0.921455, Accuracy: 83.80%\n",
      "Batch 115, Loss: 0.876090, Accuracy: 83.85%\n",
      "Batch 116, Loss: 0.924626, Accuracy: 83.84%\n",
      "Batch 117, Loss: 0.874402, Accuracy: 83.85%\n",
      "Batch 118, Loss: 0.915257, Accuracy: 83.86%\n",
      "Batch 119, Loss: 0.915858, Accuracy: 83.85%\n",
      "Batch 120, Loss: 0.940465, Accuracy: 83.82%\n",
      "Batch 121, Loss: 0.888426, Accuracy: 83.81%\n",
      "Batch 122, Loss: 0.995120, Accuracy: 83.72%\n",
      "Batch 123, Loss: 0.875543, Accuracy: 83.74%\n",
      "Batch 124, Loss: 0.926362, Accuracy: 83.72%\n",
      "Batch 125, Loss: 0.982982, Accuracy: 83.65%\n",
      "Batch 126, Loss: 0.998513, Accuracy: 83.57%\n",
      "Batch 127, Loss: 0.931979, Accuracy: 83.55%\n",
      "Batch 128, Loss: 0.928477, Accuracy: 83.53%\n",
      "Batch 129, Loss: 0.881217, Accuracy: 83.55%\n",
      "Batch 130, Loss: 0.828304, Accuracy: 83.61%\n",
      "Batch 131, Loss: 0.856445, Accuracy: 83.64%\n",
      "Batch 132, Loss: 0.886809, Accuracy: 83.65%\n",
      "Batch 133, Loss: 0.933700, Accuracy: 83.63%\n",
      "Batch 134, Loss: 0.917278, Accuracy: 83.63%\n",
      "Batch 135, Loss: 0.979831, Accuracy: 83.58%\n",
      "Batch 136, Loss: 0.908387, Accuracy: 83.57%\n",
      "Batch 137, Loss: 0.849639, Accuracy: 83.60%\n",
      "Batch 138, Loss: 0.858574, Accuracy: 83.64%\n",
      "Batch 139, Loss: 0.902883, Accuracy: 83.64%\n",
      "Batch 140, Loss: 0.817984, Accuracy: 83.72%\n",
      "Batch 141, Loss: 0.869848, Accuracy: 83.75%\n",
      "Batch 142, Loss: 0.933801, Accuracy: 83.75%\n",
      "Batch 143, Loss: 0.877684, Accuracy: 83.77%\n",
      "Batch 144, Loss: 0.935750, Accuracy: 83.77%\n",
      "Batch 145, Loss: 0.827859, Accuracy: 83.83%\n",
      "Batch 146, Loss: 0.948168, Accuracy: 83.79%\n",
      "Batch 147, Loss: 0.916700, Accuracy: 83.78%\n",
      "Batch 148, Loss: 0.920412, Accuracy: 83.78%\n",
      "Batch 149, Loss: 0.879408, Accuracy: 83.81%\n",
      "Batch 150, Loss: 0.879023, Accuracy: 83.82%\n",
      "Batch 151, Loss: 0.959946, Accuracy: 83.79%\n",
      "Batch 152, Loss: 0.928226, Accuracy: 83.76%\n",
      "Batch 153, Loss: 0.948972, Accuracy: 83.72%\n",
      "Batch 154, Loss: 0.937455, Accuracy: 83.71%\n",
      "Batch 155, Loss: 0.921860, Accuracy: 83.70%\n",
      "Batch 156, Loss: 0.914421, Accuracy: 83.69%\n",
      "Batch 157, Loss: 0.906792, Accuracy: 83.69%\n",
      "Batch 158, Loss: 0.888999, Accuracy: 83.69%\n",
      "Batch 159, Loss: 0.924638, Accuracy: 83.67%\n",
      "Batch 160, Loss: 0.920604, Accuracy: 83.65%\n",
      "Batch 161, Loss: 0.947842, Accuracy: 83.63%\n",
      "Batch 162, Loss: 0.891651, Accuracy: 83.65%\n",
      "Batch 163, Loss: 0.862565, Accuracy: 83.68%\n",
      "Batch 164, Loss: 0.970171, Accuracy: 83.64%\n",
      "Batch 165, Loss: 0.916433, Accuracy: 83.64%\n",
      "Batch 166, Loss: 0.915416, Accuracy: 83.62%\n",
      "Batch 167, Loss: 0.891785, Accuracy: 83.64%\n",
      "Batch 168, Loss: 0.868435, Accuracy: 83.66%\n",
      "Batch 169, Loss: 0.899593, Accuracy: 83.67%\n",
      "Batch 170, Loss: 1.031042, Accuracy: 83.60%\n",
      "Batch 171, Loss: 0.930689, Accuracy: 83.58%\n",
      "Batch 172, Loss: 0.896736, Accuracy: 83.60%\n",
      "Batch 173, Loss: 0.895167, Accuracy: 83.61%\n",
      "Batch 174, Loss: 0.966808, Accuracy: 83.57%\n",
      "Batch 175, Loss: 0.881938, Accuracy: 83.59%\n",
      "Batch 176, Loss: 0.854874, Accuracy: 83.62%\n",
      "Batch 177, Loss: 0.943808, Accuracy: 83.60%\n",
      "Batch 178, Loss: 0.857047, Accuracy: 83.63%\n",
      "Batch 179, Loss: 0.938706, Accuracy: 83.60%\n",
      "Batch 180, Loss: 0.954271, Accuracy: 83.57%\n",
      "Batch 181, Loss: 0.871079, Accuracy: 83.59%\n",
      "Batch 182, Loss: 0.896443, Accuracy: 83.60%\n",
      "Batch 183, Loss: 0.838120, Accuracy: 83.65%\n",
      "Batch 184, Loss: 0.890397, Accuracy: 83.65%\n",
      "Batch 185, Loss: 0.992128, Accuracy: 83.61%\n",
      "Batch 186, Loss: 0.879382, Accuracy: 83.63%\n",
      "Batch 187, Loss: 0.854603, Accuracy: 83.66%\n",
      "Batch 188, Loss: 0.891142, Accuracy: 83.68%\n",
      "Batch 189, Loss: 0.896055, Accuracy: 83.68%\n",
      "Batch 190, Loss: 0.897615, Accuracy: 83.69%\n",
      "Batch 191, Loss: 0.911671, Accuracy: 83.69%\n",
      "Batch 192, Loss: 0.876869, Accuracy: 83.69%\n",
      "Batch 193, Loss: 0.918363, Accuracy: 83.69%\n",
      "Batch 194, Loss: 0.928196, Accuracy: 83.67%\n",
      "Batch 195, Loss: 0.979752, Accuracy: 83.63%\n",
      "Batch 196, Loss: 0.923426, Accuracy: 83.62%\n",
      "Batch 197, Loss: 0.919659, Accuracy: 83.61%\n",
      "Batch 198, Loss: 0.865480, Accuracy: 83.64%\n",
      "Batch 199, Loss: 0.908230, Accuracy: 83.64%\n",
      "Batch 200, Loss: 0.947129, Accuracy: 83.62%\n",
      "Batch 201, Loss: 0.843319, Accuracy: 83.65%\n",
      "Batch 202, Loss: 0.911446, Accuracy: 83.66%\n",
      "Batch 203, Loss: 0.883592, Accuracy: 83.67%\n",
      "Batch 204, Loss: 0.903616, Accuracy: 83.69%\n",
      "Batch 205, Loss: 0.875616, Accuracy: 83.70%\n",
      "Batch 206, Loss: 0.933220, Accuracy: 83.68%\n",
      "Batch 207, Loss: 0.874039, Accuracy: 83.71%\n",
      "Batch 208, Loss: 0.950015, Accuracy: 83.69%\n",
      "Batch 209, Loss: 0.909591, Accuracy: 83.69%\n",
      "Batch 210, Loss: 0.857291, Accuracy: 83.71%\n",
      "Batch 211, Loss: 0.843821, Accuracy: 83.74%\n",
      "Batch 212, Loss: 0.929417, Accuracy: 83.73%\n",
      "Batch 213, Loss: 0.867576, Accuracy: 83.74%\n",
      "Training - Epoch 4, Loss: 0.906505, Accuracy: 83.74%\n",
      "Validation Batch 1, Loss: 0.837801, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.846345, Accuracy: 91.41%\n",
      "Validation Batch 3, Loss: 0.962984, Accuracy: 86.98%\n",
      "Validation Batch 4, Loss: 0.883178, Accuracy: 86.72%\n",
      "Validation Batch 5, Loss: 0.835918, Accuracy: 87.19%\n",
      "Validation Batch 6, Loss: 0.830336, Accuracy: 88.02%\n",
      "Validation Batch 7, Loss: 0.895374, Accuracy: 87.50%\n",
      "Validation Batch 8, Loss: 0.949158, Accuracy: 86.33%\n",
      "Validation Batch 9, Loss: 0.925646, Accuracy: 85.59%\n",
      "Validation Batch 10, Loss: 0.928167, Accuracy: 85.16%\n",
      "Validation Batch 11, Loss: 0.861639, Accuracy: 85.37%\n",
      "Validation Batch 12, Loss: 0.845986, Accuracy: 85.68%\n",
      "Validation Batch 13, Loss: 0.897379, Accuracy: 85.70%\n",
      "Validation Batch 14, Loss: 0.905892, Accuracy: 85.49%\n",
      "Validation Batch 15, Loss: 0.900409, Accuracy: 85.42%\n",
      "Validation Batch 16, Loss: 0.875650, Accuracy: 85.45%\n",
      "Validation Batch 17, Loss: 0.941332, Accuracy: 85.11%\n",
      "Validation Batch 18, Loss: 0.869711, Accuracy: 85.24%\n",
      "Validation Batch 19, Loss: 0.929405, Accuracy: 85.03%\n",
      "Validation Batch 20, Loss: 0.829184, Accuracy: 85.31%\n",
      "Validation Batch 21, Loss: 0.891515, Accuracy: 85.34%\n",
      "Validation Batch 22, Loss: 0.887489, Accuracy: 85.30%\n",
      "Validation Batch 23, Loss: 0.910351, Accuracy: 85.26%\n",
      "Validation Batch 24, Loss: 0.947623, Accuracy: 85.03%\n",
      "Validation Batch 25, Loss: 0.890870, Accuracy: 84.94%\n",
      "Validation Batch 26, Loss: 0.891764, Accuracy: 84.92%\n",
      "Validation Batch 27, Loss: 0.801690, Accuracy: 85.14%\n",
      "Validation - Epoch 4, Loss: 0.887881, Accuracy: 85.14%\n",
      "Patienceâ€”7\n",
      "Epoch 5\n",
      "Batch 1, Loss: 0.978604, Accuracy: 76.56%\n",
      "Batch 2, Loss: 0.902915, Accuracy: 80.47%\n",
      "Batch 3, Loss: 0.941590, Accuracy: 80.73%\n",
      "Batch 4, Loss: 0.893341, Accuracy: 82.03%\n",
      "Batch 5, Loss: 0.886759, Accuracy: 82.50%\n",
      "Batch 6, Loss: 0.859668, Accuracy: 83.59%\n",
      "Batch 7, Loss: 0.910330, Accuracy: 83.48%\n",
      "Batch 8, Loss: 0.869641, Accuracy: 83.98%\n",
      "Batch 9, Loss: 0.895794, Accuracy: 84.38%\n",
      "Batch 10, Loss: 0.859128, Accuracy: 84.84%\n",
      "Batch 11, Loss: 0.953145, Accuracy: 84.23%\n",
      "Batch 12, Loss: 0.952548, Accuracy: 83.59%\n",
      "Batch 13, Loss: 0.970332, Accuracy: 82.93%\n",
      "Batch 14, Loss: 0.871230, Accuracy: 83.26%\n",
      "Batch 15, Loss: 0.899222, Accuracy: 83.33%\n",
      "Batch 16, Loss: 0.890422, Accuracy: 83.50%\n",
      "Batch 17, Loss: 0.938633, Accuracy: 83.46%\n",
      "Batch 18, Loss: 0.850855, Accuracy: 83.77%\n",
      "Batch 19, Loss: 0.965070, Accuracy: 83.47%\n",
      "Batch 20, Loss: 0.906543, Accuracy: 83.52%\n",
      "Batch 21, Loss: 0.984245, Accuracy: 83.18%\n",
      "Batch 22, Loss: 0.917354, Accuracy: 83.17%\n",
      "Batch 23, Loss: 0.912471, Accuracy: 83.15%\n",
      "Batch 24, Loss: 0.903262, Accuracy: 83.27%\n",
      "Batch 25, Loss: 0.797783, Accuracy: 83.75%\n",
      "Batch 26, Loss: 0.945835, Accuracy: 83.59%\n",
      "Batch 27, Loss: 0.913391, Accuracy: 83.51%\n",
      "Batch 28, Loss: 0.851984, Accuracy: 83.71%\n",
      "Batch 29, Loss: 0.911153, Accuracy: 83.67%\n",
      "Batch 30, Loss: 0.871809, Accuracy: 83.75%\n",
      "Batch 31, Loss: 0.990625, Accuracy: 83.52%\n",
      "Batch 32, Loss: 0.894188, Accuracy: 83.54%\n",
      "Batch 33, Loss: 0.933368, Accuracy: 83.48%\n",
      "Batch 34, Loss: 0.919286, Accuracy: 83.46%\n",
      "Batch 35, Loss: 0.923426, Accuracy: 83.35%\n",
      "Batch 36, Loss: 0.950732, Accuracy: 83.20%\n",
      "Batch 37, Loss: 0.859998, Accuracy: 83.36%\n",
      "Batch 38, Loss: 0.874508, Accuracy: 83.43%\n",
      "Batch 39, Loss: 0.945234, Accuracy: 83.33%\n",
      "Batch 40, Loss: 0.891060, Accuracy: 83.36%\n",
      "Batch 41, Loss: 0.905941, Accuracy: 83.38%\n",
      "Batch 42, Loss: 0.854262, Accuracy: 83.48%\n",
      "Batch 43, Loss: 0.895773, Accuracy: 83.50%\n",
      "Batch 44, Loss: 0.848472, Accuracy: 83.66%\n",
      "Batch 45, Loss: 0.849423, Accuracy: 83.78%\n",
      "Batch 46, Loss: 0.905969, Accuracy: 83.80%\n",
      "Batch 47, Loss: 0.893082, Accuracy: 83.78%\n",
      "Batch 48, Loss: 0.895574, Accuracy: 83.79%\n",
      "Batch 49, Loss: 0.957443, Accuracy: 83.64%\n",
      "Batch 50, Loss: 0.921062, Accuracy: 83.62%\n",
      "Batch 51, Loss: 0.900882, Accuracy: 83.64%\n",
      "Batch 52, Loss: 0.869308, Accuracy: 83.74%\n",
      "Batch 53, Loss: 0.897056, Accuracy: 83.79%\n",
      "Batch 54, Loss: 0.929968, Accuracy: 83.74%\n",
      "Batch 55, Loss: 0.886051, Accuracy: 83.81%\n",
      "Batch 56, Loss: 1.013974, Accuracy: 83.62%\n",
      "Batch 57, Loss: 0.970336, Accuracy: 83.53%\n",
      "Batch 58, Loss: 0.892202, Accuracy: 83.54%\n",
      "Batch 59, Loss: 0.902358, Accuracy: 83.53%\n",
      "Batch 60, Loss: 0.865347, Accuracy: 83.62%\n",
      "Batch 61, Loss: 0.921470, Accuracy: 83.61%\n",
      "Batch 62, Loss: 0.914533, Accuracy: 83.59%\n",
      "Batch 63, Loss: 0.955561, Accuracy: 83.53%\n",
      "Batch 64, Loss: 0.888392, Accuracy: 83.59%\n",
      "Batch 65, Loss: 0.957856, Accuracy: 83.51%\n",
      "Batch 66, Loss: 0.947055, Accuracy: 83.43%\n",
      "Batch 67, Loss: 0.923704, Accuracy: 83.42%\n",
      "Batch 68, Loss: 0.898332, Accuracy: 83.43%\n",
      "Batch 69, Loss: 0.916944, Accuracy: 83.42%\n",
      "Batch 70, Loss: 0.871702, Accuracy: 83.48%\n",
      "Batch 71, Loss: 0.986231, Accuracy: 83.36%\n",
      "Batch 72, Loss: 0.934315, Accuracy: 83.31%\n",
      "Batch 73, Loss: 0.842381, Accuracy: 83.37%\n",
      "Batch 74, Loss: 0.964347, Accuracy: 83.30%\n",
      "Batch 75, Loss: 0.910996, Accuracy: 83.31%\n",
      "Batch 76, Loss: 0.914628, Accuracy: 83.29%\n",
      "Batch 77, Loss: 0.930153, Accuracy: 83.26%\n",
      "Batch 78, Loss: 0.904285, Accuracy: 83.27%\n",
      "Batch 79, Loss: 0.898695, Accuracy: 83.29%\n",
      "Batch 80, Loss: 0.889170, Accuracy: 83.32%\n",
      "Batch 81, Loss: 0.854829, Accuracy: 83.39%\n",
      "Batch 82, Loss: 0.849266, Accuracy: 83.46%\n",
      "Batch 83, Loss: 1.009716, Accuracy: 83.34%\n",
      "Batch 84, Loss: 0.896116, Accuracy: 83.33%\n",
      "Batch 85, Loss: 0.952284, Accuracy: 83.31%\n",
      "Batch 86, Loss: 0.895009, Accuracy: 83.30%\n",
      "Batch 87, Loss: 0.923435, Accuracy: 83.28%\n",
      "Batch 88, Loss: 0.945349, Accuracy: 83.24%\n",
      "Batch 89, Loss: 0.868869, Accuracy: 83.30%\n",
      "Batch 90, Loss: 0.882862, Accuracy: 83.32%\n",
      "Batch 91, Loss: 0.853300, Accuracy: 83.38%\n",
      "Batch 92, Loss: 0.878430, Accuracy: 83.41%\n",
      "Batch 93, Loss: 0.991628, Accuracy: 83.33%\n",
      "Batch 94, Loss: 0.861340, Accuracy: 83.38%\n",
      "Batch 95, Loss: 0.900641, Accuracy: 83.37%\n",
      "Batch 96, Loss: 0.835602, Accuracy: 83.45%\n",
      "Batch 97, Loss: 0.922626, Accuracy: 83.44%\n",
      "Batch 98, Loss: 0.923416, Accuracy: 83.42%\n",
      "Batch 99, Loss: 0.907980, Accuracy: 83.43%\n",
      "Batch 100, Loss: 0.926147, Accuracy: 83.41%\n",
      "Batch 101, Loss: 0.898189, Accuracy: 83.40%\n",
      "Batch 102, Loss: 0.944615, Accuracy: 83.36%\n",
      "Batch 103, Loss: 0.899770, Accuracy: 83.39%\n",
      "Batch 104, Loss: 0.896889, Accuracy: 83.40%\n",
      "Batch 105, Loss: 0.800621, Accuracy: 83.51%\n",
      "Batch 106, Loss: 0.906580, Accuracy: 83.49%\n",
      "Batch 107, Loss: 0.896535, Accuracy: 83.48%\n",
      "Batch 108, Loss: 0.890196, Accuracy: 83.49%\n",
      "Batch 109, Loss: 0.892592, Accuracy: 83.51%\n",
      "Batch 110, Loss: 0.929010, Accuracy: 83.48%\n",
      "Batch 111, Loss: 0.869668, Accuracy: 83.50%\n",
      "Batch 112, Loss: 0.961393, Accuracy: 83.47%\n",
      "Batch 113, Loss: 0.883649, Accuracy: 83.49%\n",
      "Batch 114, Loss: 0.896928, Accuracy: 83.51%\n",
      "Batch 115, Loss: 0.874649, Accuracy: 83.55%\n",
      "Batch 116, Loss: 0.891982, Accuracy: 83.55%\n",
      "Batch 117, Loss: 0.924445, Accuracy: 83.53%\n",
      "Batch 118, Loss: 0.901895, Accuracy: 83.50%\n",
      "Batch 119, Loss: 0.864386, Accuracy: 83.53%\n",
      "Batch 120, Loss: 0.917782, Accuracy: 83.52%\n",
      "Batch 121, Loss: 0.832943, Accuracy: 83.60%\n",
      "Batch 122, Loss: 0.991796, Accuracy: 83.52%\n",
      "Batch 123, Loss: 0.914026, Accuracy: 83.51%\n",
      "Batch 124, Loss: 0.980507, Accuracy: 83.44%\n",
      "Batch 125, Loss: 0.908312, Accuracy: 83.45%\n",
      "Batch 126, Loss: 0.892079, Accuracy: 83.46%\n",
      "Batch 127, Loss: 0.924227, Accuracy: 83.45%\n",
      "Batch 128, Loss: 0.961495, Accuracy: 83.41%\n",
      "Batch 129, Loss: 0.885287, Accuracy: 83.43%\n",
      "Batch 130, Loss: 0.905291, Accuracy: 83.41%\n",
      "Batch 131, Loss: 0.818408, Accuracy: 83.48%\n",
      "Batch 132, Loss: 0.935302, Accuracy: 83.46%\n",
      "Batch 133, Loss: 0.906553, Accuracy: 83.47%\n",
      "Batch 134, Loss: 0.898821, Accuracy: 83.49%\n",
      "Batch 135, Loss: 0.893999, Accuracy: 83.51%\n",
      "Batch 136, Loss: 0.845295, Accuracy: 83.56%\n",
      "Batch 137, Loss: 0.934502, Accuracy: 83.53%\n",
      "Batch 138, Loss: 0.868539, Accuracy: 83.57%\n",
      "Batch 139, Loss: 0.868698, Accuracy: 83.61%\n",
      "Batch 140, Loss: 0.972420, Accuracy: 83.55%\n",
      "Batch 141, Loss: 0.984177, Accuracy: 83.49%\n",
      "Batch 142, Loss: 0.849607, Accuracy: 83.53%\n",
      "Batch 143, Loss: 0.886377, Accuracy: 83.56%\n",
      "Batch 144, Loss: 0.915763, Accuracy: 83.55%\n",
      "Batch 145, Loss: 0.938082, Accuracy: 83.53%\n",
      "Batch 146, Loss: 0.894013, Accuracy: 83.55%\n",
      "Batch 147, Loss: 0.928372, Accuracy: 83.55%\n",
      "Batch 148, Loss: 0.918919, Accuracy: 83.54%\n",
      "Batch 149, Loss: 0.946025, Accuracy: 83.53%\n",
      "Batch 150, Loss: 0.857877, Accuracy: 83.56%\n",
      "Batch 151, Loss: 0.923055, Accuracy: 83.56%\n",
      "Batch 152, Loss: 0.978827, Accuracy: 83.50%\n",
      "Batch 153, Loss: 0.933035, Accuracy: 83.49%\n",
      "Batch 154, Loss: 0.889004, Accuracy: 83.50%\n",
      "Batch 155, Loss: 0.858373, Accuracy: 83.53%\n",
      "Batch 156, Loss: 0.926696, Accuracy: 83.51%\n",
      "Batch 157, Loss: 0.906720, Accuracy: 83.52%\n",
      "Batch 158, Loss: 0.856993, Accuracy: 83.56%\n",
      "Batch 159, Loss: 0.846887, Accuracy: 83.62%\n",
      "Batch 160, Loss: 0.986895, Accuracy: 83.56%\n",
      "Batch 161, Loss: 0.886396, Accuracy: 83.57%\n",
      "Batch 162, Loss: 0.841479, Accuracy: 83.61%\n",
      "Batch 163, Loss: 0.989894, Accuracy: 83.58%\n",
      "Batch 164, Loss: 0.908436, Accuracy: 83.57%\n",
      "Batch 165, Loss: 0.900869, Accuracy: 83.58%\n",
      "Batch 166, Loss: 0.936232, Accuracy: 83.57%\n",
      "Batch 167, Loss: 0.911485, Accuracy: 83.57%\n",
      "Batch 168, Loss: 0.919893, Accuracy: 83.55%\n",
      "Batch 169, Loss: 0.856447, Accuracy: 83.58%\n",
      "Batch 170, Loss: 0.923940, Accuracy: 83.57%\n",
      "Batch 171, Loss: 0.848873, Accuracy: 83.61%\n",
      "Batch 172, Loss: 0.869530, Accuracy: 83.63%\n",
      "Batch 173, Loss: 0.935030, Accuracy: 83.61%\n",
      "Batch 174, Loss: 0.933154, Accuracy: 83.58%\n",
      "Batch 175, Loss: 0.870020, Accuracy: 83.61%\n",
      "Batch 176, Loss: 0.853249, Accuracy: 83.65%\n",
      "Batch 177, Loss: 0.884528, Accuracy: 83.66%\n",
      "Batch 178, Loss: 0.906797, Accuracy: 83.66%\n",
      "Batch 179, Loss: 0.920001, Accuracy: 83.64%\n",
      "Batch 180, Loss: 0.887937, Accuracy: 83.65%\n",
      "Batch 181, Loss: 1.018055, Accuracy: 83.60%\n",
      "Batch 182, Loss: 0.909067, Accuracy: 83.59%\n",
      "Batch 183, Loss: 0.920576, Accuracy: 83.59%\n",
      "Batch 184, Loss: 0.862647, Accuracy: 83.63%\n",
      "Batch 185, Loss: 0.816908, Accuracy: 83.69%\n",
      "Batch 186, Loss: 0.887461, Accuracy: 83.71%\n",
      "Batch 187, Loss: 0.912000, Accuracy: 83.71%\n",
      "Batch 188, Loss: 0.914963, Accuracy: 83.70%\n",
      "Batch 189, Loss: 0.922946, Accuracy: 83.69%\n",
      "Batch 190, Loss: 0.920233, Accuracy: 83.68%\n",
      "Batch 191, Loss: 0.928729, Accuracy: 83.66%\n",
      "Batch 192, Loss: 0.946789, Accuracy: 83.64%\n",
      "Batch 193, Loss: 0.858719, Accuracy: 83.67%\n",
      "Batch 194, Loss: 0.895249, Accuracy: 83.67%\n",
      "Batch 195, Loss: 0.961356, Accuracy: 83.64%\n",
      "Batch 196, Loss: 0.935941, Accuracy: 83.63%\n",
      "Batch 197, Loss: 0.883310, Accuracy: 83.64%\n",
      "Batch 198, Loss: 0.856501, Accuracy: 83.66%\n",
      "Batch 199, Loss: 0.883554, Accuracy: 83.68%\n",
      "Batch 200, Loss: 0.989957, Accuracy: 83.63%\n",
      "Batch 201, Loss: 0.897277, Accuracy: 83.64%\n",
      "Batch 202, Loss: 0.856349, Accuracy: 83.68%\n",
      "Batch 203, Loss: 0.855290, Accuracy: 83.71%\n",
      "Batch 204, Loss: 0.856185, Accuracy: 83.73%\n",
      "Batch 205, Loss: 0.844837, Accuracy: 83.76%\n",
      "Batch 206, Loss: 0.871370, Accuracy: 83.77%\n",
      "Batch 207, Loss: 0.966078, Accuracy: 83.74%\n",
      "Batch 208, Loss: 0.922966, Accuracy: 83.73%\n",
      "Batch 209, Loss: 0.863868, Accuracy: 83.75%\n",
      "Batch 210, Loss: 0.873154, Accuracy: 83.77%\n",
      "Batch 211, Loss: 0.926622, Accuracy: 83.76%\n",
      "Batch 212, Loss: 0.877640, Accuracy: 83.78%\n",
      "Batch 213, Loss: 0.957763, Accuracy: 83.74%\n",
      "Training - Epoch 5, Loss: 0.905991, Accuracy: 83.74%\n",
      "Validation Batch 1, Loss: 0.838113, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.847419, Accuracy: 90.62%\n",
      "Validation Batch 3, Loss: 0.962395, Accuracy: 86.46%\n",
      "Validation Batch 4, Loss: 0.884770, Accuracy: 85.94%\n",
      "Validation Batch 5, Loss: 0.836073, Accuracy: 86.56%\n",
      "Validation Batch 6, Loss: 0.829918, Accuracy: 87.50%\n",
      "Validation Batch 7, Loss: 0.895527, Accuracy: 87.05%\n",
      "Validation Batch 8, Loss: 0.948701, Accuracy: 85.94%\n",
      "Validation Batch 9, Loss: 0.924015, Accuracy: 85.42%\n",
      "Validation Batch 10, Loss: 0.929120, Accuracy: 85.00%\n",
      "Validation Batch 11, Loss: 0.862377, Accuracy: 85.23%\n",
      "Validation Batch 12, Loss: 0.845312, Accuracy: 85.55%\n",
      "Validation Batch 13, Loss: 0.895059, Accuracy: 85.70%\n",
      "Validation Batch 14, Loss: 0.904688, Accuracy: 85.49%\n",
      "Validation Batch 15, Loss: 0.900788, Accuracy: 85.42%\n",
      "Validation Batch 16, Loss: 0.876372, Accuracy: 85.45%\n",
      "Validation Batch 17, Loss: 0.940683, Accuracy: 85.11%\n",
      "Validation Batch 18, Loss: 0.869123, Accuracy: 85.24%\n",
      "Validation Batch 19, Loss: 0.929004, Accuracy: 85.03%\n",
      "Validation Batch 20, Loss: 0.829665, Accuracy: 85.31%\n",
      "Validation Batch 21, Loss: 0.891659, Accuracy: 85.34%\n",
      "Validation Batch 22, Loss: 0.887620, Accuracy: 85.30%\n",
      "Validation Batch 23, Loss: 0.908385, Accuracy: 85.26%\n",
      "Validation Batch 24, Loss: 0.946899, Accuracy: 85.03%\n",
      "Validation Batch 25, Loss: 0.891101, Accuracy: 84.94%\n",
      "Validation Batch 26, Loss: 0.892760, Accuracy: 84.92%\n",
      "Validation Batch 27, Loss: 0.802256, Accuracy: 85.14%\n",
      "Validation - Epoch 5, Loss: 0.887770, Accuracy: 85.14%\n",
      "Patienceâ€”8\n",
      "Epoch 6\n",
      "Batch 1, Loss: 0.971949, Accuracy: 76.56%\n",
      "Batch 2, Loss: 0.921237, Accuracy: 78.91%\n",
      "Batch 3, Loss: 0.889298, Accuracy: 80.73%\n",
      "Batch 4, Loss: 0.914914, Accuracy: 81.25%\n",
      "Batch 5, Loss: 0.888046, Accuracy: 82.50%\n",
      "Batch 6, Loss: 0.946840, Accuracy: 82.03%\n",
      "Batch 7, Loss: 0.907408, Accuracy: 82.37%\n",
      "Batch 8, Loss: 0.973248, Accuracy: 82.03%\n",
      "Batch 9, Loss: 0.898590, Accuracy: 82.47%\n",
      "Batch 10, Loss: 0.943314, Accuracy: 82.34%\n",
      "Batch 11, Loss: 0.893494, Accuracy: 82.53%\n",
      "Batch 12, Loss: 0.979478, Accuracy: 82.03%\n",
      "Batch 13, Loss: 0.942646, Accuracy: 81.85%\n",
      "Batch 14, Loss: 0.813642, Accuracy: 82.70%\n",
      "Batch 15, Loss: 0.941349, Accuracy: 82.60%\n",
      "Batch 16, Loss: 0.887396, Accuracy: 82.81%\n",
      "Batch 17, Loss: 0.901494, Accuracy: 82.90%\n",
      "Batch 18, Loss: 0.863213, Accuracy: 83.16%\n",
      "Batch 19, Loss: 1.011367, Accuracy: 82.57%\n",
      "Batch 20, Loss: 0.883745, Accuracy: 82.89%\n",
      "Batch 21, Loss: 0.929910, Accuracy: 82.74%\n",
      "Batch 22, Loss: 0.889678, Accuracy: 82.74%\n",
      "Batch 23, Loss: 0.869580, Accuracy: 82.88%\n",
      "Batch 24, Loss: 0.951383, Accuracy: 82.75%\n",
      "Batch 25, Loss: 0.859518, Accuracy: 83.00%\n",
      "Batch 26, Loss: 0.894426, Accuracy: 83.05%\n",
      "Batch 27, Loss: 0.944607, Accuracy: 82.93%\n",
      "Batch 28, Loss: 0.905187, Accuracy: 82.98%\n",
      "Batch 29, Loss: 0.891830, Accuracy: 83.03%\n",
      "Batch 30, Loss: 0.841468, Accuracy: 83.28%\n",
      "Batch 31, Loss: 0.918036, Accuracy: 83.22%\n",
      "Batch 32, Loss: 0.943118, Accuracy: 83.11%\n",
      "Batch 33, Loss: 0.890483, Accuracy: 83.19%\n",
      "Batch 34, Loss: 0.929010, Accuracy: 83.13%\n",
      "Batch 35, Loss: 0.852334, Accuracy: 83.30%\n",
      "Batch 36, Loss: 0.893967, Accuracy: 83.42%\n",
      "Batch 37, Loss: 0.927226, Accuracy: 83.32%\n",
      "Batch 38, Loss: 0.891193, Accuracy: 83.43%\n",
      "Batch 39, Loss: 0.891167, Accuracy: 83.41%\n",
      "Batch 40, Loss: 0.916198, Accuracy: 83.40%\n",
      "Batch 41, Loss: 0.915592, Accuracy: 83.35%\n",
      "Batch 42, Loss: 0.960091, Accuracy: 83.18%\n",
      "Batch 43, Loss: 0.897021, Accuracy: 83.25%\n",
      "Batch 44, Loss: 0.932986, Accuracy: 83.24%\n",
      "Batch 45, Loss: 0.965582, Accuracy: 83.09%\n",
      "Batch 46, Loss: 0.859155, Accuracy: 83.22%\n",
      "Batch 47, Loss: 0.887395, Accuracy: 83.31%\n",
      "Batch 48, Loss: 0.877137, Accuracy: 83.37%\n",
      "Batch 49, Loss: 0.940535, Accuracy: 83.32%\n",
      "Batch 50, Loss: 0.821541, Accuracy: 83.50%\n",
      "Batch 51, Loss: 0.827693, Accuracy: 83.67%\n",
      "Batch 52, Loss: 0.966448, Accuracy: 83.59%\n",
      "Batch 53, Loss: 0.878874, Accuracy: 83.64%\n",
      "Batch 54, Loss: 0.922145, Accuracy: 83.56%\n",
      "Batch 55, Loss: 0.950796, Accuracy: 83.47%\n",
      "Batch 56, Loss: 0.962135, Accuracy: 83.34%\n",
      "Batch 57, Loss: 0.904901, Accuracy: 83.36%\n",
      "Batch 58, Loss: 0.970572, Accuracy: 83.27%\n",
      "Batch 59, Loss: 0.850967, Accuracy: 83.40%\n",
      "Batch 60, Loss: 0.862417, Accuracy: 83.46%\n",
      "Batch 61, Loss: 0.915086, Accuracy: 83.48%\n",
      "Batch 62, Loss: 0.905491, Accuracy: 83.49%\n",
      "Batch 63, Loss: 0.894187, Accuracy: 83.51%\n",
      "Batch 64, Loss: 0.892997, Accuracy: 83.52%\n",
      "Batch 65, Loss: 0.925387, Accuracy: 83.49%\n",
      "Batch 66, Loss: 0.906489, Accuracy: 83.50%\n",
      "Batch 67, Loss: 0.979028, Accuracy: 83.40%\n",
      "Batch 68, Loss: 0.871513, Accuracy: 83.46%\n",
      "Batch 69, Loss: 0.903544, Accuracy: 83.42%\n",
      "Batch 70, Loss: 0.866842, Accuracy: 83.48%\n",
      "Batch 71, Loss: 0.954810, Accuracy: 83.38%\n",
      "Batch 72, Loss: 0.955766, Accuracy: 83.31%\n",
      "Batch 73, Loss: 0.923170, Accuracy: 83.28%\n",
      "Batch 74, Loss: 0.940405, Accuracy: 83.21%\n",
      "Batch 75, Loss: 0.963470, Accuracy: 83.15%\n",
      "Batch 76, Loss: 0.860628, Accuracy: 83.22%\n",
      "Batch 77, Loss: 0.905257, Accuracy: 83.24%\n",
      "Batch 78, Loss: 0.936227, Accuracy: 83.17%\n",
      "Batch 79, Loss: 0.926155, Accuracy: 83.13%\n",
      "Batch 80, Loss: 0.907172, Accuracy: 83.14%\n",
      "Batch 81, Loss: 0.940837, Accuracy: 83.10%\n",
      "Batch 82, Loss: 0.897366, Accuracy: 83.10%\n",
      "Batch 83, Loss: 0.927606, Accuracy: 83.09%\n",
      "Batch 84, Loss: 0.954293, Accuracy: 83.05%\n",
      "Batch 85, Loss: 0.911018, Accuracy: 83.07%\n",
      "Batch 86, Loss: 0.930180, Accuracy: 83.07%\n",
      "Batch 87, Loss: 0.835203, Accuracy: 83.19%\n",
      "Batch 88, Loss: 0.896447, Accuracy: 83.20%\n",
      "Batch 89, Loss: 0.853613, Accuracy: 83.27%\n",
      "Batch 90, Loss: 0.883096, Accuracy: 83.30%\n",
      "Batch 91, Loss: 0.841573, Accuracy: 83.38%\n",
      "Batch 92, Loss: 0.849952, Accuracy: 83.46%\n",
      "Batch 93, Loss: 0.952548, Accuracy: 83.42%\n",
      "Batch 94, Loss: 0.895697, Accuracy: 83.43%\n",
      "Batch 95, Loss: 0.908335, Accuracy: 83.44%\n",
      "Batch 96, Loss: 0.912447, Accuracy: 83.43%\n",
      "Batch 97, Loss: 0.952781, Accuracy: 83.38%\n",
      "Batch 98, Loss: 0.889478, Accuracy: 83.39%\n",
      "Batch 99, Loss: 0.877585, Accuracy: 83.43%\n",
      "Batch 100, Loss: 0.965024, Accuracy: 83.38%\n",
      "Batch 101, Loss: 0.883656, Accuracy: 83.40%\n",
      "Batch 102, Loss: 0.837268, Accuracy: 83.49%\n",
      "Batch 103, Loss: 0.857389, Accuracy: 83.56%\n",
      "Batch 104, Loss: 0.877534, Accuracy: 83.56%\n",
      "Batch 105, Loss: 0.859889, Accuracy: 83.59%\n",
      "Batch 106, Loss: 0.916645, Accuracy: 83.58%\n",
      "Batch 107, Loss: 0.930648, Accuracy: 83.56%\n",
      "Batch 108, Loss: 0.965499, Accuracy: 83.51%\n",
      "Batch 109, Loss: 0.839403, Accuracy: 83.59%\n",
      "Batch 110, Loss: 0.846922, Accuracy: 83.64%\n",
      "Batch 111, Loss: 0.918792, Accuracy: 83.61%\n",
      "Batch 112, Loss: 0.844604, Accuracy: 83.68%\n",
      "Batch 113, Loss: 0.893007, Accuracy: 83.70%\n",
      "Batch 114, Loss: 0.889470, Accuracy: 83.72%\n",
      "Batch 115, Loss: 0.901491, Accuracy: 83.72%\n",
      "Batch 116, Loss: 0.916484, Accuracy: 83.71%\n",
      "Batch 117, Loss: 0.919800, Accuracy: 83.71%\n",
      "Batch 118, Loss: 0.912578, Accuracy: 83.70%\n",
      "Batch 119, Loss: 0.866070, Accuracy: 83.73%\n",
      "Batch 120, Loss: 0.927566, Accuracy: 83.68%\n",
      "Batch 121, Loss: 0.911562, Accuracy: 83.68%\n",
      "Batch 122, Loss: 0.950093, Accuracy: 83.64%\n",
      "Batch 123, Loss: 0.873290, Accuracy: 83.66%\n",
      "Batch 124, Loss: 0.942741, Accuracy: 83.63%\n",
      "Batch 125, Loss: 0.912324, Accuracy: 83.64%\n",
      "Batch 126, Loss: 0.936095, Accuracy: 83.63%\n",
      "Batch 127, Loss: 0.936531, Accuracy: 83.61%\n",
      "Batch 128, Loss: 0.869995, Accuracy: 83.65%\n",
      "Batch 129, Loss: 0.924848, Accuracy: 83.64%\n",
      "Batch 130, Loss: 1.064674, Accuracy: 83.52%\n",
      "Batch 131, Loss: 0.927195, Accuracy: 83.52%\n",
      "Batch 132, Loss: 0.882005, Accuracy: 83.53%\n",
      "Batch 133, Loss: 0.904439, Accuracy: 83.54%\n",
      "Batch 134, Loss: 0.903586, Accuracy: 83.52%\n",
      "Batch 135, Loss: 0.894994, Accuracy: 83.53%\n",
      "Batch 136, Loss: 0.912118, Accuracy: 83.52%\n",
      "Batch 137, Loss: 0.865629, Accuracy: 83.57%\n",
      "Batch 138, Loss: 0.892131, Accuracy: 83.58%\n",
      "Batch 139, Loss: 0.915247, Accuracy: 83.59%\n",
      "Batch 140, Loss: 0.893887, Accuracy: 83.59%\n",
      "Batch 141, Loss: 0.910334, Accuracy: 83.58%\n",
      "Batch 142, Loss: 0.993313, Accuracy: 83.52%\n",
      "Batch 143, Loss: 0.887621, Accuracy: 83.53%\n",
      "Batch 144, Loss: 0.947966, Accuracy: 83.51%\n",
      "Batch 145, Loss: 0.928069, Accuracy: 83.48%\n",
      "Batch 146, Loss: 0.839334, Accuracy: 83.52%\n",
      "Batch 147, Loss: 0.923106, Accuracy: 83.50%\n",
      "Batch 148, Loss: 0.911701, Accuracy: 83.51%\n",
      "Batch 149, Loss: 0.886719, Accuracy: 83.52%\n",
      "Batch 150, Loss: 0.923829, Accuracy: 83.49%\n",
      "Batch 151, Loss: 0.895820, Accuracy: 83.51%\n",
      "Batch 152, Loss: 0.890503, Accuracy: 83.50%\n",
      "Batch 153, Loss: 0.935080, Accuracy: 83.48%\n",
      "Batch 154, Loss: 0.932005, Accuracy: 83.45%\n",
      "Batch 155, Loss: 0.887603, Accuracy: 83.46%\n",
      "Batch 156, Loss: 0.908968, Accuracy: 83.46%\n",
      "Batch 157, Loss: 0.905221, Accuracy: 83.45%\n",
      "Batch 158, Loss: 0.897369, Accuracy: 83.47%\n",
      "Batch 159, Loss: 0.936926, Accuracy: 83.45%\n",
      "Batch 160, Loss: 0.885600, Accuracy: 83.46%\n",
      "Batch 161, Loss: 0.901413, Accuracy: 83.47%\n",
      "Batch 162, Loss: 0.938168, Accuracy: 83.46%\n",
      "Batch 163, Loss: 0.913174, Accuracy: 83.45%\n",
      "Batch 164, Loss: 0.912218, Accuracy: 83.43%\n",
      "Batch 165, Loss: 0.876842, Accuracy: 83.45%\n",
      "Batch 166, Loss: 0.857929, Accuracy: 83.48%\n",
      "Batch 167, Loss: 0.869174, Accuracy: 83.51%\n",
      "Batch 168, Loss: 0.884728, Accuracy: 83.53%\n",
      "Batch 169, Loss: 0.927527, Accuracy: 83.52%\n",
      "Batch 170, Loss: 0.954158, Accuracy: 83.47%\n",
      "Batch 171, Loss: 0.932257, Accuracy: 83.47%\n",
      "Batch 172, Loss: 0.871396, Accuracy: 83.51%\n",
      "Batch 173, Loss: 0.846018, Accuracy: 83.56%\n",
      "Batch 174, Loss: 0.864156, Accuracy: 83.59%\n",
      "Batch 175, Loss: 0.873616, Accuracy: 83.62%\n",
      "Batch 176, Loss: 0.817271, Accuracy: 83.67%\n",
      "Batch 177, Loss: 0.888126, Accuracy: 83.70%\n",
      "Batch 178, Loss: 0.939728, Accuracy: 83.67%\n",
      "Batch 179, Loss: 0.923656, Accuracy: 83.67%\n",
      "Batch 180, Loss: 0.849961, Accuracy: 83.71%\n",
      "Batch 181, Loss: 0.891601, Accuracy: 83.72%\n",
      "Batch 182, Loss: 0.947581, Accuracy: 83.70%\n",
      "Batch 183, Loss: 0.877911, Accuracy: 83.72%\n",
      "Batch 184, Loss: 0.863605, Accuracy: 83.74%\n",
      "Batch 185, Loss: 0.899400, Accuracy: 83.74%\n",
      "Batch 186, Loss: 0.886449, Accuracy: 83.75%\n",
      "Batch 187, Loss: 0.951563, Accuracy: 83.72%\n",
      "Batch 188, Loss: 0.937178, Accuracy: 83.69%\n",
      "Batch 189, Loss: 0.996129, Accuracy: 83.63%\n",
      "Batch 190, Loss: 0.865142, Accuracy: 83.64%\n",
      "Batch 191, Loss: 0.914772, Accuracy: 83.64%\n",
      "Batch 192, Loss: 0.917330, Accuracy: 83.63%\n",
      "Batch 193, Loss: 0.877171, Accuracy: 83.65%\n",
      "Batch 194, Loss: 0.900780, Accuracy: 83.66%\n",
      "Batch 195, Loss: 0.842494, Accuracy: 83.69%\n",
      "Batch 196, Loss: 0.860087, Accuracy: 83.73%\n",
      "Batch 197, Loss: 0.917168, Accuracy: 83.72%\n",
      "Batch 198, Loss: 0.870708, Accuracy: 83.74%\n",
      "Batch 199, Loss: 0.950141, Accuracy: 83.72%\n",
      "Batch 200, Loss: 0.887837, Accuracy: 83.73%\n",
      "Batch 201, Loss: 0.919176, Accuracy: 83.73%\n",
      "Batch 202, Loss: 0.912766, Accuracy: 83.73%\n",
      "Batch 203, Loss: 0.965985, Accuracy: 83.69%\n",
      "Batch 204, Loss: 0.890369, Accuracy: 83.70%\n",
      "Batch 205, Loss: 0.930644, Accuracy: 83.70%\n",
      "Batch 206, Loss: 0.896829, Accuracy: 83.71%\n",
      "Batch 207, Loss: 0.943178, Accuracy: 83.68%\n",
      "Batch 208, Loss: 0.908453, Accuracy: 83.67%\n",
      "Batch 209, Loss: 0.954412, Accuracy: 83.65%\n",
      "Batch 210, Loss: 0.899179, Accuracy: 83.65%\n",
      "Batch 211, Loss: 0.849138, Accuracy: 83.69%\n",
      "Batch 212, Loss: 0.849496, Accuracy: 83.71%\n",
      "Batch 213, Loss: 0.889490, Accuracy: 83.71%\n",
      "Training - Epoch 6, Loss: 0.906181, Accuracy: 83.71%\n",
      "Validation Batch 1, Loss: 0.837300, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.845736, Accuracy: 91.41%\n",
      "Validation Batch 3, Loss: 0.962319, Accuracy: 86.98%\n",
      "Validation Batch 4, Loss: 0.883110, Accuracy: 86.72%\n",
      "Validation Batch 5, Loss: 0.835862, Accuracy: 87.19%\n",
      "Validation Batch 6, Loss: 0.829820, Accuracy: 88.02%\n",
      "Validation Batch 7, Loss: 0.895391, Accuracy: 87.50%\n",
      "Validation Batch 8, Loss: 0.948408, Accuracy: 86.33%\n",
      "Validation Batch 9, Loss: 0.924297, Accuracy: 85.76%\n",
      "Validation Batch 10, Loss: 0.928812, Accuracy: 85.31%\n",
      "Validation Batch 11, Loss: 0.861348, Accuracy: 85.51%\n",
      "Validation Batch 12, Loss: 0.845522, Accuracy: 85.81%\n",
      "Validation Batch 13, Loss: 0.896321, Accuracy: 85.82%\n",
      "Validation Batch 14, Loss: 0.905778, Accuracy: 85.60%\n",
      "Validation Batch 15, Loss: 0.900111, Accuracy: 85.52%\n",
      "Validation Batch 16, Loss: 0.875716, Accuracy: 85.55%\n",
      "Validation Batch 17, Loss: 0.941099, Accuracy: 85.20%\n",
      "Validation Batch 18, Loss: 0.869283, Accuracy: 85.33%\n",
      "Validation Batch 19, Loss: 0.928861, Accuracy: 85.12%\n",
      "Validation Batch 20, Loss: 0.830052, Accuracy: 85.39%\n",
      "Validation Batch 21, Loss: 0.892013, Accuracy: 85.42%\n",
      "Validation Batch 22, Loss: 0.887480, Accuracy: 85.44%\n",
      "Validation Batch 23, Loss: 0.910385, Accuracy: 85.39%\n",
      "Validation Batch 24, Loss: 0.947399, Accuracy: 85.16%\n",
      "Validation Batch 25, Loss: 0.890822, Accuracy: 85.06%\n",
      "Validation Batch 26, Loss: 0.891807, Accuracy: 85.04%\n",
      "Validation Batch 27, Loss: 0.801351, Accuracy: 85.26%\n",
      "Validation - Epoch 6, Loss: 0.887645, Accuracy: 85.26%\n",
      "Patienceâ€”9\n",
      "Epoch 7\n",
      "Batch 1, Loss: 0.904185, Accuracy: 85.94%\n",
      "Batch 2, Loss: 0.957486, Accuracy: 83.59%\n",
      "Batch 3, Loss: 0.861305, Accuracy: 85.94%\n",
      "Batch 4, Loss: 0.975250, Accuracy: 83.98%\n",
      "Batch 5, Loss: 0.893890, Accuracy: 84.38%\n",
      "Batch 6, Loss: 0.852256, Accuracy: 85.16%\n",
      "Batch 7, Loss: 0.860480, Accuracy: 85.49%\n",
      "Batch 8, Loss: 0.844189, Accuracy: 85.94%\n",
      "Batch 9, Loss: 0.862212, Accuracy: 86.28%\n",
      "Batch 10, Loss: 0.910003, Accuracy: 86.09%\n",
      "Batch 11, Loss: 0.855094, Accuracy: 86.36%\n",
      "Batch 12, Loss: 0.916618, Accuracy: 86.20%\n",
      "Batch 13, Loss: 0.933781, Accuracy: 85.70%\n",
      "Batch 14, Loss: 0.893225, Accuracy: 85.60%\n",
      "Batch 15, Loss: 0.927886, Accuracy: 85.21%\n",
      "Batch 16, Loss: 0.905349, Accuracy: 85.16%\n",
      "Batch 17, Loss: 0.977583, Accuracy: 84.47%\n",
      "Batch 18, Loss: 0.873153, Accuracy: 84.55%\n",
      "Batch 19, Loss: 0.872467, Accuracy: 84.70%\n",
      "Batch 20, Loss: 0.920875, Accuracy: 84.69%\n",
      "Batch 21, Loss: 0.929479, Accuracy: 84.60%\n",
      "Batch 22, Loss: 0.928208, Accuracy: 84.45%\n",
      "Batch 23, Loss: 0.894561, Accuracy: 84.51%\n",
      "Batch 24, Loss: 0.912232, Accuracy: 84.51%\n",
      "Batch 25, Loss: 0.969023, Accuracy: 84.19%\n",
      "Batch 26, Loss: 0.910344, Accuracy: 84.13%\n",
      "Batch 27, Loss: 0.939370, Accuracy: 84.03%\n",
      "Batch 28, Loss: 0.937822, Accuracy: 83.93%\n",
      "Batch 29, Loss: 0.975710, Accuracy: 83.67%\n",
      "Batch 30, Loss: 0.957247, Accuracy: 83.54%\n",
      "Batch 31, Loss: 0.894058, Accuracy: 83.62%\n",
      "Batch 32, Loss: 0.914214, Accuracy: 83.59%\n",
      "Batch 33, Loss: 0.922526, Accuracy: 83.52%\n",
      "Batch 34, Loss: 0.914776, Accuracy: 83.50%\n",
      "Batch 35, Loss: 0.972316, Accuracy: 83.30%\n",
      "Batch 36, Loss: 0.902956, Accuracy: 83.33%\n",
      "Batch 37, Loss: 0.902193, Accuracy: 83.36%\n",
      "Batch 38, Loss: 0.882538, Accuracy: 83.43%\n",
      "Batch 39, Loss: 0.943885, Accuracy: 83.33%\n",
      "Batch 40, Loss: 0.869904, Accuracy: 83.44%\n",
      "Batch 41, Loss: 0.932294, Accuracy: 83.35%\n",
      "Batch 42, Loss: 0.944541, Accuracy: 83.26%\n",
      "Batch 43, Loss: 0.879956, Accuracy: 83.36%\n",
      "Batch 44, Loss: 0.895625, Accuracy: 83.38%\n",
      "Batch 45, Loss: 0.925747, Accuracy: 83.33%\n",
      "Batch 46, Loss: 0.924535, Accuracy: 83.29%\n",
      "Batch 47, Loss: 0.948421, Accuracy: 83.21%\n",
      "Batch 48, Loss: 0.893548, Accuracy: 83.20%\n",
      "Batch 49, Loss: 0.895263, Accuracy: 83.23%\n",
      "Batch 50, Loss: 0.908428, Accuracy: 83.22%\n",
      "Batch 51, Loss: 0.858552, Accuracy: 83.33%\n",
      "Batch 52, Loss: 0.929751, Accuracy: 83.32%\n",
      "Batch 53, Loss: 0.968006, Accuracy: 83.20%\n",
      "Batch 54, Loss: 0.950825, Accuracy: 83.10%\n",
      "Batch 55, Loss: 0.857868, Accuracy: 83.18%\n",
      "Batch 56, Loss: 0.948848, Accuracy: 83.12%\n",
      "Batch 57, Loss: 0.817669, Accuracy: 83.28%\n",
      "Batch 58, Loss: 0.909752, Accuracy: 83.30%\n",
      "Batch 59, Loss: 0.921457, Accuracy: 83.26%\n",
      "Batch 60, Loss: 0.811039, Accuracy: 83.46%\n",
      "Batch 61, Loss: 0.884660, Accuracy: 83.50%\n",
      "Batch 62, Loss: 0.911949, Accuracy: 83.52%\n",
      "Batch 63, Loss: 0.854875, Accuracy: 83.58%\n",
      "Batch 64, Loss: 0.903364, Accuracy: 83.59%\n",
      "Batch 65, Loss: 0.899807, Accuracy: 83.63%\n",
      "Batch 66, Loss: 0.965555, Accuracy: 83.57%\n",
      "Batch 67, Loss: 0.884112, Accuracy: 83.63%\n",
      "Batch 68, Loss: 0.902293, Accuracy: 83.66%\n",
      "Batch 69, Loss: 0.840955, Accuracy: 83.76%\n",
      "Batch 70, Loss: 0.914357, Accuracy: 83.77%\n",
      "Batch 71, Loss: 0.868879, Accuracy: 83.82%\n",
      "Batch 72, Loss: 0.864456, Accuracy: 83.90%\n",
      "Batch 73, Loss: 0.893378, Accuracy: 83.90%\n",
      "Batch 74, Loss: 0.849951, Accuracy: 84.02%\n",
      "Batch 75, Loss: 0.885065, Accuracy: 84.04%\n",
      "Batch 76, Loss: 0.893996, Accuracy: 84.07%\n",
      "Batch 77, Loss: 0.875905, Accuracy: 84.09%\n",
      "Batch 78, Loss: 0.963641, Accuracy: 84.01%\n",
      "Batch 79, Loss: 0.864017, Accuracy: 84.10%\n",
      "Batch 80, Loss: 0.843240, Accuracy: 84.18%\n",
      "Batch 81, Loss: 0.928289, Accuracy: 84.14%\n",
      "Batch 82, Loss: 0.994133, Accuracy: 84.01%\n",
      "Batch 83, Loss: 1.012123, Accuracy: 83.87%\n",
      "Batch 84, Loss: 0.870369, Accuracy: 83.91%\n",
      "Batch 85, Loss: 0.970897, Accuracy: 83.82%\n",
      "Batch 86, Loss: 0.881586, Accuracy: 83.85%\n",
      "Batch 87, Loss: 0.939332, Accuracy: 83.78%\n",
      "Batch 88, Loss: 0.902335, Accuracy: 83.77%\n",
      "Batch 89, Loss: 0.826601, Accuracy: 83.87%\n",
      "Batch 90, Loss: 0.941907, Accuracy: 83.82%\n",
      "Batch 91, Loss: 0.922198, Accuracy: 83.79%\n",
      "Batch 92, Loss: 0.824246, Accuracy: 83.90%\n",
      "Batch 93, Loss: 0.881558, Accuracy: 83.92%\n",
      "Batch 94, Loss: 0.928249, Accuracy: 83.91%\n",
      "Batch 95, Loss: 0.932186, Accuracy: 83.90%\n",
      "Batch 96, Loss: 0.903874, Accuracy: 83.90%\n",
      "Batch 97, Loss: 0.923501, Accuracy: 83.89%\n",
      "Batch 98, Loss: 0.943263, Accuracy: 83.83%\n",
      "Batch 99, Loss: 0.950962, Accuracy: 83.79%\n",
      "Batch 100, Loss: 0.851602, Accuracy: 83.86%\n",
      "Batch 101, Loss: 0.952784, Accuracy: 83.79%\n",
      "Batch 102, Loss: 0.879566, Accuracy: 83.82%\n",
      "Batch 103, Loss: 0.929440, Accuracy: 83.78%\n",
      "Batch 104, Loss: 0.854973, Accuracy: 83.83%\n",
      "Batch 105, Loss: 0.887725, Accuracy: 83.85%\n",
      "Batch 106, Loss: 0.902974, Accuracy: 83.86%\n",
      "Batch 107, Loss: 0.888112, Accuracy: 83.88%\n",
      "Batch 108, Loss: 0.910453, Accuracy: 83.90%\n",
      "Batch 109, Loss: 0.904874, Accuracy: 83.90%\n",
      "Batch 110, Loss: 0.898687, Accuracy: 83.89%\n",
      "Batch 111, Loss: 0.950312, Accuracy: 83.85%\n",
      "Batch 112, Loss: 0.944252, Accuracy: 83.80%\n",
      "Batch 113, Loss: 0.861817, Accuracy: 83.84%\n",
      "Batch 114, Loss: 0.841512, Accuracy: 83.91%\n",
      "Batch 115, Loss: 0.904810, Accuracy: 83.90%\n",
      "Batch 116, Loss: 0.880177, Accuracy: 83.92%\n",
      "Batch 117, Loss: 0.971714, Accuracy: 83.87%\n",
      "Batch 118, Loss: 0.906965, Accuracy: 83.87%\n",
      "Batch 119, Loss: 0.982640, Accuracy: 83.81%\n",
      "Batch 120, Loss: 0.829432, Accuracy: 83.89%\n",
      "Batch 121, Loss: 0.849893, Accuracy: 83.95%\n",
      "Batch 122, Loss: 0.884116, Accuracy: 83.97%\n",
      "Batch 123, Loss: 0.889652, Accuracy: 83.98%\n",
      "Batch 124, Loss: 0.819458, Accuracy: 84.06%\n",
      "Batch 125, Loss: 0.879499, Accuracy: 84.08%\n",
      "Batch 126, Loss: 1.008149, Accuracy: 83.98%\n",
      "Batch 127, Loss: 0.913077, Accuracy: 83.97%\n",
      "Batch 128, Loss: 0.868249, Accuracy: 84.00%\n",
      "Batch 129, Loss: 0.890158, Accuracy: 84.02%\n",
      "Batch 130, Loss: 0.892565, Accuracy: 84.03%\n",
      "Batch 131, Loss: 0.956981, Accuracy: 83.98%\n",
      "Batch 132, Loss: 0.927955, Accuracy: 83.97%\n",
      "Batch 133, Loss: 0.939956, Accuracy: 83.93%\n",
      "Batch 134, Loss: 0.870145, Accuracy: 83.94%\n",
      "Batch 135, Loss: 0.885077, Accuracy: 83.96%\n",
      "Batch 136, Loss: 0.920251, Accuracy: 83.95%\n",
      "Batch 137, Loss: 0.919425, Accuracy: 83.93%\n",
      "Batch 138, Loss: 0.924696, Accuracy: 83.92%\n",
      "Batch 139, Loss: 0.942356, Accuracy: 83.89%\n",
      "Batch 140, Loss: 0.931741, Accuracy: 83.87%\n",
      "Batch 141, Loss: 0.884733, Accuracy: 83.88%\n",
      "Batch 142, Loss: 0.861075, Accuracy: 83.90%\n",
      "Batch 143, Loss: 0.813975, Accuracy: 83.98%\n",
      "Batch 144, Loss: 0.835979, Accuracy: 84.03%\n",
      "Batch 145, Loss: 0.875795, Accuracy: 84.04%\n",
      "Batch 146, Loss: 0.877965, Accuracy: 84.05%\n",
      "Batch 147, Loss: 0.968384, Accuracy: 84.01%\n",
      "Batch 148, Loss: 0.898102, Accuracy: 84.02%\n",
      "Batch 149, Loss: 0.884761, Accuracy: 84.03%\n",
      "Batch 150, Loss: 0.941891, Accuracy: 84.00%\n",
      "Batch 151, Loss: 0.842161, Accuracy: 84.04%\n",
      "Batch 152, Loss: 0.838168, Accuracy: 84.09%\n",
      "Batch 153, Loss: 0.902421, Accuracy: 84.08%\n",
      "Batch 154, Loss: 0.882007, Accuracy: 84.09%\n",
      "Batch 155, Loss: 0.920977, Accuracy: 84.06%\n",
      "Batch 156, Loss: 0.912551, Accuracy: 84.05%\n",
      "Batch 157, Loss: 0.956311, Accuracy: 84.03%\n",
      "Batch 158, Loss: 0.959887, Accuracy: 83.99%\n",
      "Batch 159, Loss: 0.878212, Accuracy: 84.00%\n",
      "Batch 160, Loss: 0.929918, Accuracy: 83.97%\n",
      "Batch 161, Loss: 0.944857, Accuracy: 83.95%\n",
      "Batch 162, Loss: 0.946748, Accuracy: 83.92%\n",
      "Batch 163, Loss: 0.917266, Accuracy: 83.91%\n",
      "Batch 164, Loss: 0.899627, Accuracy: 83.92%\n",
      "Batch 165, Loss: 0.937063, Accuracy: 83.90%\n",
      "Batch 166, Loss: 0.920434, Accuracy: 83.89%\n",
      "Batch 167, Loss: 0.865929, Accuracy: 83.90%\n",
      "Batch 168, Loss: 0.926589, Accuracy: 83.89%\n",
      "Batch 169, Loss: 0.911975, Accuracy: 83.88%\n",
      "Batch 170, Loss: 0.858521, Accuracy: 83.92%\n",
      "Batch 171, Loss: 0.986412, Accuracy: 83.87%\n",
      "Batch 172, Loss: 0.912197, Accuracy: 83.86%\n",
      "Batch 173, Loss: 0.919273, Accuracy: 83.86%\n",
      "Batch 174, Loss: 0.979785, Accuracy: 83.80%\n",
      "Batch 175, Loss: 0.891989, Accuracy: 83.80%\n",
      "Batch 176, Loss: 0.883133, Accuracy: 83.82%\n",
      "Batch 177, Loss: 0.971094, Accuracy: 83.78%\n",
      "Batch 178, Loss: 0.959478, Accuracy: 83.75%\n",
      "Batch 179, Loss: 0.871469, Accuracy: 83.76%\n",
      "Batch 180, Loss: 0.891243, Accuracy: 83.78%\n",
      "Batch 181, Loss: 0.973164, Accuracy: 83.74%\n",
      "Batch 182, Loss: 0.996412, Accuracy: 83.69%\n",
      "Batch 183, Loss: 0.824764, Accuracy: 83.73%\n",
      "Batch 184, Loss: 0.884925, Accuracy: 83.74%\n",
      "Batch 185, Loss: 0.911437, Accuracy: 83.73%\n",
      "Batch 186, Loss: 0.903312, Accuracy: 83.74%\n",
      "Batch 187, Loss: 0.841789, Accuracy: 83.77%\n",
      "Batch 188, Loss: 0.951043, Accuracy: 83.75%\n",
      "Batch 189, Loss: 1.041677, Accuracy: 83.66%\n",
      "Batch 190, Loss: 0.842125, Accuracy: 83.71%\n",
      "Batch 191, Loss: 0.939813, Accuracy: 83.68%\n",
      "Batch 192, Loss: 0.942461, Accuracy: 83.67%\n",
      "Batch 193, Loss: 0.874908, Accuracy: 83.69%\n",
      "Batch 194, Loss: 0.943124, Accuracy: 83.67%\n",
      "Batch 195, Loss: 0.885865, Accuracy: 83.69%\n",
      "Batch 196, Loss: 0.856076, Accuracy: 83.71%\n",
      "Batch 197, Loss: 0.939148, Accuracy: 83.68%\n",
      "Batch 198, Loss: 0.984244, Accuracy: 83.65%\n",
      "Batch 199, Loss: 0.942035, Accuracy: 83.64%\n",
      "Batch 200, Loss: 0.850056, Accuracy: 83.66%\n",
      "Batch 201, Loss: 0.864161, Accuracy: 83.68%\n",
      "Batch 202, Loss: 0.911973, Accuracy: 83.68%\n",
      "Batch 203, Loss: 0.909302, Accuracy: 83.68%\n",
      "Batch 204, Loss: 0.917063, Accuracy: 83.68%\n",
      "Batch 205, Loss: 0.919338, Accuracy: 83.67%\n",
      "Batch 206, Loss: 0.873960, Accuracy: 83.68%\n",
      "Batch 207, Loss: 0.876330, Accuracy: 83.70%\n",
      "Batch 208, Loss: 0.993048, Accuracy: 83.65%\n",
      "Batch 209, Loss: 0.906188, Accuracy: 83.65%\n",
      "Batch 210, Loss: 0.866034, Accuracy: 83.67%\n",
      "Batch 211, Loss: 0.920622, Accuracy: 83.66%\n",
      "Batch 212, Loss: 0.953706, Accuracy: 83.63%\n",
      "Batch 213, Loss: 0.971245, Accuracy: 83.60%\n",
      "Training - Epoch 7, Loss: 0.907769, Accuracy: 83.60%\n",
      "Validation Batch 1, Loss: 0.837528, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.845832, Accuracy: 91.41%\n",
      "Validation Batch 3, Loss: 0.962343, Accuracy: 86.98%\n",
      "Validation Batch 4, Loss: 0.883271, Accuracy: 86.33%\n",
      "Validation Batch 5, Loss: 0.835946, Accuracy: 86.88%\n",
      "Validation Batch 6, Loss: 0.830059, Accuracy: 87.76%\n",
      "Validation Batch 7, Loss: 0.895461, Accuracy: 87.28%\n",
      "Validation Batch 8, Loss: 0.948435, Accuracy: 86.13%\n",
      "Validation Batch 9, Loss: 0.925318, Accuracy: 85.59%\n",
      "Validation Batch 10, Loss: 0.929245, Accuracy: 85.16%\n",
      "Validation Batch 11, Loss: 0.861921, Accuracy: 85.37%\n",
      "Validation Batch 12, Loss: 0.845158, Accuracy: 85.68%\n",
      "Validation Batch 13, Loss: 0.895930, Accuracy: 85.70%\n",
      "Validation Batch 14, Loss: 0.905634, Accuracy: 85.49%\n",
      "Validation Batch 15, Loss: 0.900296, Accuracy: 85.42%\n",
      "Validation Batch 16, Loss: 0.875694, Accuracy: 85.45%\n",
      "Validation Batch 17, Loss: 0.940896, Accuracy: 85.11%\n",
      "Validation Batch 18, Loss: 0.869147, Accuracy: 85.24%\n",
      "Validation Batch 19, Loss: 0.928606, Accuracy: 85.03%\n",
      "Validation Batch 20, Loss: 0.830084, Accuracy: 85.31%\n",
      "Validation Batch 21, Loss: 0.892202, Accuracy: 85.34%\n",
      "Validation Batch 22, Loss: 0.887198, Accuracy: 85.37%\n",
      "Validation Batch 23, Loss: 0.910418, Accuracy: 85.33%\n",
      "Validation Batch 24, Loss: 0.947321, Accuracy: 85.09%\n",
      "Validation Batch 25, Loss: 0.890982, Accuracy: 85.00%\n",
      "Validation Batch 26, Loss: 0.892320, Accuracy: 84.98%\n",
      "Validation Batch 27, Loss: 0.801710, Accuracy: 85.20%\n",
      "Validation - Epoch 7, Loss: 0.887739, Accuracy: 85.20%\n",
      "Patienceâ€”10\n",
      "Epoch 8\n",
      "Batch 1, Loss: 0.944229, Accuracy: 81.25%\n",
      "Batch 2, Loss: 0.944929, Accuracy: 81.25%\n",
      "Batch 3, Loss: 0.878288, Accuracy: 82.81%\n",
      "Batch 4, Loss: 0.955768, Accuracy: 82.03%\n",
      "Batch 5, Loss: 0.954490, Accuracy: 81.25%\n",
      "Batch 6, Loss: 0.888583, Accuracy: 81.51%\n",
      "Batch 7, Loss: 0.958441, Accuracy: 81.03%\n",
      "Batch 8, Loss: 0.939877, Accuracy: 81.05%\n",
      "Batch 9, Loss: 0.911319, Accuracy: 81.42%\n",
      "Batch 10, Loss: 0.909984, Accuracy: 81.72%\n",
      "Batch 11, Loss: 0.857909, Accuracy: 82.24%\n",
      "Batch 12, Loss: 0.898719, Accuracy: 82.29%\n",
      "Batch 13, Loss: 0.897497, Accuracy: 82.45%\n",
      "Batch 14, Loss: 0.963603, Accuracy: 82.03%\n",
      "Batch 15, Loss: 0.899728, Accuracy: 82.19%\n",
      "Batch 16, Loss: 0.868550, Accuracy: 82.62%\n",
      "Batch 17, Loss: 0.901114, Accuracy: 82.72%\n",
      "Batch 18, Loss: 0.970691, Accuracy: 82.38%\n",
      "Batch 19, Loss: 0.915338, Accuracy: 82.40%\n",
      "Batch 20, Loss: 0.878697, Accuracy: 82.66%\n",
      "Batch 21, Loss: 0.930708, Accuracy: 82.51%\n",
      "Batch 22, Loss: 0.978562, Accuracy: 82.24%\n",
      "Batch 23, Loss: 0.917934, Accuracy: 82.27%\n",
      "Batch 24, Loss: 0.944782, Accuracy: 82.03%\n",
      "Batch 25, Loss: 0.902219, Accuracy: 82.12%\n",
      "Batch 26, Loss: 0.921578, Accuracy: 82.15%\n",
      "Batch 27, Loss: 0.850857, Accuracy: 82.47%\n",
      "Batch 28, Loss: 0.901865, Accuracy: 82.53%\n",
      "Batch 29, Loss: 0.883540, Accuracy: 82.65%\n",
      "Batch 30, Loss: 0.872443, Accuracy: 82.81%\n",
      "Batch 31, Loss: 0.828845, Accuracy: 83.11%\n",
      "Batch 32, Loss: 0.911558, Accuracy: 83.15%\n",
      "Batch 33, Loss: 0.939382, Accuracy: 83.10%\n",
      "Batch 34, Loss: 0.891139, Accuracy: 83.13%\n",
      "Batch 35, Loss: 0.867017, Accuracy: 83.30%\n",
      "Batch 36, Loss: 0.908651, Accuracy: 83.29%\n",
      "Batch 37, Loss: 0.909986, Accuracy: 83.23%\n",
      "Batch 38, Loss: 0.860665, Accuracy: 83.39%\n",
      "Batch 39, Loss: 0.905274, Accuracy: 83.45%\n",
      "Batch 40, Loss: 0.957817, Accuracy: 83.32%\n",
      "Batch 41, Loss: 0.917805, Accuracy: 83.31%\n",
      "Batch 42, Loss: 0.883826, Accuracy: 83.33%\n",
      "Batch 43, Loss: 0.915255, Accuracy: 83.32%\n",
      "Batch 44, Loss: 0.962510, Accuracy: 83.20%\n",
      "Batch 45, Loss: 0.924103, Accuracy: 83.16%\n",
      "Batch 46, Loss: 0.911032, Accuracy: 83.12%\n",
      "Batch 47, Loss: 0.927794, Accuracy: 83.08%\n",
      "Batch 48, Loss: 0.885591, Accuracy: 83.17%\n",
      "Batch 49, Loss: 0.895457, Accuracy: 83.20%\n",
      "Batch 50, Loss: 0.882736, Accuracy: 83.25%\n",
      "Batch 51, Loss: 0.937004, Accuracy: 83.18%\n",
      "Batch 52, Loss: 0.829341, Accuracy: 83.35%\n",
      "Batch 53, Loss: 0.867316, Accuracy: 83.43%\n",
      "Batch 54, Loss: 0.923166, Accuracy: 83.42%\n",
      "Batch 55, Loss: 0.892388, Accuracy: 83.44%\n",
      "Batch 56, Loss: 0.781896, Accuracy: 83.68%\n",
      "Batch 57, Loss: 0.846754, Accuracy: 83.77%\n",
      "Batch 58, Loss: 0.884462, Accuracy: 83.84%\n",
      "Batch 59, Loss: 0.894200, Accuracy: 83.82%\n",
      "Batch 60, Loss: 0.863022, Accuracy: 83.91%\n",
      "Batch 61, Loss: 0.861775, Accuracy: 83.99%\n",
      "Batch 62, Loss: 0.846761, Accuracy: 84.05%\n",
      "Batch 63, Loss: 0.984680, Accuracy: 83.90%\n",
      "Batch 64, Loss: 0.931316, Accuracy: 83.84%\n",
      "Batch 65, Loss: 0.834910, Accuracy: 83.97%\n",
      "Batch 66, Loss: 0.924463, Accuracy: 83.93%\n",
      "Batch 67, Loss: 0.928327, Accuracy: 83.91%\n",
      "Batch 68, Loss: 0.928229, Accuracy: 83.87%\n",
      "Batch 69, Loss: 0.868753, Accuracy: 83.90%\n",
      "Batch 70, Loss: 0.857588, Accuracy: 83.97%\n",
      "Batch 71, Loss: 0.831049, Accuracy: 84.07%\n",
      "Batch 72, Loss: 0.807441, Accuracy: 84.20%\n",
      "Batch 73, Loss: 0.864501, Accuracy: 84.27%\n",
      "Batch 74, Loss: 0.864937, Accuracy: 84.29%\n",
      "Batch 75, Loss: 0.921521, Accuracy: 84.27%\n",
      "Batch 76, Loss: 0.902075, Accuracy: 84.25%\n",
      "Batch 77, Loss: 0.910377, Accuracy: 84.25%\n",
      "Batch 78, Loss: 0.941834, Accuracy: 84.23%\n",
      "Batch 79, Loss: 0.923211, Accuracy: 84.20%\n",
      "Batch 80, Loss: 0.974059, Accuracy: 84.08%\n",
      "Batch 81, Loss: 0.949447, Accuracy: 84.03%\n",
      "Batch 82, Loss: 0.853955, Accuracy: 84.11%\n",
      "Batch 83, Loss: 0.926003, Accuracy: 84.07%\n",
      "Batch 84, Loss: 0.868746, Accuracy: 84.11%\n",
      "Batch 85, Loss: 0.857338, Accuracy: 84.19%\n",
      "Batch 86, Loss: 0.929343, Accuracy: 84.16%\n",
      "Batch 87, Loss: 0.871185, Accuracy: 84.20%\n",
      "Batch 88, Loss: 0.912779, Accuracy: 84.18%\n",
      "Batch 89, Loss: 0.887251, Accuracy: 84.20%\n",
      "Batch 90, Loss: 0.851461, Accuracy: 84.25%\n",
      "Batch 91, Loss: 0.902926, Accuracy: 84.24%\n",
      "Batch 92, Loss: 0.876630, Accuracy: 84.27%\n",
      "Batch 93, Loss: 0.918659, Accuracy: 84.24%\n",
      "Batch 94, Loss: 0.875255, Accuracy: 84.26%\n",
      "Batch 95, Loss: 0.936400, Accuracy: 84.21%\n",
      "Batch 96, Loss: 0.924784, Accuracy: 84.18%\n",
      "Batch 97, Loss: 0.915018, Accuracy: 84.15%\n",
      "Batch 98, Loss: 0.939607, Accuracy: 84.10%\n",
      "Batch 99, Loss: 0.915097, Accuracy: 84.09%\n",
      "Batch 100, Loss: 0.897258, Accuracy: 84.09%\n",
      "Batch 101, Loss: 0.890172, Accuracy: 84.10%\n",
      "Batch 102, Loss: 0.945710, Accuracy: 84.05%\n",
      "Batch 103, Loss: 0.958543, Accuracy: 84.00%\n",
      "Batch 104, Loss: 0.866079, Accuracy: 84.03%\n",
      "Batch 105, Loss: 0.865409, Accuracy: 84.08%\n",
      "Batch 106, Loss: 0.822418, Accuracy: 84.15%\n",
      "Batch 107, Loss: 0.869035, Accuracy: 84.17%\n",
      "Batch 108, Loss: 0.854319, Accuracy: 84.20%\n",
      "Batch 109, Loss: 0.888817, Accuracy: 84.22%\n",
      "Batch 110, Loss: 0.836185, Accuracy: 84.28%\n",
      "Batch 111, Loss: 0.881601, Accuracy: 84.30%\n",
      "Batch 112, Loss: 0.956750, Accuracy: 84.25%\n",
      "Batch 113, Loss: 0.975585, Accuracy: 84.17%\n",
      "Batch 114, Loss: 0.840025, Accuracy: 84.21%\n",
      "Batch 115, Loss: 0.913418, Accuracy: 84.20%\n",
      "Batch 116, Loss: 0.981738, Accuracy: 84.12%\n",
      "Batch 117, Loss: 0.848416, Accuracy: 84.16%\n",
      "Batch 118, Loss: 0.940304, Accuracy: 84.14%\n",
      "Batch 119, Loss: 0.945783, Accuracy: 84.10%\n",
      "Batch 120, Loss: 0.880839, Accuracy: 84.10%\n",
      "Batch 121, Loss: 0.950483, Accuracy: 84.08%\n",
      "Batch 122, Loss: 0.898268, Accuracy: 84.09%\n",
      "Batch 123, Loss: 0.928480, Accuracy: 84.07%\n",
      "Batch 124, Loss: 0.957884, Accuracy: 84.03%\n",
      "Batch 125, Loss: 0.916006, Accuracy: 84.03%\n",
      "Batch 126, Loss: 0.871105, Accuracy: 84.05%\n",
      "Batch 127, Loss: 0.970657, Accuracy: 84.01%\n",
      "Batch 128, Loss: 0.905039, Accuracy: 84.00%\n",
      "Batch 129, Loss: 0.937811, Accuracy: 83.96%\n",
      "Batch 130, Loss: 0.928638, Accuracy: 83.94%\n",
      "Batch 131, Loss: 0.919444, Accuracy: 83.93%\n",
      "Batch 132, Loss: 0.934718, Accuracy: 83.91%\n",
      "Batch 133, Loss: 0.905975, Accuracy: 83.91%\n",
      "Batch 134, Loss: 0.869373, Accuracy: 83.91%\n",
      "Batch 135, Loss: 0.931735, Accuracy: 83.89%\n",
      "Batch 136, Loss: 0.883344, Accuracy: 83.90%\n",
      "Batch 137, Loss: 0.956391, Accuracy: 83.86%\n",
      "Batch 138, Loss: 0.875557, Accuracy: 83.88%\n",
      "Batch 139, Loss: 0.906676, Accuracy: 83.87%\n",
      "Batch 140, Loss: 0.847823, Accuracy: 83.93%\n",
      "Batch 141, Loss: 0.872199, Accuracy: 83.95%\n",
      "Batch 142, Loss: 0.919994, Accuracy: 83.95%\n",
      "Batch 143, Loss: 0.920653, Accuracy: 83.94%\n",
      "Batch 144, Loss: 0.942021, Accuracy: 83.91%\n",
      "Batch 145, Loss: 0.808787, Accuracy: 83.98%\n",
      "Batch 146, Loss: 0.882936, Accuracy: 84.02%\n",
      "Batch 147, Loss: 0.949870, Accuracy: 83.98%\n",
      "Batch 148, Loss: 0.881492, Accuracy: 84.01%\n",
      "Batch 149, Loss: 0.886318, Accuracy: 84.01%\n",
      "Batch 150, Loss: 0.867635, Accuracy: 84.02%\n",
      "Batch 151, Loss: 0.946812, Accuracy: 83.99%\n",
      "Batch 152, Loss: 0.844153, Accuracy: 84.04%\n",
      "Batch 153, Loss: 0.913157, Accuracy: 84.02%\n",
      "Batch 154, Loss: 0.928307, Accuracy: 84.01%\n",
      "Batch 155, Loss: 0.813216, Accuracy: 84.08%\n",
      "Batch 156, Loss: 0.869302, Accuracy: 84.12%\n",
      "Batch 157, Loss: 0.904677, Accuracy: 84.13%\n",
      "Batch 158, Loss: 0.871492, Accuracy: 84.15%\n",
      "Batch 159, Loss: 0.801892, Accuracy: 84.22%\n",
      "Batch 160, Loss: 0.961417, Accuracy: 84.19%\n",
      "Batch 161, Loss: 0.974950, Accuracy: 84.14%\n",
      "Batch 162, Loss: 0.904555, Accuracy: 84.14%\n",
      "Batch 163, Loss: 0.926878, Accuracy: 84.13%\n",
      "Batch 164, Loss: 0.835167, Accuracy: 84.17%\n",
      "Batch 165, Loss: 0.923091, Accuracy: 84.15%\n",
      "Batch 166, Loss: 0.916696, Accuracy: 84.15%\n",
      "Batch 167, Loss: 0.926913, Accuracy: 84.12%\n",
      "Batch 168, Loss: 0.992649, Accuracy: 84.07%\n",
      "Batch 169, Loss: 0.835752, Accuracy: 84.12%\n",
      "Batch 170, Loss: 0.978493, Accuracy: 84.06%\n",
      "Batch 171, Loss: 0.940444, Accuracy: 84.03%\n",
      "Batch 172, Loss: 0.942214, Accuracy: 83.99%\n",
      "Batch 173, Loss: 0.883189, Accuracy: 84.00%\n",
      "Batch 174, Loss: 0.828076, Accuracy: 84.05%\n",
      "Batch 175, Loss: 0.928228, Accuracy: 84.04%\n",
      "Batch 176, Loss: 0.864449, Accuracy: 84.07%\n",
      "Batch 177, Loss: 0.940238, Accuracy: 84.06%\n",
      "Batch 178, Loss: 0.989428, Accuracy: 84.02%\n",
      "Batch 179, Loss: 0.868644, Accuracy: 84.04%\n",
      "Batch 180, Loss: 0.974771, Accuracy: 84.00%\n",
      "Batch 181, Loss: 0.946984, Accuracy: 83.97%\n",
      "Batch 182, Loss: 0.943777, Accuracy: 83.94%\n",
      "Batch 183, Loss: 0.824652, Accuracy: 83.98%\n",
      "Batch 184, Loss: 0.925766, Accuracy: 83.98%\n",
      "Batch 185, Loss: 0.893291, Accuracy: 83.99%\n",
      "Batch 186, Loss: 0.989301, Accuracy: 83.93%\n",
      "Batch 187, Loss: 0.877259, Accuracy: 83.94%\n",
      "Batch 188, Loss: 0.966777, Accuracy: 83.90%\n",
      "Batch 189, Loss: 0.943300, Accuracy: 83.88%\n",
      "Batch 190, Loss: 0.868043, Accuracy: 83.91%\n",
      "Batch 191, Loss: 0.926907, Accuracy: 83.90%\n",
      "Batch 192, Loss: 0.915299, Accuracy: 83.89%\n",
      "Batch 193, Loss: 0.865596, Accuracy: 83.92%\n",
      "Batch 194, Loss: 1.000508, Accuracy: 83.88%\n",
      "Batch 195, Loss: 0.885655, Accuracy: 83.89%\n",
      "Batch 196, Loss: 0.839084, Accuracy: 83.94%\n",
      "Batch 197, Loss: 0.886600, Accuracy: 83.95%\n",
      "Batch 198, Loss: 0.889359, Accuracy: 83.96%\n",
      "Batch 199, Loss: 0.962119, Accuracy: 83.94%\n",
      "Batch 200, Loss: 0.974473, Accuracy: 83.90%\n",
      "Batch 201, Loss: 0.957487, Accuracy: 83.87%\n",
      "Batch 202, Loss: 0.930260, Accuracy: 83.85%\n",
      "Batch 203, Loss: 0.875622, Accuracy: 83.87%\n",
      "Batch 204, Loss: 0.835545, Accuracy: 83.90%\n",
      "Batch 205, Loss: 0.937010, Accuracy: 83.88%\n",
      "Batch 206, Loss: 0.831180, Accuracy: 83.91%\n",
      "Batch 207, Loss: 0.942719, Accuracy: 83.88%\n",
      "Batch 208, Loss: 0.902142, Accuracy: 83.89%\n",
      "Batch 209, Loss: 0.919486, Accuracy: 83.87%\n",
      "Batch 210, Loss: 0.872374, Accuracy: 83.88%\n",
      "Batch 211, Loss: 0.864806, Accuracy: 83.90%\n",
      "Batch 212, Loss: 0.929689, Accuracy: 83.90%\n",
      "Batch 213, Loss: 0.846998, Accuracy: 83.92%\n",
      "Training - Epoch 8, Loss: 0.904004, Accuracy: 83.92%\n",
      "Validation Batch 1, Loss: 0.838167, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.847646, Accuracy: 90.62%\n",
      "Validation Batch 3, Loss: 0.963822, Accuracy: 86.46%\n",
      "Validation Batch 4, Loss: 0.884696, Accuracy: 85.94%\n",
      "Validation Batch 5, Loss: 0.837717, Accuracy: 86.56%\n",
      "Validation Batch 6, Loss: 0.830815, Accuracy: 87.24%\n",
      "Validation Batch 7, Loss: 0.896120, Accuracy: 86.83%\n",
      "Validation Batch 8, Loss: 0.949017, Accuracy: 85.74%\n",
      "Validation Batch 9, Loss: 0.926828, Accuracy: 85.07%\n",
      "Validation Batch 10, Loss: 0.930057, Accuracy: 84.69%\n",
      "Validation Batch 11, Loss: 0.863059, Accuracy: 84.94%\n",
      "Validation Batch 12, Loss: 0.846060, Accuracy: 85.29%\n",
      "Validation Batch 13, Loss: 0.896610, Accuracy: 85.34%\n",
      "Validation Batch 14, Loss: 0.906824, Accuracy: 85.16%\n",
      "Validation Batch 15, Loss: 0.900931, Accuracy: 85.10%\n",
      "Validation Batch 16, Loss: 0.876126, Accuracy: 85.16%\n",
      "Validation Batch 17, Loss: 0.940929, Accuracy: 84.83%\n",
      "Validation Batch 18, Loss: 0.870552, Accuracy: 84.90%\n",
      "Validation Batch 19, Loss: 0.929824, Accuracy: 84.70%\n",
      "Validation Batch 20, Loss: 0.830554, Accuracy: 85.00%\n",
      "Validation Batch 21, Loss: 0.892938, Accuracy: 85.04%\n",
      "Validation Batch 22, Loss: 0.889114, Accuracy: 85.01%\n",
      "Validation Batch 23, Loss: 0.910928, Accuracy: 84.92%\n",
      "Validation Batch 24, Loss: 0.947725, Accuracy: 84.70%\n",
      "Validation Batch 25, Loss: 0.892057, Accuracy: 84.62%\n",
      "Validation Batch 26, Loss: 0.893788, Accuracy: 84.62%\n",
      "Validation Batch 27, Loss: 0.802905, Accuracy: 84.85%\n",
      "Validation - Epoch 8, Loss: 0.888734, Accuracy: 84.85%\n",
      "Patienceâ€”11\n",
      "Epoch 9\n",
      "Batch 1, Loss: 0.851070, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.912763, Accuracy: 87.50%\n",
      "Batch 3, Loss: 0.940696, Accuracy: 84.38%\n",
      "Batch 4, Loss: 0.864926, Accuracy: 85.94%\n",
      "Batch 5, Loss: 0.871026, Accuracy: 85.62%\n",
      "Batch 6, Loss: 0.824372, Accuracy: 86.72%\n",
      "Batch 7, Loss: 0.903628, Accuracy: 86.38%\n",
      "Batch 8, Loss: 0.943311, Accuracy: 85.35%\n",
      "Batch 9, Loss: 0.889046, Accuracy: 85.59%\n",
      "Batch 10, Loss: 0.899002, Accuracy: 85.47%\n",
      "Batch 11, Loss: 0.913098, Accuracy: 85.09%\n",
      "Batch 12, Loss: 0.830125, Accuracy: 85.55%\n",
      "Batch 13, Loss: 0.906951, Accuracy: 85.46%\n",
      "Batch 14, Loss: 0.885001, Accuracy: 85.49%\n",
      "Batch 15, Loss: 0.939232, Accuracy: 85.10%\n",
      "Batch 16, Loss: 0.942687, Accuracy: 84.86%\n",
      "Batch 17, Loss: 0.975358, Accuracy: 84.38%\n",
      "Batch 18, Loss: 0.913216, Accuracy: 84.29%\n",
      "Batch 19, Loss: 0.895404, Accuracy: 84.29%\n",
      "Batch 20, Loss: 0.854399, Accuracy: 84.61%\n",
      "Batch 21, Loss: 0.910304, Accuracy: 84.60%\n",
      "Batch 22, Loss: 0.854141, Accuracy: 84.80%\n",
      "Batch 23, Loss: 0.913843, Accuracy: 84.65%\n",
      "Batch 24, Loss: 0.878437, Accuracy: 84.77%\n",
      "Batch 25, Loss: 0.910325, Accuracy: 84.75%\n",
      "Batch 26, Loss: 0.922224, Accuracy: 84.68%\n",
      "Batch 27, Loss: 0.909913, Accuracy: 84.61%\n",
      "Batch 28, Loss: 0.944371, Accuracy: 84.43%\n",
      "Batch 29, Loss: 0.880621, Accuracy: 84.48%\n",
      "Batch 30, Loss: 0.890261, Accuracy: 84.53%\n",
      "Batch 31, Loss: 0.916360, Accuracy: 84.43%\n",
      "Batch 32, Loss: 0.881305, Accuracy: 84.47%\n",
      "Batch 33, Loss: 0.954591, Accuracy: 84.28%\n",
      "Batch 34, Loss: 0.945062, Accuracy: 84.15%\n",
      "Batch 35, Loss: 0.920113, Accuracy: 84.11%\n",
      "Batch 36, Loss: 0.911901, Accuracy: 84.07%\n",
      "Batch 37, Loss: 0.932438, Accuracy: 83.99%\n",
      "Batch 38, Loss: 0.871155, Accuracy: 84.09%\n",
      "Batch 39, Loss: 0.920234, Accuracy: 84.01%\n",
      "Batch 40, Loss: 0.933443, Accuracy: 83.87%\n",
      "Batch 41, Loss: 0.956596, Accuracy: 83.77%\n",
      "Batch 42, Loss: 0.859126, Accuracy: 83.89%\n",
      "Batch 43, Loss: 0.877795, Accuracy: 83.90%\n",
      "Batch 44, Loss: 0.932462, Accuracy: 83.88%\n",
      "Batch 45, Loss: 0.939693, Accuracy: 83.82%\n",
      "Batch 46, Loss: 0.841405, Accuracy: 83.97%\n",
      "Batch 47, Loss: 0.861848, Accuracy: 84.04%\n",
      "Batch 48, Loss: 0.970219, Accuracy: 83.89%\n",
      "Batch 49, Loss: 0.831222, Accuracy: 84.06%\n",
      "Batch 50, Loss: 0.888457, Accuracy: 84.06%\n",
      "Batch 51, Loss: 0.928788, Accuracy: 84.01%\n",
      "Batch 52, Loss: 0.866181, Accuracy: 84.10%\n",
      "Batch 53, Loss: 0.882816, Accuracy: 84.17%\n",
      "Batch 54, Loss: 0.911968, Accuracy: 84.14%\n",
      "Batch 55, Loss: 0.885952, Accuracy: 84.15%\n",
      "Batch 56, Loss: 0.858535, Accuracy: 84.24%\n",
      "Batch 57, Loss: 0.888786, Accuracy: 84.27%\n",
      "Batch 58, Loss: 0.892119, Accuracy: 84.27%\n",
      "Batch 59, Loss: 0.887082, Accuracy: 84.30%\n",
      "Batch 60, Loss: 0.892157, Accuracy: 84.32%\n",
      "Batch 61, Loss: 0.887511, Accuracy: 84.35%\n",
      "Batch 62, Loss: 0.897423, Accuracy: 84.35%\n",
      "Batch 63, Loss: 0.950702, Accuracy: 84.28%\n",
      "Batch 64, Loss: 0.852207, Accuracy: 84.35%\n",
      "Batch 65, Loss: 0.900286, Accuracy: 84.35%\n",
      "Batch 66, Loss: 0.914920, Accuracy: 84.33%\n",
      "Batch 67, Loss: 0.827220, Accuracy: 84.42%\n",
      "Batch 68, Loss: 0.923196, Accuracy: 84.40%\n",
      "Batch 69, Loss: 0.922271, Accuracy: 84.38%\n",
      "Batch 70, Loss: 0.912770, Accuracy: 84.33%\n",
      "Batch 71, Loss: 0.867441, Accuracy: 84.40%\n",
      "Batch 72, Loss: 0.910619, Accuracy: 84.38%\n",
      "Batch 73, Loss: 0.915401, Accuracy: 84.38%\n",
      "Batch 74, Loss: 0.917100, Accuracy: 84.33%\n",
      "Batch 75, Loss: 0.913795, Accuracy: 84.33%\n",
      "Batch 76, Loss: 0.945195, Accuracy: 84.27%\n",
      "Batch 77, Loss: 0.932139, Accuracy: 84.21%\n",
      "Batch 78, Loss: 0.933773, Accuracy: 84.17%\n",
      "Batch 79, Loss: 0.922091, Accuracy: 84.14%\n",
      "Batch 80, Loss: 0.892965, Accuracy: 84.16%\n",
      "Batch 81, Loss: 0.923511, Accuracy: 84.14%\n",
      "Batch 82, Loss: 0.952646, Accuracy: 84.09%\n",
      "Batch 83, Loss: 0.933450, Accuracy: 84.05%\n",
      "Batch 84, Loss: 0.859091, Accuracy: 84.11%\n",
      "Batch 85, Loss: 0.903658, Accuracy: 84.12%\n",
      "Batch 86, Loss: 0.891024, Accuracy: 84.14%\n",
      "Batch 87, Loss: 1.008886, Accuracy: 84.00%\n",
      "Batch 88, Loss: 0.886669, Accuracy: 84.00%\n",
      "Batch 89, Loss: 0.904002, Accuracy: 84.01%\n",
      "Batch 90, Loss: 0.934287, Accuracy: 83.96%\n",
      "Batch 91, Loss: 0.907365, Accuracy: 83.96%\n",
      "Batch 92, Loss: 0.951926, Accuracy: 83.92%\n",
      "Batch 93, Loss: 0.834784, Accuracy: 83.99%\n",
      "Batch 94, Loss: 0.882043, Accuracy: 84.01%\n",
      "Batch 95, Loss: 0.874320, Accuracy: 84.05%\n",
      "Batch 96, Loss: 0.908477, Accuracy: 84.05%\n",
      "Batch 97, Loss: 0.900718, Accuracy: 84.04%\n",
      "Batch 98, Loss: 0.945770, Accuracy: 84.01%\n",
      "Batch 99, Loss: 0.870672, Accuracy: 84.04%\n",
      "Batch 100, Loss: 0.857184, Accuracy: 84.06%\n",
      "Batch 101, Loss: 0.831331, Accuracy: 84.14%\n",
      "Batch 102, Loss: 0.854407, Accuracy: 84.21%\n",
      "Batch 103, Loss: 0.852890, Accuracy: 84.27%\n",
      "Batch 104, Loss: 0.913119, Accuracy: 84.24%\n",
      "Batch 105, Loss: 0.905714, Accuracy: 84.24%\n",
      "Batch 106, Loss: 0.957316, Accuracy: 84.17%\n",
      "Batch 107, Loss: 0.940179, Accuracy: 84.13%\n",
      "Batch 108, Loss: 1.033809, Accuracy: 83.98%\n",
      "Batch 109, Loss: 0.911879, Accuracy: 83.96%\n",
      "Batch 110, Loss: 0.922860, Accuracy: 83.93%\n",
      "Batch 111, Loss: 0.859115, Accuracy: 83.98%\n",
      "Batch 112, Loss: 0.857996, Accuracy: 84.04%\n",
      "Batch 113, Loss: 0.913179, Accuracy: 84.02%\n",
      "Batch 114, Loss: 0.896544, Accuracy: 84.03%\n",
      "Batch 115, Loss: 0.915593, Accuracy: 84.02%\n",
      "Batch 116, Loss: 0.923799, Accuracy: 83.98%\n",
      "Batch 117, Loss: 0.915785, Accuracy: 83.96%\n",
      "Batch 118, Loss: 0.911226, Accuracy: 83.96%\n",
      "Batch 119, Loss: 0.933706, Accuracy: 83.94%\n",
      "Batch 120, Loss: 0.888581, Accuracy: 83.95%\n",
      "Batch 121, Loss: 0.872105, Accuracy: 83.99%\n",
      "Batch 122, Loss: 0.931007, Accuracy: 83.99%\n",
      "Batch 123, Loss: 0.893029, Accuracy: 83.98%\n",
      "Batch 124, Loss: 0.894758, Accuracy: 83.98%\n",
      "Batch 125, Loss: 0.988511, Accuracy: 83.90%\n",
      "Batch 126, Loss: 0.877596, Accuracy: 83.93%\n",
      "Batch 127, Loss: 0.952535, Accuracy: 83.90%\n",
      "Batch 128, Loss: 0.902193, Accuracy: 83.91%\n",
      "Batch 129, Loss: 0.889705, Accuracy: 83.93%\n",
      "Batch 130, Loss: 0.944877, Accuracy: 83.89%\n",
      "Batch 131, Loss: 0.978692, Accuracy: 83.84%\n",
      "Batch 132, Loss: 0.953157, Accuracy: 83.79%\n",
      "Batch 133, Loss: 0.928904, Accuracy: 83.78%\n",
      "Batch 134, Loss: 0.886621, Accuracy: 83.79%\n",
      "Batch 135, Loss: 0.907688, Accuracy: 83.78%\n",
      "Batch 136, Loss: 0.942614, Accuracy: 83.77%\n",
      "Batch 137, Loss: 0.922804, Accuracy: 83.75%\n",
      "Batch 138, Loss: 0.868084, Accuracy: 83.79%\n",
      "Batch 139, Loss: 0.860958, Accuracy: 83.81%\n",
      "Batch 140, Loss: 0.870928, Accuracy: 83.84%\n",
      "Batch 141, Loss: 0.963192, Accuracy: 83.79%\n",
      "Batch 142, Loss: 0.955557, Accuracy: 83.75%\n",
      "Batch 143, Loss: 0.911055, Accuracy: 83.75%\n",
      "Batch 144, Loss: 1.023732, Accuracy: 83.67%\n",
      "Batch 145, Loss: 0.848759, Accuracy: 83.72%\n",
      "Batch 146, Loss: 0.839554, Accuracy: 83.78%\n",
      "Batch 147, Loss: 0.927082, Accuracy: 83.77%\n",
      "Batch 148, Loss: 0.908188, Accuracy: 83.78%\n",
      "Batch 149, Loss: 0.881154, Accuracy: 83.80%\n",
      "Batch 150, Loss: 0.913822, Accuracy: 83.80%\n",
      "Batch 151, Loss: 0.928168, Accuracy: 83.79%\n",
      "Batch 152, Loss: 0.907516, Accuracy: 83.79%\n",
      "Batch 153, Loss: 0.941675, Accuracy: 83.77%\n",
      "Batch 154, Loss: 0.866386, Accuracy: 83.81%\n",
      "Batch 155, Loss: 0.962963, Accuracy: 83.77%\n",
      "Batch 156, Loss: 0.931680, Accuracy: 83.76%\n",
      "Batch 157, Loss: 0.965566, Accuracy: 83.72%\n",
      "Batch 158, Loss: 0.878908, Accuracy: 83.74%\n",
      "Batch 159, Loss: 1.004951, Accuracy: 83.67%\n",
      "Batch 160, Loss: 0.907827, Accuracy: 83.67%\n",
      "Batch 161, Loss: 0.884425, Accuracy: 83.68%\n",
      "Batch 162, Loss: 0.945627, Accuracy: 83.67%\n",
      "Batch 163, Loss: 0.975232, Accuracy: 83.64%\n",
      "Batch 164, Loss: 0.972330, Accuracy: 83.59%\n",
      "Batch 165, Loss: 0.887873, Accuracy: 83.60%\n",
      "Batch 166, Loss: 0.848972, Accuracy: 83.64%\n",
      "Batch 167, Loss: 0.913430, Accuracy: 83.64%\n",
      "Batch 168, Loss: 0.921681, Accuracy: 83.62%\n",
      "Batch 169, Loss: 0.933940, Accuracy: 83.60%\n",
      "Batch 170, Loss: 0.875382, Accuracy: 83.63%\n",
      "Batch 171, Loss: 0.990934, Accuracy: 83.58%\n",
      "Batch 172, Loss: 0.888011, Accuracy: 83.60%\n",
      "Batch 173, Loss: 0.856568, Accuracy: 83.63%\n",
      "Batch 174, Loss: 0.914087, Accuracy: 83.61%\n",
      "Batch 175, Loss: 0.937915, Accuracy: 83.59%\n",
      "Batch 176, Loss: 0.892061, Accuracy: 83.60%\n",
      "Batch 177, Loss: 0.848653, Accuracy: 83.63%\n",
      "Batch 178, Loss: 0.983552, Accuracy: 83.58%\n",
      "Batch 179, Loss: 0.954234, Accuracy: 83.56%\n",
      "Batch 180, Loss: 1.101955, Accuracy: 83.45%\n",
      "Batch 181, Loss: 0.872528, Accuracy: 83.48%\n",
      "Batch 182, Loss: 0.954021, Accuracy: 83.44%\n",
      "Batch 183, Loss: 0.926799, Accuracy: 83.43%\n",
      "Batch 184, Loss: 0.995040, Accuracy: 83.38%\n",
      "Batch 185, Loss: 0.928175, Accuracy: 83.37%\n",
      "Batch 186, Loss: 0.940711, Accuracy: 83.35%\n",
      "Batch 187, Loss: 0.927653, Accuracy: 83.35%\n",
      "Batch 188, Loss: 0.950968, Accuracy: 83.33%\n",
      "Batch 189, Loss: 0.961213, Accuracy: 83.30%\n",
      "Batch 190, Loss: 0.855339, Accuracy: 83.33%\n",
      "Batch 191, Loss: 0.934675, Accuracy: 83.31%\n",
      "Batch 192, Loss: 0.897715, Accuracy: 83.32%\n",
      "Batch 193, Loss: 0.940680, Accuracy: 83.31%\n",
      "Batch 194, Loss: 0.938251, Accuracy: 83.30%\n",
      "Batch 195, Loss: 0.902311, Accuracy: 83.30%\n",
      "Batch 196, Loss: 0.905117, Accuracy: 83.31%\n",
      "Batch 197, Loss: 0.969352, Accuracy: 83.27%\n",
      "Batch 198, Loss: 0.921508, Accuracy: 83.27%\n",
      "Batch 199, Loss: 0.912992, Accuracy: 83.28%\n",
      "Batch 200, Loss: 0.896359, Accuracy: 83.27%\n",
      "Batch 201, Loss: 0.910685, Accuracy: 83.27%\n",
      "Batch 202, Loss: 0.857799, Accuracy: 83.31%\n",
      "Batch 203, Loss: 0.890806, Accuracy: 83.31%\n",
      "Batch 204, Loss: 0.853493, Accuracy: 83.34%\n",
      "Batch 205, Loss: 0.923112, Accuracy: 83.32%\n",
      "Batch 206, Loss: 0.900939, Accuracy: 83.32%\n",
      "Batch 207, Loss: 0.941579, Accuracy: 83.30%\n",
      "Batch 208, Loss: 0.915192, Accuracy: 83.31%\n",
      "Batch 209, Loss: 0.875030, Accuracy: 83.34%\n",
      "Batch 210, Loss: 0.885355, Accuracy: 83.35%\n",
      "Batch 211, Loss: 0.829362, Accuracy: 83.39%\n",
      "Batch 212, Loss: 0.890409, Accuracy: 83.39%\n",
      "Batch 213, Loss: 0.908761, Accuracy: 83.40%\n",
      "Training - Epoch 9, Loss: 0.909828, Accuracy: 83.40%\n",
      "Validation Batch 1, Loss: 0.837632, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.846234, Accuracy: 91.41%\n",
      "Validation Batch 3, Loss: 0.962559, Accuracy: 86.98%\n",
      "Validation Batch 4, Loss: 0.883127, Accuracy: 86.33%\n",
      "Validation Batch 5, Loss: 0.835652, Accuracy: 86.88%\n",
      "Validation Batch 6, Loss: 0.829866, Accuracy: 87.76%\n",
      "Validation Batch 7, Loss: 0.894693, Accuracy: 87.28%\n",
      "Validation Batch 8, Loss: 0.948804, Accuracy: 86.13%\n",
      "Validation Batch 9, Loss: 0.924590, Accuracy: 85.59%\n",
      "Validation Batch 10, Loss: 0.928594, Accuracy: 85.16%\n",
      "Validation Batch 11, Loss: 0.861534, Accuracy: 85.37%\n",
      "Validation Batch 12, Loss: 0.845649, Accuracy: 85.68%\n",
      "Validation Batch 13, Loss: 0.895893, Accuracy: 85.70%\n",
      "Validation Batch 14, Loss: 0.905703, Accuracy: 85.49%\n",
      "Validation Batch 15, Loss: 0.900187, Accuracy: 85.42%\n",
      "Validation Batch 16, Loss: 0.875812, Accuracy: 85.45%\n",
      "Validation Batch 17, Loss: 0.941019, Accuracy: 85.11%\n",
      "Validation Batch 18, Loss: 0.869572, Accuracy: 85.24%\n",
      "Validation Batch 19, Loss: 0.929013, Accuracy: 85.03%\n",
      "Validation Batch 20, Loss: 0.829376, Accuracy: 85.31%\n",
      "Validation Batch 21, Loss: 0.891645, Accuracy: 85.34%\n",
      "Validation Batch 22, Loss: 0.887574, Accuracy: 85.30%\n",
      "Validation Batch 23, Loss: 0.909805, Accuracy: 85.26%\n",
      "Validation Batch 24, Loss: 0.947466, Accuracy: 85.03%\n",
      "Validation Batch 25, Loss: 0.890494, Accuracy: 84.94%\n",
      "Validation Batch 26, Loss: 0.891836, Accuracy: 84.92%\n",
      "Validation Batch 27, Loss: 0.801173, Accuracy: 85.14%\n",
      "Validation - Epoch 9, Loss: 0.887611, Accuracy: 85.14%\n",
      "Patienceâ€”12\n",
      "Epoch 10\n",
      "Batch 1, Loss: 0.893902, Accuracy: 84.38%\n",
      "Batch 2, Loss: 1.011296, Accuracy: 78.12%\n",
      "Batch 3, Loss: 0.890317, Accuracy: 80.73%\n",
      "Batch 4, Loss: 0.928725, Accuracy: 80.86%\n",
      "Batch 5, Loss: 0.959336, Accuracy: 79.69%\n",
      "Batch 6, Loss: 0.893370, Accuracy: 80.47%\n",
      "Batch 7, Loss: 0.861613, Accuracy: 81.70%\n",
      "Batch 8, Loss: 0.853640, Accuracy: 82.62%\n",
      "Batch 9, Loss: 0.876767, Accuracy: 83.16%\n",
      "Batch 10, Loss: 0.879485, Accuracy: 83.44%\n",
      "Batch 11, Loss: 0.903712, Accuracy: 83.52%\n",
      "Batch 12, Loss: 0.894356, Accuracy: 83.59%\n",
      "Batch 13, Loss: 0.867365, Accuracy: 83.89%\n",
      "Batch 14, Loss: 0.873725, Accuracy: 84.26%\n",
      "Batch 15, Loss: 0.926777, Accuracy: 84.17%\n",
      "Batch 16, Loss: 1.018450, Accuracy: 83.40%\n",
      "Batch 17, Loss: 0.852174, Accuracy: 83.73%\n",
      "Batch 18, Loss: 0.898359, Accuracy: 83.77%\n",
      "Batch 19, Loss: 0.955343, Accuracy: 83.55%\n",
      "Batch 20, Loss: 0.906793, Accuracy: 83.59%\n",
      "Batch 21, Loss: 0.849846, Accuracy: 83.85%\n",
      "Batch 22, Loss: 0.909837, Accuracy: 83.81%\n",
      "Batch 23, Loss: 0.943844, Accuracy: 83.56%\n",
      "Batch 24, Loss: 0.928984, Accuracy: 83.46%\n",
      "Batch 25, Loss: 0.834949, Accuracy: 83.81%\n",
      "Batch 26, Loss: 0.856520, Accuracy: 84.01%\n",
      "Batch 27, Loss: 0.977911, Accuracy: 83.74%\n",
      "Batch 28, Loss: 0.912958, Accuracy: 83.65%\n",
      "Batch 29, Loss: 0.891233, Accuracy: 83.73%\n",
      "Batch 30, Loss: 0.880876, Accuracy: 83.80%\n",
      "Batch 31, Loss: 0.948007, Accuracy: 83.62%\n",
      "Batch 32, Loss: 0.913554, Accuracy: 83.64%\n",
      "Batch 33, Loss: 0.952584, Accuracy: 83.57%\n",
      "Batch 34, Loss: 1.014149, Accuracy: 83.27%\n",
      "Batch 35, Loss: 0.885246, Accuracy: 83.30%\n",
      "Batch 36, Loss: 0.880895, Accuracy: 83.38%\n",
      "Batch 37, Loss: 0.952390, Accuracy: 83.23%\n",
      "Batch 38, Loss: 0.958795, Accuracy: 83.10%\n",
      "Batch 39, Loss: 0.887994, Accuracy: 83.13%\n",
      "Batch 40, Loss: 0.903239, Accuracy: 83.12%\n",
      "Batch 41, Loss: 0.924676, Accuracy: 83.12%\n",
      "Batch 42, Loss: 0.921038, Accuracy: 83.07%\n",
      "Batch 43, Loss: 0.947696, Accuracy: 82.99%\n",
      "Batch 44, Loss: 0.933588, Accuracy: 82.95%\n",
      "Batch 45, Loss: 0.914157, Accuracy: 82.95%\n",
      "Batch 46, Loss: 0.928005, Accuracy: 82.95%\n",
      "Batch 47, Loss: 0.915929, Accuracy: 82.95%\n",
      "Batch 48, Loss: 0.875477, Accuracy: 83.04%\n",
      "Batch 49, Loss: 0.875450, Accuracy: 83.13%\n",
      "Batch 50, Loss: 0.882872, Accuracy: 83.19%\n",
      "Batch 51, Loss: 0.876669, Accuracy: 83.24%\n",
      "Batch 52, Loss: 0.928552, Accuracy: 83.17%\n",
      "Batch 53, Loss: 0.873168, Accuracy: 83.25%\n",
      "Batch 54, Loss: 0.982450, Accuracy: 83.10%\n",
      "Batch 55, Loss: 0.948761, Accuracy: 83.04%\n",
      "Batch 56, Loss: 0.898663, Accuracy: 83.06%\n",
      "Batch 57, Loss: 0.843298, Accuracy: 83.20%\n",
      "Batch 58, Loss: 0.884735, Accuracy: 83.24%\n",
      "Batch 59, Loss: 0.984864, Accuracy: 83.10%\n",
      "Batch 60, Loss: 0.910012, Accuracy: 83.12%\n",
      "Batch 61, Loss: 0.935690, Accuracy: 83.07%\n",
      "Batch 62, Loss: 0.918835, Accuracy: 83.06%\n",
      "Batch 63, Loss: 0.934569, Accuracy: 83.04%\n",
      "Batch 64, Loss: 0.905953, Accuracy: 83.03%\n",
      "Batch 65, Loss: 0.895789, Accuracy: 83.08%\n",
      "Batch 66, Loss: 0.895970, Accuracy: 83.07%\n",
      "Batch 67, Loss: 0.851143, Accuracy: 83.16%\n",
      "Batch 68, Loss: 0.862583, Accuracy: 83.25%\n",
      "Batch 69, Loss: 0.905065, Accuracy: 83.27%\n",
      "Batch 70, Loss: 0.912585, Accuracy: 83.26%\n",
      "Batch 71, Loss: 0.899031, Accuracy: 83.27%\n",
      "Batch 72, Loss: 0.823787, Accuracy: 83.40%\n",
      "Batch 73, Loss: 0.968616, Accuracy: 83.33%\n",
      "Batch 74, Loss: 0.949244, Accuracy: 83.26%\n",
      "Batch 75, Loss: 1.012154, Accuracy: 83.12%\n",
      "Batch 76, Loss: 0.871183, Accuracy: 83.18%\n",
      "Batch 77, Loss: 0.919627, Accuracy: 83.16%\n",
      "Batch 78, Loss: 0.875648, Accuracy: 83.19%\n",
      "Batch 79, Loss: 0.810362, Accuracy: 83.35%\n",
      "Batch 80, Loss: 0.988939, Accuracy: 83.22%\n",
      "Batch 81, Loss: 0.881626, Accuracy: 83.26%\n",
      "Batch 82, Loss: 0.813659, Accuracy: 83.38%\n",
      "Batch 83, Loss: 0.897367, Accuracy: 83.41%\n",
      "Batch 84, Loss: 0.873766, Accuracy: 83.46%\n",
      "Batch 85, Loss: 0.914814, Accuracy: 83.44%\n",
      "Batch 86, Loss: 0.878870, Accuracy: 83.45%\n",
      "Batch 87, Loss: 0.877522, Accuracy: 83.49%\n",
      "Batch 88, Loss: 0.958110, Accuracy: 83.45%\n",
      "Batch 89, Loss: 0.878499, Accuracy: 83.48%\n",
      "Batch 90, Loss: 0.846772, Accuracy: 83.56%\n",
      "Batch 91, Loss: 0.882415, Accuracy: 83.59%\n",
      "Batch 92, Loss: 0.821641, Accuracy: 83.71%\n",
      "Batch 93, Loss: 0.886629, Accuracy: 83.74%\n",
      "Batch 94, Loss: 0.929150, Accuracy: 83.69%\n",
      "Batch 95, Loss: 0.909584, Accuracy: 83.70%\n",
      "Batch 96, Loss: 0.882141, Accuracy: 83.72%\n",
      "Batch 97, Loss: 0.909906, Accuracy: 83.71%\n",
      "Batch 98, Loss: 0.858824, Accuracy: 83.80%\n",
      "Batch 99, Loss: 0.899213, Accuracy: 83.79%\n",
      "Batch 100, Loss: 0.959162, Accuracy: 83.73%\n",
      "Batch 101, Loss: 0.907483, Accuracy: 83.74%\n",
      "Batch 102, Loss: 0.940152, Accuracy: 83.72%\n",
      "Batch 103, Loss: 0.966437, Accuracy: 83.68%\n",
      "Batch 104, Loss: 0.919087, Accuracy: 83.65%\n",
      "Batch 105, Loss: 0.923273, Accuracy: 83.65%\n",
      "Batch 106, Loss: 0.961481, Accuracy: 83.61%\n",
      "Batch 107, Loss: 0.899659, Accuracy: 83.60%\n",
      "Batch 108, Loss: 0.913796, Accuracy: 83.58%\n",
      "Batch 109, Loss: 0.864130, Accuracy: 83.62%\n",
      "Batch 110, Loss: 0.906149, Accuracy: 83.62%\n",
      "Batch 111, Loss: 1.007787, Accuracy: 83.53%\n",
      "Batch 112, Loss: 0.992418, Accuracy: 83.47%\n",
      "Batch 113, Loss: 0.911217, Accuracy: 83.46%\n",
      "Batch 114, Loss: 0.932526, Accuracy: 83.43%\n",
      "Batch 115, Loss: 0.857438, Accuracy: 83.48%\n",
      "Batch 116, Loss: 0.886230, Accuracy: 83.49%\n",
      "Batch 117, Loss: 0.890486, Accuracy: 83.51%\n",
      "Batch 118, Loss: 0.865457, Accuracy: 83.55%\n",
      "Batch 119, Loss: 0.953410, Accuracy: 83.50%\n",
      "Batch 120, Loss: 0.850626, Accuracy: 83.54%\n",
      "Batch 121, Loss: 0.861202, Accuracy: 83.57%\n",
      "Batch 122, Loss: 0.893252, Accuracy: 83.58%\n",
      "Batch 123, Loss: 0.863955, Accuracy: 83.61%\n",
      "Batch 124, Loss: 0.867304, Accuracy: 83.66%\n",
      "Batch 125, Loss: 0.914540, Accuracy: 83.65%\n",
      "Batch 126, Loss: 0.915147, Accuracy: 83.66%\n",
      "Batch 127, Loss: 0.859991, Accuracy: 83.70%\n",
      "Batch 128, Loss: 0.945629, Accuracy: 83.65%\n",
      "Batch 129, Loss: 0.911516, Accuracy: 83.66%\n",
      "Batch 130, Loss: 0.891364, Accuracy: 83.68%\n",
      "Batch 131, Loss: 0.868071, Accuracy: 83.72%\n",
      "Batch 132, Loss: 0.885095, Accuracy: 83.74%\n",
      "Batch 133, Loss: 0.877527, Accuracy: 83.74%\n",
      "Batch 134, Loss: 0.931169, Accuracy: 83.71%\n",
      "Batch 135, Loss: 0.947599, Accuracy: 83.68%\n",
      "Batch 136, Loss: 0.930128, Accuracy: 83.65%\n",
      "Batch 137, Loss: 0.935699, Accuracy: 83.63%\n",
      "Batch 138, Loss: 0.931023, Accuracy: 83.62%\n",
      "Batch 139, Loss: 0.891958, Accuracy: 83.63%\n",
      "Batch 140, Loss: 0.942302, Accuracy: 83.60%\n",
      "Batch 141, Loss: 0.862073, Accuracy: 83.64%\n",
      "Batch 142, Loss: 0.964604, Accuracy: 83.59%\n",
      "Batch 143, Loss: 0.903003, Accuracy: 83.61%\n",
      "Batch 144, Loss: 0.902421, Accuracy: 83.62%\n",
      "Batch 145, Loss: 0.930359, Accuracy: 83.61%\n",
      "Batch 146, Loss: 0.923821, Accuracy: 83.58%\n",
      "Batch 147, Loss: 0.926246, Accuracy: 83.57%\n",
      "Batch 148, Loss: 0.897072, Accuracy: 83.57%\n",
      "Batch 149, Loss: 0.891717, Accuracy: 83.58%\n",
      "Batch 150, Loss: 0.937172, Accuracy: 83.56%\n",
      "Batch 151, Loss: 0.925307, Accuracy: 83.56%\n",
      "Batch 152, Loss: 0.961213, Accuracy: 83.52%\n",
      "Batch 153, Loss: 0.906294, Accuracy: 83.52%\n",
      "Batch 154, Loss: 0.921342, Accuracy: 83.52%\n",
      "Batch 155, Loss: 0.929872, Accuracy: 83.51%\n",
      "Batch 156, Loss: 0.945096, Accuracy: 83.49%\n",
      "Batch 157, Loss: 0.935943, Accuracy: 83.48%\n",
      "Batch 158, Loss: 0.897017, Accuracy: 83.48%\n",
      "Batch 159, Loss: 0.910614, Accuracy: 83.48%\n",
      "Batch 160, Loss: 0.909768, Accuracy: 83.49%\n",
      "Batch 161, Loss: 0.855035, Accuracy: 83.53%\n",
      "Batch 162, Loss: 0.868453, Accuracy: 83.56%\n",
      "Batch 163, Loss: 0.890153, Accuracy: 83.57%\n",
      "Batch 164, Loss: 0.861012, Accuracy: 83.59%\n",
      "Batch 165, Loss: 0.927485, Accuracy: 83.58%\n",
      "Batch 166, Loss: 0.979420, Accuracy: 83.53%\n",
      "Batch 167, Loss: 0.969800, Accuracy: 83.50%\n",
      "Batch 168, Loss: 0.906234, Accuracy: 83.50%\n",
      "Batch 169, Loss: 0.871552, Accuracy: 83.52%\n",
      "Batch 170, Loss: 1.017441, Accuracy: 83.45%\n",
      "Batch 171, Loss: 0.928596, Accuracy: 83.43%\n",
      "Batch 172, Loss: 0.919560, Accuracy: 83.44%\n",
      "Batch 173, Loss: 0.925810, Accuracy: 83.43%\n",
      "Batch 174, Loss: 0.909259, Accuracy: 83.42%\n",
      "Batch 175, Loss: 0.894827, Accuracy: 83.42%\n",
      "Batch 176, Loss: 0.956433, Accuracy: 83.39%\n",
      "Batch 177, Loss: 0.874588, Accuracy: 83.41%\n",
      "Batch 178, Loss: 0.833916, Accuracy: 83.45%\n",
      "Batch 179, Loss: 0.833584, Accuracy: 83.49%\n",
      "Batch 180, Loss: 0.923395, Accuracy: 83.48%\n",
      "Batch 181, Loss: 0.863688, Accuracy: 83.51%\n",
      "Batch 182, Loss: 0.912727, Accuracy: 83.51%\n",
      "Batch 183, Loss: 0.954988, Accuracy: 83.48%\n",
      "Batch 184, Loss: 0.829363, Accuracy: 83.52%\n",
      "Batch 185, Loss: 0.847121, Accuracy: 83.56%\n",
      "Batch 186, Loss: 0.840343, Accuracy: 83.59%\n",
      "Batch 187, Loss: 0.890091, Accuracy: 83.61%\n",
      "Batch 188, Loss: 0.857594, Accuracy: 83.63%\n",
      "Batch 189, Loss: 0.898280, Accuracy: 83.63%\n",
      "Batch 190, Loss: 1.082150, Accuracy: 83.54%\n",
      "Batch 191, Loss: 0.944780, Accuracy: 83.52%\n",
      "Batch 192, Loss: 0.926863, Accuracy: 83.50%\n",
      "Batch 193, Loss: 0.865502, Accuracy: 83.52%\n",
      "Batch 194, Loss: 0.944003, Accuracy: 83.50%\n",
      "Batch 195, Loss: 0.844871, Accuracy: 83.53%\n",
      "Batch 196, Loss: 0.867318, Accuracy: 83.55%\n",
      "Batch 197, Loss: 0.840036, Accuracy: 83.58%\n",
      "Batch 198, Loss: 0.984006, Accuracy: 83.54%\n",
      "Batch 199, Loss: 0.931803, Accuracy: 83.53%\n",
      "Batch 200, Loss: 0.914942, Accuracy: 83.52%\n",
      "Batch 201, Loss: 0.921506, Accuracy: 83.51%\n",
      "Batch 202, Loss: 0.912270, Accuracy: 83.51%\n",
      "Batch 203, Loss: 0.893398, Accuracy: 83.53%\n",
      "Batch 204, Loss: 0.928184, Accuracy: 83.52%\n",
      "Batch 205, Loss: 0.925155, Accuracy: 83.51%\n",
      "Batch 206, Loss: 0.900263, Accuracy: 83.51%\n",
      "Batch 207, Loss: 0.855981, Accuracy: 83.54%\n",
      "Batch 208, Loss: 0.945425, Accuracy: 83.53%\n",
      "Batch 209, Loss: 0.894255, Accuracy: 83.53%\n",
      "Batch 210, Loss: 0.878660, Accuracy: 83.54%\n",
      "Batch 211, Loss: 0.897509, Accuracy: 83.55%\n",
      "Batch 212, Loss: 0.917791, Accuracy: 83.54%\n",
      "Batch 213, Loss: 0.891630, Accuracy: 83.54%\n",
      "Training - Epoch 10, Loss: 0.907603, Accuracy: 83.54%\n",
      "Validation Batch 1, Loss: 0.837825, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.846143, Accuracy: 91.41%\n",
      "Validation Batch 3, Loss: 0.962128, Accuracy: 86.98%\n",
      "Validation Batch 4, Loss: 0.882957, Accuracy: 86.33%\n",
      "Validation Batch 5, Loss: 0.834663, Accuracy: 86.88%\n",
      "Validation Batch 6, Loss: 0.829513, Accuracy: 87.76%\n",
      "Validation Batch 7, Loss: 0.894121, Accuracy: 87.28%\n",
      "Validation Batch 8, Loss: 0.948639, Accuracy: 86.13%\n",
      "Validation Batch 9, Loss: 0.924071, Accuracy: 85.59%\n",
      "Validation Batch 10, Loss: 0.928210, Accuracy: 85.16%\n",
      "Validation Batch 11, Loss: 0.860382, Accuracy: 85.37%\n",
      "Validation Batch 12, Loss: 0.845153, Accuracy: 85.68%\n",
      "Validation Batch 13, Loss: 0.894893, Accuracy: 85.70%\n",
      "Validation Batch 14, Loss: 0.904698, Accuracy: 85.49%\n",
      "Validation Batch 15, Loss: 0.900236, Accuracy: 85.42%\n",
      "Validation Batch 16, Loss: 0.875825, Accuracy: 85.45%\n",
      "Validation Batch 17, Loss: 0.941100, Accuracy: 85.11%\n",
      "Validation Batch 18, Loss: 0.869139, Accuracy: 85.24%\n",
      "Validation Batch 19, Loss: 0.928773, Accuracy: 85.03%\n",
      "Validation Batch 20, Loss: 0.828578, Accuracy: 85.31%\n",
      "Validation Batch 21, Loss: 0.890982, Accuracy: 85.34%\n",
      "Validation Batch 22, Loss: 0.886982, Accuracy: 85.30%\n",
      "Validation Batch 23, Loss: 0.908935, Accuracy: 85.26%\n",
      "Validation Batch 24, Loss: 0.947144, Accuracy: 85.03%\n",
      "Validation Batch 25, Loss: 0.889582, Accuracy: 84.94%\n",
      "Validation Batch 26, Loss: 0.891358, Accuracy: 84.92%\n",
      "Validation Batch 27, Loss: 0.801263, Accuracy: 85.14%\n",
      "Validation - Epoch 10, Loss: 0.887159, Accuracy: 85.14%\n",
      "Patienceâ€”13\n",
      "Epoch 11\n",
      "Batch 1, Loss: 0.937861, Accuracy: 81.25%\n",
      "Batch 2, Loss: 0.978309, Accuracy: 78.12%\n",
      "Batch 3, Loss: 0.907119, Accuracy: 79.17%\n",
      "Batch 4, Loss: 0.942416, Accuracy: 79.30%\n",
      "Batch 5, Loss: 0.857071, Accuracy: 81.25%\n",
      "Batch 6, Loss: 0.924051, Accuracy: 81.51%\n",
      "Batch 7, Loss: 0.917694, Accuracy: 81.47%\n",
      "Batch 8, Loss: 0.896829, Accuracy: 82.03%\n",
      "Batch 9, Loss: 0.951693, Accuracy: 81.60%\n",
      "Batch 10, Loss: 0.919151, Accuracy: 81.56%\n",
      "Batch 11, Loss: 0.875217, Accuracy: 82.10%\n",
      "Batch 12, Loss: 0.845380, Accuracy: 82.68%\n",
      "Batch 13, Loss: 0.887980, Accuracy: 82.93%\n",
      "Batch 14, Loss: 0.854691, Accuracy: 83.37%\n",
      "Batch 15, Loss: 0.920162, Accuracy: 83.33%\n",
      "Batch 16, Loss: 0.823253, Accuracy: 83.98%\n",
      "Batch 17, Loss: 0.956865, Accuracy: 83.73%\n",
      "Batch 18, Loss: 0.869046, Accuracy: 83.94%\n",
      "Batch 19, Loss: 0.912482, Accuracy: 83.96%\n",
      "Batch 20, Loss: 0.904835, Accuracy: 83.91%\n",
      "Batch 21, Loss: 0.863678, Accuracy: 84.08%\n",
      "Batch 22, Loss: 0.930301, Accuracy: 83.95%\n",
      "Batch 23, Loss: 0.904887, Accuracy: 83.97%\n",
      "Batch 24, Loss: 0.983200, Accuracy: 83.66%\n",
      "Batch 25, Loss: 0.966238, Accuracy: 83.38%\n",
      "Batch 26, Loss: 0.900593, Accuracy: 83.35%\n",
      "Batch 27, Loss: 0.943408, Accuracy: 83.22%\n",
      "Batch 28, Loss: 0.928563, Accuracy: 83.09%\n",
      "Batch 29, Loss: 0.974206, Accuracy: 82.92%\n",
      "Batch 30, Loss: 0.916053, Accuracy: 82.92%\n",
      "Batch 31, Loss: 0.936724, Accuracy: 82.81%\n",
      "Batch 32, Loss: 0.886705, Accuracy: 82.91%\n",
      "Batch 33, Loss: 0.998619, Accuracy: 82.62%\n",
      "Batch 34, Loss: 0.962314, Accuracy: 82.49%\n",
      "Batch 35, Loss: 0.929765, Accuracy: 82.41%\n",
      "Batch 36, Loss: 0.900618, Accuracy: 82.42%\n",
      "Batch 37, Loss: 0.867035, Accuracy: 82.60%\n",
      "Batch 38, Loss: 0.852027, Accuracy: 82.77%\n",
      "Batch 39, Loss: 0.856751, Accuracy: 82.93%\n",
      "Batch 40, Loss: 0.903417, Accuracy: 83.01%\n",
      "Batch 41, Loss: 0.927364, Accuracy: 82.96%\n",
      "Batch 42, Loss: 0.928960, Accuracy: 82.89%\n",
      "Batch 43, Loss: 0.907480, Accuracy: 82.92%\n",
      "Batch 44, Loss: 0.919617, Accuracy: 82.92%\n",
      "Batch 45, Loss: 0.876105, Accuracy: 82.99%\n",
      "Batch 46, Loss: 0.875963, Accuracy: 83.05%\n",
      "Batch 47, Loss: 0.948753, Accuracy: 82.95%\n",
      "Batch 48, Loss: 0.932208, Accuracy: 82.94%\n",
      "Batch 49, Loss: 0.956682, Accuracy: 82.84%\n",
      "Batch 50, Loss: 0.884403, Accuracy: 82.94%\n",
      "Batch 51, Loss: 0.993113, Accuracy: 82.78%\n",
      "Batch 52, Loss: 0.966738, Accuracy: 82.66%\n",
      "Batch 53, Loss: 0.890679, Accuracy: 82.67%\n",
      "Batch 54, Loss: 0.849597, Accuracy: 82.84%\n",
      "Batch 55, Loss: 0.936066, Accuracy: 82.81%\n",
      "Batch 56, Loss: 0.921457, Accuracy: 82.81%\n",
      "Batch 57, Loss: 0.997743, Accuracy: 82.68%\n",
      "Batch 58, Loss: 0.906059, Accuracy: 82.70%\n",
      "Batch 59, Loss: 0.905051, Accuracy: 82.73%\n",
      "Batch 60, Loss: 0.945408, Accuracy: 82.68%\n",
      "Batch 61, Loss: 0.976092, Accuracy: 82.61%\n",
      "Batch 62, Loss: 0.997530, Accuracy: 82.46%\n",
      "Batch 63, Loss: 0.861866, Accuracy: 82.56%\n",
      "Batch 64, Loss: 0.921872, Accuracy: 82.54%\n",
      "Batch 65, Loss: 0.868954, Accuracy: 82.62%\n",
      "Batch 66, Loss: 0.982328, Accuracy: 82.53%\n",
      "Batch 67, Loss: 0.902529, Accuracy: 82.58%\n",
      "Batch 68, Loss: 0.943357, Accuracy: 82.56%\n",
      "Batch 69, Loss: 0.939706, Accuracy: 82.50%\n",
      "Batch 70, Loss: 0.905495, Accuracy: 82.50%\n",
      "Batch 71, Loss: 0.868450, Accuracy: 82.57%\n",
      "Batch 72, Loss: 0.897318, Accuracy: 82.62%\n",
      "Batch 73, Loss: 0.873325, Accuracy: 82.66%\n",
      "Batch 74, Loss: 0.926918, Accuracy: 82.66%\n",
      "Batch 75, Loss: 0.866857, Accuracy: 82.75%\n",
      "Batch 76, Loss: 0.877470, Accuracy: 82.79%\n",
      "Batch 77, Loss: 0.858251, Accuracy: 82.87%\n",
      "Batch 78, Loss: 0.958804, Accuracy: 82.79%\n",
      "Batch 79, Loss: 0.922826, Accuracy: 82.79%\n",
      "Batch 80, Loss: 0.851928, Accuracy: 82.87%\n",
      "Batch 81, Loss: 0.914385, Accuracy: 82.87%\n",
      "Batch 82, Loss: 0.941398, Accuracy: 82.83%\n",
      "Batch 83, Loss: 0.874042, Accuracy: 82.91%\n",
      "Batch 84, Loss: 0.859622, Accuracy: 82.98%\n",
      "Batch 85, Loss: 0.919479, Accuracy: 82.96%\n",
      "Batch 86, Loss: 0.907733, Accuracy: 82.98%\n",
      "Batch 87, Loss: 0.919223, Accuracy: 82.96%\n",
      "Batch 88, Loss: 0.873880, Accuracy: 82.99%\n",
      "Batch 89, Loss: 0.856518, Accuracy: 83.06%\n",
      "Batch 90, Loss: 0.894987, Accuracy: 83.07%\n",
      "Batch 91, Loss: 0.955043, Accuracy: 83.02%\n",
      "Batch 92, Loss: 0.920677, Accuracy: 83.02%\n",
      "Batch 93, Loss: 0.897523, Accuracy: 83.03%\n",
      "Batch 94, Loss: 0.897344, Accuracy: 83.05%\n",
      "Batch 95, Loss: 0.904104, Accuracy: 83.04%\n",
      "Batch 96, Loss: 0.978386, Accuracy: 82.98%\n",
      "Batch 97, Loss: 0.881746, Accuracy: 83.02%\n",
      "Batch 98, Loss: 0.892047, Accuracy: 83.04%\n",
      "Batch 99, Loss: 0.866712, Accuracy: 83.08%\n",
      "Batch 100, Loss: 0.881513, Accuracy: 83.11%\n",
      "Batch 101, Loss: 0.823069, Accuracy: 83.20%\n",
      "Batch 102, Loss: 0.915244, Accuracy: 83.20%\n",
      "Batch 103, Loss: 0.919837, Accuracy: 83.18%\n",
      "Batch 104, Loss: 0.883725, Accuracy: 83.20%\n",
      "Batch 105, Loss: 0.907206, Accuracy: 83.20%\n",
      "Batch 106, Loss: 0.878196, Accuracy: 83.23%\n",
      "Batch 107, Loss: 0.940181, Accuracy: 83.21%\n",
      "Batch 108, Loss: 0.930440, Accuracy: 83.19%\n",
      "Batch 109, Loss: 0.913884, Accuracy: 83.19%\n",
      "Batch 110, Loss: 0.934126, Accuracy: 83.17%\n",
      "Batch 111, Loss: 0.867603, Accuracy: 83.21%\n",
      "Batch 112, Loss: 0.962452, Accuracy: 83.15%\n",
      "Batch 113, Loss: 0.938554, Accuracy: 83.13%\n",
      "Batch 114, Loss: 0.851062, Accuracy: 83.20%\n",
      "Batch 115, Loss: 0.926518, Accuracy: 83.19%\n",
      "Batch 116, Loss: 0.899408, Accuracy: 83.19%\n",
      "Batch 117, Loss: 0.862508, Accuracy: 83.24%\n",
      "Batch 118, Loss: 0.868645, Accuracy: 83.26%\n",
      "Batch 119, Loss: 0.903571, Accuracy: 83.27%\n",
      "Batch 120, Loss: 0.961631, Accuracy: 83.22%\n",
      "Batch 121, Loss: 0.939046, Accuracy: 83.19%\n",
      "Batch 122, Loss: 0.934549, Accuracy: 83.15%\n",
      "Batch 123, Loss: 0.950849, Accuracy: 83.12%\n",
      "Batch 124, Loss: 0.977815, Accuracy: 83.06%\n",
      "Batch 125, Loss: 0.888279, Accuracy: 83.09%\n",
      "Batch 126, Loss: 0.904349, Accuracy: 83.10%\n",
      "Batch 127, Loss: 0.889229, Accuracy: 83.11%\n",
      "Batch 128, Loss: 0.845640, Accuracy: 83.17%\n",
      "Batch 129, Loss: 0.933421, Accuracy: 83.15%\n",
      "Batch 130, Loss: 0.924883, Accuracy: 83.12%\n",
      "Batch 131, Loss: 0.988919, Accuracy: 83.05%\n",
      "Batch 132, Loss: 0.842297, Accuracy: 83.11%\n",
      "Batch 133, Loss: 0.969669, Accuracy: 83.06%\n",
      "Batch 134, Loss: 0.840105, Accuracy: 83.12%\n",
      "Batch 135, Loss: 0.935905, Accuracy: 83.10%\n",
      "Batch 136, Loss: 0.913078, Accuracy: 83.10%\n",
      "Batch 137, Loss: 0.896771, Accuracy: 83.12%\n",
      "Batch 138, Loss: 0.868857, Accuracy: 83.16%\n",
      "Batch 139, Loss: 0.884229, Accuracy: 83.19%\n",
      "Batch 140, Loss: 0.861434, Accuracy: 83.24%\n",
      "Batch 141, Loss: 0.945480, Accuracy: 83.21%\n",
      "Batch 142, Loss: 0.938362, Accuracy: 83.18%\n",
      "Batch 143, Loss: 0.867473, Accuracy: 83.22%\n",
      "Batch 144, Loss: 0.879657, Accuracy: 83.24%\n",
      "Batch 145, Loss: 0.827935, Accuracy: 83.30%\n",
      "Batch 146, Loss: 0.873595, Accuracy: 83.34%\n",
      "Batch 147, Loss: 0.904432, Accuracy: 83.34%\n",
      "Batch 148, Loss: 0.874742, Accuracy: 83.37%\n",
      "Batch 149, Loss: 0.923591, Accuracy: 83.36%\n",
      "Batch 150, Loss: 0.867203, Accuracy: 83.39%\n",
      "Batch 151, Loss: 0.906986, Accuracy: 83.39%\n",
      "Batch 152, Loss: 0.875336, Accuracy: 83.42%\n",
      "Batch 153, Loss: 0.797034, Accuracy: 83.50%\n",
      "Batch 154, Loss: 0.889334, Accuracy: 83.52%\n",
      "Batch 155, Loss: 0.852025, Accuracy: 83.56%\n",
      "Batch 156, Loss: 0.914155, Accuracy: 83.55%\n",
      "Batch 157, Loss: 0.902679, Accuracy: 83.56%\n",
      "Batch 158, Loss: 0.877320, Accuracy: 83.57%\n",
      "Batch 159, Loss: 0.966708, Accuracy: 83.54%\n",
      "Batch 160, Loss: 0.916178, Accuracy: 83.54%\n",
      "Batch 161, Loss: 0.869420, Accuracy: 83.56%\n",
      "Batch 162, Loss: 0.962691, Accuracy: 83.52%\n",
      "Batch 163, Loss: 0.819292, Accuracy: 83.57%\n",
      "Batch 164, Loss: 0.822438, Accuracy: 83.63%\n",
      "Batch 165, Loss: 0.929488, Accuracy: 83.62%\n",
      "Batch 166, Loss: 0.926215, Accuracy: 83.61%\n",
      "Batch 167, Loss: 0.973151, Accuracy: 83.56%\n",
      "Batch 168, Loss: 0.951826, Accuracy: 83.53%\n",
      "Batch 169, Loss: 0.874592, Accuracy: 83.55%\n",
      "Batch 170, Loss: 1.033180, Accuracy: 83.47%\n",
      "Batch 171, Loss: 0.958240, Accuracy: 83.43%\n",
      "Batch 172, Loss: 0.979316, Accuracy: 83.41%\n",
      "Batch 173, Loss: 0.937909, Accuracy: 83.39%\n",
      "Batch 174, Loss: 0.886048, Accuracy: 83.40%\n",
      "Batch 175, Loss: 0.897063, Accuracy: 83.41%\n",
      "Batch 176, Loss: 0.928826, Accuracy: 83.41%\n",
      "Batch 177, Loss: 0.929647, Accuracy: 83.39%\n",
      "Batch 178, Loss: 0.908833, Accuracy: 83.39%\n",
      "Batch 179, Loss: 0.881565, Accuracy: 83.41%\n",
      "Batch 180, Loss: 0.876892, Accuracy: 83.43%\n",
      "Batch 181, Loss: 0.901677, Accuracy: 83.43%\n",
      "Batch 182, Loss: 0.894593, Accuracy: 83.43%\n",
      "Batch 183, Loss: 0.858291, Accuracy: 83.46%\n",
      "Batch 184, Loss: 0.900189, Accuracy: 83.46%\n",
      "Batch 185, Loss: 0.928878, Accuracy: 83.45%\n",
      "Batch 186, Loss: 0.892797, Accuracy: 83.46%\n",
      "Batch 187, Loss: 0.864423, Accuracy: 83.48%\n",
      "Batch 188, Loss: 0.923508, Accuracy: 83.46%\n",
      "Batch 189, Loss: 0.856439, Accuracy: 83.48%\n",
      "Batch 190, Loss: 0.940458, Accuracy: 83.47%\n",
      "Batch 191, Loss: 0.845582, Accuracy: 83.49%\n",
      "Batch 192, Loss: 0.962476, Accuracy: 83.47%\n",
      "Batch 193, Loss: 0.885003, Accuracy: 83.49%\n",
      "Batch 194, Loss: 0.900639, Accuracy: 83.49%\n",
      "Batch 195, Loss: 0.856747, Accuracy: 83.52%\n",
      "Batch 196, Loss: 0.996618, Accuracy: 83.47%\n",
      "Batch 197, Loss: 0.907577, Accuracy: 83.48%\n",
      "Batch 198, Loss: 1.000838, Accuracy: 83.44%\n",
      "Batch 199, Loss: 0.869950, Accuracy: 83.46%\n",
      "Batch 200, Loss: 0.922099, Accuracy: 83.45%\n",
      "Batch 201, Loss: 0.996844, Accuracy: 83.40%\n",
      "Batch 202, Loss: 0.865602, Accuracy: 83.44%\n",
      "Batch 203, Loss: 0.909530, Accuracy: 83.45%\n",
      "Batch 204, Loss: 0.904017, Accuracy: 83.47%\n",
      "Batch 205, Loss: 0.876786, Accuracy: 83.49%\n",
      "Batch 206, Loss: 0.898301, Accuracy: 83.50%\n",
      "Batch 207, Loss: 0.881924, Accuracy: 83.51%\n",
      "Batch 208, Loss: 0.887159, Accuracy: 83.52%\n",
      "Batch 209, Loss: 0.896644, Accuracy: 83.52%\n",
      "Batch 210, Loss: 0.877286, Accuracy: 83.54%\n",
      "Batch 211, Loss: 0.906688, Accuracy: 83.53%\n",
      "Batch 212, Loss: 0.871118, Accuracy: 83.54%\n",
      "Batch 213, Loss: 0.904033, Accuracy: 83.55%\n",
      "Training - Epoch 11, Loss: 0.907998, Accuracy: 83.55%\n",
      "Validation Batch 1, Loss: 0.838300, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.846300, Accuracy: 91.41%\n",
      "Validation Batch 3, Loss: 0.962196, Accuracy: 86.98%\n",
      "Validation Batch 4, Loss: 0.883162, Accuracy: 86.33%\n",
      "Validation Batch 5, Loss: 0.835150, Accuracy: 86.88%\n",
      "Validation Batch 6, Loss: 0.830159, Accuracy: 87.76%\n",
      "Validation Batch 7, Loss: 0.895084, Accuracy: 87.28%\n",
      "Validation Batch 8, Loss: 0.948574, Accuracy: 86.13%\n",
      "Validation Batch 9, Loss: 0.924655, Accuracy: 85.59%\n",
      "Validation Batch 10, Loss: 0.928520, Accuracy: 85.16%\n",
      "Validation Batch 11, Loss: 0.861145, Accuracy: 85.37%\n",
      "Validation Batch 12, Loss: 0.844922, Accuracy: 85.68%\n",
      "Validation Batch 13, Loss: 0.895452, Accuracy: 85.70%\n",
      "Validation Batch 14, Loss: 0.904812, Accuracy: 85.49%\n",
      "Validation Batch 15, Loss: 0.900295, Accuracy: 85.42%\n",
      "Validation Batch 16, Loss: 0.875631, Accuracy: 85.45%\n",
      "Validation Batch 17, Loss: 0.941178, Accuracy: 85.11%\n",
      "Validation Batch 18, Loss: 0.869333, Accuracy: 85.24%\n",
      "Validation Batch 19, Loss: 0.928841, Accuracy: 85.03%\n",
      "Validation Batch 20, Loss: 0.828598, Accuracy: 85.31%\n",
      "Validation Batch 21, Loss: 0.891283, Accuracy: 85.34%\n",
      "Validation Batch 22, Loss: 0.887334, Accuracy: 85.30%\n",
      "Validation Batch 23, Loss: 0.909762, Accuracy: 85.26%\n",
      "Validation Batch 24, Loss: 0.946995, Accuracy: 85.03%\n",
      "Validation Batch 25, Loss: 0.890278, Accuracy: 84.94%\n",
      "Validation Batch 26, Loss: 0.891801, Accuracy: 84.92%\n",
      "Validation Batch 27, Loss: 0.802136, Accuracy: 85.14%\n",
      "Validation - Epoch 11, Loss: 0.887478, Accuracy: 85.14%\n",
      "Patienceâ€”14\n",
      "Epoch 12\n",
      "Batch 1, Loss: 0.864395, Accuracy: 90.62%\n",
      "Batch 2, Loss: 0.871835, Accuracy: 89.06%\n",
      "Batch 3, Loss: 0.909204, Accuracy: 87.50%\n",
      "Batch 4, Loss: 0.925925, Accuracy: 86.33%\n",
      "Batch 5, Loss: 0.889670, Accuracy: 86.25%\n",
      "Batch 6, Loss: 0.826446, Accuracy: 87.24%\n",
      "Batch 7, Loss: 0.868141, Accuracy: 87.28%\n",
      "Batch 8, Loss: 0.886386, Accuracy: 86.91%\n",
      "Batch 9, Loss: 0.910278, Accuracy: 86.63%\n",
      "Batch 10, Loss: 0.895329, Accuracy: 86.41%\n",
      "Batch 11, Loss: 0.937058, Accuracy: 85.94%\n",
      "Batch 12, Loss: 0.885636, Accuracy: 85.94%\n",
      "Batch 13, Loss: 0.946107, Accuracy: 85.46%\n",
      "Batch 14, Loss: 0.887052, Accuracy: 85.38%\n",
      "Batch 15, Loss: 0.897283, Accuracy: 85.21%\n",
      "Batch 16, Loss: 0.927065, Accuracy: 85.16%\n",
      "Batch 17, Loss: 0.922097, Accuracy: 85.02%\n",
      "Batch 18, Loss: 0.950799, Accuracy: 84.81%\n",
      "Batch 19, Loss: 0.975484, Accuracy: 84.38%\n",
      "Batch 20, Loss: 0.878627, Accuracy: 84.53%\n",
      "Batch 21, Loss: 0.952603, Accuracy: 84.30%\n",
      "Batch 22, Loss: 0.917479, Accuracy: 84.23%\n",
      "Batch 23, Loss: 0.871045, Accuracy: 84.38%\n",
      "Batch 24, Loss: 0.952015, Accuracy: 84.05%\n",
      "Batch 25, Loss: 0.885759, Accuracy: 84.12%\n",
      "Batch 26, Loss: 0.911674, Accuracy: 84.01%\n",
      "Batch 27, Loss: 0.957026, Accuracy: 83.85%\n",
      "Batch 28, Loss: 0.964726, Accuracy: 83.71%\n",
      "Batch 29, Loss: 0.933960, Accuracy: 83.62%\n",
      "Batch 30, Loss: 0.979214, Accuracy: 83.33%\n",
      "Batch 31, Loss: 0.995345, Accuracy: 83.06%\n",
      "Batch 32, Loss: 0.850934, Accuracy: 83.30%\n",
      "Batch 33, Loss: 1.026368, Accuracy: 82.95%\n",
      "Batch 34, Loss: 0.921581, Accuracy: 82.90%\n",
      "Batch 35, Loss: 0.848305, Accuracy: 83.12%\n",
      "Batch 36, Loss: 0.966800, Accuracy: 82.94%\n",
      "Batch 37, Loss: 0.944146, Accuracy: 82.85%\n",
      "Batch 38, Loss: 0.964422, Accuracy: 82.69%\n",
      "Batch 39, Loss: 0.920893, Accuracy: 82.65%\n",
      "Batch 40, Loss: 0.906790, Accuracy: 82.66%\n",
      "Batch 41, Loss: 0.865034, Accuracy: 82.77%\n",
      "Batch 42, Loss: 0.876031, Accuracy: 82.89%\n",
      "Batch 43, Loss: 0.920161, Accuracy: 82.92%\n",
      "Batch 44, Loss: 0.898784, Accuracy: 82.95%\n",
      "Batch 45, Loss: 0.846258, Accuracy: 83.12%\n",
      "Batch 46, Loss: 0.850413, Accuracy: 83.22%\n",
      "Batch 47, Loss: 0.933003, Accuracy: 83.18%\n",
      "Batch 48, Loss: 0.931982, Accuracy: 83.11%\n",
      "Batch 49, Loss: 0.851523, Accuracy: 83.23%\n",
      "Batch 50, Loss: 0.888354, Accuracy: 83.28%\n",
      "Batch 51, Loss: 0.920188, Accuracy: 83.27%\n",
      "Batch 52, Loss: 0.897584, Accuracy: 83.29%\n",
      "Batch 53, Loss: 0.909978, Accuracy: 83.34%\n",
      "Batch 54, Loss: 0.886717, Accuracy: 83.39%\n",
      "Batch 55, Loss: 0.882125, Accuracy: 83.47%\n",
      "Batch 56, Loss: 0.864606, Accuracy: 83.54%\n",
      "Batch 57, Loss: 0.906731, Accuracy: 83.53%\n",
      "Batch 58, Loss: 0.927896, Accuracy: 83.49%\n",
      "Batch 59, Loss: 0.905679, Accuracy: 83.53%\n",
      "Batch 60, Loss: 0.884718, Accuracy: 83.57%\n",
      "Batch 61, Loss: 0.950314, Accuracy: 83.48%\n",
      "Batch 62, Loss: 0.857693, Accuracy: 83.54%\n",
      "Batch 63, Loss: 0.943767, Accuracy: 83.51%\n",
      "Batch 64, Loss: 0.867158, Accuracy: 83.59%\n",
      "Batch 65, Loss: 0.862378, Accuracy: 83.65%\n",
      "Batch 66, Loss: 0.952783, Accuracy: 83.59%\n",
      "Batch 67, Loss: 0.921325, Accuracy: 83.56%\n",
      "Batch 68, Loss: 0.887260, Accuracy: 83.59%\n",
      "Batch 69, Loss: 0.871476, Accuracy: 83.63%\n",
      "Batch 70, Loss: 0.975981, Accuracy: 83.53%\n",
      "Batch 71, Loss: 0.882054, Accuracy: 83.56%\n",
      "Batch 72, Loss: 0.921310, Accuracy: 83.57%\n",
      "Batch 73, Loss: 0.931410, Accuracy: 83.54%\n",
      "Batch 74, Loss: 0.887374, Accuracy: 83.59%\n",
      "Batch 75, Loss: 0.877428, Accuracy: 83.65%\n",
      "Batch 76, Loss: 0.951608, Accuracy: 83.61%\n",
      "Batch 77, Loss: 0.845576, Accuracy: 83.71%\n",
      "Batch 78, Loss: 0.891871, Accuracy: 83.73%\n",
      "Batch 79, Loss: 0.897332, Accuracy: 83.74%\n",
      "Batch 80, Loss: 0.864269, Accuracy: 83.81%\n",
      "Batch 81, Loss: 0.867491, Accuracy: 83.85%\n",
      "Batch 82, Loss: 0.899247, Accuracy: 83.84%\n",
      "Batch 83, Loss: 0.855344, Accuracy: 83.90%\n",
      "Batch 84, Loss: 0.911705, Accuracy: 83.91%\n",
      "Batch 85, Loss: 0.914209, Accuracy: 83.86%\n",
      "Batch 86, Loss: 0.929345, Accuracy: 83.83%\n",
      "Batch 87, Loss: 0.959668, Accuracy: 83.76%\n",
      "Batch 88, Loss: 0.979086, Accuracy: 83.70%\n",
      "Batch 89, Loss: 0.942961, Accuracy: 83.66%\n",
      "Batch 90, Loss: 0.952413, Accuracy: 83.61%\n",
      "Batch 91, Loss: 0.841957, Accuracy: 83.69%\n",
      "Batch 92, Loss: 0.874564, Accuracy: 83.75%\n",
      "Batch 93, Loss: 0.959604, Accuracy: 83.69%\n",
      "Batch 94, Loss: 0.834300, Accuracy: 83.76%\n",
      "Batch 95, Loss: 0.921859, Accuracy: 83.75%\n",
      "Batch 96, Loss: 0.994272, Accuracy: 83.66%\n",
      "Batch 97, Loss: 0.913479, Accuracy: 83.67%\n",
      "Batch 98, Loss: 0.867982, Accuracy: 83.71%\n",
      "Batch 99, Loss: 0.857192, Accuracy: 83.76%\n",
      "Batch 100, Loss: 0.925985, Accuracy: 83.73%\n",
      "Batch 101, Loss: 0.915439, Accuracy: 83.74%\n",
      "Batch 102, Loss: 0.899747, Accuracy: 83.72%\n",
      "Batch 103, Loss: 0.907281, Accuracy: 83.72%\n",
      "Batch 104, Loss: 0.934666, Accuracy: 83.70%\n",
      "Batch 105, Loss: 0.923974, Accuracy: 83.69%\n",
      "Batch 106, Loss: 0.952741, Accuracy: 83.64%\n",
      "Batch 107, Loss: 0.851589, Accuracy: 83.70%\n",
      "Batch 108, Loss: 0.929083, Accuracy: 83.68%\n",
      "Batch 109, Loss: 0.878701, Accuracy: 83.72%\n",
      "Batch 110, Loss: 0.874315, Accuracy: 83.74%\n",
      "Batch 111, Loss: 0.905711, Accuracy: 83.73%\n",
      "Batch 112, Loss: 0.949856, Accuracy: 83.71%\n",
      "Batch 113, Loss: 1.000451, Accuracy: 83.63%\n",
      "Batch 114, Loss: 0.886992, Accuracy: 83.65%\n",
      "Batch 115, Loss: 0.916803, Accuracy: 83.64%\n",
      "Batch 116, Loss: 0.864080, Accuracy: 83.67%\n",
      "Batch 117, Loss: 0.977544, Accuracy: 83.60%\n",
      "Batch 118, Loss: 0.895230, Accuracy: 83.61%\n",
      "Batch 119, Loss: 0.807003, Accuracy: 83.69%\n",
      "Batch 120, Loss: 0.933973, Accuracy: 83.66%\n",
      "Batch 121, Loss: 0.960454, Accuracy: 83.61%\n",
      "Batch 122, Loss: 0.938275, Accuracy: 83.58%\n",
      "Batch 123, Loss: 0.904418, Accuracy: 83.59%\n",
      "Batch 124, Loss: 0.892518, Accuracy: 83.61%\n",
      "Batch 125, Loss: 0.939291, Accuracy: 83.58%\n",
      "Batch 126, Loss: 0.913320, Accuracy: 83.57%\n",
      "Batch 127, Loss: 0.840609, Accuracy: 83.62%\n",
      "Batch 128, Loss: 0.885892, Accuracy: 83.64%\n",
      "Batch 129, Loss: 0.921519, Accuracy: 83.62%\n",
      "Batch 130, Loss: 0.922298, Accuracy: 83.62%\n",
      "Batch 131, Loss: 0.925001, Accuracy: 83.60%\n",
      "Batch 132, Loss: 0.858127, Accuracy: 83.63%\n",
      "Batch 133, Loss: 0.915006, Accuracy: 83.61%\n",
      "Batch 134, Loss: 0.861877, Accuracy: 83.66%\n",
      "Batch 135, Loss: 0.905685, Accuracy: 83.66%\n",
      "Batch 136, Loss: 0.969927, Accuracy: 83.62%\n",
      "Batch 137, Loss: 0.837153, Accuracy: 83.68%\n",
      "Batch 138, Loss: 0.938752, Accuracy: 83.65%\n",
      "Batch 139, Loss: 0.854054, Accuracy: 83.69%\n",
      "Batch 140, Loss: 0.917327, Accuracy: 83.69%\n",
      "Batch 141, Loss: 0.851839, Accuracy: 83.72%\n",
      "Batch 142, Loss: 1.000264, Accuracy: 83.65%\n",
      "Batch 143, Loss: 0.881069, Accuracy: 83.66%\n",
      "Batch 144, Loss: 0.919795, Accuracy: 83.65%\n",
      "Batch 145, Loss: 0.896227, Accuracy: 83.65%\n",
      "Batch 146, Loss: 0.891810, Accuracy: 83.66%\n",
      "Batch 147, Loss: 0.901675, Accuracy: 83.65%\n",
      "Batch 148, Loss: 0.857091, Accuracy: 83.69%\n",
      "Batch 149, Loss: 0.852075, Accuracy: 83.72%\n",
      "Batch 150, Loss: 0.940026, Accuracy: 83.69%\n",
      "Batch 151, Loss: 0.907006, Accuracy: 83.69%\n",
      "Batch 152, Loss: 0.854911, Accuracy: 83.73%\n",
      "Batch 153, Loss: 0.888985, Accuracy: 83.74%\n",
      "Batch 154, Loss: 0.924909, Accuracy: 83.73%\n",
      "Batch 155, Loss: 0.966082, Accuracy: 83.68%\n",
      "Batch 156, Loss: 0.852462, Accuracy: 83.71%\n",
      "Batch 157, Loss: 0.880017, Accuracy: 83.73%\n",
      "Batch 158, Loss: 0.879608, Accuracy: 83.75%\n",
      "Batch 159, Loss: 0.964833, Accuracy: 83.71%\n",
      "Batch 160, Loss: 0.959266, Accuracy: 83.66%\n",
      "Batch 161, Loss: 0.861443, Accuracy: 83.71%\n",
      "Batch 162, Loss: 0.872914, Accuracy: 83.72%\n",
      "Batch 163, Loss: 0.877897, Accuracy: 83.73%\n",
      "Batch 164, Loss: 0.933248, Accuracy: 83.72%\n",
      "Batch 165, Loss: 0.891223, Accuracy: 83.73%\n",
      "Batch 166, Loss: 1.002885, Accuracy: 83.68%\n",
      "Batch 167, Loss: 0.915072, Accuracy: 83.67%\n",
      "Batch 168, Loss: 0.920777, Accuracy: 83.67%\n",
      "Batch 169, Loss: 0.968092, Accuracy: 83.63%\n",
      "Batch 170, Loss: 0.948481, Accuracy: 83.59%\n",
      "Batch 171, Loss: 0.860786, Accuracy: 83.62%\n",
      "Batch 172, Loss: 0.895585, Accuracy: 83.62%\n",
      "Batch 173, Loss: 0.899991, Accuracy: 83.63%\n",
      "Batch 174, Loss: 0.896774, Accuracy: 83.63%\n",
      "Batch 175, Loss: 0.878413, Accuracy: 83.65%\n",
      "Batch 176, Loss: 0.876517, Accuracy: 83.67%\n",
      "Batch 177, Loss: 0.919719, Accuracy: 83.66%\n",
      "Batch 178, Loss: 0.849454, Accuracy: 83.69%\n",
      "Batch 179, Loss: 0.841082, Accuracy: 83.73%\n",
      "Batch 180, Loss: 0.956226, Accuracy: 83.70%\n",
      "Batch 181, Loss: 0.882505, Accuracy: 83.72%\n",
      "Batch 182, Loss: 0.865518, Accuracy: 83.74%\n",
      "Batch 183, Loss: 0.911442, Accuracy: 83.73%\n",
      "Batch 184, Loss: 0.885509, Accuracy: 83.74%\n",
      "Batch 185, Loss: 0.909485, Accuracy: 83.73%\n",
      "Batch 186, Loss: 0.839938, Accuracy: 83.78%\n",
      "Batch 187, Loss: 0.891731, Accuracy: 83.78%\n",
      "Batch 188, Loss: 0.881714, Accuracy: 83.79%\n",
      "Batch 189, Loss: 0.916816, Accuracy: 83.79%\n",
      "Batch 190, Loss: 0.877858, Accuracy: 83.80%\n",
      "Batch 191, Loss: 0.923794, Accuracy: 83.79%\n",
      "Batch 192, Loss: 0.974494, Accuracy: 83.76%\n",
      "Batch 193, Loss: 0.928735, Accuracy: 83.74%\n",
      "Batch 194, Loss: 0.867461, Accuracy: 83.77%\n",
      "Batch 195, Loss: 0.890126, Accuracy: 83.78%\n",
      "Batch 196, Loss: 0.845778, Accuracy: 83.82%\n",
      "Batch 197, Loss: 0.907488, Accuracy: 83.82%\n",
      "Batch 198, Loss: 0.873976, Accuracy: 83.84%\n",
      "Batch 199, Loss: 0.898550, Accuracy: 83.83%\n",
      "Batch 200, Loss: 0.885844, Accuracy: 83.84%\n",
      "Batch 201, Loss: 1.003633, Accuracy: 83.79%\n",
      "Batch 202, Loss: 1.013851, Accuracy: 83.74%\n",
      "Batch 203, Loss: 0.939707, Accuracy: 83.73%\n",
      "Batch 204, Loss: 0.846823, Accuracy: 83.77%\n",
      "Batch 205, Loss: 0.967596, Accuracy: 83.73%\n",
      "Batch 206, Loss: 0.941481, Accuracy: 83.72%\n",
      "Batch 207, Loss: 0.875438, Accuracy: 83.73%\n",
      "Batch 208, Loss: 0.834424, Accuracy: 83.76%\n",
      "Batch 209, Loss: 0.951543, Accuracy: 83.74%\n",
      "Batch 210, Loss: 0.909890, Accuracy: 83.74%\n",
      "Batch 211, Loss: 0.918202, Accuracy: 83.73%\n",
      "Batch 212, Loss: 0.994306, Accuracy: 83.69%\n",
      "Batch 213, Loss: 0.815557, Accuracy: 83.73%\n",
      "Training - Epoch 12, Loss: 0.906719, Accuracy: 83.73%\n",
      "Validation Batch 1, Loss: 0.838216, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.847391, Accuracy: 90.62%\n",
      "Validation Batch 3, Loss: 0.962609, Accuracy: 86.46%\n",
      "Validation Batch 4, Loss: 0.884413, Accuracy: 85.94%\n",
      "Validation Batch 5, Loss: 0.836055, Accuracy: 86.56%\n",
      "Validation Batch 6, Loss: 0.830293, Accuracy: 87.24%\n",
      "Validation Batch 7, Loss: 0.895786, Accuracy: 86.83%\n",
      "Validation Batch 8, Loss: 0.948766, Accuracy: 85.74%\n",
      "Validation Batch 9, Loss: 0.924589, Accuracy: 85.07%\n",
      "Validation Batch 10, Loss: 0.929116, Accuracy: 84.69%\n",
      "Validation Batch 11, Loss: 0.862837, Accuracy: 84.94%\n",
      "Validation Batch 12, Loss: 0.845132, Accuracy: 85.29%\n",
      "Validation Batch 13, Loss: 0.895835, Accuracy: 85.34%\n",
      "Validation Batch 14, Loss: 0.905069, Accuracy: 85.16%\n",
      "Validation Batch 15, Loss: 0.900650, Accuracy: 85.10%\n",
      "Validation Batch 16, Loss: 0.876073, Accuracy: 85.16%\n",
      "Validation Batch 17, Loss: 0.940710, Accuracy: 84.83%\n",
      "Validation Batch 18, Loss: 0.869430, Accuracy: 84.98%\n",
      "Validation Batch 19, Loss: 0.929154, Accuracy: 84.79%\n",
      "Validation Batch 20, Loss: 0.829436, Accuracy: 85.08%\n",
      "Validation Batch 21, Loss: 0.891915, Accuracy: 85.12%\n",
      "Validation Batch 22, Loss: 0.887650, Accuracy: 85.09%\n",
      "Validation Batch 23, Loss: 0.909154, Accuracy: 85.05%\n",
      "Validation Batch 24, Loss: 0.947075, Accuracy: 84.83%\n",
      "Validation Batch 25, Loss: 0.891094, Accuracy: 84.75%\n",
      "Validation Batch 26, Loss: 0.892945, Accuracy: 84.74%\n",
      "Validation Batch 27, Loss: 0.802715, Accuracy: 84.97%\n",
      "Validation - Epoch 12, Loss: 0.887930, Accuracy: 84.97%\n",
      "Patienceâ€”15\n",
      "Epoch 13\n",
      "Batch 1, Loss: 0.926342, Accuracy: 82.81%\n",
      "Batch 2, Loss: 0.884084, Accuracy: 83.59%\n",
      "Batch 3, Loss: 0.906395, Accuracy: 83.33%\n",
      "Batch 4, Loss: 0.909035, Accuracy: 83.20%\n",
      "Batch 5, Loss: 0.924451, Accuracy: 82.81%\n",
      "Batch 6, Loss: 0.912973, Accuracy: 83.07%\n",
      "Batch 7, Loss: 0.821490, Accuracy: 84.15%\n",
      "Batch 8, Loss: 0.971193, Accuracy: 83.40%\n",
      "Batch 9, Loss: 0.837354, Accuracy: 84.38%\n",
      "Batch 10, Loss: 1.020362, Accuracy: 82.97%\n",
      "Batch 11, Loss: 0.923769, Accuracy: 82.81%\n",
      "Batch 12, Loss: 0.925500, Accuracy: 82.68%\n",
      "Batch 13, Loss: 0.915890, Accuracy: 82.69%\n",
      "Batch 14, Loss: 0.892624, Accuracy: 82.81%\n",
      "Batch 15, Loss: 0.879276, Accuracy: 83.12%\n",
      "Batch 16, Loss: 0.987651, Accuracy: 82.62%\n",
      "Batch 17, Loss: 0.925430, Accuracy: 82.54%\n",
      "Batch 18, Loss: 0.927108, Accuracy: 82.47%\n",
      "Batch 19, Loss: 0.870231, Accuracy: 82.73%\n",
      "Batch 20, Loss: 0.864480, Accuracy: 82.97%\n",
      "Batch 21, Loss: 0.976405, Accuracy: 82.66%\n",
      "Batch 22, Loss: 0.859422, Accuracy: 83.03%\n",
      "Batch 23, Loss: 0.893301, Accuracy: 83.15%\n",
      "Batch 24, Loss: 0.892034, Accuracy: 83.20%\n",
      "Batch 25, Loss: 0.912436, Accuracy: 83.25%\n",
      "Batch 26, Loss: 0.894966, Accuracy: 83.29%\n",
      "Batch 27, Loss: 0.886562, Accuracy: 83.39%\n",
      "Batch 28, Loss: 0.854633, Accuracy: 83.59%\n",
      "Batch 29, Loss: 0.904703, Accuracy: 83.62%\n",
      "Batch 30, Loss: 0.915430, Accuracy: 83.59%\n",
      "Batch 31, Loss: 0.943118, Accuracy: 83.47%\n",
      "Batch 32, Loss: 0.883009, Accuracy: 83.54%\n",
      "Batch 33, Loss: 0.940251, Accuracy: 83.48%\n",
      "Batch 34, Loss: 0.943935, Accuracy: 83.41%\n",
      "Batch 35, Loss: 0.918858, Accuracy: 83.39%\n",
      "Batch 36, Loss: 0.888060, Accuracy: 83.51%\n",
      "Batch 37, Loss: 0.896456, Accuracy: 83.53%\n",
      "Batch 38, Loss: 0.849118, Accuracy: 83.68%\n",
      "Batch 39, Loss: 0.905683, Accuracy: 83.73%\n",
      "Batch 40, Loss: 0.895829, Accuracy: 83.71%\n",
      "Batch 41, Loss: 0.905549, Accuracy: 83.73%\n",
      "Batch 42, Loss: 0.847026, Accuracy: 83.85%\n",
      "Batch 43, Loss: 0.948051, Accuracy: 83.72%\n",
      "Batch 44, Loss: 0.847171, Accuracy: 83.84%\n",
      "Batch 45, Loss: 0.879813, Accuracy: 83.92%\n",
      "Batch 46, Loss: 0.910990, Accuracy: 83.90%\n",
      "Batch 47, Loss: 0.923194, Accuracy: 83.84%\n",
      "Batch 48, Loss: 0.877689, Accuracy: 83.92%\n",
      "Batch 49, Loss: 0.871931, Accuracy: 83.99%\n",
      "Batch 50, Loss: 0.958559, Accuracy: 83.91%\n",
      "Batch 51, Loss: 0.919094, Accuracy: 83.88%\n",
      "Batch 52, Loss: 0.950012, Accuracy: 83.77%\n",
      "Batch 53, Loss: 0.884710, Accuracy: 83.81%\n",
      "Batch 54, Loss: 0.858249, Accuracy: 83.91%\n",
      "Batch 55, Loss: 0.895685, Accuracy: 83.95%\n",
      "Batch 56, Loss: 0.829806, Accuracy: 84.12%\n",
      "Batch 57, Loss: 0.872207, Accuracy: 84.18%\n",
      "Batch 58, Loss: 0.933444, Accuracy: 84.13%\n",
      "Batch 59, Loss: 1.025469, Accuracy: 83.90%\n",
      "Batch 60, Loss: 0.913290, Accuracy: 83.88%\n",
      "Batch 61, Loss: 0.940620, Accuracy: 83.81%\n",
      "Batch 62, Loss: 0.892389, Accuracy: 83.82%\n",
      "Batch 63, Loss: 0.877567, Accuracy: 83.85%\n",
      "Batch 64, Loss: 0.890081, Accuracy: 83.89%\n",
      "Batch 65, Loss: 0.953174, Accuracy: 83.82%\n",
      "Batch 66, Loss: 0.905933, Accuracy: 83.83%\n",
      "Batch 67, Loss: 0.884604, Accuracy: 83.86%\n",
      "Batch 68, Loss: 0.848978, Accuracy: 83.94%\n",
      "Batch 69, Loss: 0.882681, Accuracy: 83.94%\n",
      "Batch 70, Loss: 0.899053, Accuracy: 84.00%\n",
      "Batch 71, Loss: 0.994557, Accuracy: 83.87%\n",
      "Batch 72, Loss: 0.887189, Accuracy: 83.90%\n",
      "Batch 73, Loss: 0.875387, Accuracy: 83.93%\n",
      "Batch 74, Loss: 0.822396, Accuracy: 84.06%\n",
      "Batch 75, Loss: 0.933928, Accuracy: 84.02%\n",
      "Batch 76, Loss: 0.891498, Accuracy: 84.05%\n",
      "Batch 77, Loss: 0.813865, Accuracy: 84.17%\n",
      "Batch 78, Loss: 0.891667, Accuracy: 84.19%\n",
      "Batch 79, Loss: 0.901969, Accuracy: 84.18%\n",
      "Batch 80, Loss: 0.864163, Accuracy: 84.22%\n",
      "Batch 81, Loss: 0.883198, Accuracy: 84.28%\n",
      "Batch 82, Loss: 0.934668, Accuracy: 84.24%\n",
      "Batch 83, Loss: 0.868483, Accuracy: 84.30%\n",
      "Batch 84, Loss: 0.873129, Accuracy: 84.34%\n",
      "Batch 85, Loss: 0.852706, Accuracy: 84.39%\n",
      "Batch 86, Loss: 0.890026, Accuracy: 84.39%\n",
      "Batch 87, Loss: 0.847280, Accuracy: 84.45%\n",
      "Batch 88, Loss: 0.920315, Accuracy: 84.45%\n",
      "Batch 89, Loss: 0.933316, Accuracy: 84.39%\n",
      "Batch 90, Loss: 0.984520, Accuracy: 84.29%\n",
      "Batch 91, Loss: 0.878130, Accuracy: 84.32%\n",
      "Batch 92, Loss: 0.869884, Accuracy: 84.38%\n",
      "Batch 93, Loss: 0.935920, Accuracy: 84.31%\n",
      "Batch 94, Loss: 0.956432, Accuracy: 84.28%\n",
      "Batch 95, Loss: 0.862230, Accuracy: 84.33%\n",
      "Batch 96, Loss: 0.847196, Accuracy: 84.38%\n",
      "Batch 97, Loss: 0.954026, Accuracy: 84.31%\n",
      "Batch 98, Loss: 0.869198, Accuracy: 84.34%\n",
      "Batch 99, Loss: 0.927623, Accuracy: 84.31%\n",
      "Batch 100, Loss: 0.843400, Accuracy: 84.38%\n",
      "Batch 101, Loss: 0.931139, Accuracy: 84.34%\n",
      "Batch 102, Loss: 0.875930, Accuracy: 84.34%\n",
      "Batch 103, Loss: 0.951658, Accuracy: 84.25%\n",
      "Batch 104, Loss: 0.882540, Accuracy: 84.27%\n",
      "Batch 105, Loss: 0.868967, Accuracy: 84.30%\n",
      "Batch 106, Loss: 0.827989, Accuracy: 84.38%\n",
      "Batch 107, Loss: 0.975611, Accuracy: 84.32%\n",
      "Batch 108, Loss: 0.911957, Accuracy: 84.32%\n",
      "Batch 109, Loss: 0.905095, Accuracy: 84.32%\n",
      "Batch 110, Loss: 0.930407, Accuracy: 84.29%\n",
      "Batch 111, Loss: 0.866252, Accuracy: 84.30%\n",
      "Batch 112, Loss: 0.858121, Accuracy: 84.35%\n",
      "Batch 113, Loss: 0.933435, Accuracy: 84.31%\n",
      "Batch 114, Loss: 0.901668, Accuracy: 84.28%\n",
      "Batch 115, Loss: 1.001251, Accuracy: 84.18%\n",
      "Batch 116, Loss: 0.900073, Accuracy: 84.19%\n",
      "Batch 117, Loss: 0.869138, Accuracy: 84.23%\n",
      "Batch 118, Loss: 0.909098, Accuracy: 84.22%\n",
      "Batch 119, Loss: 0.874246, Accuracy: 84.24%\n",
      "Batch 120, Loss: 1.043787, Accuracy: 84.14%\n",
      "Batch 121, Loss: 0.874122, Accuracy: 84.14%\n",
      "Batch 122, Loss: 0.848596, Accuracy: 84.20%\n",
      "Batch 123, Loss: 0.907836, Accuracy: 84.20%\n",
      "Batch 124, Loss: 0.913990, Accuracy: 84.19%\n",
      "Batch 125, Loss: 0.894835, Accuracy: 84.20%\n",
      "Batch 126, Loss: 0.910349, Accuracy: 84.21%\n",
      "Batch 127, Loss: 0.870100, Accuracy: 84.24%\n",
      "Batch 128, Loss: 1.003878, Accuracy: 84.17%\n",
      "Batch 129, Loss: 0.850650, Accuracy: 84.21%\n",
      "Batch 130, Loss: 0.952746, Accuracy: 84.17%\n",
      "Batch 131, Loss: 0.887172, Accuracy: 84.18%\n",
      "Batch 132, Loss: 0.957119, Accuracy: 84.14%\n",
      "Batch 133, Loss: 0.877671, Accuracy: 84.16%\n",
      "Batch 134, Loss: 0.896761, Accuracy: 84.17%\n",
      "Batch 135, Loss: 0.939028, Accuracy: 84.13%\n",
      "Batch 136, Loss: 0.920272, Accuracy: 84.12%\n",
      "Batch 137, Loss: 0.921239, Accuracy: 84.11%\n",
      "Batch 138, Loss: 0.900927, Accuracy: 84.10%\n",
      "Batch 139, Loss: 0.900856, Accuracy: 84.11%\n",
      "Batch 140, Loss: 0.880270, Accuracy: 84.12%\n",
      "Batch 141, Loss: 0.954976, Accuracy: 84.08%\n",
      "Batch 142, Loss: 0.920119, Accuracy: 84.08%\n",
      "Batch 143, Loss: 0.938089, Accuracy: 84.05%\n",
      "Batch 144, Loss: 0.979488, Accuracy: 84.00%\n",
      "Batch 145, Loss: 0.818305, Accuracy: 84.06%\n",
      "Batch 146, Loss: 0.842189, Accuracy: 84.12%\n",
      "Batch 147, Loss: 0.899972, Accuracy: 84.12%\n",
      "Batch 148, Loss: 0.966059, Accuracy: 84.08%\n",
      "Batch 149, Loss: 0.881782, Accuracy: 84.09%\n",
      "Batch 150, Loss: 0.858269, Accuracy: 84.12%\n",
      "Batch 151, Loss: 0.849133, Accuracy: 84.17%\n",
      "Batch 152, Loss: 0.903576, Accuracy: 84.17%\n",
      "Batch 153, Loss: 0.902034, Accuracy: 84.17%\n",
      "Batch 154, Loss: 0.932107, Accuracy: 84.16%\n",
      "Batch 155, Loss: 0.907851, Accuracy: 84.15%\n",
      "Batch 156, Loss: 0.984428, Accuracy: 84.10%\n",
      "Batch 157, Loss: 0.945537, Accuracy: 84.07%\n",
      "Batch 158, Loss: 0.953502, Accuracy: 84.04%\n",
      "Batch 159, Loss: 0.963359, Accuracy: 83.99%\n",
      "Batch 160, Loss: 0.898672, Accuracy: 83.98%\n",
      "Batch 161, Loss: 0.892660, Accuracy: 84.00%\n",
      "Batch 162, Loss: 0.973745, Accuracy: 83.95%\n",
      "Batch 163, Loss: 0.914295, Accuracy: 83.94%\n",
      "Batch 164, Loss: 0.922043, Accuracy: 83.93%\n",
      "Batch 165, Loss: 0.910341, Accuracy: 83.93%\n",
      "Batch 166, Loss: 0.973275, Accuracy: 83.89%\n",
      "Batch 167, Loss: 0.922026, Accuracy: 83.87%\n",
      "Batch 168, Loss: 0.958483, Accuracy: 83.83%\n",
      "Batch 169, Loss: 0.953041, Accuracy: 83.79%\n",
      "Batch 170, Loss: 0.965546, Accuracy: 83.76%\n",
      "Batch 171, Loss: 0.846580, Accuracy: 83.79%\n",
      "Batch 172, Loss: 0.916866, Accuracy: 83.77%\n",
      "Batch 173, Loss: 0.873019, Accuracy: 83.78%\n",
      "Batch 174, Loss: 0.979161, Accuracy: 83.73%\n",
      "Batch 175, Loss: 0.930731, Accuracy: 83.71%\n",
      "Batch 176, Loss: 0.896215, Accuracy: 83.73%\n",
      "Batch 177, Loss: 0.853297, Accuracy: 83.75%\n",
      "Batch 178, Loss: 0.874359, Accuracy: 83.76%\n",
      "Batch 179, Loss: 0.936586, Accuracy: 83.75%\n",
      "Batch 180, Loss: 0.869911, Accuracy: 83.76%\n",
      "Batch 181, Loss: 0.875019, Accuracy: 83.78%\n",
      "Batch 182, Loss: 0.933608, Accuracy: 83.77%\n",
      "Batch 183, Loss: 0.868958, Accuracy: 83.79%\n",
      "Batch 184, Loss: 0.880522, Accuracy: 83.79%\n",
      "Batch 185, Loss: 0.922829, Accuracy: 83.80%\n",
      "Batch 186, Loss: 0.876063, Accuracy: 83.80%\n",
      "Batch 187, Loss: 0.878462, Accuracy: 83.82%\n",
      "Batch 188, Loss: 0.880907, Accuracy: 83.83%\n",
      "Batch 189, Loss: 0.878149, Accuracy: 83.84%\n",
      "Batch 190, Loss: 0.944219, Accuracy: 83.81%\n",
      "Batch 191, Loss: 0.924257, Accuracy: 83.79%\n",
      "Batch 192, Loss: 0.887914, Accuracy: 83.81%\n",
      "Batch 193, Loss: 0.916963, Accuracy: 83.80%\n",
      "Batch 194, Loss: 0.868948, Accuracy: 83.82%\n",
      "Batch 195, Loss: 0.887787, Accuracy: 83.82%\n",
      "Batch 196, Loss: 0.835270, Accuracy: 83.86%\n",
      "Batch 197, Loss: 0.863445, Accuracy: 83.88%\n",
      "Batch 198, Loss: 0.975055, Accuracy: 83.85%\n",
      "Batch 199, Loss: 0.904200, Accuracy: 83.84%\n",
      "Batch 200, Loss: 0.954804, Accuracy: 83.81%\n",
      "Batch 201, Loss: 0.865927, Accuracy: 83.84%\n",
      "Batch 202, Loss: 0.872786, Accuracy: 83.86%\n",
      "Batch 203, Loss: 0.919956, Accuracy: 83.84%\n",
      "Batch 204, Loss: 0.960079, Accuracy: 83.82%\n",
      "Batch 205, Loss: 0.914644, Accuracy: 83.80%\n",
      "Batch 206, Loss: 0.991354, Accuracy: 83.77%\n",
      "Batch 207, Loss: 0.883183, Accuracy: 83.78%\n",
      "Batch 208, Loss: 0.859734, Accuracy: 83.80%\n",
      "Batch 209, Loss: 0.864522, Accuracy: 83.83%\n",
      "Batch 210, Loss: 0.868412, Accuracy: 83.85%\n",
      "Batch 211, Loss: 0.866130, Accuracy: 83.87%\n",
      "Batch 212, Loss: 0.957203, Accuracy: 83.84%\n",
      "Batch 213, Loss: 0.884055, Accuracy: 83.85%\n",
      "Training - Epoch 13, Loss: 0.905018, Accuracy: 83.85%\n",
      "Validation Batch 1, Loss: 0.838851, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.848638, Accuracy: 90.62%\n",
      "Validation Batch 3, Loss: 0.963709, Accuracy: 86.46%\n",
      "Validation Batch 4, Loss: 0.885020, Accuracy: 85.94%\n",
      "Validation Batch 5, Loss: 0.837974, Accuracy: 86.56%\n",
      "Validation Batch 6, Loss: 0.831731, Accuracy: 87.24%\n",
      "Validation Batch 7, Loss: 0.897118, Accuracy: 86.83%\n",
      "Validation Batch 8, Loss: 0.950286, Accuracy: 85.74%\n",
      "Validation Batch 9, Loss: 0.927377, Accuracy: 85.07%\n",
      "Validation Batch 10, Loss: 0.929609, Accuracy: 84.69%\n",
      "Validation Batch 11, Loss: 0.864215, Accuracy: 84.94%\n",
      "Validation Batch 12, Loss: 0.845862, Accuracy: 85.29%\n",
      "Validation Batch 13, Loss: 0.898473, Accuracy: 85.34%\n",
      "Validation Batch 14, Loss: 0.906496, Accuracy: 85.16%\n",
      "Validation Batch 15, Loss: 0.901271, Accuracy: 85.10%\n",
      "Validation Batch 16, Loss: 0.876148, Accuracy: 85.16%\n",
      "Validation Batch 17, Loss: 0.942260, Accuracy: 84.74%\n",
      "Validation Batch 18, Loss: 0.870875, Accuracy: 84.81%\n",
      "Validation Batch 19, Loss: 0.930491, Accuracy: 84.54%\n",
      "Validation Batch 20, Loss: 0.829559, Accuracy: 84.84%\n",
      "Validation Batch 21, Loss: 0.892363, Accuracy: 84.90%\n",
      "Validation Batch 22, Loss: 0.890187, Accuracy: 84.80%\n",
      "Validation Batch 23, Loss: 0.910591, Accuracy: 84.71%\n",
      "Validation Batch 24, Loss: 0.947807, Accuracy: 84.51%\n",
      "Validation Batch 25, Loss: 0.893244, Accuracy: 84.44%\n",
      "Validation Batch 26, Loss: 0.893850, Accuracy: 84.44%\n",
      "Validation Batch 27, Loss: 0.805177, Accuracy: 84.67%\n",
      "Validation - Epoch 13, Loss: 0.889229, Accuracy: 84.67%\n",
      "Patienceâ€”16\n",
      "Epoch 14\n",
      "Batch 1, Loss: 0.948091, Accuracy: 78.12%\n",
      "Batch 2, Loss: 0.868220, Accuracy: 82.81%\n",
      "Batch 3, Loss: 0.906186, Accuracy: 83.33%\n",
      "Batch 4, Loss: 0.916834, Accuracy: 83.59%\n",
      "Batch 5, Loss: 0.958097, Accuracy: 82.81%\n",
      "Batch 6, Loss: 0.924945, Accuracy: 82.55%\n",
      "Batch 7, Loss: 0.904799, Accuracy: 82.81%\n",
      "Batch 8, Loss: 0.838089, Accuracy: 83.98%\n",
      "Batch 9, Loss: 0.909927, Accuracy: 83.85%\n",
      "Batch 10, Loss: 0.902751, Accuracy: 83.91%\n",
      "Batch 11, Loss: 0.923023, Accuracy: 83.81%\n",
      "Batch 12, Loss: 0.926641, Accuracy: 83.59%\n",
      "Batch 13, Loss: 0.962788, Accuracy: 83.05%\n",
      "Batch 14, Loss: 0.923983, Accuracy: 82.92%\n",
      "Batch 15, Loss: 0.884289, Accuracy: 83.23%\n",
      "Batch 16, Loss: 0.871478, Accuracy: 83.50%\n",
      "Batch 17, Loss: 1.007853, Accuracy: 82.81%\n",
      "Batch 18, Loss: 0.938043, Accuracy: 82.55%\n",
      "Batch 19, Loss: 0.873880, Accuracy: 82.73%\n",
      "Batch 20, Loss: 0.942372, Accuracy: 82.50%\n",
      "Batch 21, Loss: 0.953595, Accuracy: 82.37%\n",
      "Batch 22, Loss: 0.860349, Accuracy: 82.74%\n",
      "Batch 23, Loss: 0.923498, Accuracy: 82.68%\n",
      "Batch 24, Loss: 0.895094, Accuracy: 82.75%\n",
      "Batch 25, Loss: 0.897415, Accuracy: 82.88%\n",
      "Batch 26, Loss: 0.889570, Accuracy: 82.93%\n",
      "Batch 27, Loss: 0.930719, Accuracy: 82.87%\n",
      "Batch 28, Loss: 0.923671, Accuracy: 82.92%\n",
      "Batch 29, Loss: 0.925588, Accuracy: 82.87%\n",
      "Batch 30, Loss: 0.873838, Accuracy: 83.02%\n",
      "Batch 31, Loss: 0.841712, Accuracy: 83.32%\n",
      "Batch 32, Loss: 0.897171, Accuracy: 83.35%\n",
      "Batch 33, Loss: 0.891562, Accuracy: 83.38%\n",
      "Batch 34, Loss: 0.987674, Accuracy: 83.18%\n",
      "Batch 35, Loss: 0.905087, Accuracy: 83.17%\n",
      "Batch 36, Loss: 0.875900, Accuracy: 83.29%\n",
      "Batch 37, Loss: 0.991788, Accuracy: 83.11%\n",
      "Batch 38, Loss: 1.012428, Accuracy: 82.81%\n",
      "Batch 39, Loss: 0.869827, Accuracy: 82.89%\n",
      "Batch 40, Loss: 0.926854, Accuracy: 82.89%\n",
      "Batch 41, Loss: 0.863735, Accuracy: 83.00%\n",
      "Batch 42, Loss: 0.907378, Accuracy: 83.00%\n",
      "Batch 43, Loss: 0.852833, Accuracy: 83.14%\n",
      "Batch 44, Loss: 0.904246, Accuracy: 83.17%\n",
      "Batch 45, Loss: 0.997225, Accuracy: 82.95%\n",
      "Batch 46, Loss: 0.889040, Accuracy: 83.02%\n",
      "Batch 47, Loss: 0.898882, Accuracy: 83.08%\n",
      "Batch 48, Loss: 0.882500, Accuracy: 83.14%\n",
      "Batch 49, Loss: 0.940779, Accuracy: 83.07%\n",
      "Batch 50, Loss: 0.921099, Accuracy: 83.03%\n",
      "Batch 51, Loss: 0.829649, Accuracy: 83.18%\n",
      "Batch 52, Loss: 0.965040, Accuracy: 83.08%\n",
      "Batch 53, Loss: 0.942879, Accuracy: 83.05%\n",
      "Batch 54, Loss: 0.985242, Accuracy: 82.96%\n",
      "Batch 55, Loss: 0.876241, Accuracy: 83.01%\n",
      "Batch 56, Loss: 0.869307, Accuracy: 83.01%\n",
      "Batch 57, Loss: 0.932239, Accuracy: 82.98%\n",
      "Batch 58, Loss: 0.857225, Accuracy: 83.08%\n",
      "Batch 59, Loss: 0.854040, Accuracy: 83.18%\n",
      "Batch 60, Loss: 0.829083, Accuracy: 83.33%\n",
      "Batch 61, Loss: 0.903411, Accuracy: 83.35%\n",
      "Batch 62, Loss: 0.831520, Accuracy: 83.49%\n",
      "Batch 63, Loss: 0.897969, Accuracy: 83.51%\n",
      "Batch 64, Loss: 0.947060, Accuracy: 83.45%\n",
      "Batch 65, Loss: 0.849594, Accuracy: 83.56%\n",
      "Batch 66, Loss: 0.864428, Accuracy: 83.62%\n",
      "Batch 67, Loss: 0.891397, Accuracy: 83.63%\n",
      "Batch 68, Loss: 0.862781, Accuracy: 83.71%\n",
      "Batch 69, Loss: 0.933987, Accuracy: 83.65%\n",
      "Batch 70, Loss: 0.834549, Accuracy: 83.75%\n",
      "Batch 71, Loss: 0.935003, Accuracy: 83.69%\n",
      "Batch 72, Loss: 0.897796, Accuracy: 83.70%\n",
      "Batch 73, Loss: 0.900275, Accuracy: 83.71%\n",
      "Batch 74, Loss: 0.913417, Accuracy: 83.70%\n",
      "Batch 75, Loss: 0.901742, Accuracy: 83.71%\n",
      "Batch 76, Loss: 0.969265, Accuracy: 83.61%\n",
      "Batch 77, Loss: 0.863656, Accuracy: 83.66%\n",
      "Batch 78, Loss: 0.894456, Accuracy: 83.69%\n",
      "Batch 79, Loss: 0.911406, Accuracy: 83.70%\n",
      "Batch 80, Loss: 0.945583, Accuracy: 83.65%\n",
      "Batch 81, Loss: 0.892741, Accuracy: 83.68%\n",
      "Batch 82, Loss: 0.912408, Accuracy: 83.69%\n",
      "Batch 83, Loss: 0.902758, Accuracy: 83.66%\n",
      "Batch 84, Loss: 0.930479, Accuracy: 83.61%\n",
      "Batch 85, Loss: 0.907251, Accuracy: 83.64%\n",
      "Batch 86, Loss: 0.861748, Accuracy: 83.67%\n",
      "Batch 87, Loss: 0.899715, Accuracy: 83.67%\n",
      "Batch 88, Loss: 0.892827, Accuracy: 83.68%\n",
      "Batch 89, Loss: 0.899716, Accuracy: 83.71%\n",
      "Batch 90, Loss: 0.867090, Accuracy: 83.77%\n",
      "Batch 91, Loss: 0.888196, Accuracy: 83.79%\n",
      "Batch 92, Loss: 0.933257, Accuracy: 83.78%\n",
      "Batch 93, Loss: 0.881413, Accuracy: 83.80%\n",
      "Batch 94, Loss: 0.804468, Accuracy: 83.93%\n",
      "Batch 95, Loss: 0.942007, Accuracy: 83.88%\n",
      "Batch 96, Loss: 0.931644, Accuracy: 83.87%\n",
      "Batch 97, Loss: 0.894191, Accuracy: 83.89%\n",
      "Batch 98, Loss: 0.873108, Accuracy: 83.88%\n",
      "Batch 99, Loss: 1.029120, Accuracy: 83.76%\n",
      "Batch 100, Loss: 0.887931, Accuracy: 83.80%\n",
      "Batch 101, Loss: 0.926086, Accuracy: 83.77%\n",
      "Batch 102, Loss: 0.934811, Accuracy: 83.75%\n",
      "Batch 103, Loss: 0.867562, Accuracy: 83.78%\n",
      "Batch 104, Loss: 0.940238, Accuracy: 83.77%\n",
      "Batch 105, Loss: 0.926724, Accuracy: 83.76%\n",
      "Batch 106, Loss: 0.894124, Accuracy: 83.76%\n",
      "Batch 107, Loss: 0.938035, Accuracy: 83.73%\n",
      "Batch 108, Loss: 0.926356, Accuracy: 83.70%\n",
      "Batch 109, Loss: 0.947897, Accuracy: 83.66%\n",
      "Batch 110, Loss: 0.855037, Accuracy: 83.71%\n",
      "Batch 111, Loss: 0.869126, Accuracy: 83.74%\n",
      "Batch 112, Loss: 0.860226, Accuracy: 83.78%\n",
      "Batch 113, Loss: 0.917733, Accuracy: 83.77%\n",
      "Batch 114, Loss: 0.929326, Accuracy: 83.74%\n",
      "Batch 115, Loss: 0.814926, Accuracy: 83.82%\n",
      "Batch 116, Loss: 0.970324, Accuracy: 83.73%\n",
      "Batch 117, Loss: 0.901407, Accuracy: 83.75%\n",
      "Batch 118, Loss: 0.946702, Accuracy: 83.73%\n",
      "Batch 119, Loss: 0.897316, Accuracy: 83.73%\n",
      "Batch 120, Loss: 0.985957, Accuracy: 83.66%\n",
      "Batch 121, Loss: 0.873381, Accuracy: 83.70%\n",
      "Batch 122, Loss: 0.962589, Accuracy: 83.63%\n",
      "Batch 123, Loss: 0.883440, Accuracy: 83.66%\n",
      "Batch 124, Loss: 0.933246, Accuracy: 83.63%\n",
      "Batch 125, Loss: 0.945487, Accuracy: 83.60%\n",
      "Batch 126, Loss: 0.874850, Accuracy: 83.62%\n",
      "Batch 127, Loss: 0.866037, Accuracy: 83.65%\n",
      "Batch 128, Loss: 0.941922, Accuracy: 83.62%\n",
      "Batch 129, Loss: 0.929896, Accuracy: 83.60%\n",
      "Batch 130, Loss: 0.965242, Accuracy: 83.53%\n",
      "Batch 131, Loss: 0.821325, Accuracy: 83.60%\n",
      "Batch 132, Loss: 0.897369, Accuracy: 83.62%\n",
      "Batch 133, Loss: 0.864596, Accuracy: 83.65%\n",
      "Batch 134, Loss: 0.902166, Accuracy: 83.64%\n",
      "Batch 135, Loss: 0.954553, Accuracy: 83.61%\n",
      "Batch 136, Loss: 0.995282, Accuracy: 83.55%\n",
      "Batch 137, Loss: 0.847081, Accuracy: 83.60%\n",
      "Batch 138, Loss: 0.901280, Accuracy: 83.62%\n",
      "Batch 139, Loss: 0.898553, Accuracy: 83.63%\n",
      "Batch 140, Loss: 0.898687, Accuracy: 83.64%\n",
      "Batch 141, Loss: 0.938424, Accuracy: 83.60%\n",
      "Batch 142, Loss: 0.944077, Accuracy: 83.58%\n",
      "Batch 143, Loss: 0.989017, Accuracy: 83.51%\n",
      "Batch 144, Loss: 0.887912, Accuracy: 83.53%\n",
      "Batch 145, Loss: 0.862431, Accuracy: 83.57%\n",
      "Batch 146, Loss: 0.851197, Accuracy: 83.62%\n",
      "Batch 147, Loss: 0.901738, Accuracy: 83.62%\n",
      "Batch 148, Loss: 0.935987, Accuracy: 83.59%\n",
      "Batch 149, Loss: 0.919241, Accuracy: 83.58%\n",
      "Batch 150, Loss: 0.951088, Accuracy: 83.54%\n",
      "Batch 151, Loss: 0.924900, Accuracy: 83.52%\n",
      "Batch 152, Loss: 0.854027, Accuracy: 83.55%\n",
      "Batch 153, Loss: 0.935762, Accuracy: 83.53%\n",
      "Batch 154, Loss: 0.942635, Accuracy: 83.50%\n",
      "Batch 155, Loss: 0.873138, Accuracy: 83.53%\n",
      "Batch 156, Loss: 0.934060, Accuracy: 83.51%\n",
      "Batch 157, Loss: 0.898384, Accuracy: 83.51%\n",
      "Batch 158, Loss: 0.919364, Accuracy: 83.50%\n",
      "Batch 159, Loss: 0.928216, Accuracy: 83.49%\n",
      "Batch 160, Loss: 0.849299, Accuracy: 83.54%\n",
      "Batch 161, Loss: 0.893680, Accuracy: 83.55%\n",
      "Batch 162, Loss: 0.887651, Accuracy: 83.56%\n",
      "Batch 163, Loss: 0.991050, Accuracy: 83.49%\n",
      "Batch 164, Loss: 0.948077, Accuracy: 83.46%\n",
      "Batch 165, Loss: 0.963126, Accuracy: 83.42%\n",
      "Batch 166, Loss: 0.895313, Accuracy: 83.43%\n",
      "Batch 167, Loss: 0.891928, Accuracy: 83.45%\n",
      "Batch 168, Loss: 0.914993, Accuracy: 83.44%\n",
      "Batch 169, Loss: 0.918500, Accuracy: 83.43%\n",
      "Batch 170, Loss: 0.884855, Accuracy: 83.46%\n",
      "Batch 171, Loss: 0.943053, Accuracy: 83.42%\n",
      "Batch 172, Loss: 0.945957, Accuracy: 83.40%\n",
      "Batch 173, Loss: 0.968653, Accuracy: 83.35%\n",
      "Batch 174, Loss: 0.893932, Accuracy: 83.37%\n",
      "Batch 175, Loss: 0.843183, Accuracy: 83.41%\n",
      "Batch 176, Loss: 0.836814, Accuracy: 83.45%\n",
      "Batch 177, Loss: 0.943345, Accuracy: 83.44%\n",
      "Batch 178, Loss: 0.909537, Accuracy: 83.44%\n",
      "Batch 179, Loss: 0.898285, Accuracy: 83.46%\n",
      "Batch 180, Loss: 0.896801, Accuracy: 83.47%\n",
      "Batch 181, Loss: 0.892807, Accuracy: 83.49%\n",
      "Batch 182, Loss: 0.897264, Accuracy: 83.50%\n",
      "Batch 183, Loss: 0.897861, Accuracy: 83.50%\n",
      "Batch 184, Loss: 0.925114, Accuracy: 83.48%\n",
      "Batch 185, Loss: 0.931647, Accuracy: 83.47%\n",
      "Batch 186, Loss: 0.905808, Accuracy: 83.47%\n",
      "Batch 187, Loss: 0.929554, Accuracy: 83.46%\n",
      "Batch 188, Loss: 0.952317, Accuracy: 83.45%\n",
      "Batch 189, Loss: 0.883842, Accuracy: 83.47%\n",
      "Batch 190, Loss: 0.921084, Accuracy: 83.45%\n",
      "Batch 191, Loss: 0.998212, Accuracy: 83.39%\n",
      "Batch 192, Loss: 0.895942, Accuracy: 83.39%\n",
      "Batch 193, Loss: 0.952407, Accuracy: 83.37%\n",
      "Batch 194, Loss: 0.873867, Accuracy: 83.39%\n",
      "Batch 195, Loss: 0.803410, Accuracy: 83.45%\n",
      "Batch 196, Loss: 0.877944, Accuracy: 83.47%\n",
      "Batch 197, Loss: 0.896796, Accuracy: 83.49%\n",
      "Batch 198, Loss: 0.877023, Accuracy: 83.50%\n",
      "Batch 199, Loss: 0.935088, Accuracy: 83.49%\n",
      "Batch 200, Loss: 0.906281, Accuracy: 83.49%\n",
      "Batch 201, Loss: 0.992765, Accuracy: 83.45%\n",
      "Batch 202, Loss: 0.857509, Accuracy: 83.47%\n",
      "Batch 203, Loss: 0.907504, Accuracy: 83.47%\n",
      "Batch 204, Loss: 0.860664, Accuracy: 83.49%\n",
      "Batch 205, Loss: 0.977984, Accuracy: 83.45%\n",
      "Batch 206, Loss: 0.838391, Accuracy: 83.49%\n",
      "Batch 207, Loss: 0.865605, Accuracy: 83.50%\n",
      "Batch 208, Loss: 0.926644, Accuracy: 83.50%\n",
      "Batch 209, Loss: 0.832353, Accuracy: 83.53%\n",
      "Batch 210, Loss: 0.902719, Accuracy: 83.53%\n",
      "Batch 211, Loss: 0.882163, Accuracy: 83.54%\n",
      "Batch 212, Loss: 0.937448, Accuracy: 83.53%\n",
      "Batch 213, Loss: 0.921080, Accuracy: 83.53%\n",
      "Training - Epoch 14, Loss: 0.907790, Accuracy: 83.53%\n",
      "Validation Batch 1, Loss: 0.837881, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.846971, Accuracy: 91.41%\n",
      "Validation Batch 3, Loss: 0.962530, Accuracy: 86.98%\n",
      "Validation Batch 4, Loss: 0.884187, Accuracy: 86.33%\n",
      "Validation Batch 5, Loss: 0.836735, Accuracy: 86.88%\n",
      "Validation Batch 6, Loss: 0.830424, Accuracy: 87.50%\n",
      "Validation Batch 7, Loss: 0.896329, Accuracy: 87.05%\n",
      "Validation Batch 8, Loss: 0.948924, Accuracy: 85.94%\n",
      "Validation Batch 9, Loss: 0.924775, Accuracy: 85.24%\n",
      "Validation Batch 10, Loss: 0.929798, Accuracy: 84.84%\n",
      "Validation Batch 11, Loss: 0.862814, Accuracy: 85.09%\n",
      "Validation Batch 12, Loss: 0.845007, Accuracy: 85.42%\n",
      "Validation Batch 13, Loss: 0.896319, Accuracy: 85.46%\n",
      "Validation Batch 14, Loss: 0.905568, Accuracy: 85.27%\n",
      "Validation Batch 15, Loss: 0.900569, Accuracy: 85.21%\n",
      "Validation Batch 16, Loss: 0.876110, Accuracy: 85.25%\n",
      "Validation Batch 17, Loss: 0.940993, Accuracy: 84.93%\n",
      "Validation Batch 18, Loss: 0.869364, Accuracy: 85.07%\n",
      "Validation Batch 19, Loss: 0.928995, Accuracy: 84.87%\n",
      "Validation Batch 20, Loss: 0.830403, Accuracy: 85.16%\n",
      "Validation Batch 21, Loss: 0.892358, Accuracy: 85.19%\n",
      "Validation Batch 22, Loss: 0.888228, Accuracy: 85.16%\n",
      "Validation Batch 23, Loss: 0.909885, Accuracy: 85.12%\n",
      "Validation Batch 24, Loss: 0.947303, Accuracy: 84.90%\n",
      "Validation Batch 25, Loss: 0.891980, Accuracy: 84.81%\n",
      "Validation Batch 26, Loss: 0.893274, Accuracy: 84.80%\n",
      "Validation Batch 27, Loss: 0.803777, Accuracy: 85.03%\n",
      "Validation - Epoch 14, Loss: 0.888204, Accuracy: 85.03%\n",
      "Patienceâ€”17\n",
      "Epoch 15\n",
      "Batch 1, Loss: 0.864379, Accuracy: 87.50%\n",
      "Batch 2, Loss: 0.860607, Accuracy: 89.06%\n",
      "Batch 3, Loss: 0.930253, Accuracy: 85.94%\n",
      "Batch 4, Loss: 0.840214, Accuracy: 87.11%\n",
      "Batch 5, Loss: 0.912835, Accuracy: 86.56%\n",
      "Batch 6, Loss: 0.970880, Accuracy: 84.64%\n",
      "Batch 7, Loss: 0.865186, Accuracy: 85.04%\n",
      "Batch 8, Loss: 0.909015, Accuracy: 84.57%\n",
      "Batch 9, Loss: 0.890458, Accuracy: 84.90%\n",
      "Batch 10, Loss: 0.838776, Accuracy: 85.31%\n",
      "Batch 11, Loss: 0.904512, Accuracy: 85.09%\n",
      "Batch 12, Loss: 0.875898, Accuracy: 85.29%\n",
      "Batch 13, Loss: 0.828809, Accuracy: 85.70%\n",
      "Batch 14, Loss: 0.928888, Accuracy: 85.49%\n",
      "Batch 15, Loss: 0.906596, Accuracy: 85.31%\n",
      "Batch 16, Loss: 0.853560, Accuracy: 85.55%\n",
      "Batch 17, Loss: 0.852942, Accuracy: 85.66%\n",
      "Batch 18, Loss: 0.899660, Accuracy: 85.68%\n",
      "Batch 19, Loss: 0.988016, Accuracy: 85.20%\n",
      "Batch 20, Loss: 0.909661, Accuracy: 85.16%\n",
      "Batch 21, Loss: 0.865827, Accuracy: 85.34%\n",
      "Batch 22, Loss: 0.828067, Accuracy: 85.72%\n",
      "Batch 23, Loss: 0.955599, Accuracy: 85.46%\n",
      "Batch 24, Loss: 0.865705, Accuracy: 85.61%\n",
      "Batch 25, Loss: 0.849981, Accuracy: 85.75%\n",
      "Batch 26, Loss: 0.969124, Accuracy: 85.34%\n",
      "Batch 27, Loss: 0.869758, Accuracy: 85.42%\n",
      "Batch 28, Loss: 0.914630, Accuracy: 85.49%\n",
      "Batch 29, Loss: 0.905999, Accuracy: 85.40%\n",
      "Batch 30, Loss: 0.852761, Accuracy: 85.52%\n",
      "Batch 31, Loss: 0.887784, Accuracy: 85.48%\n",
      "Batch 32, Loss: 0.867429, Accuracy: 85.55%\n",
      "Batch 33, Loss: 0.909289, Accuracy: 85.51%\n",
      "Batch 34, Loss: 0.927872, Accuracy: 85.43%\n",
      "Batch 35, Loss: 0.886938, Accuracy: 85.40%\n",
      "Batch 36, Loss: 0.906881, Accuracy: 85.37%\n",
      "Batch 37, Loss: 0.934558, Accuracy: 85.26%\n",
      "Batch 38, Loss: 0.907334, Accuracy: 85.20%\n",
      "Batch 39, Loss: 0.836322, Accuracy: 85.34%\n",
      "Batch 40, Loss: 0.837904, Accuracy: 85.51%\n",
      "Batch 41, Loss: 0.880496, Accuracy: 85.52%\n",
      "Batch 42, Loss: 0.842199, Accuracy: 85.60%\n",
      "Batch 43, Loss: 0.916796, Accuracy: 85.54%\n",
      "Batch 44, Loss: 0.949333, Accuracy: 85.40%\n",
      "Batch 45, Loss: 0.902253, Accuracy: 85.42%\n",
      "Batch 46, Loss: 0.887845, Accuracy: 85.39%\n",
      "Batch 47, Loss: 0.883800, Accuracy: 85.44%\n",
      "Batch 48, Loss: 0.902594, Accuracy: 85.42%\n",
      "Batch 49, Loss: 0.874009, Accuracy: 85.49%\n",
      "Batch 50, Loss: 0.884619, Accuracy: 85.50%\n",
      "Batch 51, Loss: 0.934141, Accuracy: 85.42%\n",
      "Batch 52, Loss: 0.876800, Accuracy: 85.46%\n",
      "Batch 53, Loss: 0.880465, Accuracy: 85.47%\n",
      "Batch 54, Loss: 0.925118, Accuracy: 85.42%\n",
      "Batch 55, Loss: 0.938349, Accuracy: 85.34%\n",
      "Batch 56, Loss: 0.907914, Accuracy: 85.32%\n",
      "Batch 57, Loss: 0.904204, Accuracy: 85.31%\n",
      "Batch 58, Loss: 0.971469, Accuracy: 85.16%\n",
      "Batch 59, Loss: 0.886260, Accuracy: 85.17%\n",
      "Batch 60, Loss: 0.982249, Accuracy: 85.03%\n",
      "Batch 61, Loss: 0.991685, Accuracy: 84.86%\n",
      "Batch 62, Loss: 0.894450, Accuracy: 84.88%\n",
      "Batch 63, Loss: 0.995085, Accuracy: 84.72%\n",
      "Batch 64, Loss: 0.972486, Accuracy: 84.62%\n",
      "Batch 65, Loss: 0.944240, Accuracy: 84.52%\n",
      "Batch 66, Loss: 0.851959, Accuracy: 84.61%\n",
      "Batch 67, Loss: 0.886414, Accuracy: 84.61%\n",
      "Batch 68, Loss: 0.974476, Accuracy: 84.47%\n",
      "Batch 69, Loss: 0.894065, Accuracy: 84.49%\n",
      "Batch 70, Loss: 0.850517, Accuracy: 84.58%\n",
      "Batch 71, Loss: 0.918105, Accuracy: 84.53%\n",
      "Batch 72, Loss: 0.962982, Accuracy: 84.42%\n",
      "Batch 73, Loss: 0.917072, Accuracy: 84.40%\n",
      "Batch 74, Loss: 0.984488, Accuracy: 84.29%\n",
      "Batch 75, Loss: 0.839922, Accuracy: 84.38%\n",
      "Batch 76, Loss: 0.951932, Accuracy: 84.29%\n",
      "Batch 77, Loss: 0.877314, Accuracy: 84.33%\n",
      "Batch 78, Loss: 0.890980, Accuracy: 84.33%\n",
      "Batch 79, Loss: 0.878189, Accuracy: 84.38%\n",
      "Batch 80, Loss: 0.857862, Accuracy: 84.43%\n",
      "Batch 81, Loss: 0.870616, Accuracy: 84.45%\n",
      "Batch 82, Loss: 0.918227, Accuracy: 84.43%\n",
      "Batch 83, Loss: 0.953322, Accuracy: 84.36%\n",
      "Batch 84, Loss: 0.839890, Accuracy: 84.43%\n",
      "Batch 85, Loss: 0.881854, Accuracy: 84.45%\n",
      "Batch 86, Loss: 0.895615, Accuracy: 84.45%\n",
      "Batch 87, Loss: 0.885974, Accuracy: 84.46%\n",
      "Batch 88, Loss: 0.921846, Accuracy: 84.46%\n",
      "Batch 89, Loss: 0.975331, Accuracy: 84.38%\n",
      "Batch 90, Loss: 0.925128, Accuracy: 84.36%\n",
      "Batch 91, Loss: 0.906152, Accuracy: 84.36%\n",
      "Batch 92, Loss: 0.970436, Accuracy: 84.31%\n",
      "Batch 93, Loss: 0.880585, Accuracy: 84.32%\n",
      "Batch 94, Loss: 0.874220, Accuracy: 84.36%\n",
      "Batch 95, Loss: 0.862754, Accuracy: 84.39%\n",
      "Batch 96, Loss: 0.961098, Accuracy: 84.31%\n",
      "Batch 97, Loss: 0.868267, Accuracy: 84.36%\n",
      "Batch 98, Loss: 0.941239, Accuracy: 84.31%\n",
      "Batch 99, Loss: 0.802704, Accuracy: 84.42%\n",
      "Batch 100, Loss: 0.860138, Accuracy: 84.48%\n",
      "Batch 101, Loss: 0.901150, Accuracy: 84.48%\n",
      "Batch 102, Loss: 0.903354, Accuracy: 84.48%\n",
      "Batch 103, Loss: 0.867415, Accuracy: 84.53%\n",
      "Batch 104, Loss: 0.908937, Accuracy: 84.51%\n",
      "Batch 105, Loss: 0.950720, Accuracy: 84.45%\n",
      "Batch 106, Loss: 0.970963, Accuracy: 84.38%\n",
      "Batch 107, Loss: 0.893992, Accuracy: 84.38%\n",
      "Batch 108, Loss: 0.961127, Accuracy: 84.33%\n",
      "Batch 109, Loss: 0.825407, Accuracy: 84.40%\n",
      "Batch 110, Loss: 0.882785, Accuracy: 84.42%\n",
      "Batch 111, Loss: 0.958584, Accuracy: 84.36%\n",
      "Batch 112, Loss: 0.910055, Accuracy: 84.36%\n",
      "Batch 113, Loss: 0.889734, Accuracy: 84.35%\n",
      "Batch 114, Loss: 0.887185, Accuracy: 84.36%\n",
      "Batch 115, Loss: 0.920664, Accuracy: 84.35%\n",
      "Batch 116, Loss: 0.872019, Accuracy: 84.38%\n",
      "Batch 117, Loss: 0.953137, Accuracy: 84.31%\n",
      "Batch 118, Loss: 0.892130, Accuracy: 84.31%\n",
      "Batch 119, Loss: 0.894747, Accuracy: 84.31%\n",
      "Batch 120, Loss: 0.996715, Accuracy: 84.23%\n",
      "Batch 121, Loss: 0.944906, Accuracy: 84.19%\n",
      "Batch 122, Loss: 0.920566, Accuracy: 84.18%\n",
      "Batch 123, Loss: 0.893537, Accuracy: 84.18%\n",
      "Batch 124, Loss: 0.959345, Accuracy: 84.14%\n",
      "Batch 125, Loss: 0.932087, Accuracy: 84.11%\n",
      "Batch 126, Loss: 0.932324, Accuracy: 84.09%\n",
      "Batch 127, Loss: 0.932345, Accuracy: 84.07%\n",
      "Batch 128, Loss: 0.864056, Accuracy: 84.08%\n",
      "Batch 129, Loss: 0.913702, Accuracy: 84.06%\n",
      "Batch 130, Loss: 0.941079, Accuracy: 84.03%\n",
      "Batch 131, Loss: 0.873486, Accuracy: 84.05%\n",
      "Batch 132, Loss: 0.943093, Accuracy: 84.03%\n",
      "Batch 133, Loss: 0.934965, Accuracy: 84.01%\n",
      "Batch 134, Loss: 0.885804, Accuracy: 84.04%\n",
      "Batch 135, Loss: 0.930390, Accuracy: 84.02%\n",
      "Batch 136, Loss: 0.919866, Accuracy: 84.01%\n",
      "Batch 137, Loss: 0.942758, Accuracy: 83.96%\n",
      "Batch 138, Loss: 0.884653, Accuracy: 83.97%\n",
      "Batch 139, Loss: 0.874361, Accuracy: 83.99%\n",
      "Batch 140, Loss: 0.937211, Accuracy: 83.97%\n",
      "Batch 141, Loss: 0.972838, Accuracy: 83.92%\n",
      "Batch 142, Loss: 0.872603, Accuracy: 83.96%\n",
      "Batch 143, Loss: 0.842066, Accuracy: 84.00%\n",
      "Batch 144, Loss: 0.891132, Accuracy: 84.03%\n",
      "Batch 145, Loss: 0.961913, Accuracy: 83.99%\n",
      "Batch 146, Loss: 0.870770, Accuracy: 84.02%\n",
      "Batch 147, Loss: 0.932251, Accuracy: 84.01%\n",
      "Batch 148, Loss: 1.004665, Accuracy: 83.93%\n",
      "Batch 149, Loss: 0.940042, Accuracy: 83.89%\n",
      "Batch 150, Loss: 0.891384, Accuracy: 83.90%\n",
      "Batch 151, Loss: 0.873660, Accuracy: 83.92%\n",
      "Batch 152, Loss: 0.911434, Accuracy: 83.92%\n",
      "Batch 153, Loss: 0.914500, Accuracy: 83.93%\n",
      "Batch 154, Loss: 0.900530, Accuracy: 83.93%\n",
      "Batch 155, Loss: 0.832175, Accuracy: 83.98%\n",
      "Batch 156, Loss: 0.939269, Accuracy: 83.94%\n",
      "Batch 157, Loss: 0.852942, Accuracy: 83.99%\n",
      "Batch 158, Loss: 0.938122, Accuracy: 83.97%\n",
      "Batch 159, Loss: 0.916199, Accuracy: 83.97%\n",
      "Batch 160, Loss: 0.901219, Accuracy: 83.97%\n",
      "Batch 161, Loss: 0.944210, Accuracy: 83.96%\n",
      "Batch 162, Loss: 1.010287, Accuracy: 83.89%\n",
      "Batch 163, Loss: 0.906692, Accuracy: 83.90%\n",
      "Batch 164, Loss: 0.988161, Accuracy: 83.85%\n",
      "Batch 165, Loss: 0.825007, Accuracy: 83.90%\n",
      "Batch 166, Loss: 0.888019, Accuracy: 83.92%\n",
      "Batch 167, Loss: 0.889539, Accuracy: 83.94%\n",
      "Batch 168, Loss: 0.860172, Accuracy: 83.96%\n",
      "Batch 169, Loss: 0.962371, Accuracy: 83.92%\n",
      "Batch 170, Loss: 0.964299, Accuracy: 83.88%\n",
      "Batch 171, Loss: 0.867499, Accuracy: 83.90%\n",
      "Batch 172, Loss: 0.943253, Accuracy: 83.88%\n",
      "Batch 173, Loss: 0.873555, Accuracy: 83.90%\n",
      "Batch 174, Loss: 0.920961, Accuracy: 83.89%\n",
      "Batch 175, Loss: 0.919416, Accuracy: 83.88%\n",
      "Batch 176, Loss: 0.894946, Accuracy: 83.90%\n",
      "Batch 177, Loss: 0.902434, Accuracy: 83.91%\n",
      "Batch 178, Loss: 0.906853, Accuracy: 83.91%\n",
      "Batch 179, Loss: 0.996687, Accuracy: 83.85%\n",
      "Batch 180, Loss: 0.878815, Accuracy: 83.87%\n",
      "Batch 181, Loss: 0.965304, Accuracy: 83.84%\n",
      "Batch 182, Loss: 0.858562, Accuracy: 83.88%\n",
      "Batch 183, Loss: 0.857823, Accuracy: 83.91%\n",
      "Batch 184, Loss: 0.910101, Accuracy: 83.90%\n",
      "Batch 185, Loss: 0.835773, Accuracy: 83.94%\n",
      "Batch 186, Loss: 0.884989, Accuracy: 83.95%\n",
      "Batch 187, Loss: 0.957369, Accuracy: 83.92%\n",
      "Batch 188, Loss: 0.886113, Accuracy: 83.93%\n",
      "Batch 189, Loss: 0.895781, Accuracy: 83.93%\n",
      "Batch 190, Loss: 0.859131, Accuracy: 83.96%\n",
      "Batch 191, Loss: 0.910247, Accuracy: 83.97%\n",
      "Batch 192, Loss: 0.864049, Accuracy: 83.98%\n",
      "Batch 193, Loss: 0.853175, Accuracy: 84.01%\n",
      "Batch 194, Loss: 0.826363, Accuracy: 84.04%\n",
      "Batch 195, Loss: 0.853314, Accuracy: 84.07%\n",
      "Batch 196, Loss: 0.923527, Accuracy: 84.05%\n",
      "Batch 197, Loss: 0.985062, Accuracy: 84.02%\n",
      "Batch 198, Loss: 0.886562, Accuracy: 84.03%\n",
      "Batch 199, Loss: 0.891126, Accuracy: 84.04%\n",
      "Batch 200, Loss: 0.807818, Accuracy: 84.09%\n",
      "Batch 201, Loss: 0.913660, Accuracy: 84.08%\n",
      "Batch 202, Loss: 0.991374, Accuracy: 84.03%\n",
      "Batch 203, Loss: 0.899486, Accuracy: 84.04%\n",
      "Batch 204, Loss: 0.908149, Accuracy: 84.03%\n",
      "Batch 205, Loss: 0.952865, Accuracy: 84.00%\n",
      "Batch 206, Loss: 0.961237, Accuracy: 83.97%\n",
      "Batch 207, Loss: 0.882009, Accuracy: 83.98%\n",
      "Batch 208, Loss: 0.913109, Accuracy: 83.98%\n",
      "Batch 209, Loss: 0.972109, Accuracy: 83.94%\n",
      "Batch 210, Loss: 0.907460, Accuracy: 83.94%\n",
      "Batch 211, Loss: 0.906974, Accuracy: 83.94%\n",
      "Batch 212, Loss: 0.851346, Accuracy: 83.96%\n",
      "Batch 213, Loss: 0.852924, Accuracy: 83.98%\n",
      "Training - Epoch 15, Loss: 0.905167, Accuracy: 83.98%\n",
      "Validation Batch 1, Loss: 0.837667, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.845626, Accuracy: 91.41%\n",
      "Validation Batch 3, Loss: 0.962289, Accuracy: 86.98%\n",
      "Validation Batch 4, Loss: 0.882594, Accuracy: 86.72%\n",
      "Validation Batch 5, Loss: 0.835189, Accuracy: 87.19%\n",
      "Validation Batch 6, Loss: 0.829624, Accuracy: 88.02%\n",
      "Validation Batch 7, Loss: 0.894463, Accuracy: 87.50%\n",
      "Validation Batch 8, Loss: 0.948961, Accuracy: 86.33%\n",
      "Validation Batch 9, Loss: 0.925199, Accuracy: 85.76%\n",
      "Validation Batch 10, Loss: 0.928045, Accuracy: 85.31%\n",
      "Validation Batch 11, Loss: 0.860669, Accuracy: 85.51%\n",
      "Validation Batch 12, Loss: 0.845557, Accuracy: 85.81%\n",
      "Validation Batch 13, Loss: 0.895473, Accuracy: 85.82%\n",
      "Validation Batch 14, Loss: 0.905498, Accuracy: 85.60%\n",
      "Validation Batch 15, Loss: 0.900280, Accuracy: 85.52%\n",
      "Validation Batch 16, Loss: 0.875468, Accuracy: 85.55%\n",
      "Validation Batch 17, Loss: 0.941073, Accuracy: 85.20%\n",
      "Validation Batch 18, Loss: 0.869793, Accuracy: 85.24%\n",
      "Validation Batch 19, Loss: 0.928693, Accuracy: 85.03%\n",
      "Validation Batch 20, Loss: 0.828977, Accuracy: 85.31%\n",
      "Validation Batch 21, Loss: 0.891531, Accuracy: 85.34%\n",
      "Validation Batch 22, Loss: 0.887110, Accuracy: 85.37%\n",
      "Validation Batch 23, Loss: 0.909686, Accuracy: 85.33%\n",
      "Validation Batch 24, Loss: 0.947518, Accuracy: 85.09%\n",
      "Validation Batch 25, Loss: 0.890478, Accuracy: 85.00%\n",
      "Validation Batch 26, Loss: 0.891269, Accuracy: 84.98%\n",
      "Validation Batch 27, Loss: 0.800487, Accuracy: 85.20%\n",
      "Validation - Epoch 15, Loss: 0.887378, Accuracy: 85.20%\n",
      "Patienceâ€”18\n",
      "Epoch 16\n",
      "Batch 1, Loss: 1.041174, Accuracy: 68.75%\n",
      "Batch 2, Loss: 0.831872, Accuracy: 80.47%\n",
      "Batch 3, Loss: 0.818763, Accuracy: 84.38%\n",
      "Batch 4, Loss: 0.872170, Accuracy: 85.16%\n",
      "Batch 5, Loss: 1.010944, Accuracy: 82.50%\n",
      "Batch 6, Loss: 0.892484, Accuracy: 82.81%\n",
      "Batch 7, Loss: 0.892662, Accuracy: 83.04%\n",
      "Batch 8, Loss: 0.891159, Accuracy: 83.59%\n",
      "Batch 9, Loss: 0.919264, Accuracy: 83.33%\n",
      "Batch 10, Loss: 0.904609, Accuracy: 83.12%\n",
      "Batch 11, Loss: 0.892343, Accuracy: 83.38%\n",
      "Batch 12, Loss: 0.950690, Accuracy: 83.07%\n",
      "Batch 13, Loss: 0.888593, Accuracy: 83.29%\n",
      "Batch 14, Loss: 0.849980, Accuracy: 83.82%\n",
      "Batch 15, Loss: 0.932900, Accuracy: 83.65%\n",
      "Batch 16, Loss: 0.937511, Accuracy: 83.40%\n",
      "Batch 17, Loss: 0.912060, Accuracy: 83.27%\n",
      "Batch 18, Loss: 0.937366, Accuracy: 83.25%\n",
      "Batch 19, Loss: 0.889636, Accuracy: 83.22%\n",
      "Batch 20, Loss: 0.958244, Accuracy: 82.97%\n",
      "Batch 21, Loss: 0.914162, Accuracy: 82.96%\n",
      "Batch 22, Loss: 0.851959, Accuracy: 83.24%\n",
      "Batch 23, Loss: 0.992723, Accuracy: 82.88%\n",
      "Batch 24, Loss: 0.916121, Accuracy: 82.88%\n",
      "Batch 25, Loss: 0.896641, Accuracy: 83.00%\n",
      "Batch 26, Loss: 0.901873, Accuracy: 83.11%\n",
      "Batch 27, Loss: 0.876472, Accuracy: 83.22%\n",
      "Batch 28, Loss: 0.932114, Accuracy: 83.20%\n",
      "Batch 29, Loss: 0.949651, Accuracy: 83.03%\n",
      "Batch 30, Loss: 0.844531, Accuracy: 83.33%\n",
      "Batch 31, Loss: 0.831733, Accuracy: 83.62%\n",
      "Batch 32, Loss: 0.865429, Accuracy: 83.79%\n",
      "Batch 33, Loss: 0.970974, Accuracy: 83.57%\n",
      "Batch 34, Loss: 0.854560, Accuracy: 83.73%\n",
      "Batch 35, Loss: 0.980112, Accuracy: 83.53%\n",
      "Batch 36, Loss: 0.918412, Accuracy: 83.55%\n",
      "Batch 37, Loss: 0.868026, Accuracy: 83.70%\n",
      "Batch 38, Loss: 0.954224, Accuracy: 83.55%\n",
      "Batch 39, Loss: 0.884972, Accuracy: 83.61%\n",
      "Batch 40, Loss: 0.948788, Accuracy: 83.52%\n",
      "Batch 41, Loss: 0.975214, Accuracy: 83.38%\n",
      "Batch 42, Loss: 0.904608, Accuracy: 83.37%\n",
      "Batch 43, Loss: 0.872613, Accuracy: 83.39%\n",
      "Batch 44, Loss: 0.959696, Accuracy: 83.24%\n",
      "Batch 45, Loss: 0.923469, Accuracy: 83.19%\n",
      "Batch 46, Loss: 0.834773, Accuracy: 83.36%\n",
      "Batch 47, Loss: 0.869858, Accuracy: 83.44%\n",
      "Batch 48, Loss: 0.907575, Accuracy: 83.46%\n",
      "Batch 49, Loss: 0.885540, Accuracy: 83.55%\n",
      "Batch 50, Loss: 0.874994, Accuracy: 83.66%\n",
      "Batch 51, Loss: 0.932849, Accuracy: 83.64%\n",
      "Batch 52, Loss: 0.929793, Accuracy: 83.59%\n",
      "Batch 53, Loss: 0.885639, Accuracy: 83.67%\n",
      "Batch 54, Loss: 0.849770, Accuracy: 83.80%\n",
      "Batch 55, Loss: 0.952942, Accuracy: 83.69%\n",
      "Batch 56, Loss: 0.930971, Accuracy: 83.62%\n",
      "Batch 57, Loss: 0.917022, Accuracy: 83.61%\n",
      "Batch 58, Loss: 0.966714, Accuracy: 83.51%\n",
      "Batch 59, Loss: 0.929699, Accuracy: 83.45%\n",
      "Batch 60, Loss: 0.923598, Accuracy: 83.41%\n",
      "Batch 61, Loss: 0.860279, Accuracy: 83.50%\n",
      "Batch 62, Loss: 1.009477, Accuracy: 83.34%\n",
      "Batch 63, Loss: 0.862849, Accuracy: 83.41%\n",
      "Batch 64, Loss: 0.929063, Accuracy: 83.37%\n",
      "Batch 65, Loss: 0.969737, Accuracy: 83.27%\n",
      "Batch 66, Loss: 0.923906, Accuracy: 83.24%\n",
      "Batch 67, Loss: 0.856928, Accuracy: 83.30%\n",
      "Batch 68, Loss: 0.923535, Accuracy: 83.27%\n",
      "Batch 69, Loss: 0.954521, Accuracy: 83.20%\n",
      "Batch 70, Loss: 0.832424, Accuracy: 83.30%\n",
      "Batch 71, Loss: 0.947034, Accuracy: 83.25%\n",
      "Batch 72, Loss: 0.862763, Accuracy: 83.33%\n",
      "Batch 73, Loss: 0.894682, Accuracy: 83.39%\n",
      "Batch 74, Loss: 0.843666, Accuracy: 83.49%\n",
      "Batch 75, Loss: 0.989074, Accuracy: 83.38%\n",
      "Batch 76, Loss: 0.893332, Accuracy: 83.41%\n",
      "Batch 77, Loss: 0.874370, Accuracy: 83.44%\n",
      "Batch 78, Loss: 0.889807, Accuracy: 83.45%\n",
      "Batch 79, Loss: 0.906610, Accuracy: 83.48%\n",
      "Batch 80, Loss: 0.906744, Accuracy: 83.50%\n",
      "Batch 81, Loss: 0.894817, Accuracy: 83.53%\n",
      "Batch 82, Loss: 0.856704, Accuracy: 83.59%\n",
      "Batch 83, Loss: 0.868948, Accuracy: 83.64%\n",
      "Batch 84, Loss: 0.948209, Accuracy: 83.59%\n",
      "Batch 85, Loss: 0.874476, Accuracy: 83.62%\n",
      "Batch 86, Loss: 0.908720, Accuracy: 83.61%\n",
      "Batch 87, Loss: 0.877789, Accuracy: 83.64%\n",
      "Batch 88, Loss: 1.020847, Accuracy: 83.49%\n",
      "Batch 89, Loss: 0.901409, Accuracy: 83.50%\n",
      "Batch 90, Loss: 0.880112, Accuracy: 83.54%\n",
      "Batch 91, Loss: 0.915423, Accuracy: 83.52%\n",
      "Batch 92, Loss: 0.863176, Accuracy: 83.58%\n",
      "Batch 93, Loss: 0.929169, Accuracy: 83.53%\n",
      "Batch 94, Loss: 0.882894, Accuracy: 83.56%\n",
      "Batch 95, Loss: 0.867595, Accuracy: 83.63%\n",
      "Batch 96, Loss: 0.907303, Accuracy: 83.64%\n",
      "Batch 97, Loss: 0.889724, Accuracy: 83.67%\n",
      "Batch 98, Loss: 0.971005, Accuracy: 83.58%\n",
      "Batch 99, Loss: 0.934656, Accuracy: 83.55%\n",
      "Batch 100, Loss: 0.915500, Accuracy: 83.56%\n",
      "Batch 101, Loss: 0.941450, Accuracy: 83.52%\n",
      "Batch 102, Loss: 0.885740, Accuracy: 83.55%\n",
      "Batch 103, Loss: 0.953596, Accuracy: 83.51%\n",
      "Batch 104, Loss: 0.943631, Accuracy: 83.47%\n",
      "Batch 105, Loss: 0.963732, Accuracy: 83.41%\n",
      "Batch 106, Loss: 0.928466, Accuracy: 83.39%\n",
      "Batch 107, Loss: 0.874826, Accuracy: 83.40%\n",
      "Batch 108, Loss: 0.909825, Accuracy: 83.41%\n",
      "Batch 109, Loss: 0.901765, Accuracy: 83.41%\n",
      "Batch 110, Loss: 0.907094, Accuracy: 83.41%\n",
      "Batch 111, Loss: 0.927713, Accuracy: 83.39%\n",
      "Batch 112, Loss: 0.830328, Accuracy: 83.45%\n",
      "Batch 113, Loss: 0.895305, Accuracy: 83.48%\n",
      "Batch 114, Loss: 0.843610, Accuracy: 83.54%\n",
      "Batch 115, Loss: 0.922381, Accuracy: 83.53%\n",
      "Batch 116, Loss: 0.860636, Accuracy: 83.59%\n",
      "Batch 117, Loss: 0.881603, Accuracy: 83.61%\n",
      "Batch 118, Loss: 0.943848, Accuracy: 83.59%\n",
      "Batch 119, Loss: 0.835229, Accuracy: 83.65%\n",
      "Batch 120, Loss: 0.881031, Accuracy: 83.68%\n",
      "Batch 121, Loss: 0.852073, Accuracy: 83.73%\n",
      "Batch 122, Loss: 0.871701, Accuracy: 83.76%\n",
      "Batch 123, Loss: 0.936917, Accuracy: 83.74%\n",
      "Batch 124, Loss: 1.082941, Accuracy: 83.59%\n",
      "Batch 125, Loss: 0.897921, Accuracy: 83.60%\n",
      "Batch 126, Loss: 0.926993, Accuracy: 83.56%\n",
      "Batch 127, Loss: 0.860107, Accuracy: 83.60%\n",
      "Batch 128, Loss: 0.960927, Accuracy: 83.54%\n",
      "Batch 129, Loss: 0.868095, Accuracy: 83.58%\n",
      "Batch 130, Loss: 0.878866, Accuracy: 83.61%\n",
      "Batch 131, Loss: 0.966927, Accuracy: 83.54%\n",
      "Batch 132, Loss: 0.893786, Accuracy: 83.53%\n",
      "Batch 133, Loss: 0.964785, Accuracy: 83.49%\n",
      "Batch 134, Loss: 0.929418, Accuracy: 83.47%\n",
      "Batch 135, Loss: 0.963504, Accuracy: 83.44%\n",
      "Batch 136, Loss: 0.870390, Accuracy: 83.48%\n",
      "Batch 137, Loss: 0.910393, Accuracy: 83.49%\n",
      "Batch 138, Loss: 0.925290, Accuracy: 83.47%\n",
      "Batch 139, Loss: 0.886708, Accuracy: 83.50%\n",
      "Batch 140, Loss: 0.909482, Accuracy: 83.49%\n",
      "Batch 141, Loss: 0.872344, Accuracy: 83.52%\n",
      "Batch 142, Loss: 0.911036, Accuracy: 83.52%\n",
      "Batch 143, Loss: 0.841722, Accuracy: 83.57%\n",
      "Batch 144, Loss: 0.937460, Accuracy: 83.55%\n",
      "Batch 145, Loss: 0.905246, Accuracy: 83.55%\n",
      "Batch 146, Loss: 0.835531, Accuracy: 83.59%\n",
      "Batch 147, Loss: 0.917490, Accuracy: 83.58%\n",
      "Batch 148, Loss: 0.923753, Accuracy: 83.56%\n",
      "Batch 149, Loss: 0.963328, Accuracy: 83.52%\n",
      "Batch 150, Loss: 0.850583, Accuracy: 83.54%\n",
      "Batch 151, Loss: 0.878469, Accuracy: 83.56%\n",
      "Batch 152, Loss: 0.955481, Accuracy: 83.53%\n",
      "Batch 153, Loss: 0.878299, Accuracy: 83.55%\n",
      "Batch 154, Loss: 0.913452, Accuracy: 83.53%\n",
      "Batch 155, Loss: 0.925580, Accuracy: 83.52%\n",
      "Batch 156, Loss: 0.876382, Accuracy: 83.54%\n",
      "Batch 157, Loss: 0.946880, Accuracy: 83.52%\n",
      "Batch 158, Loss: 0.930986, Accuracy: 83.50%\n",
      "Batch 159, Loss: 0.898973, Accuracy: 83.50%\n",
      "Batch 160, Loss: 0.914005, Accuracy: 83.50%\n",
      "Batch 161, Loss: 0.889070, Accuracy: 83.50%\n",
      "Batch 162, Loss: 0.932766, Accuracy: 83.49%\n",
      "Batch 163, Loss: 0.898189, Accuracy: 83.50%\n",
      "Batch 164, Loss: 1.020067, Accuracy: 83.42%\n",
      "Batch 165, Loss: 0.835451, Accuracy: 83.48%\n",
      "Batch 166, Loss: 0.884372, Accuracy: 83.50%\n",
      "Batch 167, Loss: 0.890393, Accuracy: 83.50%\n",
      "Batch 168, Loss: 0.862826, Accuracy: 83.53%\n",
      "Batch 169, Loss: 0.896366, Accuracy: 83.53%\n",
      "Batch 170, Loss: 0.845021, Accuracy: 83.57%\n",
      "Batch 171, Loss: 0.906119, Accuracy: 83.56%\n",
      "Batch 172, Loss: 0.886097, Accuracy: 83.58%\n",
      "Batch 173, Loss: 0.967342, Accuracy: 83.54%\n",
      "Batch 174, Loss: 0.825380, Accuracy: 83.58%\n",
      "Batch 175, Loss: 0.794795, Accuracy: 83.64%\n",
      "Batch 176, Loss: 0.913516, Accuracy: 83.64%\n",
      "Batch 177, Loss: 0.880652, Accuracy: 83.65%\n",
      "Batch 178, Loss: 0.863123, Accuracy: 83.66%\n",
      "Batch 179, Loss: 0.857116, Accuracy: 83.69%\n",
      "Batch 180, Loss: 0.846256, Accuracy: 83.72%\n",
      "Batch 181, Loss: 0.843170, Accuracy: 83.75%\n",
      "Batch 182, Loss: 0.907518, Accuracy: 83.76%\n",
      "Batch 183, Loss: 0.894427, Accuracy: 83.76%\n",
      "Batch 184, Loss: 0.852272, Accuracy: 83.80%\n",
      "Batch 185, Loss: 0.910600, Accuracy: 83.79%\n",
      "Batch 186, Loss: 0.906544, Accuracy: 83.78%\n",
      "Batch 187, Loss: 0.926102, Accuracy: 83.77%\n",
      "Batch 188, Loss: 0.962107, Accuracy: 83.74%\n",
      "Batch 189, Loss: 0.899674, Accuracy: 83.75%\n",
      "Batch 190, Loss: 0.961599, Accuracy: 83.72%\n",
      "Batch 191, Loss: 0.862078, Accuracy: 83.75%\n",
      "Batch 192, Loss: 0.913885, Accuracy: 83.73%\n",
      "Batch 193, Loss: 0.953076, Accuracy: 83.71%\n",
      "Batch 194, Loss: 0.837048, Accuracy: 83.75%\n",
      "Batch 195, Loss: 0.903011, Accuracy: 83.74%\n",
      "Batch 196, Loss: 0.890169, Accuracy: 83.75%\n",
      "Batch 197, Loss: 0.897765, Accuracy: 83.75%\n",
      "Batch 198, Loss: 0.974789, Accuracy: 83.70%\n",
      "Batch 199, Loss: 0.833962, Accuracy: 83.74%\n",
      "Batch 200, Loss: 0.876476, Accuracy: 83.77%\n",
      "Batch 201, Loss: 0.887452, Accuracy: 83.78%\n",
      "Batch 202, Loss: 0.827824, Accuracy: 83.82%\n",
      "Batch 203, Loss: 0.888087, Accuracy: 83.82%\n",
      "Batch 204, Loss: 0.924144, Accuracy: 83.81%\n",
      "Batch 205, Loss: 0.975825, Accuracy: 83.77%\n",
      "Batch 206, Loss: 0.948574, Accuracy: 83.74%\n",
      "Batch 207, Loss: 0.961734, Accuracy: 83.71%\n",
      "Batch 208, Loss: 0.889982, Accuracy: 83.71%\n",
      "Batch 209, Loss: 0.903195, Accuracy: 83.71%\n",
      "Batch 210, Loss: 0.907502, Accuracy: 83.72%\n",
      "Batch 211, Loss: 0.905545, Accuracy: 83.72%\n",
      "Batch 212, Loss: 0.952809, Accuracy: 83.69%\n",
      "Batch 213, Loss: 0.887123, Accuracy: 83.69%\n",
      "Training - Epoch 16, Loss: 0.905500, Accuracy: 83.69%\n",
      "Validation Batch 1, Loss: 0.837246, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.844742, Accuracy: 91.41%\n",
      "Validation Batch 3, Loss: 0.961573, Accuracy: 86.98%\n",
      "Validation Batch 4, Loss: 0.881719, Accuracy: 87.11%\n",
      "Validation Batch 5, Loss: 0.833676, Accuracy: 87.50%\n",
      "Validation Batch 6, Loss: 0.828371, Accuracy: 88.28%\n",
      "Validation Batch 7, Loss: 0.892936, Accuracy: 87.72%\n",
      "Validation Batch 8, Loss: 0.947601, Accuracy: 86.52%\n",
      "Validation Batch 9, Loss: 0.922650, Accuracy: 85.94%\n",
      "Validation Batch 10, Loss: 0.926837, Accuracy: 85.47%\n",
      "Validation Batch 11, Loss: 0.859259, Accuracy: 85.65%\n",
      "Validation Batch 12, Loss: 0.845632, Accuracy: 85.94%\n",
      "Validation Batch 13, Loss: 0.894715, Accuracy: 85.94%\n",
      "Validation Batch 14, Loss: 0.904589, Accuracy: 85.71%\n",
      "Validation Batch 15, Loss: 0.899835, Accuracy: 85.62%\n",
      "Validation Batch 16, Loss: 0.875629, Accuracy: 85.64%\n",
      "Validation Batch 17, Loss: 0.940464, Accuracy: 85.29%\n",
      "Validation Batch 18, Loss: 0.868541, Accuracy: 85.42%\n",
      "Validation Batch 19, Loss: 0.927917, Accuracy: 85.28%\n",
      "Validation Batch 20, Loss: 0.828309, Accuracy: 85.55%\n",
      "Validation Batch 21, Loss: 0.890526, Accuracy: 85.57%\n",
      "Validation Batch 22, Loss: 0.885222, Accuracy: 85.58%\n",
      "Validation Batch 23, Loss: 0.908596, Accuracy: 85.53%\n",
      "Validation Batch 24, Loss: 0.946801, Accuracy: 85.29%\n",
      "Validation Batch 25, Loss: 0.888213, Accuracy: 85.19%\n",
      "Validation Batch 26, Loss: 0.890064, Accuracy: 85.16%\n",
      "Validation Batch 27, Loss: 0.797963, Accuracy: 85.44%\n",
      "Validation - Epoch 16, Loss: 0.886283, Accuracy: 85.44%\n",
      "Patienceâ€”19\n",
      "Epoch 17\n",
      "Batch 1, Loss: 0.918801, Accuracy: 81.25%\n",
      "Batch 2, Loss: 0.910129, Accuracy: 82.03%\n",
      "Batch 3, Loss: 0.862088, Accuracy: 84.38%\n",
      "Batch 4, Loss: 0.936689, Accuracy: 83.59%\n",
      "Batch 5, Loss: 0.920942, Accuracy: 83.12%\n",
      "Batch 6, Loss: 0.890420, Accuracy: 83.59%\n",
      "Batch 7, Loss: 0.895761, Accuracy: 83.71%\n",
      "Batch 8, Loss: 0.905117, Accuracy: 83.79%\n",
      "Batch 9, Loss: 0.901961, Accuracy: 84.03%\n",
      "Batch 10, Loss: 0.935559, Accuracy: 83.75%\n",
      "Batch 11, Loss: 0.922077, Accuracy: 83.38%\n",
      "Batch 12, Loss: 0.876554, Accuracy: 83.72%\n",
      "Batch 13, Loss: 0.871267, Accuracy: 84.01%\n",
      "Batch 14, Loss: 0.854950, Accuracy: 84.49%\n",
      "Batch 15, Loss: 0.963552, Accuracy: 84.06%\n",
      "Batch 16, Loss: 0.904734, Accuracy: 83.89%\n",
      "Batch 17, Loss: 0.884906, Accuracy: 84.19%\n",
      "Batch 18, Loss: 0.929437, Accuracy: 84.03%\n",
      "Batch 19, Loss: 0.937456, Accuracy: 83.72%\n",
      "Batch 20, Loss: 0.916395, Accuracy: 83.59%\n",
      "Batch 21, Loss: 0.960783, Accuracy: 83.41%\n",
      "Batch 22, Loss: 0.874103, Accuracy: 83.59%\n",
      "Batch 23, Loss: 0.873187, Accuracy: 83.76%\n",
      "Batch 24, Loss: 0.889123, Accuracy: 83.79%\n",
      "Batch 25, Loss: 0.915423, Accuracy: 83.62%\n",
      "Batch 26, Loss: 0.829677, Accuracy: 83.95%\n",
      "Batch 27, Loss: 0.901585, Accuracy: 83.97%\n",
      "Batch 28, Loss: 0.994389, Accuracy: 83.59%\n",
      "Batch 29, Loss: 0.868994, Accuracy: 83.73%\n",
      "Batch 30, Loss: 0.882861, Accuracy: 83.85%\n",
      "Batch 31, Loss: 0.853612, Accuracy: 84.07%\n",
      "Batch 32, Loss: 0.920744, Accuracy: 83.94%\n",
      "Batch 33, Loss: 0.924033, Accuracy: 83.90%\n",
      "Batch 34, Loss: 0.920629, Accuracy: 83.82%\n",
      "Batch 35, Loss: 0.920141, Accuracy: 83.79%\n",
      "Batch 36, Loss: 0.882099, Accuracy: 83.85%\n",
      "Batch 37, Loss: 0.877159, Accuracy: 83.91%\n",
      "Batch 38, Loss: 0.938227, Accuracy: 83.88%\n",
      "Batch 39, Loss: 0.876541, Accuracy: 83.93%\n",
      "Batch 40, Loss: 0.881705, Accuracy: 83.98%\n",
      "Batch 41, Loss: 0.864766, Accuracy: 84.11%\n",
      "Batch 42, Loss: 0.849673, Accuracy: 84.26%\n",
      "Batch 43, Loss: 0.855466, Accuracy: 84.38%\n",
      "Batch 44, Loss: 0.971466, Accuracy: 84.20%\n",
      "Batch 45, Loss: 0.938176, Accuracy: 84.10%\n",
      "Batch 46, Loss: 0.860805, Accuracy: 84.21%\n",
      "Batch 47, Loss: 0.868985, Accuracy: 84.31%\n",
      "Batch 48, Loss: 0.874201, Accuracy: 84.38%\n",
      "Batch 49, Loss: 0.926054, Accuracy: 84.31%\n",
      "Batch 50, Loss: 0.954695, Accuracy: 84.22%\n",
      "Batch 51, Loss: 0.838849, Accuracy: 84.34%\n",
      "Batch 52, Loss: 0.874715, Accuracy: 84.38%\n",
      "Batch 53, Loss: 0.884211, Accuracy: 84.43%\n",
      "Batch 54, Loss: 0.902991, Accuracy: 84.43%\n",
      "Batch 55, Loss: 0.870578, Accuracy: 84.49%\n",
      "Batch 56, Loss: 0.847160, Accuracy: 84.60%\n",
      "Batch 57, Loss: 0.838370, Accuracy: 84.70%\n",
      "Batch 58, Loss: 0.949297, Accuracy: 84.62%\n",
      "Batch 59, Loss: 0.961751, Accuracy: 84.48%\n",
      "Batch 60, Loss: 0.937611, Accuracy: 84.40%\n",
      "Batch 61, Loss: 0.903296, Accuracy: 84.40%\n",
      "Batch 62, Loss: 0.993465, Accuracy: 84.22%\n",
      "Batch 63, Loss: 0.904857, Accuracy: 84.20%\n",
      "Batch 64, Loss: 0.998795, Accuracy: 84.06%\n",
      "Batch 65, Loss: 1.025062, Accuracy: 83.85%\n",
      "Batch 66, Loss: 0.963288, Accuracy: 83.74%\n",
      "Batch 67, Loss: 0.893397, Accuracy: 83.75%\n",
      "Batch 68, Loss: 0.882081, Accuracy: 83.75%\n",
      "Batch 69, Loss: 0.855582, Accuracy: 83.83%\n",
      "Batch 70, Loss: 0.894545, Accuracy: 83.82%\n",
      "Batch 71, Loss: 0.927874, Accuracy: 83.76%\n",
      "Batch 72, Loss: 0.956820, Accuracy: 83.68%\n",
      "Batch 73, Loss: 1.014690, Accuracy: 83.52%\n",
      "Batch 74, Loss: 0.914778, Accuracy: 83.51%\n",
      "Batch 75, Loss: 0.907609, Accuracy: 83.52%\n",
      "Batch 76, Loss: 0.895809, Accuracy: 83.55%\n",
      "Batch 77, Loss: 0.858891, Accuracy: 83.64%\n",
      "Batch 78, Loss: 0.844453, Accuracy: 83.71%\n",
      "Batch 79, Loss: 0.926880, Accuracy: 83.70%\n",
      "Batch 80, Loss: 0.879856, Accuracy: 83.73%\n",
      "Batch 81, Loss: 0.900484, Accuracy: 83.74%\n",
      "Batch 82, Loss: 0.899909, Accuracy: 83.77%\n",
      "Batch 83, Loss: 0.864436, Accuracy: 83.83%\n",
      "Batch 84, Loss: 0.865416, Accuracy: 83.89%\n",
      "Batch 85, Loss: 0.927873, Accuracy: 83.86%\n",
      "Batch 86, Loss: 0.883732, Accuracy: 83.88%\n",
      "Batch 87, Loss: 0.956496, Accuracy: 83.80%\n",
      "Batch 88, Loss: 0.858519, Accuracy: 83.84%\n",
      "Batch 89, Loss: 0.867630, Accuracy: 83.87%\n",
      "Batch 90, Loss: 0.935095, Accuracy: 83.84%\n",
      "Batch 91, Loss: 0.871102, Accuracy: 83.89%\n",
      "Batch 92, Loss: 0.944389, Accuracy: 83.83%\n",
      "Batch 93, Loss: 0.940109, Accuracy: 83.80%\n",
      "Batch 94, Loss: 0.949629, Accuracy: 83.74%\n",
      "Batch 95, Loss: 0.945459, Accuracy: 83.70%\n",
      "Batch 96, Loss: 0.867418, Accuracy: 83.72%\n",
      "Batch 97, Loss: 0.868260, Accuracy: 83.76%\n",
      "Batch 98, Loss: 0.908574, Accuracy: 83.75%\n",
      "Batch 99, Loss: 0.898478, Accuracy: 83.79%\n",
      "Batch 100, Loss: 0.853863, Accuracy: 83.86%\n",
      "Batch 101, Loss: 0.865649, Accuracy: 83.90%\n",
      "Batch 102, Loss: 0.860438, Accuracy: 83.95%\n",
      "Batch 103, Loss: 0.914396, Accuracy: 83.95%\n",
      "Batch 104, Loss: 0.934303, Accuracy: 83.92%\n",
      "Batch 105, Loss: 0.941814, Accuracy: 83.88%\n",
      "Batch 106, Loss: 0.919046, Accuracy: 83.86%\n",
      "Batch 107, Loss: 0.916581, Accuracy: 83.85%\n",
      "Batch 108, Loss: 0.856069, Accuracy: 83.91%\n",
      "Batch 109, Loss: 0.951490, Accuracy: 83.86%\n",
      "Batch 110, Loss: 0.988811, Accuracy: 83.78%\n",
      "Batch 111, Loss: 0.954987, Accuracy: 83.73%\n",
      "Batch 112, Loss: 0.880422, Accuracy: 83.75%\n",
      "Batch 113, Loss: 1.003169, Accuracy: 83.64%\n",
      "Batch 114, Loss: 0.858815, Accuracy: 83.69%\n",
      "Batch 115, Loss: 0.855653, Accuracy: 83.74%\n",
      "Batch 116, Loss: 0.918363, Accuracy: 83.74%\n",
      "Batch 117, Loss: 0.971043, Accuracy: 83.69%\n",
      "Batch 118, Loss: 0.971525, Accuracy: 83.61%\n",
      "Batch 119, Loss: 0.880557, Accuracy: 83.61%\n",
      "Batch 120, Loss: 0.884360, Accuracy: 83.63%\n",
      "Batch 121, Loss: 0.865418, Accuracy: 83.66%\n",
      "Batch 122, Loss: 1.057872, Accuracy: 83.54%\n",
      "Batch 123, Loss: 0.876127, Accuracy: 83.57%\n",
      "Batch 124, Loss: 0.934880, Accuracy: 83.56%\n",
      "Batch 125, Loss: 1.008997, Accuracy: 83.47%\n",
      "Batch 126, Loss: 0.934619, Accuracy: 83.44%\n",
      "Batch 127, Loss: 0.875399, Accuracy: 83.48%\n",
      "Batch 128, Loss: 0.918236, Accuracy: 83.50%\n",
      "Batch 129, Loss: 0.886246, Accuracy: 83.53%\n",
      "Batch 130, Loss: 0.828049, Accuracy: 83.59%\n",
      "Batch 131, Loss: 0.935710, Accuracy: 83.58%\n",
      "Batch 132, Loss: 0.938874, Accuracy: 83.57%\n",
      "Batch 133, Loss: 0.922746, Accuracy: 83.55%\n",
      "Batch 134, Loss: 0.857162, Accuracy: 83.59%\n",
      "Batch 135, Loss: 0.961429, Accuracy: 83.55%\n",
      "Batch 136, Loss: 0.943218, Accuracy: 83.55%\n",
      "Batch 137, Loss: 0.922918, Accuracy: 83.53%\n",
      "Batch 138, Loss: 0.934620, Accuracy: 83.51%\n",
      "Batch 139, Loss: 0.927965, Accuracy: 83.50%\n",
      "Batch 140, Loss: 0.931123, Accuracy: 83.48%\n",
      "Batch 141, Loss: 0.892627, Accuracy: 83.49%\n",
      "Batch 142, Loss: 0.961376, Accuracy: 83.45%\n",
      "Batch 143, Loss: 0.876710, Accuracy: 83.48%\n",
      "Batch 144, Loss: 0.843917, Accuracy: 83.52%\n",
      "Batch 145, Loss: 0.960906, Accuracy: 83.48%\n",
      "Batch 146, Loss: 0.941891, Accuracy: 83.44%\n",
      "Batch 147, Loss: 0.907009, Accuracy: 83.44%\n",
      "Batch 148, Loss: 0.867835, Accuracy: 83.47%\n",
      "Batch 149, Loss: 0.946457, Accuracy: 83.42%\n",
      "Batch 150, Loss: 0.930655, Accuracy: 83.42%\n",
      "Batch 151, Loss: 0.887940, Accuracy: 83.44%\n",
      "Batch 152, Loss: 0.948539, Accuracy: 83.41%\n",
      "Batch 153, Loss: 0.836342, Accuracy: 83.46%\n",
      "Batch 154, Loss: 0.905648, Accuracy: 83.46%\n",
      "Batch 155, Loss: 0.877741, Accuracy: 83.49%\n",
      "Batch 156, Loss: 0.931745, Accuracy: 83.47%\n",
      "Batch 157, Loss: 0.942061, Accuracy: 83.45%\n",
      "Batch 158, Loss: 0.906258, Accuracy: 83.46%\n",
      "Batch 159, Loss: 0.859432, Accuracy: 83.50%\n",
      "Batch 160, Loss: 0.947304, Accuracy: 83.48%\n",
      "Batch 161, Loss: 0.981667, Accuracy: 83.42%\n",
      "Batch 162, Loss: 0.843388, Accuracy: 83.47%\n",
      "Batch 163, Loss: 1.008195, Accuracy: 83.41%\n",
      "Batch 164, Loss: 0.935830, Accuracy: 83.39%\n",
      "Batch 165, Loss: 0.905491, Accuracy: 83.41%\n",
      "Batch 166, Loss: 0.931045, Accuracy: 83.41%\n",
      "Batch 167, Loss: 0.885573, Accuracy: 83.41%\n",
      "Batch 168, Loss: 0.867832, Accuracy: 83.44%\n",
      "Batch 169, Loss: 0.907390, Accuracy: 83.43%\n",
      "Batch 170, Loss: 0.906778, Accuracy: 83.44%\n",
      "Batch 171, Loss: 0.978121, Accuracy: 83.39%\n",
      "Batch 172, Loss: 0.862007, Accuracy: 83.41%\n",
      "Batch 173, Loss: 0.919081, Accuracy: 83.42%\n",
      "Batch 174, Loss: 0.889395, Accuracy: 83.42%\n",
      "Batch 175, Loss: 0.985715, Accuracy: 83.38%\n",
      "Batch 176, Loss: 0.874996, Accuracy: 83.41%\n",
      "Batch 177, Loss: 0.858525, Accuracy: 83.43%\n",
      "Batch 178, Loss: 0.857701, Accuracy: 83.46%\n",
      "Batch 179, Loss: 0.896223, Accuracy: 83.47%\n",
      "Batch 180, Loss: 0.917940, Accuracy: 83.46%\n",
      "Batch 181, Loss: 0.881133, Accuracy: 83.49%\n",
      "Batch 182, Loss: 0.911164, Accuracy: 83.48%\n",
      "Batch 183, Loss: 0.807953, Accuracy: 83.55%\n",
      "Batch 184, Loss: 0.919430, Accuracy: 83.55%\n",
      "Batch 185, Loss: 0.876901, Accuracy: 83.56%\n",
      "Batch 186, Loss: 0.874269, Accuracy: 83.59%\n",
      "Batch 187, Loss: 0.913919, Accuracy: 83.59%\n",
      "Batch 188, Loss: 0.853505, Accuracy: 83.63%\n",
      "Batch 189, Loss: 0.882723, Accuracy: 83.65%\n",
      "Batch 190, Loss: 0.869828, Accuracy: 83.67%\n",
      "Batch 191, Loss: 0.875787, Accuracy: 83.69%\n",
      "Batch 192, Loss: 0.842216, Accuracy: 83.72%\n",
      "Batch 193, Loss: 0.892864, Accuracy: 83.74%\n",
      "Batch 194, Loss: 0.871574, Accuracy: 83.75%\n",
      "Batch 195, Loss: 0.914277, Accuracy: 83.75%\n",
      "Batch 196, Loss: 0.964295, Accuracy: 83.72%\n",
      "Batch 197, Loss: 0.900380, Accuracy: 83.72%\n",
      "Batch 198, Loss: 0.991345, Accuracy: 83.69%\n",
      "Batch 199, Loss: 0.896953, Accuracy: 83.68%\n",
      "Batch 200, Loss: 0.870808, Accuracy: 83.71%\n",
      "Batch 201, Loss: 0.908578, Accuracy: 83.71%\n",
      "Batch 202, Loss: 0.890715, Accuracy: 83.73%\n",
      "Batch 203, Loss: 0.882029, Accuracy: 83.74%\n",
      "Batch 204, Loss: 0.950553, Accuracy: 83.72%\n",
      "Batch 205, Loss: 0.948583, Accuracy: 83.70%\n",
      "Batch 206, Loss: 0.876966, Accuracy: 83.72%\n",
      "Batch 207, Loss: 0.953868, Accuracy: 83.69%\n",
      "Batch 208, Loss: 0.878648, Accuracy: 83.71%\n",
      "Batch 209, Loss: 0.894866, Accuracy: 83.71%\n",
      "Batch 210, Loss: 0.981145, Accuracy: 83.66%\n",
      "Batch 211, Loss: 0.828738, Accuracy: 83.70%\n",
      "Batch 212, Loss: 0.894397, Accuracy: 83.70%\n",
      "Batch 213, Loss: 0.807774, Accuracy: 83.75%\n",
      "Training - Epoch 17, Loss: 0.906691, Accuracy: 83.75%\n",
      "Validation Batch 1, Loss: 0.838389, Accuracy: 92.19%\n",
      "Validation Batch 2, Loss: 0.847358, Accuracy: 90.62%\n",
      "Validation Batch 3, Loss: 0.962489, Accuracy: 86.46%\n",
      "Validation Batch 4, Loss: 0.884048, Accuracy: 85.94%\n",
      "Validation Batch 5, Loss: 0.835836, Accuracy: 86.56%\n",
      "Validation Batch 6, Loss: 0.830130, Accuracy: 87.50%\n",
      "Validation Batch 7, Loss: 0.895316, Accuracy: 87.05%\n",
      "Validation Batch 8, Loss: 0.948856, Accuracy: 85.94%\n",
      "Validation Batch 9, Loss: 0.925232, Accuracy: 85.24%\n",
      "Validation Batch 10, Loss: 0.928671, Accuracy: 84.84%\n",
      "Validation Batch 11, Loss: 0.861963, Accuracy: 85.09%\n",
      "Validation Batch 12, Loss: 0.845209, Accuracy: 85.42%\n",
      "Validation Batch 13, Loss: 0.895971, Accuracy: 85.46%\n",
      "Validation Batch 14, Loss: 0.905007, Accuracy: 85.27%\n",
      "Validation Batch 15, Loss: 0.900591, Accuracy: 85.21%\n",
      "Validation Batch 16, Loss: 0.876018, Accuracy: 85.25%\n",
      "Validation Batch 17, Loss: 0.941215, Accuracy: 84.93%\n",
      "Validation Batch 18, Loss: 0.869566, Accuracy: 85.07%\n",
      "Validation Batch 19, Loss: 0.929390, Accuracy: 84.87%\n",
      "Validation Batch 20, Loss: 0.828807, Accuracy: 85.16%\n",
      "Validation Batch 21, Loss: 0.891237, Accuracy: 85.19%\n",
      "Validation Batch 22, Loss: 0.887875, Accuracy: 85.16%\n",
      "Validation Batch 23, Loss: 0.908779, Accuracy: 85.12%\n",
      "Validation Batch 24, Loss: 0.947021, Accuracy: 84.90%\n",
      "Validation Batch 25, Loss: 0.890901, Accuracy: 84.81%\n",
      "Validation Batch 26, Loss: 0.892522, Accuracy: 84.80%\n",
      "Validation Batch 27, Loss: 0.802901, Accuracy: 85.03%\n",
      "Validation - Epoch 17, Loss: 0.887826, Accuracy: 85.03%\n",
      "Patienceâ€”20\n",
      "Early stopping at epoch 17\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 130  \n",
    "patience = 20\n",
    "patience_counter = 3   \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)  \n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(images)  \n",
    "        loss = criterion(outputs, labels)  \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Batch {batch_idx+1}, Loss: {loss.item():.6f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Training - Epoch {epoch+1}, Loss: {train_loss:.6f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()  \n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for batch_idx, (images, labels) in enumerate(val_loader):\n",
    "            images, labels = images.to(device), labels.to(device)  \n",
    "\n",
    "            outputs = model(images) \n",
    "            loss = criterion(outputs, labels) \n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            print(f\"Validation Batch {batch_idx+1}, Loss: {loss.item():.6f}, Accuracy: {100 * val_correct / val_total:.2f}%\")\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    print(f\"Validation - Epoch {epoch+1}, Loss: {val_loss:.6f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accuracy_list.append(val_accuracy)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    print(f'Patienceâ€”{patience_counter}')\n",
    "\n",
    "    # Check for early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "model = best_model\n",
    "torch.save(model.state_dict(), f'{experiment}-{model_type}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
